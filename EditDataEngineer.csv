,Job_Title,Company,Salary,Location,Full_Description
0,Data Engineer,GUESS?. INC.,,"Los Angeles, CA","Design, develop and implement data integration components and backend solutions that support project and enterprise level data strategies while taking lead responsibility of all backend processes for all areas of the data warehouse.

ESSENTIAL FUNCTIONS:

Work closely with Data Analysts and Data Modelers to develop enterprise data integration solutions that promote reusability and standardization. Prepare and present data collection and analyses for business.
Modify and create new and existing backend processes using shell scripts/java/perl/python. Execute all phases of programming activities including program design, coding, debugging, testing, documentation and implementation.
Development of automated data quality checks in auditing collected data for completeness, accuracy, and errors. Re-Develop and tune existing Data Warehouse applications to ensure optimum performance.
Monitor existing Data Warehouse production processes and resolve issues as needed.

EDUCATION: Bachelor's Degree

YEARS OF EXPERIENCE: 2-4 Years
Location - City, Region or Area
Los Angeles, CA
Location
LA – World Headquarters"
1,Data Engineer,Park Jockey,,"San Francisco, CA","Who You’ll Work For

REEF Technology is the ecosystem that connects the world to your block. Each REEF hub is a thriving, connected ecosystem of businesses, cities and people, that enables and provides the delivery of products and services to more people than ever before. Each location offers a variety of services including micro-fulfillment centers, bike and scooter rental stations, electric vehicle charging, rideshare and autonomous vehicle buffering areas, community spaces for pop-up businesses, and more.


REEF Technology has reimagined the role of a parking facility. We are the largest network of parking lots in North America, believing these locations can do a lot more than just store your car. They serve as buffers for high density, high activity areas and, as such, alleviate congestion and the ensuing pollution. But, with the explosive growth of the sharing and on-demand economy, it is expected that the need for parking to solely store cars will be outgrown by other needs.


We are part of SoftBank, and its portfolio of leading companies transforming business and commerce at the cutting edge of technology in the world today


What You’ll Do

Develop, construct, integrate, and test large relational and non-relational databases to build new stable, scalable, rapid, and efficient databases
Ensure that the architecture will support the current and future business needs
Help with data collection and cleaning
Develop data processing pipelines for datasets that would be consumed by the data science teamWhat We Want From You
Degree in a quantitative field (e.g., Computer Science, Engineering, Mathematics, Statistics, Operations Research or other related fields)
Work with a team of data scientists and engineers to build scalable, end-to-end data science solutions
Experience working with diverse large-scale structured and unstructured datasets
Capable of writing production-level code for implementing data and machine-learning pipelines that are robust, scalable, and performant
Architect, design, and deploy data structures using best practices in data modeling, ETL and ELT processes
Work with technology teams to deploy and monitor data and machine-learning pipelines
Work with other teams to help them gather, model, and store data with best practices
Model data and metadata for ad-hoc and pre-built reporting
Experience with Big Data tools such as Spark and Hadoop
Experience with Greenplum, Postgres, and MySQL
Working with public clouds (e.g., AWS and/or Google cloud)
Desired but not Required

Hands-on work with graph databases


What We’ll Provide:
Medical
Dental
Vision
401K
Paid Time Off (PTO)

PHYSICAL DEMANDS:
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job.

Frequently operate small office equipment such as a computer, calculator, and copier/printer
Will remain in seated position for extended periods of time

WORKING CONDITIONS:

Work is performed indoors for extended periods of time, including up to the entire duration of shift.


REEF Technology is an equal opportunity employer, and we value diversity at our company. We don't discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

#Dice

Back Share
Apply Now"
2,Data Engineer,Screen Actors Guild- Producers Pension & Health Plans,,"Burbank, CA","Data Engineer
IT Applications DATAE01212
Apply now

Posted: August 9, 2019
Full-Time
Burbank, CA, USA
Job Details
Description
As a Data Engineer, you will design, develop, test and maintain efficient and sustainable data models to keep data accessible and ready for analysis. Working closely with the Analytics Team, you will engage with business teams to understand requirements, design conceptual, logical and physical data models, and perform root-cause analysis and recommend solutions. This will be a hands-on role utilizing Extract, Transfer & Load – ETL tools to deliver source to target mappings, physical and logical data models and related scripts to automate the data transformation and loading processes

Essential Job Functions:

Work closely with data experts to build and maintain KPI data dictionary, metadata, data standards, and ensure adherence to the Plans Analytics Method and data standards.
Engage business teams to understand requirements, document them and deliver robust and scalable solutions in the form of data models that can be leveraged for self-service analytics.
Build a data confidence model by establish and maintaining provenance, integrity and security of data used for self-service reporting, ad-hoc analysis or other levels of analysis.
Explore ways of modeling the Plans unstrcutured voice and text data into frameworks fit for analysis.
Design conceptual, logical and physical data models, maintain data dictionary and capture metadata.
Perform gap analysis as needed for purposes of maintaining, continuously enhancing data models and integrating KPI’s into the analytics platform as and when new KPI and business metrics are adapted by the organization.
Work with application developement team to deploy analytics data products through such ways as embeding analysis models into business applications and mobile solutions.
Utilize ETL tools and other data pipeline automation techniques to develop and maintain source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.
Ensure developed data models are easy to use and efficient to access data thus enable transaparency of data lineage to business teams and all stakeholders
Knowledge, Skills, & Abilities:

Proficient in leading business and internal team discussions to gather requirements, brain-storm and propose robust and scalable solutions; leverage business partner/unit input to enhance data models.
Expert knowledge of relational DBMS, specifically Oracle.
Expert knowledge in areas of advanced data techniques including unstructured and spatial data.


Competencies:

Customer focus - Gains insight into customer needs; identifies opportunities that benefit the customer; builds and delivers solutions that meet customer expectations; establishes and maintains effective customer relationships.
Decision quality – Makes sound decisions, even in the absence of complete information; relies on a mixture of analysis, wisdom, experience, and judgment when making decisions; considers all relevant factors and uses appropriate decision-making criteria and principles; recognizes when a quick 80% solution will suffice.
Communicates effectively – Is effective in a variety of communication settings: one-on-one, small and large groups, or among diverse styles and position levels; attentively listens to others; adjusts to fit the audience and the message; provides timely and helpful information to others across the organization; encourages the open expression of diverse ideas and opinions.
Ensures accountability – follows through on commitments and makes sure others do the same; acts with a clear sense of ownership; takes personal responsibility for decisions, actions, and failures; establishes clear responsibilities and processes for monitoring work and measuring results; designs feedback loops into work.
Instills trust – follows through on commitments; is seen as direct and truthful; keeps confidences; practices what he/she preaches; shows consistency between words and actions.
Manages Ambiguity- Operating effectively, even when things are not certain or the way forward is not clear.
Minimum Qualifications:

Bachelor’s Degree in Computer Science, Information Systems, or other related field as well as equivalent work experience.
Minimum 5 years of data engineering, data modeling or data architecture experience with a focus on multidimensional data modeling for both structured and unstructured data.
Experience designing conceptual, logical and physical data models and maintaining data dictionary and capturing metadata.
Experience creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards.
Experience in developing and maintaining source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.
Experience in relevant technical languages and tools such as SQL, Python, NoSQL, Airflow, Quartz, ERWIN or equivalent.

Qualifications
Motivations
Preferred
Goal Completion: Inspired to perform well by the completion of tasks
Self-Starter: Inspired to perform without outside help
Ability to Make an Impact: Inspired to perform well by the ability to contribute to the success of a project or the organization
Education
Required
Bachelors or better in Computer Science or related field.
Experience
Required
Relevant technical languages and tools such as SQL, Python, NoSQL, Airflow, Quartz, ERWIN or equivalent. Preferred qualifications
Developing and maintaining source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.
Creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards.
Designing conceptual, logical and physical data models and maintaining data dictionary and capturing metadata.
5 years: Data engineering, data modeling or data architecture experience with a focus on multidimensional data modeling for both structured and unstructured data.
Preferred
Work experience in the HealthCare industry preferred"
3,Data Engineer,Clarify Health Solutions.,,"San Francisco, CA","COMPANY OVERVIEW
Clarify has amassed an unrivaled dataset in healthcare representing over 150 million lives with longitudinal clinical, financial, social and behavioral data that paints a complete picture of individual wellness. What if you could use this data to precisely identify patients at risk while giving clinicians actionable insights to deliver better care? What if payers could predict the total healthcare cost of a person to optimize their care? What if pharmaceutical companies could bring lifesaving drugs to market faster and with fewer side effects? What if we could all receive a personalized care experience?

Come be part of figuring out the answers to these and other questions.

Clarify has built a machine learning platform on top of this huge dataset leveraging the latest cloud and big data technologies to run hundreds of ML models in real-time to predict and improve health outcomes. Our leadership team hails from top healthcare and fintech companies like Health Catalyst, Advent, and Microsoft. And our business team, from McKinsey and Boston Consulting Group, have decades of experience driving change within some of the largest provider, payer and life science companies.
The Role
Contributing to the design, development, and implementation of custom data models to solve real-world problems.
Conducting machine learning on a growing database of longitudinal health information, with the goal of widening the breadth of applications while increasing the fidelity of our models in an automatable fashion
Introducing new technologies and solutions to our data engineering processes that will facilitate machine learning at a higher scale
Utilizing ETL tools to integrate build production and post-production data pipelines that move data from a variety of sources into a warehouse, monitor data quality, check for errors and conform data to standards
Participating in data team workshops by interpreting business problems and sometimes complex statistical approaches into actions, balancing creativity with engineering practicality.
What we are looking for
We are a small team, still in the early phases of startup growth. As such, we are ideally looking for “all-around athletes” with strong leadership, analytics, and communication skills. We are also looking for new team members who will be a solid fit with our culture and have a strong passion for impact.
This job opening to join in Data Engineering is the general application to use if you don’t see a particular opening on our website. We are in a rapid growth stage and are looking for great candidates at all levels.
Please note that, given the nature of customer and prospect discussions, some limited travel may be required.
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, on the basis of disability or any other federal, state or local protected class."
4,Data Engineer,Albertsons,,"Pleasanton, CA 94588","Albertsons-Safeway Company is one of the largest food and drug retailers with 2,300+ stores. The Albertsons-Safeway family of brands includes some of the most prominent brands in food retailing, with a growing base of loyal shoppers. Thanks to the professionalism, diversity, spirit and friendliness of our people, we have locations across the U.S.

The Information Technology Department has an opening for a Data Engineer. This position is located in Pleasanton, California.


Position Purpose

Work on various projects to optimize data collection, integration, aggregation, analysis, and data visualizations that support business objectives in a Big Data Cloud environment.


Key Responsibilities include, but are not limited to:

Work in conjunction with other augmented staff to ingest data from varying technologies including internal and external sources. Incremental and historical data are to be ingested with an emphasis on streaming and a framework - Content Integration Network (CIN) will be developed with a goal to:

Create re-usable components
Auditability and logging
Governance and Security
Standards for DevOps manageability
Data Quality integrated
Configuration driven


Qualifications:

3+ years in a lead role with proven experience in planning, managing work, and leading development teams to meet deliverable dates.
3+ years proven experience in and providing guidance to team members to yield quality deliverables and grow their skills
8+ years proven experience in developing and deploying in the Cloud; Azure and Snowflake experience is a plus
8+ years proven expertise in creating pipelines for real time and near real time integration
3+ years proven experience with Spark SQL, Spark Streaming and using Core Spark API to explore Spark features to build data pipelines
2+ years proven experience with Python
Databricks and Delta table knowledge is a plus
Extensive experience in data transformations for various business use cases.
Knowledge for handling exceptions and automated re-processing and reconciling
Passion for Data Quality with an ability to integrate these capabilities into the deliverables
Prior use of Big Data components and the ability to rationalize and align their fit for a business case
Experience in working with different data sources - flat files, XML files and databases
Proficiency in techniques for slowly changing dimensions
Knowledge of Jenkins for continuous integration and End-to-End automation for application build and deployments
Proven experience and ability to work with people across the organization
Proven capabilities for strong written and oral communication skills
Ability to integrate into a project team environment and contribute to project planning activities
Experience in developing implementation plans and schedules and preparing documentation for the jobs according to the business requirements


How to Apply: Interested candidates are encouraged to submit a resume by visiting https://www.albertsonscompanies.com/careers.html


Diversity is fundamental at Albertsons-Safeway. We foster an inclusive working environment where the different strengths and perspectives of each employee is both recognized and valued. We believe that building successful relationships with our customers and our communities is only possible through the diversity of our people. A diverse workforce leads to better teamwork and creative thinking, as well as mutual understanding and respect.


The Albertsons-Safeway policy is to provide employment, training, compensation, promotion and other conditions of employment without regard to race, color, religion, sexual orientation, gender identity, national origin, sex, age, disability, veteran status, medical condition, marital status or any other legally protected status.


We support a drug-free workplace - all applicants offered a position are required to pass a pre-employment drug test before they are hired.


AN EQUAL OPPORTUNITY EMPLOYER"
5,Data Engineer,University of California San Francisco,,"San Francisco, CA","The Arnaout Lab at UCSF seeks an experienced Data Engineer to whom wants to participate in cutting-edge research with transformational impact to clinical and research medicine across a wide array of diseases, working with decades of high-quality medical data alongside clinical domain experts. The successful candidate will use local, hybrid, and / or cloud computing to develop strategies and software / hardware pipelines for modular, secure automation, scaling, and crowdsourcing of data mining, preprocessing, storage, labeling and computing. The position also provides opportunities to publish, present at research conferences, and for professional advancement.

Develops and optimizes a variety of computational, data science, and CI research tools and components. Performs research on current and future HPC, data, and CI technologies, hardware and software projects. Works on algorithm development, optimization, programming, performance analysis and / or benchmarking assignments of moderate scope where the tasks involve knowledge of either domain / computer science research requirements and / or CI design / implementation requirements.

MEDICINE / CARDIOLOGY / ARNAOUT LABORATORY
The Arnaout laboratory studies deep and machine learning for biomedical imaging and related clinical data, with the goals of decreasing diagnostic error and developing and scaling novel phenotypes to drive precision medicine research.

UCSF is a top-10 medical center and a leader in cross-campus efforts to mine, harmonize, and analyze multi-modal clinical data for the University of California’s 15 million patients.

The Arnaout laboratory is part of both the Bakar Computational Health Sciences Institute, where the abovementioned efforts are based, and the nationally ranked Department of Medicine (DOM). Projects focus on deep learning for medical imaging, and through collaborative work with intra- and inter-institutional partners, touch the electronic health record, genetics, and other sources of data.

ABOUT UCSF

The University of California, San Francisco (UCSF) is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It is the only campus in the 10-campus UC system dedicated exclusively to the health sciences."
6,College Intern - Big Data Engineer,LOCKHEED MARTIN CORPORATION,,"Fort Worth, TX 76137","This individual will perform as an integral team member of the F-35 Sustainment Performance and Decision Analytics team, supporting the sustainment of the delivered F-35 aircraft and Performance Based Logistics (PBL) reporting and analyses. Candidate will research, develop, validate, and document complex data relationships in a data warehouse environment and create reusable SQL queries and/or reports, as well as visualize data using applications such as Tableau.
Basic Qualifications:
Pursuing Bachelor's or Master's degree from an accredited college or university in an engineering or technical field of study.

Coursework in software tools for storing, extracting, transforming, loading, and visualizing big data.
Desired Skills:
Database knowledge (Oracle or HANA)
SQL knowledge
Data visualization knowledge (Tableau)
Software development knowledge (Python, Java, C#, or C++)
BASIC QUALIFICATIONS:
job.Qualifications

Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.

As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.
EXPERIENCE LEVEL:
Co-op/Summer Intern"
7,Data Engineer,"Amazon.com-Amazon.com Services, Inc.",,"Seattle, WA","Bachelor’s degree in CS or related technical field3+ years experience in dimensional data modeling, ETL development, and Data WarehousingExperience with Redshift and/or other distributed computing systems.Excellent knowledge of SQL and Linux OSSQL performance tuningServer management and administration including basic scriptingBasic DBA tasksSolid experience in at least one business intelligence reporting too

Are you passionate about data? Does the prospect of dealing with massive volumes of data excite you? Do you want to build data engineering solutions that process billions of records a day in a scalable fashion using AWS technologies? Do you want to create the next-generation tools for intuitive data access?

Amazon's Finance Technology team is seeking an outstanding Data Engineer to join the team that is shaping the future of the finance data platform. The team is committed to building the next generation big data platform that will be one of the world's largest finance data warehouse to support Amazon's rapidly growing and dynamic businesses, and use it to deliver the BI applications which will have an immediate influence on day-to-day decision making. Amazon has culture of data-driven decision-making, and demands data that is timely, accurate, and actionable. Our platform serves Amazon's finance, tax and accounting functions across the globe.

As a Data Engineer, you should be an expert with data warehousing technical components (e.g. Data Modeling, ETL and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of extremely large data-sets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions. You should be enthusiastic about learning new technologies and be able to implement solutions using them to provide new functionality to the users or to scale the existing platform. Excellent written and verbal communication skills are required as the person will work very closely with diverse teams. Having strong analytical skills is a plus. Above all, you should be passionate about working with huge data sets and someone who loves to bring data-sets together to answer business questions and drive change.

Our ideal candidate thrives in a fast-paced environment, relishes working with large transactional volumes and big data, enjoys the challenge of highly complex business contexts (that are typically being defined in real-time), and, above all, is a passionate about data and analytics. In this role you will be part of a team of engineers to create world's largest financial data warehouses and BI tools for Amazon's expanding global footprint.

Responsibilities:
Design, implement, and support a platform providing secured access to large datasets.Interface with tax, finance and accounting customers, gathering requirements and delivering complete BI solutions.Model data and metadata to support ad-hoc and pre-built reporting.Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.Tune application and query performance using profiling tools and SQL.Analyze and solve problems at their root, stepping back to understand the broader context.Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.

Master’s degree in Information Systems or a related field.Knowledge of Big Data Solutions. Experience with Hadoop, Hive or Pig.Experience with Redshift and other AWS services.Excellent communication (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teams.Knowledge of a programming or scripting language (R, Python, Ruby, or JavaScript).Experience with Java and Map Reduce frameworks such as Hive/Hadoop.Strong organizational and multitasking skills with ability to balance competing priorities.An ability to work in a fast-paced environment where continuous innovation is occurring and ambiguity is the norm.
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation"
8,Google Data Engineer,Accenture,,"Los Angeles, CA","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet today’s high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on GCP and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security (Cloud IAM, Data Loss Prevention API, etc)Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutionsMinimum of 3 years of hands-on experience in GCP and Big Data technologies such as Java, Node.js, C##, Python, PySpark, Spark/SparkSQL, Hadoop, Hive, Pig, Oozie and streaming technologies such as Kafka, Stream Ingestion API, Unix shell/Perl scripting etc.
Extensive experience providing practical direction with the GCP Native and Hadoop ecosystem
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Experience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps – Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plus
Understanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus


Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
9,Data Engineer,Federal Reserve Bank of San Francisco,,"San Francisco, CA 94105","Data Engineer-260669
Federal Reserve Bank of San Francisco
Primary Location CA-San Francisco
Other Locations UT-Salt Lake City
Full-time / Part-time Full-time
Employee Status Temporary
Overtime Status Exempt
Job Type Experienced
Travel Yes, 5 % of the Time
Shift Day Job
Job Sensitivity Tier II - No Credit Check

The Federal Reserve Bank of San Francisco is looking for a Data Engineer for a temporary assignment (one year) here in our San Francisco location. As a term employee of the Fed, you are salaried and benefited, and work directly for the Bank for a defined period of time. Position has potential to become full time/regular after one year or sooner.


The Advanced Data and Analytics Capabilities team leads and develops solutions for various business lines in the system as well as National IT. We employ state of the art technologies that are part of the Hadoop ecosystem, which includes tools used for data integration, data modeling, and data analytics. You will have an opportunity to apply your critical thinking and technical skills across many disciplines.

In this role, you will contribute to high quality technology solutions that address business needs by developing utilities for the platform or applications for the customer business lines and providing production support. You should have strong communication skills as you will work closely with other groups, including development and testing efforts of your assigned application components to ensure the successful delivery of the project.


Essential Duties and Responsibilities:

Develop code on common utilities in Big Data environments using Scala/Python/Java/Scripting etc.
Provide end-to-end support for solution integration, including designing, developing, testing, deploying, and supporting solutions on Hadoop environment
Build schedules, scripts and development of new mappings and workflows
Build workflows covering source code development through go-live
Build Run Books and troubleshooting guides for different types of workflows and Control-M jobs in the Big Data environment
Test submitted software changes prior to production roll outs
Develop, execute and document unit test plans and support application testing
Assist in the deployment of new modules, upgrades and fixes to the production environment
Validate deployment to staging and production environments
Provide operational and production support for applications and utilities
Tackle issues and participate in defect and incident root cause analyses
Collaborate with Developers, DevOps, Release Management and Operations
Maintain security in accordance with Bank security policies
Participate in an Agile development environment by attending daily standups and sprint planning activities
Create change management packages and implementation plans for migration to different environments
Automate execution of batch applications using Control-M
Assist in technical writing on Big Data components
Assist in testing upgrades of Big Data environments
Should be open to cross-training and assignments with other division groups
Contribute to initiatives such as mining new data sources, developing data tools, evaluating data visualization software tools, or developing documentation
Explore data to derive business insight
Independently determine methods and procedures on new assignments, and may provide work direction to others
Analysis of complex issues, situations and data utilizing your in-depth evaluation of variable factors for resolution
Use your judgment and analytical skills in selecting methods, techniques and evaluation criteria for obtaining results
Qualifications:

Undergraduate degree in computer science, MIS, engineering, statistics, data science or related field
At senior level, requires five or more years of relevant technical or business work experience; at Lead level, requires seven or more years of relevant technical or business work experience
3+ years programming skills in Java or Python or Scala preferred
Knowledge of HDFS data distribution and processing
Understanding of Hive/Impala/Spark
Knowledge of Hadoop ecosystem, machine learning algorithm/ text analytics
Strong skills in programming and scripting on UNIX / Linux. (i.e. Python or Bash)
Experience with CTRL-M, Cron and scheduling of batch jobs
Experience with workflow processing on the Hadoop ecosystem including Oozie, NiFi, etc.
Passion for technology and data, a critical thinker, problem solver and a self-starter
Strong quantitative and analytical skills
Strong attention to detail
Ability to communicate effectively (both verbal and written) and work in a team environment
Ability to balance multiple assignments and shift gears when new priorities arise
Experience performing 24 by 7 Production Support on applications
Familiar with Agile methodologies
Ability to learn and document an existing system
Strong analytical skills
Must be a US Citizen or Green Card holder
Nice to have:

Working experience at Government or quasi-Government organizations
Cloud experience and using big data technologies on the Cloud
The Federal Reserve Bank of San Francisco believes in the diversity of our people, ideas, and experiences and are committed to building an inclusive culture that is representative of the communities we serve.

The Federal Reserve Bank of San Francisco is an Equal Opportunity Employer."
10,Data Engineer,Asics Digital,,"Boston, MA","Are you looking for the opportunity to shape the future of data engineering at a fast-growing company? If you’re a Data Engineer, looking for the next challenge to help scale our production and analytics databases to handle massive volume, then the Platforms team at ASICS Digital has the role for you!

ASICS Digital creates and brings to market cutting-edge digital apps and products for the ASICS brand. With over 50 million users, we possess an incredible wealth of data rooted in fitness training, goal-setting, and how technology can contribute to a healthy lifestyle. Managing and making sense of all that data is an extraordinary challenge and opportunity.

The Data Engineer will help build, run and improve the current data engineering platform while contributing to the design and rollout of the next platform iteration. You’ll be working cross-functionally with several teams including Analytics, Product, Marketing and eCommerce. Your key focuses will be building up our data warehousing infrastructure and providing best practices for data engineering.

ASICS Digital is a division of ASICS Corporation based in Boston, Massachusetts. Our goal is to build innovative digital technologies and commerce experiences to better connect consumers to the ASICS brand. We are responsible for the continued development of innovative eCommerce solutions and other digital services that inspire people to move, get fit and stay healthy.
What You'll Do
Design, implement, maintain, and QA the data warehousing and other data engineering platforms
Contribute to the direction, tooling, and platforms that deliver data to the global business and it’s initiatives
Working closely with our regional and internal stakeholders; our Product, Marketing and eCommerce teams, to build out necessary data warehousing infrastructure
Own and drive data-engineering initiatives autonomously; identifying issues and providing solutions
Maintain and scale our ETL tooling process to ensure flexibility for future enhancements
Tune and implement improvements—in terms of quality, reliability, scalability, or data access—for our production data stores
Bring an analytical mindset to every conversation across the company
Provide guidance and mentorship to other team members
What You'll Have
3+ years experience with Data Warehousing principles and practices
3+ years experience handling large sets of data (i.e. hundreds of millions of rows of data) and make it performant
Experience with Java and/or PythonExperience with any ETL tool such as Informatica, Pentaho, Matillion, AWS Glue
Experience working with relational databases
Experience with NewSQL databases
Experience testing data integrity with automation
Experience with docker or other linux containers
Interest in AWS services
Nice To Have
Experience with AWS services (e.g. lambda, S3)
Experience with Snowflake Cloud Data Warehouse
Exposure to the platforms, processes, and practices related to Continuous Integration and Continuous Delivery (CI/CD)
 ASICS Digital is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran status, or fitness level."
11,Data Engineer,Brave,,"San Francisco, CA","We are looking for an experienced data engineer to be responsible for the design, development and maintenance of large data collection and processing systems at Brave. This position will focus on scaling our backend systems to handle the increased load of data collection, processing and presentation.

-----------------

Responsibilities:
-----------------


The position would primarily work with the statistics and engineering teams, along with day to day communication with the product and marketing team.
Design and develop AWS systems to collect and manage business events at scale.
Respond to request from the product, engineering and marketing team for timely data analysis.
Participate in multi-team planning for future data collection needs.

-------------

Requirements:
-------------

Strong AWS skills with extensive knowledge of the data collection, storage and processing systems that AWS provides. Experience building ETL pipelines at scale.

Familiarity with Redshift, Kinesis, Athena, Glue, Data Pipeline, MongoDB, Postgresql.

Strong JavaScript and SQL skills.

Extensive knowledge of modern open source software develop practices and tools i.e. Git, Github, Unit and integration testing, pull requests, code review etc…

Additional qualities:

Strives to deliver clean, maintainable and testable code
Has a passion for user privacy
Is open to learning new languages and frameworks
Has a soft spot for JavaScript

Benefits


Competitive salary
4 weeks (20 days) of paid vacation per year
Excellent medical coverage
Generous 401k plan
Stock option grant
Travel and conference budgets
Commuters benefit (On­site only)
Hip office in the SoMA neighborhood of SF

Candidates must be legally authorized to work in the United States or Canada."
12,Data Engineer,Deloitte,,"San Jose, CA 95113","Analytics & Cognitive – Project Delivery Senior Analyst – Data Engineer


Are you an experienced, passionate data scientist in technology – a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity within Deloitte’s Project Delivery Model (PDM) offering.


Work you’ll do/Responsibilities

Implement data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data.
Communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work
Develop technical solutions to business problems


The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.


The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:

Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements


Qualifications

Required

Bachelor-level degree in engineering, information technology, data communications, telecommunications, computer science, or equivalent professional experience and/or qualifications
Two (2+) years of experience in Python, R or SAS
Two (2+) years or experience in Unix Shell Scripting
Two (2+) years of experience in building scalable and high-performance data pipelines using Apache Hadoop, Map Reduce, Pig & Hive
Previous experience with big data cross platform compatible file formats like Apache Avro & Apache Parquet
Advanced knowledge of SQL/Hive
Experience in data analysis and ability to drive and extrapolate insights from business questions


Additional Requirements

Must be willing to live and work in the Greater San Jose, CA area. Relocation assistance provided to qualifying candidates.


How you’ll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career.


Benefits

At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits.


Deloitte’s culture

Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives.


Corporate citizenship

Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.


Recruiter tips

We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals."
13,Data Engineer,Invoice2go,,"Redwood City, CA","Location: Redwood City

Job Description:
The data team as Invoice2go is dedicated to supporting the rest of the company by transforming our raw data into high quality and high fidelity insights. We're building data sets for an array of uses including performance analysis, personalization, growth and marketing communications. Engineers on this team are critical to making sure Invoice2go continues to be one of the top business apps in fintech.

What You'll Be Doing


Using Databricks and Spark to transform our largest data feeds into a columnar storage format for easy, cost-effective access in S3
Experimenting with Spectrum to allow queries that run on both Redshift and S3
Optimizing our ETL process to make sure we can provide faster data without sacrificing quality

Who We Are Looking For


Someone who can collaborate with product owners to experiment, iterate and deliver new tools
Can learn new languages as needed, depending on the best framework or tool for the job
Strong understanding of schema design and modeling for large datasets.
Someone who loves what they do, is passionate about software development and wants to contribute to the company culture everyday
Experience with SQL and Python is helpful
Someone that owns their work, from conception to release and beyond.

------

------

-----------
ABOUT US...
-----------

Invoice2go is the world's top selling invoicing app, but we haven't stopped there. Equipping business owners with the most straightforward way to run a business, Invoice2go brings together all the tools needed to get the job done: From winning jobs, tracking estimates and payments, and offering the ability to pay any way.

We strive to give small businesses control over their time and their business.

Invoice2go was founded in Australia by Chris Strode, a small business owner who came from a family of tradespeople, and wanted to help them streamline their invoicing.

Today, we are backed by $60 million in funding from Accel, Ribbit Capital and OCV Partners, and trusted by business owners across 160 countries to send $24 billion in invoicing every year. The company employs a world-class team from it's offices in Redwood City, California and Sydney, Australia.

We're working hard to solve big challenges for the smallest of businesses, and we're always looking for talented people to join our team!!

Invoice2go is an equal opportunity employer. In accordance with applicable law, we prohibit discrimination against any applicant or employee based on any legally-recognized basis, including, but not limited to: veteran status, uniformed service member status, race, color, religion, sex, age (40 and over), pregnancy (including childbirth, lactation and related medical conditions), national origin or ancestry, physical or mental disability, genetic information (including testing and characteristics) or any other consideration protected by federal, state or local law. Our commitment to equal opportunity employment applies to all persons involved in our operations and prohibits unlawful discrimination by any employee, including supervisors and co-workers."
14,Big Data Engineer,primesoftinc,,"San Francisco, CA","Role: Big Data Engineer

Exp: 9+ Years

Loc: San Franscisco, CA

Visa: Independent candidates

MOI: Telephonic+Skype

Skills Required:

must have work experience on Spark (Java or Scala or Python),

Hands on experience with managing production clusters (Hadoop, Kafka, Spark, more)
Hive
3-5 years of SQL and Data Modeling Development experience with Hadoop data tool ecosystem (Spark, Hive, Pig, MapReduce, etc.)

Hadoop (Spark jobs/Spark transformations)"
15,Data Engineer,University of Colorado,,"Aurora, CO","University of Colorado | CU Anschutz Medical Campus
Center for Biomedical Informatics and Personalized Medicine
Health Data Compass
Data Engineer (IT Professional)
Position #00764891 – Requisition #16925

* Applications are accepted electronically ONLY at www.cu.edu/cu-careers *

The University of Colorado Anschutz Medical Campus seeks individuals with demonstrated commitment to creating an inclusive learning and working environment. We value the ability to engage effectively with students, faculty and staff of diverse backgrounds.

Center for Biomedical Informatics and Personalized Medicine Health Data Compass
has an opening for a full-time University Staff (unclassified) Data Engineer (IT Profressional) position.

The University of Colorado Anschutz Medical Campus is a public education, clinical and research facility serving 4,500 students, and a world-class medical destination at the forefront of life-changing science, medicine, and healthcare. CU Anschutz offers more than 42 highly rated degree programs through 6 schools and colleges, and receives over $500 million in research awards each year. We are the single largest health professions education provider in Colorado, awarding nearly 1,450 degrees annually. Powered by our award-winning faculty, renowned researchers and a reputation for academic excellence, the CU Anschutz Medical Campus drives innovation from the classroom to the laboratory to the delivery of unparalleled patient care. Read CU Anschutz Quick Facts here.

Health Data Compass is the technology innovation arm of the Colorado Center for Personalized Medicine, CU’s banner initiative to improve patient health through the science of personalized medicine. Our team is implementing a data integration and analytics platform on cutting-edge, cloud-based technologies to support biomedical research and clinical excellence throughout the CU research enterprise and our hospital partners. Compass services are hosted on cutting-edge, cloud-based technologies.
Nature of Work:
The primary responsibility of the Data Engineer will be to design and develop code, applications, and scripts to support Compass’s services which include the enterprise data warehouse and analytics platform. The Data Engineer will also perform a variety of tasks to deliver technology solutions, including participating in data warehouse architecture and data modeling, as well as working with team members produce code to support cloud-based solutions.
Examples of Work Performed:
Designs and develops code and processes with Google Cloud Platform applications and tools, as well as open source/commercial tools and python scripting. (25%)
Validates, unit tests, and peer reviews code, monitors performance, and performs tuning as necessary. (25%)
Helps drive technical strategies in support of the data warehouse and infrastructure platforms. (15%)
Creates and updates documentation to support development work, including data source to target mappings, code review logs, validation results, execution logs, profiling results, etc. (15%)
Translates business and customer needs into system solutions using rigorous logic and methods to solve difficult problems with effective solutions. (10%)
Manages incidents and resolves problems with thorough analysis, troubleshooting and validation techniques. (10%)
Salary and Benefits:

Salary is negotiable and commensurate with skills and experience.

Your total compensation goes beyond the number on your paycheck. The University of Colorado provides generous leave, health plans and retirement contributions that add to your bottom line.
Benefits: https://www.cu.edu/employee-services/benefits.
Total Compensation Calculator: https://www.cu.edu/employee-services/total-compensation
Diversity and Equity:

Please click here for information on disability accommodations: http://www.ucdenver.edu/about/departments/HR/jobs/Pages/JobsatCUDenver.aspx

The University of Colorado Denver | Anschutz Medical Campus is committed to recruiting and supporting a diverse student body, faculty and administrative staff. The university strives to promote a culture of inclusiveness, respect, communication and understanding. We encourage applications from women, ethnic minorities, persons with disabilities and all veterans. The University of Colorado is committed to diversity and equality in education and employment.

The University of Colorado Denver | Anschutz Medical Campus is dedicated to ensuring a safe and secure environment for our faculty, staff, students and visitors. To assist in achieving that goal, we conduct background investigations for all prospective employees.


Qualifications

Minimum Qualifications:
Bachelor’s degree in Computer Science, Computer Information Systems or a directly related field from an accredited institution.
One (1) year programming projects or applications or Data Warehousing and/or Business Intelligence experience.

Preferred Qualifications:
Experience with healthcare data from EMRs like Epic.
Experience with any type of -omics data.
Experience with Google Cloud Platform.
Experience with secure coding practices.
Familiarity with HIPAA regulations and their application to technology systems,

Knowledge, Skills, and Abilities:
Strong technical skills relating to design, development, performance and validation.
Programming skills with Google Cloud Platform products and applications.
Working knowledge of SQL and PL/SQL.
Demonstrated ability to design, develop and validate complex database load and maintenance processes with Python.
Ability to excel in an integrated environment, working closely with various IT/Business/Clinical teams to accomplish objectives.
Ability to analyze clinical and business source systems on a variety of platforms.
Strong interpersonal and excellent written and verbal communication skills.
Organized, with strong attention to detail.
Ability to handle multiple simultaneous tasks and effectively.
Able to work independently, self-starter.


Special Instructions to Applicants: Required Application Materials: To apply, please visit: http://www.cu.edu/cu-careers and attach: 1. A letter of application which specifically addresses the job requirements and outlines qualifications 2. A current CV/resume 3. List of three to five professional references (we will notify you prior to contacting both on and off-list references) Questions should be directed to: aileen.sanders@cuanschutz.edu


Application Materials Required: Cover Letter, Resume/CV, List of References

Application Materials Instructions: Application Deadline: Applications are accepted electronically ONLY at www.cu.edu/cu-careers. Review of applications will begin immediately, and will continue until the position is filled."
16,Data Engineer,TenX,,"San Mateo, CA","Brief Description
Job Advertisement
Data Engineer

Ten-X Commercial is the CRE marketplace that is a force multiplier for sellers, buyers and brokers. Ten-X precision-matches assets, accelerates close rates, and streamlines the entire transaction process with more than $55 billion in sales and increasing daily. Leveraging desktop and mobile technology, Ten-X allows people to safely and easily complete real estate transactions entirely online. We bring quality assets to the market and attract prospective investors from around the world. By virtue of our best-in-class marketing and scalable technology platform, buyers and seller are able to conduct transactions in an efficient manner.
Ten-X empowers consumers, investors and real estate professionals with unprecedented levels of flexibility, control and simplicity – and the convenience of transacting properties whenever and wherever they want. As real estate continues to move online, Ten-X is uniquely positioned at the forefront of this dramatic industry evolution.

https://www.ten-x.com/

The Role:
Data and our ability to leverage it is seen and championed as a key competitive advantage from our CEO on down. We are looking for a top tier data engineer to work with our data science team on building out proprietary tools and models around our customer and asset data (both internal and external sets). You will be working on key projects that have board level visibility.

Responsibilities

Play a leading role in designing, developing and implementing Big Data databases (Hadoop, Graph, MySQL, NoSQL, MongoDB) that contains multiple data sets from both internal and external sources
Lead the setup of data pipelines of new internal and external data sets into the database
Work with Data Scientists to help dedupe and fuzzy match data
Work with software engineers on developing APIs

Experience

Undergraduate degree (ideally a Masters) in a relevant quantitative subject (Math, Statistics, Computer Science, Engineering, Economics, etc.)
5+ Years’ Experience in data engineering, including: 2+ years in a modern data stack environment, specifically the Hadoop stack, 3+ Years' Python experience relating to data engineering
Experience with iterative Agile methodologies and use of supporting tools like JIRA, Confluence and Git
Experience in the following will be a plus:
Spark
Kafka
Clickstream data
Machine Learning
Streaming Data
Elastic Search
Containers (Docker)
Fuzzy Matching / NLP
Ability to understand business problems and translate them into data science requirements
Understanding and Familiarity with:
Hadoop and all the related stack (Pig, Hive, HBase, etc.)
SQL skills and SQL Databases
Strong oral and written communication skills and be able to communicate complex technical knowledge in meaning terms
Ability to work in a fast-paced environment and fluidly adapt to changing priorities
Must be passionate about getting to the root cause of issues and driving to whys
Proven ability to obtain buy-in/ partner with the data science team, including demonstrated ability to partner with functional leaders toward common goals
Well-developed analytical and interpersonal skills with ability to draw conclusions and communicate/present them confidently and effectively to broad audiences, including senior leadership
High energy and passion about solving business needs through data
Organized, structured thinker with ability to handle multiple assignments, remain calm under pressure, and digest information from multiple, disparate parts
Continuous improvement mindset
Not afraid to challenge conventional thinking or analyses

"
17,Data Engineer,Teladoc Health,,"Sunnyvale, CA","Mission to Our Employees:
Teladoc Health is the global virtual care leader, offering the only comprehensive virtual care solution spanning telehealth, expert medical, and licensed platform services. Teladoc Health serves the world's leading insurers, employers, and health systems and helps millions of people around the world resolve their healthcare needs with confidence. We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Teladoc Health. Above all, Employees will be provided the same concern, respect, values and caring attitude within the organization that they are expected to share with our clients.
What we are looking for:
We are seeking a smart, ambitious and analytical person to be responsible for our data that is used for business, product, and marketing analytics. You will own the entire data life cycle. You will develop systems to gather, transform, and surface the data to other data, marketing, and product analysts. You will also be involved in the analysis yourself and drive real business decisions based on that analysis. If you are looking to work in the intersection of data engineering and data analysis, this is the position for you.
What you will do:
Manage, develop, maintain our ETL.
Manage, develop, maintain our data warehouse.
Manage, develop, maintain our BI tools.
Work with hundreds of millions of rows of data.
Analyze our data to gather actionable insights to drive decisions in product and marketing.
Work with product managers and marketers so they have the right data.
Enjoy great teamwork, have lots of fun, and take pride in building a world-class product that makes a difference on people's lives.
Requirements:
ETL experience.
Data Warehouse experience.
SQL experience.
Programming experience.
Comfortable in a CLI.
Experience in quantitative data analysis.
Ability to analyze an enormous amount of complex data and deliver meaningful conclusions that can be turned into actionable decisions.
Knowledge of statistics.
Great grasp of numbers with love for data and analytics.
Excellent communication (oral and written), attention to detail, time management and organizational skills.
Company Benefits:
Very competitive salary & equity compensation.
Excellent health, dental, vision coverage.
401k benefits with employer matching contribution; immediately vested.
Awesome people to work with.
Becoming a part of a movement in telemedicine and building something that matters.

For a more detailed look at our company and values, visit our website at https://teladochealth.com/en/about/

At Teladoc Health we thrive on difference and individuality. Teladoc Health is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status."
18,Data Engineer,Volta Charging,,"San Francisco, CA","Our data lives closest to the services that use it in the format best suited for its retrieval. Join us as the first engineer focused on how data is managed, migrated, and exposed to other teams. Become a champion for data security and scalability at Volta as you work across all Platforms and Teams.

At Volta, we’re on a mission to accelerate the adoption of the electric vehicle. Volta creates free electric vehicle charging networks in major metropolitan areas. Our charging networks are free-to-use, free to our property partners, and monetized through brand sponsorships and digital media content.

Our company is small but focused and ambitious. By joining our team, you will have the opportunity to contribute in a number of ways to our Software Engineering department and the evolution of the company itself. For this role, you will help own our data ingestion and exposure pipelines. You will use tools like Alooma, Snowflake, and Sigma, (similar to Tableau), to help us move our data around and understand it better so that other teams can use it more effectively.
Week to week, all Software Engineers are responsible for breaking down new project requirements into epics and stories, planning out their approach to implementation, and presenting said approach to the team. We practice lowercase agile in a flexible warehouse/office environment. Many different teams work at the office, (Media Sales, Operations, etc), and cross-pollination of ideas and conversation is strongly encouraged. Some team members opt to work remotely occasionally to avoid distraction if they have a difficult or large problem to solve.
Requirements
Comfortable designing and launching ETL pipelines in greenfield environments
Opinionated about your schemas
Experienced with PostgreSQL, or excited about diving headfirst into it
Someone who has seen multiple interesting problems of complexity and scale over 4 years of experience
A Computer Science or related degree holder, or able to demonstrate knowledge of the field
Experienced with JavaScript and familiarity with >ES6 features
Ability to express technical abilities and limitations to non-technical peers
Interested in researching and experimenting with new technologies outside of your core responsibilities
Comfortable reading source code and passionate about understanding how technologies are implemented
Self-directed, able to self-prioritize work and identify under-defined requirements
Fascinated with green technologies and transportation
Responsibilities
Continue to expand our usage of DynamoDB, PostgreSQL, Snowflake, and whatever helps us solve the next problem we face
Design, implement and maintain new pipelines that propagate data across environments and between teams with real-time speed and efficiency
Grow and formalize our data security strategy
Work closely with other members of the product team to define new requirements for our data pipeline
Help build a reliable and self-monitoring network, with minimal downtime
Hold architecture and design reviews with other team members
Manage the deployment and lifecycle of your code and tools"
19,Data Engineer,Cramer-Krasselt,,"Chicago, IL 60601","C-K (Cramer-Krasselt) is one of the largest independent, totally integrated agencies in the country with over $700 million in billings, almost $400 million in media assets under management and 61% of revenue from digital.

With a mission to Make Friends, Not Ads®, C-K has built a reputation for changing perception and behaviors that lead to purchasing action for brands. It’s how we helped Porsche achieve seven years of consecutive record-breaking growth, how Corona became the #1 import and Pacifico grew 19% in a single year.

We’ve done it by interconnecting an ever-expanding range of disciplines from strategic branding to digital, social, analytics, media/programmatic, SEM, PR, UX and more.
Major brands include Benihana, Cedar Fair Entertainment Company (Knott’s Berry Farm, Cedar Point and nine others), Cintas, Corona Extra, Edward Jones, Kroger Divisions, Pacifico Beer and Porsche.
About C-K Chicago:

Chicago’s independent spirit makes it the perfect headquarters for C-K, one of the nation’s oldest and largest independent agencies. C-K’s Chicago office, located just off the Magnificent Mile, is full of native Midwesterners and transplants alike that are united in the values of hard work and collaboration.
At C-K in Chicago, we work with major global and national brands like Porsche, Corona Extra, Cedar Fair, BIC, Edward Jones and many others. We’re proud of these partnerships and the integrated approach we take to every client and every element of a campaign, be it large or small.
Our experts here have backgrounds spanning every marketing, advertising and communications discipline across all categories imaginable. C-K Chicago is a hub of innovation and the nerve center of the agency.
We also apply our work ethic to the community. Our flagship charity in the Chicago office is Off the Street Club (OTSC), the oldest boys and girls club in the city. Throughout the year we participate in the planning and execution of several key OTSC fundraising events such as Swing for the Kids, The Firefly Ball, Summer Bake Sale and Holiday Luncheon. We also participate in the Battle for Hope, the Chicago-area advertising agency battle of the bands. In 2015, our house band, the Angry Pickles, won the Battle for Hope and helped raise thousands for OTSC in the process.


Data Engineer Summary

The Data Engineer will work with internal clients to help develop and automate agency analytics tasks. The candidate should be comfortable exploring, recommending and building solutions for extracting, transforming, and loading data from various disparate sources (APIs, databases, flat files, etc.) A successful Data Engineer will partner with data scientists and analyst teams to develop data pipelines and data products that address agency needs.

Major Tasks / Responsibilities
Build, test, and maintain architecture, including databases and data processing.Influence the direction of analytics product development at the agency.Develop systems and tools that improve consumption and understanding of data.Collaborate with analysts, data scientists and solution architects to understand data needs and arrive at broader implementation requirements.Define and manage SLA for data sets in allocated areas of ownership.Identify, triage, and resolve data pipeline, infrastructure, other potential challenges.

Qualifications
Bachelor's degree in computer science or math or equivalent work experience3+ years of experience working with a data warehouse.3+ years of experience working with different data pipeline/processing infrastructure (Amazon Web Services, Google Cloud, proprietary).3+ years of experience building and maintaining ETL data pipelines.3+ years of experience working with RDBMS, normalized data modeling.3+ years of experience identifying, triaging, and resolving data pipeline, infrastructure, and other potential challenges.Ability to adapt to a quickly changing environment and work on a broad project/pipeline base with diverse needs and functional responsibilities.Very comfortable working within Linux command-line and Linux servers.Strong familiarity with at least one of: Python, Ruby, Scala, Go.Strong familiarity with object-oriented software development concepts.Strong SQL query writing and query efficiency evaluation skills.Strong communication skills, and ability to communicate technical ideas, solutions, and issues with technical and non-technical team members.Strong time management and organizational skills.Familiarity with data visualization software (Tableau).Familiarity with NoSQL."
20,Data Engineer,"Resume Tree Recruiting Group, LLC",$45 - $65 an hour,"San Jose, CA 95134","Resume Tree Recruiting Group is actively seeking an experienced Data Engineer in the San Jose, CA and the Sunnyvale, CA area.

Requirements:

Solid experience as a Data Engineer for at least seven years.
Strong skill-sets in Python, Hadoop Stack, Data Pipeline using ETL or other tool.
Equipped in handling high volume data and AWS (S3 or lambdass)."
21,Data Engineer,Intuit,,"Eagle, ID 83616","Overview
Do you love the border between architecture and analysis? Can you play nice with both humans and machines? We are looking for a savvy Data Engineer to join our close knit team of data experts. You must be honest and humble, have a willingness to persevere and work hard, and a constant openness to learning. You are self-directed and comfortable supporting the data needs of multiple teams, systems and products. We seek team members who are courageous but not a bully, strong but not rude and humble but not timid. In a couple words, emotionally intelligent.
Responsibilities
Design and build data management capabilities including the enterprise data warehouse platform, associated data models, ETL process, integration pipelines and data architecture requirements.
Implement frameworks for automated and scripted testing.
Constantly evaluate, optimize and document deployment of data models to achieve greater efficiency and reliability
Assist with ad hoc data investigations and analysis and new data source discovery
Build cross functional relationships with data analytic teams and business leaders to understand their requirements and data needs
Interact and integrate with internal and external teams and systems to extract, transform, and load data from a wide variety of sources
Participate in rotational on-call support and provide ongoing maintenance of all data warehouse infrastructure
Qualifications
Current experience with writing advanced SQL queries and performance tuning
Data modeling and ETL development experience
Demonstrate familiarity with Kimball data warehouse concepts.
Solid understanding of database theory and architecture
Experience with at least one scripting language
Understanding cloud data warehousing technologies
Familiarity with BI and analytic tools
Basic understanding of data governance and master data management principles
Experience with database management systems including performance optimizing and tuning.
Excellent verbal and written communication skills
Demonstrate advanced critical thinking and trouble-shooting capabilities to assess, prioritize, plan, and implement tasks and solutions effectively, including the ability to manage multiple projects at a time
Must be self-directed and able to function both independently and as part of a team"
22,Data Engineer,Laserfiche,,"Long Beach, CA 90807","If you’re a problem-solver who loves working with data, Laserfiche has a great opportunity for you! As a Data Engineer on the Operations team, you’ll build, maintain, monitor and improve enterprise-wide data pipelines and analytics platforms. You’ll have the opportunity to build the foundation upon which our data is structured, as well as create reports and tools to help stakeholders make critical decisions regarding our key corporate initiatives.

Learn and grow your career via a variety of career paths, including a technical engineering track, a project management track, and a hybrid of the two. You’ll have the support of a collaborative team and the freedom to explore best-practice approaches to develop solutions on your own. To succeed, you’ll need to learn quickly and think creatively.

If you’re up to the challenge, we want to talk to you! Both experienced engineers and recent college graduates are encouraged to apply.

Responsibilities Include:
Build, maintain, monitor and improve data pipelines and analytics platforms at scale

Define and calculate metrics to be analyzed
Develop tools for electronic data collection
Query, merge and extract data across sources
Maintain data refresh and update pipelines
Data cleaning/manipulation

Clean datasets for analysis
Improve data quality through data inaccuracy profiling and recommend process improvements
Reporting & Visualization

Develop data reports, visualizations and/or interactive BI reports
Develop, implement and automate reporting by working with stakeholders in their design, planning and implementation while ensuring consistency
Statistical Modeling & Machine Learning

Create summary statistics
Define, calculate and validate algorithms
Conduct analysis (descriptive, correlational, inferential and predictive)
What You’ll Need:

BA/BS required in computer science, engineering or related technical field
Excellent analytical and problem-solving skills
Strong coding proficiency
Knowledge of statistics a plus
Experience with deriving meaningful insights and analysis from structured and unstructured datasets
Strong attention to detail
Self-starter mentality and team-player attitude
Resourcefulness and creativity in finding solutions
Click here to learn more about Life at Laserfiche"
23,Federal - Big Data Engineer,Accenture,,"Arlington, VA 22209","Organization: Accenture Federal Services

Location: Arlington, VA - Washington DC
Accenture Federal Services, a wholly owned subsidiary of Accenture LLP, is a U.S. company that helps clients transform bold ideas into breakthrough outcomes. We serve every cabinet-level department and 30 of the largest federal organizations. Our 9,000 dedicated colleagues and change makers work with clients at the heart of the nation's priorities in defense, intelligence, public safety, civilian and health to make a difference for the people they employ, serve, and protect.


AFS is seeking a Big Data Engineer to support our Federal portfolio. This role involves supporting the full software development lifecycle, in designing and developing data pipelines, interfaces, and architecture to support big data and analytics initiatives. This team will be implementing best practices for data engineering on cloud platforms to support a multitude of use cases in delivering actionable insights for the client.

Basic Qualifications:
3+ years of work experience with ETL and data modeling
2+ year of experience with open source technologies (Spark, Kafka, Presto, Hive, Cassandra etc.)
2+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale
2+ year of experience with Cloud Technologies (AWS, Azure, Google, etc.)
3+ years of experience with at least one SQL language such as T-SQL or PL/SQL
Bachelor’s Degree
Preferred qualifications:

Experience in real-time analytics applications.
Experience in both batch and stream processing technologies
Experience with Java or Scala programming languages
Machine learning experience with Spark or similar Ability to manage numerous requests concurrently and be able to prioritize and deliver
Good communication skills and dynamic team player

An active security clearance or the ability to obtain one may be required for this role.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Applicants for employment in the U.S. must possess work authorization which does not require now or in the future sponsorship by the employer for a visa.

Accenture is a federal contractor, an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
24,Data Engineer,Clear Street,,"New York, NY","AtClear Street ( https://clearstreet.io ), we are disrupting the institutional brokerage and clearing market by modernizing archaic industry segments with brand new technology. We're changing the way institutional investors interact with the markets; offering an alternative to working with big banks. Our cloud-based API technology will empower clients to clear, settle, and finance trading activity while accessing real-time risk and position information. Our platform is built on an incredibly modern tech-stack, by pragmatic engineers focused on building intuitive and frictionless user experiences. Our tech-infused suite of customer experience-oriented prime service offerings will increase our clients' efficiency and provide real-time insights they've never previously experienced.

As a Data Engineer, you will design and implement all aspects of our underlying data analytics infrastructure. You will architect, build, and launch data pipelines along with a processing and research framework. Your systems will be scalable by design. You will develop standards and processes that deliver data with integrity and consistency. You will optimize data structures and algorithms to improve data processing efficiency. In short, you'll touch every aspect of the system.

Data Engineer Characteristics:

You have a passion for architecting and optimizing big data platforms and have extensive experience with Hadoop, Spark, and Kafka
You dig deep into the latest data technologies and frameworks to build tools that empower researchers to maximize the impact of their insights
You live by the integrity of the data flowing through your system - you define metrics and measurements, and build frameworks to validate and monitor data quality
You write Python scripts to automate workflows and improve processes at every opportunity
You possess strong Computer Science fundamentals
You enjoy learning, exploring new ideas, and synthesizing information. You invest in yourself

We offer:

The opportunity to join a small and growing team of good people, where you can make a difference
A new, high-quality code base with little technical debt and room to build new services and features
An environment that embraces the utility of a DevOps oriented culture and combines it with a focus on CI/CD methodology
A meritocratic philosophy that champions collaboration
Competitive compensation, benefits, and perks

"
25,"Data Engineer, Amazon Air","Amazon.com Services, Inc.",,"Seattle, WA","Bachelor degree in Engineering, Computer Science, or Statistics; or Masters in Math/Statistics/Finance or related disciplineMinimum three (3) years of experience data modeling, ETL, data warehousing, and transformation of large scale data sources using SQL, Redshift, Oracle, or other Big Data technologiesMinimum five (5) years of demonstrated quantitative and qualitative data science experience with impact to a business, a track record of problem solving using software systems, and the desire to create and maintain data warehouse systemsAbility to source and combine disparate data sets to answer business questionsMinimum of (2) years Advanced SQL with Oracle, SQL or MySQL, and Columnar Databases

Amazon seeks a passionate, results-oriented, Data Engineer to identify strategic initiatives that will form the next generation of air delivery to delight Amazon customers. As an Amazon.com Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. This individual should have deep expertise in the design, creation, management, and business use of extremely large datasets. This high impact role will have an opportunity to help design and build our data infrastructure from the ground up, work with emerging technologies such as Redshift, while driving business intelligence solutions end-to-end: business requirements, workflow instrumentation, data modeling, ETL, metadata, reporting, and dashboard development. He/she should be an expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications. This individual should also be able to work with business customers in understanding the business requirements and implementing reporting solutions. The role requires someone who loves data, understands enterprise information systems, has a strong business sense, and is an excellent communicator.

Responsibilities include:
Conduct deep-dive investigations into business problems and identify potential opportunitiesIdentify and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentationContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customersWork with in-house data scientists, global supply chain, transportation and logistics teams, and software teams to identify new features and projectsIdentify ways to automate analysis through smarter software systemsTrack realized savings and impacts, and communicate results with senior leadersThis is an individual contributor role that will partner with internal stakeholders across multiple teams, gathering requirements and deliver complete solutions

Master degree in Engineering or Math/Statistics/Finance or related disciplineAbility to work in a deadline-driven work environment; ability to re-prioritize on a regular basis in order to remain current with business needsExperience guiding and mentoring other data engineers and influencing large scale projects or organizations
Amazon.com is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.

#AmazonAir"
26,Data Engineer,TekWissen LLC,,"Atlanta, GA 30328","Typical Training / Experience –
Typically requires BS/BA or Associates degree in related discipline;
Generally 2-5 years of experience in related field; certification is required in some areas OR MS/MA and generally 2-4 years of experience in related field.
Certification is required in some areas

Dept/Org Scope & Impact - Fully competent and productive professional contributor, working independently on larger, moderately complex projects/assignments that have direct impact on department and area results

Problem Complexity –
Performs full range of standard professional level work that typically requires processing and interpreting, Identifies problems and possible solutions and takes appropriate action to resolve more complex, less clearly-defined issues.
Demonstrates skill in data analysis techniques by resolving missing/incomplete information or inconsistencies/anomalies in more complex research/data

Autonomy - Nature of work requires increasing independence; receives guidance only on unusual complex problems or issues; Work review typically involves periodic review of output by supervisor and/or direct 'customers' of the process

Knowledge - Possesses and applies a broad knowledge of principles, practices, and procedures of particular field of specialization to the completion of moderately complex assignments. Solid knowledge of organization's technologies and practices

Influence/People Leadership - May provide general guidance/direction or train junior level support and professional personnel Additional Information: Years of Experience does not matter. Can be someone fresh out of college or somebody who has been doing it for 10 years. What matters is the knowledge of SQL, Alteryx (it is not a “must have” but “highly desirable”), and just being good with data.

Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture from various databases that include Oracle DB, Adobe, and Amazon S3
Assemble large, complex data sets that meet business requirements.
ETL processes
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Data investigation/troubleshooting tasks out of EDW and other databases.
List pulls for digital media, troubleshooting data with digital media partners.

Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases. Ability to understand, edit, and create complex SQL queries.
Experience building and optimizing data pipelines, architectures and data sets.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, and metadata.
Quickly learn complicated data warehouse structure.
Strong project management and organizational skills.
Technical skills:

o Experience with relational SQL databases specifically Oracle

o Experience with Alteryx, Tableau, and Adobe Analytics a plus"
27,Surreal Data Engineer,Facebook,,"Redmond, WA","Come invent the future!

At Facebook Reality Labs (FRL), computer vision and machine learning (CV/ML) are vital to turbocharging our scientific explorations and generating viable paths to the consumer products people will use for decades to come — products that will literally give people super powers.

We are looking for a Data Engineer to drive managing and organization of our algorithm analysis data from our computer vision and machine learning (CV/ML) teams. This is a massive amount of data that we are collecting in order to inform and refine algorithm choices and to ensure we have the best algorithms. You'll work with world class researchers, engineers and software developers to enable a large group of people to do ground breaking work in their fields.
RESPONSIBILITIES
Create and implement reports and dashboards for ongoing research into various projects, such as SLAM, relocalization, anonymization, mapping
Propose, collect data, analyze and report on deep investigations into algorithm performance/architecture proposals
Explore and analyze current data for insights into areas of improvement of current solutions
Develop and manage data set processes across the team and across the organization at large
Assist in creation and maintenance of evaluation pipeline
Collaborate in a team environment across multiple scientific and engineering disciplines
Spread knowledge and best practices across the team, enabling teams to do local analysis and build smarter solutions (when possible)
Learn constantly, educate others, dive into new areas with unfamiliar technologies, and embrace the ambiguity of AR/VR problem solving
MINIMUM QUALIFICATIONS
3+ years experience with SQL, Hive
3+ years experience with Python, C++
Experience managing large sets of data
Experience analyzing and producing reports from large sets of data
Knowledge with computer vision
Bachelors degree in Computer Science, Computer Vision, Engineering or similar
Communication experience and demonstrated experience working across disciplines to drive solutions
Facebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com."
28,Data Engineer,CapTech Consulting,,"Denver, CO","Job Description

The Data Engineer, Analytics role falls into the Data Management & Business Intelligence practice area at CapTech, through which our consultants provide a broad spectrum of services to help our clients define and implement a strategy to deliver lasting and mission-critical information capabilities. Our Data Integration consultants bridge the gap between the business and IT side of companies. By partnering with clients to fully understand both their business philosophy and IT strategy, CapTech consultants maintain the vision that data integration should be built to help the organization make better decisions by providing the right data at the right time.
Specific responsibilities for the Data Engineer, Analytics position include:

Design, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, C, etc)
Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insights
Design, develop, and implement data processing pipelines at scale
Present programming documentation and design to team members and convey complex information in a clear and concise manner.
Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.
Write and refine code to ensure performance and reliability of data extraction and processing.
Communicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customers
Participate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.
Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.
Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.
Some of our technologies might include: HDFS, Cassandra, Spark, Java, Scala, Informatica, SQL Server, Oracle, Ab Initio, Kafka.

Qualifications

Specific qualifications for the Data Engineer, Analytics position include:

Development experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system
Strong SQL development skills
Development experience with at least two different programming languages (Python, Java, C, etc.)
Development experience with Unix tools and shell scripts
Development experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.)
Minimum of 3 years experience designing, developing, and testing software aligned with defined requirements
Experience tuning SQL queries to ensure performance and reliability
Software engineering best-practices, including version control (Git, TFS, JIRA, etc.) and test driven development
Exposure to Business Intelligence tools such as Business Objects, Informatica, SSRS, Cognos, MicroStrategy, Tableau, QlikView, SpotFire, etc.
Additional Information

We offer challenging and impactful jobs with professional career paths. All CapTechers can keep their hands-on technology no matter what position they hold. Our employees find their work exciting and rewarding in a culture filled with opportunities to have fun along the way.
At CapTech we offer a competitive and comprehensive benefits package including, but not limited to:
Competitive salary with performance-based bonus opportunities
Single and Family Health Insurance plans, including Dental coverage
Short-Term and Long-Term disability
Matching 401(k)
Competitive Paid Time Off
Training and Certification opportunities eligible for expense reimbursement
Team building and social activities
Mentor program to help you develop your career
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace.
Candidates must be eligible to work in the U.S. for any employer directly (we are not open to contract or “corp to corp” agreements). At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
CapTech is a Drug-Free work place.
Candidates must have the ability to work at CapTech’s client locations.
All positions include the possibility of travel.
CapTech has not contracted/does not contract with any outside vendors in its recruitment process. If you are interested in this position, please apply to CapTech directly."
29,Data Engineer Intern,Warner Bros Careers,,"Needham, MA","Company Overview
Warner Bros. has been entertaining audiences for more than 90 years through the world’s most-loved characters and franchises. Warner Bros. employs people all over the world in a wide variety of disciplines. We're always on the lookout for energetic, creative people to join our team.
Business Unit Overview
WB Games Boston, is a Warner Bros. Interactive Entertainment development studio focused on creating free-to-play games for mobile platforms. The studio is utilizing its extensive expertise in creating persistent online worlds that foster powerful social gaming communities to develop its hit mobile game, Game of Thrones: Conquest.
Opportunity Overview
The WB Games Boston studio is looking for passionate and curious professionals to help level up their data and analytics team. Like us, you love games, and love to have fun. We look for those whose curiosity drives them to try new things, aren’t afraid to fail, and continuously learn.

The Data challenges at WB Games are extensive and you can expect to level up in all data domains. This includes software engineering, distributed systems, data modelling and machine learning.

What part will you play?
Be part of the game development process, helping teams design and implement analytic-based features.
Learn the internals of building a robust end-to-end data pipeline on all levels especially in software development, databases, infrastructure, data modelling and data processing.
Contribute code to production data systems and gain strong software engineering practice.
Gain hands-on experience in distributed data processing technologies: Redshift, Airflow, and Presto.
Learn techniques for high volume distributed data processing and storages.
Ensure the integrity, availability, and confidentiality of data, database systems and supporting services.
What do we require from you?
You are familiar with one or more of these technologies: Java, Python, or PostgreSQL.
You are comfortable with DevOps cloud tools (Amazon, EC2, or Google Cloud) and have some working knowledge of monitoring tools.
You have side projects or viewable public code on Github, Bitbucket, etc.
You are currently pursuing a BS or MS in Computer Science, Engineering, or related technical discipline, with an expected graduation date of 2019 or 2020.
Bonus points:
You are well-versed in object-oriented programming (OOP).
You have some experience with Hadoop or other data engineering or database software.

The Warner Bros. Entertainment Group of Companies is an equal opportunity employer and considers all candidates for employment regardless of race, color, religion, sex, national origin, citizenship, age, disability, marital status, military or veteran's status (including protected veterans, as may be required by federal law), sexual orientation, gender identity or any other category protected by law."
30,Data Engineer,Capital One - US,,"New York, NY","114 5th Ave (22114), United States of America, New York, New York

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer

Being Capital One Tech:
At Capital One, we consider ourselves the bank a technology company would build. We’re delivering best-in-class innovation so that our 65 million customers - and counting - can manage their finances with ease. Our reality and vision empower our engineers to use artificial intelligence and machine learning to transform real-time data, software, and algorithms into financial clarity.

We’re all-in on the cloud and a leader in the adoption of open source, RESTful APIs, microservices, and containers. We build our own products and release them with a speed and agility that allows us to get new customer experiences to market quickly. We’re going boldly where no bank has gone before. And, as a founder-led company, we’re inspired and empowered to make, break, do, and do good . So, let’s do something great together.

Your #LifeatCapitalOne
Looking to work somewhere with the flexibility of a start-up but the financial muscle of a Top-10 bank? You’re in the right place! And here’s what that means for you…

You'll have a flexible work schedule—we want to understand where and when you're at your best so you have a healthy work-life balance. Diversity and Inclusion are cultural norms here—you’ll have access to active local chapters of Women in Tech, Blacks in Tech, and Hispanics in Tech and more. Plus, you’ll be given time to support the next generation of technologists by volunteering with youth programs like Capital One Coders - our engineer-led experience that teaches middle school students in underserved communities how to code. Want to learn more? See what our associates are up to at #LifeatCapitalOne !

Calling All Data Engineers:
A hub for innovation, our New York presence is expanding, and we need Data Engineers who know their stuff to join our team. As a Capital One Data Engineer , you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One. You’ll work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems. You’ll collaborate with digital product managers, and deliver robust cloud-based solutions to drive powerful experiences that help millions of Americans achieve financial empowerment. Want to learn more? Check out the low-down on our high-tech .

Who You Are:
You are fun to work with – you’re excited by a team environment

You are curious. You like to learn new technologies , and you adapt well to change

You are passionate about current state-of-the-art software technologies and tools, with experience implementing them effectively

You are excited about working with cloud-native stack, building on AWS using technologies like Kubernetes and Serverless

You possess a sense of intellectual curiosity and a burning desire to learn

You are motivated and actively looking for ways to contribute

You are passionately focused on the customer and the details that make their experience exceptional

You value data and truth over ego

You possess a strong sense of engineering craftsmanship, take pride in your code

You’re pragmatic - you make the best use of time and resources to find the simplest workable solution

You think and act like an owner, taking personal responsibility for both team and product success

You possess great communication and reasoning skills, including the ability to influence and make a strong case for technology choices

You thrive in collaborative agile teams and are ready to take on new and unexpected challenges while building the next wave of engineering solutions

What You’ll Own:
Collaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies

Leading the craftsmanship, security, availability, resilience, and scalability of your solutions

Bringing a passion to stay on top of current trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community

Encouraging innovation, implementation of cutting-edge technologies, outside-of-the-box thinking, teamwork, and self-organization

Assisting in the hiring of top engineering talent and maintaining our commitment to diversity and inclusion

Basic Qualifications:
Bachelor’s Degree

At least 2 years of experience in application development

At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)

Preferred Qualifications:
Master's Degree

3+ years of experience in application development

1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink

1+ years of experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service

1+ years of experience with Ansible / Terraform

2+ years of experience with Agile engineering practices

1+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)

1+ years of experience with NoSQL implementation (Mongo, Cassandra)

2+ years of experience developing Java based software solutions

2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)

2+ years of experience developing software solutions to solve complex business problems

2+ years of experience with UNIX/Linux including basic commands and shell scripting

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
31,Data Engineer,Contract,,"San Francisco, CA","Job Description
Job Title: Data Engineer
Location: San Francisco, CA
Terms: Full-time, Contract, Contact-to-Hire
About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.
What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.
As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do
Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:
Cloud
Analytics
Digitization
Infrastructure
Security
Job Description
Overview
Data is the way our clients make decisions. It is the core to their business, helping create an experience for customers and providing insights into the effectiveness of our product launch & features.

As a Data Engineer , you will be a part of an early stage team that builds the data pipelines, collection, and storage, and exposes services that make data a first-class citizen. We are looking for a Data Engineer to build a scalable data platform. You'll have ownership of core data pipelines that powers top line metrics; You will also use data expertise to help evolve data models in several components of the data stack; You will help architect, building, and launching scalable data pipelines to support growing data processing and analytics needs. Your efforts will allow access to business and user behavior insights, using huge amounts of data to fuel several teams such as Analytics, Data Science, Marketplace and many others.

Responsibilities

Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth
Evolve data model and data schema based on business and engineering needs
Implement systems tracking data quality and consistency
Develop tools supporting self-service data pipeline management (ETL)
SQL and MapReduce job tuning to improve data processing performance

Experience

3+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct advanced performance tuning
Strong skills in scripting language (Python, Ruby, Bash)
1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)
Comfortable working directly with data analytics to bridge Lyft's business goals with data engineering

We are Growing Rapidly: 2019 Highlights
Trianz is growing above the average of the professional services industry. Here are some highlights.
Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.
Won the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.
Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.
Featured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.
Achieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.
Talk to us, Join us & Develop into Leaders
Come join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.
We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
Equal Opportunity Employer
Trianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law)."
32,Data Engineer,Pluralsight,,"San Francisco, CA","Job Description

Our Data Engineering team builds and maintains a secure, scalable, flexible and user-friendly analytics hub that allows us to make informed and data-driven decisions. They also construct and curate business-critical data sets that allow us to realize the value of all the data we collect.
A Data Engineer utilizes a multidisciplinary approach to providing ETL solutions for the business, combining technical, analytical, and domain knowledge. The perfect applicant for this role has strong development skills, experience transforming and profiling data to determine risks associated with proposed analytics solutions, a willingness to continually interface with analysts in order to determine an optimal approach, and an eagerness to explore data sources to understand the availability, utility, and integrity of our data.
What you'll own:
Data pipeline / ETL development:
Building and enhancing data curation pipelines using tools like SQL, Python, Glue, Spark and other AWS technologies
Focus on data curation on top of datalake data to produce trusted datasets for analytics teams
Data Curation:
Processing and cleansing data from a variety of sources to transform collected data into an accessible and curated state for Analysts and Data Scientists
Migrating self-serve data pipeline to centrally managed ETL pipelines
Advanced SQL development and performance tuning
Some exposure to Spark, Glue or other distributed processing frameworks helpful
Work with business data stewards & analytics team to research and identify data quality issues to be resolved in the curation process
Data Modeling:
Design and build master dimensions to support analytic data requirements
Replacing legacy data structures with new datasets sourced from streaming data feeds from the core product and other operational systems
Design, build and support pipelines to deliver business critical datasets
Resolve complex data design issues & provide optimal solutions that meet business requirements and benefit system performance
Query Engine Expertise & Performance Tuning:
Assist Analytics teams with tuning efforts
Curated dataset design for performance
Orchestration:
Management of job scheduling
Dependency management mapping and support
Documentation of issue resolution procedures
Data Access
Design and management of data access controls mapped to curated datasets
Leveraging devops best practices, such as IAC and CI/CD to build upon a scalable and extensible data environment

Experience you'll need:
Strong experience designing and building end-to-end data pipelines
Extensive SQL development experience
Knowledge of data management fundamentals and data storage principles
Data modeling:
Normalization
Dimensional/OLAP design and data warehousing
Master data management patterns
Modeling trade-offs impacting data management & processing/query performance
Knowledge of distributed systems as it pertains to data storage, data processing and querying
Extensive experience in ETL and DB performance tuning
Hands on experience with a scripting language (Python, bash, etc.)
Some experience with Hadoop, Spark, Kafka, Impala, or other big data technologies helpful

Familiarity with the technology stacks available for:
Metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Data management, data processing and curation:
Postgres, Hadoop, Hive, Impala, Presto, Spark, Glue, etc.
Experience in data modeling for batch processing and streaming data feeds; structured and unstructured data
Experience in data security / access management, data cataloging and overall data environment management

Experience with cloud services such as AWS and APIs helpful
You’d be a great fit if your current track record looks like this:
5+ years of progressive experience data engineering and data warehousing
Experience with a variety of data management platforms (e.g. RDBMS (Postgres), Hadoop (CDH, EMR))
Experience with high performance query engines (Hive, Impala, Presto, Athena, MPP engines like RedShift)
Strong capability to manipulate and analyze complex, high-volume data from a variety of sources
Effective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language
Ability to problem solve independently and prioritize work based on the anticipated business value

Qualifications

null

Additional Information

All your information will be kept confidential according to EEO guidelines."
33,Data Engineer,Cross River Bank,,"Fort Lee, NJ 07024","Who We Are

At Cross River, we’re building the best banking and technology products for fintechs, all through APIs. As venture-backed pioneers of the platform-based financial services model known as the sponsor, or partner, bank, we work closely with our partners to offer products such as payments rails and loan origination services to the latest in regulatory and compliance solutions. What drives us every day? A desire to innovate in the service of financial inclusion. Giving access to consumers, whether they are looking for credit or for a better way to pay has been in our DNA from day one. Our nimble, adaptive, one-step ahead culture is powered by our people, and as a result of their creativity, tech-forward thinking and collaborative spirit, we are experiencing explosive growth.

What we’re looking for

Cross River’s Technology team is made up of problem solvers hungry to perfect new products and systems. As a Data Engineer, you play a key role. The Data Engineer will have a critical role in the development, implementation and maintenance of our data pipeline and development of our data warehouse.

At Cross River, our technology team has an open and transparent environment around process, architecture and development, and everyone’s work is impactful. From building APIs, designing new applications, managing our data platform, deploying new systems or supporting it all, we work together in a cohesive and nimble team that gets things done.

The Technology team cares about results, not activity, and we have fun doing it. If you like challenging problems, are analytical, and a great team player—we want to hear from you!

Responsibilities:

Working independently or under supervisionInteracts with analysts and business stakeholders in the creation of technical specificationsAnalyze data requirements, complex source data, and the data model, and determine the best methods in extracting, transforming and loading the data into the data staging, warehouse and other system integration projectsAnalyze business requirements and outline solutionsProvide technical assistance in identifying, evaluating, and developing systems and proceduresDesign, develop, optimize, maintain and support ETL processes using data warehouse design best practices and SQL Server Integration Services (SSIS) to integrate data from multiple source systems into a Data Warehouse, cleansing data, transforming and loading data from multiple formats using SSIS and stored proceduresDocument all ETL and data warehouse processes and flows. Document all schema and programming using comments in code and ConfluenceData exploration on new and existing data sources in support of reporting requirementsWrite and troubleshoot complex / custom stored procedures based on business requirementsDesign, develop and streamline daily/nightly data processing tasksImprove the overall quality of our data and reportsA successful candidate will have hands-on experience in a multitude of domains; including, but not limited to database design, data warehousing, business intelligence, big data, database tuning, application optimization, security, virtual computing and storage, incident tracking, and general database administration

Qualifications:

Associate’s degree required, Bachelor’s degree preferred, in Computer Science or a Business-related field5+ years of experience working with ETL tools for data integration tasks: Microsoft SQL Server, SSIS and python2+ years of experience in developing data warehouse environmentsBasic understanding of application design and development principlesPossesses good customer service, communication and presentation skillsSolid experience with change controlHands-on experience troubleshooting, writing and optimizing complex T-SQL queries, stored procedures and user-defined functions (SQL Server 2012+)Relational normalized and de-normalized data warehousing schema design conceptual knowledgeExperience with data quality issues and remediationEffective communication skills, as well as the ability to work independently or collaborate with all levels of the organization when necessaryExcellent time management skills and the multi-tasking abilitiesAbility to clearly document data structures, data definitions, and data provenanceExperience in an Agile environment and with Atlassian suite of tools (JIRA, Confluence, etc) and Git"
34,Data Engineer,Piper Companies,,"Durham, NC 27707","Piper Enterprise Solutions is currently looking for a Data Engineer in Durham, NC to work for an international health insurance organization.

Responsibilities for the Data Engineer include:
Attend architecture meetings and analyze how systems are connected
Mentor junior Data Engineers
Manage data flow and data governance
Design data platform for all employees on data team moving forward.
Build a new platform on top of existing AWS database
Qualifications for the Data Engineer include:
3+ years of building ETL and data pipelines with Python and/or Spark
Strong knowledge and experience with SQL
Exposure to Hive SQL
Desire to work with multiple data-oriented tools (Hadoop, Spark, Pig, Big Data, etc.)
Knowledge of AWS and how to build data pipelines on top of the platform
Compensation/Benefits for the Data Engineer include:
Salary Range: $110,000-$120,000
Full Benefits: Cigna Healthcare, Metlife Dental, VSP Vision, 401k with Voya, and Paid Time Off

Please send resumes to Ethan Foster at efoster@pipercompanies.com"
35,Data Engineer,Maine Technology Users Group,,"Portland, OR","Our client, a local fast-growing company who is committed to delivering independent, reliable, and insightful data solutions to clients nationwide, is seeking to add a Data Engineer to their team. This person should have passion for building and maintaining a scalable and secure big data platform. This is a great opportunity for a recent grad who study data engineering and has some SQL skills.

This position will be responsible for loading, processing, and analyzing pour client’s Claims Data Manager (CDM) data warehouse. The successful candidate will become proficient in Advanced SQL, API, big data pipelines using Java, web development using Ruby and React, and other tools employed by our client in order to effectively troubleshoot problem areas and implement new features and recommendations. The position will develop, through research, training, and mentoring, a strong understanding of the data being processed along with underlying data structures and processes to provide effective quality assurance and operating support of our client’s data management systems and data. Responsibilities for the position are split among systems analysis, development, devops, quality assurance engineering and production support.

Essential Functions

Owns the design, development and support of optimal data pipelines
Develops code and automated tests to implement new features and resolve system issues
Builds processes supporting data loading, data transformation, metadata, dependency and workload management
Ensure performance of code meets non-functional requirements
Writes and runs database scripts to understand system behaviors and data anomalies
Troubleshoots and triages system and data issues and creates thoroughly documented trouble tickets
Qualifications

A bachelor’s degree or higher in computer science or related field
Excellent organizational, interpersonal and time management skills
Excellent written and verbal communication skills
Ability to effectively work within and across teams
Experience coding data pipeline programs and working with complex database structures
Strong analytical skills related to working with structured and unstructured datasets
Ability and willingness to quickly learn new technologies
Experience with SQL, Java, API, and web development preferred
Experience with AWS or other cloud services: EC2, Hadoop, Spark, RDS, Redshift preferred
Pro Search, Inc. was established in Portland, Maine in 1994 to provide companies with a full range of search, staffing and contracting services, specializing in the functional areas of information technology, accounting and finance, sales and marketing, customer service, human resource and office support utilizing a consultative approach. Pro Search, Inc. strives to develop true partnerships with client companies and candidates to fully understand and satisfy their employment needs.

Pro Search, Inc. is an Equal Opportunity/Affirmative Action Employer. As such, it is our policy to follow a concept of non-discrimination in the hiring and promotion of employees without regard to their race, religion, sex, age, color, national origin or veteran or handicapped status."
36,Data Streaming Platform Engineer,Zillow Group,,"Seattle, WA 98101","About the team
We build the real time streaming platform responsible for ingesting of billions of real-time events and terabytes of data. We aim to provide teams across all of Zillow Group a robust, scalable and lightning fast way of passing their data to diverse set of use cases which enrich Zillow’s unparalleled living database of all homes and hundreds of millions of customers and empowers teams downstream to build analytics tools and products to delight our users.

Small team = huge impact. Engineering teams are highly decentralized in order to create the small team speed and autonomy of a start-up environment but backed by big company resources.Fast-moving, developer driven organization full of forward-thinking and ambitious people.Learn more about what we are doing at https://www.zillow.com/engineering and https://www.zillow.com/data-science
About the role
We are looking for a strong software engineer with a background in real time streaming to create an end to end streaming platform. As a Data Engineer, you will be responsible for all phases of the development cycle: design, implementation, release and operations. You will leverage your knowledge and experience to provide technical leadership for the team, take ideas from zero to completion, and get down to the details for building a system from scratch. You will:
Build and maintain highly scalable, low-latency, fault-tolerant streaming data platform that empowers Data Scientists, Engineers to build real time data applications.
Work closely with business and technology stakeholders to build the next generation Distributed Streaming Data Platform using Apache Kafka
Work closely with the real time data processing platform team to build shared infrastructure abstractions and self service tooling
Who you are
Data Engineer with experience using streaming technologies (Kafka/Kinesis/EventHubs)
Experience running applications on top of cloud platforms (AWS/Azure/GCP)
Experience with Java
Excellent communication skills and demonstrative empathy
Understanding of distributed systems and concepts
Proven track record of leading and delivering large projects independently
Proven ability to learn new technologies quickly
Knowledge of Computer Science fundamentals (CS Degree or related)
Experience running Apache Kafka cluster is a plus
Get to know us
Zillow Group houses the largest portfolio of real estate brands on mobile and the web. We are on a mission to rewire the real estate transaction and are building transformational tools and services that make it easier for everyone to find and get into a home they love. We are working to create an on-demand real estate transaction experience for every stage of the home lifecycle - for buyers, sellers, renters and borrowers - and we're well on our way. No matter what job you're in, you will play a critical role in making this vision a reality for millions of people.
At Zillow Group, we're powered by our inclusive work culture, where everyone has the support and resources to do the best work of their careers. Our efforts to streamline the real estate transaction is supported by our passion to empower people and enrich lives around everything home, a deep-rooted culture of innovation, a fundamental commitment to Equity and Belonging, and world-class benefits. But, don't just take our word for it. Read our reviews on Glassdoor and recent recognition from multiple organizations, including: Fortune 100 Best Companies to Work For (#69), Fortune Best Workplaces for Diversity (#38), Fortune Best Workplaces for Parents (#31), Fortune Best Workplaces for Women (#20), Fatherly's Best Workplaces for New Dads (#37), JUST Capital 100 Company (#69), Bloomberg Gender Equality Index constituent.
Zillow Group is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. Therefore, we provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. If there are preparations we can make to help ensure you have a comfortable and positive interview experience, please let us know."
37,Jr. Data Engineer,Charles Schwab,,"Lone Tree, CO 80124","Your Opportunity
Do you want to be part of a Data Solutions Delivery team managing over 150+ terabytes of data and building the next generation analytics platform for a leading financial firm with over $3.2 trillion in assets under management? At Schwab, the Global Data Technology (GDT) organization governs the strategy and implementation of the enterprise data warehouse and emerging data platforms. We help Marketing, Finance and executive leadership make fact-based decisions by integrating and analyzing data.
We are looking for a Data Engineer who has passion for data and comes with data engineering background. Someone who has experience in designing and coding batch as well as real time ETL and one who wants to be part of a team that is actively designing and implementing the big data lake and analytical architecture on Hadoop. You will have the opportunity to grow in responsibility, work on exciting and challenging projects, train on emerging technologies and help set the future of the Data Solution Delivery team.
What you’re good at
Designing schemas, data models and data architecture for Hadoop and HBase environments
Building and maintaining code for real time data ingestion using Java, MapR-Streams (Kafka) and STORM.
Implementing data flow scripts using Unix / Hive QL / Pig scripting
Designing, building and support data processing pipelines to transform data using Hadoop technologies
Designing, building data assets in MapR-DB (HBASE), and HIVE
Developing and executing quality assurance and test scripts
Working with business analysts to understand business requirements and use cases
What you have
Minimum of 2 years of experience in understanding of best practices for building and designing ETL code Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required
Knowledge in schema design, developing data models and proven ability to work with complex data is preferred
Hands-on experience in Java object oriented programming (At least 2 years)
Hands-on experience with Hadoop, MapReduce, Hive, Pig, Flume, STORM, SPARK, Kafka and HBASE is preferred
Understanding Hadoop file format and compressions is preferred
Familiarity with MapR distribution of Hadoop is preferred
Understanding of best practices for building Data Lake and analytical architecture on Hadoop is preferred
Scripting / programming with UNIX, Java, Python, Scala etc. is preferred
Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required
Knowledge in real time data ingestion into Hadoop is preferred
Experience in working in large environments such as RDBMS, EDW, NoSQL, etc. is preferred
Knowledge of Big Data ETL such as Informatica BDM and Talend tools is preferred
Understanding security, encryption and masking using Kerberos, MapR-tickets, Vormetric and Voltage is preferred
Experience with Test Driven Code Development, SCM tools such as GIT, Jenkins is preferred
Experience with Graph database is preferred"
38,Entry Level Data Engineer,Naval Nuclear Laboratory,,"Niskayuna, NY 12309","Description
The Naval Nuclear Laboratory is seeking an entry level Data Engineer to join a growing team dedicated to collecting and transforming data from the Naval Nuclear fleet for analysis throughout the Laboratory.
As a Data Engineer, you will be responsible for designing, developing, optimizing, and maintaining the Extract, Transform, and Load (ETL) process for incoming shipboard data used in naval nuclear propulsion systems in a Hadoop environment. You will design and implement scripts and software that defines and optimizes the architecture that stores and serves data to internal engineering customers. You will evaluate new and emerging technologies to deliver scaleable solutions to transforming shipboard data in an enterprise-wide data warehouse from which operational metrics and support analytics can be easily and reliably generated.

Requirements
Associate:
BS degree in engineering or Bachelor's degree in a science related field from an accredited college or university
Experience in Linux and scripting (SQL , Python).
Intermediate:
BS degree in engineering or Bachelor's degree in a science related field from an accredited college or university and a minimum of two years relevant experience; or
MS degree in engineering or Master's degree in a science related field from an accredited college or university
Experience in Linux and scripting (SQL , Python).
Preferred Skills
Knowledge of distributed technologies (Hadoop, Hive, Spark, etc).
Expertise in optimization and performance tuning of ETL workloads and understanding of database internals.
Knowledge of Rstudio and Tableau."
39,Data Engineer,Spin Electric Scooters,,"San Francisco, CA","About Spin

Spin operates electric scooters in cities and campuses nationwide, bringing sustainable last-mile mobility solutions to diverse communities. Recognized for its consistent cooperation and collaboration with cities, Spin partners closely with transportation planners, elected officials, community groups, and university administrators to bring stationless mobility options to streets in a responsible and carefully orchestrated manner.

Based in San Francisco, Spin is a diverse team of engineers, designers, urban planners, policymakers, lawyers and operators with experience from Y Combinator, Lyft, Uber, local and federal government, and the transportation advocacy world. Spin was known for launching the first stationless mobility program in Seattle, and has since expanded to become the exclusive electric scooter partner in mid-sized cities like Coral Gables, Florida and Lexington, Kentucky, and one of a few permitted scooter operators in large cities like Denver, Detroit, and Washington, D.C. The team embeds in cities and neighborhoods to understand their specific transportation needs, and hires locally from the community.

Spin is expanding quickly and looking for top-tier talent to help us bring affordable and accessible transportation options to cities and define what future safe streets will look like.

About the Role

Being a data informed company, data helps us create exceptional experience for our customers and provide insights into the effectiveness of our product.

We are looking for Data Engineers that will build and maintain our data warehouse and data pipelines, collect data from multiple sources, and expose services that make data a first class citizen at Spin. You will be building, architecting and launching highly reliable and scalable data pipelines to support data processing and analytics needs. Your efforts will allow access to business and user behavior insights.

The Team

Our engineering team consists engineers that are passionate about creating finely polished and intuitive experiences and, at the same time, obsess over performance and reliability of what we build. We challenge the status quo and strive towards finding the best way to solve problems.

We promote being a more well rounded engineer by working on different parts of the engineering stack. We also work in very small groups to keep processes and overhead low, so we have a lot of trust and accountability to perform the work required to build the best product.
Responsibilities
Build and maintain our data warehouse and data pipelines
Scaling up our data infrastructure to meet business needs
Deploy sophisticated analytics programs, machine learning and statistical methods
Work cross-functionally with our product, business, finance and engineering teams
Qualifications
Minimum of 3 years of relevant experience in building and architecting data solutions
Deep understanding of distributed systems
Expert in SQL and high-level languages such as Python, Java, or Scala
Built and maintained data warehouses and ETL pipelines
Experience with Data modeling
You have worked with big data solutions like Redshift, Snowflake, Hadoop or Hive
Experience with realtime data streaming infrastructure like AWS Kinesis, Spark or Kafka
Worked with Cloud-based architecture such as AWS or Google Cloud
Benefits & Perks
Opportunity to join a fast-growing startup and help shape and establish the company’s industry leadershipCompetitive health benefitsDaily catered lunch in our SF officeUnlimited PTO for salaried rolesCommuter stipend plus pre-tax benefitsMonthly cell phone bill stipendWellness perk for salaried roles

Spin is an equal opportunity employer and will not discriminate against any employee or applicant for employment in an unlawful matter. We celebrate diversity and are committed to creating an inclusive environment for all individuals. Spin treats all employees and job applicants on the basis of merit, qualifications, and competence without regard to any qualified individuals' sex, race, color, religion, national origin, ancestry, gender (including pregnancy, breastfeeding, or related medical condition), sexual orientation, gender identity, gender expression, age, physical or mental disability, medical condition, genetic characteristic or information, marital status, military and veteran status, or any other characteristic protected by state or federal law. Spin also considers qualified applicants with criminal histories, consistent with applicable local, state, and federal law.

Spin is committed to providing reasonable accommodations for qualified individuals with disabilities in its job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at job_accommodations@spin.pm."
40,Data Engineer,Zillow Group,,"Lincoln, NE 68502","About the team
Mortech, a Zillow Business, is seeking a savvy Data Engineer to join our team of Mortgage Data experts. You will be responsible for building out collection, transformation and processing pipeline automation for mortgage related data. You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection. The ideal candidate is an experienced data pipeline builder and enjoys optimizing data systems and building them from the ground up. You must be self-directed and comfortable supporting the needs of an experienced team and established products.
About the role
The right candidate will be excited at the prospect of optimizing or even reimagining our company’s data architecture to support our next generation of products and data initiatives.
Key Responsibilities:
Work with data and product teams to develop new tools and systems to support the growth of the business
Optimize our data pipeline architecture
Modernize our current ETL to leverage cloud based solutions (AWS)
Create and deliver scalable and reliable software to collect, process and validate data for our mortgage applications.
Support data infrastructure and database processes
Build compliance and operational requirements like tracking, logging and monitoring into data transformation framework.
Ready the data platform for extensibility by the data science and tech teams
Knowledgeable about data modeling, data access, and data storage techniques
Who you are
Qualifications:
BS in CS or similar and 3+ years professional experience, or MS and 1+ years
Strong experience with databases, SQL, writing and debugging
Professional experience with Python development
Working knowledge of Java, AWS Services,
Experience working with MySQL, or similar
Experience with cloud based data solutions (AWS preferred EMR, Redshift, Kinesis)
Experience with automation/configuration management/enterprise schedulers
System monitoring and alerting, dashboarding experience
Working understanding of code and script (Java, Python, JavaScript, etc)
Who you are
You know how to work with high volume of rapidly changing data
You are knowledgeable about data modeling, data access, and data storage techniques
You appreciate agile software processes, data-driven development, compliance, reliability, accuracy, and responsible experimentation.
You understand the value of partnership within teams and create excitement around your ideas and work to achieve them
Get to know us
Mortech supplies thousands of mortgage professionals with a number of services and tools, such as all-in-one pricing, rate notification, prospect management tools, custom rate sheets, loan product eligibility and guideline services.
At Zillow Group, we're powered by our inclusive work culture, where everyone has the support and resources to do the best work of their careers. Our efforts to streamline the real estate transaction is supported by our passion to empower people and enrich lives around everything home, a deep-rooted culture of innovation, a fundamental commitment to Equity and Belonging, and world-class benefits. But, don't just take our word for it. Read our reviews on Glassdoor and recent recognition from multiple organizations, including: Fortune 100 Best Companies to Work For (#69), Fortune Best Workplaces for Diversity (#38), Fortune Best Workplaces for Parents (#31), Fortune Best Workplaces for Women (#20), Fatherly's Best Workplaces for New Dads (#37), JUST Capital 100 Company (#69), Bloomberg Gender Equality Index constituent.
Zillow Group is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. Therefore, we provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. If there are preparations we can make to help ensure you have a comfortable and positive interview experience, please let us know."
41,Data Engineer,Amgen,,"Tampa, FL","Career Category
Information Systems
Job Description
Global Commercial Operational IS is looking for a talented Data Engineer, who is curious to learn and able to develop data engineering and data analytics solution in a fast-moving environment. Candidate will work closely with senior data engineer and product owner/business analyst to understand the requirement. This role will be part of the newly established technical/engineering team, develop data flow pipelines to extract, transform, and load data from various data sources in various data format to enterprise data lake and data warehouse system in three regions in AWS. Provide data analytics and predictive analysis to business users.
The Data Engineer will report to the Senior Manager, Global Medical based out of Amgen’s Capability Center in Tampa, FL. At Amgen, our mission is simple: to serve patients. Our Tampa Capability Center provides essential services that enable us to better pursue this mission. This state-of-the-art center serves as a base for finance, information systems, and human resources professionals to make a meaningful impact at one of the world’s leading biotechnology companies.
Responsibilities:
Be a key team member assisting in design and development of the Data pipeline for Global Data and Analytics team
Collaborate with Data Architects, Business SME’s, and Data Scientists to design, develop end-to-end data pipeline to meet fast paced business need across geographic regions
Build data products and service processes which perform data transformation, metadata extraction, workload management and error processing management
Implement standardized, automated operational and quality control processes to deliver accurate and timely data and reporting to meet or exceed SLAs
Adhere to best practices for coding, testing and designing reusable code/component
Contribute to the Exploration and understanding of new tools, and techniques and propose improvements to the data pipeline
Integrate the operations data platform with other Amgen enterprise data lake platform/product
Participate in sprint planning meetings and provide estimations on technical implementation; Collaborate and communicate effectively with the product teams
Act as a run manager, provide Run/DevOps support
Travel – Approximately 15% of work time
Basic Qualifications:
Master’s degree
OR
Bachelor’s degree and 2 years of Data Engineering and/or and Software Engineering experience
OR
Associate’s degree 6 years of Data Engineering and/or Software Engineering experience
OR
High school diploma and 8 years of Data Engineering and/or Software Engineering experience
Preferred Qualifications:
Experience with software development (Java, Python preferred), end-to-end system design
Experience with data modeling for both OLAP and OLTP databases, hands-on experience with SQL, preferred Oracle, PostgreSQL, and Hive SQL
Experience with ETL tool, for example Informatica PowerCenter, Snaplogic
Ability to learn quickly, be organized and detail oriented
Hands on development experience with Informatica Power Center and MDM
Experience with software DevOps CI/CD tools, such Git, Jenkins
Experience on AWS, familiar with EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, and API gateway
Experience with Apache Airflow and Apache Spark
Experience with Tableau Dashboard and Tableau Server
Experience with Pharmaceutical industry, commercial operations
Amgen is committed to unlocking the potential of biology for patients suffering from serious illnesses by discovering, developing, manufacturing and delivering innovative human therapeutics. This approach begins by using tools like advanced human genetics to unravel the complexities of disease and understand the fundamentals of human biology.
Amgen focuses on areas of high unmet medical need and uses its expertise to strive for solutions that improve health outcomes and dramatically improve people’s lives. A biotechnology pioneer since 1980, Amgen has grown to be one of the world’s leading independent biotechnology companies, has reached millions of patients around the world and is developing a pipeline of medicines with breakaway potential.
Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
Join Us
If you're seeking a career where you can truly make a difference in the lives of others, a career where you can work at the absolute forefront of biotechnology with the top minds in the field, you'll find it at Amgen.
Amgen, a biotechnology pioneer, discovers, develops and delivers innovative human therapeutics. Our medicines have helped millions of patients in the fight against cancer, kidney disease, rheumatoid arthritis and other serious illnesses.
As an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other but compete intensely to win. Together, we live the Amgen values as we continue advancing science to serve patients.
Amgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
."
42,Data Engineer,City Furniture,,"Tamarac, FL 33321","City Furniture is seeking a Data Engineer to join our growing and innovative IT Department. The purpose of the Data Engineer is to develop and maintain the data architecture of City Furniture’s Infrastructure System. The Data Engineer will be responsible for expanding and optimizing our Enterprise Data Warehouse.
Responsibilities include:
Design, develop and support data solutions at City Furniture.
Collaborate with Architect and Senior Engineers to design and implement a corporate data model and data catalog.
Assist with translating business requirements into technical solutions.
Utilizes data warehousing technologies to automate and optimize the data pipeline
Create reports using enterprise analytics tools to provide business insights.
Work with stakeholders to assist with data related issues and support infrastructure need.

With 31 City Furniture and Ashley HomeStore locations throughout Florida, City Furniture continues to be recognized as one of the top furniture companies in South Florida and in the top 30 nationwide. With planned local growth, plus growth into new markets over the next several years, the City Furniture team is excited about the future. Join our fun, family-spirited team to build a long-lasting career at a company that will continue to challenge, develop, and appreciate its Associates.

Benefits:
Competitive Compensation Package
Year-end Performance Bonus
Comprehensive Health Insurance Package
401k with employer match
Associate Discount Program
Tuition Reimbursement
Paid Vacation
Free Onsite Health Clinic (Tamarac)
Promote-from-within culture, with A LOT of opportunity to grow
Job Requirements
Qualifications Required
Education – Bachelor’s Degree or Associates Degree in applicable field
Work Experience – Previous 1 year of full time, part-time, or project-based experience working with SQL and relational databases.
Computer Knowledge and Skills - Below is the list of the technologies, programming languages, and software applications that are desired for this position
SQL knowledge and experience designing and working with relational databases
Strong analytical skills to work with structured and unstructured data sets"
43,Big Data Engineer,Cognizant,,"San Francisco, CA 94102","Our strength is built on our ability
to work together. Our diverse backgrounds offer different perspectives and new
ways of thinking. It encourages lively discussions, inspires thought
leadership, and helps us build better solutions for our clients. We want
someone who thrives in this setting and is inspired to craft meaningful
solutions through true collaboration.

If you are comfortable with
ambiguity, excited by change, and excel through autonomy, we would love to hear
from you.

Why Choose Cognizant?

It takes a lot to succeed in today’s
fast-paced market, and Cognizant Digital Business has become a proven leader in
the industry. Cognizant love big ideas and even bigger ambitions. We stand out
because we put human experiences at the core.

We help clients engage customers by
envisioning and building innovative products and services. But we don’t stop
there. We develop go-to-market strategies and invent entirely new business
models, ensuring that every company we work with walks away with both
inspiration and a plan.

Everything we do at Cognizant we do
with passion—for our clients, our communities, and our organization. We look
for in our people the defining attribute.

Data
Engineer

Responsibilities:
Ensure weekly reports are circulated to the relevant
stakeholders as per agreed timelines.
Responsible for End to End Service Delivery
Ensure service performance of the project through
defined metrics and KPI's.
Participate in PMR and Service performance review
meeting and ensure availability of all relevant performance data.
Ensure that status reports & dashboards are sent to
the customer for the project(s) at the agreed intervals.
Resolving all customer requests, issues / escalations
in a timely manner.
Technically sound and hands on experience in Big Data
technologies such as Hive/Presto/HDFS to perform data engineering work

Require

skills:
Python coding skills required to make sure automation
required for the project
Strong SQL knowledge is required to perform data
analysis and data quality checks
AWS
Well verse with GitHub code versioning.
Good verbal and written communication skills

Desire

Code Configuration using GitHub/other tools is
desirable
Good to have- Apache Airflow and Einstein analytics
understanding.
Good to have Airflow Working experience on big data
Hadoop QE Skills
Strong SQL Big data Hive QE Skills

Cognizant is one of the world's
leading professional services companies, transforming clients' business,
operating and technology models for the digital era. Our excellent
industry-based, consultative approach helps clients envision, build and run
more creative and efficient businesses. Headquartered in the U.S.,

Cognizant, a member of the
NASDAQ-100 ranked 205 on the Fortune 500 and consistently listed among the most
admired companies in the world.

Technical Skills SNo Primary Skill Proficiency Level * Rqrd./Dsrd. 1 Hive PL3 Required 2 Apache Hadoop PL3 Required 3 Python PL3 Required


Proficiency Legends Proficiency Level Generic Reference PL1 The associate has basic awareness and comprehension of the skill and is in the process of acquiring this skill through various channels. PL2 The associate possesses working knowledge of the skill, and can actively and independently apply this skill in engagements and projects. PL3 The associate has comprehensive, in-depth and specialized knowledge of the skill. She / he has extensively demonstrated successful application of the skill in engagements or projects. PL4 The associate can function as a subject matter expert for this skill. The associate is capable of analyzing, evaluating and synthesizing solutions using the skill.

Employee Status : Full Time Employee
Shift : Day Job
Travel : No
Job Posting : Aug 09 2019

About Cognizant Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 193 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @Cognizant.
Cognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service."
44,Cloud Data Engineer,Accenture,,"Seattle, WA 98104","Cloud Data Engineer – Consultant

Come join Accenture and Innovate in a company with AI and data analytics in its DNA. Leverage our unparalleled scale, scope, investment and global footprint to solve clients’ business needs. You’ll drive results accessing 750+ industrialized apps and solutions, 800+ analytics and 300+ AI patents/patents-pending, and the Applied Intelligence Platform (API) that combines cutting edge and advanced analytics and automated AI with an integrated suite of leading tools and technologies.

Accenture Applied Intelligence is the world’s largest team in applying data science, machine learning, and AI with deep industry experience to solve clients most sophisticated and difficult challenges. We are a team of experts in data science, data engineering, artificial intelligence and human ingenuity with industry knowledge that spans every industrialized area - energy, health care, transportation, retail, social media, and more. By deploying AI responsibly and combining it with our deep industry and analytics expertise, we enable the digital transformation of organizations, extend human capabilities, and make intelligent products and services a reality.

Role Description

The Cloud Data Engineer role is responsible for delivery and implementation of Google Cloud Platform (GCP) Infrastructure and solutions. The ideal candidate would also be responsible for developing and delivering cloud solutions to meet today’s high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, server less, etc. Using Google Cloud Platform (GCP) public cloud technologies, our Solution Architect professionals implement state-of-the-art, scalable, high performance solutions that meet the need of today’s corporate and emerging digital applications.

Key Responsibilities

Assist in a full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.
Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.
Apply Methodology, reusable assets, and previous work experience to deliver consistently high-quality work.
Deliver written or oral status reports regularly.
Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.
Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.
Use considerable judgment to define solution and seeks guidance on complex problems.
Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures on new assignments with guidance.
Extensive travel may be required.

Basic Qualifications
Minimum of 3 years of technical solutions implementation, architecture design, evaluation, and investigation in a cloud environment.
Minimum of 1 year of Client Management and/or Project Management experience
Minimum of 1 year of professional experience in Solution/technical architecture in the cloud
Minimum of 1 year of experience in Big Data/analytics/information analysis/database management/ event-driven/microservices in the cloud

Preferred Skills

Minimum of 1 year of experience in Google Cloud Platform (GCP)
Google Certified Professional Cloud Architect – a big plus
Experience in any of Messaging platforms (Kafka, PubSub, MuleSoft, etc. )
Hands-on experience with any of the following programming languages (Python, Java, C++, Go, Ruby, C)

Professional Skills Requirements

Excellent communication (written and oral) and interpersonal skills
Proven ability to work creatively and analytically in a problem-solving environment.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration."
45,Data Engineer,Simon-Kucher & Partners,,"Boston, MA","To support our project teams, we are looking for
Data Engineer-USA
for our office in Boston, United States
About Simon-Kucher & Partners
For over 30 years, Simon-Kucher & Partners has helped development stage to Global Fortune 500 companies in addressing their strategic and marketing challenges. We are regarded as the world’s leading pricing advisor and thought leader. We focus on TopLine Power® by helping clients grow their top-line through strategy, pricing, marketing and sales.

Globalization is a core element of our goals and strategy. Today we are an international and multilingual team of over 1,300 full-time employees from diverse academic and professional backgrounds in over 38 offices across the US, Europe, Asia Pacific, and South America.
The Role:
The Simon-Kucher Data Engineering team plays a crucial role on project teams, enabling team members to efficiently handle data, conduct various analyses, and deliver solutions to clients.
As a Data Engineer at Simon-Kucher & Partners, you will be responsible for key tasks related to data extraction and insights. You will support the data handover process with clients and build out the core project databases that will be used to conduct analyses throughout the project. You will gain experience through frequently changing project assignments and no year-long IT implementation projects.
Key Responsibilities:
Validate data and ensure completeness, correctness, and relevance for analyses
Develop and automate analytical tools based on the generated cubes to provide marketing, sales and pricing insights
Detect, solve, and implement internal process and technology improvements, as well as create, optimize, and maintain new and existing data transfer channels
Advise and support clients on how to model, implement and automate robust ETL processes
Work with internal and external business and technology experts across the world
Basic Qualifications:
Degree in a quantitative field, such as computer science, engineering, statistics, operations research, data science, or equivalent experience
Experience in relational data management software, languages (esp. SQL), and ETL processes
Proven capabilities to build and maintain large and complex data sets
Extensive knowledge of analytical / Business Intelligence software, programming languages (e. g. Tableau, PowerBI, SAS)
Sharp analytical mindset with a pro-active and reliable attitude
Preferred Qualifications:
Experience with large scale data processing tools (Spark, Hadoop, NoSQL, etc.)
Strong programming skills in R and/or Python
Data modeling (variable transformation & summarization, algorithm development)
Experience with cloud-based data infrastructure (AWS, Azure, Google, etc.)
Familiarity with machine learning models and data pipelines
To Apply:
Submit your Resume/CV and Cover Letter via the Simon-Kucher job portal. Please ensure your Cover Letter includes:
Why you are interested in a job at Simon-Kucher.
Specific reasons how your application differentiates you from other candidates."
46,Sr. Data Engineer,Screen Actors Guild- Producers Pension & Health Plans,,"Burbank, CA","Sr. Data Engineer
IT Applications SRDAT01211
Apply now

Posted: August 9, 2019
Full-Time
Burbank, CA, USA
Job Details
Description
Essential job functions

Carry out a comprehensive assessment of existing data warehouse design and provide a recommendation alongside with an implementation roadmap.
Take ownership of the process to design conceptual, logical and physical data models, maintain data dictionary and capture metadata.
Explore and provide recommendations on ways of modeling the Plans unstructured voice and text data into frameworks fit for analysis.
Lead and establish best practices of implementing data models that are fit-for purpose; efficient to access data thus enable transparency of data lineage to business teams and all stakeholders
Establish and maintain provenance, integrity and security of data used for self-service reporting, ad-hoc analysis or other levels of analysis.
Work closely with data experts to build and maintain KPI data dictionary, metadata, data standards, and ensure adherence to the Plans Analytics Method and data standards.
Engage business teams to understand requirements, document them and deliver robust and scalable solutions in the form of data models that can be leveraged for self-service analytics.
Perform gap analysis as needed for purposes of maintaining, continuously enhancing data models and integrating KPI’s into the analytics platform as and when new KPI and business metrics are adapted by the organization.
Work with application development team to deploy analytics data products through such ways as embedding analysis models into business applications and mobile solutions.
Utilize ETL tools and other data pipeline automation techniques to develop and maintain source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.
Competencies

Customer focus - Gains insight into customer needs; identifies opportunities that benefit the customer; builds and delivers solutions that meet customer expectations; establishes and maintains effective customer relationships.
Decision quality – Makes sound decisions, even in the absence of complete information; relies on a mixture of analysis, wisdom, experience, and judgment when making decisions; considers all relevant factors and uses appropriate decision-making criteria and principles; recognizes when a quick 80% solution will suffice.
Communicates effectively – Is effective in a variety of communication settings: one-on-one, small and large groups, or among diverse styles and position levels; attentively listens to others; adjusts to fit the audience and the message; provides timely and helpful information to others across the organization; encourages the open expression of diverse ideas and opinions.
Ensures accountability – follows through on commitments and makes sure others do the same; acts with a clear sense of ownership; takes personal responsibility for decisions, actions, and failures; establishes clear responsibilities and processes for monitoring work and measuring results; designs feedback loops into work.
Instills trust – follows through on commitments; is seen as direct and truthful; keeps confidences; practices what he/she preaches; shows consistency between words and actions.
Manages Ambiguity- Operating effectively, even when things are not certain or the way forward is not clear.
Knowledge, skills, and abilities

Ability to lead planning, re-design and implementation of enterprise data warehouse
Proficient in leading business and internal team discussions to gather requirements, brain-storm and propose robust and scalable solutions; leverage business partner/unit input to enhance data models.
Institute and champion best practices on data management
Expert knowledge of relational DBMS, pipeline management, and database management .
Expert knowledge in areas of advanced data techniques including unstructured and spatial data.

Qualifications
Behaviors
Preferred
Detail Oriented: Capable of carrying out a given task with all details necessary to get the task done well
Dedicated: Devoted to a task or purpose with loyalty or integrity
Education
Required
Bachelors or better in Computer Science or related field.
Experience
Required
Experience in relevant technical languages and tools such as SQL, Python, NoSQL, Airflow, Quartz, ERWIN or equivalent.
Experience in developing and maintaining source to target mapping that includes extract requirements, derived field logic, domain values and data lineage.
Experience creating and maintaining automated data pipelines, data standards, and best practices to maintain integrity and security of the data; ensure adherence to developed standards.
Experience designing conceptual, logical and physical data models and maintaining data dictionary and capturing metadata.
Relevant experience designing and implementing data warehouse, owning data models and related data ingestion processes.
8 years: Data engineering, data modeling or data architecture experience with a focus on multidimensional data modeling for both structured and unstructured data.
Preferred
Experience building data models for analytics and reporting projects.
Previous work experience in the Healthcare industry preferred."
47,Marketing Data Engineer,Tableau,,"Seattle, WA 98107","What you’ll be doing…
The Marketing Data Engineer contributes in technical solutions on behalf of Tableau’s Marketing Data Engineering team. This team is responsible for building and operating our cloud-based Marketing data platform that enables self-service Marketing analytics. The Data Engineer role will help solve challenging data integration problems by participating in development, architecture, and operations work related to building the Marketing data platform. The role will collaborate with product managers, engineers, and internal customers to scope and define implementation plans that meet business requirements. The Data Engineer supports Tableau’s core value commitments to delight our customers and work as a team.
Some of the things you’ll be doing include…
Help to build and architect the Marketing Data Platform.
Build robust scalable data processing and data integration pipelines using Python and SQL.
Develop data quality automations and unit tests to ensure the accuracy of the data delivered to the Analysts and Business Customers.
Build solutions that scale as our data volumes grow exponentially.
Define and implement monitoring and alerting policies for data solutions.
Help modernize our SDLC for CI/CD automation and the Cloud.
Develop data models that support analytical models used by Tableau.
Participate in code reviews and related processes.
Work with product managers, engineers, and internal customers to identify and scope of the requests and define implementation plans.
Who you are…
Experienced . 2-3+ years professional experience in software engineering role. 1-2+ years ETL development experience
Technical . A technical background in data architecture, data pipeline architecture & development, data warehousing concepts. Demonstrated experience writing Python code. Demonstrated experience writing complex, highly-optimized SQL queries across large data sets. Experience with: AWS services such as EC2, S3, IAM roles, ECS, CloudWatch, etc., Gilt, Snowflake is handy but not required, Airflow and Kafka is handy but not required, containers and container orchestration tools such as Docker and Kubernetes are handy but not required, and CI/CD pipelines is handy but not required.
Domain . Strong familiarity with marketing data and marketing analytics uses-cases & needs.
Team Player . A willingness to jump in and help when needed, learn and teach new skills, and have the experience and professionalism required to meet objectives.
A Problem Solver . You love tackling the most difficult of challenges and know how to get to the best solution.
Educated. BA/BS in Computer Science or equivalent. WS Certifications on either a Developer or Architect track preferred but not required.
You are a Recruiter ! Tableau hires company builders and, in this role, you will be asked to be on the constant lookout for the best talent to bring onboard to help us continue to build one of the best companies in the world!
#LI-SC2
Tableau Software is an Equal Opportunity Employer.
Tableau Software is a company on a mission. We help people see and understand their data. After a highly successful IPO in 2013, Tableau has become a market-defining company in the business intelligence industry. Our culture is casual and high-energy. We are passionate about our product and our mission and we are loyal to each other and our company. We value work/life balance, efficiency, simplicity, freakishly friendly customer service, and making a difference in the world!"
48,Data Engineer,Waystar,,"Chicago, IL","master Solid understanding of workflow systems and their application to customer business process improvement Experience working with HL7 interface engines such as Mirth Connect, Cloverleaf, Corepoint, or Ensemble General understanding of healthcare revenue cycle management (RCM), including patient accounting, claims processing, follow-up processes, and reimbursement practices Familiarity with Salesforce.com
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire."
49,Data Engineer,"Pinpoint Pharma, LLC",$25 an hour,"Melrose Park, IL","Job Title: Data Engineer
Pay: $25.00/hour
Job Overview:
Assisting in implementing Data Mining, Data Management & Data Analytics projects by identifying sources of data, facilitating the collection of clean data, performing analysis and deploying algorithms for machine learning and automated data analytics as well as acting as a liaison between the company and third party vendors of Digitalization solutions.
Job Qualificiations:
Experience with Data Analytics.
Knowledge of machine learning.
Degree in Engineering, Analytics, etc."
50,Data Engineer,Homesite Insurance,,"Seattle, WA","Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.
One thing that's stayed the same since our founding: our commitment to our customers, partners and employees.
Join us on our journey as we continue to grow into a powerful contender in the field of insurance.
Data Engineer

We’re looking for a Data Engineer to help us transform our data systems and architecture to support greater variety, volume, and velocity of data and data sources. You might be a good fit if:

You enjoy extracting data from a variety of sources and find ways to connect them and make them suitable for use in software systems and for the development of models and algorithms.
You enjoy interacting with new database systems and learning new data technologies and are interesting in developing your knowledge of new tools and techniques.
You are interested in automating data engineering efforts to minimize human interaction and optimizing data quality.
You have an interest in developing your knowledge of practical data science techniques and technologies in addition to your data engineering knowledge and experience.
This role requires comprehensive data engineering skills and is not a SQL developer role though SQL is a required skill.
Responsibilities:
We’re looking for an experienced data engineer to help us:
Build and Maintain serverless data ingestion and refresh pipelines in terabyte scale using AWS cloud services – Amazon Glue, Amazon Redshift, Amazon S3, Amazon Athena, DynamoDB, and others
Incorporate new data sources from external vendors using flat files, APIs, web-scraping, and databases.
Maintain and provide support for the existing data pipelines using Python, Glue, Spark, and SQL
Work to develop and enhance the database architecture of the new analytic data environment that includes recommending optimal choices between relational, columnar, and document databases based on requirement
Identify and deploy appropriate file formats for data ingestion into various storage and/or compute services via Glue for multiple use cases
Develop real-time/near real-time data ingestion from web and web service logs from Splunk
Maintain existing processes and develop new methods to match external data sources to Homesite data using exact and fuzzy methods
Implement and use machine learning based data wrangling tools like Trifacta to cleanse and reshape 3rd party data to make suitable for use.
Develop and implement tests to ensure data quality across all integrated data sources.
Serve as internal subject matter expert and coach to train team members in the use of distributed computing frameworks for data analysis and modeling including AWS services and Apache projects
Qualifications:
Master’s degree in Computer Science, Engineering, or equivalent work experience
Two to four years’ experience working with datasets with hundreds of millions of rows using a variety of technologies
Intermediate to expert level programming experience in Python and SQL in Windows and Mac/Linux environment
Intermediate level experience working with distributed computing frameworks, especially Spark"
51,Data Engineer,GumGum,,"Santa Monica, CA","At GumGum, our ad servers produce over 50 TB of new raw data every day. It amounts to ~100 billion events that needs to be processed per day. Dealing with data at this scale is challenging in a number of ways. We deal with a number of off-the-shelf frameworks including Spark, Kafka, Cassandra, DynamoDB, Redshift, but often push them past their limits. This team is responsible for providing critical ad reporting data for GumGum’s internal and external customers.
Responsibilities
Refining our data infrastructure technologies such as Kafka, Spark, Druid, Fluentd to support real time analysis of data
Own the core data pipelines and scale our data processing flow.
Build scalable systems with various AWS & Big Data technologies, lead technical discussions, participate in code reviews, guide the team in engineering best practices. Must be able to write quality code and build secure, highly available systems.
Work on GumGum’s proprietary Reporting Server
Work on various reports using Groovy, SQL and Java
Work on GumGum’s proprietary forecasting system
Minimum Qualifications
At least 1 year of Apache Spark experience
At least a Bachelor's degree in Computer Science or equivalent
3+ years of Software Engineering experience (Java/Scala/Python)
Experience with large scale distributed real-time systems with tools such as AWS, Spark, Kafka, Hadoop
Familiar with various AWS services, Serverless architecture and containers
Experience with high volume, high availability production systems.
Strong problem solving skills, strong verbal and written communication skills
Benefits & Perks
Competitive health, vision and dental benefits
Healthcare and dependent care FSA
Employer-matched 401(k) plan
Stock incentive program
Paid parental leave
Fitness reimbursement and wellness workshops
Discounted Pet Health Program
Flexible time off and work schedule
Commuter perks
Stocked kitchen & Everytable Smartfridge
Dog-friendly HQ office - we love our fur babies!
Incredible work/life balance with a collaborative and friendly work space
GumGum Gives Back volunteering opportunities
Team building lunches and events, and monthly company celebrations
Located in hotbed of tech startups, just blocks from the beach
Career & Personal Development Focus
Ongoing learning and development for education opportunities such as webinars, books, classes, relevant conferences and events
Opportunities to pursue business related side projects and yearly Hackathon
Highly encouraged to contribute to open source software, including our own open source software
Environment of learning from peers, including meetups, presentations and blog posts
Opportunity to work with cutting edge technology
Life Skills sessions - geared towards the whole life/health/person
Leadership Bites Dinner Series - connecting current and future GumGum leaders over great food and meaningful conversation
____________________________________________________________

We invite you to learn more about GumGum!
Our Team...
GumGum is an AI company with expertise in computer vision built on the imagination of its people. Each day, our talented team of thinkers and doers comes together to solve hard problems on behalf of the world’s most successful businesses. Through a combination of computer science, creativity and hustle, we’ve produced a series of major technology breakthroughs across a variety of industries - from advertising to sports.
Our Tech...
GumGum is constantly evolving to be at the forefront of computer vision and machine learning advancements. With nearly 10 years experience, we use our proprietary image recognition technology to deliver highly engaging, contextually relevant ad experiences across premium publishers all over the world.
Our Products & Services...
Our Advertising business serves contextual marketing messages in line with content users are actively engaged with.
Our Sports business helps marketers and rights holders understand the full media value of their sponsorship investments across broadcast and social media. And we deliver on brand safety by leveraging our AI to detect unsafe text and imagery, allowing us to deliver ads in brand safe, contextually relevant environments.
Our Hackathons...
What’s a Hackathon? We’re glad you asked. Our employees split into teams and spend 48 hours ‘hacking’ together before presenting their ideas to our executive team. It’s a chance for anyone within our company to showcase the visions they want to bring to fruition.
Our Culture...
GumGum recently earned LA’s Best Places to Work Award, and it’s no surprise why. With company-sponsored social hours, annual holiday celebrations and o-site gatherings, GumGummers enjoy a fun, creative and collaborative workplace. We provide ourselves on our strong track record of giving employees the autonomy and support they need to succeed.
Oh, and doggies...
We love our dogs so much, we even have an Instagram dedicated to them! Follow us: @dogsofgumgum
In Addition...
About GumGum
Careers at GumGum
GumGum Women in Tech Panel - Living a Balanced Life
Women of GumGum Around the World
Innovators in Esports Content
Sponsorship Targeting with Visual Intelligence
GumGum Sports - Sizzle Reel - 2019 (EMEA)
GumGum - LinkedIn
GumGum Advertising
GumGum Sports
Forbes"
52,Informatica Big Data Engineer,Accenture,,"Boston, MA 02199","Join Accenture and help transform leading organizations and communities around the world. The

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity.

Job Description
Data and Analytics professionals define strategies, develop and deliver solutions that enable the collection, processing and management of information from one or more sources, and the subsequent delivery of information to audiences in support of key business processes.
Extract, Transform and Load data primarily in Informatica Powercenter and Big Data Management with an emphasis on advocacy toward end-users to produce high quality software designs that are well-documented.
Demonstrate an understanding of technology and digital frameworks in the context of data integration utilizing Informatica.
Ensure code and design quality through the execution of test plans and assist in development of standards, methodology and repeatable processes, working closely with internal and external design, business, and technical counterparts.

Basic Qualifications
Minimum 4 years of experience developing and implementing Informatica Powercenter or Informatica Big Data Management
Bachelor’s Degree or Associate’s Degree with 6 years of work experience or equivalent work experience of 12 years
Preferred Qualifications
Experience in ETL Tools in addition to Informatica, including Business Objects Data Services (BODS), DataStage, Ab Initio, Talend, and Pentaho
Experience implementing or supporting Data Integration of Big Data with Sqoop or similar tools
Knowledge of Big Data Solutions such as Hadoop Ecosystem
Database experience (Teradata, Oracle, SQL Server, DB2, Azure SQL)
Strong knowledge and experience of SQL
Understanding of Entity relationship data models and Dimensional Models
Experience with development and production support
Professional Skill Requirements
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve clients, our employees must be able to travel when needed. This role requires 100% flexibility to travel and work onsite with clients (typically Monday through Thursday).
Proven success in contributing to a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills

All of our consulting professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or"
53,Data Engineer,Insurity,,"Denver, CO","About Us:
Join a high performing and rapidly growing team
Valen Analytics is a rapidly expanding advanced data and predictive analytics company headquartered in downtown Denver. Valen’s state-of-the-art analytics and predictive modeling products are built on Valen’s unique industry-wide consortium data platform, specifically designed for property and casualty insurance carriers. If you want to help build an industry-changing company and you thrive in a fast-paced environment where the only rule is to WOW customers, colleagues and shareholders, then we encourage you to submit your resume. This is not a job. It is an opportunity to advance your career by helping to build an innovative, growing and highly respected organization. The right candidate will be entrepreneurial, driven and passionate about being successful and absolutely committed to making Valen successful.

We are currently looking to fill the following position in our Denver, CO office:
Data Engineer
Valen Analytics is looking for a Data Engineer to expand our growing data processing needs. We are looking for candidates with at least 2 years of experience, who demonstrate a curious analytical mind with ability to understand business objectives, ask insightful questions, and be detail oriented in implementation.
As a Data Engineer, you will work with customers, Valen team members, and 3rd party data providers, to develop, maintain, and enhance our data engineering capabilities in support of our data and predictive analytic offerings to the market.
What you’ll be doing:
In this role, you will be responsible to identify, acquire, validate, cleanse, and produce data and datasets to be used in advanced analytics and predictive modeling initiatives by our customers and internal teams. How is this accomplished? By combining data processing experience with software engineering concepts into solutions that are hosted in our cloud-based platform, InsureRight.
What you’ll be doing:
Develop technical solutions to improve data and data usage
Troubleshoot basic data and data engineering issues
Perform database and data schema design and maintenance
Perform data cleansing and validation
Process and interpret data sets
Develop reports, dashboards, and tools for business and data science users
Stay up-to-date on industry and job-related trends and best practices, including reading relevant publications, articles, blogs, etc.
Other duties as assigned
What you’ll need:
Minimum 2 years’ Data Engineering experience with TSQL, python, map reduce or functional programming
Bachelor’s degree in Computer Science, Engineering or related computational/quantitative field
Reporting/data warehousing experience
Working knowledge of:
Databases/data structures
SQL
XML, JSON
Data visualization software and tools
Version Control Systems
Programming and scripting
MS Office Suite
Ability to work in a fast-paced environment and handle multiple priorities or tasks
Collaborate with team members in the development and maintenance of our solutions
Identify opportunities to automate data engineering tasks and workflow
Fostering continuous delivery pipelines and managing and maintaining metadata
Insurance industry knowledge or experience with insurance data a plus
In this role you will leverage the following tools and platforms:Microsoft SQL ServerPostgreSQLMongoDBMicrosoft SSISBIRST BIValen InsureRight platform

What you’ll get:
RTD EcoPass | Collaborative Culture | Competitive Salary
Flexible Hours | Growth Opportunities | Generous Time Off
Comprehensive Benefits | 401K Matching | Great Location

Does Valen sound like the right place for you? If yes, then submit your application today for immediate consideration.
Thank you for your interest in Valen; only those candidates selected will be contacted.
Insurity is proud to be an Equal Opportunity Employer"
54,Data Engineer,Valen Analytics-Simply Hired,,"Denver, CO","Join a high performing and rapidly growing team

Valen Analytics is a rapidly expanding advanced data and predictive analytics company headquartered in downtown Denver. Valen’s state-of-the-art analytics and predictive modeling products are built on Valen’s unique industry-wide consortium data platform, specifically designed for property and casualty insurance carriers.

Valen Analytics is looking for a Data Engineer to expand our growing data processing needs. We are looking for candidates with at least 2 years of experience, who demonstrate a curious analytical mind with ability to understand business objectives, ask insightful questions, and be detail oriented in implementation.

As a Data Engineer, you will work with customers, Valen team members, and 3rd party data providers, to develop, maintain, and enhance our data engineering capabilities in support of our data and predictive analytic offerings to the market.

Responsibilities

This position will be part of an existing team whose primary responsibilities are to identify, acquire, validate, cleanse, and produce data and datasets to be used in advanced analytics and predictive modeling initiatives by our customers and internal teams. This is accomplished by combining data processing experience with software engineering concepts into solutions that are hosted in our cloud-based platform, InsureRight.

This position will leverage the following tools and platforms:

Microsoft SQL Server
PostgreSQL
MongoDB
Microsoft SSIS
BIRST BI
Valen InsureRight platform
This position will make sure of the following skills:

Data extraction, transformation, and cleansing
Data profiling and visualization
Collaborating with data scientists, software engineers, production operations, subject matter experts and customers
Fostering continuous delivery pipelines
Managing and maintaining metadata
This position requires the ability to:

Work in a fast-paced environment as part of a small team
Collaborate with team members in the development and maintenance of our solutions
Identify opportunities to automate data engineering tasks and workflow
Fostering continuous delivery pipelines
Managing and maintaining metadata
Education & Experience

2+ years Data Engineering experience with TSQL, python, map reduce or functional programming
Bachelor’s degree in programming or related technical areas
Developing and supporting an end user production system
Reporting/data warehousing experience
Insurance industry knowledge or experience with insurance data a plus
The Valen Team
Valen’s mission is to help our clients achieve their goals and solve problems by leveraging data to make more informed decisions.

Guide Customer Success: We relentlessly pursue making our customers successful.

Live the Golden Rule: We treat our customers, employees, vendors and shareholders how we expect to be treated as customers, employees, vendors and shareholders…period.

Be Agile: Valen is a test and learn environment. We organize everything we do around our customer’s success to provide something of value quickly. We learn and then adapt. Then, we learn some more.

Have Fun: We have great attitudes and we have fun. We do not take ourselves too seriously, we celebrate our successes and we enjoy our work. Most of all, we live passionately.

Embrace Simplicity: We endeavor to make everything we provide our customers ridiculously easy.

Expect Ownership: At Valen we take responsibility for our actions and we build trusting relationships by making and meeting our commitments."
55,Data Engineer Intern (Summer 2020),Wolverine Trading,,"Chicago, IL 60604","Data Engineer Intern (Summer 2020)

Data Engineer Intern (Summer 2020) at Wolverine Trading
What You’ll Do
As a Wolverine Data Engineer Intern, you will have the opportunity to be involved in all aspects of a performance-driven database infrastructure geared towards a fast paced trading environment. On the technical front, the Database team touches every layer of the data stack from hardware to the application layer. That means that the team manages design and implementation from the physical disk-arrays, fibre channel, servers, and OS through to SQL development and administration. You don’t need to be an expert on everything but be ready to leverage your strengths while learning a lot to improve your weaknesses. On the business front, this team works directly with software engineers and traders alike, so you will interact closely with both team members and stakeholders. The ability to have a keen technical understanding but communicate in layman’s terms is important. Day to day tasks can vary between database design, support, tuning, SQL report writing, scripting and anything else that could touch the database layer of the firm.
Afraid you won't be a fit because you don't know about the trading industry? No worries. Wolverine offers classroom education, hands-on training, and mock trading. Wolverine also provides fully furnished apartments that are located close to the office, making your morning commute quick and easy. Additionally, Wolverine will host several social activities throughout the summer to ensure an optimal work-life balance.
What We Look For
Major in CS, MIS, engineering, or an applied science.
0-3 years of experience.
Excellent problem solving skills.
Rising Junior, Senior, or graduate student.
Database related coursework.
Key elements for the internship. Elaborate on these when applying:
MS SQL Server experience.
CHashtag development skills.
Programming or scripting experience (T-SQL preferred).
Financial or trading industry experience.
Database design and performance optimization experience.
Database administration (backups/restores, security, HA).
Networking, DNS, AD, SAN, RAID.
About Wolverine
Founded in 1994, the Wolverine companies comprise a number of diversified financial institutions specializing in proprietary trading, asset management, order execution services, and technology solutions. We are recognized as a market leader in derivatives valuation, trading, and value-added order execution across global equity, options, and futures markets. With a focus on innovation, achievement, and integrity, we take pride in serving the interests of both our clients and colleagues. The Wolverine companies are headquartered in Chicago with offices in New York and San Francisco and a proprietary trading affiliate office located in London.
Sponsorship is not available for this position.





Are you a returning applicant?


Previous Applicants:

Email:

Password:




If you do not remember your password click here."
56,Data Engineer,Air Space Technologies,,"Carlsbad, CA 92011","What we do:

Airspace Technologies is a critical logistics startup. We’re what happens when next-day shipping isn’t fast enough! From Airplane parts to movie reels and even human organs, our software is revolutionizing the industry by giving shippers a view of their packages in every stage of its transportation. We calculate optimal routes, track airplanes in flight, and automatically dispatch drivers using our native mobile app. We have the obligatory pantry full of snacks, fun company activities, unlimited PTO, gym subsidy, and a growing list of benefits that keep employees happy.

We are looking for an innovative Data Engineer to join our growing team of experts. As the Data Engineer, you will be responsible for designing, implementing, and scaling our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Our ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They will support our software developers, database architects, etc. on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. They should be self-directed and comfortable supporting the data needs of multiple teams, systems and products. If you’re excited by the prospect of building our company’s data architecture to support our next generation of data initiatives, this may be the position for you!


What you will do:

Create and maintain optimal data pipeline architecture,

Assemble large, complex data sets that meet functional / non-functional business requirements.

Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.

Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.

Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.

Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.

Work with data and analytics experts to strive for greater functionality in our data systems.

What you will bring:

Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.

Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.

Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

Strong analytic skills related to working with unstructured datasets.

Build processes supporting data transformation, data structures, metadata, dependency and workload management.

A successful history of manipulating, processing and extracting value from large disconnected datasets.

Knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.

Project management and organizational skills.

Experience supporting and working with cross-functional teams in a dynamic environment.

Experience & Skills:


Experience with relational SQL, specifically Postgres.

Experience with AWS cloud services: EC2, EMR, RDS, Redshift

Experience with stream-processing systems: Storm, Spark-Streaming, etc.

Experience with object-oriented/object function scripting languages: Ruby, Java, Python, C++, etc.

Experience with big data tools like Hadoop, Spark, Kafka, Cassandra etc."
57,Snaplogic Data Engineer,Innointel LLC,,"Pleasanton, CA","Role: Snaplogic Data Engineer*Pleasanton, CAExp: 5+yrs*JOB DESCRIPTION: 3+ years of experience with the Snaplogic data integration platformStrong understanding of application and ETL / data integration conceptsAbility to independently create testable, efficient solutionsAttention to Detail: Databases are complex, and a minute error can cause huge problemsProblem-Solving Skills: Data Engineers look at an issue that needs to be solved and come up with solutions quickly.Ability to take initiative and drive things on their own.Job Type: Contract"
58,Data Engineer,Apollo.io,,"San Francisco, CA","About the Company

Apollo accelerates the growth and success of your entire sales org with the first truly reliable, scalable revenue engine and account-based sales platform. We’ve created the solution for the persistent pain that reps aren’t sending the right messages, to the right people, at the right time despite the three to six sales point solutions they use each day.

Managers and reps alike can trust our unified platform, which includes an up-to-date database of 200M+ contacts, a full engagement stack, and the industry’s only advanced Rules Engine and fully custom Analytics suite. Reps get a platform with their team’s best practices built in, so they can focus on selling, and managers can build strategies based on advanced revenue data, not guesswork.

Apollo is the foundation of your entire go-to-market strategy.

The Engineering Team

Apollo is run by a lean team of mission-driven value centric individuals. Our incredibly high caliber team is focused on building a great product our users love. We’re looking to add a passionate, skilled, friendly front-end engineer who is looking to invest their time and energy in the right company.

About the Role

As our first full-time data engineer you will collaborate with our CTO to design and build out our database. Apollo manages a multi-terrabyte scale database consisting of trillions of data points. The primary job of the data engineer will be to expand upon this industry-leading database. You will get the chance to make critical db schema decisions and architectural improvements that lay the DB foundation for the upcoming years. We expect a very strong ability in reasoning through different edge cases, innovating upon data pipelines, cleaning up corrupt data, and architecturing core backend components. The right candidate should be excited by the prospect of re-designing entire data architectures to support our next generation of products and data initiatives.
Requirements
Minimum of 2+ years experience in software engineering or data engineering. CS or equivalent degree preferred
Expert knowledge of at least one open-source popular database, both SQL and NoSQL OK. Knowledge of MongoDB preferred but not required.
Expertise architecturing database schema and pipelines at the terabyte scale
Expertise in at least one modern programming languages such as Python, Ruby, or C++; comfort with writing significant software code and scripts
Assemble large, complex data sets that meet functional / non-functional business requirements.
Work with stakeholders including the Executive, Product, Marketing, and Sales teams to assist with data-related technical issues and support their data infrastructure needs.
Creative and innovative problem solver with experience working in a startup environment
Perks
Competitive salary, equity grants
Top of the line healthcare coverage (medical, dental, vision) and 401(k)
Flexible time-off - recharging and taking time off is a priority for us
Healthy catered lunches every day and a fully-stocked kitchen with breakfast items, snacks and beverages
Downtown location with easy access to BART, CalTrain, and MUNI and great views of San Francisco and the Salesforce Park
Pre-tax commuter benefitsTeam happy hours and team building events
Dog-friendly office - we love our furry friends
If this sounds interesting, we would love to hear from you! Please include whatever info you believe is relevant: resume, GitHub profile, code samples, links to personal projects, etc."
59,"Data Engineer, Payments ML","Amazon.com Services, Inc.",,"Seattle, WA","Bachelor’s degree in math, computer science, engineering, finance, statistics, or a related technical field5+ years of professional experience and passion for working with large data sets plus deep experience in statistical analysis, advanced modeling techniques, data mining and business analysisAgile project management experience and ability to drive successful end-to-end project executionAbility to thrive in an environment that is tasked with providing data-driven decision support and business intelligence that is timely, accurate and actionableHands on experience with AWS data products including EMR, S3, Redshift as well as SQL, Excel, data mining, data visualization software

Are you ready to own all the data infrastructure and pipelines for a machine learning product? Amazon Payment Products is a growing business with an established ML team. We are looking for a technical leader to join our team of machine learning scientists, software developers, research scientists. This technical leader will lead our data engineering team and own all data infrastructure for our machine learning, launching new capabilities and experiences for Amazon customers.

In this role, you will work closely with machine learning scientists, product managers and SDEs on launch decisions. You will drive high value customer actions and influence senior management decisions and metrics. You will provide proactive, deep insights into metrics driving the business on a regular cadence. The right candidate will be passionate about working in with large datasets and emerging businesses.

Over time, you will lead the development of statistical and other machine learning models. You will work closely with our engineering teams to influence the product roadmap and implement solutions designed to improve operations and controls reporting.

Outstanding leadership, data engineering expertise and a keen interest in machine learning are required for this role. Strong interpersonal and communication skills are key.

Ability to think big, understand business strategy, provide consultative business analysis, and leverage technical skills to create insightful, effective BI solutionsHands on experience with data extraction, manipulation, statistical analysis and predictive modeling.Experience in machine learning (decision trees, multivariate and logistic regression, etc.)Proven track record of strong verbal/written communication & data presentation skills, including an ability to effectively communicate with both business and technical teams
Amazon is an equal opportunity employer."
60,Data Engineer,Recorded Future,,"Somerville, MA","This Role: The Data Science team takes ownership of this unique dataset. We manipulate our data structures to obtain solutions and insights, explore new use cases and capabilities for the data, and empower our clients to take proactive action against potential threats. We're looking for smart and ambitious people with a strong engineering foundation and a quantitative mindset to help us take this on. In this role, you will build frameworks for transforming vast amounts of data into the key pieces of high-quality information our clients rely on. You will join a team of motivated people working on a tough problem, and help us shine a light into the dark corners of the internet.

What you'll bring:

Technical education: BA, BS, MS, or PhD in Computer Science or a related discipline, with a strong academic record.
Data skills: Comfort working with complex data structures.
Mindset: You love working with heterogeneous data to answer real human questions, and you know how to write the code that makes that happen at scale.
Programming: 1-3 years work experience or equivalent academic background in data processing using Python. You are comfortable with developing scalable production code.
Excellent communication: Your clarity of thought is always apparent in your crisp and articulate emails, Slack chats, phone calls, and in-person conversations.
Preference for prior experience working with Dask, Spark, or MapReduce.
Bonus if you have experience or interest in Information Security, Cybersecurity, or Threat Intelligence.

Why should you join Recorded Future?
There's a reason why over 90% of Fortune 100 Companies rely on us for their threat intelligence needs: our patented web intelligence engine has the ability to unlock insights that radically improve cyber threat visibility for our clients. Our dedication to empowering clients with intelligence to reduce risk has earned us a 4.7-star user rating from Gartner.

If you're full of passion, ambition, and dedication you may be well on your way to becoming a Futurist. From over 35 nationalities, our Futurists are the perfect recipe of humility, accountability, and collaborative attitudes to put our team at the front line of securing the internet. If you want to be a part of this awesome team, apply today!

Want more info? Check out the links below for more from the Recorded Future team, special guests, and our partners.
Blog & Podcast ( https://www.recordedfuture.com/blog/ ): Learn everything you want to know (and maybe some things you'd rather not know) about the world of cyber threat intelligence
Instagram ( https://www.instagram.com/recordedfuture/ ) &Twitter ( https://twitter.com/RecordedFuture ): What's happening at Recorded Future
Timeline ( https://www.recordedfuture.com/about/#post-9424 ): History of Recorded Future

Recorded Future will not discharge, discipline or in any other manner discriminate against any employee or applicant for employment because such employee or applicant has inquired about, discussed, or disclosed the compensation of the employee or applicant or another employee or applicant.

Recorded Future is an equal opportunity employer and we encourage candidates from all backgrounds to apply."
61,Data Engineer (All Levels),Convoy,,"Seattle, WA","About Convoy:

Convoy is transforming the $800 billion trucking industry. Our mission is to transport the world with endless capacity and zero waste. The industry is huge and so is the opportunity to fundamentally changing the way freight moves across America and beyond for the better.

We are passionate about thinking big and solving really complex problems to make the lives of truck drivers, shippers and other people in freight industry easier through our innovation and technology. There will always be a better, more efficient way to transport goods, and we won’t ever stop inventing it.

Founded in 2015, we’re a mission-driven, well-funded, fast-growing startup, backed by world-class investors, including CapitalG, the growth equity investment fund of Google, and leading industry disruptors, including the founders and CEOs of Amazon, Salesforce, eBay, Linkedin, Expedia, Dropbox, KKR, Starbucks, and others. We were named one of Washington State’s top places to work, a LinkedIn Top Startup and were the recipient of the GeekWire Next Text Titan in 2018.

Who we’re looking for:

Data and our data infrastructure are core to Convoy’s mission to automate the logistics industry. Today, we use machine learning to figure out freight prices, shipment relevance for carriers, auction bidding strategy, and automate other internal processes. However, our models are only as good as the data on which they are built. Convoy’s Data Science team is looking for a talented Data Engineer with a strong background in ETL, data warehousing, ML tooling, and interest in collaborating with Data Scientists to evolve the data foundation for our business.
In this role you will own the following:
Architecture design and implementation of next generation data platform and model deployment solutions.
Building robust and scalable data integration (ETL) pipelines using SQL, EMR, Python, and Spark.
Mentoring and developing other Data Engineers and Data Analysts.
Building and delivering high quality data architecture to support data analysts, data scientists, and customer reporting needs.
Interfacing with other technology teams to extract, transform, and load data from a wide variety of data sources.
Continually improving reporting and analysis processes, automating or simplifying self-service support for customers
You have the following skills & experience:
Bachelor's degree or higher in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, or Engineering.
5+ years of relevant experience in one of the following areas: data engineering, database engineering, business intelligence or business analytics.
5+ years of hands-on experience in writing SQL queries across data sets.
2+ years of experience in scripting languages like Python.
Experience with data modeling, ETL development, and Data warehousing.
Experience with Snowflake, Redshift, or similar.
Experience with AWS services such as ECS, S3, and RDS.
Experience with Big Data Technologies (e.g. Spark, Hadoop, Hive, etc.)
You have experience with working and delivering end-to-end projects independently.
Knowledge of distributed systems as it pertains to data storage and computing.
Benefits of working at Convoy:
Health &Wellness: Taking care of your health comes first. Convoy covers 100% of your medical, dental, and vision premiums, and 85% for your family. We also provide access to the gym and showers in our building.
In-Office Happiness: Enjoy catered meals multiple times through the week, and fully stocked fridges and pantries to keep you running full.
Commuter Benefits: We transport the world; the least we can do is help get you to work. Every employee receives a fully paid Orca Card to get you around on the bus, light rail, or water taxi. We also provide locked bike storage if you prefer to pedal in.
Time Off: Life outside of work matters! Convoy has a flexible vacation policy, so take the time off you need- we don’t track vacation or sick time. We also offer generous parental leave for those expecting new additions to their family.
Compensation: We offer market competitive compensation, including stock options and a 401k plan to help save for retirement.
Candidates must be eligible to work at Convoy HQ in Seattle.

Convoy is an equal-opportunity employer and we welcome applicants from all backgrounds. If you’re a passionate team player who wants to have an outsized impact on a diverse and dynamic team, we’d love to hear from you!"
62,Data Engineer,Discover Financial Services,,"Riverwoods, IL","At Discover, be part of a culture where diversity, teamwork and collaboration reign. Join a company that is just as employee-focused as it is on its customers and is consistently awarded for both. We’re all about people, and our employees are why Discover is a great place to work. Be the reason we help millions of consumers build a brighter financial future and achieve yours along the way with a rewarding career.

The role will focus on Discover’s Application Portfolio and analysis, update and additions needed to support the Application Portfolio Management Strategy.

Responsibilities:

Provides senior-level technical consulting to peer-data engineers during design and development for highly complex and critical data projects.
Provides engineering leadership to create and enhance data solutions enabling seamless integration and flow of data across the data ecosystem.
Designs and develops data ingestion frameworks leveraging Open-Source tools such as NiFi, Sqoop, Hive, Java, Pig, Python, as well as data processing/transformation frameworks leveraging Open-Source tools.
Provides support for deployed data applications and analytical models.
Designs and develops real-time processing solutions using Open-Source tools.
Responsible for using knowledge to apply a hands-on approach with next generation technologies to contribute to the team that delivers the latest data-driven platforms & next generation analytic technologies.
Minimum Qualifications

At a minimum, here’s what we need from you:

Bachelor’s Degree in Computer Science or related field
6+ years of experience in Data Platform Administration/Engineering
Preferred Qualifications

If we had our say, we’d also look for:

8+ years of experience in Data Platform Administration/Engineering
Discover Financial Services is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, among other things, or as a qualified individual with a disability.
Work Perks:

24-Hour Nurse Hotline & Telehealth Services
7 Paid Holidays
Adoption Assistance
Annual Flu Shots
Commuter Benefits
Employee Assistance Program
Flexible Work Environment
Group Auto, Home and Pet Insurance
Healthy Eating Program
Legal Assistance Plan
Mother’s Rooms
Onsite Emotional Health Counselors
Onsite Fitness Centers
Onsite Weight Watchers at Work
Paid Parental Leave
Professional and Leadership Development Programs
Recognition Program
Service Anniversary Awards
Tuition Reimbursement


Health:

Annual Health Evaluation and Health Coaching
Critical Illness Insurance
Health Savings Account, Health Reimbursement Account and Flexible Spending Accounts
Health, Vision and Dental Insurance
Life and Accident Insurance
Long-term and Short-term Disability Insurance
Onsite Health Services Center with Nurse Practitioner

Financial Wellness:

401(k) Savings Plan with Fixed and Matching Contributions
Employee Stock Purchase Plan
Financial Engines
“Financial Wellness for You” Learning Programs"
63,Data Engineer,2K Games,,"Novato, CA","Who We Are:

2K publishes some of the most popular video game franchises on the planet including Mafia, Borderlands, BioShock, NBA 2K, WWE 2K, Evolve, XCOM, and Sid Meier’s Civilization. The analytics group is responsible for collecting, processing and utilizing the data in the right way to identify problems and opportunities across the company and to build solutions for them. As part of this, the analytics team works with the studios and other partners to build reliable, scalable, and high-performance data pipelines to help inform all aspects of our business. From data designed to improve our development processes to data designed to drive critical business decisions, our group is constantly facing fun and challenging problems in the big data space.


What We Need:

We are looking for a Data Engineer to be part of our growing data engineering team within our analytics group. Collaborate with cross-functional teams, studios, external data providers to architect, design and develop a metadata driven data pipeline framework focusing on reusability, scalability, and productivity. S/he will work with data analysts and data scientists to understand data requirements, design and develop data pipelines to ingest data from multiple disparate sources. It’s a once in a lifetime opportunity to be part of a great team chartered to define the future of data and analytics platform for 2k. This position will be reporting directly to the Director of Data Engineering, located at our Novato office.


What You Will Do:
Actively contribute to the architecture and design of data platform and data engineering practice
Design and develop a data pipeline framework for ingesting structured and unstructured data
Work with the game, marketing, and analytics teams to gather requirements, build, test, and deploy new data pipelines based on business requirements
Translate data and BI requirements into technical design documents and data mapping documents
Mentor junior engineers and be part of their career growth


Who We Think Will Be a Great Fit:
If you have experience building large scale data solutions, are interested in taking up another challenge to shape the future of data and platform capabilities in a fast-paced environment, we think you will be a great fit and we’d love to hear from you!

5+ Years of experience in developing near real-time data pipelines
Strong hands-on experience with object-oriented/object function scripting languages: Python[Preferred], Java, Scala, etc.
Expert knowledge in Data warehouse concepts and implementation of Dimensional and star models
Experience with data pipeline and workflow management tools: Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, Kinesis, RDS
Prior Implementation Experience with stream-processing systems: Storm, Kafka, Spark-Streaming etc.
Proficient using Source Control, build and deploy tools like perforce/Git and Jenkins
Strong project management, organizational and interpersonal skills"
64,Data Engineer Senior,CopperPoint Mutual Insurance Company,,"Phoenix, AZ","Overview
Founded in 1925, CopperPoint Insurance Companies is a leading provider of workers’ compensation and commercial insurance solutions. With an expanded Line of insurance products and a growing six-state footprint in the southwestern United States, CopperPoint embodies stability for policyholders in Arizona, California, Colorado, Nevada, New Mexico and Utah. CopperPoint Mutual Insurance Holding Company is the corporate parent of Arizona-based CopperPoint Insurance Companies, California-based Pacific Compensation Insurance Company and other CopperPoint Insurance Entities.

CopperPoint has an exciting opportunity for a Data Engineer Senior position located in Phoenix, Arizona. The Data Engineer Senior will have deep knowledge of enterprise data design, data modeling and data warehouse applications. They will develop and deliver data solutions to key stakeholders providing thought leadership and guidance to help ensure the solutions meet the stakeholders’ needs.
Responsibilities
Job Responsibilities:
Lead, analyze, design, develop, test and implement data warehouse and business intelligence solutions
Design highly scalable ETL processes with complex data transformations
Design and deliver custom reporting, dashboards, and data extracts
Gather and document business requirements and translate into technical architecture/design
Ability to understand and document data flows in and between different systems and map data from a data source to target tables in a data warehouse
Qualifications
Qualifications/Competencies:
Minimum of 5 years of experience in data and business intelligence solutions
3-5 years of experience in developing ETL and data warehouse solutions
Experience designing and developing enterprise data solutions
Expertise in MS-SQL Server and SSIS
Excellent analytical, problem solving and organizational skills including the ability to prioritize.
Excellent written and verbal communication skills.

CopperPoint’s culture of compassion extends to the community through employee volunteerism, corporate matching, Board service, program sponsorships and in-kind contributions. We empower employees by providing 12-hours of paid volunteer time annually and matching their personal contribution to the charities of their choice up to $500 per year. In 2018, CopperPoint employees reported 3,500 volunteer hours.

CopperPoint offers a competitive compensation package and comprehensive benefits package including major medical, dental, vision and a wide range of competitive benefits programs, generous matching contributions to your 401(k) plan, generous paid time off, tuition reimbursement and other education benefits and business casual dress. CopperPoint is an equal employment opportunity employer. All qualified applicants will receive consideration without regard to race, color, sex, religion, age, national origin, disability, veteran status, sexual orientation, gender identity or expression, marital status, ancestry or citizenship status, genetic information, pregnancy status or any other characteristic protected by state, federal or local law. CopperPoint maintains a drug-free workplace."
65,Data Engineer,The New York Times,,"New York, NY 10036","The New York Times is seeking inventive and motivated data engineers at all levels of experience to join the Data Engineering group. In this role, you will build critical data infrastructure that surfaces data and insights across the company.
About Us
Our Data Engineering teams are at the intersection of business analytics, data warehousing, and software engineering. As Maxime Beauchemin wrote in “The Rise of Data Engineering”, ETL and data modeling have evolved, and the changes are about distributed systems, stream processing, and computation at scale. They’re about working with data using the same practices that guide software engineering at large. A strong data foundation is essential for The New York Times and we’re responsible for it. We use our data infrastructure to power analytics and data products and to deliver relevant experiences to our customers in real-time. We enable our company to validate strategic decisions, make smarter choices, and react to the fast changing world. We are part of a New York based technology organization with a remote-friendly workplace that includes engineers around the world. We value transparency and openness, learning, community, and continuous improvement. Check out the Times Open blog, which is written by engineers and other technical team members, and follow @nytdevs on Twitter to see what we’re up to.
About the Job
We focus on the software engineering related to data replication, storage, centralized computation, and data API’s. We provide customers and partners with data tools, shared frameworks, and data services. These are the foundational core of our group which enables ourselves and others to work with data from a common underpinning. Our tools and services enable our group to scale and avoid blocking others. We reduce data redundancy by creating systems and datasets that serve as sources of record. We enable discovery and governance of our data. We support key business goals like growing our digital subscriber base, understanding how our customers use our products, and retaining our print subscribers.
As a data engineer, you will:
Run and support a production enterprise data platform
Design and develop data models
Work with languages like Java, Python, Go, Bash, and SQL
Build batch and streaming data pipelines with tools such as Spark, Airflow, and cloud-based data services like Google’s BigQuery, Dataproc, and Pub/Sub
Develop processes for automating, testing, and deploying your work
About You
To thrive in this role, you are excited about data and motivated to learn new technologies. You are comfortable collaborating with engineers from other teams, product owners, business teams, and data analysts and data scientists. You are own and shape your technical domain area and move the related business goals forward. You are eager to resolve upstream data issues at the source instead of applying workarounds. You analyze and test changes to our data architectures and processes, and determine what the possible downstream effects and potential impacts to data consumers will be.
Benefits and Perks:
Make an impact by supporting our original, independent and deeply reported journalism.
We provide competitive health, dental, vision and life insurance for employees and their families
We support responsible retirement planning with a generous 401(k) company match.
We offer a generous parental-leave policy, which we recently expanded in response to employee feedback. Birth mothers receive 16 weeks fully paid; non gestational parents receive 10 weeks, also fully paid.
We are committed to career development, supported by a formal mentoring program and $8,000 annual tuition reimbursement.
We have frequent panel discussions and talks by a wide variety of news makers and industry leaders.
Join a community committed to the richness of diversity, experiences and talents in the world we cover, supported by a variety of employee resource groups.
#LI-AM1
The New York Times is committed to a diverse and inclusive workforce, one that reflects the varied global community we serve. Our journalism and the products we build in the service of that journalism greatly benefit from a range of perspectives, which can only come from diversity of all types, across our ranks, at all levels of the organization. Achieving true diversity and inclusion is the right thing to do. It is also the smart thing for our business. So we strongly encourage women, veterans, people with disabilities, people of color and gender nonconforming candidates to apply.
The New York Times Company is an Equal Opportunity Employer and does not discriminate on the basis of an individual's sex, age, race, color, creed, national origin, alienage, religion, marital status, pregnancy, sexual orientation or affectional preference, gender identity and expression, disability, genetic trait or predisposition, carrier status, citizenship, veteran or military status and other personal characteristics protected by law. All applications will receive consideration for employment without regard to legally protected characteristics."
66,Data Engineer,Frontapp,,"San Francisco, CA","Email is the universal communication tool for work. It’s where you discuss work, answer questions, and talk to all of your customers, vendors, and partners. But email wasn’t made for business and hasn’t evolved to help you work with a team. So you’re dropping the ball, missing important context, and relying on many siloed apps that make you less productive as a whole.

With more than 5,000 customers and $79 million in funding from Sequoia, Threshold (formerly DFJ), and others, Front is reinventing the inbox so people can accomplish more together. We’ve created one place where you communicate internally and externally, gain context about customers and projects, and access all your other tools so you can be more efficient, more fulfilled, and ultimately happier at work.

As a part of the Data Team at Front, your core responsibility will be to help maintain and scale our infrastructure for analytics as our data volume and needs continue to grow at a rapid pace. This is a high impact role, where you will be driving initiatives affecting teams and decisions across the company and setting standards for all our data stakeholders. You’ll be a great fit if you thrive when given ownership, as you would be the key decision maker in the realm of architecture and implementation.
What will you be doing?
Architect data pipelines that provide fast, optimized, and robust end-to-end solutions for internal users of the analytics infrastructure.
Automate manual processes and create a platform in favor of self-service data consumption.
Own the quality of our analytics data and ensure the Data Team’s SLAs are met on a timely basis.
Design data schemas and fine-tune queries around large, complex data sets.
Interface with data scientists, analysts, product managers and all data stakeholders to understand their needs and promote best data practices.
Keep our data available and secure across multiple data centers and regions.
Implement a robust monitoring & logging framework that guarantees the traceability and auditability.
Vet tools and technologies for the most viable solution for each problem at hand. Manage tools and data vendors involved.
What skills and experience do you need?
BS/BA in Computer Science, Mathematics, or relevant technical field.
At least 2 years of experience as a Data Engineer, or in a role with ETL expertise.
Experience in managing data warehouse plans and communicate them to internal stakeholders.
Strong overall programming skills, able to write modular, maintainable code.
Advanced level of SQL is required.
Working knowledge and experience with at least one of the big data technologies: HDFS, EMR, Redshift, Spark, Flink, or Presto.
Experience with workflow management tools: Airflow, Luigi, etc.
Experience with R or Python is a plus.
You are proactive, have a positive attitude with a “can-do”, service-oriented mentality.
Ability to juggle multiple projects/tasks with multiple stakeholders.
Front provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age or disability."
67,"Data Engineer (Outward, Inc.)","Williams-Sonoma, Inc.",,"San Jose, CA 95112","Location: San Jose, CA

About Outward, Inc.

Outward, Inc. is based in San Jose, CA and is a wholly owned subsidiary of Williams Sonoma, Inc. ( www.outwardinc.com )

At Outward Inc. our vision is to 'lower the friction' with regards to all aspects of the customer journey for our parent company and our retail customers. We do this by offering new technology solutions that enable new experiences and top-notch visualizations of their products. We are continuously pushing the boundaries of how 3D and AR/ VR technologies will drive the next generation shopping experience.

Through our portfolio of premium lifestyle brands - our mission is to deepen consumer connections with the products that matter and deliver an innovative experience.

We are positioned as a technology leader in the visual merchandising space for retail, with a focus on improving customer experiences with next-generation product visualizations.

Come and join a growing team of engineers as we solve technological riddles and push the envelope of what can be done on the web!


Responsibilities

Work with Engineers, Product Owners, and Designers to understand their data needs
Automate frequently requested analyses using Python
Evaluate and define critical business metrics and identify new levers to help move these metrics
Design and evaluate A/B experiments
Monitor key product metrics and identify root causes behind anomalies
Build and analyze dashboards and reports
Influence product teams through a presentation of data-based recommendations
Communicate state of business and experiment results to product teams
Minimum Qualifications

Bachelor's degree or equivalent work experience
2+ years of Python development experience
2+ years of SQL (Hive, Oracle, MySQL, PostgreSQL) experience
Professional experience using XML


Nice-to-have Qualifications

2+ years of experience with data visualization and data-mining
Experience analyzing data to identify deliverables, gaps and inconsistencies
Experience initiating and driving projects to completion with minimal guidance
Experience communicating the results of analyses
Experience with AWS services like lambda, Cloud Formation, RDS, EC2, IAM
Outward's Benefits & Perks:

Medical, Dental, Vision
401K
Paid time off
Company-sponsored team events such as regular staff parties
Fantastic new headquarters
Fully stocked kitchens with catered lunches twice a week
On-site gym & game room
Office dogs! Bring your furry pal with you to work
Flexible working hours
Friendly, caring co-workers and management


This position will not offer relocation assistance or remote work.
Outward, Inc. is an Equal Opportunity Employer.
Outward, Inc. will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the California Fair Employment Act (AB 1008), or other applicable state or local laws and ordinances.

#LI-JQ1 | rev 7.9.19

-"
68,Data Engineer,Bodybuilding.com,,"Boise, ID 83704","Core Values:
CUSTOMER
F - Fearless & Fast …We are not afraid to fail! We embrace risk and drive change with urgency.
I - Inclusive …We honor, respect and encourage diversity and differences! We celebrate wins by collaborating, sharing goals, supporting each other and having fun.
R - Results Driven …We are passionate about customers and business results! We achieve results through talent nurturing, intentional goal setting, data-driven decisions and execution.
S - Serve Communities …We give back to the communities to which we owe our success! Our customers, colleagues, partners, family, friends and neighbors.
T - Trust & Accountability …We empower and hold each other accountable to deliver on commitments, and trust each other’s positive intent.
Our Mission:
To help people change their life and become their best self!
Summary-
The Analytics data team is responsible for designing, developing, and supporting our Data Warehouse. We supply information to the organization in the form of data models, reports, and data analysis and in turn supply integrated data and data science algorithms for engineering groups.
We are looking for a Data Engineer, Data Warehouse developer, or DBA to join our team.
The ideal candidate will be an independent thinker who is technically astute with hands on experience with data warehousing on various platforms.
Responsibilities
Maintain and improve applications, data extracts, and data integration processes
Maintain and develop data warehouse infrastructure, data models, and applications
Research and troubleshoot data questions
Collaborate on problem resolution and systemic optimizations
Participate in on-call rotation
Requirements
Bachelor’s degree in computer science, engineering, business or equivalent experience
2+ years business intelligence development experience with SSIS, and SSMS
2+ years experience with SQL query writing
2+ years business intelligence development experience
2+ years of Software Development in various languages (java, javascript, python, C#)
Skills
Excellent communication and organizational skills
Skilled with Structured Query Language (SQL)
Understand DW principles, Kimball methodology, and ETL
Skilled with concepts for data acquisition, movement, and persistence
Able to work and communicate technically with DBA’s and Software Engineers
Experience with cloud services (GCP, AWS, Azure)
Knowledge of GCP, BigQuery a plus
Bodybuilding.com offers its employees several benefits such as: health, dental and vision insurance; 401(k); Competitive bonuses; Gym Membership Reimbursement; Employee Discount

Bodybuilding.com is an Equal Opportunity Employer. The above information has been designed to indicate the general nature and level of work performed by employees in this classification. It is not designed to contain or to be interpreted as a comprehensive inventory of all duties, responsibilities, and qualifications required of the employee assigned to this job."
69,DATA ENGINEER,Change Healthcare,,"Emeryville, CA","Transforming the future of healthcare isn’t something we take lightly. It takes teams of the best and the brightest, working together to make an impact.
As one of the largest healthcare technology companies in the U.S., we are a catalyst to accelerate the journey toward improved lives and healthier communities.
Here at Change Healthcare, we’re using our influence to drive positive changes across the industry, and we want motivated and passionate people like you to help us continue to bring new and innovative ideas to life.


If you’re ready to embrace your passion and do what you love with a company that’s committed to supporting your future, then you belong at Change Healthcare.
Pursue purpose. Champion innovation. Earn trust. Be agile. Include all.
Empower Your Future. Make a Difference.
The new Data Science group was formed to dramatically increase leverage of Change Healthcare data assets to create material new revenue opportunities, both within specific Business areas and also across multiple lines of business. With data and transactions from more than 1,200 payers and 340,000 providers, Change Healthcare is uniquely positioned to impact US Healthcare.
Responsibilities:
Interface with data scientists, engineers and product managers to understand data needs. Build data expertise and own data quality for allocated areas of ownership. Design, build and launch new data models. Design, build and launch new data extraction, transformation and loading processes. Building key data sets to empower operational and exploratory analysis. Define and manage SLA for all data sets in allocated areas of ownership. Work with data infrastructure to triage infra issues and drive to resolution.
Minimum Requirements
A Bachelor's degree in Information Technology, Computer Science, Mathematics, or related technical field of study and three years of experience in the job offered or three years as a Big Data Engineer, Data Pipeline Engineer, or closely related position. Applicants must have three years of experience with: Data warehouse space; Custom ETL design, implementation and maintenance; Working with SQL and writing SQL statements; Programming languages, including Python, SQL, and any one of the following: Spark, Scala, or Pyspark; Schema design and data modeling; Analyzing data to identify deliverables, gaps and inconsistencies; Identifying and communicating data driven insights such as anomalies, trends, outliers, counts, and frequency tables; Spark and Spark SQL; and Large datasets, 100GB or more.
Join our team today where we are creating a better coordinated, increasingly collaborative, and more efficient healthcare system!
Equal Opportunity/Affirmative Action Commitment

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status."
70,Data Engineer : 19-03787,Akraya Inc.,,"Sunnyvale, CA 94086","Primary Skills: Teradata, Teradata Utilties, Big Data, ETL, SQL
Duration: 06 Months (with possible extension)
Contract Type: W2 Only

Technical Requirements :
Knowledge/experience on Teradata Physical Design and Implementation, Teradata SQL Performance Optimization.
Experience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport)
Advanced SQL (preferably Teradata).
Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.).
Strong Hadoop scripting skills to process petabytes of data.
Experience in Unix/Linux shell scripting or similar programming/scripting knowledge
Experience in ETL/ processes.
Real time data ingestion (Kafka).
 To follow up with any questions, please contact Khushbu at 408-512-2361

Akraya is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US. We offer comprehensive benefits including Health Insurance (medical, dental, and vision), Cafeteria Plan (HSA, FSA, and dependent care), 401(k) (enrollment subject to eligibility), and Sick Pay (varies based on city and state laws).
If this position is not quite what you're looking for, visit akraya.com and submit a copy of your resume. We will get to work finding you a job that is a better fit at one of our many amazing clients.

Akraya is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. Akraya is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation."
71,Hadoop Data Engineer,Zensar Technologies,,"San Jose, CA 95131","Hadoop Data Engineer - (0041675)
Description

Zensar is a leading digital solutions and technology services company that specializes in partnering with global organizations across industries on their Digital Transformation journey.

Zensar’s comprehensive range of digital and technology services and solutions enables its customers to achieve new thresholds of business performance. Zensar helps clients deliberate not only on executing Digital initiatives but on realizing the Return on Digital®.

For enterprises to be resilient and successful in the long run, they will need to focus on three aspects - Digital Agility, Cross-over IT and fundamentally the Stability of Core Enterprise Systems.

We practice Digital internally as we promote externally – Zensar runs on Digital. Through Engaging, Operating, Partnering, and Managing Digital, we believe in Living Digital.

Qualifications:
7+ years of experience
Strong experience in Hadoop and related technologies
Proven experience in Hive and Spark
Practical experience using Python
Experienced in SQL and writing complex queries
Experience of working on large data sets, data analysis & validation
Very good communication skills
Ability to work with cross functional teams
Strong analytical skills


Primary Location: United States of America-California-San Jose
Job Posting: Aug 13, 2019, 12:58:41 PM
 6 To 9"
72,Big Data Engineer - Bachelor's (Co-Op) - United States,Cisco Careers,,"San Jose, CA","What You’ll Do Design and deliver automated transformation of large data sets influencing MapReduce, streaming, and other new technologies Use HBase, Elasticsearch, etc. to ingest transformed data at scale Collaborate with security experts to deliver high-impact web-based APIs Implement high-volume data integration solutions Analyze, monitor, and optimize for performance Produce and maintain high-quality user documentation
Who You'll Work With
Join us as we transform the world of tomorrow. Develop creative ideas on how to work better and smarter. Influence and participate in top-priority projects that have a real impact.
Who You Are Currently enrolled in an accredited university co-op program pursuing a Bachelor’s in Computer Science, Computer Engineering, Electrical Engineering, or a related major such as Math, Physics Minimum of a 3.0 GPA or equivalent Track record of developing technology to enable large scale data transformation Strong Java experience and hands-on Hadoop ecosystem experience – HBase, Hive, Spark, etc. Possess knowledge of software engineering standard methodologies Real passion for solving hard problems and exploring new technologies Excellent communication and user documentation skills
Why Cisco
#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!"
73,Data Engineer,ResMed,,"San Diego, CA 92123","Data Engineer
At ResMed (NYSE: RMD, ASX: RMD) we pioneer innovative solutions that treat and keep people out of the hospital, empowering them to live healthier, higher-quality lives. Our cloud-connected medical devices transform care for people with sleep apnea, COPD and other chronic diseases. Our comprehensive out-of-hospital software platforms support the professionals and caregivers who help people stay healthy in the home or care setting of their choice. By enabling better care, we improve quality of life, reduce the impact of chronic disease and lower costs for consumers and healthcare systems in more than 120 countries. To learn more, visit ResMed.com and follow @ResMed.

Let's talk about the team and you:
We are passionate and innovative problem solvers that support business units across the globe, providing on-going opportunities to engage with new and challenging projects. You are passionate about data and using it in meaningful ways. You have a deep understanding of a wide-variety of big data tools and technologies which you can implement to achieve desired business outcomes. You are a team player who lifts the entire team through collaboration, mentoring and sharing your experience.

As a Data Engineer within ResMed’s Advanced Analytics team, you are responsible for transforming and analyzing data by expanding and optimizing our data and data pipeline in AWS architecture. The Data Engineer will support our architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Reporting to the Manager of Data Engineering, you will play a key role of developing, constructing, testing and maintaining data on the AWS platform.

Let's talk about Responsibilities:

Assemble large, complex data that meet functional/non-functional business requirements.

Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing transformation for greater scalability, etc.

Use the infrastructure/services required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS services.

Create data for analytics and data scientist team members that assist them in building and optimizing our product.

Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data requirement needs.

Create and maintain optimal data pipeline architecture in AWS.
Keep our data separated and secure across boundaries through multiple AWS regions.

Let's talk about Qualifications and Experience:

Bachelor/Engineering degree in IT / Computer Science/ software engineering or relevant field

3+ years of relevant experience in a complex, technical environment

Must have AWS Certified Solutions Architect - Associate/Professional level

Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets

Excellent working knowledge on Linux

Experience working with Data Scientist

Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.

Must have experience working on Spark/Scala, Kafka, Elasticsearch and Python (at least two)

Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.

Strong analytic skills related to working with unstructured datasets.

Experience in manipulating, processing and extracting value from large disconnected datasets.

Experience with AWS cloud services: (Redshift, RDS, EMR, Kinesis, S3, Glue, DMS-Batch/CDC, Athena, EC2, Lambda, SQS, SNS etc.)

Experience working on AWS Data Lakes

Experience working on AWS Data Pipeline and CI/CD processes

Exposure to Hadoop Ecosystem preferably on AWS/EMR, NoSQL-based, SQL-like technologies

Experience with Data Science tools & technologies on AWS Cloud is plus

Experience supporting and working with cross-functional teams in a dynamic environment.

Excellent communication and able to work with stakeholders

 All listed duties, requirements and responsibilities are deemed as essential functions to this position; however, business conditions may require reasonable accommodations for additional task and responsibilities.
#LI-NF1
Okay, so what’s next?
Joining ResMed is more than saying “yes” to making the world a healthier place. It’s discovering a career that’s challenging, supportive and inspiring. Where a culture driven by excellence helps you not only meet your goals, but also create new ones. We focus on creating a diverse and inclusive culture, encouraging individual expression in the workplace and thrive on the innovative ideas this generates. Our hope is that each day you’ll uncover a new reason to love what you do. If this sounds like the workplace for you, apply now!
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions."
74,Data Engineer,Scoop Technologies,,"San Francisco, CA","About Scoop

Scoop brings co-workers and neighbors together to enjoy a smooth carpooling experience—unlocking new opportunities to create friendships, improve their well-being, and make the most of their valuable time.

Learn more in Crunchbase: https://news.crunchbase.com/news/scoop-raises-60m-for-corporate-carpooling-as-gridlock-ruins-america/

Engineering @ Scoop

Few companies get to face such diverse technical challenges as Scoop, and we’ve built a team of people excited to face these challenges together while investing in each others’ growth.

Scoop’s engineering team may move bits and pixels, but we also put real, live human beings in cars together. We’re touching problems academics have written about for years, and have data that no other company has ever collected.

But Scoop knows engineering is not a lone discipline. We’re a small team with varied backgrounds: big companies, VC-backed startups, bootcamps, academia. We like to build together, and we like to learn together. Our entire team and process are built around helping you grow and be successful. And we’d love to tell you more about the impact you could have at Scoop.
In this role, you will:
Architect, develop, and deploy infrastructure for large scale ETL pipelines with data processing frameworks
Productionizing machine learning - from research into fault-tolerant, production-scale deployments
Manage data workflows
Work closely with the Data Science, Analytics, Product and Platform teams to understand business needs and create a data platform that serves business needs
You should:
5+ years or equivalent experience
Be proficient in Python and/or Scala (Scala preferred)
Experience designing and operating distributed systems
Emphasis in high code quality, high code clarity, automated testing, and engineering best practices
Be able to clearly communicate complex technical concepts

Preferred Qualifications:
Mobility/Transportation domain knowledge
Worked with open sourced projects like Spark, Kafka or Airflow
Life @ Scoop

Founded in 2015 and based in downtown SF, our team mixes technology and elbow grease every day, with one statistic in our crosshairs: 80% of Americans drive alone to work. At Scoop, we envision a world where commuters feel empowered — starting with a choice to make their commute a meaningful part of their day. We embody that same spirit within our own culture, empowering every team member to make this the most meaningful experience of their career.

Walk into Scoop and you’ll find a furry, tail-wagging welcoming committee. In many ways these fluffy faces exemplify the energy that flows through our office. They are a reminder that while we’re focused and driven, we shouldn’t take ourselves too seriously. They also help bridge the gap between our homes and our workplace, just like a Scoop carpool.

The atmosphere overall is dynamic and unique. It’s influenced by our backgrounds at successful startups, big tech companies, and premier consulting firms — blended and crafted into what feels natural and right for this company. It plays out in our balance of scrappy and strategic, frameworks and fast thinking.

At Scoop, we’re all united by our desire to change the way people get to work — and committed to enjoying the journey together along the way."
75,Data Engineer - Sustainability,"Amazon.com Services, Inc.",,"Seattle, WA","This position requires a Bachelor's Degree in Computer Science or a related technical field, and 3+ years of relevant employment experience.2+ years’ work experience in using SQL and large databases in a business environment.Experience operating very large Data Warehouses and tools such as Oracle, Redshift, Hadoop, Greenplum etc.Expertise in Data Visualization.

At Amazon, we're working to be the most customer-centric company on earth and to grow in a safe environment for both our Customers and our associates. To get there, we need exceptionally talented, bright, dynamic and driven people. If you'd like to help us build the place to find and buy anything online, this is your chance to make history.

As a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. You will play a thought leadership role in our team – the team will look to you for advice on analytical and business issues facing them. You will work efficiently and routinely to deliver the right things. You will have a company-wide view of the analytical solutions that you build, and you will consistently think in terms of automating or expanding the results company-wide. This high impact role will have an opportunity to lead a team to help design and build our data infrastructure from the ground up.

This role requires an individual with excellent statistical and analytical abilities, deep knowledge of business intelligence solutions and data engineering practices as well as outstanding business acumen and ability to work with various teams across Amazon. The successful candidate will be a self-starter comfortable with ambiguity, with strong attention to detail, an ability to work in a fast-paced and ever-changing environment, and driven by a desire to innovate in this space.

Responsibilities:
Conduct deep dive investigations into business problems and identify potential opportunitiesSupport and challenge your team to meet critical milestonesLead the design, implement and support a scalable and reliable infrastructure for Data Warehousing, BI and Analysis to drive key business decisionsDevelop reports and dashboards using various technologies (i.e. RUBY, HTML, PHP, Python, Java, and AJAX) to allow internal teams (technology, product management, operations teams) to understand issues and identify actions to be taken to solve these issuesOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.Conduct deep dive analyses of business problems and formulate conclusions and recommendations to be presented to senior leadership.Identify ways to automate analysis through smarter software systemsBuilding strong relationships and working across with different teams.

Have ability to consistently deliver results with quick turnaround. Strong spirit of innovation, self-starter, independent, and with ability to come up with solutions to meet business problems.A passion for technology. We are looking for someone who is keen to leverage their existing skills while trying new approaches.Solid communication skills and a team player.Knowledge in using OLAP technologies and BI Analytics.Linux skills will be an advantage.Experience with Java and Map Reduce frameworks such as Hive/Hadoop.Master’s degree in Computer Science, Engineering, Math, Finance, Statistics or related discipline.AWS experience.Understanding of streaming data.
Amazon.com is an Equal Opportunity Employer – Minority /
Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age."
76,Data Engineer,The Walt Disney Studios,,"Glendale, CA","The Walt Disney Studios is comprised of large team of creative professionals who produce and acquire live-action and animated motion pictures, direct-to-video content, musical recordings and live stage plays that tell memorable, life changing stories. Few media and entertainment companies can rival the brands we have the honor of bringing to the world.

As part of the Studio’s Technology team you’ll be joining a group of passionate, dedicated technologists solving a range of interesting problems in innovative ways in an exciting and dynamic industry.

Responsibilities :
You will be instrumental in building scalable, extensible, maintainable data ingest, transformation, and reporting tools, infrastructure, and applications. We store, analyze, visualize and report on data for everything from the financial engines that keep the Studio running to consumer applications that touch millions of people.

You will:
Design, build and deliver high-quality data and software tools and solutions
Collaborate with other software engineers and cross-functional teams
Contribute new ideas to a larger community of high-caliber professionals
Balance resources, requirements, and complexity


Basic Qualifications :
The Walt Disney Studios seeks a Data Engineer to join the Studio Technology Data Services team. This team is a highly visible department that plays an integral part in key business decisions across the Studio organization and the company.


We’re seeking the following in a candidate:
An educational background that has enabled your trajectory in technology. Surprise us!
2+ years of professional software experience
Expertise in data analysis and database engineering
Experience in agile software development process
Smart, motivated, and curious about both the technology and the business
Able to demonstrate working knowledge of some:
Java/JVM, Python, SQL, R, Scala
Apache Spark & associated tooling
Data science notebooks (Databricks, Jupyter)
Git, Github, Gitlab AWS, Terraform
Docker, Kubernetes, Helm
Job Schedulers (Azkaban/Airflow)
Test-driven development


Preferred Qualifications:
It would be cool if you knew and could teach people about:
R, Scala
Kafka, KStreams, KSQL
GraphQL
RDS, DynamoDB"
77,Data Engineer,Uplift,,"Menlo Park, CA","Uplift was founded by a passionate team of travel industry veterans to provide a better way to shop, book, pay and experience travel. Our mission is to make travel more accessible, affordable and rewarding for everyone. By offering simple, flexible payments, Uplift benefits consumers who are able to book now and pay over time. Uplift is a well financed Series C startup serving the $1.4 Trillion consumer travel segment.

Uplift Pay Monthly is used by top travel brands such as the vacations sites of United, American, Southwest, cruise lines such as Norwegian, theme parks such as Universal, and many more. Learn more at www.uplift.com.
Responsibilities
Design, build and maintain data pipelines in cloud infrastructure (AWS)
Develop and maintain real time data pipelines
Build Python libraries, tools, serverless applications and workflows
Internal process improvements such as automating manual processes, building alerting/monitoring bots
Collaborate closely with product teams to build tools, frameworks, reports to run experiments, analyze A/B test results
Work with analysts and data scientists to extract actionable insights from data that shape the direction of the company
Actively engage in design and code reviews - learn from your peers and teach your peers
Lead initiatives to research, analyze and propose new technologies and tooling for our stack
Requirements
2-3 years of related work experience
BA/BS degree in Engineering, CS, or equivalent, Master's degree a plus
Experience with Big Data, ETL and data modeling
Solid coder with Python and Bash
Strong SQL knowledge
Experience with cloud data warehouse (preferably Snowflake or AWS Redshift)
Familiar with BI tools (Tableau or Looker)
Familiar with Linux, AWS, and Docker
Experience in developing and operating high-volume, high-available and scalable environments
Ability to align with rapid business changes: new requirements, evolving goals and strategies and technological advancements
Entrepreneurial, persistent, with the desire to go deep in details
Comfortable working in a startup culture with the ability to earn trust
Benefits
Company will provide either a MacBook or Windows laptop
Free daily catered lunches and fully stocked kitchen
10 company paid holidays and unlimited PTO
Medical and dental insurance, vision reimbursement program
Pre-IPO stock options
401K Plan
Gym Reimbursement
Professional development allowance
Commuter benefits program
Easy commute: Menlo Park - across from Caltrain station
Uplift is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.

Note: Uplift does not accept agency resumes. Please do not forward resumes to any recruiting alias or employee. Uplift is not responsible for any fees related to unsolicited resumes."
78,Data Engineer,Tom Steyer 2020,,"San Francisco, CA","About Tom Steyer 2020

Tom Steyer 2020 aims to run an inclusive presidential campaign. We practice the core progressive values of diversity, equity, and inclusion (DEI) via our hiring practices, and we welcome candidates of diverse backgrounds - including people of color, women, LGBTQi, differently abled, the formerly incarcerated, and those with a non-traditional education to apply. We endeavor to hire a qualified group of campaign staff that truly reflects the American electorate. Tom Steyer believes that a commitment to DEI is essential to American success.

Position Overview:
Tom Steyer 2020 is looking for Data Engineers to join our Technology and Product team. You will build infrastructure to help scale our campaign's use of data, ensuring all departments have access to the data they need to effectively run and optimize their programs.

This is an opportunity to work with large scale data and build solutions that push the cutting edge of how campaigns use data to increase voter participation and engage with activists.

This person will:

Build batch and real-time data pipelines with data processing frameworks like Airflow (Cloud Composer), Beam (Dataflow), and the Google Cloud Platform.
Use best practices in continuous integration and delivery.
Collaborate with other software engineers, ML experts and stakeholders, and data analysts.
Perform ETL that shapes data into easy consumption for dashboards, data analysis, and data models.
Work in multi-functional agile teams to experiment continuously, iterate, and deliver on new product objectives.

Qualifications, Knowledge, and Skills:

4+ years of professional experience in data engineering and/or data architecture fields.
Fluency in SQL and ETL processes.
Strong development experience with Python, R or other scripting languages.
Familiarity with version control software (Git/GitHub) and continuous deployment.
An analytical approach to provide engineering solutions to business needs.
Willingness to work from our campaign HQ offices in San Francisco.

"
79,Data Engineer,Cerner Corporation,,"Kansas City, MO 64116","Cerner Intelligence is a new, innovative organization within Cerner focusing on creating contextual, intelligent experiences by leveraging the power of data to discover new evidence-based insights and workflow interventions that drive client value and help achieve the quadruple aim in healthcare. We seek the best and brightest talent to join the team – individuals who thrive on analytical problem solving and data-driven analysis who are smart, ambitious, and inquisitive.

As a Data Engineer you will be vital to ensuring the data integrity, confidentiality and availability throughout various database solutions and in securing data in transit. They are responsible for ensuring a high level of performance, integrity and security in their domain.

Client Services
Working directly with our clients is one of the most impactful careers you will find. Whether your background is in health care, business or technology you can help transform the quality of care for all of us.
Qualifications
Basic Qualifications:

Bachelors degree in Computer Science, Computer Engineering or Information Systems or related field, or equivalent relevant work experience
3 years of Software engineering work experience
1 year Data mining, quantitative analysis, and/or statistical modeling including predictive performance and algorithm optimization work experience

Expectations:
Must be currently residing in or willing to relocate to the Kansas City metro area
Willing to work additional or irregular hours as needed and allowed by local regulations
Work in accordance with corporate and organizational security policies and procedures, understand personal role in safeguarding corporate and client assets, and take appropriate action to prevent and report any compromises of security within scope of position
Additional Information
Applicants for U.S. based positions with Cerner Corporation must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available for this position.

Some Cerner positions may be obligated to comply with client-facing requirements and occupational health requests, including but not limited to, an immunization set, an annual flu shot, an annual TB screen, an updated background check, and/or an updated drug screen.

Relocation Assistance Available for this Job:
Yes - Domestic/Regional
Virtual Eligible Job
No
Cerner is a place where people are encouraged to innovate with confidence and focus on what is important – people’s health and the care they receive. We are transforming health care by developing tools and technologies that make it more efficient for care providers and patients to navigate the complexity of our health. From single offices to entire countries, Cerner solutions are licensed at more than 25,000 facilities in over 35 countries.

Cerner’s policy is to provide equal opportunity to all people without regard to race, color, religion, national origin, ancestry, marital status, veteran status, age, disability, pregnancy, genetic information, citizenship status, sex, sexual orientation, gender identity or any other legally protected category. Cerner is proud to be a drug-free workplace."
80,Associate Data Engineer (Full Time Starting Summer 2020),EAB,,"Washington, DC 20036","About EAB

At EAB our mission is to make education smarter and our communities stronger. We harness the collective power of more than 1,500 schools, colleges, and universities to uncover and apply proven practices and transformative insights. And since complex problems require multifaceted solutions, we work with each school differently to apply these insights through a customized blend of research, technology, and services. From kindergarten to college and beyond, EAB partners with education leaders, practitioners, and staff to accelerate progress and drive results across three key areas: enrollment management, student success, and institutional operations and strategy.

At EAB, we serve not only our members but each other—that's why we are always working to make sure our employees love their jobs and are invested in their community. See how we've been recognized for this dedication to our employees by checking out our recent awards.

For more information, visit our Careers page.

The Role in Brief

Associate Data Engineer (Full Time Starting Summer 2020)

Are you a data enthusiast who seeks to tease out meaning from complex data flows and assets? Are you a talented problem solver who can transform abstract problems into elegant technical solutions? We are looking for a Data Modeler to join our team of engineers and data analysts focused on designing, creating, and delivering data solutions as part of our state-of-the-art cloud based products. The successful candidate will have the opportunity to build a world-class solution to help our higher education clients solve challenging problems through data.

Opportunities based in Washington, DC and Richmond, VA.

Primary Responsibilities:

Responsible for data modeling and schema design that will range across multiple business domains within higher education
Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas
Work with clients to research and conduct business information flow studies
Codify high-performing SQL for efficient data transformation
Coordinate work with external teams to ensure a smooth development process
Support operations by identifying, researching and resolving performance and production issues
Basic Qualifications:

Experience working with relational or multi-dimensional databases
Experience developing logical data models within a data warehouse
Experience developing ETL processes
Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2
Demonstrated mastery in database concepts and large-scale database implementations and design patterns
Proven ability to work with users to define requirements and business issues
Excellent analytic and troubleshooting skills
Strong written and oral communication skills
Ideal Qualifications:

Bachelor’s or Master’s degree in Computer Science or Computer Engineering
Experience working in an AGILE environment
Experience developing commercial software products
Experience with AWS data warehouse infrastructure (redshift, EMR/spark)
GIT expertise

Benefits

Consistent with our belief that our employees are our most valuable resource, EAB offers a competitive benefits package.
Medical, dental, and vision insurance, dependents eligible401(k) retirement plan with company matchGenerous PTODaytime leave policy for community service or fitness activities (up to 10 hours a month each)Wellness programs including gym discounts and incentives to promote healthy livingDynamic growth opportunities with merit-based promotion philosophyBenefits kick in day one, see the full details here.


At EAB, we believe that to fulfill our mission to “make education smarter and our communities stronger” we need team members who bring a diversity of perspectives to the table and a workplace where each team member is valued, respected and heard.

To that end, EAB is an Equal Opportunity Employer, and we make employment decisions on the basis of qualifications, merit and business need. We don’t discriminate on the basis of race, religion, color, sex, gender identity or expression, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law."
81,Data Engineer,Blue Origin,,"Kent, WA","Description:
As part of a small, passionate and accomplished team of experts, you will work with stakeholders and technical product managers to create a world class decision support system. To successfully accomplish this task, you will design and implement data pipelines from scores of source systems, create flexible and powerful data models and pathways to allow reliable and timely information to be securely delivered downstream to systems and people. This position requires a commitment to quality and attention to detail that will directly impact the history of space exploration and will require your dedicated commitment and detailed attention towards safe and repeatable spaceflight.
Responsibilities:
Collaborate with departments and technical product managers to collect, transform and aggregate information that leads to business insights
Build and maintain tools, data pipelines, analytics, reports to highlight technical performance metrics and other key information identified by programs and functional leadership
Work with application developers to collect data from custom applications
Establish processes and tools for monitoring and improving performance and effectivity of new and existing data integrations and pipelines
Perform quality assurance and code reviews to ensure both functional and non-functional requirements are being met
Qualifications:
5+ years of data engineering, ETL and/or data warehouse development
Master’s Degree in Computer Science (or similar area of study)
Technical expertise and experience both SQL and NOSQL databases
Advanced understanding of a wide array of data models including relational, dimensional, document-based, object oriented, object-relational, and graphical
Advanced experience in database interrogation of SQL and NOSQL databases
Experience implementing High Availability systems requirement
Experience with web based APIs (e.g. REST, SOAP)
Experience with AWS Stack (RDS, Kinesis, Lambda, Redshift, SQS, etc)
Proficiency in scripting languages (e.g. Python, Bash)
Strong analytic skill set and a high degree of proficiency in data mining
Excellent written communication and presentation skills
Must be a U.S. citizen or national, U.S. permanent resident (current Green Card holder), or lawfully admitted into the U.S. as a refugee or granted asylum.
Desired:
Experience with and knowledge of project management principles and practices
Experience in manufacturing processes such as Integrated Supply Chain
Experience with OLAP Cubes or similar BI constructs
Experience with Kafka, Spark and other big data pipeline technologies
Experience with IoT / Smart Factory data collection and aggregation
Blue Origin offers a phenomenal work environment and awesome culture with competitive compensation, benefits, 401K, and relocation.


Blue Origin is an equal opportunity employer . In addition to EEO being the law, it is a policy that is fully consistent with Blue's principles. All qualified applicants will receive consideration for employment without regard to status as a protected veteran or a qualified individual with a disability, or other protected status such as race, religion, color, national origin, sex, sexual orientation, gender identity, genetic information, pregnancy or age. Blue Origin prohibits any form of workplace harassment."
82,Data Engineer,BlueVine,,"Redwood City, CA","Small business owners don’t get the love they deserve from traditional banks. That’s where BlueVine comes in. BlueVine is a leading provider of online working capital financing to small and mid-sized businesses. We’re using cutting edge technology to disrupt the traditional banking industry and develop the next generation of fast and simple financial products, designed for today’s small businesses. BlueVine is headquartered in Redwood City, CA and backed by leading investors including Menlo Ventures, Lightspeed Ventures, Citi Ventures, SVB Financial, Nationwide Insurance, M12 (Microsoft’s Venture Arm).

We are looking for a proven Data Engineer to work with our business intelligence and business operations teams to help design, develop and build a robust data infrastructure required for optimal extraction, transformation, loading of data from a wide variety of data sources that allow our key business units to thrive and grow.

Note: This is not a Data Scientist or Data Analytics role. You must be a Software Engineer specializing in Data to be considered.
WHAT YOU'LL DO:
Help build robust data pipelines and ETL tools
Work collaboratively across the organization to address and predict data performance bottlenecks
Work with key stakeholders to plan catalog data improvements so as to maximize customer improvement impact
Enable effective decision making by retrieving and aggregating data from multiple sources and compiling it into a digestible and actionable format
Utilize a combination of open source and in house developed software to meet company goals
Possess a sound understanding of areas of Computer Science such as algorithms, data structures, object-oriented design, and databases
WHAT WE LOOK FOR:
Experience in the complete life cycle of data warehouse projects, including functional/technical design, coding, and testing
Possess a sound understanding of areas of Computer Science such as algorithms, data structures, object-oriented design, and databases
Minimum 2-3 years of software development experience
Minimum 1 year of Python and SQL experience
Experience working with AWS cloud services
Experience with relational databases a must
NICE TO HAVES:
Experience with Docker containers
Experience with Postgres database
Experience with Redshift Experience with using open source or non-open source ETL tools
CS degree or related field
BENEFIT & PERKS:
Receive over $1,000 annually for a wellness benefit of your choice
Free commuter benefits - CalTrain passes for SF employees and a monthly parking allowance for SF/NJ employees
Weekly catered lunches
Unlimited snacks in fully stocked kitchens
Generous PTO and holidays
Excellent health coverage and life insurance benefits
Generous, paid parental leave which covers up to 15 weeks
Pet-friendly offices"
83,Associate Data Engineer,Customers Bank,,"New Haven, CT","Overview
BankMobile is seeking an Associate Data Engineer. The Business Intelligence and Analytics department controls the entire data ecosystem and is accountable for all data analysis, both quantitative and qualitative, across the entire enterprise. You will be responsible for developing and maintaining our various technological systems. The Business Intelligence team utilizes a cloud based Teradata data warehouse, MicroStrategy v10.4 BI Enterprise reporting system, Wherescape v6.8.7 ETL tool and a variety of smaller tools to complete our work and analyses.

This position will report to the Senior Vice President of Business Intelligence and Analytics.

Required Competencies:
To perform this job successfully, the individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required.
Ability to think holistically and analyze & synthesize data collected from a variety of sources.
Ability to recognize relationships between data elements and extract the deeper narrative
Excellent project management and interpersonal skills.
Strong verbal and written communication skills.
Responsibilities
Knowledge on creation of database objects like tables, views, joins, indexes, and database statistics;
Familiar with the process of ETL in the creation of data warehouses and data marts;
Knowledge of an ETL tool (Wherescape is helpful but not required);
Knowledge of a BI tool (MicroStrategy is helpful but not required);
Ability to meet deadlines and produce quality results under pressure;
High energy self-starter.
Partner with technical teams to implement data capture, ETL, validation, and test methods
Assist with multiple projects and adjust to rapidly changing priorities. Focus on, and be able to balance project timeliness, while maintaining overall quality across multiple projects and consultations
Maintain and assist in data model documentation, data dictionary, data flow, data mapping and other MDM and Data Governance documentation
Capable of leading a project from start to finish
Qualifications
Minimum of 1 years’ experience with data warehousing projects and solutions
Proficiency in SQL (ANSI) with the ability to write complex freehand SQL including sub-queries, nested queries, and other advanced SQL features.
Knowledge of physical database design and data structures
About us
About us About BankMobile: BankMobile, a division of Customers Bank, is the largest and fastest growing mobile-first bank in the country. Our mission is to make banking affordable, effortless, and financially empowering for the millions of Americans who need it the most – the underbanked, millennials and working-class Americans. BankMobile is the first bank in the U.S. to offer a completely fee-free banking experience and uses innovative mobile technology to make banking enjoyable and seamless. The Bank is also committed to making each of its customers feel financially empowered through education and access to a free financial coach. Through constant innovation, its white label banking distribution model, and putting the customer experience above all else, BankMobile is truly disrupting the banking space. BankMobile, a division of Customers Bank, will provide consideration for employment to qualified applicants without regard to their race, color, religion, national origin, sex, protected veteran status or disability. BankMobile, a division of Customers Bank. Member FDIC - Equal Housing Lender - All Rights Reserved"
84,Associate Data Engineer,Digital Trends,,"Portland, OR 97204","Job Title: Associate Data Engineer

Dept: Business Intelligence

Reports to: Senior Data Engineer

FLSA: Salary, Exempt, Full-time

Location: Portland

About Us:
Digital Trends ( https://www.digitaltrends.com/ ), the largest independent tech media site in the world, is a technology/lifestyle brand with offices in Portland, Oregon; New York; Chicago; and more. We cover anything and everything in technology- from the QLED vs OLED 4k TVs, racing drones, Apple product recalls, to innovations in autonomous driving. Our content spans from news, expert analysis, guides, in-depth reviews, and more. Digital Trends provides insights to our readers in a way that enhances their lifestyle. ""Tech for the Way You Live.""

Who We Are Looking For:
We are looking for someone passionate about data, who's interested in breaking-into their career to join our growing team as an Associate Data Engineer. This individual will help maintain Digital Trends' BI datastores as well as ensure that ETL data pipelines are functional and deliver on time. The Associate Data Engineer will work closely with the Senior Data Engineer and the Data Analysts to ensure reliable and performant delivery of critical data from multiple disparate sources for regular analysis and reporting.

What You'll Do:

Monitor all data pipelines for accuracy and availability. Troubleshoot and resolve any problems that occur.
Respond to ticket requests from the Data Team for data corrections and pipeline updates.
Cloud infrastructure monitoring and maintenance with a focus on reliability and performance.
Create reusable scripts and programs for data Extraction, Transformation and Loading (ETL).

What We Require:

2+ years of Python experience (with a focus on Pandas, NumPy, Boto3, Luigi, and API protocols such as REST and OAuth)
1+ years source code management experience (Git preferred)
Relational database experience (Redshift preferred or SQL Server, MySQL)
Ability to maintain current and accurate process and project documentation
Familiarity with BI Visualization platform integration (Looker preferred)
Degree in Computer Science or equivalent practical experience
Ability to work in an office environment.
Ability to handle multiple tasks effectively and meet deadlines.
Excellent organizational skills and attention to detail.
Excellent communication and interpersonal skills. Able to communicate and provide feedback effectively to others.

Optional Skills:

AWS Certification and/or extensive cloud infrastructure experience
Familiarity with AWS or similar services: Redshift, DynamoDB, SQS, SES, Lambda, Cloudwatch, S3, IAM, EC2
Familiarity with High Availability infrastructure
NoSQL and Graph database experience
Front End stack experience a plus (HTML, JavaScript, React, etc)
Familiarity with Agile Kanban methodology and related technology (Atlassian JIRA)
Snowplow Analytics experience

What's In It For You:

Exceptional benefits package including: medical, prescription, dental and vision insurance, 401k, flexible spending accounts, pre-tax commuter transit and parking benefits, paid parental leave, sabbatical leave after 5 years of service, flexible paid time off, life insurance, company-paid short-term and long-term disability.
Fun working environment! Our perks include: free beer and kombucha on tap, free espresso, free drinks and snacks, foosball table & ping pong table, big-screen TVs, bike racks, in-office shower, in-office laundry machines (while testing them for reviews), full kitchen and much more!
Company paid Tri-Met pass (PDX)
Free membership to gym within building (PDX)

Digital Trends is an equal opportunity employer. We encourage, value and honor diversity."
85,Data Engineer,Adobe,,"San Jose, CA","The Creative Cloud Engagement Analytics team is looking for a passionate Data Engineer to join our growing team of analytics experts. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Tapping into our massive product usage data sets, you will architect, build and optimize analytics platform and pipelines to harness our data, derive actionable insights to help guide the business with data. One must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing the data architecture to support our next generation of products and data initiatives.
What you’ll do
Architect, build and maintain scalable automated data pipelines ground up. Be an expert of stitching and calibrating data across various data sources.
Work with Adobe’s data ingestion, data platform and product teams to understand and validate instrumentation and data flow.
Develop data set processes for data modeling, mining and production.
Integrate new data management technologies and software engineering tools into existing structures
Support regular ad-hoc data querying and analysis to better understand customer behaviors.
Understand, monitor, QA, translate, collaborate with business teams to ensure ongoing data quality.
What you need to succeed
Bachelor’s degree in Computer Science, Information Systems or a related field is required, master’s preferred.
5-7 years of experience building and maintaining big data pipelines and/or analytical or reporting systems at scale.
Expert level skills working with Apache Hadoop and related technology stack like Pig, Hive, Oozie etc.
A strong proficiency in querying, manipulating and analyzing large data sets using SQL and/or SQL-like languages.
Approaching data organization challenges with a clear eye on what is important; employing the right approach/methods to make the maximum use of time and human resources.
Good attention to detail and ability to stitch and QA multiple data sources. Exploring new territories and finding creative and unusual ways to solve data management problems.
Be a self-starter.
Good interpersonal skills.
Preferred (but not required) Skills:
Experience working with at least one other big data platform like Apache Spark, Redshift, Teradata or SAP HANA.
Familiarity with streaming platforms like Apache Kafka, Amazon Kinesis etc.
Knowledge of Data Science, Machine Learning and Statistical Models is desirable.
Familiarity with Microsoft SSAS and Cubes and advanced Excel skills.
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status."
86,Data Engineer,Storyfit,,"Austin, TX","Employment Type
Full Time
Job Location
Austin, Texas
Description
StoryFit applies NLP & AI to narrative content (books & screenplays) to provide insights to the entertainment industry.


Are you a strong Python developer that would like to learn more about machine learning and gain great experience managing enormous data resources? StoryFit is the world’s leading platform for machine reading of books, movie and TV scripts. We’re looking for an exceptional developer to design, implement and maintain our data collection pipelines, keep our database clean and organized, and assist our data scientists with collecting data from a wide variety of sources.


This is a unique opportunity to join a dynamic group of creative and professional individuals destined to change the face of the entertainment industry. We are a lean startup team combining the disciplines of data science and media expertise in the tech-savvy community of Austin, Texas.


If you enjoy peering into the future and shaping its outcome, then you’re just who we’re looking for. (Preference is given to Austin-based candidates.)

Responsibilities
Expand existing software to meet the changing needs of our users
Develop new software/products from the ground up, staying true to our company’s core values and needs while lending your own creativity to the mix
Focus on creating fault-tolerant programming
Create scalable, automated solutions for our users
Establish multi-platform versions of the software package
Write tests for existing and created code to ensure compatibility and stability
Qualifications
Bachelor’s Degree in operations research, applied statistics, data mining, machine learning, physics or a related quantitative discipline. Master’s Degree preferred.
3+ years of experience
Strong skills with Python, SQL, PostgreSQL
Experience with collecting data using web tools and third parties
Management of large databases
Ability to work well with a team"
87,Azure Data Engineer,Accenture,,"Los Angeles, CA","Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
88,Data Engineer,"Parallax Volatility Advisers, LP",,"San Francisco, CA","REPORTING RELATIONSHIP: Chief Information Officer

Summary Description of Role:
As our volumes increase and markets expands, we need to continue to evolve our TCA (Trade Cost Analysis) infrastructure so that we have unbiased metrics to explain our P&L which can drive future trading decisions. These analytics are highly data driven so we need to ensure the accuracy of all data inputs that feed these calculations.

This role would do just that and focus on the data quality of the various TCA data sets. Along with having a technical background, candidates must have a solid foundation in financial concepts and comfort in working with everyone from the technical teams to traders to ops to quants.

Specific Responsibilities include:

Operational Support: A big part of this role will be ensuring data quality by proactively:
Searching for and fixing data issues
Handling investigations as traders identify issues
Leading retrospective discussions and suggesting long term process fixes based on the issues that are arising
Checks and Measurements: We need way of measuring data quality through well-defined data quality checks. This role would work to put the necessary checks in place, so we are able to quickly fix issues and confirm we are improving our data quality over time.
Development: Some data issues will require us to deploy new (or changes to our existing) ETL processes.

QUALIFICATIONS:

Bachelors' Degree in Computer Science or other relevant field of study
Minimum 2-3 years of relevant development experience
Financial background in both derivative pricing and operational processes (e.g. settlement)
Must be a U.S citizen or authorized to work in the U.S on a permanent basis
Willingness to work market hours (~6:00am-3:00pm)

SKILLS/KNOWLEDGE:

Knowledge of a higher-level programming language (e.g. C#, Python, Java)
Familiarity with data quality methodology and standard processes
Strong debugging skills
Great attention to detail and ability to work in a fast-paced environment
Strong technical background in SqlServer and ETL processes
Ability to communicate with both technical and non-technical personnel

Parallax offers extremely competitive compensation and benefits, including (not limited to): 100% paid health insurance, casual dress, all meals during working hours, commuter reimbursement and continuing education opportunities.

Parallax is an equal opportunity employer.

Please no calls or recruiters."
89,DATA ENGINEER,"Big Bright International-Parallax Volatility Advisers, LP",,"Irvine, CA","Data Engineer at various unanticipated client locations throughout the US to provide design, development, testing and implementation for business computer systems; provide ETL big data integration; utilize Data Stage to populate tables in data warehouse and data marts; design hive schema, create hive tables, and load and analyze data using hive queries."
90,Data Engineer,Codecademy,,"New York, NY","Hello, World! Codecademy has helped over 45 million people from around the world upgrade their careers with engaging, accessible, and flexible education on programming and data skills. We provide over 200 hands-on interactive lessons ranging from Python to R to Javascript and everything in between. Our learners have gone on to start companies, new jobs, and new lives thanks to what they've learned with Codecademy, and we're thrilled to be working to take that impact to the next level.

Codecademy was started in 2011 by two college students in a dorm room at Columbia that were frustrated by the huge gap between education and employment. A few years later, we are a rapidly growing, diverse team of 75+ in SoHo, NYC. We've raised over $40m in venture capital funding from top investors including Union Square Ventures, Kleiner Perkins, Naspers, Y Combinator, and more.

If you want to help build a business that impacts tens of millions of people each year and helps them lead better lives, join us!

Codecademy is one of the biggest online educators in the world. We have one of the largest data sets in learning - billions of lines of code, hundreds of millions of submissions, and, we think, the ability to find out how all of that can make the best learning experience in the world. We're not simply using data to solve the usual problems - recommendation engines, search rankings, etc. - but instead are using it to improve the ability of our students to learn skills that can change their lives. At Codecademy, we believe in the value of fast iteration and being data driven. We're looking for a data engineer who is passionate about building systems to tackle large data sets ETL and facilitate analytics-driven experimentation across the Codecademy team.

What You'll Do
--------------


Data infrastructure: Consolidate the ETL processes across various internal and external data sources and proactively make the quality of our data better and easier to access
Learner session analysis: Analyze learner session data to provide actionable insight to improve our user experience. For example, you'll be diving deep into where learners succeed and drop off, picking up on the insights from that data to help people learn at a faster clip and to stay motivated in order to achieve their goals
Business intelligence visualization: Visualize and provide insights of our key growth, retention, and user engagement metrics. With your help, define the metrics that matter for Codecademy, help to present them to the team, and guide product development by showcasing the path to growth
Be lean, be data driven: Work with internal team members to form testable hypothesis, be creative to design the cheapest experiment, and draw valuable insights from experiment results

What You'll Need
----------------


Mastery of large scale ETL systems
Mastery of at least one of the following languages: Ruby, Python or Java
Experience working with distributed NoSQL database systems in a production environment
Analytical problem solving skills
Ability to make pragmatic engineering decisions in a short amount of time

What Will Make You Stand Out
----------------------------


Experience with Apache Airflow

At Codecademy, we are committed to teaching people the skills they need to upgrade their careers. Codecademy aims to educate a richly diverse demographic of users with our product and in order to accomplish this, we believe our team should reflect that rich diversity. Our company celebrates diversity in all of its forms-- race, gender, color, national origin, marital status, sexuality, religion, veteran status, age, ability, disability status-- and works to create an inclusive workplace where people of all backgrounds and beliefs are empowered to better their futures."
91,Data Engineer,Mixmax,,"San Francisco, CA","-------------
Data Engineer
-------------

We're looking to hire our first data engineer to lay the foundation for all aspects of Mixmax's data infrastructure from end to end. You'll be a key communicator, working both cross-functionally in our San Francisco office and across our distributed organization.

As a data engineer you'll help start a team specifically focused on ensuring the company runs on accurate and repeatable data. This means being a member of a team that values continuous and collective learning, culture over process, data driven development and always asking tons of questions. We actively blog about our work, contribute to open source, sponsor Open Collectives, and host/present at meetups - we actively encourage you to do the same and under your own name.

Responsibilities:
-----------------

As a data engineer, you'll:

Design, build, and support data-centric services including but not limited to event streaming, ETL pipelines, distributed data storage, and real-time data processing.
Work on high impact projects that optimize data availability and quality, and provide reliable access to data across the company.
Collaborate with partner teams to understand their business contexts and analytical challenges, and to transform and sprinkle data-driven fairy dust on their products.
Develop machine-learning software using analytical data models that can generalize across Mixmax customers, but can automatically adapt to each of their individual features.
Communicate strategies and processes around data modeling and architecture to multi-functional groups.
Work with fellow engineers to build out other parts of the data infrastructure, effectively communicating your needs and understanding theirs.

Requirements and skills you possess:
------------------------------------


Exceptional coding and design skills, particularly in Java/Scala or Python.
Extensive previous experience of working with large data volumes, including processing, transforming and transporting large-scale datasets for analytics and business purposes.
Extensive experience data warehousing and ETL pipelines.
Great communication and collaboration skills.

Bonus points:

Previous experience with AWS like EC2, RDS, S3, Redshift, SNS, SQS, etc.
Previous experience with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra
Previous experience building out high volume, distributed event systems (such as working w/ Kafka or similar)

Bonus points ++:

Experience using terraform or other infrastructure as code
Contributor to open source technologies

Diversity and inclusion are core to our culture, and we are actively committed to building a more inclusive work environment. If you are a member of an underrepresented group in technology, we strongly encourage you to apply."
92,"Software Engineer, Data",Fathom Health,,"San Francisco, CA","We’re on a mission to understand and structure the world’s medical data, starting by making sense of the terabytes of clinician notes contained within the electronic health records of the world’s largest health systems.

We’re seeking exceptional Data Engineers to work on data products that drive the core of our business-a backend expert able to unify data, and build systems that scale from both an operational and an organizational perspective.
As a Data Engineer you will:
Develop data infrastructure to ingest, sanitize and normalize a broad range of medical data, such as electronics health records, journals, established medical ontologies, crowd-sourced labelling and other human inputs.
Build performant and expressive interfaces to the data
Build infrastructure to help us not only scale up data ingest, but large-scale cloud-based machine learning

We’re looking for teammates who bring:
Experience building data pipelines from disparate sources
Hands-on experience building and scaling up compute clusters
Excitement about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration.
A solid understanding of databases and large-scale data processing frameworks like Hadoop or Spark. You’ve not only worked with a variety of technologies, but know how to pick the right tool for the job.
A unique combination of creative and analytic skills capable of designing a system capable of pulling together, training, and testing dozens of data sources under a unified ontology.

Bonus points if you have experience with:
Developing systems to do or support machine learning, including experience working with NLP toolkits like Stanford CoreNLP, OpenNLP, and/or Python’s NLTK.
Expertise with wrangling healthcare data and/or HIPAA.
Experience with managing large-scale data labelling and acquisition, through tools such as through Amazon Turk or DeepDive."
93,Data Engineer,Empower Retirement,,"Greenwood Village, CO","Looking to make a real difference?
You belong right here.
Come build a rewarding career helping others achieve their financial dream at an organization that values your own long-term success. With your unique talents, you have what it takes to be bold and brilliant in everything you do and reach new heights for a company dedicated to diversity and inclusion, community and you.
If you share that belief, this is where you belong.
Join our team of nearly 6,000 associates across 40 different locations worldwide and start your future today.
Empower-Retirement is looking for an experienced Data Engineer who is responsible for the maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The Data Engineer works with the business units, internal technology full stack teams, data analytics teams, and data scientists in order to understand and aid in the implementation of database requirements, analyze performance, and troubleshoot any existent issues.
DUTIES / RESPONSIBILITIES / ESSENTIAL FUNCTIONS:
Create content maps (decomposition of Tableau reports and data sources)
Analyze data discrepancies to determine root cause and identify correct course of action
Document data lineage (source to target), data transformations, business logic, calculations, data dictionary
Validate data and transformations throughout the entire data lifecycle
Validate expectations and accuracy of the data with business and development leaders
Provide guidance on automated data QA checks throughout data lifecycle
Revamp/optimize ETL scripts/processes and reporting database
Design, construct, install, test and maintain data management systems.
Build high-performance algorithms, predictive models, and prototypes.
Ensure that all systems meet the business/company requirements as well as industry practices.
Integrate up-and-coming data management and software engineering technologies into existing data structures.
Develop set processes for data mining, data modeling, and data production.
Create custom software components and analytics applications.
Research new uses for existing data.
Employ an array of technological languages and tools to connect systems together.
Collaborate with members of your team (eg, data architects, the IT team, data scientists) on the project’s goals.
Install/update disaster recovery procedures
Recommend different ways to constantly improve data reliability and quality
EDUCATION:
Bachelor’s degree in computer science, software/computer engineering, applied mathematics, or physics statistics.
OTHER PREFERRED QUALIFICATIONS:
2-3 years working experience in data engineering, data warehousing, data integration or business intelligence
2-3 years of experience managing, debugging, and optimizing databases that are critical to the business’s mission.
Strong experience with Python, Postgres, and MySQL writing data transformation jobs
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Excellent communication skills with the ability to collaborate with non-technical partners
Familiarity with cloud-based data engineering in AWS Cloud, EC2, CloudFrontRoute5, API Gateway, CloudWatch, CloudTrail, CloudFormation, RDS, Redshift, DynamoDB, S3, Aurora
Experience working in AWS: Glue, Lambda, Step, EMR, Data Pipeline, Kenisis, Athena, Quicksight strongly preferred
Experience with Business Intelligence, end-to-end implementation and requirements gathering, in Tableau, Power BI, Looker, Databricks, Alteryx, or Snowflake in strongly desired
Familiarity with Scala, Java, .Net
Experience working in an Agile environment working on a Scrum team
Strong understanding of concepts or experience with ETL and ELT, Data pipelines, Data Warehousing and Data Marts, Big Data and Data Lakes, Kimball Vs. Inmon architecture patterns, TDD, Source control, CI/CD, DevOps"
94,Int Data Engineer,Transamerica,,"Denver, CO 80221","Job Description Summary
The Intermediate Data Engineer is responsible for the development and support of systems, services, and applications required for the collection, storage, processing, and analysis of all forms of data to enable data-driven decisions and outcomes across the enterprise.
Job Description
Responsibilities
Work collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment.
Build and support the operation of Cloud and On-Premises enterprise data infrastructure and tools.
Design robust, reusable and scalable data driven solutions, and data pipeline frameworks to automate the ingestion, processing and delivery of structured and unstructured batch and real-time streaming data.
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications.
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities.
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage.

Qualifications
Bachelor’s degree in computer science, math, engineering, or relevant technical field
Two years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration and data integration concepts and methodologies
One year of experience developing and supporting modern data architectures
One year of experience developing and supporting large-scale distributed applications
One year of experience with Linux operations and development, including basic commands and shell scripting
One year of experience with execution of DevOps methodologies and continuous integration/continuous delivery
Knowledge of SQL for data profiling, analysis and extraction
Results oriented with a strong customer focus
Ability to work within a team environment
Ability to prioritize and meet tight deadlines
Ability to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables
Preferred Qualifications
Master’s degree in a technical field (e.g. computer science, math, engineering)
Understanding of big data and real time streaming analytics processing architecture and ecosystems
Understanding of data warehousing architecture and implementation, including source to target mappings and ETL
Experience with advanced analytics and machine learning concepts and technology implementations
Experience with data analysis and using data visualization tools to describe data
Relevant technology or platform certification (AWS, Microsoft, etc.)
Software development experience in relevant programming languages (i.e. Java, Python, Scala, Node.js)
Working Conditions
Office environment
Occasional travel"
95,Big Data Engineer,Piper Companies,,"Malvern, PA 19355","Piper Companies is looking for a Big Data Engineer for a niche SaaS organization in Malvern, PA.
Responsibilities for the Big Data Engineer:
Design and develop solutions for data transformations that integrate with external partner apps
Develop clean, immutable, code
Qualifications for the Big Data Engineer:
4+ years of Data Engineering experience
Experience building data warehouse platforms
Hands on ETL experience (Python, Kafka, Storm)
Understanding of NoSQL databases
Knowledge of AWS - S3, Redshift, Kinesis
Knowledge of EAI/SOA best practices/standards
Compensation/Benefits for the Big Data Engineer:
$140,000 + Bonus
Flexible schedule
Comprehensive benefit package; Medical, Dental, Vision, 401k
Please send resume to Dan Compas: dcompas@pipercompanies.com"
96,Data Engineer,"Cal Poly, San Luis Obispo",,"San Luis Obispo, CA 93407","Cal Poly is currently transitioning to a new applicant tracking system, PageUp! To view this job please click here

Minimum Qualifications (Staff Only):


Required Qualifications:


Preferred Qualifications:


Special Conditions:


Review Begin Date:

Requisition Number: 105312

To Apply: All applicants must apply online at: https://www.calpolyjobs.org.

About Us
Cal Poly is a nationally ranked, four-year, comprehensive public university. Founded in 1901, the 6,000-acre campus is nestled in the foothills of San Luis Obispo, along California's scenic central coast and midway between Los Angeles and San Francisco. With a budget of $168.5 million, the university has a student enrollment exceeding 17,000 students and employs more than 2,000 faculty and staff members. Cal Poly is part of the 23-campus California State University system.

At California Polytechnic State University, San Luis Obispo, we believe that cultivating an environment that embraces and promotes diversity is fundamental to the success of our students, our employees and our community. Bringing people together from different backgrounds, experiences and value systems fosters the innovative and creative thinking that exemplifies Cal Poly's values of free inquiry, cultural and intellectual diversity, mutual respect, civic engagement, and social and environmental responsibility.

Cal Poly's commitment to diversity informs our efforts in recruitment, hiring and retention. California Polytechnic State University is an affirmative action/equal opportunity employer."
97,Data Engineer,Verint Systems Inc.,,"Ann Arbor, MI","Overview of Job Function:
The Data Engineer (the “Engineer”) will design and build scalable data processing and AI pipelines for our Big Data and AI Platform. The Engineer will be responsible for processing billions of events from marquee customers in real-time for generating customer satisfaction analytics, AI based predictions, recommendations and engagement. Our platform is designed with cutting edge technologies and the Engineer will work with Kafka, spark, vertx and Mesos amongst others
Principal Duties:
Build/manage Kafka cluster to handle high volume real-time and batch processing pipelines for analytics and AI;
Performance tune and scale Kafka clusters across various products and systems;
Develop software /product that analyzes large volume of data in bigdata system, as well as deep learning using various AI/ML technologies;
Evaluate new technologies in bigdata pipeline processing.
Minimum Requirements:
Bachelor’s degree in Computer Science or equivalent experience;
At least 3 years relevant working experience managing large Kafka clusters in 24x7 production environment;
At least 3 years relevant experience in development, asynchronous programming and data streaming in Java;
Highly skilled on Kafka cluster upgrade, monitoring and troubleshooting, with deep understanding of impact on partition reassignment, Kafka consumer pause/resume, broker network latency optimization, etc.;
A passion for innovation and thinking “out of the box” but focused on delivering product on time;
Successful completion of the background check process, including but not limited to employment, education, criminal convictions, OFAC, SS Verification and credit, where available and in accordance with federal and local regulations.
Preferred Requirements:
Ph.D. level knowledge of related areas – Statistics, Data Science, Marketing (quantitative), Econometrics, or Computer Science;
Knowledge of Python or Spark for real time data processing is a big plus;
Vertx knowledge is a plus.

As an equal opportunity employer, Verint Systems Inc. prides itself in providing employees with a work environment in which all individuals are treated with respect and dignity. This means we are committed to providing equal opportunity to all qualified employees and applicants for employment without regard to one’s race, color, religion, national origin, age, gender, disability, alienage or citizenship status, marital status, creed, genetic predisposition or carrier status, sexual orientation, Veteran status or any other classification protected by applicable federal, state or local laws. This policy applies to all terms and conditions of employment including but not limited to hiring, placement, promotion, compensation, training, leave of absence or termination."
98,Data Engineer - Card Technology,Capital One - US,,"McLean, VA","McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer - Card Technology

Capital One (yes, the “what’s in your wallet?” company!) is rethinking the way the world approaches banking. As a Capital One Data Engineer, you will develop fast data infrastructure leveraging Apache Spark, Databricks, and Apache Kafka to manage and create information products using the data streamed from our fleet of over 2000 ATM’s . Whether a new feature or a bug fix, you will lead your work and deliver the most elegant and scalable solutions, all while learning and growing your skills. Most importantly, you’ll work and collaborate with a nimble, autonomous, cross-functional team of makers, breakers, doers, and disruptors who love to solve real problems and meet real customer needs.

The person we're looking for:
has a sense of intellectual curiosity and a burning desire to learn

is self-driven, actively looks for ways to contribute, and knows how to get things done

is deliriously customer-focused

values data and truth over ego

has a strong sense of engineering craftsmanship, takes pride in the code they write

believes that good software development includes good testing, good documentation, and good collaboration

has great communication and reasoning skills, including the ability to make a strong case for technology choices

Basic Qualifications:

Bachelor’s degree
At least 1 year of experience with leading big data technologies such as Apache Spark, Apache Hadoop, or Apache Kafka
At least 2 years of professional experience with data engineering concepts

Preferred Qualifications:

2+ years experience with AWS cloud
2+ years of experience in Java, Scala, or Python
2+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python
2+ years of experience building data pipelines
At least 1 year of Cloud (AWS, Azure, Google) development experience
Experience with Streaming and/or NoSQL implementation (Mongo, Cassandra, etc.) a plus

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
99,Data Engineer,Inspire11,,"Chicago, IL","Turn it up to 11 as a Data Engineer!

Do you want to solve some of the biggest challenges facing companies today using data and analytics? Do you want the opportunity to work with a team of incredibly intelligent, fun-loving, and collaborative individuals? Are you looking to grow and develop your skills across a variety of technologies and tools?

If you answered yes to any of those questions, then Inspire11 could be a great fit for you! We are a full services local consulting firm that is building out customized solutions to help our clients stay relevant in an ever-changing technological environment. Our project work spans across a variety of specialties from data warehousing to data science and a variety of industries.

We partner with our clients to optimize their business, and ultimately blow their minds with the solutions we're able to implement. Our team is always tackling the most difficult problems that client teams face, and are never stuck in maintenance mode.

Our team is always learning from each other and pushing the boundaries of what's possible.

You're still not sold? There's More:

Growth opportunity: We are always learning new technologies and educating our clients on what's available in the data space. You also will have the opportunity to engage in growing the team and there are ample opportunities to take ownership
Work life balance: We are respectful of people's boundaries and have unlimited time off so that our team has time to recharge and do their best work
Flexible hours: We understand that our team members have different needs and do our best to work with their schedules while accommodating client needs
Inclusive environment: We are committed to building an inclusive environment where all teammates feel comfortable and supported
Fun! We love to keep things fun, both within our client work and at company wide events

So what do I have to do to join?


Participate in the full life-cycle of development, through definition, design, implementation, and testing
Identify data sources, provide data flow diagrams and documents source to target mapping and process
Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes
Work with the client and consulting team to help gather requirements; understand different processes as it relates to different parts of the business and where there is overlap
Assist with report development using tools such as: Tableau, PowerBI, Qlickview
Regularly contribute to ongoing improvements in engineering process and product development
Support business decisions with ad hoc analysis as needed

I can do that! Any other skills that I need?


5+ years of experience as a data/software engineer
Strong understanding of data modeling concepts and design
Strong understanding of data warehousing technologies, ETL processes and data flow architectures and tools
Experience in Software Development Lifecycle (SDLC) utilizing the Agile approach
Organized, detailed oriented, and can manage multiple projects at the same time
Excellent communication skills
Comfortable working in fast paced environments, are able to wear many hats, and have a general fear of being bored

We believe that everyone drives change, and everyone is an owner. Nothing excites us more than having the ability to collaborate with intelligent, highly-motivated and talented people on challenging problems as we work to change the face of the digital and analytics space."
100,Big Data Engineer,CradlePoint,,"Boise, ID 83702","Overview
Cradlepoint has hundreds of thousands of routers used in many mission critical applications across several verticals. How we manage the data, provide insights and monitoring is a huge value for our customers. This role will enable the continued performance of our applications as we ‘ride the wave’ of 5G. This role will include building data services that our platform and product teams leverage when developing software. Identifying data service needs and create narratives and user scenarios. Produce working code in using agile methodology, with clear documentation for consumption by developers. Working closely with our Principle and Distinguished Engineers to design and build software components for the service. Building APIs and development scaffolding for use of the services.
Responsibilities
This person will be working with the Data Rollup/ Analytics team. He/she will work on new features/products based on the product roadmaps from the Product team. He/she will also provide support to the existing applications in production in the form of performance tuning, performance analysis, capacity planning, production support and killing bugs as needed.
Qualifications
BS in Comp Sci or equivalent work experience

Must have at least 3 years of hands on experience with:
Multiple types of persistent storage technology, such as RDBMS, NoSQL databases and caching frameworks
Desired: Cassandra, Postgres, Redis, DynamoDB, Redshift
Database performance optimization
Managing large data sets (e.g., multi-million row tables)
Data modeling
At least one public cloud platform like Amazon Web Services (AWS), including PAAS and IAAS offerings, such as Dynamo DB, S3, EC2, SQS, SNS, Firehose, and Kinesis
Must have 1-2 years of experience with:
Web based application development
Desired language: Python
Agile Methodologies
Design and implementation of REST APIs, microservices, and messaging service frameworks
Distributed computing
Desired additional skills/experience:
Has worked on a customer facing website for a reasonably sized company
Capable of working quickly, responsibly, and independently
Understands of concepts such as CI/CD, regression testing, load testing, and capacity planning"
101,Data Engineer (Remote),Apollo.io,,Remote,"About the Company

Apollo accelerates the growth and success of your entire sales org with the first truly reliable, scalable revenue engine and account-based sales platform. We’ve created the solution for the persistent pain that reps aren’t sending the right messages, to the right people, at the right time despite the three to six sales point solutions they use each day.

Managers and reps alike can trust our unified platform, which includes an up-to-date database of 200M+ contacts, a full engagement stack, and the industry’s only advanced Rules Engine and fully custom Analytics suite. Reps get a platform with their team’s best practices built in, so they can focus on selling, and managers can build strategies based on advanced revenue data, not guesswork.

Apollo is the foundation of your entire go-to-market strategy.

The Engineering Team

Apollo is run by a lean team of mission-driven value centric individuals. Our incredibly high caliber team is focused on building a great product our users love. We’re looking to add a passionate, skilled, friendly front-end engineer who is looking to invest their time and energy in the right company.

About the Role

As our first full-time data engineer you will collaborate with our CTO to design and build out our database. Apollo manages a multi-terrabyte scale database consisting of trillions of data points. The primary job of the data engineer will be to expand upon this industry-leading database. You will get the chance to make critical db schema decisions and architectural improvements that lay the DB foundation for the upcoming years. We expect a very strong ability in reasoning through different edge cases, innovating upon data pipelines, cleaning up corrupt data, and architecturing core backend components. The right candidate should be excited by the prospect of re-designing entire data architectures to support our next generation of products and data initiatives.

***This is a remote position available from anywhere in the US or anywhere in the world.***

As we are scaling our company, we are committed to providing the best possible environment for our engineering team. The less time our engineers spend dealing with cumbersome processes, inefficient systems, and commuting, the more time they have to solve problems and make an impact. We're a technology-first team dedicated to building the greatest engagement software available. We keep in touch with Slack and regular video conferences. You can work from home, a coffee shop or the beach as long as your work is aligned with your goals.

You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You'll have the chance to work in an open and collaborative environment, receive and give hands-on mentorship and have opportunities to grow and accelerate your career.
Requirements
Minimum of 2+ years experience in software engineering or data engineering. CS or equivalent degree preferred
Expert knowledge of at least one open-source popular database, both SQL and NoSQL OK. Knowledge of MongoDB preferred but not required.
Expertise architecturing database schema and pipelines at the terabyte scale
Expertise in at least one modern programming languages such as Python, Ruby, or C++; comfort with writing significant software code and scripts
Assemble large, complex data sets that meet functional / non-functional business requirements.
Work with stakeholders including the Executive, Product, Marketing, and Sales teams to assist with data-related technical issues and support their data infrastructure needs.
Creative and innovative problem solver with experience working in a startup environment
Perks
Competitive salary, equity grants
Top of the line healthcare coverage (medical, dental, vision) and 401(k)
Flexible time-off - recharging and taking time off is a priority for us
Healthy catered lunches every day and a fully-stocked kitchen with breakfast items, snacks and beverages
Downtown location with easy access to BART, CalTrain, and MUNI and great views of San Francisco and the Salesforce Park
Pre-tax commuter benefitsTeam happy hours and team building events
Dog-friendly office - we love our furry friends
If this sounds interesting, we would love to hear from you! Please include whatever info you believe is relevant: resume, GitHub profile, code samples, links to personal projects, etc."
102,Machine Learning Data Engineer I,Mitek Systems,,"San Diego, CA 92123","Location
San Diego
Position Type
Full Time
Team
Engineering
Mitek is looking for a Data Engineer to join our global technology organization. The Data Engineer will be a contributing member of a newly formed cross functional document engineering team responsible for supporting our quality, research and development organizations to deliver best-in-class machine learning solutions and global document coverage for our Digital Identity Verification platforms. To do this, the Data Engineer will work as a part of our Data Operations function to deliver solutions that collect, organize, standardize and process datasets to support ongoing development initiatives.
Mitek delivers a large scale, globally distributed cloud platform that relies on large quantities of structured, semi-structured and unstructured data to deliver new capabilities and derive insights that drive all aspects of our platform delivery. For this reason, we need someone who is technically sharp, a creative problem solver, productive working independently or collaboratively, and has a data driven mindset.
What you will do
Leverage data management principles, and data engineering concepts to implement solutions to manage Mitek’s machine learning processes and data requirements
Implement solutions to ingest, label and manage Mitek’s structured, semi-structured, and unstructured data requirements
Execute team-defined processes to ensure the quality, consistency and versioning of datasets used in the delivery of Mitek products
Facilitate data labeling activities and ensure the quality of all data used to advance Mitek’s machine learning practice
Participate in data collection efforts to assist teams to appropriately collect and ingest data into Mitek’s data lake
Run data through test tools to create reports and identify data anomalies and patterns
rovide additional support as necessary to create and modify datasets, label data, and manipulate data for product improvement or development purposes
Who you are
Detail oriented, with a data-driven mindset
Strong problem solving/troubleshooting skills, with an analytical yet, creative, and innovative approach
Self-starter, with relentless curiosity
Thrives in a fast-paced start-up team-focused culture and adapts to a changing environment
Positive, people-oriented, and energetic attitude
Excellent verbal and written communication skills
Ability to summarize complex issues simply and effectively
What you need
Bachelor’s degree in Mathematics, Statistics, Computer Science or related field, accompanied with 0 - 2 years of relevant experience
Successful history of providing documentation to convey information and drive decision making
Working knowledge of scripting and development languages to manage and manipulate data to meet identified needs (Python and/or Go preferred)
Exposure to working with cloud-based delivery platforms (AWS, Azure, GCP)
Understanding of working with datasets used in the delivery of machine learning based solutions
Demonstrated ability to work with ambiguous requirements, adapt, and learn
What would be nice
Knowledge of data mining, machine learning, natural language processing, or information retrieval
Knowledge of Amazon Web Services and associated technologies
Exposure to Big Data platforms and technologies
Exposure to SQL and NoSQL databases and document stores such as MySQL, Aurora, RedShift, MongoDB, RavenDB etc.
Experience processing large amounts of structured and unstructured data
Prior experience in secure practices of handling sensitive data and PII
2-4 years of experience in a quantitative role or 1-2 years experience in software engineering role"
103,Business Intelligence Data Engineer,Credible,,"San Francisco, CA","Who is Credible?

We believe life's changes create financial needs for people and that the traditional financial system often puts up unnecessary obstacles. People celebrate major milestones like going to college, getting married, and buying a home. And most of the time, these milestones come with financial implications.

At Credible, we have built a company with the mission of bringing transparency, choice, simple processes and savings to accessing credit for life's important moments. What you see is what you get. We are committed to being upfront, honest, and clear about your options. There are no mysteries, no hidden fees, and no secret clauses.

Credible is a fast-growing Australian Securities Exchange (ASX) listed Fintech company that has world class management, has raised multiple rounds of funding, is generating significant revenue and is disrupting the lending market and helping people save money and get out of debt faster.

About the role

Our Business Intelligence team is looking for a Business Intelligence Data Engineer who is passionate about data, analytics, and business strategy. You will help the team learn more about our business, teach others in the company about analytics, and improve the use of our data. You'll be an integral part of providing data-driven insights that inform significant company decisions.

You Will:

Build data pipelines and python-based ETL tools for getting, processing, and delivering data
Partner with teams across the organization to understand their analytics needs and create dashboards and reporting that allow them to execute more effectively
Work with business leaders to define key metrics and build reporting to monitor and understand performance along those metrics
Conduct in-depth data analyses that lead to actionable insights, owning the entire process from ideation to execution to presentation of findings to stakeholders
Develop data models in our data warehouse that enable performant, intuitive analysis
Become an expert on all aspects of Credible's data and analytics infrastructure
Be the driving force behind the adoption and effective use of our BI tool within every team at Credible

Education and Experience:

BA/BS in a quantitative field
1-2 years of work experience as a data analyst, data engineer, or in a highly analytical role
Experience writing SQL queries and using a BI tool
Experience with a scripting language (preferably Python) for data processing and analysis a plus
Experience using the command line and git
Strong grasp of statistics and experience conducting rigorous data analyses
Experience developing models and visualizations in Looker a plus
Experience at an e-commerce or fintech company a plus

Personality and Values:

The capacity to juggle multiple priorities effectively within a fast-paced environment is critical
You're a highly motivated self-starter with the ability to work efficiently with minimal supervision.
Anticipate business needs and think with a business owner mindset – think critically about analyses, don't just complete them
Passion for spreading the value of data throughout the company and communicating insights to a broad audience with varying levels of technical expertise

Why you want to work at Credible

We are a fast moving, fun-loving, seriously smart group of people who really care about impacting the lives of our customers. We empower our employees to make decisions, take risks, drive our business and make changes when we don't get it right. These are our values:


Exceed Customer Expectations: We provide an exceptional experience to each and every customer that compels them to share it with others.
Take Ownership: We are trusted to make decisions that are in the best interests of our customers and our business. We think and act like owners. We care – and that makes all the difference.
Be Curious: We are curious, ask questions, seek to understand and try new things.
Do the Right Thing: We earn trust by being transparent, respectful and honest with each person with whom we interact.
Get Results: Results fuel our excitement and we know how our personal accomplishments tie to the success of the company
Be Bold: We are courageous and take risks that scare us. Our enthusiasm for experimenting is how we will find the next breakthrough.

Our benefits: We offer competitive compensation, generous benefits, free food and a flexible vacation policy.

But mainly, you want to work at Credible because you believe in our mission and want to have a major role in delivering on it! We look forward to getting to know you.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation,age,marital status, veteran status, or disability status. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
104,Data Engineer,B-Stock Solutions,,"Belmont, CA 94002","THE COMPANY
B-Stock is the world’s largest online marketplace for returned, excess, and other liquidation merchandise. Our customers range from SMB to the world’s largest brands and retailers (including nine of the top 10 U.S. retailers). Led by eBay veterans, B-Stock completes over 150,000 transactions per year, selling 70 million items annually, making us a clear leader in the space.

The amount of inventory that is returned or unsold each year is growing very rapidly; in 2018, the value of this merchandise was estimated at $500 billion. Much of it ends up being liquidated for pennies on the dollar; some of it is even destroyed or landfilled. We believe there is tremendous value in and demand for this inventory - no matter the category, condition, or location. The B-Stock platform gives buyers a simple and direct way to buy valuable products, and offers sellers a trusted replacement for traditional liquidation and a critical boost in operational efficiency.

Backed by top investors including Spectrum Equity, True Ventures, and Susquehanna Growth Equity, B-Stock runs lean, fast, and shows no signs of slowing down. Our core values (teamwork, honesty, humor, and the passion to build something great) have shaped the company we are today and will certainly drive our success for many years to come.

For more information, visit www.bstock.com/careers/

JOB SUMMARY
B-Stock is looking for a Data Engineer to help design, build, scale and maintain the next generation of the company's SaaS infrastructure. You will partner closely with cross-functional teams, including Data Science, Engineering, and Product / Business Technology, to build data infrastructure, processes, and tooling.

ESSENTIAL JOB DUTIES AND RESPONSIBILITIES

Manage and optimize core data infrastructure
Build monitoring infrastructure to give visibility into the pipeline’s status
Monitor all jobs for impact on cluster performance
Run maintenance routines regularly
Tune table schemas (i.e. partitions, compression, distribution) to minimize costs and maximize performance
Develop custom data infrastructure not available off-the-shelf
Build and maintain custom ingestion pipelines
Support data team resources with design and performance optimization
Build non-SQL transformation pipelines

MINIMUM QUALIFICATIONS, JOB SKILLS, AND ABILITIES

EDUCATION:

Bachelor's degree in a technical and/or quantitative field of study—e.g., computer science, math, physics or statistics, or equivalent and/or appropriate experience

EXPERIENCE:

3+ years of experience working with distributed data technologies
Experience working with server-side concepts such as containers, micro-services, caching, performance monitoring, and API design
Experience with cloud technologies such as AWS, Azure, and Google Cloud
Experience using Python, preferred
Experience with databases such as MySQL, and PostgreSQL, preferred
Experience with highly scalable ETL/ELT/Data Lake technologies, nice to have

OUR VALUES
Be honest. We do the right thing because it’s right.

Have passion for building something great. We empower employees to “think like an owner” so we dare to try. Let’s find new ways to grow B-Stock together.

Humor. Take whatever you are doing very seriously but do not take yourself too seriously.

Teamwork. Our successes are achieved because we work in stride, leveraging each other’s strengths as a unified effort.

Respect. We show consideration for each other and recognize the power in our diversity.

EMPLOYEE BENEFITS

Competitive compensation packages including bonus and options
Medical, dental, and vision benefits
Paid Time Off, telecommuting and flexible schedule options
Support for continuing education
Team off-sites, social events and extracurricular activities are a staple
Snacks, drinks, and the occasional box of donuts

No applicant will face discrimination/harassment based on: race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status. Above and beyond discrimination/harassment based on “protected categories,” B-Stock also strives to prevent other, subtler forms of inappropriate behavior (e.g., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place at B-Stock.

US Work Authorization required."
105,Data Engineer,Grant Street Group,,Remote,"Are you ready to build a foundation of data-driven decision-making for a growing company and its clients? Grant Street Group is looking for a talented data engineer to join our team.

We are a privately held software company that supports a variety of clients in government and finance. Our web applications support tax collection, electronic payments, bond auctions and more.

If the following describes you, we want to hear from you!

You get things done: You will help our clients and internal users by adding new data sources to our data warehouse. You excel at reverse-engineering new data sources, and ingesting the data into our data warehouse, while still following ETL best practices.

You’re quick to jump in, and you love to troubleshoot: You’ll investigate when something goes wrong with our data replicators. You won’t stop until you’ve found the root cause and fixed it. You don’t mind occasional support calls at night or on weekends (typically fewer than 3 calls a week shared between two engineers) if it keeps our clients up and running.

You’re lazy, but in a good way: Your answer to support calls is to bulletproof your code. Your answer to repetitive tasks is to automate them. You write good code, but only if you can’t use someone else’s good code to get the job done.

You love working in a team: You will join a small team that manages the entire data lifecycle. You’ll collaborate with our clients and product teams to provide fresh, accurate data to our users.

You may be the perfect fit if you have these skills and knowledge:

ETL expertise
Experience with data warehouse administration
Strong SQL skills
Basic Linux knowledge
Basic knowledge of a scripting language (Python, Perl…)
If you have any of the following, that would be a plus:

Data analysis or business intelligence expertise
Experience with Apache NiFi
Programming experience in Go
Experience with Exasol or Microstrategy
This position is open to telecommuters. There is minimal travel: 2-4 weeks per year for on-site meetings in Pittsburgh. If you live (or want to) in Pittsburgh, you can work in our beautiful headquarters atop the Heinz building.

Our extraordinary company culture is the foundation of our unmatched customer service — just ask our clients! We reward teamwork, professional excellence, and individual responsibility. Using the best collaboration tools available, we offer a technology-rich work environment that makes it possible for us to support on-site and telecommuting positions tailored to the needs of our employees. If you are passionate about your work, you have entrepreneurial spirit and you want to be on a team of exceptional professionals, then maybe Grant Street is for you!"
106,Data Engineer,Zoetis,,"Union City, CA","Position Summary:
Zoetis has created a new Data and Analytics Center of Excellence (COE) that is driving a step change in the company’s ability to harness data, particularly data generated by the expanding portfolio of Digital Products, to generate disruptive data insights for our customers and colleagues. As the world leader in animal health, Zoetis seeks to integrate Data and Analytics with its diverse portfolio of animal health products to create additional value to customers and enhance overall customer experience.
In your new role within the Data and Analytics COE, you will have the opportunity to address greenfield opportunities to apply IoT, data analytics, data science and machine learning in the animal health space. Our view of analytics spans from the sensor data collection and analysis on the edge, to the delivery of data insights that help optimize and, in some cases, automate the decision-making process. You will partner with world-class experts in animal health to develop innovative digital and analytics products. The COE team is leveraging the best-in-class analytics and IoT tools and techniques, and you will have the opportunity to hone you experience with them and help evolve our application of them.
Responsibilities:
Implement both batch and streaming workflows that combine, transform and enrich data, utilizing Microsoft Azure and Zoetis’ own Data and Analytics Platform
Analyze and document the end-user requirements for analytical datasets; provide high-level design and solution concepts that address the need
Collaborate with source system experts to understand the nuances surrounding the data, including the underlying business process that generate the data and known quality issues
Contribute to the evolution of the Zoetis’ Dimensional Data Model by onboarding the new business processes per Kimball methodology, and by adding new dimension features
Become an expert on Zoetis’ Dimensional Data Model; provide advice and consulting to the various analytical users of the Model
Test and validate data workflows against real-world data (scope, volume, etc.)
Monitor the execution performance of batch workflows to identify scalability issues and opportunities for improving performance
Re-implement existing non-scalable workflows as horizontally-scalable versions
Develop and maintain the ability to implement data workflows in a variety of tools that balance the speed of development, understandability, performance and scalability
Assist Data Consultant / Business Intelligence specialists with designing analytical datasets that meet specific analytical product requirements (dashboards, reports, etc.)
Assist Data Scientists with designing data input and outputs for analytical modules (predictive, machine learning, etc.) and implementing these modules as another type of data workflow
Knowledge, Skills, Ability Requirements:
Experience with developing data workflows in multiple tools and languages (SQL, Spark, Map/Reduce, traditional ETL tools, Alteryx, etc.)
Experience with at least one scripting or programming language (Python, R, Java, C#, etc.)
Experience with the SQL language, particularly complex SELECT statements
Demonstrated subject matter competency and fluency (from the data perspective) in at least one functional area of business
Familiarity with a public cloud platform, preferably Microsoft Azure
Experience with the dimensional modeling approach (Kimball), from both consumption / use and design perspectives
Ability to design 3NF data models and/or UML class diagrams based on a statement of product requirements and interviews with subject matter experts
Experience with Alteryx is a plus
Experience with at least one popular self-service BI tools such as Tableau, Power BI, Qlik, etc.
Excellent critical thinking and reasoning skills particularly as applied to tracing data issues
Passion for solving complex problems and making a difference
Ability to drive a project and work both independently and in a team
Excellent verbal and written communication skills, which includes the ability to present complex topics to non-technical audiences
Qualification (Training, Education & Prior Experience)
Education:
Bachelor’s degree in Computer Science, Computer Engineering, Electrical Engineering, or a related field
Experience:
Minimum 4 years of experience in a closely related field and role
Full time
Regular
Colleague
Zoetis is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status or any other protected classification. Disabled individuals are given an equal opportunity to use our online application system. We offer reasonable accommodations as an alternative if requested by an individual with a disability. Please contact Zoetis Colleague Services at zoetiscolleagueservices@zoetis.com to request an accommodation. Zoetis also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as employment eligibility verification requirements of the Immigration and Nationality Act. All applicants must possess or obtain authorization to work in the US for Zoetis. Zoetis retains sole and exclusive discretion to pursue sponsorship for the acquisition or maintenance of nonimmigrant status and employment eligibility, considering factors such as availability of qualified US workers. Individuals requiring sponsorship must disclose this fact. Please note that Zoetis seeks information related to job applications from candidates for jobs in the U.S. solely via the following: (1) our company website at www.Zoetis.com/careers site, or (2) via email to/from addresses using only the Zoetis domain of “@zoetis.com”. In addition, Zoetis does not use Google Hangout for any recruitment related activities. Any solicitation or request for information related to job applications with Zoetis via any other means and/or utilizing email addresses with any other domain should be disregarded. In addition, Zoetis will never ask candidates to make any type of personal financial investment related to gaining employment with Zoetis."
107,Data Engineer,The J. Paul Getty Trust,,"Los Angeles, CA 90049","Job Summary
The J. Paul Getty Trust is looking for an enthusiastic Data Engineer, with the experience and passion to carry out the execution of technical projects to support, enrich and ensure the persistence of the institution's cultural heritage knowledge bases. Our aim is to provide a deeply connected and consistent experience for scholars, researchers, and enthusiasts as they explore the complex information held across the organization, and your participation is crucial for that to be successful.

You will report to the Enterprise Semantic Architect, and interact with software engineers, data engineers and content specialists in the cultural heritage programs. Your work will improve the quality, reliability, connectedness, and consistency of our data by engineering project-specific data pipelines and validation tools, configuring Linked Open Usable Data (LOUD) platforms such as Arches and assisting with the implementation of our, and the community's, overall data model. You will have a hands-on role with content specialists in the programs, and be responsible for working with them to understand data requirements and then implement those requirements in software.

The Getty is among the most prestigious cultural heritage organizations in the world, dedicated to furthering the study of the history of art. You will work on an amazing campus amongst fabulous art, architecture, and information systems, collaborating with world-class scientists, curators, librarians, archivists, and academics. We offer paid vacation, personal and sick leave plus every other Friday off, excellent benefits, and a very strong commitment to balancing work and personal life.
Major Job Responsibilities
With the Semantic Architect, work with technical and content stakeholders to understand data-oriented project requirements
With the Semantic Architect, design and document the institution's data model and resulting APIs
With other Data Engineers, ensure the accuracy of data transformation pipelines to migrate legacy datasets into Linked Open Usable Data (LOUD) within our ecosystem
With other Data Engineers, design, implement and ensure the accuracy of validation and related services for data models
Integrate external content services to enrich and reconcile our data
Work in an agile way, including supporting testing, continuous integration and deployment
Configure institutional LOUD data management instances built on the Arches Platform
Assist software engineering teams by translating stakeholder requirements into feature requests
Qualifications
Bachelor's degree in a related field or a combination of education and relevant experience
2-5 years software development experience
Knowledge, Skills and Abilities
Experience of data-oriented work within cultural heritage organizations, including transformation of JSON, CSV and/or XML formats
Attention to detail combined with a focus on usability
Excellent verbal and written communication skills, especially when interacting with non-technical stakeholders
Proficiency in Python, or willingness to translate experience in equivalent language
Proficiency in relational and document oriented databases
Familiarity with Linked Open Data standards and technologies
Familiarity with cultural heritage data standards
Familiarity with engineering tools such as git and docker
Familiarity with test driven and agile software development methodologies
Familiarity with machine learning techniques"
108,Data Engineer II,Premera Blue Cross,,"Mountlake Terrace, WA","Join Our Team: Do Meaningful Work and Improve People’s Lives

Our purpose, to improve customers’ lives by making healthcare work better, is far from ordinary. And so are our employees. Working at Premera means you have the opportunity to drive real change by transforming healthcare.

To better serve our customers, we’re creating a culture that promotes employee growth, collaborative innovation, and inspired leadership. We are committed to creating an environment where employees can do their best work and where best-in-class talent comes, stays, and thrives!
Data powers all the decisions we make at Premera. It is the ‘secret sauce’ to help and improve the health of people we serve. The combination of the health industry standards and the fast-growing data sources makes data flow at Premera complex and challenging.
The Corporate Data and Analytics team is looking for a Data Engineer II to help create, modify, and test the code, forms, and scripts that construct the data sets that drive our Data Science and Business Intelligence capabilities. This work is derived from specifications developed in collaboration with those consuming or interacting with those data sets. You will work to develop and write solutions to store, locate, and retrieve specific documents, data, and information and distribute through multiple methods. They will perform ETL (extract, transform, load) processes. As they continue to build knowledge and expertise in this area, they will follow professional standards and best practices as well as departmental guidelines to independently solve problems of moderate scope and complexity.
What you'll Do:
Solve business and data science problems using data centric programming and scripting skills to create data models and pipelines.
Work closely with the business to create understanding of the needs, pace and direction for our business partners. Translate these needs into requirements and specifications and maintain contact with the customers throughout project completion.
Troubleshoot issues as they arise and solve problems independently and collaboratively.
Collaborate with data scientists and other analysts to further understand business problems.
Develop code to complete effective solutions using applicable technology.
Perform thorough peer design and code reviews
Use developing data ETL experience to develop data pipelines to support data product automation.
Other duties as assigned.
What you'll Bring:
Bachelor’s degree in computer science, computer engineering, or similar area and two-five (2-5) years’ experience in data integration, design and management.
A knowledge of software development lifecycle, relational database theory, and skills to utilize one or more programming languages.
At a minimum, candidates must possess the equivalent in education, experience, and skills of a bachelor’s degree in a related field and two (2) years’ relevant experience.
Preferred Qualifications:
Familiarity with healthcare specific regulatory requirements for data management.
Experience providing data integration services within healthcare organizations.
Knowledge of Tableau, SAS, R, and other analytic tools.
Knowledge, Skills and Abilities:
Good problem definition, analytical, problem solving skills, and technical writing skills.
Strong data processing programming skills across SQL-based and Hadoop-based technologies.
Good written and verbal communication skills and ability to deal with management of various levels and communicate information and ideas in writing so others will understand.
Knowledge of Agile and Scrum project methodologies utilizing TFS, Jira/Confluence.
Ability to use Extract Transform Load (ETL) tools (SSIS, Data Stage, Cask)
Ability to use Kimball methodology for dimensional data modeling, 3rd Normal form DW.
What we offer
Medical, vision and dental coverage
Life and disability insurance
Retirement programs (401K employer match and pension plan)
Wellness incentives, onsite services, a discount program and more
Tuition assistance for undergraduate and graduate degrees
Generous Paid Time Off to reenergize
Free parking
Equal employment opportunity/affirmative action:
Premera is an equal opportunity/affirmative action employer. Premera seeks to attract and retain the most qualified individuals without regard to race, color, religion, sex, national origin, age, disability, marital status, veteran status, gender or gender identity, sexual orientation, genetic information or any other protected characteristic under applicable law.
If you need an accommodation to apply online for positions at Premera, please contact Premera Human Resources via email at careers@premera.com or via phone at 425-918-4785."
109,Data Engineer,"Cotiviti, Inc.",,"Atlanta, GA","PRIMARY RESPONSIBILITIES
Build and operate stable, scalable data pipelines that cleanse, structure and integrate disparate big data sets into accessible format to support enterprise-level reporting.
Identify data requirements to integrate new data sources. Perform technical analytics to confirm data integration meets or exceeds defined expectations.
Maintain Tableau Server-based data source repository which serves as primary reporting layer for front-end developers.
Solve complex technical problems and mentor other technical staff on data modeling and ETL related issues.
Participate in requirement gathering efforts and write low-level and high-level specifications.
Analyze existing procedures to identify system/process changes needed to meet such requirements including but not limited to streamlining processes and quality assurance.
Ensure that consistent documentation is developed and actively maintained throughout all phases of work.
Supports development of other analysts on the team by sharing and training on best practices.
Assist in the successful completion of deliverables and ensure all requirements are accurately met.

MINIMUM QUALIFICATIONS
4+ years of experience with data warehouse technical architectures, ETL/ELT, reporting/analytic tools and scripting.
2+ years experience supporting Data Warehouse operations at a mid- to large-sized organization (Healthcare highly preferred)
Advanced knowledge and expertise with data modelling, data aggregation, standardization, linking, quality check mechanisms and reporting.
Experience in the analysis, design and development of solutions and strategies for creating extraction, transformation and loading (ETL) and real-time applications.
Experience with RDBMS (SQL Server, Oracle, ) and using T-SQL or other data integration/ETL tools such as SSIS. Understanding of new technologies like HDFS and NOSQL a plus.
Experience implementing and supporting SQL Server auditing features (Change Tracking, Change Data Capture and SQL Server Auditing) to drive ETL operations in complex data warehouse environments.
Experience with reporting/visualization tools (Tableau, MicroStrategy, etc.)
Experience with ticketing and documentation tools such as Jira.
Strong knowledge of coding and ability to follow best practices while developing and deploying code.
Must be highly analytical, well-organized and possess strong attention to detail.
Bachelor’s degree in relevant field such as Computer Science, Engineering, a related field, or equivalent experience.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information."
110,Data Engineer,CBS,,"San Francisco, CA 94103","Data Engineer
REF#: 34423
CBS BUSINESS UNIT: CBS Interactive
JOB TYPE: Full-Time Staff
JOB SCHEDULE:
JOB LOCATION: San Francisco, CA
ABOUT US:
Truly premium content. At true scale. Only CBSi. CBS Interactive is the premier online content network for information and online operations of CBS Corporation as well as some of the top native digital brands in the entertainment industry. Our brands dive deep into the things people care about across entertainment, technology, news, games, business and sports. With over 1 billion users visiting our properties every quarter, we are a global top 10 web property and one of the largest premium content networks online.
Check us out on [1] The Muse, [2] Instagram and [3] YouTube for an inside look into 'Life At CBSi' through employee testimonials, office photos and company updates.
References
Visible links
https://www.themuse.com/companies/cbsinteractive
https://www.instagram.com/cbsinteractive/?hl=en
https://www.youtube.com/channel/UCAvGapyifCtUlmNTagAl_sQ
DESCRIPTION:
Division Overview:
Our team is a diverse and agile group of engineers that run data operations for CNET Media Group Business Intelligence team. We are responsible for developing tagging, data pipelines and data products to drive user growth, engagement and revenue opportunities. This position will focus primarily on CBS Interactive’s CNET Media Group properties, including CNET, GameSpot, TVGuide, ZDNet and Techrepublic among others.
Role Details:
As a Data Engineer, you'll be working on developing workflows to ingest, store, and process data leveraging Google Cloud Platform products and services with emphasis on scalability and reliability. You will be working closely with Business Intelligence, Product, Revenue Optimization and other Engineering teams to build and enhance our BigQuery Data Warehouse. This role provides opportunity to work on the latest cloud and open source technologies to develop and evolve our data analytics platform.
Your Day-to-Day:
Collaborate with stakeholders to understand data needs and provide end-end data solutions
Develop and enhance ELT/ETL pipelines to ensure data availability and data quality
Create new data models to support data products and intuitive analytics
Use your Python and SQL coding skills to process and transform data
Research and promote data engineering best practices
Key Projects:
Migration of existing on-prem data pipelines to cloud
Develop and enhance internal revenue data models and pipelines
Explore usage of new GCP data products and features (Data Catalog, BI Engine, BigQuery ML)
QUALIFICATIONS:
What you bring to the team:
You have -
Bachelor's degree in Computer Science or equivalent experience in a related field
3+ years of hands-on experience working in data warehousing or data engineering environment
Strong Python and SQL programming skills
2+ years experience developing data solutions on GCP or AWS
Strong experience in authoring, scheduling and monitoring of workflows (Airflow, Luigi)
Experience in ingestion of data from external APIs and data stores
Experience in design, build and operationalization of big data pipelines on distributed processing back-ends (Cloud Dataflow, Spark, Flink)
Strong communication & interpersonal skills
Can-do attitude on problem solving, quality and ability to execute
You might also have -
Experience implementing best practices for monitoring, alerting, and metadata management
Knowledge of Git, Jinja2, Docker, Bitbucket, and Bamboo
Google Cloud Certified - Professional Data Engineer certification would be a plus
EEO STATEMENT:
Equal Opportunity Employer Minorities/Women/Veterans/Disabled"
111,Distinguished Data Engineer,Capital One - US,,"New York, NY 10011","114 5th Ave (22114), United States of America, New York, New York

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Distinguished Data Engineer

At Capital One, we believe in the values of Excellence and Doing the Right Thing. We are a technology-oriented company delivering financial products to market through modern technology and constant innovation at a massive scale.

Distinguished Engineers are...

Deep technical experts, senior-level individual contributors and thought leaders that help accelerate adoption of the very best engineering practices, while maintaining knowledge on industry innovations, trends and practices

Visionaries, helping solve Capital One’s toughest technology challenges, to deliver on business needs that directly impact the lives of millions of our customers and associates

Role models and mentors, helping to coach and strengthen the technical expertise and know-how of our engineering and product community

Supporters, both internally and externally, helping to elevate the Distinguished Engineering community and establish themselves as a go-to resource on given technologies and technology-enabled capabilities

Leaders who gain the trust and confidence of those around them, from hands on engineers to executives

Whether a member of our engineering or architecture teams, Distinguished Engineers are individual contributors expected to solve problems in a fast-paced, collaborative, and iterative delivery environment. In order to meet these demands, candidates should be influential engineering leaders with deep technology expertise, and a collaborative style that brings others into the decision-making process. S/He will significantly impact the Tech agenda within their organization and devise clear roadmaps to deliver next generation technology solutions across organizational boundaries.

Responsibilities:
Leverage sound judgment and problem solving to tackle some of Capital One’s most critical problems and connect the dots to broader implications of the work

Build awareness, increase knowledge and drive adoption of modern technologies and architecture patterns, sharing customer and engineering benefits to gain buy-in

Strike the right balance between lending expertise and providing an inclusive environment where others’ ideas can be heard and championed; leverage expertise to grow skills in the broader Capital One team

Promote a culture of engineering excellence and being well-managed, using opportunities to reuse and innersource solutions where possible

Effectively communicate with and influence key stakeholders across the enterprise, at all levels of the organization

Operate as a trusted advisor for a specific technology, platform or capability domain, helping to shape use cases and implementation in an integrated manner

Lead the way in creating next-generation talent for Tech, mentoring internal talent and actively recruiting external talent to bolster Capital One’s Tech talent

Integrate the modern core with key financial systems

Build a powerful set of APIs and event streams that create highly distributed, massively scalable “virtual core” allowing vital financial systems to easily interact with modern and legacy ledgers

Operate in a cutting edge cloud-native, microservices-driven architecture

Basic Qualifications:
Bachelor’s Degree

At least 10 years’ of enterprise software engineering experience

At least 5 years' of data engineering experience

Preferred Qualifications:
Masters’ Degree or higher in computer science or a related field

7+ years of building large scale data systems

5+ years of building web APIs for high-throughput systems

5+ years of writing back-end systems in open source languages such as Java and Go

5+ years of using AWS ecosystem of tools

3+ years building large data sets in Cassandra

2+ years of experience with Kafka

2+ years of building microservices deployed as containers, preferably on Kubernetes

Capital One will consider sponsoring a new qualified applicant for employment authorization for this position."
112,Data Engineer,Acumen LLC,,"Burlingame, CA 94010","Are you someone who enjoys working with data? Are you a self-motivated thinker who wants to make an impact in the fast-growing healthcare data industry?

We are looking for a Data Engineer who highly values research, wants to work with multifaceted datasets, and craves new challenges in programming. As a Data Engineer, you will have the opportunity to gain first-hand experience integrating and structuring the healthcare data that shapes policy on many key topics, such as the Affordable Care Act, Medicare, and Medicaid. You will also have the chance to work closely with seasoned programmers, developing the skills to work with data management tools and various programming languages. In addition, you will work alongside smart, vibrant people with a passion for the exciting future of healthcare research.

The Data Engineer will:

Extract, transform, and load (ETL) big data.
Develop complex data processing algorithms that combine multiple data sources, while optimizing run-time efficiency.
Develop data structures, databases, and querying programs which facilitate efficient data access.
Develop data structures from claims and enrollment data which support research and analytic activities of in-house analysts as well as congressional and federal agencies.
Ensure data inventory is complete and accurate through application design, including fault analysis and detection, quality control, and the development of tracking systems.
Collaborate with other Data Engineers and in-house researchers to maintain systems, produce documentation, and educate internal and external users about company resources.
Perform validation checks across multiple sources to verify data integrity as needed.
Perform other duties and responsibilities as assigned.


Required Skills
Qualifications Required

A Bachelor’s in Computer Science, Statistics, Mathematics, Operations Research, Economics, Public Health, or related field with quantitative emphasis
Strong organizational, planning, and problem solving skills
Team player with strong interpersonal skills
Excellent written and oral communication skills
Familiarity with one or more computer programming languages
Qualifications Desired

Master’s in Information Management Systems, Statistics, Mathematics, Operations Research, Economics, Public Health, a related field with quantitative emphasis, or 2+ years of work experience in a field with quantitative emphasis
Interest in big data
Interest in making an impact in the field of healthcare policy research
Previous experience in a Data Analyst/Data Engineer position
1+ years of experience working with programming languages such as SAS, SQL, Python, or R
1+ years of experience working with databases or data pipelining tools
Please submit a cover letter and resume to be considered for this position.

Due to the sensitive nature of much of our work, all Acumen employees must undergo a background check. Your employment will be contingent upon your completing, and Acumen reviewing to its satisfaction, a mandatory background check.
Employees who work with particularly sensitive information may be asked to undergo an additional background check after starting work.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, physical or mental handicap, disability, age or status as a disabled veteran, or veteran of the Vietnam era.
Required Experience"
113,Data Engineer,Sony Music Entertainment,,"New York, NY 10010","Overview
Sony Music Entertainment is a global recorded music company with a roster of current artists that includes a broad array of both local artists and international superstars, as well as a vast catalog that comprises some of the most important recordings in history. Sony Music Entertainment is a wholly owned subsidiary of Sony Corporation of America.

Sony Music is committed to providing equal employment opportunity for all persons regardless of age, disability, national origin, race, color, religion, sex, sexual orientation, gender, gender identity or expression, pregnancy, veteran or military status, marital and civil partnership/union status, alienage or citizenship status, creed, genetic information or any other status protected by applicable federal, state, or local law.

The Data Engineer will report to the Head of A&R Research. This role will explore and build products using the latest and greatest in Data Analytics, Cognitive Services, Machine Learning and more. This role is based at Sony Music’s offices in New York
Responsibilities
Create and maintain systems to load and transform very large data sets from digital media retailers (iTunes, Spotify, YouTube, etc) as well as social media sources.
Work with a cross-functional team to create data-driven insights and reports for business stakeholders.
Work with other members of the team to create customer-facing analytics tools and visualizations.
Process millions of rows of data daily to provide analytics to our end users.
Take advantage of our continuous integration and deployment.
Participate in technical design and peer review for new projects.
Qualifications
Experience using Snowflake and Google Cloud Platform preferred.
Experience with AWS ecosystem. Some preferred services are Redshift, RDS, S3, and SWF.
Proven experience with ETL frameworks (Airflow, Luigi, or our own open sourced garcon ).
Expertise with at least one distributed data stores (Redshift, Cassandra, Snowflake).
Familiarity with noSQL technologies (mongoDB, DynamoDB).
Proficient in scripting language of choice. Python is strongly preferred, PHP a plus.
Highly proficient in writing SQL for a relational datastore (MySQL, PostgreSQL).
Knowledge of technologies that can deal with Big Data is a Big Plus (Kafka, Spark, Hive, Hadoop/MapReduce).
Ability to write automated tests (unit, functional, and integration) to ensure code works as expected.
Desire to collaborate with other engineers through peer code reviews.
Deep understanding of data structures and schema design.
Detail-oriented, proactive problem solving skills."
114,Data Engineer,Nevro Corporation,,"Redwood City, CA","About Nevro
Nevro (NYSE: NVRO) is a public multinational medical technology company headquartered in Redwood City, California. We have developed HF10™ therapy, an innovative, evidence-based neuromodulation platform. We started with a simple mission to help more patients suffering from chronic pain. At each stage of development, our research was subject to the highest levels of scientific rigor, resulting in a new therapy that has impacted the lives of over 45,000 patients around the world. The Nevro® Senza® SCS System received CE mark in 2010, TGA approval in 2011, FDA approval in 2015, and is commercially available in Europe, Australia, and the United States.
Job Summary & Responsibilities
The Data Engineer shall be a foundational member in research and product development team to collaborate on the design and implementation of improving and automating clinical data to analyze, identify and report data and trends.
Use data engineering expertise to design and build solutions/products for analyzing data collection from various data sources.
Create data tools for analytics and technical team members to assist them in building and optimizing the product.
Conduct advanced statistical analysis to determine trends and significant data relationships.
Work with data and machine learning experts to strive for greater functionality in our data and model life cycle management systems.
Other duties as assigned
Role Requirements
Experience with relational SQL and NoSQL databases required
BS degree in Computer Science, Informatics, Information Systems or another related field and a minimum of 2 years’ experience in AWS, Python required
Years of experience can be substitute for the education
Skills and Knowledge
Proficient in data visualization skills with technologies such as PowerBI, Tableau
Experience with Data and Model pipeline and workflow management tools
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Have built processes supporting data transformation, data structures, metadata, dependency and workload management.
Nice to have skill would include: experience with RESTful APIs and cross-platform development
Attention to detail and organization/ documentation skills
Able to work effectively under pressure, independently, and within a collaborative team-oriented environment using sound judgment in decision making
Strong analytic skills related to working with unstructured datasets.
Project management and interpersonal skills.
Experience supporting and working with cross-functional teams in a dynamic environment
LI-TS1"
115,Data Engineer,AllianceChicago,,"Chicago, IL 60654","Job Title: Data Engineer Reports To:
Deputy Director/Chief Informatics
Officer
Location: Chicago Travel Required:
Level/Salary Range: DOE Position Type: Exempt
HR Contact: Claudria Hurt Date Posted: 07/31/2019
External Posting URL:
Applications Accepted By:
Email: careers@alliancechicago.org or Fax: 312.274.0069
Subject Line: Data Engineer
Job Description:
Position Overview:
Responsible for building out and migrating the AllianceChicago data warehouse to the Health Catalyst platform,
and then expanding and maintaining the warehouse. This position has primary responsibility for the extraction of
health information from diverse clinical and business sources and its transformation into objects in a standardized
data model. These objects will be the primary source for quality and performance reporting activities for
AllianceChicago, its members, stakeholders and user community or primary care delivery sites. The platform also
supports research and grant reporting needs, queries for public health surveillance, and other deliverables. The
data extraction work involves design, construction, and maintenance of modules using the Health Catalyst toolset.

The work provides the data for analysists and presentation specialists working in Power BI, SQL, SSRS, Excel and
Health Catalyst presentation tools, and benefits from familiarity with these technologies. Overall the position
contributes to the design, build, testing, and maintenance of the AllianceChicago’s data warehouse.
Essential Duties:
Overseeing the initial load to Health Catalyst data lake, a process that will be staffed by Health Catalyst
Working with AllianceChicago subject matter experts to design the rules to transform data lake data into
standardized models of health data (Health Catalysts DOS environment) including encounters, problems,
screenings, interventions, tests, and medications
Acquiring from Health Catalyst the knowledge to implement the design of the data model
Manage the transition and go-live from the current data warehouse environment based on Microsoft
APS/PDW and SSIS extraction to the Health Catalyst environment
Maintenance of the extract and transformation process
Supports Alliance SQL query analysts, report writers and Alliance partners who perform data analysis
from the DOS model or the data lake
Works with query writers and report designers to resolves data issues with current reports.
Is a member of the Alliance Data Warehouse team, and works collaboratively with its members and other
stakeholders
Oversees bringing new source systems into the Catalyst’s Data Operating System
Ability to dig into the data and understand business logic within the source system data
Perform data validation tests to ensure extractions and transformations are true to the source
Required Skills:
Intermediate to advanced level in Structured Query Language (SQL)
Experience working with EMR\EHR systems and an understanding of the healthcare clinical domain
Exposure to Extract, Transform and Load (ETL) concepts and processes
Excellent analytical and troubleshooting skills
Working knowledge of database principles, processes, technologies and tools
Other Requirements:
Flexibility and ability to work with individuals of diverse backgrounds and educational levels
Ability to function in a collaborative and collegial environment as a team player.
Ability to coordinate and communicate effectively with other team members
Ability to generate trust and build alliances with coworkers
Self-motivated; comfortable working under general direction
Strong sense of customer service to consistently and effectively address client needs
Ability to work in variety of settings
Detail oriented; highly organized; ability to prioritize and set expectations
Strong technical writing skills and written communications skills
Strong Analytical skills
Ability to work independently to organize work in a manner that ensures accuracy and efficiency
Familiarity with agile development process


Education/Training/Expertise:

Bachelor Level preparation in computer, mathematical, information sciences or equivalent training –
Master Level Preferred
Experience/Years:

At least three years of experience with Microsoft SQL Server 2008 or higher including SSIS and SSAS,
and familiarity with the MS BI stack. Familiarity with other data, analytic and reporting tools
5-8 years of experience in computer science or information science related field
Experience with Relational Databases and data mining

Working Conditions:
General office setting, extensive telephone and desk work at computer terminal
May be required to lift, carry, bend, reach and stand with parcels up to 25 lbs.
Will work in a close multidisciplinary team environment
May interface with clients in various settings and may be working during on-site visits in clinical
environments where medical equipment, chemicals and where communicable diseases and certain
pathogens are present.

ORGANIZATIONAL OVERVIEW:
Founded by four partner Community Health Centers in 1997, AllianceChicago’s three core areas of focus are
Health Care Collaboration, Health Information Technology, and Health Research & Education. AllianceChicago
supports the use of HIT to improve quality, efficiency, and access to services in a national network of community
Safety Net health care organizations. The mission of AllianceChicago is to improve personal, community, and
public health through innovative collaboration.

ADA Statement: The Americans with Disabilities Act prohibits discrimination and ensures equal opportunity for
persons with disabilities in employment, state and local government services, public accommodations, commercial
facilities, and transportation. It also mandates the establishment of TDD/telephone relay services.

EEO Statement: AllianceChicago believes that all applicants and employees are entitled to equal employment
opportunities and maintains a policy of non‐discrimination with respect to religion, color, sex, sexual orientation,
national origin, age, veteran status, marital status, physical or mental disability, or any other legally protected class
in accordance with applicable law, except where a bona fide occupational qualification exists. AllianceChicago will
comply with all phases of employment including, but not limited to, hiring practices, transfers, promotions, benefits,
discipline, and discharge.


Disclaimer: The above statements are intended to describe the general nature and level of work being performed
by employees assigned to this position. They are not intended to be construed as an exhaustive list of all
responsibilities, duties, and skills required of personnel as qualified."
116,Big Data Engineer,Virtusa,,"Fremont, CA","Primary Location: US-CA-Fremont
Schedule: Full Time
Job Type: Experienced
Travel: No
Job Posting: 05/06/2019, 3:54:04 PM"
117,Data Engineer II,Amazon Services LLC,,"Seattle, WA","A desire to work in a collaborative, intellectually curious environment.Degree in Computer Science, Engineering, Mathematics, or a related field and 4-5+ years industry experienceOne year of experience in the following skill(s):Developing and operating large-scale data structures for business intelligence analytics using: ETL/ELT processes; OLAP technologies; data modeling; SQL;Experience with at least one relational database technology such as Redshift, Oracle, MySQL or MS SQLExperience with at least one massively parallel processing data technology such as Redshift, Teradata, Netezza, Spark, or Hadoop based big data solutionsCoding proficiency in at least one modern programming language (Python, Ruby, Java, etc)Experience in gathering requirements and formulating business metrics for reporting

HS3C-Compliance is responsible for keeping our Customers and partners safe, and ensuring we maintain WW compliance. We build scalable solutions that grow with the Amazon business. HS3C-Compliance team collects petabytes of data from thousands of data sources inside and outside Amazon including the Amazon catalog system, inventory system, customer order system, and page views on the website. We provide interfaces for our internal customers to access and query the data hundreds of thousands of times per day, using Amazon Web Service’s (AWS) Redshift, Hive, and Spark.
HS3C-Compliance is growing, and the data processing landscape is shifting. Our data is consumed by teams across HS3C including Research Scientists, Machine Learning Specialists, Business Analysts, and Data Engineers. We are seeking an outstanding Data Engineer to join the HS3C-Compliance data technologies team. The HS3C-Compliance data technologies team manages the core HS3C business data from hundreds of source systems. Amazon has culture of data-driven decision-making, and demands business intelligence that is timely, accurate, and actionable. If you join the HS3C-Compliance data technologies team, your work will have an immediate influence on day-to-day decision making at Amazon.

As an Amazon Data Engineer II, you will be working in one of the world's largest cloud-based data lakes. You should be skilled in the architecture of data warehouse solutions for the Enterprise using multiple platforms (EMR, RDBMS, Columnar, Cloud). You should have extensive experience in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.

As a Data Engineer II on the HS3C-Compliance data technologies team, you will design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for analytics and deep learning. Implement data ingestion routines both real time and batch using best practices in data modeling, ETL/ELT processes leveraging AWS technologies and Big data tools. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture. Analyze source data systems and drive best practices in source teams. Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance. Produce comprehensive, usable dataset documentation and metadata. Evaluate and make decisions around dataset implementations designed and proposed by peer data engineers. Evaluate and make decisions around the use of new or existing software products and tools. Mentor junior data engineers.

Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data setsExperience building data products incrementally and integrating and managing datasets from multiple sourcesQuery performance tuning skills using Unix profiling tools and SQLExperience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologiesExperience providing technical leadership and mentor other engineers for the best practices on the data engineering spaceLinux/UNIX including to process large data sets.Experience with AWS"
118,AWS Data Engineer,Accenture,,"Palo Alto, CA","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the AWS Native and Hadoop
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
§ Certified AWS Developer - Associate
§ Certified AWS DevOps – Professional (Nice to have)
§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an AWS platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, Matillion
Strong in Java, C##, Spark, PySpark, Unix shell/Perl scripting
IoT, event-driven, microservices, containers/Kubernetes in the cloud

Experience in Apache Maven a plus
Understanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
119,Data Engineer,"PennyMac Loan Services, LLC",,"Agoura Hills, CA","The Data Engineer will work with the data Analyst and project managers to determine logical and physical database designs for new analytics models. The Data Engineer will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The Data Engineer will support database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout analytics ongoing projects.
Job Description
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet business requirements
Build analytics tools that utilize the data pipeline to provide actionable insights into operational efficiency, financial reports and other key business performance metrics
Work with stakeholders including the Engineering and Analytic teams to assist with data-related technical issues and support their data infrastructure needs
Create data tools to support business informational technology process
Work with the data analytics team to strive for greater functionality in our data systems
Perform other related duties as required and assigned
Demonstrate behaviors which are aligned with the organization’s desired culture and values
Ideal Candidate will have the following:
Moderate experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Moderate knowledge of AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, C++
Experience building and optimizing AWS data pipelines, architectures and data sets.
Strong project management and organizational skills.
Moderate skill in business intelligence tools such as Tableau or Qlik
Moderate skills with MS Office, including Excel & PowerPoint
Must be a team player with strong attention to detail and able to work independently
Proven track record at delivering timely and accurate information in a fast-paced environment
Excellent critical thinking, problem solving, and mathematical skills, and sound judgment
Strong business acumen and ability to interface with executive management"
120,SQA Data Engineer,GreatCall,,"San Diego, CA 92130","POSITION: SQA Data Engineer

DEPARTMENT: Network Technology & Data Services

REPORTING SUPERVISOR: Solutions Architect, Data Services

DIRECT REPORTS: No

FLSA: Non-Exempt

EMPLOYMENT STATUS: Direct Hire

TRAVEL REQUIREMENTS: N/A

ABOUT THE TEAM:

The Data Services team mission is to unify data across the enterprise to optimize the business decisions made at the strategic, tactical, and operational levels of the organization. We accomplish this by providing an Enterprise Data Warehouse, reporting platform, and business processes that can provide quality data, in a timely fashion, from any channel of the company and present it in such a manner to maximize the value of that data for both internal and external customers.

ABOUT THE JOB:

The Data Engineer (Quality) is responsible for designing, developing, and executing test plans for automated ETL processes. This role will be part of a team adopting and implementing new data technologies and tools like Snowflake, Spark, Hadoop, HDFS, MongoDB and/or others to provide faster, more accurate, and deeper insights into our products and services. As a member of the growing data services and business analytics team, this person will be a key contributor to the successful deployment of strategic future state capabilities critical for our business. As a Data Engineer who is focused on evaluating the quality of the solutions others build, this person is responsible for producing methods of testing the quality of code produced by our Data Engineering staff and adherence to Data Engineering developed standards. This individual will also monitor jobs for performance and error as well as make/suggest changes to address those concerns by working with the members of the Data Engineering team and the Database Administrator. This position will be responsible for designing, documenting, automating, executing, and maintaining test plans and reporting back to the author the results. This individual will implement moderate to complex test scripts to test the functionality.

RESPONSIBILITIES:
Be an active participant in the development process and possess an understanding of the intent of the design and the desired outcomes of the Data Engineering team members.Design, document, create, test, and report on the quality of the individual modules, jobs, functions, stored procedures, SQL scripts, etc.Review for existing standards and adjust to new standards as they mature.Under general guidance from other team members and the Quality Department, design effective and maintainable test scripts that can be automated or run as needed to ensure the solution meets the gathered requirements, as well as, meets reliability, accuracy, and performance standards.Interact cross-functionally with a wide variety of people and teams. Work closely with development teams outside the Data Engineering group, including, Business Intelligence Developers, Product Development, and Data Scientists, who will use the data being delivered.Produce and maintain accurate test plans and test system overview documentation.Collaborate with various data providers to evaluate and help resolve data issues.Occasionally take on special projects and additional duties.

QUALIFICATIONS:

Education: Bachelor's degree preferred or equivalent experience

Experience:
Minimum 3 years of experience with SQL as a developer or as a member of a quality team required.
Minimum 1 year of experience with database design and/or development required.
Minimum 1 year of experience with MS SQL Server, MySQL, Oracle, PostgreSQL, or MongoDB preferred.
Experience with big data technologies like Snowflake, Spark, Scala, Kafka, Hbase, Hadoop, HDFS, AVRO, MongoDB are also a plus.

Knowledge/Skills/Abilities:
Expert knowledge on how to test processes that affect data
Moderate to advanced working knowledge of Microsoft Transact-SQL (T-SQL) or ANSI SQL and demonstrated ability to create ad-hoc SQL queries to analyze data, create prototypes, etc.
Familiarity with visual data analysis using tools such as Tableau or other Business Intelligence platforms
Strong verbal and written communication skills

Personal Attributes:
Detail oriented
Quality driven
Analytical
Passionate about data
Positive and creative attitude
Committed to personal development and professional growth


Back Share
Apply Now"
121,Data Engineer,Schoolzilla,,United States,"The Schoolzilla Mission: Empower People to Use Data to Increase Student Success

We want to change millions of students’ lives by enabling people to use data to run great schools. Teachers and school leaders need lots of data to make good decisions for their students, but most of them can’t get the data they need in any kind of useful, actionable format. Schoolzilla's team of developers, data visualizers, seasoned educators and K-12 experts have done just that: we've made data easy to find, understand, and act on for school districts everywhere. We've already helped thousands of schools make better, faster decisions with our platform.

We are a Public Benefit Corporation, and we have in our bylaws our social mission to enable people to use data to improve educational outcomes for students, especially students from underserved communities. Every day we live our values: Driven by Mission, Better Together, Teammates Matter, Equity At The Center, and Intellectual Humilarity (humility + hilarity).

How You Can Help

Schoolzilla’s Engineering team is responsible for empowering school systems across the country to take data-driven action by delivering timely, accurate, and actionable data. As a member of this team, you'll be at the center of product innovation. You'll collaborate with product managers, designers, and your fellow engineers to take ideas from concept to reality, utilizing your full range of technical skills and a healthy dose of user empathy.


What You'll Do
Craft fault-tolerant data pipelines and distributed systems that can scale by millions of students
Ensure that timely, accurate data is delivered to the variety of tools and systems that depend on it
Partner with fellow engineers and product owners to bring new features and products to market
Design and build connections to proprietary data systems, allowing us to bring new customers onboard easily and cost-effectively
Define and calculate new key performance indicators for a deeper understanding of student performance at a school or district
Develop and improve internal services, scripts, and tools that will be leveraged throughout the company
Establish new patterns and frameworks for quickly aggregating millions of raw data records into actionable metrics
Craft sample data sets for use in demos and testing
Skills & Experience
Experience designing, implementing, and supporting modern enterprise-scale ETL / ETL pipelines that are both efficient and intuitive
Experience architecting data warehouses and data lakes that are organized, performant, and easy to use
Proven fluency in SQL and one or more major programming languages such as Python
Proficiency with major relational databases, particularly MS SQL Server and Postgres
Knowledge of modern, cloud-based data pipeline best practices
Bonus Points For
Understanding of how data is used in K-12 education
Experience with the tools we use, like AWS, SQL Server, Postgres, Flask, Jinja, and Docker - plus experience with tools we don’t use, but should, and the wisdom to know when to recommend them
Experience using non-relational data stores, such as document stores, key-value stores, and graph databases
Experience working remotely or as part of a geographically distributed team
Experience working in an Agile software development organization
Mission-Driven: You feel a deep sense of ownership for your work, and a relentless desire to deliver better results. You are passionate about solving problems for our users
A Lifelong Learner: You love learning new things. You're curious and ask good questions. You solicit feedback from others, accept it with grace, and act on it
Flexible: You are comfortable with technical ambiguity and welcome rapid iteration


Schoolzilla isn't just an equal opportunity employer. We are actively seeking to building a team and workplace that reflect the diversity of the communities we serve. We especially encourage people underrepresented in the tech industry to apply, and welcome your application even if you do not meet every one of the above requirements.

Schoolzilla does not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We have an office in Oakland, CA, and we are also remote-friendly, though candidates must be legally eligible to work in the United States."
122,Big Data Engineer/Architect,LeapOut Inc,$65 - $75 an hour,"Boston, MA","Details
Job Title: Big Data Engineer / Architect

Location: Boston, MA

Duration: 1+ year, likely extension

Rate: Challenging

Requirements/Responsibilities:

Experience communicating (orally and in writing) technical concepts with peers, business stakeholders and senior leaders in a meaningful way to promote understanding and dialogue.
Design and implementation experience with enterprise cloud based data solutions using technologies like: AWS, Apache Spark into HBase Hadoop databases.
Experience working in AWS environments leveraging EC2, S3, Lambda, RDS, etc.
Experience with data ingestion from a myriad of data sources: Datasets will typically originate from large RDBMS and unstructured data sources (multi-Terabyte) through the development of highly-efficient reusable code.
The ideal candidate will have broad knowledge of Hadoop tools such as Kafka, Flume, Sqoop and Oozie; Knowledge of data formats and ETL and ELT processes in a Hadoop environment including Hive, Parquet, MapReduce, YARN, HBase and other NoSQL databases.
General knowledge of how data science tools such as R Studio, Anaconda, H2o, Tableau leverage data throughout the big data ecosystem.
Top 5 Qualifications:

Quick list of top five skill sets and experience, health insurance domain knowledge and experience is very helpful.

Experienced in AWS and Cloudera (particularly hue, impala, HDFS, hive, hbase, oozie , spark, yarn).
Experienced in jupyterhub, jupyter notebook.
Experienced in SQL and large data joins.
Python as programming language, R is nice to have.
Wide knowledge of tools in analytics and ML space, such as tableau, exploratory, RStudio, etc.
Apply : https://bit.ly/32Ys2BJ"
123,Data Engineer,Creative Arts Agency (CAA),,"Los Angeles, CA","Job Description
The Role
CAA is seeking a full-stack web developer with excellent software architecture and data engineering skills to join our product development team.
The product development team designs and builds software experiences for mobile and web that are uniquely valuable for CAA’s diverse, global, and high- energy entertainment industry culture of agents, executives, artists, and business partners.
Responsibilities
You and your teammates have the opportunity to work collaboratively with other developers to design and deliver the data infrastructure (warehouse, schema, pipelines, OLTP, ETL) that supports CAA’s portfolio of applications and analytics tools, and we need your help building and maintaining supporting services for that infrastructure.
You will be able to use your software development skills in an agile environment to develop new data pipelines and APIs, and your experience with continuous delivery will support the end-to-end lifecycle: design, deployment, test, operations, monitoring and support. You will have the chance to directly interact with the people who are using your applications.
Required Capabilities
Languages and Frameworks. You should be proficient in Python with at least a year of experience delivering production Python data projects to production. We welcome candidates with experience in other languages such as Ruby, PHP, or Javascript. Experience with Javascript is a plus. We need you to have experience in the Azure data stack (Azure Data Factory, Azure Data Lake, Azure Data Lake Analytics, Tabular Model / DAX, Azure Analysis Services, HDInsight). If you have strong experience with complementary open source data technologies such as Hadoop, Amazon Glue, Spark, and so on we would love to talk to you.
Databases. We’d like to see real SQL and NoSQL production experience. We’re particularly interested in your expertise with SQL Server, Azure SQL, Azure Data Warehouse, CosmosDB, MongoDB and Elastic. If you bring specific skills in Kafka, Samza, Cassandra, let’s talk.
Tools. You should have experience with maintaining projects using a distributed version control system such as Git and Github. We desire some ops experience using a web serving technology (Apache, Nginx, IIS) and configuration management toolchains like Jenkins, Travis, Chef, Puppet, or Ansible.
Techniques. You should be experienced with agile methods and the concepts behind continuous delivery. We expect you are passionate about developing data structures, pipelines, and APIs - so we want to see your experience with security, performance, JSON, and REST. We are eager to hear about your experience and opinions around data quality, test strategy, and test automation.
Education. We prefer you have a BS in Computer Science, or equivalent experience, which means between three to five years of delivering production data pipelines.
Focus. We expect you to have a passion for and experience shipping and maintaining production software products to business users. You must have strong verbal, written and visual presentation skills. We are most interested in candidates who want to work in an environment where attention to detail, solving problems, simplicity, quality, and moving quickly are highly prized.
Environment
We are looking for candidates who are local to our offices in Century City, CA.
We have a service-oriented collaborative environment where teamwork and care and helping others to succeed are highly valued."
124,Data Engineer,Capital Group,,"Irvine, CA 92618","Req ID: 30334
Experience Level: Entry-Level
Other Location(s): N/A

Come grow with us

At Capital Group, how we work is defined by shared values that include absolute integrity, respect and collaboration. But it’s more than that. It’s smart and highly driven people united in purpose to serve our investors and one another.

Bring your energy and unique perspective to Capital and you’ll have the opportunity to grow with us professionally, personally, and financially. You’ll be part of a team that genuinely cares about helping you succeed. You’ll work alongside talented colleagues, many of whom build long careers while progressing through multiple roles, establishing lifelong friendships and making a difference in our communities. In return for your contributions, you’ll receive premier compensation and benefits, and a company-funded retirement plan that ranks among the most generous.

Our Information Management team in the Information & Integration Services (IIS) organization is focused on enabling Capital Group's quest to unlock the value of its data assets by leveraging data technologies and maturing data governance. The recently formed team has its initial focus on enabling information strategies across investment management and distribution through adoption of technologies that improve data quality and metadata management. Our team is looking for a Data Engineer who will support the evolution of related technology capabilities and facilitate adoption.

Responsibilities:
Support implementation and management of technology solutions to profile, monitor, and manage data quality in data significant applications and to automate the acquisition & provision of metadata from these applications.
Gain understanding of CG's data landscape through exposure to data significant systems and data flows spanning Investment Management and Marketing, Sales & Service, and supporting capabilities. Apply the knowledge to support root cause analysis of data quality issues in IIS products (IDH, LASR, etc.) and their sources using Data Quality & Metadata tools.
Support the assessment of emerging technologies to advance data quality & metadata management objectives, e.g., use of machine learning in advanced data profiling & exception detection, auto-classification; publication of metadata and data quality insights through APIs.
Analyze data quality issues, perform business and IT impact assessment, and support development of solution alternatives.
Support development of roadmaps for enabling technologies and related capabilities.
Qualifications:
Experience with Automation 1+ years of experience with data integration, reporting, business intelligence and analytics
Knowledge in translating unstructured business problems into clearly defined requirements and solution
Experience with data requirements analysis
Bachelor's degree required
Experience of SQL
Knowledge of technical and functional designs for databases, ETL, data warehousing, and reporting.
Knowledge of analytics tools such as R, Python, Alteryx, etc.
Knowledge of reporting/data visualization tools such as Tableau, Microsoft SSRS, etc.
Experience with programming, scripting in PowerShell, Bash, and Ksh
Experience with ETL tools such as Informatica, Microsoft SSIS and/or Ab Initio desirable
Experience with version control systems - Git preferred

Company Overview:
Founded in 1931, Capital Group is one of the world’s largest and most trusted investment management companies and home to the American Funds. We manage more than US$1.7 trillion in assets, and our 7,500 associates make our clients their first priority every day. When we do our job right, millions of investors around the world fulfill their dreams and financial goals, from home ownership and higher education, to a comfortable retirement. Our long-term investment results and outstanding service set us apart from our competitors, while our workplace sets us apart from other employers.

We are an equal opportunity employer, which means we comply with all federal, state and local laws that prohibit discrimination when making all decisions about employment. As equal opportunity employers, our policies prohibit unlawful discrimination on the basis of race, religion, color, national origin, ancestry, sex (including gender and gender identity), pregnancy, childbirth and related medical conditions, age, physical or mental disability, medical condition, genetic information, marital status, sexual orientation, citizenship status, AIDS/HIV status, political activities or affiliations, military or veteran status, status as a victim of domestic violence, assault or stalking or any other characteristic protected by federal, state or local law."
125,"Data Engineer, Display Ads","Amazon.com-Amazon.com Services, Inc.",,"Seattle, WA","7+ years of professional experience in statistical analysis, data modeling, and data miningBachelor’s degree in Computer Science, Machine Learning, Operations Research, Applied Mathematics or related technical fieldExpertise with SQL and relational database systemsKnowledge of data warehousing conceptsExperience in data mining, ETL, SQL etc. and using databases in a business environment with large-scale, complex datasetsTop notch communication (verbal and written) and interpersonal skills to convey key insights from complex analysis in summarized business terms and an ability to effectively communicate with technical teamsAbility to work with shifting deadlines in a fast paced environment

The Amazon Advertising Yield team is looking for a talented Senior Data Engineer to play a key role in implementing our next generation datamart solution and making use of latest machine learning and big data technologies to provide deeper insights on the efficacy of our ad products and proprietary targeting solutions, delighting our customers with highly relevant marketing efforts.

As a Sr. Data Engineer on the Yield team, you will have the opportunity to work on one of the world's largest consumer and Advertising datasets, as well as influence the long-term evolution of our analytical capability. You should be able to develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for the Amazon Media organization.

The ideal candidate will have extensive experience with traditional Data Warehouse (DW) concepts and have the aptitude to incorporate new approaches and methodologies while dealing with big data. The candidate should be enthusiastic about learning new technologies and be able to implement solutions using these technologies to empower internal customers and scale the existing platform. You should have excellent business and communication skills and be able to work with business owners to develop and define key business questions, then build the data sets that answer those questions. You should be expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing reporting applications.

Primary Responsibilities
Full ownership of the Advertising Yield infrastructure building controls and mechanisms to ensure data availability and fidelity.Partner with internal tech teams to identify process and system improvement opportunities.Translate complex or ambiguous business problem statements into analysis and reporting requirements.Adopt AWS tools to enable faster read/write capabilities in our infrastructure.

MS in Computer Science, Machine Learning, Operations Research, Applied Mathematics or in another highly quantitative fieldFluency with business intelligence and visualization software (e.g., OBIEE, Tableau Server, etc.) to empower non-technical, internal customers to drive their own analytics and reporting.Experience in designing and building large data warehouse systemsExperience with Python, R, Matlab/Octave or similar advanced data analysis programing language for development of basic Machine learning scriptsAbility to distill problem definitions, models and constraints from informal business requirements, and to deal with ambiguity and competing objectives
Amazon is an Equal Opportunity-Affirmative Action Employer - Female/Minority/Disability/Veteran/Gender Identity/Sexual Orientation"
126,Data Engineer,Klaviyo,,"Boston, MA","We are looking for a Data Engineer who wants to launch a career in business analytics. You'll work on the Business Intelligence team helping answer the hardest questions at Klaviyo. We're looking for someone that has a background in working with billions of records, designing datasets, writing ETL's, and working in a process driven engineering environment.
The Business Intelligence team is still in its early days and you'll have a big impact on our direction and how we operate. You'll be central to managing the data environment that everyone uses to understand the health of the business. You'll also get to help analyze business situations important to Klaviyo.
We love data and the excitement that comes with using that data to help our business grow. We are the kind of people that are not afraid of any question or project. We love to move fast, keep learning and get stuff done.
Responsibilities:
This is a cross functional role that requires framing business problems, identifying and obtaining data needed for analysis, synthesizing results and making recommendations to the executive team and other leaders in various departments
About 75% of this role is engineering. 25% is business analysis
Designing Datasets, writing ETL’s and maintaining the BI data environment
Typical business issues will involve framing and solving problems around revenue like acquisition channels, drivers of churn and customer retention, customer lifetime values, product engagement etc. As Klaviyo grows rapidly, projects looking at costs and investments will become a larger focus
Some financial and quantitative modeling will also be required / can be learnt
There will be frequent interaction with customer success, support, product, engineering, sales, marketing functions
Ideal Candidate:
Engineering or quantitative background (experience with AWS/Kafka/SQL/Python/R/Redshift/Snowflake or similar)
An individual motivated by continuous personal growth and learning, ideally with a desire to obtain an MBA, or MS in Business Analytics in the future
Familiarity with reporting tools a plus
Someone who is very curious about what makes a business / operation function successful
Good writing and communication skills. Should be able to write 1- or 3-page analysis reports on various aspects of Klaviyo’s business or operations
Does not have to be from a traditional business intelligence background"
127,Data Engineer,ChargePoint,,"Campbell, CA","About Us
-

With electric vehicles (EVs) expected to be 25% of vehicle sales by 2025 and more than 50% by 2040, electric mobility is in the midst of a tipping point and becoming reality. ChargePoint is at the center of the e-mobility revolution, powering the world's leading EV charging network and most complete set of hardware, software and mobile solutions for every EV charging need. Whether it's to ride, driver or deliver on electric fuel, we bring together people, vehicle fleets, businesses, automakers, policymakers, utilities and other stakeholders to make e-mobility a reality globally.

Our fanatical focus on charging and 10+ years in business has made us an industry leader. Supported by more than half a billion dollars in funding from investors, including Quantum Energy Partners, GIC, Clearvision, Daimler Trucks & Buses, Daimler, Siemens, Linse Capital, American Electric Power, Canada Pension Plan Investment Board, Chevron Technology Ventures, Rho Capital Partners, BMW i Ventures and Braemar Energy Ventures, ChargePoint offers a once-in-a-lifetime chance to be part of creating an all-electric future and shaping a trillion-dollar market. Join the team that built the EV charging industry and make your mark on how people and goods will get everywhere they need to go, in any context, for generations to come.

Reports To
-

Software Engineering Manager

What You Will Be Doing
-

Being a leading EV charging network we have a lot of data that is used to enhance end-user experience, optimize network operations and placement of charging stations, and improve internal operations and manufacturing processes. This is just the beginning and we see a huge opportunity to leverage data to predict failures before they happen, build delightful products and enable electrification of all forms of transportation.

A Data Engineer in this role will work with several teams across the company to identify use cases/scenarios for optimization, build a data lake to enable efficient querying and analytics, and use data science and ML to analyze the data and build data solutions.

What You Will Bring to ChargePoint
-


Fundamental understanding of data science and ML
Experience building data pipelines, analyzing big data & building data solutions
Bias to action
Be an evangelist for data analysis and data-oriented decision making

Requirements
-


1-3 years of object-oriented programming & SQL experience
1-3 years of experience with big data technologies (e.g., Hadoop, Spark, etc.) and data viz and ETL tools (e.g., Pentaho, Tableau etc.)
Relevant coursework and experience in data science and ML
Solid software engineering fundamentals
Fundamental understanding of RDBMS, NoSQL databases and big data solutions
Ability to work with ambiguity, create quick PoCs and engineer an E2E solution
Strong scripting or programming skills for automating repetitive tasks

Preferred
-


Experience with AWS
Experience building data lake or data solutions from scratch

Location
-

Campbell, CA

We are committed to an inclusive and diverse team. ChargePoint is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status.

If there is a match between your experiences/skills and the Company needs, we will contact you directly.

ChargePoint is an equal opportunity employer.
Applicants only - Recruiting agencies do not contact.

#LI-SH1"
128,Data Engineer,InSync,,"Houston, TX","InSync Systems Inc. is a privately-owned boutique Canadian Resourcing and Consulting Services Company that works closely with a range of corporate clients across multiple industries to bring them solutions that effectively address their business needs.
Our client is looking for Data Engineers for contract positions in Houston, Texas. 6-month contracts with possible extensions.
As a Data Engineer, you’ll help ingest, transform and store clean and enriched data in ready for business intelligence consumption.
Responsibilities:
Work independently on complex data engineering problems to support data science strategy of products
Use broad and deep technical knowledge in the data engineering space to tackle complex data problems for product teams, with a core focus on using technical expertise
Improve the data availability by acting as a liaison between Lab teams and source systems
Collect, blend, and transform data using ETL tools, database management system tools, and code development
Implement data models and structures data in ready-for business consumption formats
Aggregate data across various warehousing models (e.g. OLAP cubes, star schemas, etc.) for BI purposes
Collaborate with business teams and understand how data needs to be structured for consumption

Required skills:
Experience in a Data Engineer role (5+ years), with a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Experience building and maintaining optimal data pipeline architecture.
Experience assembling large, complex data sets that meet functional / non-functional business requirements.
Experience identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, data quality checks, minimize Cloud cost, etc.
Experience building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, DataBricks, No-SQL
Experience building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Experience documenting and communicating standard methods and tools used.
Working with other data engineers, data ingestion specialists, and experts across the company to consolidate methods and tool standards where practical.
Experienced using the following software/tools:
Big data tools: Hadoop, HDI, & Spark
Relational SQL and NoSQL databases, including COSMOS
Data pipeline and workflow management tools: DataBricks (Spark), ADF, Dataflow
Microsoft Azure
Stream-processing systems: Storm, Streaming-Analytics, IoT Hub, Event Hub
Object-oriented/object function scripting languages: Python, Scala, SQL"
129,Data Engineer,Deep 6 AI,,"Pasadena, CA","Deep 6 AI is a fast-growing tech startup company in Pasadena, California looking for talented, dynamic team members who want to help shape our groundbreaking artificial intelligence platform. Our healthcare technology helps doctors and researchers find patients for clinical trials, speeding up the process significantly and getting life-saving cures to people in need more quickly. Come join a fun team of scientists, engineers and problem-solvers dedicated to innovating in healthcare and improving health and wellness! You’ll enjoy competitive compensation, an awesome work environment in downtown Pasadena, and early equity in a fast-growing tech company! In 2018, we were named one of the top 25 Machine Learning startups to watch by Forbes.

Deep 6 AI is seeking a Data Engineer to help us successfully deliver and integrate our products with our client’s environments. We’re looking for someone with a commitment to the customer, a passion for solving challenges, and a vision for delivery. As a member of the team, you will be working with engineering and business to craft solutions and integrate our systems into our customers infrastructure.
WHAT YOU'LL DO:
Coordinate with clients and technical team to execute integrations within deadlines.
Continuously improve processes and documentation.
Develop integration solutions to successfully incorporate client’s datasets into the ETL pipeline.
Work with the data and product team to continuously improve our ETL strategies and application feature set.
Answer and address client inquiries.
Monitor, inspect, and address data quality and issues.
ABOUT YOU:
Demonstrable proficiency in statically typed languages. Experience in Java Python, or Scala a plus.
Understanding of Data Analytical platforms (Hadoop, Spark)
Domain knowledge of Healthcare IT
Effective written and verbal communication
BS or MS in Computer Science or Engineering"
130,Data Engineer,Confluent,,"Palo Alto, CA 94301","Dubbed an ""open-source unicorn"" by Forbes, Confluent is the fastest-growing enterprise subscription company our investors have ever seen. And how are we growing so fast? By pioneering a new technology category with an event streaming platform, which enables companies to leverage their data as a continually updating stream of events, not as static snapshots. This innovation has led Sequoia Capital, Benchmark, and Index Ventures to recently invest a combined $125 million in our Series D financing. Our product has been adopted by Fortune 100 customers across all industries, and we’re being led by the best in the space—our founders were the original creators of Apache Kafka®. We’re looking for talented and amazing team players who want to accelerate our growth, while doing some of the best work of their careers. Join us as we build the next transformative technology platform!

The mission of the Data Science team at Confluent is to serve as the central nervous system of all things data for the company: we build analytics infrastructure, insights, models and tools, to empower data-driven thinking, and optimize every part of the business. Data Engineers on the team will be the enabler and amplifiers. This position offers limitless opportunities for an ambitious data science engineer to make an immediate and meaningful impact within a hyper growth start-up, and contribute to a highly engaged open source community.

We are looking for a talented and driven individual to build and scale our data analytics infrastructure and tooling. This person will build state of art data warehousing, ETL, and BI platforms, to make data accessible to the entire company. He/she will also partner closely with data scientists and cross functional leaders to develop internal data products. Data engineers are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
Responsibilities:
Collaboration with data scientists, engineers, and business partners to understand data needs to drive key decision making throughout the company
Implementing a solid, robust, extensible data warehousing design that supports key business flows
Performing all of the necessary data transformations to populate data into a warehouse table structure that is optimized for reporting and analysis; Deploy inclusive data quality checks to ensure high quality of data
Developing strong subject matter expertise and manage the SLAs for those data pipelines
Set up and improve BI tooling and platforms to help the team create dynamic tools and reporting
Partnering with data scientists and business partners to develop internal data products to improve operational efficiencies organizationally
Building and growing partnership with cross functional teams, and evangelize data-driven culture
Contributing to innovations that fuel Confluent’s vision and mission
What We're Looking For:
4+ years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines, BI tooling and/or data apps development
Bachelor or advanced degree in Computer Science, Mathematics, Statistics, Engineering, or related technical discipline
Highly proficient in Python and SQL coding
Highly proficient with tuning and optimizing data models and pipelines
Experience in developing data apps with Python, Javascript, high charts etc
The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
What Gives You An Edge:
Experience with Apache Kafka
Experience with B2B enterprise apps data: Salesforce, Marketo, Zendesk, etc
Experience in developing data apps with Python, Javascript, high charts, etc"
131,"Associate, Data Engineer",KPMG,,"Chicago, IL 60290","Known for being a great place to work and build a career, KPMG provides audit, tax and advisory services for organizations in today's most important industries. Our growth is driven by delivering real results for our clients. It's also enabled by our culture, which encourages individual development, embraces an inclusive environment, rewards innovative excellence and supports our communities. With qualities like those, it's no wonder we're consistently ranked among the best companies to work for by Fortune Magazine, Consulting Magazine, Working Mother Magazine, Diversity Inc. and others. If you're as passionate about your future as we are, join our team.
KPMG is currently seeking an Associate in Financial Due Diligence for our Deal Advisory practice.
Responsibilities:
Assess, capture, and translate complex business issues and solution requirements into a structured use case, including rapid learning of industry standards and development of effective work stream plans
Assist the development of big data analytics and cloud applications to facilitate the work of data scientists and solution developers
Design, build, launch, optimize, and extend full-stack data and business intelligence solutions spanning extraction, storage, transformation, and visualization layers
Support build of big data environments that enable analytics solutions on a variety of big data platforms
Qualifications:
A minimum of three years' experience working as a data engineer
Bachelor's degree from an accredited college/university in a numerate subject (Computer Science, Mathematics, Electrical Engineering, Statistics, or Science) or equivalent practical experience
Experience writing and maintaining extract, transform, and load scripts (ETLs) which operate on a variety of structured and unstructured sources and familiarity with programming methodologies such as version control, testing, and Quality Assurance (QA)
Familiarity with development methodologies such as Waterfall and Agile and experience working on cloud platforms (AWS, Azure, GCP)
Experience with several programming languages (Python, Scala, R, Java, and C++)
Ability to travel as needed and required for the role
Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future
KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please."
132,Data Engineer,Facebook,,"Seattle, WA 98101","Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
At Facebook, we have many opportunities to work with data each and every day. In this role as a Data Engineer on the Analytics team, your primary responsibility will be to partner with key stakeholders, data scientists and software engineers to support and enable the continued growth critical to Facebook's Data Center organization. You will be responsible for creating the technology that moves and translates data used to inform our most critical strategic and real-time decisions. You will also help translate business needs into requirements and identify efficiency opportunities. In addition to extracting and transforming data, you will be expected to use your expertise and provide meaningful recommendations and actionable strategies to partnering data scientist for performance enhancements and development of best practices, including streamlining of data sources and related programmatic initiatives. The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. This position is part of the Infrastructure Data Center team and located in Fremont, CA.
RESPONSIBILITIES
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Collaboration with the Data Center SMEs, Data Scientists, and Program Managers
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data
MINIMUM QUALIFICATIONS
BS/MS in Computer Science or a related technical field
7+ years of SQL (Oracle, Vertica, Hive, etc.) experience and relational databases experience (Oracle, MySQL)
7+ years of experience in custom or structured (i.e. Informatica/Talend/Pentaho) ETL design, implementation and maintenance
7+ years’ experience in data engineering, experience in applying DWH/ETL best practices
7+ years of Java and/or Python development experience
2+ years experience in LAMP and the Big Data stack environments (Hadoop, MapReduce, Hive)
2+ years experience working with enterprise DE tools and experience learning in-house DE tools
Facebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com."
133,Big Data Engineer - Masters (Co-Op) – United States,Cisco Careers,,"San Jose, CA","What You’ll Do
Design and deliver automated transformation of large data sets leveraging MapReduce, streaming, and other emerging technologiesLeverage HBase, Elasticsearch, etc. to ingest transformed data at scaleCollaborate with security experts to deliver high-impact web-based APIs
 Implement high-volume data integration solutions Analyze, monitor, and optimize for performance
Produce and maintain high-quality technical documentation
Who You'll Work With
Join us as we transform the world of tomorrow. Develop creative ideas on how to work better and smarter. Influence and participate in top-priority projects that have a real impact.
Who You Are
Currently enrolled in an accredited university co-op program pursuing a Master’s degree in Computer Science, Computer Engineering, Electrical Engineering, or a related major such as Math, PhysicsMinimum of a 3.0 GPA or equivalentTrack record of developing technology to enable large scale data transformationStrong Java experience and hands-on Hadoop ecosystem experience – HBase, Hive, Spark, etc.Possess knowledge of software engineering best practicesPassion for solving hard problems and exploring new technologiesExcellent communication and technical documentation skills
Why Cisco
#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!

This position is available to Master’s level Students. Positions are located East Coast, West Coast and Central US. Not all positions offer sponsorship or are available at all locations. Relocation is available for some locations and or positions."
134,Senior Data Engineer,Common Networks,,"San Francisco, CA","Common Networks was founded on the idea that everyone should have a choice for fast, affordable access to broadband internet. Right now, most homes in the U.S. don't. In fact, 62% of homes live in a monopoly broadband market. High-speed access unlocks all the superpowers on the internet. When it works, it can be a great leveling force across the world, giving everyone access to educational tools, entertainment, immediate translations, or even medical care that they wouldn’t otherwise have.

Common Networks provides suburban neighborhoods with internet using wireless technology. We interconnect homes in a neighborhood, creating a mesh network between homes and our fiber internet sources. A whole community can then have fast and reliable internet service with only a few locations needing fiber access.

Role

As a Senior Data Engineer, you’ll be responsible for designing and building our data platform. You’ll work closely with Product, Business, Data Science, and the rest of the engineering team to build new data-enabled internet features; ensure robust and timely collection of critical metrics; flow data into performant, user-friendly dashboards; and reinforce a culture that prizes decision-making justified by excellent data.
You will:
Design and implement high performance batch and real-time data pipelines
Design and build visualization and analytics tools to empower internal and external customers with actionable insights
Instrument metrics across Common’s wireless mesh network and suite of products
Translate product and business requirements into data models that are maintainable and extensible
Champion data quality, retention, security and privacy within the company. Document and promote best practices for working with Common data
Who you are
You love to code, and you’re excited to work at a place where you spend 90% of your day heads down coding.
You’re not afraid to get your hands dirty. You’re happy to instrument new metrics in an app or debug broken graphs in a dashboard - whatever it takes to make sure we have the data we need to make the best decisions possible.
You’re relentlessly curious. When something breaks, you’re not satisfied with surface-level explanations and proximate causes, you need to know what the underlying issue was and you’re not afraid to dig in and find out for yourself.
You enjoy working closely with others on a cross-functional team, whether it’s teaching engineers ETL best practices or working with business operators to define critical metrics. You thrive in a highly collaborative environment and embrace diversity of thought and experience when thinking through your designs.
You’ve mastered your craft over years of professional data engineering. You know how to tune, tweak and optimize every kind of query, you know how to fully explore a solution space, and you know the value of good monitoring, alerting, and automated tests.
Requirements
5+ years working as a data engineer
Experience working with data ETL pipelines
Mastery of at least one programming language
Mastery of SQL, performance tuning, and general database skills
Experience leading large data projects through design, implementation, and long term maintenance
Nice to have
Experience with Google’s suite of data services (BigQuery, Datalab, Dataproc, Cloud Pub/Sub, etc)
Experience with golang
Experience with linux / the linux networking stack
Experience with visualization platforms (e.g. Tableau) and libraries (e.g. D3.js)
Equal Employment Opportunity

Common Networks is committed to being an equal opportunity employer – we evaluate all employees and job applicants equally, based on merit, competence, and qualifications. We do not discriminate on the basis of race, religion, color, national origin, gender identity, gender expression, sexual orientation, age, marital status, veteran status, disability status, or any other characteristic protected by law."
135,Principal Data Engineer,Piper Companies,"$125,000 - $135,000 a year","Durham, NC 27708","Piper Enterprise Solutions is currently seeking a Principal Data Engineer to join a growing team within a non-profit in Durham, NC. The focus of this non-profit is better health care and more consumer-focused health.

Responsibilities for the Principal Data Engineer include:
Create complex applications and tools through software engineering
Support, manage and develop complex data
Build upon newly developed AWS platform
Stay up on industry trends and help in the continuous improvement process

Qualifications for the Principal Data Engineer include:
5 years of experience in data engineering, including extensive knowledge of Python and SQL
Expert in building visualizations
Advanced knowledge of Data IQ preferred
Bachelor’s degree in Math, Computer Science or related field

Compensation for the Principal Data Engineer includes:
Salary Range: $125,000 - $135,0000 based upon experience
Full Benefits: Healthcare, Dental, Vision, 401k

Please send resumes to Melvin Crowder at mcrowder@pipercompanies.com"
136,Big Data Engineer Contractor,Ten-X,,"San Mateo, CA","Auction.com is the nation’s leading online real estate marketplace focused exclusively on the sale of residential bank-owned and foreclosure properties via online auctions and live trustee sale events. By offering access to exclusive properties and technology designed to seamlessly connect buyers and sellers, Auction.com empowers residential real estate investors and financial institutions to achieve optimal, mutually beneficial results – to go beyond the bid.
Responsibilities:
Responsible for design, implementation, and ongoing support of the Big Data platforms (Hadoop, HBase, HIVE, Spark), ensure high availability and reliability
Migrating large data sets between data centers and the cloud (such as AWS SQL server and Hadoop)
Design, test and implement cloud BI / DW infrastructure
Understand and support AWS native big data / analytics system
Use Streaming, Spark & Big Data technologies to enrich and transform data for real time ingestion and build low latency feeds.

Requirements:
Demonstrated experience with AWS Data Lakes/AWS EMR is required
Thorough understanding of Hadoop ecosystem (HDFS, YARN, Hive, Pig, MapReduce, Spark, Spark2, Sqoop, Solr, kafka, oozie)
Strong experience in setting up, configuraing, upgrading and managing security for Hadoop clusters, setting up Ranger policies for HDFS and Hive
Experience in managing Hadoop cluster with Ambari and developing custom tools/scripts to monitor the Hadoop Cluster health
Strong knowledge and hands on experience related to mission critical backup and recovery
Strong experience with load balancing and high volume, high availability environments
Able to automate administrative tasks using scripting languages (Python, Shell, Ansible)
Strong working experiences of implementing Big Data processing using MapReduce algorithms and Hadoop/Spark APIs
Experience building workflow to perform predictive analysis, muilti-dimensional analysis, data enrichment etc
Understanding of software development methodologies and coding standards.
A burning desire to master new technologies and apply them to real world challenges
Nice to have:
Relational design, understand business requirements and perform data design reviews
Data migration ETL concepts, open source ETL tools
Working experience at a web or internet start-up experience

To all recruitment agencies: Auction.com does not accept agency resumes unless you are part of our preferred partner network. Please do not forward resumes to our jobs alias, Auction.com employees or any other company location. Auction.com is not responsible for any fees related to unsolicited resumes."
137,Data Engineer,PlayQ,,"Santa Monica, CA","About Us:
PlayQ is a rapidly growing global entertainment and technology company delivering high-quality mobile titles and innovative game development solutions to a worldwide audience. Our games have been downloaded more than 60 million times across the globe, with millions of users playing every day!

Our dedicated teams, based in downtown Santa Monica, CA, work together to craft the clever, visually stunning, and unforgettable experiences that our players love. Our emphasis on individual leadership means each team member has the opportunity to make a big impact, while our commitment to creative freedom gives them the ability to create whatever they can imagine.

It's this mindset that has led us to develop our own IP, infuse games with rich storytelling, build our own development tools, and solve the deepest technical challenges - all in the name of disrupting the mobile gaming landscape.

Job Overview:
Are you ready to help build the next unforgettable gaming experience? PlayQ's team is led by the world's top engineering talent to build the most clever and diverse products for our global audience. Our engineers develop next-generation technologies that impact how millions of our players connect, explore, and interact with our top-grossing games.

We are looking for a next level Data Engineer to own our most heavily leveraged dataset across PlayQ: global player data. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new challenges as we continue to push technology forward. You will have freedom to innovate as you work closely with our data and engineering teams to see the big picture and develop new ways to store and analyze business critical information.

Our ideal candidate will have a breadth of experience managing big data and cloud infrastructure with the ability to tie engineering initiatives to business impact. If you share the desire to push the envelope and trail-blaze in your function then we'd love to hear from you.

Responsibilities:

Design, develop and launch data ingestion and storage systems with high availability and reliability that can scale
Drive the advancement of data infrastructure by developing and implementing underlying logic and structure for how data is set up, cleansed and stored
Take an integral role in designing and implementing a data lake strategy
Architect, launch, and manage automated ETL (Extraction, Transformation, Load) processes
Collaborate with development teams on design, architecture and expansion of infrastructure

Requirements:

BS from an accredited university in Computer Science, Engineering, Math or related field
3+ year operations experience with SQL and NoSQL databases
Understanding of MPP/Columnar data warehouse solutions (Redshift, Vertica, etc.) a plus
Experience with AWS environments: Redshift, EC2, Data Pipeline, S3, RDS, Glue, Spectrum
Experience working with Python, bash or other scripting languages
Experience managing large data sets

Bonus Points:

Experience in the mobile games industry

Perks:

Competitive compensation and equity options
Comprehensive medical, dental, vision, life and long term disability insurance
Flexible time off
401K plan with company match
Stocked kitchen with free snacks and beverages of your choice
Catered weekly team lunches
Brand new penthouse office space equipped with outdoor patios offering beachfront views
Monthly team outings and volunteer opportunities
Help build and support awesome GAMES. For a living! Who doesn't love games?

Interested? Please get in touch!"
138,Associate Data Engineer,Integral Ad Science,,"Chicago, IL","Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions for the advertising industry and we're looking for an Associate Data Engineer (entry-level) to join our Data Engineering team. If you are excited by technology that has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day; and evaluate thousands of data-points in real-time all while responding in just a few milliseconds, then IAS is the place for you!

As an Associate Data Engineer you will build and expand upon the testing framework and testing infrastructure of IAS' core ad verification, analytics and anti ad fraud software products.

The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment. You should be able to not take yourself too seriously as well.

What you'll do:

Extract, transform, and analyze large volumes of structured and unstructured data
Implement data processing solutions using Big data stack including but not limited to Hadoop, Spark, Vertica, and Hive
Write SQL queries and other types of data requests in the context of massive data sets
Build data pipelines and data stores optimized for data science needs
Implement systems for large-scale data analysis and machine learning.
Identify and implement automated tests for software components. Deploy and maintain data processing systems in production
Partner with engineering teams on deployments of data science driven solutions

Who you are and what you have:

Technology enthusiast eager to learn and use new technologies
Strong communication skills
Team player
0-1 years professional experience programming in object oriented languages such as Java, C++, Scala or Python
Basic understanding of algorithms and data structures

What puts you over the top:

Bachelors in Computer Engineering, Computer Science, Electronics Engineering, or related fields
Basic understanding of databases such as Oracle, Postgres, SQL Server or MySql
Basic understanding of scripting using Perl or Shell
Basic understanding of AWS Analytics technologies

About Integral Ad Science

Integral Ad Science (IAS) is a global technology company that offers data and solutions to establish a safer, more effective advertising ecosystem. We partner with advertisers and publishers to protect their investments, capture consumer attention, and drive business impact. Founded in 2009, IAS is headquartered in New York with global operations in 13 countries. Our growth and innovation have been recognized in the Inc. 5000, Crain's Fast 50, Forbes America's Most Promising Companies, and I-COM's Smart Data Marketing Technology Company. Learn more at www.integralads.com ( http://www.integralads.com ).

Equal Opportunity Employer:
IAS is an equal opportunity employer, committed to our diversity and inclusiveness. We will consider all qualified applicants without regard to race, color, nationality, gender, gender identity or expression, sexual orientation, religion, disability or age. We strongly encourage women, people of color, members of the LGBTQIA community, people with disabilities and veterans to apply.

To learn more about us, please visit http://integralads.com/ ( http://integralads.com/ ) and https://muse.cm/2t8eGlN ( https://muse.cm/2t8eGlN )

Attention agency/3rd party recruiters: IAS does not accept any unsolicited resumes or candidate profiles. If you are interested in becoming an IAS recruiting partner, please send an email introducing your company to recruitingagencies@integralads.com. We will get back to you if there's interest in a partnership."
139,"Data Engineer, Analytics, Intern",Facebook,,"New York, NY 10017","Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
Would you like to work with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. You will work with some of the brightest minds in the industry, and you'll have the opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match. This is an internship position based in Menlo Park, Seattle, or New York.
RESPONSIBILITIES
Architect, implement and deploy new data models and data processes in production.
Perform data analysis to generate business insights.
Interface with Engineers, Product Managers and Product Analysts to understand product goals and data needs.
Build data expertise and own data quality for allocated areas of ownership.
Manage data warehouse plans for a product or a group of products.
Support critical data processes running in production.
MINIMUM QUALIFICATIONS
Currently has, or is in the process of obtaining, a Bachelors, Masters or PhD degree in Computer Science, Mathematics, or related technical field
Programming expertise in a language of your choice
Knowledge of SQL
Knowledge of database systems
Must obtain work authorization in country of employment at the time of hire, and maintain ongoing work authorization during employment
PREFERRED QUALIFICATIONS
Intent to return to degree-program after the completion of the internship
Curious, self-driven, analytical and excited to play with data
Ability to thrive in a fast paced work environment
Facebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com."
140,Data Engineer,McDonald's Corporate,,"Chicago, IL","McDonalds is looking to hire a Data Engineer. The role will work closely with data scientists and includes developing, maintaining, and testing models to support business use cases. Responsibilities also include collaborating with business leaders to translate business requirements into technical, scalable solution.

Responsibilities


Bachelor’s or Master’s Degree in Mathematics, Computer Science, or Information technology preferred
Ability to present to senior leadership and partners
Experience managing applications in AWS and familiarity with core services including EC2, S3, RDS, Redshift, EMR.
Experience in ETL and data warehouse technologies (Oracle, SQL Server, etc.)
Skilled manipulating Big Data using HDFS/Hadoop eco system tools
Familiarity with modern Machine Learning techniques
Experience and desire to work in a Global delivery environment is a plus
Strong knowledge of relational and multi-dimensional database architecture
Strong verbal and written communication skills, and ability to synthesize technical information for a business audience

Minimum Requirements

Who are we?

We are proud to be one of the most recognized brands in the world, with restaurants in over 100 countries and billions of customers served each year. McDonald's is people business just as much as we are a restaurant business. We strive to be the most inclusive brand on the globe by building a workforce with different strengths who make delicious, feel good moments that are easy for everyone to enjoy.

At McDonald's, we are dedicated to using our scale for good: good for people, our industry and the planet. We see every single day as a chance to have a lasting impact on our customers, our people and our partners. We will continue to pursue big, global initiatives while remaining kind neighbors and supporters of our local communities.

We are moving fast and are building a passionate team to help us. This means the company is looking for innovators, leaders, sprinters who are focused on crafting memorable experiences for our customers, employees and partners. Joining McDonald's means thinking big daily and preparing for a career that can have impact around the world.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

CountryUnited States
Requisition Number

6871BR

EOE Statement

McDonald’s Corporation is an equal opportunity employer committed to a diverse and inclusive workforce."
141,"Data Engineer, Analytics - Seattle",Facebook,,"Seattle, WA 98101","Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
Do you like working with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. In this role, you will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.


This is a full-time position based in our office in Seattle.
RESPONSIBILITIES
Manage data warehouse plans for a product or a group of products.
Interface with engineers, product managers and product analysts to understand data needs.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data models in production.
Design, build and launch new data extraction, transformation and loading processes in production.
Support existing processes running in production.
Define and manage SLA for all data sets in allocated areas of ownership.
Work with data infrastructure to triage infra issues and drive to resolution.
MINIMUM QUALIFICATIONS
2+ years experience in the data warehouse space.
2+ years experience in custom ETL design, implementation and maintenance.
2+ years experience working with either a MapReduce or an MPP system.
2+ years experience with object-oriented programming languages.
2+ years experience with schema design and dimensional data modeling.
2+ years experience in writing SQL statements.
Experience analyzing data to identify deliverables, gaps and inconsistencies.
Experience managing and communicating data warehouse plans to internal clients.
PREFERRED QUALIFICATIONS
BS/BA in Technical Field, Computer Science or Mathematics.
Knowledge in Python or Java.
Facebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com."
142,Senior Data Engineer,DraftKings,,"Boston, MA 02110","We’re reimagining sports and technology.
DraftKings is bringing sports fans closer to the games they love and becoming an essential part of their experience in the process. An industry pioneer since our founding in 2012, we believe we can continue to define what it means to be a technology company in sports entertainment. We love what we do and we think you will too.

Love data? We do too.
As a Senior Data Engineer, you’ll be a creative contributor, leader and mentor to our data analysis and scalability processes, and you will use your experience to provide key insights that help us make data-driven decisions. Analytical thinking drives our business and when you join our team, you’ll not only solve new problems every day, you’ll see your data solutions immediately improve our users’ experience.
Who are we a good fit for?
We love working with talented people but more than that, we seek out compassionate co-workers with a collaborative spirit. Our work moves quickly and we’re great at coming together to find creative solutions to some of tech’s most interesting problems. If that sounds good to you, join us."
143,Big Data Engineer,Paypal,,"San Jose, CA 95131","Who we are
Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPal’s 277 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom, enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
When applying for a job you are required to create an account, if you have already created account - click Sign In.
Creating an account will allow you to follow the progress of your applications.
Note:
Provide full legal first Name/Family Name
DO: Capitalize first letter of First and Last Name. Example: John Smith
DON'T: Capitalize entire First and/or Last Name. Example: JOHN SMITH
NOTE: Use correct grammar for Names with multiple cases. Example: McDonald or O'Connell

Provide full address details
Resume is required
Multiple attachments can be uploaded including Resume and Cover Letter for each application

Job Description Summary:
The Global Product Data Services is a newly formed team in the Payments organization under Customer Experiences and Technology. Its vision is to “Provide Enterprise Product Data at lower cost and better quality”. To achieve this vision, we are looking for people with a passion and curiosity to solve customer problems with data. The internal stakeholders for the Program include but are not limited to PayPal Inc.’s Product Teams, Accounting and Finance, Sales, Risk, Customer Service, Treasury and Strategy Teams. Our External stakeholders include Regulators, Customers, Third Party Service Providers and Banks.

GPDS is looking for motivated and experienced big data engineers, to join a highly skilled team of experienced professionals, which believe in best-of-breed software craftsmanship, clean and elegant coding, using the right tool for the job, and always exploring and learning new technologies and approaches. This Big Data Developer will work on a high volume data pipeline, data integration hub, data transformation with Transaction State Management. This involves working across PayPal and its subsidiaries to ingest payments, contextual and end to end transaction state data into an Operational Data Store for downstream analytics by multiple stakeholders. There will be four pillars of activities in this initiative. They are the following:


1. A Real time enterprise wide data integration hub (iHub)

2. High volume data processing to create transaction state instrumentation

3. Near Real Time Operational Data Store

4. First line Data Governance
Job Description:
Responsibilities/Requirements:
As an engineer, you will be responsible for working on applications and services that handle all types of PayPal transactions. You will work in a fast-paced environment where continuous innovation and experimentation are a given. You will master both established and cutting-edge technologies like Hive, Spark, Presto, Hadoop, Oracle, Casandra, Storm, Kafka, Druid, MongoDB, CouchBase among others.
Typical GPDS team engineer responsibilities include:
Design, development, and testing of features/functions delivered via applications and services
Collaborating with peers and seniors both within their team and across the organization
Working with product managers using agile methodologies to deliver high quality solutions on time
Working with operations teams to ensure your applications and services are highly available and reliable
Supporting your applications and/or services as and when required on a 24x7 basis
Successful GPDS team engineers will:
Design, develop, test, and debug large scale complex platform using big data technologies.
Develop tools and automation for the effective management and operation of PayPal’s big data platform.
Analyze and improve stability, efficiency, and scalability of the platform.
Collaborate with architects, engineers, and business on product design and feature.
Hadoop development, debugging, and implementation of workflows and common algorithms
Apache Hadoop and data ETL (extract, transform, load), ingestion, and processing with Hadoop tools
Knowledge of building a scalable and integrated Data Lake for an Enterprise
Use the HDFS architecture, including how HDFS implements file sizes, block sizes, and block abstraction. Understand default replication values and storage requirements for replication. Determine how HDFS stores, reads, and writes files.
Be driven to get results and not let anything get in your way
Be proactive and anticipate/handle most issues before they blowup
Exhibit a strong backbone and challenge the status quo when needed
Demonstrate a high level of curiosity and keep abreast of the latest technologies
Show pride of ownership and strive for excellence in everything they do

Job Requirements:
2 + years software development experience
BS in Computer Science or related degree required. MS preferred
Expert in SQL, Hive, Spark, Presto
Expert in Hadoop, HBase, Yarn
Expert on Big Data Technologies such as Kafka, Apache Spark
Experience in database/storage technologies like Oracle, Cassandra, CouchBase and so on.
Competent in design/implementation for reliability, availability, scalability and performance
Competent in software engineering tools and best practices
EDUCATIONAL REQUIREMENT : Master’s degree, or foreign equivalent, in Computer Science, Engineering or closely related quantitative discipline or Bachelor’s degree, or foreign equivalent, in Computer Science, Engineering, or closely related quantitative discipline
EXPERIENCE REQUIREMENT :
With a Master’s degree: One (1) year of experience in the job offered or one (1) year of experience in the field of software engineering or program analysis.
With a Bachelor’s degree: Two (2) years of experience in the job offered or two (2) years of progressively responsible experience in the field of software engineering or program analysis.
Subsidiary:
PayPal
Travel Percent:
0
Primary Location:
San Jose, California, United States of America
Additional Locations:
We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom.
Paypal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities."
144,"Big Data Engineer- HIVE, SPARK, SQOOP PS20009","Anthem, Inc.",,United States,"Your Talent. Our Vision. At Anthem, Inc., it’s a powerful combination, and the foundation upon which we’re creating greater access to care for our members, greater value for our customers, and greater health for our communities. Join us and together we will drive the future of health care.

This is an exceptional opportunity to do innovative work that means more to you and those we serve at one of America's leading health benefits companies and a Fortune Top 50 Company.

Business Info Developer Consultant (Enterprise HEDIS Team)
Location: Work from Home or any Anthem Office

The quality data management team ensures the collection, production, and reporting of HEDIS measures and related data. This team also supports clinical data analytics platforms utilized by data scientists. The ideal candidate will support optimizations and enhancements of the HEDIS reporting cycle and support development of expansion and enhancements made to the clinical data analytics platform.

Responsibilities will include (1) develop applications to standardize and ETL data; (2) design large scale data analytics and reporting platforms; (3) analyze operational environment and recommend improvements; (4) general oversight of the clinical data analytics platform and codebase; (5) aide in reporting management design and code maintenance; and (6) aide other team members and business partners in report development.

Primary Duties include
Writing SQL code for data analysis as well as to expand and maintain our clinical data platform and datasets.
Oversight of clinical data platform and its code base.
Perform querying optimization, index tuning, and other optimizations to ensure the health of code base and platforms.
Develop ETL code and complex procedures relating to data transformation.
Develop strategic reporting and data-warehousing applications
Perform in-depth analysis on operational and infrastructure issues.
Work closely with data scientists and other clinical analysts.
Demonstrate a willingness to problem solve, collaborate, and think creatively to further server our partners.

Qualifications.Requires a BS/BA degree and 3-5 years’ developer experience, or any combination of education and experience which would provide and equivalent background.Requires healthcare experience and prior knowledge of healthcare data and clinical concepts.3-5 years SQL coding experience. Familiarity with concepts like update, indexes, case statements, joins, subqueries, aggregate functions, etc. is required.Familiarity with relational database environments such as MS SQL Server, Teradata, Oracle required. Experience in distributed environments like Hadoop preferred.Establishes and maintains excellent knowledge of data warehouse database design, data definitions, system capabilities, programming languages, and data integrity issues.Experience with major ETL technologies required, informatica preferred.Experience working with large datasets (10+ TB).HEDIS measure experience.Big Data Experience Preferred. Experience developing in Hadoop preferred.Anthem, Inc. is ranked as one of America’s Most Admired Companies among health insurers by Fortune magazine and is a 2018 DiversityInc magazine Top 50 Company for Diversity. To learn more about our company and apply, please visit us at careers.antheminc.com. An Equal Opportunity Employer/Disability/Veteran."
145,Data Engineer,LOCKHEED MARTIN CORPORATION,,"Aguadilla, PR 00603","This position is for a data engineer within Lockheed Martin (LM) Enterprise IT (formerly EBS) Analytics Center of Excellence (ACE) supporting the Rotary and Mission Systems (RMS) Information Technology (IT) Applications organization.

The work location is the Aguadilla, Puerto Rico facility; full-time telecommuters will not be considered for this position.
Duties and responsibilities include, but are not limited to:
Exhibits a degree of ingenuity, creativity, and resourcefulness when collecting, and processing data for analysis while working as part of an Agile scrum teamApplies and/or develops workflows that clean, transform and aggregate unorganized data into databases or data sourcesResponsible for collecting, ingesting, processing, and storing large datasets from a wide variety of data sources and stakeholdersMaintain data systems performance by identifying and resolving production and application development problems; calculating optimum values for parameters; evaluating, integrating, and installing new releasesCollaborate with data architects and data analysts to develop solutions across the entire data processing pipelineUS Citizenship is required
Basic Qualifications:
US CitizenshipDemonstrated SQL experience in database modeling and data transformationsUnderstands the data lifecycle and can interpret and communicate the state of data throughout the processing pipelineMust have strong problem-solving skills and excellent oral and written communication skillsAbility to document technical solutions in support of standards and best practicesDemonstrated experience using performance tuning and query optimization techniquesA Bachelor’s degree in Data Science, Computer Science, Business Information Systems, or other relevant area
Desired Skills:
Experience with data analytics and/or visualization toolsDomain knowledge in production Engineering, Aftermarket or OperationsKnowledge of Continuous Integration/ Continuous Development practicesExperience developing data APIsExperience specifically in data warehousing solutionsUnderstanding of data governance, data security, and data qualityExperience with SQL, SSIS, SLT, Glue, Hadoop / Big Data, SAP HANA, or SAP Business Objects suite
BASIC QUALIFICATIONS:
job.Qualifications

Lockheed Martin is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Join us at Lockheed Martin, where your mission is ours. Our customers tackle the hardest missions. Those that demand extraordinary amounts of courage, resilience and precision. They’re dangerous. Critical. Sometimes they even provide an opportunity to change the world and save lives. Those are the missions we care about.

As a leading technology innovation company, Lockheed Martin’s vast team works with partners around the world to bring proven performance to our customers’ toughest challenges. Lockheed Martin has employees based in many states throughout the U.S., and Internationally, with business locations in many nations and territories.
EXPERIENCE LEVEL:
Experienced Professional"
146,Data Engineer,Assurance,,"Bellevue, WA","About Assurance
At Assurance we are disrupting the antiquated and inefficient world of insurance and financial services. Our team of world class software engineers, data scientists, and business professionals are modernizing how people obtain and manage their financial life all through our powerful platform ecosystem. We are rapidly growing as we expand our product offerings and global footprint, and this growth continues to present new and exciting challenges as we push our industry into its future. We eliminate waste throughout the industry and calculate the complex into simple, valuable solutions to improve people's lives. We are humble, driven, and committed to improving the lives of millions.

About the Position
As we build the future of consumer insurance in a modern age, data is at the core of everything that we do. The role requires team members who are adept at building software tools to move and organize data with an approach that is rooted in improving the insights and efficiency of the business. Our team uses a variety of data mining and analysis methods, a variety of data tools, builds and implements models, develops algorithms, and creates simulations. Our Data Engineers design and build the backbone that makes this development possible with no support from engineering (we own our stack end to end). At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise.
To be successful in this role, you must possess the following:
Expertise in modeling data
Experience with Spark, Hadoop/EMR, SQL
Ability to optimize data access for speed/reliability/velocity as needed by the business
Comfort with QA’ing your own data, to include ‘menial tasks’ like listening to calls or scrubbing excel files to ensure everything is correct
Comfort with learning new technologies to help the team explore new solutions to existing problems
A drive to move fast and deliver business value
Excellent communication ability – you can explain your work in a way that anyone on the team can understand, and you can frame problems in a way that ensures the right question is being asked.
Business Acumen – you are always eager to understand how the business works, and more specifically, how your work impacts the business.
Enthusiastic yet humble – you are excited about the work you do, but you are also humble enough to embrace feedback – you don’t need to be the smartest person in the room.
The following additional experience is desired:
Capable of modifying an existing job to add a new field and get it into production within a day.
Capable of creating a new data pipeline/job within 2-3 days.
You have a proven ability to drive business results by building the right infrastructure that enables data-based insights. You are comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for enabling the discovery of solutions hidden in large data sets and working with stakeholders to improve business outcomes. We’re growing at a rapid pace, so it’s important that you embrace the opportunity to blaze your own trail. You thrive in a fast-paced environment where priorities can shift rapidly as we corner opportunity. You can work independently, with little oversight or guidance.

At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise. If this sounds like a good fit for you, give us a shout, we’d love to chat!"
147,Data Engineer - Alexa,"Amazon.com-Amazon.com Services, Inc.",,"Seattle, WA","Bachelor’s degree in Computer Science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience.Relevant experience in analytics, data engineering, business intelligence, market research or related fieldExperience gathering business requirements, using industry standard business intelligence tool(s) to extract data, formulate metrics and build reportsExperience using SQL, ETL and databases with large-scale, complex datasets

Interested in Amazon Alexa? Come work on it. We’re building the speech and language solutions behind Amazon Echo and other Amazon products and services. We’re working hard, having fun, and making history!

We are looking for candidates who want to help shape the future of human-computer interactions. Specifically, we are looking for an outstanding Data Engineer who is looking to work in a new space to help define how we use data to understand customer behavior and satisfaction. In this role, you will develop and support the analytic technologies that give our teams flexible and structured access to their data, including implementation of a BI platform, defining metrics and KPIs, and automating reporting and data visualization.

The successful candidate will be an expert with SQL, ETL (and general data wrangling) and have exemplary communication skills. The candidate will need to be a self-starter, comfortable with ambiguity in a fast-paced and ever-changing environment, and able to think big while paying careful attention to detail.

Responsibilities

You know and love working with business intelligence tools, can model multidimensional datasets, and can partner with customers to answer key business questions. You will also have the opportunity to display your skills in the following areas:

· · Design, implement, and support a platform providing ad hoc access to large datasets
· · Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL
· · Manage AWS Resources
· · Model data and metadata for ad hoc and pre-built reporting
· · Interface with business customers, gathering requirements and delivering complete reporting solutions
· · Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions
· · Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation
· · Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers

Graduate degree in computer science, business, mathematics, statistics, economics, or other quantitative fieldBoth technically deep and business savvy enough to interface with all levels and disciplines within the organizationDemonstrated ability to coordinate projects across functional teams, including engineering, IT, product management, marketing, finance, and operationsKnowledge of Advanced SQL and a programming languageExperience with data visualization using Tableau or similar toolsExperience with large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologiesProven track record of successful communication of analytical outcomes through written communication, including an ability to effectively communicate with both business and technical teams
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation."
148,Data Engineer Intern - Fall 2019,Internship,,"Denton, TX","Company Information
PACCAR is a Fortune 500 company established in 1905. PACCAR Inc is recognized as a global leader in the commercial vehicle, financial, and customer service fields with internationally recognized brands such as Kenworth, Peterbilt, and DAF trucks. PACCAR is a global technology leader in the design, manufacture and customer support of high-quality light-, medium-, and heavy-duty trucks under the Kenworth, Peterbilt and DAF nameplates. PACCAR also designs and manufactures advanced diesel engines, provides financial services and information technology, and distributes truck parts related to its principal business.
Whether you want to design the transportation technology of tomorrow, support the staff functions of a dynamic, international leader, or build our excellent products and services — you can develop the career you desire with PACCAR. Get started!

Peterbilt Motors Company
On highways, construction sites, city streets, logging roads - everywhere our customers earn their living - Peterbilt's red oval is a familiar symbol of performance, reliability and pride. Peterbilt has reigned as America's premium quality truck manufacturer since the company's founding in 1939. Our dedication to deliver products and services focused on improving customers' performance, image, profitability and peace of mind truly makes Peterbilt the Class of the Industry.

Requisition Summary
 Peterbilt Motors Company, a PACCAR Company, has an opening for a Data Engineer Intern to add to our Data Science team in Denton, Texas for Fall 2019.
Job Functions / Responsibilities
Build data pipelines, data validation frameworks, job schedules with emphasis on automation and scale
Contribute to overall architecture, framework, and design patterns to store and process high data volumes
Ensure product and technical features are delivered to spec and on-time
Design and implement features in collaboration with product owners, reporting analysts / data analysts, and business partners within an Agile / Scrum methodology
Proactively support product health by building solutions that are automated, scalable, and sustainable – be relentlessly focused on minimizing defects and technical debt
Qualifications
Masters’ or Bachelors' degree in Computer Science or a related field
Familiarity with in large-scale software development with emphasis on data analytics and high-volume data processing
Experience in data engineering development
Experience implementing scalable data architectures
Experience with AWS and related services (e.g., EC2, S3, DynamoDB, ElasticSearch, SQS, SNS, Lambda, Airflow, Snowflake)
Experience in data-centric programming languages (e.g., Python, GO, Ruby, Javascript, Scala)
Proficiency with ETL tools and techniques.
Knowledge of and experience with RDBMS platforms, such as MS SQL Server, Oracle, DB2, IMS, MySQL, Postgres, SAP HANA, and Teradata.
Experience with participating in projects in a highly collaborative, multi-discipline team environment
Education
The candidate we seek will be recently graduated or currently enrolled in an IT, Business, or related program and have completed their Sophomore year of college.
Additional Job Board Information
If you would like more information about what makes PACCAR an excellent place to work, please visit the PACCAR Career Site.

PACCAR is an Equal Opportunity Employer/Protected Veteran/Disability."
149,Big Data Engineer,Simply Hired,,"Santa Clara, CA","8+ years of back-end engineering experience, preferably Java Maven or Scala or Node.js
5+ years of experience in building IoT Bigdata and Analytics solution utilizing ETL tools, Kafka, Java or Scala, PL/SQL, Pentaho or Big Query SQL and Shell scripting
5+ years of Web development using Django or Angular JS, JavaScript frameworks, EXT JS, MongoDB or NoSQL or RDBMS
3+ years of API design, development and integration experience
2+ years of hands on experience creating applications and tools using Cloud Platform (example: Google Big Query, Cloud Storage, Cloud Functions, Pub/Sub, App Engine, Cloud SQL)
1+ year of experience in deploying AI models in preferably Google Cloud using Google Cloud Platform tools.
1+ year of experience in designing and scaling cloud-based applications (Google Cloud Platform and Amazon Web Services)."
150,Data Engineer,Chipotle Mexican Grill,,"Newport Beach, CA 92660","Data Engineer (19021830) Description
THE OPPORTUNITY
We are looking for the Data Engineers who will help make our next decade just as revolutionary as our past. If you're one of the super-talented who thrive on change, aren't afraid to take risks, love to make a difference, want to cultivate the better world, you're the right fit. Come grow with us.
The Data Engineer for Enterprise Data Management at Chipotle will be assisting in building the innovative enterprise data solution leveraging both cloud and on-prem technology. You will be responsible for building data engineering solution and processes to enable analytics, business intelligence, MDM and mobility. This role will be responsible for creating and maintaining the new data pipelines to leverage data at scale.
WHAT YOU’LL DO
Chipotle is seeking highly motivated, results oriented individual with strong data engineering skills in MS Stack and cloud technologies for Enterprise Data and Business Intelligence function. This role will influence the future processes and architecture to make difference at enterprise level. You will be part of the exciting journey at Chipotle. These responsibilities include:
Design, develop and maintain scalable data pipelines
Develop data ingestion and integrations (REST, SOAP, SFTP, MQ, etc.) processes
Assist in technology discovery and implementation for both on-prem and in Cloud (i.e. Azure or AWS) to build solution for future systems
Develop high performance scripts in SQL/Python/etc. to achieve objectives of enterprise data, BI and analytics needs.
Incorporate standards and best practices into engineering solutions
Manage code versions in source control and coordinate changes across team
Participate in architecture design and discussions
Provide logical and physical data design, and database modeling
Be part of the Agile team to collaborate and to help shape requirements
Coordinate with senior resources to solve complex data issues around data integration, unusable data elements, unstructured data sets, and other data processing incidents
Supports the development and design of the internal data integration framework
Works with system owners to resolve source data issues and refine transformation rules
Partner with enterprise teams, data scientist, architects to define requirements and solution
WHAT YOU’LL BRING TO THE TABLE
Have a B.A./B.S. and 2+ years of relevant work experience; or an equivalent in education and experience
Hands on experience with Microsoft Stack – SSIS, SQL, etc.
Possess strong analytical skills with the ability to analyze raw data, draw conclusions, and develop actionable recommendations
Experience with the Agile development process preferred
Proven track-record of excellence and consistently delivered past project successfully
Hands on experience with Azure data factory V2, Azure Databricks, SQLDW or Snowflake, Azure analysis services and Cosmos DB
Experience with Python or Scala.
Understanding of continuous integration and continuous deployment on Azure
Experience with large scale data lake or warehouse implementation on any of the public cloud (AWS, Azure, GCP)
Have excellent interpersonal and written/verbal communication skills
Manage financial information in a confidential and professional manner
Be highly motivated and flexible
Effectively handle multiple projects simultaneously and pay close attention to detail
Have experience in a multi-dimensional data environment
WHO WE ARE

Chipotle Mexican Grill, Inc. (NYSE: CMG) is cultivating a better world by serving responsibly sourced, classically-cooked, real food with wholesome ingredients without artificial colors, flavors or preservatives. Chipotle had nearly 2,500 restaurants as of December 31, 2018 in the United States, Canada, the United Kingdom, France and Germany and is the only restaurant company of its size that owns and operates all its restaurants. With more than 70,000 employees passionate about providing a great guest experience, Chipotle is a longtime leader and innovator in the food industry. Chipotle is committed to making its food more accessible to everyone while continuing to be a brand with a demonstrated purpose as it leads the way in digital, technology and sustainable business practices. Steve Ells, founder and executive chairman, first opened Chipotle with a single restaurant in Denver, Colorado in 1993. For more information or to place an order online, visit WWW.CHIPOTLE.COM.

CULTIVATING A BETTER WORLD
Food served fast doesn’t have to be a typical fast food experience. Chipotle has always done things differently, both in and out of our restaurants. We're changing the face of fast food, starting conversations, and directly supporting efforts to shift the future of farming and food. We hope you'll join us as we continue to learn, evolve, and shape what comes next on our mission to make better food accessible to everyone.
Primary Location: California - Newport Beach - 9998 - 610 Newport Office-(09998) Work Location: 9998 - 610 Newport Office-(09998) 610 Newport Center Drive Newport Beach 92660"
151,Data Engineer,Yang2020,,"New York, NY","Join Andrew Yang and the fight to put Humanity First as he runs for president in 2020!

One of the most exciting and unique candidates in the 2020 election, Andrew Yang is an entrepreneur running for President as a Democrat in 2020. In 2011, he founded Venture for America, a national entrepreneurship fellowship, and spent helped create 2,500 jobs in cities like Cleveland, Detroit, Baltimore, and Pittsburgh. When Andrew realized that new technology like artificial intelligence threatened to eliminate one-third of all American jobs, he knew he had to do something. In The War on Normal People (2018), he explains the mounting crisis and makes the case for implementing a universal basic income: $1,000 a month for every American adult, no strings attached.

You can learn more about the campaign at Yang2020.com, or listening to Andrew on the Joe Rogan Experience.

Every campaign claims to be data driven. After working for us, you'll realize everyone else is just fooling themselves.

To support this, we need data engineering talent to help build the data pipelines the campaign will rely on. We're integrating multiple data sources, many of which are difficult to work with.

We're looking for data engineers who enjoy automating data flows, know how to deal with messy data, and who don't mind doing a few (dozen) manual uploads if that's what the job takes.

We would prefer to hire people in NY or who can relocate, but we're open to remote work for the right candidate.
Skills:
Python & Pandas
Spark, MapReduce
AWS
SQL, Postgresql
Past ETL experience
Experience with data governance and ownership of data pipelines
Data is the lifeblood of the campaign, and as a data engineer you'll be responsible for keeping it moving. This is true in every campaign, but no other organization recognizes that fact or values their engineering staff to the extent that we do at Yang2020.

Yang2020 is committed to diversity among its staff, and recognizes that its continued success requires the highest commitment to obtaining and retaining a diverse staff that provides the best quality services to supporters and constituents. We strongly encourage diverse candidates to apply. Yang2020 is an Equal Opportunity Employer and does not discriminate on the basis of race, color, religion, sex, age, national origin, genetic information, protected veterans, marital or familial status, sexual orientation, gender identity or expression, disability status, criminal record information (except where permitted under applicable law), or any other category prohibited by local, state or federal law. This policy applies to all aspects of employment, including recruitment, placement, promotion, transfer, demotion, compensation, benefits, social and recreational activities and termination. For more information about equal employment opportunity, please click here for “EEO Is the Law.” For information regarding your Right to Work, click here for details in English and Spanish."
152,Data Engineer,"OneSource IT, Inc.","$120,000 - $130,000 a year","Tysons Corner, VA","Data Engineer

Key to Success in this role:

Demonstrates flexibility and adaptability to changes
Strong organizational and time management skills and can work on multiple priorities and deadlines
Strong quantitative, analytical, and problem-solving skills
Strong organizational and time management skills
Excellent communication skills
Customer Focus – Personally engage with customers to learn and deliver on their needs
Partnership – Align myself and my team with what is right for the company
Drive for Execution – Focus on results rather than the appearance of results
Required Qualifications:

Degree in Information Management, Computer Science or related field with 5+ years related experience
5+ years’ work experience in Systems Analysis and production support activities.
7+ years of data analysis experience, with a combination of business and technical skills with experience working with data marts/database from structured/unstructured data.
Experience with database, data model and data warehouse concepts
7+ years of SQL programming experience
5+ years of SAS, Informatica, Unix programming skills or similar programming experience
Experience working with Big Data Hadoop needed, Hive, PySpark. (MUST Have)
Experience working with Python is a must.
Experience working with JSON, XML and unstructured data processing.
Mortgage industry experience or financial analysis
Proficiency with Microsoft Office Suite (Work, Excel, etc.)"
153,Process / Industrial / Data Engineer,Motorola Solutions,,"Chicago, IL","Company Overview
At Motorola Solutions, we create technologies our customers refer to as their lifeline. Our technology platforms in communications, software, video and services help our customers work safely and more efficiently. Whether it’s helping firefighters see through smoke, enabling police officers to see around street corners, or reliably keeping the lights on in homes and businesses around the world, our work supports those who put their lives on the line to keep us safe. Bring your passion, potential and talents to Motorola Solutions, and help us usher in a new era in public safety and security.

Department Overview
The Continuous Improvement team supports the Elgin Operations to Advance the Lifeline. This team helps create the tools to improve everyday efforts, change management, and continuous improvement efforts.

Job Description
Investigates, proposes, develops, optimizes and evaluates processes and equipment. Performs experiments, analyzes data and/or failures, and determine root causes for failures. Monitors assigned operations to ensure compliance with quality standards and approved methods

Basic Requirements
Bachelors of Science in an Engineering field (Industrial, Mechanical, Electrical, etc.)
Experience with Oracle, SQL, and/or Tableau is preferred.

Travel Requirements
Under 10%

Relocation Provided
None

Position Type
Experienced

Referral Payment Plan
Yes


EEO Statement
Motorola Solutions is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran's status, or, any other protected characteristic."
154,Data Engineer,Calico,,"South San Francisco, CA 94080","Who we are:
Calico is a research and development company whose mission is to harness advanced technologies to increase our understanding of the biology that controls lifespan, and to devise interventions that enable people to lead longer and healthier lives. Executing on this mission will require an unprecedented level of interdisciplinary effort and a long-term focus for which funding is already in place.

Position Description:
Great software engineering and data science are increasingly crucial to biology. We are in the midst of an explosion in the quantity and quality of biological and medical data that are transformative to our understanding of biology and disease. But the tools to store, process, and analyze these data are often primitive, and in some cases don't yet exist. Calico is seeking an exceptional data engineer to join our computing group and be a part of changing that story.

In this role, you will work closely with computational and research scientists to define strategies and implement systems for modeling, collecting, storing, and accessing diverse scientific data and metadata. Collaborating with other scientists and engineers, you will design, build, and maintain databases and data warehouses that underpin our scientific endeavors and accelerate our ability to ask new, sophisticated questions spanning multiple organisms, data modalities, and timescales. You will not only build tools to support existing scientific workflows, but also help set the vision for future data generation and collection efforts.

If you are passionate about data, passionate about biology, and passionate about their intersection—this is the job for you.

What you'll do:

Work with computational and research scientists to understand common analysis use cases and data access needs.
Design strategies for data storage and integration across different data sources (both internal and external) for multiple use cases.
Implement, document, and maintain processing pipelines, databases, and data warehouse infrastructure.
Work closely with full-stack engineers to develop APIs and GUIs for accessing and visualizing scientific data.
Set data engineering vision and drive both independent and collaborative software development projects end-to-end.
Contribute to a range of projects, from one-off solutions to long-term, complex systems.
Build out core infrastructure, tooling, and software development processes.

Position requirements:

5+ years working with contemporary ETL tools and frameworks.
3+ years building Python-based backend systems.
Fluent knowledge of SQL.
Experience implementing RESTful APIs, GraphQL, and other programmatic interfaces to complex multidimensional data.
Experience deploying high-performance data backends in the cloud with Amazon Web Services, Heroku, Google Cloud Platform, or a similar service.
Firm grasp on software testing and test-driven development.
Demonstrated success in owning projects end-to-end, including working with non-technical stakeholders to define requirements and seek feedback.

Nice to have:

Worked with machine learning tools and infrastructure, e.g. TensorFlow and PyTorch.
Built back-ends for high-dimensional graph or network data.
Worked in biology or life sciences, and have familiarity with databases and data types used by computational biologists.
Built software with technologies like ElasticSearch, GraphQL, and Google Cloud Platform.

Some projects you may contribute to:

Data warehouse—a system to extract, transform, and load public and private datasets into a single repository, then making these data available for analysis visually with either off-the-shelf or custom-built GUIs.
Exploratory data visualization & analysis tools—apps to help scientists explore and understand diverse, complex, and multidimensional data.
Data platform—a modern, React (front-end) and Python (back-end) application that our scientists use to manage and process experimental data.
Automation—software to ingest and transform data from custom high-throughput instrumentation.

"
155,Data Engineer,Perfect World Entertainment,,"Redwood City, CA 94065","Join our Data Engineering team and help build a scalable real-time analytics platform that processes streaming data to make our product even more intelligent! Own and extend our data pipeline, perform data modeling, and improve data reliability and quality. Become part of a team focused on creating innovative real-time analytics and machine learning feedback loops.

Responsibilities
Work with the team to manage the data warehouse and ETL for all of Perfect World Entertainment products.
Design, build and launch new data models in production.
Interface with engineers from other products to ensure proper data collection.
Implement new requests from product managers and data analysts to fulfill their data needs.
Ensure data quality by implementing data detection mechanisms.
Support existing processes running in production and optimize it when possible.



Required Qualifications
2+ years of industry experience in a Data Engineer role.
A Bachelor degree in a quantitative field, such as Computer Science, Applied Mathematics, or Statistics, or equivalent professional experience.
Working experience with SQL, Python (3.x) and Scala is a plus.
Working experience on an ETL system.
Strong communication skills, both written and oral, and an ability to convey complex results in a clear manner.

Desired Qualifications
Working experience with Machine Learning and predictive analytics.
Familiarity with Hadoop framework.
Experience with Spark is highly desirable.
Familiar with data visualization through Tableau, or other tools."
156,Data Engineer,Voloridge Investment Management,,"Jupiter, FL","Voloridge Investment Management is a rapidlygrowing, fast-paced, quantitative investment management firm based in Jupiter,FL, managing over $2B in assets. We areseeking an enthusiastic, self-motivated Data Engineer to collaborate with dataanalysts, data scientists and architects to develop and build scalable datapipelines in a fast-paced environment. At Voloridge we are passionate about expanding our knowledge andcapabilities. Our teams are comprised of energetic, hard-working, highlyanalytical individuals that thrive on innovation and problem solving at thenexus of big data and finance.
The Data Engineer should have extensiveexperience in data management and data quality. Ability to process and execute data normalization, summarization andmanipulation through common techniques using common scripting language. The candidate will be knowledgeable invarious database engines.
Summary of Job Functions

Build, analyze, and maintain all data flows and related processes, continually striving to improve them, including increased automation, usability, transparency, documentation, and QA.
Design build and support both real-time data flows, and batch/ETL data flows.
Diligently identify and pursue data anomalies, always striving to correct not just the data but also the source of the corruption
Understand the business both from a historical/warehouse perspective, and an operational/transactional perspective.
Support the logical and physical integration of all applications that are developed or licensed, including how data are used for multiple purposes.
Communicate frequently and effectively with all internal associates, including research, management, development, and other operations personnel.
Remain calm, yet highly focused and effective when under duress, such as when a critical process fails and requires immediate attention.
Update and maintain financial, accounting, trading, and other data in required databases/systems.
Minimum Requirements

Expertise with one or more relational database platforms such as SQL Server, Postgre, Teradata
Both physical and logical sides
Expertise using Python for data manipulation
Experience with both transactional databases and data warehouses
Experience using advanced data analytics, including cloud-based data platforms
4-year college degree (or higher)
7+ years of data engineering work experience
The ability to work daily, onsite in our Jupiter, FL office
Preferred Skills and Previous Experience

Experience in performance tuning, server monitoring, and query optimization
Experience in troubleshooting and resolving database integrity issues, performance issues, blocking and deadlocking issues, replication issues, log issues, connectivity issues, security issues, etc.
Experience with database administration
Familiarity Transact-SQL, Visual Studio and SQL Server Management Studio to create/manage SSIS/ ETL packages, esp. with high volume data, and other SQL Server tools
Experience using reporting tools such as SSRS and/or Tableau
Experience with developing in one or more languages (R, C++,Python, Java, etc)
Familiarity with investment management data models
Experience working with trading / financial / investment / accounting data
Compensation and Benefits
Relocation assistance available for the right candidate
Highly competitive base salary
Profit sharing bonus
Health, dental, vision, life, and disability insurance
401K
Additional Information
Voloridge Investment Management is an SEC registered investment advisor.A private investment company founded in 2009, our mission is to deliversuperior risk-adjusted returns for qualified investors, using advancedproprietary modeling technology, conservative investment tactics andsophisticated risk management. Our market neutral equities strategy takes bothlong and short positions in the most actively traded equities and is designedto capture alpha while limiting exposure to directional markets risks. Ourfutures strategy takes both long and short positions in the most activelytraded global futures and is also built to maximize alpha captured across allfutures markets traded while capping exposure to any particular sector at agiven time.
Voloridge Investment Management is an Equal Opportunity Employer. Allqualified applicants are encouraged to apply and will receive consideration foremployment without regard to race, color, religion, sex, sexual orientation,gender identity, national origin, disability, protected veteran status, or anyother legally protected characteristic or status."
157,Data Engineer,Centerfield Media,,"Los Angeles, CA 90094","The Data Engineer will build massive reservoirs for big data. He will develop, construct, test and maintain architectures such as databases and large-scale data processing systems. Once continuous pipelines are installed to – and from – these huge “pools” of filtered information, data scientists and business stakeholders can pull relevant data sets for their analyses.

More specifically, you will:
Work with business and cross functional teams on full life-cycle of data warehouse development projects – 10% of time spent
Play an essential role in gathering requirements from business units and translating them to ETL (extract, transform and load) design documents - 10% of time spent
Work with other Data Engineering team members in designing, developing, coding, and testing Extract Transform and Load (ETL) solutions - 12% of time spent
Work with the project management team regarding the timelines, planning and the level of effort for the projects and work orders - 5% of time spent
Provide production support, resolve production issues, and participate in Agile product development process as a member of the Data Engineering team - 15% of time spent
Oversee data warehouse in Redshift, as well as configure and maintain reports in Tableau - 6% of time spent
Will perform data analysis using SQL programming language and build data warehouses using ETL tools such as Talend Big Data Module - 6% of time spent
Extract, translate, and load data using Talend and Informatica, and deliver appropriate BI (business intelligence), data warehousing, reporting, and analytical infrastructure required to support the Petitioner’s assets - 12% of time spent
Coding in ETL tools using Talend, SQL coding, and coding with C# - 12% of time spent
Utilize SQL server components such as SSIS and Stored Procedures, and use Tableau or other BI tool, including Microstrategy, Power BI and Redash to create queries - 12% of time spent

Minimum Education Requirements: a minimum of a Master’s degree in Computer Science or a related field."
158,Associate Data Engineer,Finish Line,,"Boulder, CO 80302","Come work for us!
We are looking for dedicated employees to join our team to help our customers have the best experience possible every time they enter a Finish Line Store.
Our employees are key to our success.
Job Summary:
Engineer, Enterprise Data Solutions performs activities related to the data foundation of Finish Line/JD Sports; including development & support of pipelines, ETL jobs and tools, data marts, data lake, and data warehouse in multiple environments. The Engineering position partners with the Product Manager to understand business requirements (user stories), and develops solutions to meet business objectives. The Engineering team (EDS) is responsible for data from production system to BI tool, including movement and transformation. This role reports to the Consulting Engineer.
Key Responsibilities and Tasks:
Development of jobs & pipelines from multiple production data sources into Data Lake environments
Engineers production ready solutions, inclusive of alerting and error handling
Works with Cloud based tools (Google GCP, Big Query, Dataproc, Composer, Steamsets, Looker, etc.) to deliver best-in-class cloud based data solutions
Works collaboratively with DBA team for operational execution and reliability of data solutions, both in Oracle and BigQuery
Assists in maintaining data governance through documentation of data solutions, through ERDs, Confluence documentation, or external tools
Engineer & model curated and keyed Data Warehouse solutions that meet business objectives that perform efficiently and effectively
Works in Agile product management method, managing tasks & objectives (user stories) through JIRA and providing updates to SCRUM master
Partners with Product Manager (PO) to understand business requirements across multiple functional areas; Store Operations, Merchandising, Supply Chain, Finance, Digital, Customer & Loyalty, Legal, & Data Science
Support current Data Warehouse ETL jobs, respond to tickets and inquiries from business partners when data quality issues occur
Other projects and duties as assigned
Required Education and/or Experience
Bachelor’s degree (B.A.) in Information Systems or other related field from a four-year college or university, or equivalent combination of education and experience. 1-3 years of proven work ability in data analytics, data engineering, and process documentation required. Must have experience with partnering with stakeholders of all levels of the organization to plan and solve problems.
Physical Demands – The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
EEOC Statement:
The Finish Line, Inc. is an Equal Opportunity Employer and is committed to complying with all federal, state, and local EEO laws. The Finish Line, Inc. prohibits discrimination against employees and applicants for employment based on the individual's race or color, religion or creed, national origin, alienage or citizenship status, marital status, sex, pregnancy status, age, military status, disability, or any other protected characteristic or class protected by law. The Finish Line, Inc. provides reasonable accommodation for disabilities in accordance with applicable laws.

Need accessibility assistance to apply?

Applicants who require accessibility assistance to submit an employment application can either call Finish Line at ( 317) 613-6890 or email us at talentacquisition@finishline.com. A member of our Talent Acquisition team will respond as soon as reasonably possible. ( This email address and phone number is only for individuals seeking accommodation when applying for a job. )"
159,Data Engineer,AETNA,,"New York, NY 10016","Description:
Participates in the design, build and management of large scale data structures and pipelines and efficient Extract/Load/Transform (ETL) workflows.62988

Fundamental Components:
Assists in the development of large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Applies understanding of key business drivers to accomplish own work. Uses expertise, judgment and precedents to contribute to the resolution of moderately complex problems. Leads portions of initiatives of limited scope, with guidance and direction. Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing. Collaborates with client team to transform data and integrate algorithms and models into automated processes. Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support clients and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.

Background Experience:
Strong problem solving skills and critical thinking ability.Strong collaboration and communication skills within and across teams.3 or more years of progressively complex related experience.Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.Ability to understand complex systems and solve challenging analytical problems.Experience with bash shell scripts, UNIX utilities & UNIX Commands.Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment.Experience building data transformation and processing solutions.Has strong knowledge of large scale search applications and building high volume data pipelines. Master’s degree or PhD preferred.Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.

Potential Telework Position:
No

Percent of Travel Required:
0 - 10%

EEO Statement:
Aetna is an Equal Opportunity, Affirmative Action Employer

Benefit Eligibility:
Benefit eligibility may vary by position. Click here to review the benefits associated with this position.

Candidate Privacy Information:
Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately."
160,"Data Engineer, GMS",Facebook,,"Menlo Park, CA","Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
Facebook is looking for exceptionally talented and experienced engineers to join GMS Technology team in Menlo Park, California. Our team provides analytics and workflow tools for Global Marketing Solutions (GMS), partnering with sales, marketing, measurement, support and operations teams. In this role, you’ll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with stakeholders/teams and building scalable, reliable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness. This is a full-time position based in Menlo Park, CA.
RESPONSIBILITIES
Manage data warehouse plans for a business vertical or a group of business verticals
Build data expertise and own data quality for allocated areas of ownership
Design, build, optimize, launch and support new and existing data models and analytical solutions
Partner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutions
Conduct design and code reviews
Work with data infrastructure to triage infra issues and drive to resolution
Manage the delivery of high impact dashboards, tools and data visualizations
MINIMUM QUALIFICATIONS
Bachelor in Computer Engineering or Computer Science
Experience with data architecture, data modeling, schema design and software development
Experience in leading data driven projects from definition through interpretation and execution
Experience with large data sets, Hadoop, and data visualization tools
Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders
2 or more years of experience with ETL Design, maintenance, or data warehousing
2 or more years of experience with SQL and Python or PHP, JavaScript, etc.
PREFERRED QUALIFICATIONS
Experience leading projects in a cross-functional setting
Facebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com."
161,Data Engineer,Applied Intuition,,"Sunnyvale, CA","We are looking for a Data Engineer who is excited about building products that wrangle AV data to supercharge our customers. You will drive the design and development of data infrastructure across our products and internal tools. At Applied, we encourage all engineers to take ownership over technical and product decisions, closely interact with users to collect feedback, and contribute to a thoughtful, dynamic team culture.

At Applied, you will
Design powerful data pipelines that process fast sensor streams, leverage appropriate data stores, and offer easy-to-use APIs
Develop and deploy high-quality software using modern tooling and frameworks
Work with products and teams across Applied Intuition
Work with customers across the AV ecosystem to understand their needs and the innards of their data systems

We’re looking for someone who
Has 1.5+ years experience building scalable big data pipelines
Has experience with open source data processing frameworks (Spark, Kafka, etc.)
Has experience with different data storages (e.g., relational and NoSQL)
Has experience with containerization and other modern software development workflows
Takes initiative and ownership in a fast-paced environment

Nice to have
Expertise with multiple modern programming languages (Python, C++, Go, etc.)
Prior work in enterprise software, including on-prem and/or cloud deployments
Prior work in either autonomy or simulation products

Autonomy is one of the leading technological advances of this century that will come to impact our lives. The work you’ll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting edge technology while working with major players across the industry and the globe."
162,Data Engineer,Zayo Group,,"Boulder, CO","Company Description
Zayo Group Holdings, Inc. (NYSE: ZAYO) provides communications infrastructure solutions, including fiber and bandwidth connectivity, colocation and cloud infrastructure to the world’s leading businesses. Customers include wireless and wireline carriers, media and content companies, cloud providers, finance and professional services and other large enterprises. Zayo provides customers with flexible solutions and self-service through Tranzact, an innovative online platform for managing and purchasing bandwidth.
Position Description
The Data Engineer position is responsible for the design, development, and deployment of MSSQL Data Warehouse to support analysis and reporting. The role is also responsible for maintaining, supporting, and enhancing existing integration systems as well as assisting in the ongoing development of technical best practices for data movement, data quality, data cleansing, and other Extract, Transform, Load (ETL) related activities. This is an individual contributor position with no personnel responsibilities.
Responsibilities
Member of the Data Architecture Team working to protect, improve, and leverage the value of our data assets.
Implement interfaces to meet changing integration requirements after fully understanding company-specific configurations.
Maintain and develop current data warehouse integrations and processes.
Provide production support for integration servers.
Provide SQL and reporting support across various business groups.
Manage and coordinate data loads in production environment.
Follow standard test plans to test each client and ensure that configured functionality meets the needs of the company.
Negotiate and make changes to existing interfaces as needed.
Qualifications
Intermediate to advanced SQL skills.
Proficiency in MS SQL Server, SQL Serve Reporting Services (SSRS) and MS BI stack preferred.
Bachelor’s Degree in Business Intelligence, Computer Information Systems, or similar.
5+ years of database design, data management, reporting, and ETL.
The ability to collaborate across functional groups.
The ability to communicate effectively with all levels of the organization.
Prior experience with data migration/integration efforts.
Ability to handle multiple priorities and meet aggressive deadlines.
This position will be required to be part of an on-call rotation and will occasionally be responsible for after-hours maintenance work.
Prior experience with Salesforce, Cast Iron integration application, and DBAmp considered a plus.
Rewards
Competitive compensation
Excellent benefits including health, dental, vision, 401 (k), disability and life insurance
Fitness membership discounts
Generous paid time off policy including paid parental leave"
163,Data Engineer,Farm Credit Services of America,"$98,220 - $152,250 a year","Omaha, NE","Farm Credit Services of America (FCSAmerica) is looking to hire some who will be responsible for building the infrastructure to answer questions with data, using software engineering best practices, data management fundamentals, data storage principles, and recent advances in distributed systems.
In this role, you will champion adoption of best practices in data management: data integrity, test design, analysis, validation and automation. While delivering on data solutions you will help drive the architecture and technology choices that enable a world-class user experience.
Responsibilities
Work with architecture, engineering leads and other teams to ensure quality solutions are implemented, and engineering best practices are defined and adhered to
Serve as an expert in data engineering for other technology teams and serve as a mentor/coach
Understands complex multi-tier, multi-platform systems
Develop sustainable data driven solutions with current next gen data technologies to meet the needs of our customers, business, and organization
Profile and analyze data for the purpose of designing scalable solutions
Define and apply appropriate data acquisition and consumption strategies for given technical scenarios
Design and implement distributed data processing pipelines using tools and languages
Contribute to overall architecture, frameworks and patterns for processing and storing data
Translate data product backlog items into engineering designs and logical units of work
Develop and improve the current data architecture and improve monitoring, alerting, remediation and automation.
Anticipate, identify and solve issues concerning data management to drive data quality, privacy and a wonderful end user experience.
Minimum Qualifications
Bachelor’s degree in Computer Science, Management Information Systems, Data Governance, Data Management or comparable experience in a related field required.
Minimum of five years Data Engineering, Software Development or Infrastructure Design
Preferred Qualifications
Desired skills or experience in: Data Lake ecosystems, Metadata Management, Data Modeling, Data Cataloging, Data Quality Workflow and Remediation tools, and Master Data Management are preferred.
Demonstrated strong analytical, investigative, written, oral, and organizational skills.
Effective human relations skills and ability to develop strong relationships with leadership and others within the Associations.
Our Benefits
18 vacation days and 15 sick leave days begin accruing the day you start
7 paid holidays
Quality health, dental, vision insurance with competitive monthly premiums
6 weeks paid parental leave
All employees eligible for market competitive annual incentive program
401k Potential for 9% employer contribution on day one
Charitable giving donation match up to $250/year
Tuition reimbursement and education assistance programs
Health and wellness reimbursements
About the Team
The Data Management team’s vision is to create the central nervous system of FCSAmerica, which drives data privacy, quality and agility. The team is focused on increasing FCSAmerica’s data maturity as an organization by operationalizing the data life cycle through shared services enabling core and advanced data and analytics capabilities.
By bringing enterprise-level capabilities online and driving adoption, the Data Management team enables our organization to execute strategic initiatives, programs and projects. Through continued support and enhancement, the team increases our organization’s data literacy and maturity.
Our Culture
Farm Credit Services of America (FCSAmerica) is great place to work. You see it in our people and the relationships they have with each other and our customers. Our passion and commitment to serving both rural America and each other is key to our success in the marketplace. That is why we seek highly motivated, positive-thinking people who foster honesty and integrity – the core values that guide how we work and treat others.
Who is Farm Credit Services of America
FCSAmerica is a leading provider of credit and crop insurance to farmers and ranchers in Iowa, Nebraska, South Dakota and Wyoming. Our 51,000 customers also are our owners, and they share in our success. Since 2011, we have returned more than $1.2 billion in cash-back dividends to our customer-owners. We are proud of the financial strength we have built and our mission to finance the growth of rural America.
FCSAmerica serves every aspect of agriculture today, whether it is a small farm run by a young and beginning producer or a large, complex agribusiness. We are the biggest provider of crop insurance in the country and offer digital tools and one-of-a-kind farm management software to help agricultural producers run their businesses. We also finance farm equipment and rural homes.
It takes a lot of expertise to meet the needs of today’s agricultural industry, and our 1,700 employees provide that in areas of lending, risk management, technology, marketing, customer and employee education. We are headquartered in Omaha, Nebraska, and support our customers and their rural communities from 43 local offices across our four-state territory.
You will find locally owned Farm Credit cooperatives serving agricultural producers in every state. We are all part of the Farm Credit System, formed more than a century ago to ensure the country’s farmers and ranchers had a reliable source of credit to provide plentiful and affordable food to the country.
Today, U.S. agriculture is the leading provider of food for the world, and the Farm Credit System has helped make that possible by evolving with the financial needs of a growing industry. As the largest Farm Credit cooperative, FCSAmerica strives to be a leader in the Farm Credit System and the industry through best practices, innovation and a commitment to be agriculture’s most valued financial partner. It is our valued employees who make all this possible.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law."
164,Data Engineer,Bay State,,"Herndon, VA","ay State Computers, Inc. is a professional services firm and a leading provider of Information Technology (IT) services and products to the U.S. Federal Government and Industry. Bay State brings together experienced IT professionals and the latest state-of-the-art technology tools, practices, and products to support projects and task order requirements for our customers. For more information about Bay State visit our website, connect with us on LinkedIn, or follow us on Twitter.
We have an exciting opportunity for a Data Engineer to join our team.
Key Responsibilities:
Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data.
The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them.
Engineer will also be responsible for integrating them with the architecture used across the SPOT project.
strong dimensional data modeling experience, star schemas including an understanding of extraction, transformation, and load processes from operational source systems into the data warehouse.
Required Experience/Skills:
3-6 years experience working with MS SQL 2012 or later, Prefer experience MS SQL technologies including SSAS, SSIS, SSRS, Prefer previous experience with clustered database environments.
Candidate must be a US citizen
Education: HS diploma or GED
Benefits: Full-time employees (permanent or contract employees who are employed for a term greater than 6 months) are eligible for benefits including time-off benefits, such as vacations and holidays, and insurance and other plan benefits.
Location: Herndon, VA
Bay State Computers, Inc. is an Equal Opportunity/Affirmative Action Employer. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, or any other characteristic protected by applicable law."
165,AI+Data Engineer,Solstice,,"Chicago, IL 60614","Solstice is an innovation and emerging technology firm that helps Fortune 500 companies seize new opportunities through world-changing digital solutions. As strategists and consultants, we help organizations evolve their digital strategy to solve mission-critical problems. As designers and developers, we build incredible hardware and software solutions that transcend a standalone product and transform an organization's relationship with its customers.

Solstice is looking to hire an AI+Data Engineer to join our growing capabilities team. If you are innovative, passionate about data and AI technologies, and looks to continually learn and enjoys sharing expertise, read on!

About you


You have a strong passion for building innovative and intelligent solutions around data
Experience with designing data models and ETL.
Experience in working with message queuing, stream processing, and highly scalable big data stores.
Experience with big data tools like Google BigQuery, Hadoop, Spark, Kafka, Elasticsearch etc.
Experience with relational SQL and NoSQL databases such as Postgres and Cassandra.
Experience with data pipeline and workflow management tools (any in particular?)
Experience with stream-processing systems such as Storm and Spark-Streaming
Strong background in Python, Java and/or .NET, knowledge with Kotlin and Scala is a huge plus
Familiar with Microservice design patterns including Serverless and BFF
Experience designing, building, integrating and testing with RESTful APIs
Experience in developing and implementing scripts for database maintenance, monitoring, and performance tuning to be applied across the business
You have the ability to effectively communicate technical topics to product owners, stakeholders and other business team members
Experience with data visualization and reporting tools like Looker, Tableau or PowerBi
Experience with cloud technologies such as Google Cloud Platform (GCP) or Amazon Web Services (AWS)
Strong verbal and written communication is a must
Experience working in an Agile Scrum development environment, or in a consulting capacity is a plus

What You Will be Doing


Designing, Migrating, Building, and Testing large scale data processing architectures
Building enterprise applications on the cloud and technologies such as Google Cloud, BigQuery, AutoML, Google Data Studio
Helping clients implement ways to improve data reliability, efficiency, and quality, and build intelligent solutions leveraging data
Working with designers to help visualize data to provide insights to end-users
Performing ad-hoc analyses of data stored in the business's MySQL/MS SQL databases and writing SQL scripts, stored procedures, functions, and views
Interfacing with our clients and providing technical recommendations
Helping evaluate emerging cross-platform frameworks and enterprise application platforms
Bridging the gap between elegant front end design and existing enterprise back end architectures
Working with experienced data engineers, data scientists, and data architects to foster your experience and growth

"
166,Data Engineer,Cyborg Mobile,,"Renton, WA","COMPANY OVERVIEW
Founded in 2009, Cyborg Mobile is a human-centered consultancy providing Technology and Management Consulting Services. Cyborg Mobile provides solutions in Experience Design, Program Leadership, Organizational Change, Product Innovation, Digital Strategy, and Consumer Mobile Technology.
At Cyborg Mobile, we use our combined expertise and empathetic approach to build products that delight customers. Our team members are passionate about growth, innovation and collaboration. We enjoy learning for fun and staying curious. If you have growth and ownership mindset and can work cohesively with a team, you are probably a great fit for our team!
THE POSITION
Cyborg Mobile seeks a Data Engineer to support its growing consultancy firm. This is a contract position for a public sector client of ours, with opportunity to extend into a full-time role. The key project is a data modernization effort that will help the client collect, manage, process and extract insights from data to better serve internal stakeholders and the public. This role will be part of the Data Services IT team and work closely with business analysts and SMEs.
POSITION RESPONSIBILITIES
As a Data Engineer, you will be responsible for all aspects of the software development lifecycle, including design, coding, integration testing, deployment and documentation. You will work in an Agile team setting to create and maintain new data applications relying heavily on experience and judgment to plan and accomplish goals.
Specific breakdown of responsibilities:
Analyze and resolve complex challenges around data and tools. Optimize analytical workflows by identifying opportunities and automating them
Collaborate with members of your team (eg, business analysts, data architect, subject matter experts) on the project's goals to understand and document business requirements
Translate customer requirements into unambiguous, scalable, robust and flexible technical solutions for implementation
Create and maintain architecture diagrams, data models, mapping documents, business rules, data flow diagrams and other design related artifacts
Assist the data warehouse team in designing efficient processes to load and manage data, including assessment of data quality in the source systems and implement appropriate business rules, data mappings, and transformation rules
Actively participate in code reviews, unit testing, system integration testing and remedy solution defects
Analyze and troubleshoot production issues quickly to ensure system uptime meets service level agreements
QUALIFICATIONS/REQUIREMENTS
Working knowledge in data migration/integration with off-premise/cloud services such as Azure and/or AWS
5+ years of experience building, administering and managing scalable analytical platforms containing both structured and unstructured data
Experience with Full-stack DevOps engineering—compute, network, storage and cost-optimized implementations plus application development
Experience working with object-oriented and scripting languages: Java, C#, Python, Javascript, etc.
Experience building infrastructure required for optimal ETL/ELT process for large data sets in a variety of structured and unstructured formats
Knowledge of big data ecosystem using tools like Hadoop, MapReduce, HBase, oozie, Flume, MongoDB, Cassandra and Pig
Experience working with NoSQL databases and DevOps tools: ADO, Git, Jenkins, Docker, etc.
Knowledge of machine learning, including pattern recognition clustering, text mining, etc.
Ability to work in version control and change/release management processes, alongside experience with source control mediums such as Team Foundation Server (TFS), Visual Studio Team Services (VSTS) or Git (preferred)
Solid understanding of data warehouse principles and multi-dimensional data modeling concepts, source to target mapping and data integration architecture. Foundational knowledge of traditional end-end ETL/OLAP solutions, preferably but not required, on the Microsoft SSIS/SSAS stack
Excellent written and verbal communication skills with the ability to communicate to non-technical audiences"
167,Data Engineer,Zest Finance,,"Los Angeles, CA","In this role you will:
Work with Modeling, Product and Business to define requirements, design and build the next generation of ZestFinance product roadmap features.
Identify and evangelize programming best practices with the Data Engineering and Modeling teams.
Work closely with Analysis to ensure quality & availability of data in the Data Warehouse along with support of our Business Intelligence platform
Design/Implement robust end-2-end ETL pipelines
Develop solutions with a mind towards quality, scalability and high performance on large data sets
We are looking for:
B.S. in Computer Science or equivalent experience
Hands-on coding experience in R, Python, Scala or similar
Excellent verbal and written communication skills including the ability to explain technical issues to a non-technical audience

Perks and benefits:
People – the best part of Zest
Robust healthcare plans, matching 401K and unlimited vacation time
Dog friendly office with lounge areas, video games and gigantic jigsaw puzzles
On-site gym with fitness classes
Generous family leave policy (6 month maternity leave/3 month paternity leave)
Tuition reimbursement, conference allowance and Zest talks
Complimentary massages, manicures, pedicures and more
Daily catered lunches from LA’s best restaurants and fully stocked kitchen
Company happy hours, social events and outings
About ZestFinance:

ZestFinance, Inc. applies its unique credit-decisioning technology platform — based on data science and machine learning — to help lenders effectively predict credit risk so they can increase revenues, reduce risk and ensure compliance. ZestFinance was founded in 2009 by Douglas Merrill and a team of former Google employees with the mission of making fair and transparent credit available to everyone.

We are committed to diversity in hiring, professional development, and everyday discussion. Zest is determined to hire crazy smart people who are different from each other to create broad thinking, lots of different ideas, and by extension, the best team possible."
168,Sr. Data Engineer,"Operating Company ACTIVISION PUBLISHING, INC",,"Sherman Oaks, CA","Develop scalable, efficient, forward-looking privacy solutions that support requirements of GDPR, CA Privacy Act, and other global privacy regulations.
Work closely with other engineering teams and business stakeholders to design automated solutions for privacy and compliance.
Be the engineering voice of privacy and data governance at Activision.
Implement best in class data protection standards and solutions for data pseudonymization, data anonymization, access control, logging, and auditing.
Implement data governance tools such as data catalog and processes for effectively managing metadata, data classification, and lineage.
Develop solutions for data profiling, data quality monitoring, reporting, and testing.
Support creation and implementation of data governance policies, standards, and processes.

Player Profile
Proven verbal and written communication skills, including the ability to communicate complex technical ideas to the business stakeholders.
Experience working with big data technologies (Hadoop, Hive, Spark, Presto, or similar.)
3+ years of production engineering experience.
Experience in working with cloud infrastructures.
Familiarity with GDPR, CCPA, and other privacy regulations.
Pluses
Production experience in Python and/or Java
Experience writing / debugging complex SQL statements
Experience in implementation of Data Catalog tools such as Atlas, Collibra, or Alation
Your Platform
We’re headquartered in Santa Monica, California, housing multiple teams across many disciplines of Marketing, PR, Sales, Supply Chain and other corporate functions such as HR, IT, Legal, Facilities and Finance. Santa Monica is the nerve center of our company, where the best ideas combine with unrivalled rigor to create the biggest and best entertainment experiences in the world.
Our World
Great Games Start with Great People! This is an exciting time to join us!
Activision has been changing the way people play for the past 40 years—as the leading developer and publisher of video games, our teams have created some of the world's biggest, most ground-breaking, and beloved entertainment franchises in the industry. Ask anyone who works at Activision what their favorite thing about it is, and they’ll tell you it’s the people. We have world class brands, infrastructure, and resources, but our success comes from people producing greatness together. We are nothing without our employee’s brilliance.
Activision is more than just the leading developer and publisher of video games; we are the creators of some of the world’s biggest, most ground-breaking titles in the industry. Our current portfolio includes Call of Duty®, Skylanders®, and Crash Bandicoot™.
Activision Blizzard (NASDAQ: ATVI), headquartered in Santa Monica, California with locations across the globe, is one of the largest and most successful interactive entertainment companies in the world. Sitting at the intersection of media, technology, and entertainment, our employees are some of the best and brightest across engineering, entertainment, media, and technology. A member of the S&P 500 Activision Blizzard is proud to be included as one of FORTUNE's ""100 Best Companies To Work For®,"" ""World's Best Employers,"" as well as ""Most Admired Companies."" In 2019 we were honored to receive a perfect score on our entry to the Corporate Equality Index as a Best
Workplace for LGBTQ professionals and once again made Fast Company's World's Most Innovative Companies in Gaming.
The video game industry and therefore our business is fast-paced and will continue to evolve. As such, the duties and responsibilities of this role may be changed as directed by the Company at any time to promote and support our business and relationships with industry partners.
Activision is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, protected veteran status, or any other basis protected by applicable law, and will not be discriminated against on the basis of disability."
169,Python Data Engineer,SoftVision - North America & UK,,"New York, NY 10018","WE ARE A TRANSFORMATIONAL PARTNER
We marry design and engineering language in ways that produce impactful and memorable experience journeys. We partner all the way to continuously improve our clients’ digital maturity. Our Studio network brings the optimal combination of skill, scale, and cost for each stage of the product development lifecycle. And to do this, we need great transformational people that want to impact the projects and organizations that they work with.

We are looking for an exceptional Python Data Engineer to work with our cross-functional team and join our world-class community of talented experts in New York, NY. Core to this need are:


Minimum 3 years of relevant experience
Proficient in Python Scripting
Experience on Spark platform
Experience with Hadoop ecosystem
Strong Experience in SQL and relational databases
Self-starter, motivated and good communication skills
Strong sense of ownership and driven to manage tasks to completion
Comfortable with Agile operating models
B.S. or higher in Computer Science

We are a thriving Community of top technology talent that is globally connected. We Engage, Make, Run and Evolve the technology that makes many brands that you know and love. So let’s take this journey together. No matter where you are on your digital career roadmap, we can help you grow and have fun doing it.

At this time, Softvision LLC will not sponsor a new applicant for employment authorization for this position. Relocation assistance is available within the US. Softvision LLC is an Equal Opportunity Employer. No 3rd Party Agency Candidates, please."
170,Data Engineer,Fullscreen,,"Playa Vista, CA","We are looking for a talented Data Engineer responsible for building a scalable and modern data warehouse and operational database environment, data modeling, and ETL data management tools & processes. The ideal candidate would have a passion for and knowledge of social video platforms, digital entertainment and have a proven track record supporting and fulfilling the strategic objectives of a highly cross-functional organization.
RESPONSIBILITIES
Responsible for designing, implementing and maintaining Data Warehouse solutions which will handle our growing business needs
Interface with other technology teams to extract, transform, and load (ETL) data from a wide variety of in-house and 3rd party data sources
Design, implement, and support infrastructure providing secured access to large datasets
Utilize blended data from multiple data sources to provide comprehensive reporting solutions
Perform ad-hoc analyses and respond to data/analytical requests
Glean insights from a large data warehouse using SQL
QUALIFICATIONS
3+ years of Data Engineering experience (ETL/Data warehouse) with deep proficiency in SQL & Python
Experience with ETL (using Python) and databases in a business environment with large-scale, complex data sets
Extensive experience working with an enterprise data warehouse and large sets of data
Excellent understanding of databases both, normalized and denormalized star schemas
Proven ability for looking at solutions unconventionally and explore opportunities and devise innovative solutions
Excellent communication skills (verbal and written) and interpersonal skills to convey key insights from complex analysis in summarized business terms and an ability to effectively communicate with technical teams
BONUS POINTS
NoSQL Databases & Technology (e.g. Hadoop)
Amazon Redshift and Amazon Web Services: Glue, Lambda, S3, and Athena
Experience working with Spark (pyspark)
Experience developing jobs in Luigi
Digital Media industry experience
Benefits & Perks
Unlimited PTO
Flexible dress code
Kitchen stocked with snacks
Pet Insurance
Catered meal on Fridays
Health, dental and vision benefits
401(k) with a company match
Both paid maternity time and paid paternity time
ABOUT FULLSCREEN:

Fullscreen is a social content company that provides creative, strategy, and marketing services for both talent and brand clients in order to grow, engage, and monetize their social audiences. Its unique clientele includes over 400 brands and over 2,500 creators and celebrities that generate more than 7 billion monthly video views across a global network of social channels.

For talent clients, Fullscreen provides management services, multi-platform content optimization, brand partnerships, and a powerful technology platform that helps our talent seamlessly analyze their data and create meaningful relationships with their fans.

For brand clients, Fullscreen works closely with marketers who seek to engage the new generation of consumers for whom social reach and community-building are essential for driving both brand awareness and affinity. Using proprietary data and technology, we predict and track consumer behavior to develop culturally-relevant social content based on trend insights.

Fullscreen has offices in Los Angeles and New York as well as team members across the country. Please visit fullscreen.com for more information.


Fullscreen, Inc. is an equal opportunity employer. This means we are committed to recruiting, training, compensating and promoting our employees regardless of race, color, religion, sex, disability, national origin, age, sexual orientation, gender or any other protected classes as required by applicable law that might make us unique or different. As a media company, we are dedicated to reflecting the diversity, multiculturalism, and inclusion found in our viewers, creators, and partners. Inclusion is at the heart of what we do, from the way we craft our job descriptions, to the values we espouse daily."
171,Data Engineer,Protecht,,"Newport Beach, CA 92663","Data Engineer
Protecht's industry leading live event technology and consumer-facing protection products help provide ticketing platforms and organizers with control over inventory, additional lines of revenue, fraud prevention tools, and fan engagement through enhancements like an improved checkout flow, internal automation, and upgraded security.

Job Description:
The Engineering Team at Protecht is looking to add a Data Engineer to our growing team! You would be the first of your skillset to take on this role and help Protecht become a truly data driven company. You are self-motivated, adaptable, performance driven and love working with Data. Working alongside the SVP of Engineering, you will:
Manage data warehouse plans for a group of products
Interface with product team and product analysts to understand data needs
Design, build and launch new data models, data extraction, transformation and loading processes in production in a microservice architecture
Design, build and maintain infrastructure to support all data operations
Qualifications:
Bachelor's degree in Informatics, Computer Science, or related.
2+ years experience in the data warehouse space
Experience with custom ETL design, implementation and maintenance
2+ years experience with programming languages, Python preferred.
Hands on and deep experience with schema design and dimensional data modeling
Ability to write efficient SQL statements
You're comfortable operating in a rapidly changing environment.
Why Protecht:
At Protecht, you are part of a high-performance team that focuses on driving innovation, critical solutions and building next-gen technology that will disrupt the live entertainment industry. If you are excited about massive scale, quality, performance, craftsmanship, and want a fair challenge and a leap forward, come join us and enjoy the following sweet perks:
Competitive Salary
Flexible Hours
Full Medical Benefits
Life Insurance
Continuous Learning Program
Paid Volunteer Days
Unlimited PTO
Fully-Stocked Kitchen
Fun Company Sponsored Events
Reimbursements for live event ticket purchases (up to $1200 annually!)
foKpaPz2nc"
172,Data Engineer,The J. M. Smucker Company,,"Akron, OH","For more than 120 years, The J.M. Smucker Company has brought families together to share memorable meals and moments. Guided by a vision to engage, delight, and inspire consumers through trusted food and beverage brands that bring joy throughout their lives, Smucker has grown to be a well-respected North American marketer and manufacturer with a balanced portfolio of leading and emerging, on-trend brands. In consumer foods and beverages, its brands include Smucker's ®, Folgers ®, Jif ®, Dunkin’ Donuts ®, Crisco ®, Café Bustelo ®, R.W. Knudsen Family ®, Sahale Snacks®, Smucker's ® Uncrustables ®, Robin Hood ®, and Bick’s ®. In pet food and pet snacks, its brands include Rachael Ray ® Nutrish®, Meow Mix®, Milk-Bone®, Kibbles 'n Bits®, Natural Balance®, and Nature’s Recipe® . The Company remains rooted in the Basic Beliefs of Quality, People, Ethics, Growth, and Independence established by its founder and namesake more than a century ago. For more information about our Company, visit jmsmucker.com .
TITLE
Data Engineer
LOCATION
Orrville, OH (Close proximity to Cleveland/Akron)
REPORTS TO
Senior Manager, Data Science and Enterprise Systems
SUMMARY
As a member of the MDO Data Science team, the Data Engineer will empower and enable the creation of data-driven insights by researching, designing, building, and maintaining the necessary storage and computational environments and data pipelines. The successful candidate will be able to provide the thought leadership around and construction of the necessary data science infrastructure while also being able to quickly acquire, process, and surface data needed for analyses.
KEY RESPONSIBILITIES
Support the data science and SSBI EDL teams through the design and creation of the next generation data science environment
Monitor performance and tune the data to enable the fast adoption and use by the data scientists and MDO Analytics team.
Determine best practices for acquiring the data necessary for analyses and curate run-model ETL processes to stage the data in the data science environment
Keep track of and adjust to changing data source outputs and formats
Identify new data sources to further analysis results and opportunities
Increase the capability and knowledge-base of the data science team by researching and sharing new data processing techniques
Ensure data quality and intelligent curation of data sets to meet MDO Analytics quality KPI metrics.
SELECTION CRITERIA
Education
Bachelor's or Master’s degree in a Computer Science, Computer Engineering, Computer Systems, or related field
Experience
1+ years in a Data Engineer / System Architect / DevOps role
Consumer Packaged Goods industry experience preferred
Experience with:
SQL | RDMS systems
Python and/or R
Web services and APIs
GPC, Azure, or Amazon AWS services a plus
Data science libraries including TensorFlow / Keras a plus
Spark, Hadoop a plus
Git
GPU configurations a plus
Scripting/bash a plus
Other
Passionately curious
Ability to “connect-the-dots” applying concepts/ideas/learnings from one project/functional group to another and/or combining ideas from several areas into a larger idea/project
Demonstrated ability of balancing and prioritizing numerous requests while providing visibility to stakeholders
Demonstrated ability of blue-sky thinking ideating towards “the art of the possible”
ind123
#LI-123"
173,Data Engineer,ADP,,"Tempe, AZ 85281","ADP is hiring a Data Engineer. In this position you will develop new data management, business intelligence and analytics capabilities that provide business with new insights for their external clients and internal operations. In this role the Data Engineer will help to manage initiatives in Data, BI and Analytics, supporting existing and planned activities within its business unit as well as the broader ADP organization. You will assist in defining standardized methods to capture and report key metrics and KPI's related to business objectives and initiatives. You will engage with key stakeholders to define and adopt Data Analytics methodologies and standards. This position will report up to the Senior Director of Business Intelligence and will work closely with client services, business operations and product management, as well as key members of the technology organization.

RESPONSIBILITIES:

Participate in the design and development of data management, business intelligence and analytics projects to drive strategic decisions through data driven actionable metrics and insights.
Assist in evaluating and selecting the best technologies and tools to support the data mission.
Support development and production issues impacting our clients and operations.
Monitor and improve data and BI processes to meet firm SLAs and increase efficiency.
Help define user stories as a part of the BI scrum team by analyzing business requirements, defining technical specifications and sizing the development effort.
Leverage technology capabilities and standards while working with technology system owners to identify appropriate data sources, define and build required data transformation logic.
Perform detailed data analysis to derive insights and deliver operational mechanism such as dashboards or ad-hoc reports.


QUALIFICATIONS REQUIRED:

Minimum of 5 years of hands-on experience with SQL Server, Oracle and SSIS
Minimum of 1 year of hands-on experience with building semantic-layer business intelligence solutions including metrics, dashboards and data visualization
Bachelor's Degree or equivalent in Computer Science or Information Management
Minimum of 3 years of hands-on experience with data engineering including ETL and data warehousing


PREFERRED QUALIFICATIONS:


Working experience with Tableau, Power BI and Alteryx
Working experience in the HCM or financial industry
Working experience with large or medium size companies
Expert problem solving skills to rapidly create ad-hoc queries to answer complex business questions or debug data issues
Experience developing complex queries against normalized and dimensional data models
Ability to work co-operatively as part of a team, as well as independently
Excellent verbal and written communication skills
#LITECH
We're designing a better way to work, so you can achieve what you're working for. Consistently named one of the 'Most Admired Companies' by FORTUNE® Magazine, and recognized by DiversityInc® as one of the 'Top 50 Companies for Diversity,' ADP works with more than 740,000 organizations across the globe to help their people work smarter, embrace new challenges, and unleash their talent. ""Always Designing for People"" means we're creating platforms that will transform how great work gets done, so together we can unlock a world of opportunity.
At ADP, we believe that diversity fuels innovation. ADP is committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance."
174,Data Engineer,Datadog,,"New York, NY","At Datadog, we’re on a mission to build the best monitoring platform in the world. We operate at high scale—trillions of data points per day—and high availability, providing always-on alerting, visualization, and tracing for our customers' infrastructure and applications around the globe.

We are building a first-class Internal Analytics team composed of Data Engineers and Data Analysts. If you’re excited to work on a fast-moving team with cutting-edge open-source data collection, transformation and analysis tools, we want to meet you.

What you will do
Collect data from a wide range of sources: AWS S3, Redshift, PostgreSQL, and various APIs
Build data ETL pipelines using Spark, Luigi and other open-source technologies, with programming languages like Scala, Python, and SQL
Tune Spark jobs to improve performance
Work closely with product managers, designers, and engineers in order to collect the right data that will help them better understand our customers, product usage, or our own operations
Work with Data Analysts to build the right analytics reports
Have a meaningful impact on many teams at Datadog thanks to data
Join a tightly knit team solving hard problems the right way
Grow with the company

Who we are looking for
You are fluent in several programming languages such as Python, R, or Scala
You have 2+ years of work experience in building ETL pipelines in production
You value code simplicity and performance
You have work experience with data storage such as AWS S3, Redshift or similar.
Being a SQL expert is a minimum for this position
You are fluent with command line
You enjoy wrangling huge amounts of data and exploring new data sets
You have a natural curiosity and investigative mindset - driven to know “why”.
You can explain complex datasets in very clear ways
You want to work in a fast, high-growth startup environment and thrive on autonomy
Bonus points
You are familiar with Spark and/or Hadoop
Experience with AWS Redshift and S3"
175,Data Engineer,Ubisoft,,"San Francisco, CA 94107","Job Description

We are currently searching for a Data Engineer to join the Enterprise Data team. Embedded in the Marketing team, your primary role will be to implement business intelligence solutions on the Enterprise Data Platform to support the social and media marketing activity.
Leveraging big data technologies, you will work in close collaboration with marketing business analysts and other IT teams in order to help marketers obtain a holistic view of media campaigns and their performance in achieving the desired consumer response.

RESPONSIBILITIES:

Design, Implement and Maintain data pipelines in our architecture
Structure the data in our data platform (Data Lake and Data Warehouse)
Ensure best practices and data quality guidelines during project phases
Implement and monitor data quality indicators across the entire data pipeline
Contribute to the development of internal tools and automation of processes
Be proactive in quality control such as monitoring and fixing data quality issues
Collaborate with team members, product managers and product analysts to understand business and data needs
Support the existing processes running in production
Define and manage SLA for all data sets in allocated areas of ownership

Qualifications

3-5 years of experience in custom ETL design, implementation and maintenance
Bachelor’s Degree or equivalent experience in IT related field
Advanced experience with SQL (Teradata preferred)
Experience with programming languages (Java, Python, Shell scripting…)
Experience in analyzing data to identify deliverables, gaps and inconsistencies
Experience managing and communicating data warehouse plans to internal clients
Experience with data modeling
Experience working with agile methodologies and tools
Experience working with Big Data technologies
Experience with reporting tools (Tableau, MicroStrategy …)
Experience in Quality Control (QC), testing and Data Quality Management
Gamer and passionate about the game industry
Additional Information

Ubisoft is committed to creating an inclusive work environment that reflects the diversity of our player community. We are an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, ethnicity, religion, gender, sexual orientation, age or disability status."
176,Data Engineer,Elephant Insurance,,"Henrico, VA 23233","About our Data Engineer
Elephant Insurance is currently seeking a Data Engineer to support our Pricing & Actuarial team at our Headquarters in the Glen Allen, Virginia corporate office. The ideal candidate is a high-achieving, analytical thinker who has high attention to detail and isn't afraid to think outside of the box to deliver comprehensive data solutions for our Pricing and Analytics team. You will work closely with and learn from other data professionals skilled in BI, data science, and data warehousing. This position requires an individual to manage multiple assignments concurrently and demonstrate the ability to complete work with minimal guidance.
Summary of Benefits:
22 days of paid time off each year plus an additional day each year until your 6th year
Shares of stock that pay dividends throughout the year and vest three years from the date they are granted
401k match of 100% up to 5% of employee earnings
Health savings account option and choice of three medical plans
Dental, vision, legal and disability coverage
Gym membership reimbursement and athletic event reimbursement
Local discounts with various restaurants, gyms and other companies
Responsibilities Include:
Responsible for designing, implementing, and maintaining Data Warehouse solutions which will handle the analytical needs of the Pricing Team.
Utilize blended data from multiple data sources to provide comprehensive reporting solutions.
Interface with internal analysts and other technology teams to extract, transform, and load (ETL) data from a wide variety of in-house and 3rd party data sources.
Producing pricing proposals for senior management including implementation and testing of new pricing factors, assisting with filing and actuarial justification to state regulators.
Preferred Qualifications:
Bachelor's degree in Mathematics, Statistics, Economics, Computer Science, Engineering, or other related business or quantitative discipline.
0-2 Years of Experience
Understanding of databases, data warehousing, relational structures, star schemas, T-SQL experience.
Excellent quantitative, analytical and problem-solving skills.
Strong programming interesting and background in R, Python, or SAS
Workplace Qualifications:
Ability to work on a computer for a full workday
Ability to type on a keyboard
Ability to sit at a desk for a full workday
Attendance is an essential job function
Education
Required
Bachelors or better in Business Administration or related field"
177,"Intern, Data Engineer, Data and Services",MasterCard,,"Arlington, VA","Who is Mastercard?
We are the global technology company behind the world’s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless ®. We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.
Job Title
Intern, Data Engineer, Data and Services
Data Engineer Interns are fundamental to the success of our clients; you will be the bridge between raw client data and Mastercard's software. You will be responsible for:


Designing processes to extract, transform, and load (ETL) terabytes of client data into Mastercard's analytics platform using SQL and other technologies
Working across multiple client teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set.
Tackling big data problems across various industries, utilizing your creative thinking skills

Make an Impact as a Data Engineer Intern:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies (such as Microsoft SQL Server & Business Intelligence Tools) and to explore a variety of directions
Flexibility to work on many new and challenging projects across a diversity of industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven management team
Collaborate with other Mastercard departments and focus on internal development, during which you will develop new tools and processes that will be used across Mastercard


Bring your passion and expertise

Understanding of relational databases and ETL Processes (preferably Microsoft SQL Server)
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language a plus (Powershell, .Net, Perl, Python, VB Script, C#)
Strong troubleshooting and problem solving capabilities
Demonstrated analytical/quantitative skills
Working towards a Bachelor's degree with an established history of academic success

Mastercard Worldwide is an Equal Employment Opportunity Employer and does not discriminate in employment on the basis of age, race, color, gender, national origin, disability, veteran status, or any other basis that is prohibited by applicable law.
Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.
If you require accommodations or assistance to complete the online application process, please contact reasonable.accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly."
178,Master Data Engineer,Capital One - US,,"McLean, VA","McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Master Data Engineer

Capital One (yes, the “what’s in your wallet?” company!) is rethinking the way the world approaches banking. As a Capital One Data Engineer, you will develop fast data infrastructure leveraging Apache Spark, Databricks, and Apache Kafka to manage and create information products using the data streamed from our fleet of over 2000 ATM’s. Whether a new feature or a bug fix, you will lead your work and deliver the most elegant and scalable solutions, all while learning and growing your skills. Most importantly, you’ll work and collaborate with a nimble, autonomous, cross-functional team of makers, breakers, doers, and disruptors who love to solve real problems and meet real customer needs.

The person we're looking for:
has a sense of intellectual curiosity and a burning desire to learn

is self-driven, actively looks for ways to contribute, and knows how to get things done

is deliriously customer-focused

values data and truth over ego

has a strong sense of engineering craftsmanship, takes pride in the code they write

believes that good software development includes good testing, good documentation, and good collaboration

has great communication and reasoning skills, including the ability to make a strong case for technology choices

Basic Qualifications:

Bachelor’s degree
At least 3 year of experience with leading big data technologies such as Apache Spark, Apache Hadoop, or Apache Kafka
At least 4 years of professional experience with data engineering concepts

Preferred Qualifications:

2+ years experience with AWS cloud
2+ years of experience in Java, Scala, or Python
2+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python
2+ years of experience building data pipelines
At least 1 year of Cloud (AWS, Azure, Google) development experience
Experience with Streaming and/or NoSQL implementation (Mongo, Cassandra, etc.) a plus

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
179,New York Hiring Conference - Data Engineer,"Amazon.com-Amazon.com Services, Inc.",,"New York, NY","Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Economics, Finance, Mathematics, Statistics, Engineering).4+ years of relevant experience in one of the following areas: data science, data engineering, business intelligence or business analytics.Strong analytical and problem-solving skills.Expertise in the design, creation and management of large datasets/data models.Expert-level proficiency in writing complex, highly-optimized SQL queries across large datasets.Ability to work with business owners to define key business requirements and convert to technical specifications.Ability to manage priorities simultaneously and drive projects to completion.

Amazon Global Finance & Finance Tech teams are coming to New York!
We are hosting an exclusive hiring event for lead engineers in the data space on October 10th & 11th, 2019 – if you are passionate about Big Data, BI systems, Cloud/AWS & ML, and always enjoy a good challenge of highly complex technical contexts, we have the opportunity for you!

Even the best analysts’ and scientists’ impact is dependent on having access to high quality, reliable data at scale. We are looking for top data engineers to join various teams within the Finance & Finance Tech space in our Seattle HQ, and the person will be responsible of partnering with our research team to understand data needs, establish/manage a data store, work with teams across multiple functions to identify normative data sources, and build data pipelines for production level systems. As a Data Engineer, you will be owning the technical architecture of BI and Data platforms, working with very large data sets in one of the world's largest and most complex data warehouse environments, and you will work closely with the business and technical teams in analyzing many unique business problems and use creative problem-solving to deliver results. You will work in a fast paced environment with some of the brightest engineers to innovate on behalf of the customer. You should be somebody that is passionate about solving customers’ problems and gets excited about owning infrastructure services that serve critical finance systems. You will also guide the team on software development best practices and set examples by using them in the solutions you build.

In summary, a typical Data Engineer in Amazon works on:
Architecture design and implementation of next generation BI solutions, enabling stakeholders to manage the business and make effective decisions.Designing, planning, and building for secure, available, scalable, stable, and cost-effective data solutions in the various engineering subject areas as it relates to data storage and movement solutions: data warehousing, enterprise system data architecture, data design (e.g., Logical and Physical Modeling), data persistence technologies, data processing, data management, and data analysis.

Masters in computer science, mathematics, statistics, economics, or other quantitative field.Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.Experience providing technical leadership and mentoring other engineers for best practices on data engineering.Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.Experience with AWS services including S3, Redshift, EMR and RDS.Knowledge with statistical and/or econometric modeling.Experience in BI/DW as a change leader providing strategic research, recommendations, and implementations.
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation"
180,"Data Engineer, Junior",Booz Allen Hamilton,,"Norfolk, VA 23502","Key Role:
Maintain responsibility for gathering, modeling, and transforming large amounts of data with the goal of highlighting useful information, suggesting conclusions, and supporting decision making. Work with subject matter experts and various stakeholders to facilitate data identification, prioritization, analysis, and gathering. Manage and track the performance of data gathering and migration initiatives and identify, communicate, and mitigate risks.

Basic Qualifications:
Experience with developing and using data collection tools and providing data reportsExperience with using Microsoft Access and Excel to develop creative client-ready productsExperience with conducting trend analysis on large data sets and analyzing corrective action plansExperience with gathering and analyzing quantitative and qualitative dataExperience with object-oriented computer programmingKnowledge of data modelingAbility to work in a team-oriented, inclusive, very collaborative work environment to achieve consensusAbility to obtain a security clearanceBA or BS degree in CS, Computer Engineering, Information Systems, Data Analytics, or Data Engineering
Additional Qualifications:
Experience with SQL, Java, C++, and VBAExperience with data migration strategiesSecret clearance
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
We’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change.
NMC"
181,Data Engineer,RAYMOND JAMES FINANCIAL,,"Denver, CO","Description

About the role:
Data Engineer works in the Data Engineering team and has primary responsibility for building Enterprise Data Integration solutions by working on enterprise class data integration initiatives. The Data Engineer will be responsible for building solutions which are flexible, performant and scalable. In this role, you are the primary resource on the most complex or escalated issues and may provide direction, guidance and mentoring to team members. You will apply specialized business knowledge, technical skills and creativity to significant deliverables and projects that involve multiple IT departments and business units which have enterprise scope. The Data Engineer should be able to explore newer technology options, if need be, and must have a high sense of ownership over every deliverable.
Responsibilities:
Builds scalable and reliable Data Integration solutions which are flexible, scalable and elastic.Develops low latency Data Integration solutions to provision data near real time for multiple consumers.Collaborates with Data Engineers, Data Architects and Service developers to build optimal and efficient ETL and Database code.Produces dynamic, data driven solutions to support the strategic business goals.Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute to meet business objectives.Help develop an enterprise scale Data WarehouseDesign and develop new systems and tools to enable stakeholders to consume and understand data fasterSupports ETL processing.Provides on-call support of Data Integration processes on a rotating basis and other on-call as required.Produces dynamic, data driven solutions to support the strategic business goals.Performs other duties and responsibilities as assigned.

Qualifications
Minimum of a Bachelor’s degree in Computer Science, MIS or related degree and five (5) years of relevant development or engineering experience or combination of education, training and experience.Expert/Advanced level experience with ETL Tools, ODI preferably.Expert Level experience with Oracle as a Database Platform.Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases.Experience or understanding of programming languages like Python, Java, R etc.Experience or understanding of Cloud Data Platforms a plus.Strong understanding of Data Warehousing concepts.Financial Services Industry knowledge is a plus.May occasionally work a non-standard shift including nights and/or weekends and/or have on-call responsibilities.
Licenses/Certifications:
None required"
182,"Data Engineer, Search Engineering",MailChimp,,"San Francisco, CA","Mailchimp is a leading marketing platform for small business. We empower millions of customers around the world to build their brands and grow their companies with a suite of marketing automation, multichannel campaign, CRM, and analytics tools.

MailChimp's Engineering team is responsible for infrastructure that makes that possible. Team members work closely with our Product, Marketing, Support, and Data Research teams to provide the infrastructure needed to move the company and our products forward. We take a pragmatic and practical approach to our stacks; we use tried and true components and build our own logic and complexity on top of well understood building blocks. We are growing fast and need people who can help advance our infrastructure by listening hard and changing fast to take care of with the needs of the present and the future.

As a part of the Search Engineering team, you'll join us in maintaining, scaling, and improving our data pipeline infrastructure that consists of 50 Elasticsearch clusters comprised of 700 nodes. We also maintain a large Kafka footprint that pushes up to 10 billion events per day. Together we'll provide our internal and external customers with highly available and approachable data pipelines and discovery tools. Search Engineering enables Mailchimp's understanding of business-critical processes and supports the tools and features that Mailchimp's customers use to find and fine-tune their audiences.

You'll help us architect and design new patterns that will enable long term technical strategy. You'll help us improve our current infrastructure while pushing us towards a more hybrid environment. You'll work on automating processes, creating tools, and building features that will improve everyone's experience with our data pipeline infrastructure.

What you'll do:

You'll join an amazing team that designs, develops and maintains a scalable real-time data infrastructure
You'll share in the on-call rotation to ensure the data always flows smoothly
You'll assist other teams that depend on this infrastructure to implement new features and functionality into the Mailchimp app
You'll migrate specific parts of the infrastructure to GCP where it makes sense

We'd love to hear from you if:

You're experienced in search technologies such as Elasticsearch, Solr, or CrateDB
You've written APIs, tools, and scripts (ex. Python, Bash, Ruby, or Golang)
You're familiar with data modeling, creating visualizations, and dashboarding
You have experience with moving and transforming large amounts of data
You've complemented your Elasticsearch experience with Kafka or other data pipeline technologies

Bonus points if:

You have hands-on experience supporting and scaling similar distributed systems
You've worked with Puppet profiles, roles, and other patterns
You've migrated infrastructure to GCP or know your way around cloud infrastructure
You've built and deployed services using containers and used container orchestration tooling (Kubernetes, Docker, GKE)

Mailchimp is a founder-owned and highly profitable company ( https://www.forbes.com/sites/alexkonrad/2018/10/08/the-new-atlanta-billionaires-behind-an-unlikely-tech-unicorn/#177eabcd31a2 ). Our purpose is to empower the underdog, and our mission is to democratize cutting edge marketing technology for small business. We love Oakland and are happy to have a small but growing office right downtown, especially convenient to the BART.

Our headquarters are in the heart of Atlanta in the historic Ponce City Market, right on the Beltline. We offer our employees an exceptional workplace, extremely competitive compensation, fully paid benefits (for employees and their families), and generous profit sharing. We hire humble, collaborative, and ambitious people, and give them endless opportunities to grow and succeed. If you'd like to be considered for this position, please apply below. We look forward to meeting you!

Mailchimp is an equal opportunity employer, and we value diversity at our company. We don't discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
183,Sr Data Engineer Architect,Standav,,"Santa Clara, CA","Title: Sr. Data Engineer/Architect

Location: Santa Clara, CA (On Site)

Duration: Long Term Contract Position
Skills: Big Data & Analytics, PySpark, SQL

Job Description:

The candidate must have at least 8+ of experience.
The ideal candidate must have prior work experience as a Big Data Engineer, He/she will be an integral member of the Big Data & Analytics team responsible for Leading design and development scalable data processing and storage systems.
The person must have experience coding in PySpark
This person will be building data pipelines and ETL using heterogeneous sources using PySpark
Must have a very strong experience working on SQL"
184,Data Engineer (BI),Simply Business,,"Boston, MA","Simply Business is more than our name. It's how we approach insurance: Make it clear. Make it simple. Make it affordable. By combining exceptional talent, technology, data, and knowledge, Simply Business is the go-to online insurance brokerage that protects small businesses and the entrepreneurs who work hard to build them.

We want team members who have the drive to challenge boundaries. If you’re smart and passionate about delivering brilliant customer experiences, we’d love to hear from you.

The Simply Business Data and Analytics team is looking for a Data Engineer. The Data Engineer will help design, build and maintain the systems that create and provide actionable information to help executives and managers make informed business decisions. This includes:
Data warehouses and data lakesBusiness intelligence and analytics platformTools and solutions for ingesting, transforming and consuming data
As a Data Engineer, you will:
Be hands-on in developing our products using best practices, appropriate tools and technologies.
Highlight areas for continuous improvement and drive their prioritisation.
Be proactive in collaborating and communicating with your colleagues near you and across the ocean.
Champion data thinking whilst creating amazingly useful systems in a collaborative way for our customers.
What we are looking for:
Experience building and maintaining data warehouses and developing ETL pipelines.
Intermediate or advanced fluency in SQL and experience with data modeling.
Coding experience is highly valued, especially for API integrations. In the Data and Analytics team we use Ruby, Scala and Python but you don’t need experience in all of them. If you're happy to learn them we will support you with that.
Strong interpersonal skills to work well in our very open and friendly environment.
What are the benefits?

Here are some of the great benefits and perks that come from being a Simply Business employee:A salary that reflects your experience, our pay policy, and the market we’re in from your first dayGroup plan for medical, dental, and prescription drug coverageShort term disability, long term disability, and life insurance coverageParticipation in the Company’s bonus programParticipation in 401(k) plan with a 3% employer matchCommuter benefits to help cut down on commuting costs25 days of vacation time plus 10 company holidaysFlexible working hours and working from homeAnnual company trip, regular outings, and volunteer opportunitiesAn awesome WeWork office with cold brew coffee, beer on tap, local pop-up events, and more

As a company, we pride ourselves on inclusion in the workplace. Simply Business is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.

Want more info on working at Simply Business? Check out our careers page: https://www.simplybusiness.com/careers/"
185,Infrastructure & Data Engineer,Jane Technologies,,"San Francisco, CA","Jane Technologies- Infrastructure & Data Engineer

Jane is building the future of eCommerce.

Jane is an MIT-founded, high growth, and rapidly expanding technology company in the cannabis industry. As the cannabis industry's first complete real-time marketplace, we aim to provide consumers with a confident, safe and simple shopping experience. Users can browse local products in real-time, compare by price, proximity or popularity and place orders at local stores for pickup or delivery. Our platform integrates directly with POS systems at retail locations and leverages this real time data to provide an ""it just works"" experience for both the retail operators and end consumers. Additionally, Jane provides key data insights to industry stakeholders via our growing analytics platform.

Culture is the single most important component of Jane's success to date. A successful candidate will thrive in our environment of mutual support, relentless pursuit of excellence, creativity, and complete lack of ego. To learn more about who we are, our culture, and whether this is the right place for you, read our Key Values profile: https://www.keyvalues.com/jane ( https://www.keyvalues.com/jane ). Check out our product at: https://www.iheartjane.com/ ( https://www.iheartjane.com/ )

About Us:

We are a full stack company, i.e. we are building Point-Of-Sale (POS) integration, analytics systems, and user experiences
We are a small close-knit team of highly technical engineers with diverse backgrounds
We have a strong engineering culture, which values lean development, data-driven practices, and open-source
We are rapidly growing 20% month over month and are always tackling challenging and interesting technical problems

What You'll do:

Create and optimize our ETL and data pipelines that span dozens of data sources and third-party systems
Build scalable and resilient services
Own and automate all aspects of our data infrastructure
Implement reliable NPL/NLU solutions to extract value from our real-time customer data and POS
Contribute to our continuous efforts to improve our infrastructure and processes

You Have:

Bachelor's degree or equivalent experience
6+ years of relevant experience
Strong Computer Science fundamentals
Ability to write clean and maintainable code, preferably in Python and/or Go
Knowledge of SQL and experience working with relational databases and data modeling
Experience with data wrangling libraries (Pandas, Numpy)
Hands-on experience building and scaling systems that support microservice-oriented architectures and related technologies (e.g. Kubernetes, Kafka, Celery/RabbitMQ, nginx, Redis, Airflow, etc)
Experience with AWS cloud services
Experience in Infrastructure as Code (e.g. Terraform, SaltStack, Ansible, Chef, Puppet)

What We Offer:

Competitive salary and equity
Beautiful office space within walking distance to the surf break at Pleasure Point in Santa Cruz
Medical Health Insurance, Dental Insurance

How To Apply:

Your resume (PDF or Markdown/text preferred)
Links to some of your work (if possible) - (GitHub or similar preferred)
(Optional) An example of something that inspires you

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
186,"Visualization & Data Engineer, Tableau",Twitter,,"San Francisco, CA 94103","Twitter is a growing organization that faces new challenges as it evolves. The IT Enterprise Applications Team focuses on the building blocks of applications and tools that improve the productivity of employees across the company.


Who We Are


At Twitter, our IT Business Intelligence team focuses on implementing and supporting applications to help our internal customers meet their operational and reporting needs. We are responsible for establishing processes for continual improvement, guiding administration, and partnering with multiple stakeholders on a project-to-project basis to help them through their reporting and visualization needs. Our mission is to keep our business running effectively while delivering the data required to assist in analytical insights.


Who You Are


To continually strengthen our growing team, we are looking for an experienced visualization and data engineer with a deep background in Tableau visualization and report development, as well as Google Analytics. You will partner with business and engineering teams to prioritize requests and consistently deliver excellent results against time-sensitive priorities. You have practiced interpersonal communication skills, possess a broad range of data related experience, are team-oriented, and you thrive in a fast paced environment.


Roles & Responsibilities


Absorbing and managing complex analytical requests
Prioritizing requests through management of team backlog
Coordinating across multiple engineering teams
Assisting in data analysis to drive proactive planning for future development
Participating in data modeling & visualization exercises
Build solution driven views and dashboards in Tableau to support the IT Enterprise Applications team
Conduct training for Tableau and hold office hours for internal Tableau users to help them with advanced questions / scenarios
Support and utilize all Tableau tools including Tableau Desktop, Tableau Server, and Tableau Prep
Serve as the secondary on-call for the internal Tableau Server deployment, which will involve server maintenance tasks and ad-hoc user requests
Help Business Analysts build dashboards using techniques for advanced analytics, interactive dashboard design, and visual best practices to convey the story within the data
Open collaboration through timely communication is a must!
Providing input and guidance for team growth and process definition
Help teams who own sites tracked in Google Analytics to understand and make sense of their GA data to drive business outcomes


Qualifications


Bachelor's Degree in a relevant field with 3+ years of professional experience OR Master’s Degree with 2+ years of professional experience
Strong experience in analytics in a fast-pace, big-data environment
Strong experience with Tableau versions 2018.1 and above
Hands-on experience with creating solution driven, compelling and well-organized views and dashboards in Tableau
Experienced in Tableau best practices for dashboard development and data source management (e.g. certified data sources, creating template workbooks, optimizing workbook performance)
Experience with troubleshooting and performance tuning of Tableau workbooks
Working knowledge of Statistical methods and Mathematical models
Ability to communicate effectively with business partners, engineers, and analysts


Preferred Qualifications


Tableau Server Administration experience (e.g. server setup and maintenance; supporting high availability clusters, server migrations, etc.)
Experience with MySQL, Vertica, and/or other database systems (MySQL highly preferred)
Advanced experience with Google Analytics’ data model
Ability to write Bash, Python scripts for monitoring"
187,Data Engineer I,Regal,,"Knoxville, TN",": The Data Engineer I participates in the design, development, implementation, security and maintenance of the company data warehouse and related technologies. Responsible for improving the accuracy of data, creating performant, low-impact collections, and supporting enterprise data through the full life cycle from application to collection to presentation to archiving. This role requires a familiarity with business processes and collaboration with developers and analysts. The Data Engineer I will perform their duties with the support and guidance of the team as necessary.

Essential Duties and Responsibilities include the following. Other duties may be assigned.


Collaborate with the Data Operations and Other IT Development TeamsPerform Necessary Production Support for Data Operations ProcessesMaintain and Create Extract, Transform, and Load (ETL) ProcessesDevelop and Tune T-SQL QueriesCreate Relational Data Sets for Production ConsumptionProfile Relational Data SourcesPrepare and Maintain Team Key Performance Indicators (KPIs)Enhance and Maintain System Logging and Performance MonitoringSupport End Users of Data Operations ProductsCreate Reporting ProductsRegular and Consistent Attendance
Qualifications:
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

Professional Skills:
Functional Use of Microsoft T-SQL in an Enterprise EnvironmentExperience with Microsoft SQL Server Integration Services (SSIS)Experience with Microsoft SQL Server Analysis Services (SSAS)Experience with Microsoft SQL Server Reporting Services (SSRS)Understanding of Relational Data ModelsConceptual and Logical Knowledge of Enterprise Data SystemsAbility to understand and communicate business needsAbility to develop and maintain business-oriented metadata definitionsAbility to prioritize end-user requirementsAbility to create professional documentationProficiency with Microsoft Office Suite, particularly Excel

Education/Experience: Associate's or Bachelor’s degree in Relevant Field of Study or one (1) to two (2) years’ experience working with enterprise data. Relevant course work or academic projects will be considered for professional experience.

Certificates, Licenses, Registrations:
Microsoft Technology Associate (MTA) or Microsoft Certified Solutions Associate (MCSA) a plus.

Language Ability:
Must have good reading, writing and speaking skills to effectively communicate with studios, managers, and co-workers.

Math Ability:
Comfort with basic statistics and common business formulas. Ability to add, subtract, multiply, and divide in all units of measure, using whole numbers, common fractions, and decimals. Ability to compute rate, ratio, and percent, and to develop and interpret charts.

Reasoning Ability:
Perform under pressure and/or opposition at times relying on your own independent judgment and knowledge to decide the best directions and solutions. While representing the company, the marketing coordinator will be required to determine answers to questions and situations which may be socially or politically volatile. Execution and communication of such solutions are often in the public eye.


Computer Skills:
Microsoft Outlook, Word, Excel, database software

Supervisory Responsibilities:
This position has no direct supervisory responsibilities.

Work Environment:
The work environment for this position is an open plan corporate office. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
The noise level in the environment is low to moderate as to be expected in a collaborative work environment.


Physical Demands:
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
While performing the duties of this job the employee is frequently required to stand; walk; sit and use hands to finger, handle, or feel; reach with hands and arms; climb or balance; stoop, kneel, crouch, or crawl and talk or hear. The vision requirements include: close vision, distance vision, peripheral vision, depth perception and ability to adjust focus. The employee is occasionally required to lift up to 50 pounds."
188,Data Engineer,Propeller Health,,"Madison, WI","Almost every part of our daily life is now digital, from hailing a cab to paying a bill. But our medicines have not caught up. They’re still stuck in an analog era.

Propeller Health is changing that. Our mission is to improve people’s health by connecting the world’s medicines. Our growing team enjoys working together to innovate new technology to address challenging problems whose solutions create enormous social value and improve patient health.

We empower people with chronic diseases to lead their lives without limits; you’ll learn something new every day as you pioneer new digital health solutions.

Consider Joining Us If You Are

Eager to work at a fast-paced company on a 5 to 7 person, self-governed, cross-functional team
Looking to learn from some of the best engineers, data scientists and researchers in the San Francisco and Madison areas
Passionate about eliminating waste, and getting stuff done
Not afraid to experiment and fail. Failure is an opportunity to learn, and is embraced as a core tenet of Propeller
Naturally curious, and believe in lifelong personal growth
A believer that technology can help solve hard problems and change the world
Aware that technology, by itself, does not delight customers; the sweet spot is when we combine technology with practical solutions, surprises, utility, and social interaction
What You Will Do

Build data pipelines for: machine learning models, business intelligence data warehouse and batch processing jobs
Be the liaison between our data and engineering teams by providing guidance, expertise and engineering time to build batch processing jobs and Data Warehouse queries used by our product
Find creative and pragmatic solutions to power our Data Science and Research teams in discovering new signals to respiratory diseases
Work hand in hand with our security and devops teams to build guard rails to all data for regulatory, privacy and customer requirements
Act like an Owner. When you see gaps, fill them. We trust you to make decisions.
Don’t suck at failing. Be fearless. Own it. Learn from it.
Experience We’re Looking For

3-5+ years experience creating and maintaining data pipelines and analytical platforms
Proficiency running Apache Spark or equivalent batch processing technology in a production product
History of finding creative solutions for finding and implementing new signals to problems
Bonus Points

Experience with streaming data processing technologies
Familiarity in medical device & healthcare regulatory environments
Trained and iterated on machine learning models
Served machine learning models in production products
Worked with time series data at TB scale
Deployed a data analytics solution in the cloud
Used infrastructure as code in day to day work
Implemented cost effective testing solutions for analytics platforms
Experience with geographic databases and systems
Created a production ETL with AWS Glue & pyspark
Here’s What’s In It For You:

Join a talented and independent team to find creative solutions to respiratory disease at scale. For example, previous endeavors include: building custom geographic databases, correlating asthma attacks with massive open datasets, predicting future asthma attacks and advising major cities on cost effective changes to help people with respiratory disease.
Shape how we apply data to help patients suffering from respiratory disease. Contribute to patient facing production systems driven by real customer needs. Everything we do with data starts with how we can help a patient become healthier.
Work with a highly talented and diverse team of professionals. Solving respiratory disease requires a wide array of disciplines to work together. For example, we have epidemiologists, physicians/clinicians on staff combined with engineers, statisticians and data scientists to build the best respiratory solution possible.

Propeller Health is proud to be an Equal Opportunity Employer. We are dedicated to building a diverse and inclusive team with a wide range of backgrounds and experiences, each helping us understand our patients better, and strengthen our team. We firmly believe that hiring and supporting a diverse team allows us to do the best work of our lives, and make the disease experience different. To do this, we strive to maximize access to every opportunity, cultivate a diverse set of candidates, and if hired, providing them with support, mentorship, and growth opportunities. We don’t discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, ability status, or any other differences. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. All are welcome here.


Apply for this Job
* Required
First Name *
Last Name *
Email *
Phone *
Resume/CV *
Drop files here

Attach Dropbox Paste

Cover Letter
Drop files here

Attach Dropbox Paste

Why Propeller? *

Are you based in Madison, WI? If not, do you have existing plans to relocate? *

Are you authorized to work for Propeller legally in the United States? *

What are your salary expectations for this role (Range)? *

Did you meet us at an event? If so, who did you speak with?

GitHub Profile

Demographic Questions
Inclusion is important to us.

Propeller sees the diversity of our team as an organizational strength, understanding that drawing on a wide variety of perspectives, capabilities and insights enhances decision-making quality and entrepreneurship. For this reason we're asking you to share some information about yourself. Please note that the information you provide can't be identified back to you, or your application.

We use these data so we can understand how effectively we're reaching a diverse group of candidates for our positions, and to know when and how we need to adjust course. Thanks for trusting us with your information.
Ethnicity
Black/of African descent
 
East Asian
 
Latinx
 
Middle Eastern
 
Native American/Alaskan Native
 
Pacific Islander
 
South Asian
 
Southeast Asian
 
White (Western European)
 
White (Eastern European)
 
Prefer not to say
 
Prefer to self-describe

Gender Identity
Man
 
Woman
 
Bigender
 
Androgynous
 
Agender
 
Prefer not to say
 
Prefer to self-describe

Family Status
No children
 
Partnered Parent/Legal Guardian
 
Single Parent/Legal Guardian
 
Prefer not to say
 
Prefer to self-describe

Veteran Status
Veteran
 
Not a veteran
 
Prefer not to say

Age
<18
 
18-25
 
26-30
 
31-34
 
35-44
 
45-54
 
55-64
 
65+
 
Prefer not to say"
189,Data Engineer,"GSW Sports, LLC",,"San Francisco, CA","The Golden State Warriors are looking for a savvy and innovative Data Engineer to join our growing Basketball Operations team. In this role, you will have the opportunity to utilize your skills to develop and define data requirements and recommend data structure for BI applications. To be successful, you will need a strong data engineering background and the ability to thrive in a dynamic environment. You will be responsible for acquisition, ingestion, transformation, cleansing and aggregation of data.

Do you strive to build something new? If so, then we want to talk to you! This is an exciting opportunity to share your expertise and knowledge within a growing sports and entertainment organization that values your initiative, creativity and drive for results. We are seeking an individual who thrives in an environment that is ever changing and full of diversity and pride themselves on community!

This is full-time position located in Oakland, CA relocating to San Francisco later this summer

------------------------------
Required Skills and Experience
------------------------------


Bachelor's degree in Computer Science, Statistics or STEM related field; Master's degree preferred
3+ years of experience with backend engineering, SAS, MSSQL and other markdown languages (JSON, HTML, XML)
Proven expertise with Python and IPython with focus on panda
Experience with ETL and parallelized data pipelining
Experience with Apache Airflow
Experience with cloud technologies Google Cloud or AWS preferred
Experience with data visualization (Tableau, Looker)
Proficiency with XML, JSON, CSV parsing
Understanding of statistical modeling
Basic proficiency of sci-kit learn
Proficiency with Git, Slack, Confluence

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Golden State Warriors is an equal opportunity employer.

Want to learn more about who we are and what we value? Visit www.warriors.com/employment ( http://www.warriors.com/employment )"
190,Data Engineer,Havenly,,"Denver, CO","At Havenly, we believe everyone deserves a beautiful home that they love. Through our proprietary technology and our team of talented designers, we make designing and shopping for your home, fun, easy, and accessible for all.

In this role, you'll work closely with our Data Scientists, Product Managers, and Engineers to optimize the core user, designer, and e-commerce experience - all using massive amounts of data (320+ million data points arriving daily from dozens of vendors). Data is the heartbeat of what we do and continues to factor into the direction we move at Havenly. In this role, you will see a direct link between your work, company growth, and user satisfaction. You'll get the opportunity to solve some of our most challenging business problems that will unlock huge areas of opportunity for the business.

WHAT YOU'LL DO:

Develop a highly scalable, reliable, and real-time data processing pipeline.
Partner with the Data Science team to provide data infrastructure for a variety of projects including ETLs to the data warehouse powering the BI of Havenly.
Design, build and launch new ETL processes into production.
Work with data infrastructure to triage issues and drive to resolution.
Work with our entire team to help provide a consistent, fast, and delightful experience to our customers and designers.
Be part of an engineering organization that exists to efficiently deliver high-quality code to production that powers the business

WHAT YOU'LL BRING:

BS/MS in Engineering, Computer Science, Math, Physics, or equivalent work experience.
3+ years in a Data Engineering role.
5+ years experience with Python development.
Experience with modern data platforms (Spark, Hadoop/Map Reduce, Hive, Airflow, Kafka/Kinesis)
Experience with Docker and Kubernetes
Experience analyzing data to identify deliverables, gaps and inconsistencies.
Experience working cross-functionally to communicate data plans that address business challenges.
Ability to develop and scale ETL pipelines.
Expertise with SQL, database management, and best practices.
The desire to always leave code better than you found it.

WHY HAVENLY?

It's challenging- You get the opportunity to work hard, learn a ton, and grow your skillset. This is not a 9-5 job, we have high expectations, and every day you'll be faced with new challenges where you have to figure out how to put one foot in front of the other and move forward.

It's fulfilling - We get the opportunity to affect one of the most personal aspects of someone's life, their home, every single day. We feel really lucky to be able to create spaces where people feel comfortable, make lifetime memories in, and call home.

It's fun - We truly love what we do. Growing a business is fun. Working with a team of incredibly talented people who also love what they do is fun. Getting to do what you love to do and make an impact is fun."
191,Data Engineer,Maestro Technologies Inc,,"New York, NY 10004","Position Overview: Seeking a highly qualified Data Engineer with a track record of managing and architecting complex data platforms. The Data Engineer will work on data management projects for a wide variety of companies.. This person will be responsible to creating programs for loading, transforming, and validating data. This person will also be responsible to lead small teams responsible for the end to end implementation of data platforms for our clients. We are seeking a “player/coach” with the adaptability and flexibility to develop strategies but also get into the weeds and write code. As such, a successful candidate will have deep technical and management expertise across a variety of methodologies as well as have demonstrated management skills.Responsibilities: Design and build data management platformsDesign and build scalable data platforms to securely ingest, process, validate, analyze and publish dataPerform POC (proof of concepts), choose the right tools and technologies to be usedManage large & complex data and analytical projects: data transformation, exploration, performance evaluation & testingMove existing ETL jobs from traditional data warehouse processing to the big data processing platform, ensure that the jobs are designed to scaleManage and lead data engineering projectsCollaborate with internal and portfolio company stakeholders in a broad range of sectors to identify business use cases and develop solutions in driving impact through data and analyticsManage and execute the successful delivery of the data (ETL/ELT) pipelines, analytics platforms and reporting tools to meet business needsPerform analyses of large structured and unstructured data to solve multiple and complex business problems utilizing advanced statistical techniques, and specialized expertise in the organization and/or industryAssess data management platformsAssess the complete landscape of a data refinery (data discovery, data loading, data transformation, data validation and data publish)Technical Skills: 1-5+ years professional work experience as a data engineerExtensive experience building highly optimized and scalable data infrastructures using traditional, open sources and cloud computing technologiesDeep experience in data warehousing, data architecture, data quality processes, data warehousing design and implementation, table structure, fact and dimension tables, logical and physical database design, data modeling, reporting process metadata, and ETL processesProven experience in developing analytics strategy, roadmap and delivering major change initiativesProven experience in negotiating contracts, license optimization, service levels & managing vendorsAbility to set and manage priorities judiciouslyJob Type: ContractExperience:relevant: 1 year (Preferred)"
192,Data Engineer - FinCrime,Revolut,,"San Francisco, CA","ABOUT THE TEAM
Data sits at the heart of Revolut and plays a uniquely crucial role in what we do. With data we build intelligent real-time systems to personalise our product, tackle financial crime, automate reporting, track team performances and enhance customer experiences.
Fundamentally, data underpins all operations at Revolut and being part of the team gives you the chance to have a major impact across the company – apply today to join our world class data department.

ABOUT THE ROLE
You will work with the Financial Crime - Transaction monitoring team. We are a full stack team of data engineers, data scientists, backend engineers and financial crime specialists who are solving some of the hardest anti-fraud and anti-money laundering problems in the world. Our team builds artificial intelligence (AI) driven systems that continuously learn anomalous patterns typical of fraud. In this regards, we need a senior data engineer to scale our various microservices and machine learning systems. If you love thinking analytically about big data computations at microsecond latencies, then this is your gig!

What you'll be doing:You will be responsible for writing scalable backend systems that use a machine learning model to score users in near real-time.You will research and productionize stream processing algorithms that can efficiently compute various statistics (mean, standard deviation, percentiles) on a data stream.
You will work on a small team of other data engineers to standardize features across our many different models into a unified feature store.
At Revolut, our data and backend engineers are also responsible for “dev ops” of the systems they write and hence, you will be responsible for building and maintaining our Kubernetes based deployment pipeline along with pagerduty and alerting instrumentation.
Our data stack is based predominantly on Python on the backend with Exasol as our data warehouse. We are hosted on Google Cloud Platform and our data scientists and engineers rely heavily on DataFlow, Big Query, Composer and Apache Beam, for their machine learning pipelines.


WHAT SKILLS YOU’LL NEED
Fluency with Python3 years (or more) experience in back-end development or data engineeringBachelor’s Degree (or above) in Computer Science/Maths/Physics/similarPrevious background in working with machine learning or data engineering teams is a plus (not required)Quick learner with an ambitious and results driven personalityWorking well as part of a team in a fast-paced environmentExcellent communication and organisational skills


A LITTLE ABOUT US
We believe that there are better ways for people to control their money. Easier, fairer ways.
Revolut started in 2015 with card transactions abroad without rubbish exchange rates or hidden fees. We’ve since added business accounts, vaults, insurance and even access to cryptocurrency exposure.
We reached 5 million customers in June 2019 and we’re adding another million every quarter.

WHAT WE ARE LOOKING FOR
From employee 1 to employee 1000, you will be slotting into a global team that shares a number of traits.
You can work autonomously and take ownership. We thrive with the space and responsibility to solve problems.
You operate best without lots of bureaucracy. We don’t hide behind fancy job titles or clunky processes ‘because that’s how things are done’.
You approach work in a logical way. We are not afraid to make mistakes but we use data and logic to backup decisions and improve understanding.
And you share our mission to improve people’s relationship with their money.

THE BENEFITSCompetitive salaryBiannual equity bonusesAll the latest tech you needSkip the commute and work from home once a weekRoll with a free Revolut Metal subscription

Please only submit an application for one posting."
193,Investment Data Engineer,State of Washington State Investment Board,"$90,000 - $130,000 a year","Olympia, WA","Description
The Washington State Investment Board (WSIB) is seeking highly qualified candidates to fill an Investment Data Engineer position within the Investment Data Office. This is a full-time exempt position, reporting to the Investment Data Manager. The Investment Data Office is responsible for the curation, integrity and governance of all WSIB Investment data. The team analyzes market and portfolio data to provide critical and timely reporting for investment teams.

Duties
This position is focused on technical management of all of our investment data through its complete lifecycle. This includes analyzing all of our investment portfolio holdings and providing executive-level insight into our $134+ billion dollar portfolio that spans 74 countries and 40+ currencies.

Designing, developing, troubleshooting, evaluating, and deploying data management and business intelligence systems, enabling investment teams to access critical reports.
Develop and maintain high functioning relationships with investment teams and support investment allocation decisions with data and analysis.
Advance the use of modern, cloud-based data analytic technologies to improve access to data via mobile platforms and harness the power of scalable computing, modern algorithms, and tools.
Interrogate large quantities of data and identify trends that lead to predictive analytics that anticipate impacts across aggregate investment datasets.
Aid the agency in providing timely, consistent, and reliable investment master data.
Have a deep understanding of our portfolio holdings.
Qualifications
REQUIRED QUALIFICATIONS:

A passion, creativity, and curiosity about data, technology and their intersection with finance.
A strong grasp of SQL and at least one scripting or programming language (e.g. C#).
Knowledge and comfort working with cloud data platforms (e.g. Azure).
Bachelor's Degree in Computer Science or a related technical field.
Excellent written and verbal communication skills highlighting the ability to distill ideas and concepts succinctly.
Ability and interest to build expertise and knowledge in investment management and finance.
A positive, forward looking approach that is solution focused and team oriented.

DESIRED QUALIFICATIONS:

Experience with and detailed knowledge of data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures, and hands-on SQL coding.
Demonstrates intellectual curiosity resulting in influence over what data queries are run, what data the agency should be interested in evaluating, and identifying what the agency might not know but would be interested to know or understand.
Master's degree in Computer Science or a related technical field, or five years of relevant experience.
Strong proficiency in data analysis (e.g. Python, R), particularly with large, complex datasets.
Reporting and data visualization experience (e.g. Tableau, Power BI).
Master's degree in Computer Science or a related technical field.
CFA certification or prior experience with investment management.
Supplemental Information
Failure to follow the application instructions below may lead to disqualification.

Complete application profile.
A cover letter specifying why this position is of interest to you and how you meet the qualifications listed above. This letter should be no more than two pages.
A current chronological resume.

SPECIAL NOTE:
Prior to a new hire, a background check including criminal record history will be conducted. Information from the background check will not necessarily preclude employment but will be considered in determining applicant's suitability and competence to perform in the position.

CONTACT:
hr@sib.wa.gov

Why work at WSIB?
WSIB is a respected institutional investor and thought leader in its industry. A solid reputation and large scale offer the opportunity to invest in the world's leading investors on behalf of more than 500,000 public employee beneficiaries.

We are located in Olympia, on the southernmost tip of Puget Sound. Olympians enjoy a quality of life enhanced by natural beauty and a mild year-round climate that promotes outdoor activities such as skiing, sailing, fishing, hiking, kayaking, and mountain climbing. Its close proximity to Seattle provides a variety of cultural and culinary experiences, while its residents enjoy a lower cost of living. Within a two-hour drive from Olympia are the Pacific Ocean, the Cascade and Olympic Mountains, and Seattle.

WSIB Offers:

Opportunities for professional development, training, growth and advancement.
Tuition reimbursement.
A comprehensive benefits package, including health, dental, life and long-term disability insurance, as well as vacation, sick, military and civil leave, and 11 paid holidays per year.
Membership in the Public Employees' Retirement System.
Opportunities to participate in the Deferred Compensation and Dependent Care Assistance Programs.

The Washington State Investment Board is an equal opportunity employer. Women, racial and ethnic minorities, persons of disability, persons over 40 years of age, and disabled and Vietnam era veterans are encouraged to apply. Persons of disability needing assistance in the application process, or those needing this announcement in an alternative format, please contact Mary Hougan, Human Resources Consultant at (360) 956-4716 or via email to hr@sib.wa.gov."
194,Data Engineer II - Payment Acceptance & Experience,"Amazon.com-Amazon.com Services, Inc.",,"Seattle, WA","Bachelor’s Degree in Computer Science or related field, or 4+ year relevant work experience2+ years professional experience in software developmentComputer Science fundamentals in object-oriented designComputer Science fundamentals in data structuresComputer Science fundamentals in algorithm design, problem solving, and complexity analysis Proficiency in, at least, one modern programming language such as C, C++, Java, or PerlExperience with AWS technologies2+ years professional experience in data engineeringData engineering fundamentals in data set designData engineering fundamentals in data architecture

The Payments Acceptance & Experience (PAE) organization makes paying on Amazon possible for millions of around the world. The PAE Data team is a cross functional team of data engineers, scientists, and business intelligence professionals that are constantly looking to improve data availability and accessibility for a variety of use cases. Our team provides a full-stack data solution - everything from building data capture mechanisms into software services to building business forecasts and deploying machine learning (ML) models. We are looking for motivated individuals to contribute immediately to our team in Seattle, Washington. The selected candidates will act as data engineers to design and develop sustainable, large-scale data solutions that integrate with various software and ML services using AWS building blocks whenever possible. We are also looking to develop a flexible ML data suite, with a feature repository, run time feature engineering capabilities, and versioning control to accelerate model building and deployment, so functional SDE skills are required for these positions.

Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation

Experience building complex software systems that have been successfully delivered to customersKnowledge of professional software engineering practices & best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operationsAbility to take a project from scoping requirements through actual launch of the projectExperience in communicating with users, other technical teams, and management to collect requirements, describe software product features, and technical designs.
Amazon is an equal opportunity employer"
195,Data Engineer,"Episode Solutions, LLC",,"Nashville, TN 37205","Description:
Job Description

Scope and Opportunity:
This highly visible role provides our BI developers, analysts and data scientists access to various data sources related to bundled payments business. The role applies engineering skillset, analytical thought processes, business judgment, project management, and reporting expertise to translate financial and clinical data into data sets that result in various analyses, recommendations, and that ultimately help guide the decisions of the organization. The ideal candidate should want to make a substantial impact on decision-making, to learn quickly in a dynamic, high-growth healthcare technology company, and to be a mentor and in-demand partner across the organization.

Under general supervision, the Data Engineer responsibilities include, but are not limited to developing, constructing, testing and maintaining architectures, such as databases and large-scale processing systems. This candidate will focus on data flow development & big data and should be an expert in data warehousing solutions and be able to work with the latest database technologies. The data engineer needs to understand how to apply technologies to solve data problems and to develop big data solutions. The candidate will be responsible for expanding & optimizing our data and data pipeline architecture, as well as optimize data flow & collection for cross functional teams.

Primary Responsibility and Essential Duties:

Architecting highly scalable distributed systems, using different open source tools
Use many different scripting languages, understanding the nuances and benefits to each, to combine systems
Research & discover new methods to acquire data, and new applications for existing data
Create reliable, fault-tolerant data pipelines
Architect & combine data stores
Define data retention and privacy related policies
Manage organization’s big data infrastructure, and can obtain results from vast amounts of data quickly
Collaborate with BI developers, analysts and data scientists & build the right solutions for them
Create data tools for different functions within the organization
Perform root cause analysis on internal and external data and process to answer specific business questions & identify opportunities for improvement
Other duties as assigned.

Nothing in this job description restricts management’s right to assign or reassign duties and responsibilities to this job at any time.

Requirements:
Qualifications:

BS in Computer Science or Software Engineering, Master’s preferred
3-5 years of Data Engineering or Backend Software Engineering experience
Experience with object-oriented design, coding and testing patterns, solid understanding of how algorithms work and have experience building high performance algorithms
Experience with Python and other scripting languages like shell scripting
Expert in various flavors of SQL/NOSQL, T-SQL experience preferred
Experience with data modeling, data warehousing, ETL/ELT development, building and optimizing ‘big data’ pipelines
Experience with big data tech stack, such as Hadoop, Spark, Kafka or other frameworks
Experience with C#, data factory and Azure eco system preferred
Experience with multi cloud systems a plus
DevOps experience a plus

Other Attributes

Excellent communication skills, both verbal and written
Strong analytic skills related to working with related to working with unstructured datasets
Ability to work corroboratively in a team environment, as well as independently
Ability to work & communicate with all levels of the organization
Ability to influence people, build relationships & collaborate
Effectively handle multiple projects simultaneously in a deadline driven environment
High level of Business Acumen
Work with little direction or guidance and take ownership of his/her work

Work Environment
This position operates in a professional office environment. The role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets and fax machines.

Physical Requirements:
Work is performed in a clean office environment. While performing the duties of this job, the employee is regularly required to remain in a stationary position more than 50% of the time; occasionally move about inside the office; operate a computer and other office productivity machinery; occasionally ascend/descend stairs; frequently pulls and removes files from cabinets, occasionally in the basement and other areas; frequently communicates with various internal and external populations including individuals representing legal and governmental agencies.

Travel:
10% travel may be required."
196,Data Engineer,Walmart,,"Bentonville, AR 72712","Position Description

A Data Scientist is responsible for analyzing large data sets to develop custom models and algorithms to drive business solutions. Data Scientists work on project teams in order to provide analytical support to projects (for example, email targeting, business optimization, consumer recommendations) for Walmart eCommerce. Data Scientists are responsible for building large data sets from multiple sources in order to build algorithms for predicting future data characteristics. Those algorithms will be tested, validated, and applied to large data sets. Data Scientists are responsible for training the algorithms so they can be applied to future data sets and provide the appropriate search results. Data Scientists are responsible for researching new trends in the industry and utilizing up-to-date technology (for example, HBase, MapReduce, LAPack, Gurobi) and analytical skills to support their assigned project.
Build complex data sets from multiple data sources, both internally and externally.
Build learning systems to analyze and filter continuous data flows and offline data analysis.
Combine data features to determine search models.
Conduct advanced statistical analysis to determine trends and significant data relationships.
Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans
Develop custom data models to drive innovative business solutions.
Develop models of current state in order to determine needed improvements
Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity
Provides and supports the implementation of business solutions
Research new techniques and best practices within the industry.
Scale new algorithms to large data sets.
Train algorithms to apply models to new data sets.
Utilize system tools including (MySQL, Hadoop, Weka, R, Matlab,ILog).
Validate models and algorithmic techniques.
Work with cross-functional partners across the business.

Minimum Qualifications

Bachelor of Science and 5 years' data science experience OR Master of Science and 2 years' data science experience.

Additional Preferred Qualifications

Company Summary
Local Theory is a portfolio company under Walmart’s startup incubator Store No. 8. At Local Theory, we are obsessed with finding data science solutions to challenging problems in Merchandising.We believe that the relevance of the products we sell in our stores is the life blood of Walmart. Although Walmart is the largest retailer in the world, we want each individual store to be tailored to the community and optimized for the associates serving that community. At Local Theory, we are passionate about ideas; our default is to move fast and iterate to develop them. We are building a team of optimistic and talented associates in an environment which has the dynamism of a startup but the resources and data of the world’s largest corporation."
197,"Data Engineer, Woot!",Amazon.com-Woot Services LLC,,"Seattle, WA","§ Bachelor’s degree in computer science, mathematics, statistics, economics, or other quantitative field
§ 3+ years of relevant work experience in a role requiring application of analytic skills to integrate data into operational/business planning or advanced degree
§ Strong experience with ETL development, data modeling, data warehousing, MySQL, and databases in a business environment with large-scale, complex datasets
§ Advanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management as required.
§ Experience in gathering requirements and formulating business metrics for reporting.
§ Experienced working in a fast-paced, high-tech environment and comfortable navigating conflicting priorities and ambiguous problems
§ Strong track record in converting data analysis into tangible and significant real-world changes.
§ Strong grasp of quantitative data analysis and statistics.
§ Python scripting experience
§ AWS or Azure production development and managment
§ The ability to exclaim Woot!

Do you feel passionate about data? Are you excited about figuring out ways slice, dice, and send data to multiple sources? Woot.com, the innovative company who invented the deal-of-the-day business model, is looking for a talented, self-motivated Data Engineer.

Woot specializes in daily deals across multiple categories and offers our customers unique content giving them many reasons to visit our website and mobile apps frequently.

As a result of strong business growth, we are looking for a DE to join our Data Team to further build and automate our reporting, high value action (HVA) recommending, and business prioritizing system to enhance team efficiency and effectiveness. The ideal candidate will be passionate about contributing to the team growth through his/her expertise in utilizing big data technology to answer business questions and identify growth opportunities. This person will build new business intelligence solutions as an owner end-to-end, at the same time, collaborate with data engineers and data scientists to automate recommendation and prioritization system.

To be successful in this role, you should have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards and using visualization tools, always applying analytical rigor to solve business problems. You should have strong business and communication skills and be able to work closely with product managers and business units to define key business questions and build data sets and models that answer them. Our DEs are expected to have a detailed understanding of our business and never lose sight of the broader problems that we are trying to solve.

The primary responsibilities of this role include:
Design, develop and maintain scalable, automated, user-friendly systems, reports, dashboards, etc. that will support our analytical and business needsWrite SQL code to retrieve and analyze data from database tables (ex. Redshift, MySQL, DBs), and learn and understand a broad range of Amazon’s data resources and know how, when, and which to use and which not to useDevelop queries and visualizations for ad-hoc requests and projects, as well as ongoing reportingCreate pipelines for automated Use analytical and statistical rigor to solve complex problems and drive business decisions.Develop Machine Learning modelsWrite scripts in Python for data processing
Congrats on making it this far! Woot has Screaming Monkeys, lots of Screaming Monkeys, oh, and bags of crap! If that's your thing, hit the ""apply"" button ASAP.

§ MBA or Master’s degree in Computer Science, Engineering, Statistics, Mathematics or related field
§ Expert in writing and tuning SQL scripts
§ Experience working in very large data warehouse environments
§ 3+ years of experience in a data engineer or BIE role with a technology company
§ Experience conducting large scale data analysis to support business decision making
§ Strong verbal/written communication and data presentation skills, including an ability to effectively communicate with both business and technical teams
§ Be self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty data.
§ Strong verbal/written communication & data presentation skills, including an ability to effectively communicate with both business and technical teams.
§ Strong dashboarding skills with Tableau/Microstrategy/Looker, etc.
§ Understand the meaning of Moofi"
198,Big Data Engineer,Avani Systems,,"Seattle, WA","Job Description:

Ensure high throughput of development teams by identifying potential issues, removing impediments or guiding the team to remove impediments by collaborating with the appropriate resource
Manage sprint planning and execution which includes the management of project progress and provide status and visibility
Facilitate release planning and scheduling by providing empirical Scrum team statistics, identifying project dependencies, and creating velocity forecasts
Assist with internal and external communications to improve transparency and radiate information ensuring the team’s progress and successes are highly visible to all stakeholders including the team itself (e.g. backlogs, burn down/up charts, etc.)
Develop pipelines using copy activity from different sources like FTP, Windows Blob Storage, SQL SERVER, COSMOS big data etc. and scheduling the pipelines as per requirement using azure data factory.
Requirements:

Required minimum Bachelor’s degree in Computer Science"
199,Data Engineer,ZEFR,,"Marina del Rey, CA","ZEFR is hiring! Our Software Engineering team is hiring a Data Engineer to be involved in designing and building large-scale applications and systems to acquire, process, store, and illuminate multi-terabytes of YouTube, Facebook and other social media data. This role is an important part of the rapidly scaling infrastructure and data management demands of being the leader in VideoID technology for enterprise media companies and content owners.

Technology @ ZEFR:
Languages: Python, Scala, Java, and Kotlin
Data stores: PostreSQL, Elasticsearch, Redshift
Data processing: Apache Kafka, Apache Spark
DevOps: Jenkins, Docker, Terraform, Ansible, AWS ECS, AWS EMR

Here's what you'll get to do:

Provide seamless and timely data access for your users
Build reliable and dependable ETL
Build and maintain production machine learning infrastructure
Troubleshoot complex issues in distributed systems
Debate data processing philosophies and methodologies with your team

Here's what we're looking for:

Bachelor's or Master's degree in Computer Science or related field
Fluency with Python, Java, Kotlin, or Scala
Experience with distributed systems
Strong foundation in data structures, algorithms and software design
Experience with digital media, social media, and video APIs such as YouTube's Data API is a big plus
Thorough testing and code review standards/practices
Strong verbal and written communication skills
Openness to new technologies and creative solutions

"
200,Big Data Engineer,StatFunding,,"Miami, FL","Location:
Miami, FL
The Role:
Define architecture;
choose technology stack, research new technologies, and prototype new ideas;
Plan architecture rollout and technology stack updates;
Define standards and best practices to ensure consistency and high quality of UI design and implementation;
Work with Product Management to understand requirements and translate them into UI design and implementation;
Work with UX team to define and implement reusable UI components;
Participate in building Scrum process and Agile culture in the company;
Ideal Candidate:
3+ years developing front end applications and reusable UI components for web and mobile applications;
Genuine interest in UI design and development;
Ability to define and drive front end technology strategy;
Solid understanding of Object Oriented JavaScript Programming using Design and Patterns;
In-depth understanding of Javascript, Ajax, HTTP, HTML templates (e.g., Handlebars), document object model and performance in the browser;
Experience in test driven development using JavaScript testing frameworks such as Jasmine and Mocha;
Experience with Angular, Backbone, Node, NPM, Gulp, Grunt, Bootstrap;
Expert knowledge of Ruby and PHP;
Expert knowledge of Oracle, Hadoop, NoSQL, MySQL, SQL, Postgres ;
Expert knowledge of common engineering concepts (algorithms, design patterns, and modularization) ;
Expertise with cross-browser, cross-platform, and design constraints on the web;
Experience with REST APIs;
Email: jobs@StatLending.com"
201,Data Engineer - Python,FanDuel,,"Los Angeles, CA","About Us:
FanDuel Group is an innovative sports-tech entertainment company that is changing the way consumers engage with their favourite sports, teams, and leagues. The premier gaming destination in the United States, FanDuel Group consists of a portfolio of leading brands across gaming, sports betting, daily fantasy sports, advance-deposit wagering, and TV/media, including FanDuel, Betfair US, DRAFT, and TVG. FanDuel Group have a presence across 45 states and 8 million customers. The company is based in New York with offices in California, New Jersey, Florida, Oregon, and Scotland.

Our competitive edge comes from making decisions based on accurate and timely data. As a Data Engineer, you will help us build scalable systems to provide access to that data across the company.

What we're looking for

We are looking for an experienced Data Engineer, ideally well versed in Python, with a deep understanding of large scale data handling and processing best practices in a cloud environment. You should be comfortable building complex yet performant SQL queries on large data sets. Our current stack is built on AWS with Spark and Hive on Amazon EMR for batch processing and Redshift for the data warehouse. Experience working with and tuning these for large scale workloads would be a plus.

Data is a key component of the business used by almost every facet of the company including product development, marketing, operations and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability. We operate a rigorous code review process, so you need to be able to continuously give and take feedback and act on it.

As our data is always growing it is important that we have a cost effective data warehouse with data that is modelled to suit our users needs.

Looking ahead to the next phase of our data platform we are keen to do more more with real time data processing and working with our data scientists to create machine learning pipelines. We would love to hear how you have tackled these before.

What you can expect:

A supportive, trusting and open work environment.
Opportunity to work on a variety of projects.
Collaborate on shared resources, tooling, infrastructure and platform.

What you get in return


You'll be working for an organization that creates a fun product with a sports centric user base.
We invest in our team and adapt to fulfill personal development goals, learning new skills and educating others
Enjoy a fun office environment.
We offer competitive compensation, benefits packages, Value Creation Plan, flexible vacation policy.

FanDuel is an equal opportunities employer. Diversity and inclusion in FanDuel means that we respect and value everyone as individuals. We don't tolerate bias, judgment or harassment. Our focus is on developing employees so that they reach their full potential.harassment. Our focus is on developing employees so that they reach their full potential."
202,Data Engineer,Butterfly Network,,"New York, NY","Job Description

At Butterfly Data Engineering we are fusing a diverse set of data streams such as manufacturing data, commercial data, real-time IoT data, mobile analytics data, logistics data to continuously improve our product design, guide feature development and optimize manufacturing and operations.

We seek data engineers with a track record of rapidly implementing and maintaining elegant and robust data pipelines and creating self-service data analytics infrastructure that immediately create value for business owners.

Qualifications

Your area of expertise includes
Data modeling and hands on development and administration of relational and NoSQL databases (BigQuery, Elastic Search)
 Principles and patterns of data processing architectures (batch, streaming, lambda, serverless, etc)
 Dev-Ops experience managing services on GCP (Ansible, Docker, Kubernetes)
 Hands on knowledge of ETL tools, frameworks and practices: Airflow, Python 3
 Data Visualization and dashboard systems (Tableau, Plotly, Seaborn, Matplotlib)
 Strong programming experience in Python (Pandas, Numpy, Notebooks)
Additional Information

In addition to working with colleagues and advisors who are the best and brightest in their fields, building a revolutionary product and helping to save lives, we offer great perks:
Fully funded medical insurance, dental and vision coverage
401K plan
Competitive salary and equity in the company
Free on-site meals, unlimited healthy snacks
Brand new office in Flatiron
Pre-tax commuter benefits
Butterfly Network is an equal opportunity employer regardless of race, color, ancestry, religion, gender, national origin, sexual orientation, age, citizenship, marital status, disability, or Veteran status."
203,Data Engineer – Data Warehouses,ViaSat,,"Carlsbad, CA 92009","We are currently seeking contributors to leverage the latest RDBMS and Big data technologies to instrument, ingest, store, process, and analyze the torrent of data our operational systems produce.
We have a mix of traditional RDBMS and MPP databases and are looking for someone who knows how to organize data (curate) in a human understandable way, understands how to select the correct tool for the job, and is excited to learn new things.
The Data Engineer will work as part of a Data Warehouse team and will report to the Data Warehouse Architect. Primary responsibilities include curating data processes that populate databases as well as troubleshooting, monitoring and coordinating defect resolution related to ETL processing.
Collaborating with the architect, analysts, data scientists, and developers, the Data Engineer will resolve complex data design issues/provide optimal solutions that meet business requirements and benefits system performance.
Responsibilities
Design and build automated analytic processes including testing, identifying bottlenecks, and data quality issues
Provide operational support for the data warehousing applications, including quarantine maintenance, troubleshooting, monitoring and resolving bugs and production issues in a timely manner
Develop, manage and maintain data dictionaries, metadata, and process flow documentation
Work with business data stewards to research and identify data quality issues
Using data quality tools to profile source data, define and validate metadata, and verify staged data
Implement slowly changing dimensions as well as transaction, accumulating snapshot, and periodic snapshot fact
Requirements
2 - 5 years’ experience working with data warehouses/marts/stores/feeds
2 - 5 years’ experience query mining/reporting for business analytics
2 - 5 years’ experience designing and developing extract, transform and load (ETL) processes
2 - 5 years’ experience in SQL development on RDBMS (Postgres preferred)
BS in Computer Science, Computer Engineering, Software Engineering, Electrical Engineering, Math, Physics or related field
Experience with data modeling/data workflow diagrams (conceptual, logical and physical)
Extensive experience in ETL and DB performance tuning
Hands on experience with any scripting language (Python, Ruby, BASH, etc)
Preferences
Experience in the telecommunications industry or knowledge of subscription based services
Experience with Hadoop, Spark, Kafka, Impala, or other big data technologies
Our Carlsbad, CA office is just 30 minutes north of San Diego and 5 minutes from the beach. We know there is more to life than work, and with full gym access, volleyball/basketball courts and meal services onsite, you’ll never want to leave our beautiful campus. You can also reach out to help others in the community by being involved in our VPartners program.

We are searching for candidates who enjoy working with people and have a technical mind that excels when being challenged. If you have a drive to succeed and grow your career, ViaSat might be a fit for you!"
204,"Data Engineer, Global Specialty Fulfillment","Amazon.com Services, Inc.",,"Seattle, WA","BS in Computer Science, Math, Physics, or Engineering6+ years relevant work experience in software development or related data-driven fieldKnowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and computingDemonstrated experience in relational database concepts with an expert knowledge of SQLDemonstrated ability in data modeling, ETL development, and Data warehousing

Love food? We do! The AmazonFresh and Prime Now operations finance team is seeking an experienced and innovative Data Engineer to build tools that support Operations teams in AmazonFresh and Prime Now. We are an analytics team responsible for building tools, analysis, and reporting to support internal leaders within fulfillment, last mile, and supply chain operations. This is a unique opportunity for someone interested in Amazon’s start-up consumables-focused environment. AmazonFresh and Prime Now experiment, fail fast, learn, and scale rapidly.

Ultra-fast delivery delights Amazon customers by delivering what they want quickly: medication for a sick kid, lunch at work when you forgot, food and drinks for a party, last minute gifts, dinner from a local restaurant, and so many more uses.

The business model of ultra-fast delivery is attractive, and offers our Engineering team the opportunity to work on any number of complex technical problems. Our team designs, builds and owns our end-to-end services from the ground up and works on large scale back-end systems to support the entirety of our order and inventory pipelines.

We are seeking Data Engineer. In this role you will:

You help build the infrastructure to answer questions with data, using software engineering best practices, data management fundamentals, data storage principles, and recent advances in distributed systems
You manage AWS resources.
You collaborate with Business Intelligence Engineers (BIEs) to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation
You help drive the architecture and technology choices that enable a world-class user experience
You develop expertise in a broad range of Amazon’s data resources and know when, how, and which to use and which not to use
You encourage the organization to adopt next-generation data architecture strategies, proposing both data flows and storage solutions
You are comfortable with a degree of ambiguity and willing to develop quick proof of concepts, iterate and improve
You create extensible designs and easy to maintain solutions with the long term vision in mind
You have an understanding and empathy for business objectives, and continually align your work with those objectives and seek to deliver business value. You listen effectively.
You are comfortable presenting your findings to large groups

We have a very flat team structure, and offer a unique opportunity for technical leaders who want to work closely with the business in defining, designing, building and operating products that are in the early stage of fast expansion.

Experience working with AWS Big Data TechnologiesExperience working with Open Source Big Data toolsProven track record of delivering a big data solutionExperience developing tools for data engineers and machine learningExperience working with both Batch and Real Time data processing systems
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Vet / Gender Identity / Sexual Orientation / Age"
205,Data Engineer,Alion Science and Technology,,"Linthicum Heights, MD","Responsibilities/Qualification
Enlighten IT Consulting, a MacAulay-Brown, Inc. company is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets from across the Department of Defense Information Network (DoDIN). The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either an analyst, engineer, architect, or developer. The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past. To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation. The candidate will work both independently and as part of a large team to accomplish client objectives. The candidate will regularly brief senior leadership on progress related to the objectives.
Required Skills:
B.S. degree in Computer Science, Information Technology, Electrical Engineering, Statistics, or equivalent fields
5+ years of professional experience, minimum of which 3 years were in a Government/ Military/ Contractor role
Demonstrated experience in writing data query, normalization and transportation software tools
Proficient understanding and usage of MS Word, Excel, PowerPoint, and Visio
Strong written and oral communication skills
Strong critical thinking skills
Ability to present information in a clear and concise manner to diverse audience
Ability to travel to DOE customer sight whenever needed.
Desired Skills:
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
#CJMB

Security Clearance
A current Secret level security clearance is required and therefore all candidates must be a U.S. Citizen.
Diversity Statement
Women, minorities, individuals with disabilities and veterans are encouraged to apply. Alion will provide a reasonable accommodation to individuals with disabilities and disabled veterans who need assistance to apply. Please visit the Alion Careers site for more information

U.S. Citizenship Required."
206,Data Engineer,LendingTree,,"Charlotte, NC 28277","The Position:

The My LendingTree platform team at LendingTree works very closely with the analytics team to make data driven business decisions. You will be partnering with the platform team and BI/database team to provide the right insights by ensuring data quality. Which means you should love data (unstructured and structured) and have the unique blend of business insight with technical skills. You constantly ask questions about how to use analytics to improve the business and constantly make impactful strategic decisions. You enjoy collaborating with teams across digital and traditional channels to understand and support their goals.

Responsibilities:

You will partner with product team to help make the right business decision.
Collaborate with engineers, product managers and product analysts to understand data needs.
Lead data warehouse plan for MyLT platform.
Build data expertise and own data quality for allocated areas of ownership.
Define and manager SLA for all data needs.
Work with BI team to triage data issues and work towards a resolution.
Define critical metrics and craft standard reports and dashboards to analyze performance; work with cross-functional teams to set targets and measure performance.
Requirements:

Bachelor’s Degree in Technical Field, Computer Science or Mathematics.
4+ years’ experience in writing SQL statements and with schema design and dimensional data modeling.
Experience with Tableau is preferred.
Proficiency in Excel, PowerPoint, and working with large data sets.
Experience with programming languages like Java or Python a plus.
Ability to synthesize data from multiple sources to drive actionable insights.
Ability to analyze data to identify deliverables, gaps and inconsistencies.
Interpersonal skills including the commitment to identify and communicate data driven insights.
Problem-solving & critical thinking skills. You know how to rip apart difficult problems and construct elegant solutions.
Pragmatic: you know the 80/20 rule and know when to boil the ocean and when not to. The solutions you develop are understandable and implementable.
Self-directed. You check in regularly with your manager, but you can run with a ton of autonomy.
Great teammate with competitive spirit and inquisitive mind.
Motivated to work hard in a dynamic environment – business decisions here can connect and impact millions of users.


About LendingTree:

LendingTree was founded in 1996 by CEO Doug Lebda to help people comparison shop and get a great deal on the single biggest transaction of their lives: their mortgage. In doing so we fundamentally disrupted the mortgage industry making it much more competitive and consumer focused, while helping over 30 million people in the home buying and refinancing processes. LendingTree is now a well-known household brand which has expanded to include comparison shopping for credit cards, student loans, personal loans, auto loans, and business loans as well as related categories like home services and online college degrees. We are quite literally the place consumers come to shop for a better deal on their money.

LendingTree is publicly traded (TREE) and our stock has delivered impressive performance month after month. If you’re looking for an opportunity with a dynamic company that is fanatically pro-consumer and that champions your entrepreneurial spirit, you’ve come to the right place!

Our Culture:

Our clothes are casual and relaxed, and our work ethic is highly professional. It is our culture for each team member to challenge the status quo, express their opinions, and to stand up, ask for the ball and run with it to meet our aggressive goals. We also have a lot of fun together! We’re always looking for the best, brightest, high energy, results-driven Rock Stars to join our team. We reward innovation, creativity and the ability to just GET STUFF DONE.

LendingTree is the kind of company that not only promotes diversity and inclusion; we thrive because of these values. We don’t discriminate based on race, color, religion (or creed), gender, gender expression, age, national origin, disability, marital status, sexual orientation or military status."
207,"Data Engineer, Global Consumer BI","Amazon.com Services, Inc.",,"Seattle, WA","Bachelors in Computer Science, Engineering, Statistics, Mathematics or related fieldStrong experience with Redshift and OracleDemonstrated ability in ETL developmentDemonstrated ability in writing and tuning SQL scriptsExperience in Python, or other scripting languageSolid communication skills and team playerDemonstrated ability to manage and prioritize workload

Amazon is seeking an exceptional Data Engineer to join the Global Consumer BI team, which is part of the Consumer FP&A team. The person in this position will play a key role in supporting the business decisions for our global Consumer business.

Our team develops and maintains datasets which are considered to be the “single source of truth” for the entire Consumer organization. We support a variety of datasets, tools and reports used by Consumer leadership to drive the business. This role will focus on:

Contributing to the development of our consolidated and optimized datasetsDeveloping new analytical platforms to support in-depth analyses of business performance
There will be opportunities to utilize emerging technologies such as Redshift, EDX and Dataforge, as well as EMR and other big data solutions. The ideal candidate will be passionate about building datasets, that support reports and tools which are leveraged across the globe and provide unique insights to leaders of the Consumer organization.

The successful candidate will have strong technical acumen and experience in report development, strong SQL and ETL experience, strong communication skills, with an ability to work effectively with cross-functional teams and an ability to work in a fast-paced and ever-changing environment.

Master’s Degree in Business, Engineering, Statistics, Computer Science, or MathematicsExperience using very large datasets5+ years prior experience in a data engineering or BI role with a technology company or financial institution.Experience using big data solutions (e.g. EMR, Hadoop)
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation"
208,Data Engineer,Datalitical,,"Dallas, TX","Datalitical Inc is one largest consulting firms in U.S and with lots of engagements. We are a diverse community of people, all working together to bring high-end solution to any business problem and provide the best service to our clients. We’re looking for talented individuals who want to work in an energetic, respectful, collaborative environment. With a wide array of jobs, internships, training and more, there are countless opportunities for you to grow your career with us.

Responsibilities:
Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth at Datalitical Consistently evolve data model & data schema based on business and engineering needs Implement systems tracking data quality and consistency Develop tools supporting self-service data pipeline management (ETL) SQL and MapReduce job tuning to improve data processing performance
Experience & Skills:
Extensive experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet) Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle) Good understanding of SQL Engine and able to conduct advanced performance tuning Strong skills in scripting language (Python, Ruby, Perl, Bash) Experience with workflow management tools (Airflow, Oozie, Azkaban, UC4) Comfortable working directly with data analytics to bridge business requirements with data engineering
Bonus:
MPP database experience (Redshift, Vertica, Teradata) Experience with building tools to support self-service pipeline Experience with one of the messaging system (Kafka, SQS, Kinesis) and different data serialization (json, protobuf, avro)

Our corporate office supports and offers a competitive benefits package including medical/dental/vision, term life insurance, paid vacation/holidays, 401(k) savings plan with company match.

Apply: hr@datalitical.com"
209,"Data Engineer, AmazonSmile","Amazon.com-Amazon.com Services, Inc.",,"Seattle, WA","Candidates are expected to have the following qualifications:
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline2+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasetsData Warehousing Experience with Oracle, Redshift, PostgreSQL, etc.Query performance tuning skillsCoding proficiency in at least one modern programming language (Python, Ruby, Java, etc)Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, etc.)Knowledge of data management fundamentals and data storage principles

AmazonSmile is redefining how technology, business and philanthropy intersect and is putting the scale of Amazon to good use by creating innovative products that both engage customers and provide much needed assistance to charitable organizations. Having donated over $130 million in the last five years to thousands of charities, we are poised to grow even larger.

The team is looking for a talented Data Engineer who will design and build the future data infrastructure for the team. AmazonSmile’s systems interact with millions of customers and orders per day, and we need someone who can transform how we think about the massive data sets we own and create efficiencies in the processing of tens of terabytes of data each day.

Your role will include:
The design, implementation, and support of a platform providing access to large datasets.Interfacing with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and other AWS technologiesImplementing data structures using best practices in data modeling, ETL processes, SQL, Redshift, and EMROptimizing the performance of business-critical queriesAdvise SDEs by serving as the data subject matter expert on the teamInterfacing with business customers, gathering requirements and delivering complete reporting solutionsContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers

3+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in a company with large, complex data sources.Experience with MicroStrategyExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data setsExperience working with AWS big data technologies (EMR, Redshift, S3)Demonstrated strength in data modeling, ETL development, and data warehousingProven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Amazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation."
210,Data Engineer - Data Engineering - CIMD Tech,Goldman Sachs,,"San Francisco, CA 94104","MORE ABOUT THIS JOB
CONSUMER (MARCUS BY GOLDMAN SACHS)
Marcus by Goldman Sachs is the firm’s consumer business, combining the entrepreneurial spirit of a startup with 150 years of experience. Today, Marcus has $50 billion in deposits, $5 billion in loan balances and 4 million customers across our lending and deposits businesses, as well as the personal financial management app, Clarity Money. Through the use of insights and intuitive design, we provide customers with powerful tools and products that are grounded in value, transparency and simplicity. We are backed by our unique team, comprised of individual contributors from leading agile technology companies, fintechs and consumer financial services companies, allowing us to disrupt the industry, while helping consumers take control of their financial lives.

RESPONSIBILITIES AND QUALIFICATIONS
HOW YOU WILL FULFILL YOUR POTENTIAL
Design and develop data ingest and transform processesDevelop data visualizations using BI tools and web-based technologiesWork as part of a global team using Agile software methodologiesPartner with Marcus risk, product, acquisition and servicing teamsUse Marcus data to drive change throughout the Marcus business

QUALIFICATIONS
Minimum 3 years of relevant professional experienceBachelor’s degree or equivalent requiredExperience with SQL and relational databasesProficient at Python, Spark and the Hadoop ecosystemSelf-starter, motivated, and good communication skills Strong sense of ownership and driven to manage tasks to completion

CONSUMER AND INVESTMENT MANAGEMENT DIVISION (CIMD)
The Consumer and Investment Management Division includes Goldman Sachs Asset Management (GSAM), Private Wealth Management (PWM) and our Consumer business (Marcus by Goldman Sachs). We provide asset management, wealth management and banking expertise to consumers and institutions around the world. CIMD partners with various teams across the firm to help individuals and institutions navigate changing markets and take control of their financial lives.
ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

Â© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet."
211,Senior Data Engineer,Clarify Health Solutions.,,"San Francisco, CA","COMPANY OVERVIEW
Clarify has amassed an unrivaled dataset in healthcare representing over 150 million lives with longitudinal clinical, financial, social and behavioral data that paints a complete picture of individual wellness. What if you could use this data to precisely identify patients at risk while giving clinicians actionable insights to deliver better care? What if payers could predict the total healthcare cost of a person to optimize their care? What if pharmaceutical companies could bring lifesaving drugs to market faster and with fewer side effects? What if we could all receive a personalized care experience?

Come be part of figuring out the answers to these and other questions.

Clarify has built a machine learning platform on top of this huge dataset leveraging the latest cloud and big data technologies to run hundreds of ML models in real-time to predict and improve health outcomes. Our leadership team hails from top healthcare and fintech companies like Health Catalyst, Advent, and Microsoft. And our business team, from McKinsey and Boston Consulting Group, have decades of experience driving change within some of the largest provider, payer and life science companies.
The Role
Contributing to the design, development, and implementation of custom data models to solve real-world problems.
Conducting machine learning on a growing database of longitudinal health information, with the goal of widening the breadth of applications while increasing the fidelity of our models in an automatable fashion
Introducing new technologies and solutions to our data engineering processes that will facilitate machine learning at a higher scale
Utilizing ETL tools to integrate build production and post-production data pipelines that move data from a variety of sources into a warehouse, monitor data quality, check for errors and conform data to standards
Participating in data team workshops by interpreting business problems and sometimes complex statistical approaches into actions, balancing creativity with engineering practicality.
What we are looking for
We are a small but rapidly growing team. As such, we are ideally looking for “all-around athletes” with strong leadership, analytics, and communication skills. We are also looking for new team members who will be a solid fit with our culture and have a strong passion for impact.
The Senior Data Engineer will have:
Demonstrable experience in applying machine learning and authoring production-level code in an industry setting, preferably with applications in the healthcare field (such as bioinformatics, biostatistics, epidemiology, economics, genomics, or public health)
Self-sufficiency to query relational databases, research new features, and build resources necessary where they are not already existing
Resourcefulness in a variety of machine learning packages in R or Python, with a keen knowledge of optimal selections with respect to predictive performance, computation time, and model interpretability
Familiarity with generalized linear models and healthcare-related statistical methods such as hierarchical/mixed-effects modeling, regularization, survival analysis, and propensity score matching.
Experience with integrating with a wide variety of data sources including web services, files, databases and web pages
BS in computer science, information technology
5+ years direct data science experience
Preferred technologies
PostgreSQL, Redshift
R, Python, and their related configuration management tools
ETL tools such as Airflow, Luigi, AWS Glue
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, on the basis of disability or any other federal, state or local protected class."
212,Data Engineer,Hewlett Packard Enterprise,,"San Jose, CA","Role and Responsibilities: Apply advanced statistical and machinelearning techniques to solve industry problemsand address opportunities Leadtechnical discussions with customersto proposetailored Machine learning models forspecificuse cases Prototype proof of value solutions to answer customers’ requirements. Develop and iterateMachine Learning techniques to solve computer Vision, NLP, Time series / forecasting related problems Workin collaboration withsolution architects and sales teams to integratethefullsolution pipeline Able to relate with the Different R&D labs and able to provide feedback from the industry and customers point of view
Education and Experience:
Master degree/Ph.D in Machine learning, Mathematics, computer science, or related fields.
Experience in project deliveryofmachine learning to solve concrete industry challenges and explore new opportunities
Skills and Abilities:
 Deep understanding of theory and application of ML algorithms (clustering, classification, regression, deep neural networks…). Deep understanding and experience with Computer Vision algorithms, Natural Language processing algorithms, Reinforcement learning.
 Knowledge of AI models (LSTM, CNN, RNN, etc.) Deep knowledge around tuning an optimizing Deep learning models.
 Up to speed with the latest developments in the field Curiosity and interest for continuously learning and deeply understanding of new ML techniques Strong technical background in Computer Science, Mathematics, Statistical Modelling and Machine Learning algorithms Experience with ML programming environment (Python, Pandas, Scikit-Learn, Keras, Tensorflow, Pytorch, Horovod). Very good technical knowledge sharing skills, and ability to explain complex ML concepts both to business and technical audiences
1049033"
213,Data Engineer,Varidesk LLC,,"Coppell, TX 75019","SUMMARY
As a Data & Insights Engineer, you will be responsible for integrating, modeling, and designing analytic applications to create actionable insights for the organization. You will analyze, design, build, and support the data warehouse (Microsoft SQL Server/Azure), analytical models in SSAS, and reporting in PowerBI. You will also participate in the evaluation and selection of future-state technologies. You will also interact with a variety of business professionals (including C-Levels), and work on critical projects within all aspects of the organization.
DUTIES AND RESPONSIBILITES
Create and maintain optimal data pipeline architecture using Microsoft SQL Server, SQL Data Warehouse, and Azure Data Lake/Blob Storage
Develop ETL processes using SQL, SSIS, Azure Data Factory with consideration to fault-tolerance, error logging, auditing and data quality
Build/automate reports/dashboards (in Power BI and Microsoft Analysis Services) to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Implement data cleanup procedures, transformations, scripts, stored procedures, and execution of test plans for landing data successfully into the appropriate destinations
Create tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader
SKILLS AND ATTRIBUTES
Must be an experienced thought leader and expert in Business Insights, Warehouses, Data Enrichment, and Analytics
Understanding of scrum and agile development methodologies
Ability to be a quick problem solver
Outstanding client management skills and the ability to work with customers to execute on an implementation plans
Excellent communication on both technical and non-technical contexts
Ability to work under deadlines in a fast-paced environment while prioritizing competing demands
Experience performing root cause analysis on internal and external data and processes, to answer specific business questions and identify opportunities for improvement.
Strong Data Modeling skills to include data quality, source systems analysis, business rules validation, source to target, mapping design, performance tuning and high volume data loads
QUALIFICATIONS
5+ years of progressive experience in relevant Business Data & Insights Administration & Support positions
Advanced working knowledge of modern ETL/ELT, BI, and Relational and Analytic data stores (ex. SSIS, T-SQL, SQL Server, SQL Data Warehouse, Qlik, Tableau, Snowflake, HANA, Vertica, and Hadoop)
Experience building and optimizing big data pipelines, architectures and data sets.
Excellent communication skills
Experience working in an agile team environment
Company Description
We made the first VARIDESK to help a friend with his back pain. The solution was so disruptive and successful we got a ton of requests to make more. So, we did, and we haven’t stopped since.
Our line of sit-stand desks and active office products continue to grow, and our customers now span the globe. We’re passionate about elevating people through happier, healthier, and more productive workspaces.
WHY VARIDESK?
We’re award winners:
Best Place to Work Top 10 – Dallas Business Journal
National Entrepreneurs of the Year – EY
Fastest Growing Company in DFW – SMU
We take health seriously:
On-site gym with Peloton Cycle and daily group classes
Comprehensive health plans
Healthy foods and snacks
Wellness program and insurance premium discounts
Nicotine-free facility
We value our employees:
All employees use VARIDESK products
Enhanced paternity / maternity programs
Three weeks of personal time off a year
Offsite events and happy hours
Short-term and long-term disability premiums covered at 100% by VARIDESK

VARIDESK provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability"
214,Lead Data Engineer,Manulife,,"Boston, MA 02116","Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference, within a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.
Job Description

If you are ready to unleash your potential it’s time to start your career with Manulife/John Hancock.

About Manulife

Manulife Financial Corporation is a leading international financial services group that helps people achieve their dreams and aspirations by putting customers' needs first and providing the right advice and solutions. We operate as John Hancock in the United States and Manulife elsewhere. We provide financial advice, insurance, as well as wealth and asset management solutions for individuals, groups and institutions. At the end of 2016, we had approximately 35,000 employees, 70,000 agents, and thousands of distribution partners, serving more than 22 million customers. At the end of 2016, we had $977 billion (US$728 billion) in assets under management and administration, and in the previous 12 months we made almost $26 billion in payments to our customers.


Our principal operations are in Asia, Canada and the United States where we have served customers for more than 100 years. With our global headquarters in Toronto, Canada, we trade as 'MFC' on the Toronto, New York, and the Philippine stock exchanges and under '945' in Hong Kong.

Manulife is committed to supporting a culture of diversity and accessibility across the organization. It is our priority to remove barriers to provide equal access to employment. A Human Resources representative will consult with applicants contacted to participate at any stage of the recruitment process who request an accommodation. Information received regarding the accommodation needs of applicants will be addressed confidentially."
215,"Data Engineer, PerfectMile","Amazon.com-Amazon.com Services, Inc.",,"Seattle, WA","Experience performing quantitative analysis, preferably for an Internet with large, complex data sources.Hands-on experience on Big data technologies and frameworks. Hive, Spark, Hadoop, SQL on Big Data, RedshiftExperience in near real time analyticsExperience with scripting languages i.e. Python, Perl, etc.Experience with ETL, Data Modeling, and working with large-scale datasets. Extremely proficient in writing performant SQL working with large data volumesAbility to manage competing priorities simultaneously and drive projects to completion.Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering).

Amazon Logistics is looking for a smart and ambitious individual to develop and support Amazon Logistics business intelligence and operations reporting and management systems. Amazon Logistics is a large, rapidly growing package delivery operation, that handles packages for Amazon and our customers with worldwide operations.

Do you want to be in the forefront of engineering big data solutions that takes Transportation models to the next generation? Do you have a solid analytical thinking, metrics driven decision making and want to solve problems with solutions that will meet the growing worldwide need? We are looking for top notch Data Engineers to be part of our world class Transportation Business Intelligence team.

The ideal candidate relishes working with large volumes of data, enjoys the challenge of highly complex technical contexts, and, above all else, is passionate about data and analytics. He/she is an expert with data modeling, ETL design and business intelligence tools and passionately partners with the business to identify strategic opportunities where improvements in data infrastructure creates out-sized business impact. He/she is a self-starter, comfortable with ambiguity, able to think big (while paying careful attention to detail), and enjoys working in a fast-paced and global team. It's a big ask, and we're excited to talk to those up to the challenge!

Experience performing quantitative analysis using complex data sources.Hands-on experience on Big data technologies and frameworks. Hive, Spark, Hadoop, SQL on Big Data, RedshiftExperience in near real time analyticsExperience with scripting languages i.e. Python, Perl, etc.Experience with ETL, Data Modeling, and working with large-scale datasets. Extremely proficient in writing performant SQL working with large data volumesAbility to manage competing priorities simultaneously and drive projects to completion.Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering).Experience with large scale data processing, data structure optimization and scalability of algorithms a plus

· Experience with large scale data processing, data structure optimization and scalability of algorithms a plus"
216,Big Data Engineer : 19-02898,AKRAYA INC.,,"San Jose, CA 95110","Primary Skills: Hadoop, Big Data, SQL, Hive, Data Warehouse, ETL, Oozie, Python
Duration: 3-6 Months (possible extension)
Tax Term: W2 Only

Responsibilities :
Querying and manipulating large data sets for analytical purposes using SQL-like languages (Hive is strongly preferred).
Experience with Hadoop/big data environments to synthesize and analyze data.
Professional experience in the data warehouse space.
Good attention to detail and ability to QA multiple data source.
Experience working on building scalable ETL pipelines, data warehousing and schema modeling.
Experience working with Oozie Workflow.
Experience with script language such as Python.
2 - 3 years of relevant experience.
Degree in Computer Science/Computer Engineering or a related field.
 To follow up with any questions, please contact Ajitabh at 408-907-2956

Akraya is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US. We offer comprehensive benefits including Health Insurance (medical, dental, and vision), Cafeteria Plan (HSA, FSA, and dependent care), 401(k) (enrollment subject to eligibility), and Sick Pay (varies based on city and state laws).

If this position is not quite what you're looking for, visit akraya.com and submit a copy of your resume. We will get to work finding you a job that is a better fit at one of our many amazing clients.

Akraya is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. Akraya is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation."
217,"Data Engineer, ETL",Shipt,,"San Francisco, CA","Job Description
Shipt is a membership-based marketplace that helps people get the things they need. Our friendly shoppers handpick fresh groceries and household essentials and deliver them to members in as soon as one hour.

Shipt is a data driven company where the data is both the lifestream and secret sauce to our success. We're looking to grow the Data Engineering team to help expand and grow our large-scale data processing platform, pipelines, and tools. If you are self-directed, enjoy autonomy in your work, and an excellent participant in a team, come join Shipt.
What does Data Engineering Do at Shipt?
Data Engineering at Shipt primarily focuses on retailer catalog and general product data for e-commerce purposes. The team focuses on developing pipeline frameworks; specific processes to ingest, clean, and normalize a variety of data sources; and tools to improve data quality and fidelity.
As catalog data is fundamental to the grocery delivery experience, our team plays a large part in the Shipt marketplace on a daily basis and helps facilitate the company's growth plans. Our roles are ever evolving as we grow into new verticals, expand product features, and build relationships with other companies.

Your Responsibilities
Develop and maintain pipelines responsible for ingesting large amounts of data from a wide range sources
Develop and scale our data processing platform and services so that we can quickly and reliably process large amounts of data
Help evolve our data model for new retailers and new retail verticals
Work with other teams in the organization (e.g., Engineering, Catalog) to build tools and solutions that support and help manage data within the Shipt ecosystem
Collaborate with other teams across the organization (e.g., Partner Success, Data Science) to enable the better use and understanding of data
Keep the big picture in mind so that our architectural patterns can better consume and validate source data
Build and experiment with different tools and tech, and share your learnings with the broader organization
You may be a fit for this role if you
Like to be challenged by a variety of projects, each with different goals, teams, and technology
Want to build interesting solutions to unique problems
Enjoy collaborating with others, both as part of the same team and across departments
Some Projects You Might Work On
Architect a series of data pipelines to retrieve new product content from external APIs and coordinate mass updates across our product libraries
Research and evaluate tradeoffs between new data processing technologies to incorporate into our data stack
Develop and implement a shopper pick list sorted by past deliveries from highly efficient shoppers & orders to reduce grocery shop time
Requirements
2+ years of direct experience in a full-time data engineering role
Proficiency in Python is required (this is our primary language)
Proficiency in SQL is required (we use PostgreSQL and Redshift)
A keen attention to detail
Experience with queues and/or streams (we primarily use AWS SNS + SQS)
Experience with key-value stores (we primarily use Redis and DynamoDB)
Experience with a large-scale framework (e.g., Spark) is a plus
Experience with any/all of Go, Scala, Java, or Ruby is a major plus
A Bachelor's Degree in a technical field or equivalent work experience
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
218,Data Engineer,Apple,,"Santa Clara Valley, CA 95014","Summary
Posted: Jul 22, 2019
Weekly Hours: 40
Role Number: 200081452
At Apple, excellent ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis? If so, Apple's Global Business Intelligence (GBI) team is seeking an expert Data Engineer to build high quality, scalable and resilient distributed systems that power apple's analytics platform and data pipelines.
Apple's Enterprise Data warehouse system cater to a wide variety of real-time, near real-time and batch analytical solutions. These solutions are integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet services enabling business drivers to make critical decisions. We use a diverse technology stack such as Teradata, HANA, Vertica, Hadoop, Kafka, Spark, and Cassandra and beyond. Designing, Developing and scaling these big data technologies are a core part of our daily job. The team member will be able think outside of the box and should have passion for building analytics solutions to enable business in making time sensitive and critical decisions.
Key Qualifications
We would like for you to have In-depth understanding of data structures and algorithms
We are looking for experience in designing and building dimensional data models to improve accessibility, efficiency, and quality of data
Database development experience with Relational or MPP/distributed systems such as Oracle/Teradata/Vertica/Hadoop
We are seeking programming experience in building high quality software in Java, Python or Scala preferred
Experience in designing and developing ETL data pipelines. Should be proficient in writing Advanced SQLs, Expertise in performance tuning of SQLs
You will demonstrate excellent understanding of development processes and agile methodologies
Strong analytical and interpersonal skills
Self-driven, highly motivated and ability to learn quick
Experience with or advance courses on data science and machine learning is ideal
Work/project experience with big data and advanced programming languages is a plus
Experience developing Big Data/Hadoop applications using java, Spark, Hive, Oozie, Kafka, and Map Reduce is a huge plus
Description
You will build and design data structures on MPP platform like Teradata, Hadoop to provide efficient reporting and analytics capability
Design and build highly scalable data pipelines using new generation tools and technologies like Spark, Kafka to induct data from various systems
Translate complex business requirements into scalable technical solutions meeting data warehousing design standards. Strong understanding of analytics needs and proactive-ness to build generic solutions to improve the efficiency Build dashboards using Self-Service tools like Tableau and perform data analysis to support business
Collaborate with multiple cross functional teams and work on solutions which has larger impact on Apple business
We seek a self starter, visionary person with strong leadership capabilities
Ability to communicate effectively, both written and verbal, with technical and non-technical multi-functional teams
You will interact with many other group’s internal team to lead and deliver elite products in an exciting rapidly changing environment
Dynamic, smart people and inspiring, innovative technologies are the norm here. Will you join us in crafting solutions that do not yet exist?
Education & Experience
Bachelors Degree"
219,Data Engineer – Biologics Drug Discovery,Amgen,,"Thousand Oaks, CA 91360","Amgen is dedicated to discovering transformative medicines that address the leading causes of death and disability. As part of a multi-year initiative to unlock the potential of data generated from Amgen’s past and current research projects, we are currently seeking a computational data engineer to join our multidisciplinary research team of computational and experimental scientists. In this role, you will be working with bench scientists and data scientists to develop tools and processes to capture and analyze data and to derive actionable insights from discovery research. The key responsibilities will include data preparation, analysis, visualization, and building and maintaining scientific tools and infrastructures for drug discovery research.
Key responsibilities will include:
Using programming and scripting to parse/prepare data
Building and maintaining scientific tools and infrastructure
Applying machine learning to uncover novel trends in experimental data
Working effectively with project teams to develop informatics solutions to satisfy business needs
Communicating research results internally and externally
#LI-POST
Basic Qualifications:
Master’s degree and 2 years of data analysis experience
OR
Bachelor’s degree and 4 years of data analysis experience
Preferred Qualifications:
BS or MS in computational sciences (computational biology, bioinformatics, or related fields), computer science, applied math/statistics
Experience in scientific programming
Experience in data analysis (especially as pertains to protein sequence and structure, and/or next-generation DNA sequence analysis)
Excellent problem-solving skills
Knowledge of biochemistry, bioinformatics, protein structures, molecular biology, and NGS data analysis
Ability to execute and automate standard statistical analyses using tools/languages including UNIX, Python, R, SQL, etc.
Knowledge of cloud computing, AWS, Docker, etc.
Experience in scientific software administration and tool development
Strong communication skills
Enthusiasm for working in an interdisciplinary team environment
Amgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

Join Us

If you're seeking a career where you can truly make a difference in the lives of others, a career where you can work at the absolute forefront of biotechnology with the top minds in the field, you'll find it at Amgen.

Amgen, a biotechnology pioneer, discovers, develops and delivers innovative human therapeutics. Our medicines have helped millions of patients in the fight against cancer, kidney disease, rheumatoid arthritis and other serious illnesses.

As an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other but compete intensely to win. Together, we live the Amgen values as we continue advancing science to serve patients.

Amgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status."
220,Data Engineer,Other,,"North Palm Beach, FL 33408","Founded in 2007, Initiate Government Solutions (IGS) a Woman Owned Small Business, specializes in healthcare information solutions with an emphasis on technology spectrum. IGS partners with government and commercial clients to tackle the most challenging healthcare information technology issues including large scale implementations, business informatics, analytics, and electronic health record support.

IGS is currently recruiting for a Data Engineer to support our work with the Office of the Secretary (OS), Office of the Information Officer (OCIO), Office of the Chief Product Officer (OCPO), all under the Department Health and Human Services (HHS).

The HHS OCIO and OCPO are responsible for providing shared, enterprise IT services across HHS. OCPO supports all software and database development and enhancement/maintenance activities for HHS and OS. OCPO has an array of legacy, HHS-wide and new IT applications in various HHS business areas. OCPO utilizes HHS Enterprise Performance Life Cycle (EPLC) paradigm for all projects. The EPLC process incorporates all the necessary steps to ensure compliance with web coding standards and Section 508 of the Rehabilitation Act policies and guidelines; and the Federal Information Security Management Act (FISMA) and the National Institute of Standards and Technology (NIST), HHS security requirements. All OCPO developed applications must undergo Security Assessment and Authorization prior to being released into production.

The objective of this project is to obtain support for all the IT tasks and subtasks required: Business Analysis Services, Requirements Development and Management Services, Modeling Services, Data Integration Services, and Metadata Management Services.

Assignment of Work and Travel:
All work will be performed at the federal offices, located in Rockville, MD. Telecommuting is permissible with authorization from the Federal PM, but otherwise, contractors must able to commute daily. Contractor will access HHS systems and therein use approved HHS provided communication systems. Contractors may be required to travel 5-10% in support of this contract. When required, travel will be re-reimbursed at current GSA per diem rates.

Responsibilities and Duties (Included but not limited to):
Ability to prepare a variety of technical documentation for all types of business situations
Focus on the design, implementation, and operation of data management systems to meet OCPO’s business needs
Design how the data will be stored, consumed, integrated, and managed by different data entities and digital systems
Work with data consumers to determine, create, and populate optimal data architectures, structures, and systems
Plan, design, and optimize for data throughput and query performance issues by constantly updating expertise in areas such as platform, network and storage technologies, bandwidth management, data bus implications, and design
Write complex and efficient queries to transform data
Work with data scientists to refine machine learning and data processing algorithms
Minimum Qualifications:
Ability to obtain a Public Trust Security Clearance
BA/BS in related field or equivalent with at least 4 or more years of experience working with large scale IT projects, preferably at the Federal level
Expertise in data modeling, process modeling, system architecture modeling, data exchange design and implementation, data integration design and implementation, metadata design and management
Solid experience with documentation including creating user training materials, application documentation, and marketing materials
Proficiency with Microsoft Office Tools
Impeccable attention to detail, strong organizational skills, and ability to multi-task under pressure
Excellent written and communication skills
Capacity and eagerness to learn quickly on the job
Preferred Qualifications and Core Competencies:
Current Public Trust
At least 4 years or more of experience working with large scale IT projects, preferably at the Federal level
Experience working in Health IT
Microsoft Certified Solutions Expert: Data Management and Analytics
Familiarity with Salesforce
Familiarity with Section 508 compliance standards
Benefits:
Initiate Government Solutions offers competitive compensation and a strong benefits package including comprehensive medical, dental and vision care, matching 401K, paid time off, flexible spending accounts, life insurance, disability coverage, and other benefits that help provide financial protection for you and your family.

Initiate Government Solutions participates in the Electronic Employment Verification Program.

Initiate Government Solutions is an equal opportunity employer. Our company policy is to provide equal opportunity in all areas of employment practice without regard to race, color, religion, sex, sexual orientation, national origin, age, marital status, veteran status, citizenship or disability."
221,Data Engineer,Reddit,,"San Francisco, CA","""The front page of the internet,"" Reddit brings over 330 million people together each month through their common interests, inviting them to share, vote, comment, and create across thousands of communities. Come for the cats, stay for the empathy.

Reddit is poised to rapidly innovate and grow like no other time in its history. This is a unique opportunity to leave your mark on one of the most influential and trafficked corners of the internet.

As a data engineer, you'll build the exciting systems, services, and infrastructure tools needed to fuel the next wave of Reddit's growth. You'll bring your experience with complex distributed systems, passion for performance and optimization, and ability to write highly scalable / fault tolerant code to a team of top engineers.

With our small team, you work will directly impact hundreds of millions of users around the world. Join us and help build the future of Reddit!

Responsibilities:

Refining our data infrastructure technologies to support real-time analysis of millions of users.
Build analytics tools utilizing the data pipeline to provide actionable insights for our product and data science teams.
Consistently evolve data model & data schema based on business and engineering requirements.
Own the core company data pipeline and scale our data processing flow.

Qualifications:

2+ years of experience building clean, maintainable, and well-tested code.
2+ years experience with large scale distributed real-time systems with tools such as Kafka, Flink, or Spark.
Expertise with Python and/or Scala.
Bonus points for experience with (or desire to learn) Kubernetes.
ETL design (both implementation and maintenance).
Excellent communication skills to collaborate with stakeholders in engineering, data science, and product.

"
222,Data Engineer,Qbase,,"Beavercreek, OH 45431","The Data Engineer will be part developer, part data integration engineer and delivery expert, and will be integral to the automation and maintenance of our Finch for Text knowledgebase. The ideal candidate should have some software development background married with strong data fusion skills to support engineers, customers, and senior management. They will be working directly with the Finch Computing development team, working with our clients and other engineers and developers gathering requirements, architecting and communicating knowledgebase improvements.
Responsibilities:
Analyze data describing entities (people, organizations, etc.) received from a variety of external data sources to determine and document the quality of the data, how well populated the data fields are and the uniqueness of the data within the data fields.
Identify the data fields used by different data sources to uniquely identify the entity
Identify common data that can be used to compare entities from different data sources to determine if they are the same entity
Develop software to transform and normalize data from a variety of data sources so that related data can be easily compared and merged.
Develop algorithms to match entities from multiple different data sources based on how closely the data describing those entities align.
Analyze matching results to identify false positives (entities that matched that shouldn’t have) and false negatives (entities that should have matched but didn’t) and to use that information to improve the matching algorithms
Document matching results and work with other data engineers to integrate data from the matched entities into a common knowledge base
Work independently with minimal supervision

Knowledge and Skills:
Strong knowledge of MongoDB.
Experience with Python and Java programming languages.
Experience working with JSON-data for integration and data fusion purposes.
Experience performing hands-on data integration, data linking, fuzzy matching, and data transformations (ETL).
Guru-level troubleshooting experience and a passion for providing excellent service to all internal and external customers
Experience working in an agile environment

Education and work experience:
Bachelors Degree in related field
5+ years' experience in data engineering with fusion focus
Location: This position will be based out of our Beavercreek, Ohio office."
223,Sr Talend Engineer ( Data Engineer),Third Republic,,"New York, NY 10017","Position: Sr Talend Engineer ( Data Engineer)
Location: New York, United States
Remuneration: $ 100.00 per hour
Who is hiring?
We are currently working with a leading Data Consulting client in the US that specialize in all things data and analytics including big data, modern data architecture, cloud migration, enterprise data management, business intelligence, data visualization, advanced analytics, and machine learning.

They seek to hire a Sr. Talend Engineer to work on Data Pipeline Development, Customer Identity Resolution Enhancement and migrate from Talend to Spark for 50% of their workload.

What will you be doing?
Talend Focused Engineering

Build pipelines in Talend using Spark
Perform process analysis, source system analysis, and data profiling
Look for opportunities to clean, conform, and consolidate data
Produce requirements and development artifacts
Perform testing of Talend workflows so that jobs will correctly identify data exceptions
Act as a mentor and resource

Develop data pipelines:
Design and implementation of data ingestion pipelines
Build reliable workflows for gathering & ingesting data from customer files
Ensure data pipelines validate, extract, and normalize data upon ingestion
Assist developing a strategy for integration and ingestion of data
Write quality, maintainable code with extensive test coverage
Ensure appropriate notifications and error handling/auditing controls are in place.

2. Customer Identity Resolution Enhancement:
Design and implement an ID Resolution process in Talend
Use Talend/Spark pipelines to do matching and create parquet files
Work with the client to determine fuzzy matching requirements
Implement fuzzy matching algorithms per the requirements
Test and rework as needed

3. Identify opportunities for improvement:
Make recommendations and demonstrate software engineering best practices
Review user data access patterns and provide feedback to create more dynamic
Collaborate with cross-functional teams and stakeholders to understand the data needs and standardize the architecture

Why you shouldn’t miss this opportunity?
As a Data Engineer at this client, you’ll work in small teams to deliver innovative solutions using core cloud data warehouse tools and Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you’ll be working with some of the most forward-thinking organizations in data and analytics.
Data Science(Data Engineer), Python, Spark, Talend, Talend Big Data Integration, Talend Data Integration"
224,Data Engineer,Turo,,"San Francisco, CA","Data is our fuel at Turo. It is ever-more abundant and valuable, but it's a raw material. Harnessed by data scientists and machine learning engineers, it propels Turo on its mission to put the world's 1.5+ billion cars to better use, delighting our customers with matching the right car for their next adventure from an exceptionally diverse selection, and at the same time helping our marketplace remain safe.

About You

At Turo, you will have the opportunity to use the latest technologies to build robust scalable solutions for collecting, analyzing large data sets, creating and maintaining data pipelines, data structures and reports. In this role, you'll partner closely with software engineers, data analysts, and data scientists to power analytical data products, experimentation, and machine learning models.

Responsibilities


On a daily basis, you will work with members of the team to update our data engineering roadmap and execute upon those initiatives
Build new technology stack for highly scalable and available data pipelines used by Turo Product, Engineering, Data Scientists, Marketing, Customer Operations, and Finance teams
Design canonical data models for various business domains. Formulate a vision to connect all data in the Turo ecosystem
Develop, deploy and maintain workflow management tools such as Airflow, Jenkins etc in cloud environments.
Develop, deploy and maintain streaming solution such as Apache Spark streaming, Kafka and Apache Flink in cloud environments.
Using cloud technology such as AWS, Kubernetes, Docker, Redshift, EMR
Data security automation
Data microservices development

Requirements


Past experience building ETL processes
Strong programmer who views their code as a craft
Experience with a workflow manager — Airflow, Luigi, Jenkins, etc.
Experience working with data tools in the public cloud (AWS, GCP, Azure)
Able to understand technical details and communicate with other engineers, as well as communicating with less technical members from other teams.
Enjoys mentoring & teaching other engineers
3+ years of relevant experience

Benefits


Competitive salary and equity for all full-time employees
Employer paid medical, dental, and vision insurance
Generous paid time off, paid holidays, paid volunteer time off, and paid parental leave
Weekly catered lunch with a fully-stocked kitchen
Company-sponsored happy hours and team events
Turo host matching and vehicle reimbursement program
Turo travel credit every month

About Turo

Turo is the world's largest peer-to-peer car sharing marketplace where you can book any car you want, wherever you want it, from a vibrant community of trusted hosts across the US, Canada, the UK, and Germany. Guests choose from a totally unique selection of nearby cars, while hosts earn extra money to offset the costs of car ownership. A pioneer of the sharing economy and the travel industry, Turo is a safe, supportive community where the car you book is part of a story, not a fleet. Discover Turo at https://turo.com ( https://turo.com/ ), the App Store, and Google Play, and check out our blog, Field Notes ( https://blog.turo.com/ ).

Turo has raised $450M to date from top-tier investors, including IAC, Daimler AG, Kleiner Perkins, GV, Canaan Partners, August Capital, and Shasta Ventures.

Turo cultivates a tight-knit team of smart, critical thinkers who care about their work and their colleagues. Our recruiting team is always on the lookout for supportive, down-to-earth, pioneering, and efficient candidates to grow our team's talent and enrich our culture.

Read more ( https://medium.com/@andre_haddad/connecting-the-dots-to-a-compelling-not-cultish-company-culture-35dc871cba08 ) about the Turo culture according to Turo CEO, Andre Haddad.

We're an equal opportunity employer and value diversity at our company. We don't discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status. When in doubt, please apply!"
225,#7 – Data Engineer,Mass Genics,,"Plano, TX","Data Engineer
6 Month+ Contract
Job Description:
We are looking for a Data Engineer to communicate to the cloud and powers our engineering and data science teams to build efficient streaming solutions, smart analytic to help make these data pipelines more efficient and contribute to the design and development of data services for the customers of this platform.
What you will do:
Operate in a highly-iterative Agile development environment
Data Engineer efficient, adaptable and scalable data pipelines in cloud to process unstructured big data
Think independently to solve difficult data problems
Work closely with product owners and other team members to deliver next generation connected car data services
Data Engineer will build data expertise and own data quality for ingestion pipelines
Make information available to large scale, next generation, predictive analytics applications
Design, build and launch new data extraction, transformation and loading processes in production
Develop and recommend innovative solutions and approaches to solving business and technical problems
Standardize terminology and transformation of data through the architectural zones
Qualifications:
Data Engineer will be skilled in Python and preferably in one or more programming languages like C++, Java, Go, etc
Experience working with SQL and NoSQL based database solutions
Public cloud technology experience in production (Azure, AWS, or Equivalent)
3+ years of collective experience in data engineering and data analysis
2+ years of experience architecting, building and administering large-scale distributed applications"
226,Data Engineer,Psyonix,,"San Diego, CA","Psyonix is a video game developer located in downtown San Diego. Best known for Rocket League, the award-winning game that combines soccer, driving, and highly competitive and addictive gameplay, we are an industry-leading game studio rooted in a crunch-free philosophy that emphasizes a healthy work-life balance.

We are looking for a qualified and passionate Data Engineer to join us on the analytics team. In this role, you will work with fellow team members on both analytics and programming projects to steward data ingestion, pipeline, storage, and other systems through the software development cycle.
Primary Duties & Responsibilities

Architect, deploy, maintain, and refactor new and existing software into multiple environmentsUnify existing disparate systems and ensure their continued compatibilityInteract with and help maintain a multitude of big and small data systemsCollaborate on analytics initiatives to explore and extract insights across data repositoriesContribute unique, personal ideas towards all aspects of game production and developmentLiaise with other departments to determine project needs and requirements

Ideal Qualifications & Requirements
4+ years of experience in object oriented programming, preferably .NET or Python4+ years of experience with writing and maintaining SQL objects, preferably MySQLExperience working with both batch and streaming data processingProven ability to write and maintain effective technical design documentationDemonstrated experience working with large and complex datasetsExperience simultaneously addressing multiple project and stakeholder needsStrong written and verbal communication skills

Additional Preferred Experience
Experience with analysis and reportingKnowledge of the game industry’s inner workingsFamiliarity with PubSub / Kubernetes / Docker / PHP / Visual Studio / LinuxWorking knowledge of Cloud Computing (AWS, Google Cloud, Azure) and associated infrastructure"
227,Data Engineer,Hive,,"San Francisco, CA 94109","About Hive

Hive is a full-stack deep learning platform helping to bring companies into the AI era. We take complex visual challenges and build custom machine learning models to solve them. For AI to work, companies need large volumes of high quality training data. We generate this data through Hive Data, our proprietary data labeling platform with over 1,000,000 globally distributed workers, generating millions of high quality pieces of data per day. We then use this training data to build machine learning models for verticals such as Media, Autonomous Driving, Security, and Retail. Today, we work with some of the largest companies in the world to redefine how they think about unstructured visual data. Together, we build solutions that incorporate AI into their businesses to completely transform industries.

We are fortunate that investors like Peter Thiel (Founders Fund), General Catalyst, 8VC, and others see Hive's potential to be groundbreaking in AI business solutions. We have over 120 rock stars globally in our San Francisco and Delhi offices. Please reach out if you are interested in joining the AI revolution!

Data Engineer Role

In order to execute our vision, we need to grow our team of best-in-class data engineers. We are looking for developers who conduct impeccable data practices and implement high quality data infrastructures. We value hard workers who are comfortable improvising solutions to big data challenges while building a system that can stand the test of time. Our ideal candidate has experience building data infrastructure from the ground up, contributes innovative ideas and ingenious implementations to the team, and is capable of planning out scalable, maintainable data pipelines.

As a data engineer, you would at first work primarily on our Hive Media product, taking real-time data from hundreds of television streams and turning them into a combination of real-time and scheduled outputs, especially our signature ads feed. Your work would improve the quality of our results while reducing computational cost and latency. Expect truly novel challenges.
Responsibilities
Writing scheduled Spark pipelines that perform sophisticated queries on the entirety of our datasets
Writing real-time pipelines that execute complex operations on incoming data
Synchronizing large amounts of data between unstructured and structured formats on various data sources
Creating testing and alerting for data pipelines
Building out our data infrastructure and managing dependencies between data pipelines
Defining and implementing metrics that provide visibility into our data quality
Requirements
You have an undergraduate and / or graduate degree in computer science or a similar technical field, with a sound understanding of statistics
You have 1-2 years of industry experience as a data engineer
You have hands-on experience doing ETL and have written data pipelines in either Spark, Hadoop, or similar technologies
You have a sound understanding of SQL
You have worked with data lakes such as S3 or HDFS
You have worked with various databases, such as Postgres, Cassandra, or Redshift before, and understand their pros and cons
You have a working knowledge of the following technologies, or are not afraid of picking them up on the fly: Mesos, Chronos, Marathon, Jenkins
You are fluent in at least one scripting language (preferably NodeJS or python) and one compiled language (such as Scala, Java, or C)
You have great communication skills and ability to work with others
You are a strong team player, with a do-whatever-it-takes attitude
What We Offer You

We are a group of ambitious individuals who are passionate about creating a revolutionary machine learning company. At Hive, you will have a significant career development opportunity and a chance to contribute to one of the fastest growing AI startups in San Francisco. The work you do here will have a noticeable and direct impact on the development of Hive.

Our benefits include competitive pay, equity, health / vision / dental insurance, catered lunch and dinner, a corporate gym membership, etc.

Thank you for your interest in Hive."
228,data engineer,Health Data Analytics Institute,,"Dedham, MA","About us
HDAI is an entrepreneurial company developing and commercializing proprietary quantitative solutions to capture, integrate and interpret healthcare data to improve decisions and outcomes. We specialize in transforming this data into actionable information for clients across a broad array of industries and applications. HDAI is located in Dedham, MA near the Commuter Rail (25 minutes from Back Bay and South Station) and I-95/128.
Job Description
The Data Engineer and Programmer will be primarily responsible for developing SAS code to support data analysis, data cleansing, and integration with other applications including SQL. The role will assist with optimizing and maintaining data, creating data tables and implementing tools and techniques to synthesize and transform complex data sets into functional structures and code. In addition to the primary responsibilities, we are looking for a programmer who is able to support other statistical applications, is able to understand statistical approaches as well being proficient in area of other languages such as C#, and is knowledgeable of the .Net framework. While this is an individual contributor role the ideal candidate should be able to coordinate with a team to solve complex business and technological problems.
Specific daily duties
Develop and optimize SAS code, data models and with statistical mindset
Translate requirements into functional code and solutions
Design, build, and maintain efficient, reusable, and reliable code
Ability to understand and work with complex data sets and relationships
Ensure the best possible performance, quality, and responsiveness of code
Identify bottlenecks and bugs, and devise solutions to mitigate and address these issues
Help maintain code quality, organization, and automatization
Compensation
Competitive based on experience
Required Expertise
5+ years of relevant work experience
Proficient in SAS programming supporting analytic business solutions
Strong knowledge of computer science and computing
Proficient in C, C++, C#, or other comparable languages
Familiar with various design and architectural patterns and fundamental understanding of design principles for building a scalable application
Working expertise with relational datasets and database management software (e.g., SQL)
Strong command of verbal and written English
Outstanding critical thinking and attention to detail
Time management skills to complete work within allocated time frames
Highly qualified recent graduates will be considered.
Preferred Education
Bachelor’s Degree (minimum) in the field of Computer Science, Information Systems, Statistics, Mathematics, Engineering or related field.
EO Statement
HDAI is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Send resume to
Katelyn Bianculli - katelyn.bianculli@HDA-Institute.com."
229,Data Engineer,Health Data Analytics Institute-[m]Platform,,"New York, NY","OVERVIEW:
The Data and Analytics organization in GroupM is responsible for managing and analyzing massive amounts of data as well as supporting the whole organization in delivering value from data. Our products are hosted on a variety of infrastructure options including but not limited to local servers, public and private clouds and enable a suit of advanced analytics-based products, specific Data collection modules and customized Data transformation packages.

You are a Data Engineer who can:
Steadily and consistently perform routine activities
Use a systematic approach to organizing work
Present instructions in a methodical, step-by-step manner
Respect established rules and processes
Use a deliberate, methodical approach to solving problems
Develop efficient approaches that improve performance and maintain quality
Develop strict, rigid routines
Closely follow a specific plan for how things will be done

3 Months
Effectively perform the ETL process, the goal being error free data sets. Use a combination of Python, Alteryx, SQL and HIVE to cut, query and QA.
6 Months
Create your own workflows in Alteryx, unsupervised, and publish an app. [eg: automation of metadata]
Tech support for proprietary software which takes the output from media mix models, and generates forecasts with multiple dimensions (budgets, sales).
Code maintenance and development work with the back-end tables in Python/Django, SQL with a goal of improving overall product performance.
12 Months
Enhance the Alteryx published app ecosystem through the pro-active identification and fix of problems.

BEST THINGS ABOUT THE JOB:
Access to infinite data. Opportunities to work creatively and tell stories with highly varied data.
High quality, large international clients.
ABOUT [m]PLATFORM:
[m]Platform is GroupM’s suite of proprietary technology and data that enables truly individualized media planning and activation. Backed by the resources and client roster of the world’s largest media agency group, [m]Platform has the product, organization and scale to transform the way the largest advertisers approach media: from the traditional broadcast model that still dominates our world to an addressable, audience-first model that elevates the power and role of media in marketing.

Take A Virtual Office Tour: https://roundme.com/tour/368213/

GroupM and all its affiliates embrace and celebrate diversity, inclusivity, and equal opportunity. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. We are a worldwide media agency network that represents global clients. The more inclusive we are, the greater work we can create together."
230,Data Engineer,Glidewell Dental,,"Irvine, CA","Glidewell Dental is an industry-leading provider of dental technology products and services to dental professionals worldwide. Headquartered in Newport Beach/Irvine, CA, the Glidewell campus is home to 3,300 employees and complete with an employee gym, cafeteria, massage therapist, acupuncturist, and of course, a dentist. Glidewell Dental employs over 5,000 employees worldwide and is privately owned.
So what is a dental company doing with technology?
Glidewell is dedicated to building the future of dental through additive manufacturing, data science, and even machine learning. We research and develop new innovative products and services for the dental industry at affordable costs. Just ask your dentist!
If you have the following qualifications (listed below), we want to hear from you! We are always looking to bring onboard the brightest and most qualified candidates to our Glidewell Family.
PURPOSE OF POSITION:
To assist in designing, creation and debugging of MS SQL ETL packages (SSIS, SP, Views, etc..) as well as exploring existing/new Amazon Web Services (AWS). The ideal candidate would be driven, self-motivated, have a “can do” attitude, and be eager to learn new technology as they will be assisting in transitioning/creating ETL’s on the AWS platform.
Essential Functions:
Contributes to creating Extract, Transform, and Load (ETL) tools using SQL Server Integration Services (SSIS), or other open-source languages such as Python, Java Script, and GO programming language to extract data from transactional or staging databases under the guidance of management.
Collaborates with Data Architect and other stakeholders to understand requirements, relation between various data sets, and frequency of updates.
Demonstrates basic understanding of data origin to ensure consistent and accurate data.
Assists in identifying and implementing any transformation or conversions necessary to provide consistency and usability of data.
Identifies and resolves inefficiencies and gaps in existing Business Intelligence solutions.
Defines metrics and calculations along with the corresponding updates to the semantic layer under the supervision of management.
Collaborates with the business community to identify, analyze and document BI needs.
Supports with the design and development of reports and dashboards for various business units.
Assists in the definition of Business Intelligence strategy within the IT Department.
Maintains up-to-date business domain knowledge and technical skills in the BI technologies and methodologies under the supervision of management.
Maintains documentation for BI processes and procedures.
Performs other related duties and projects as business needs require at direction of management.
Education and Experience:
Bachelor’s Degree in Computer Technology required.
2+ years working with SQL
General experience in Business Objectives and Data Transformation Services (DTS) /SQL Server. Integration Services (SSIS)
Experience with Redshift, Tableau, and AWS services, a plus
Benefits:
Full Medical, Dental, and Vision benefits
401k plan with employer match
Vacation, sick time
Employee Gym with showers
Employee Medical Center with on-site Physician, Massage Therapist, Acupuncturist, Physical Therapist
Employee Dentist and Hygienist
Can be selected to get reconstructive work done on teeth for free
$1000 per year on Glidewell products
Opportunities to attend industry conferences and events
On-site cafeteria
Game room
Employee appreciation days (Company BBQ, special events, etc.)
Theme park trip (friends and family invited)"
231,"Data Engineer - Riverview, FL",BlueGrace Logistics,,"Riverview, FL 33578","Job Summary
This role is responsible for developing relational database and related reporting solutions that support information systems by studying operations; designing, developing, and deploying solutions; supports and guides data modeling and data access strategies for the software development team.
Duties and responsibilities:
Designing and coding database tables to store application data
Enforce referential integrity and business constraints
Working with application developers to create optimized stored procedures
Identify and create indexes to improve database performance
Tune existing T-SQL queries/stored procedures to improve performance
Ensure that new database code meets company standards for readability, reliability, naming conventions, and performance
Qualifications:
Bachelor’s degree in recognized program for information systems or computer science or equivalent experience required.
2+ years of SQL Server database development experience.
Strong experience with SQL Server queries for all CRUD operations
Comfortable with complex joins, nested subqueries, etc.
Strong experience with stored procedure creation.
Performance tuning/query refactoring a strong plus.
Ability to provide quality customer service and teamwork while managing multiple tasks and projects under competing deadlines a must.
Excellent verbal and written communication skills.
Understanding of IT principles and an ability to communicate technical concepts effectively to a varied audience.
Working with Azure and RedGate Tools
Using Git as source control for backend code
Understanding or working knowledge of DevOps process
Working on an Agile/Scrum team
General DBA knowledge (backups, restores, AlwaysOn, Job Scheduling, Alerting, etc.)
Extraction, Transformation and Loading (ETL) (Microsoft SSIS)
Report Development (Microsoft SSRS)
Analytics / OLAP Cube Development (Microsoft SSAS and MDX)"
232,Data Engineer,BlackBerry,,"Irvine, CA","expressing:
Why you are interested in working at Blackberry Cylance
The skills, strengths and expertise you will contribute to our diverse team of extraordinary talent and humble hearts
Job Family Group Name:
Product Development
Scheduled Weekly Hours:
40"
233,Data Engineer,"Amazon.com-A2Z Development Center, Inc.",,"Santa Barbara, CA","Bachelor's degree in Computer Science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience3+ years of relevant work experience in analytics, data engineering, business intelligence or related field, and 3+ years professional experience2+ years of experience in implementing big data processing technology: Hadoop, Apache Spark, etc.Experience using SQL queries, experience in writing and optimizing SQL queries in a business environment with large-scale, complex datasetsDetailed knowledge of data warehouse technical architecture, infrastructure components, ETL and reporting/analytic tools and environmentsExperience in data visualization software (Tableau/Qlikview) or open-source project

Alexa is Amazon’s groundbreaking virtual assistant designed for voice interactions. We believe voice is the most natural interface for interacting with technology across many domains. We are looking for a Data Engineer to join our Analytics team located in beautiful Santa Barbara, CA.

As a Data Engineer, you will build data pipelines, tools, and reports that enable analysts, knowledge engineers, software engineers, product managers, and executives improve Alexa’s answering capabilities across multiple information verticals: Sports, Business, History, Science, etc. In this highly visible role, you will work across teams to gather requirements for data logging, storing, transforming, and reporting, and will build scalable solutions under fast-paced environment.

Key Responsibilities:
You love building tools and data pipelines, can create clear and effective reports and data visualizations, and can partner with stakeholders to answer key business questions. You will also have the opportunity to display your skills in the following areas:

Design, implement, and automate deployment of our distributed system for collecting and processing log events from multiple sourcesDesign data schema and operate internal data warehouses and SQL/NoSQL database systemsWrite Extract-Transform-Load (ETL) jobs and Spark/Hadoop jobs to calculate business metricsOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisionsMonitor and troubleshoot operational or data issues in the data pipelinesDrive architectural plans and implementation for future data storage, reporting, and analytic solutions
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation

Graduate degree in Computer Science, Mathematics, Statistics, Finance, related technical fieldStrong ability to effectively communicate with both business and technical teamsDemonstrated experience delivering actionable insights for a consumer businessProficiency with search technologies (Elasticsearch and the Elastic stack)Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)Experience with AWS technologies including Redshift, RDS, S3, EMR"
234,Data Engineer - AI Labs,BlackRock,,"Palo Alto, CA 94301","About this role
Data Science at BlackRock:
In February 2018, BlackRock announced the creation of a new central Data Science team in order to accelerate innovation and technology in artificial intelligence, and to have firm-wide impact using data science to solve strategic problems. The team is led by two experienced data science leaders, Dr. Sherry Marcus and Dr. Rachel Schutt.
BlackRock manages $6.2+ trillion in assets on behalf of investors worldwide. As such, there is a rich problem space for data scientists and engineers across all areas of the business including investments, sales, marketing, operations, product, UX, etc. and the potential to have large scale impact.
The kinds of problems you’d be working on:
Building a dynamic pricing and auto-bidding engine for the security lending business
Alpha generation: extracting signals from alternative data sets that provide investment opportunities to investors.
Predictive models in sales and marketing applications in order to anticipate client behavior and needs.
Natural language processing in order to extract and correlate n-grams from unstructured text including from financial reports, news, and contracts in order to drive contextual understanding in different business applications across the firm.
Graph Analysis for path generation for data lineage/provenance, ontological development, or network analytics.
Automating repeatable tasks done by humans to free them up to work on the tasks that require their human intelligence
The firm-wide policy on algorithmic accountability and ethics of data science
The team you’d be part of:
In the first year, the team has grown to 30+ data scientists and data engineers. The team works collaboratively; and is a multi-disciplinary team with the following skills and capabilities: machine learning, statistical modeling, exploratory data analysis, natural language processing, data visualization, network/graph modeling, ETL, data pipelines, data architecture, communication, project /product management and strategy. We work with data from a wide variety of sources including text, news feeds, financial reports, time series transactions, user behavior logs, imagery, and real-time data.
We will be hiring a mix of tech leads and individual contributors with deep expertise in certain areas, as well as generalists. All individuals will be expected to have solid statistical/mathematical, and/or algorithmic/computational foundation and writing code is required. Each individual will be expected to contribute and lead based on their experience and expertise.
As this is a new team, we will be in start-up mode, so you would be helping to build and shape the team and culture from the ground up. You should therefore be comfortable with ambiguity and willing to be pro-active in your contributions, and evolve as the team grows.
We are looking for candidates with unique backgrounds and diverse skill sets with fresh perspectives to accelerate and amplify our efforts to make an impact at BlackRock. Data Science Core aims to bring best of class technologies, analytics, and insights to the entirety of the firm and to our clients utilizing data from a wide variety of sources including text, news feeds, financial reports, time series transactions, logs, imagery, and real-time data.
Check this out:
BlackRock in the News
Job Description:
As Data Engineer, you will improve BlackRock’s product and services suite by creating, expanding and optimizing our data and data pipeline architecture. You will act as architecture lead on a multi-discipline, multi-region team of data scientists, engineers, and investment professionals on a corporate-wide set of client, investor, and operational problems. You will create and operationalize data pipelines to enable squads to deliver high quality data-driven product. You will be accountable for managing high-quality datasets exposed for internal and external consumption by downstream users and applications. The successful candidate will be highly motivated to be the lead architect that creates, optimizes, or redesigns data pipelines to support our next generation of products and data initiatives.
Responsibilities:
Contribute to the creation and maintenance of optimized data pipeline architectures on large and complex data sets.
Assemble large, complex data sets that meet BlackRock business requirements.
Act as lead to Identify, design, and implement internal process improvements and relay to relevant technology organization.
Work with stakeholders to assist in the data-related technical issues and support their data infrastructure needs.
Automate manual ingest processes and optimize data delivery subject to service level agreements; work with infrastructure on re-design for greater scalability.
Keep data separated and segregated according to relevant data policies.
Work with data scientists to develop data ready tools to support their job.
Assist in the development of business recommendations with effective presentation of findings at multiple levels of stakeholders using visual analytic displays of quantitative information. Communicate findings with stakeholders as necessary.
Qualifications:
3-5+ years of experience in a data engineer role with a BA or MS degree in a quantitative discipline (computer science, mathematics, statistics, data science, economics, physics, engineering or related field)
Experience with building and optimizing ‘big data’ pipelines, architectures, and data sets. Familiarity with data pipeline and workflow management tools Luigi, Airflow
Advanced working SQL knowledge and experience with relational databases.
Experience with Hadoop, Spark, and Kafka
Experience with Amazon AWS and Google Cloud Platforms
Experience with stream-processing systems: Storm, Spark-Streaming
Experience with OO or object scripting language such as Python, Scala, and Java
About BlackRock
BlackRock’s purpose is to help more and more people experience financial well-being. As a fiduciary to investors and a leading provider of financial technology, our clients turn to us for the solutions they need when planning for their most important goals. As of June 30, 2019, the firm managed approximately $6.84 trillion in assets on behalf of investors worldwide. For additional information on BlackRock, please visit www.blackrock.com/corporate | Twitter: @blackrock | Blog: www.blackrockblog.com | LinkedIn: www.linkedin.com/company/blackrock.
BlackRock is proud to be an Equal Opportunity and Affirmative Action Employer. We evaluate qualified applicants without regard to race, color, national origin, religion, sex, sexual orientation, gender identity, disability, protected veteran status, and other statuses protected by law.
We recruit, hire, train, promote, pay, and administer all personnel actions without regard to race, color, religion, sex (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), sex stereotyping (including assumptions about a person’s appearance or behavior, gender roles, gender expression, or gender identity), gender, gender identity, gender expression, national origin, age, mental or physical disability, ancestry, medical condition, marital status, military or veteran status, citizenship status, sexual orientation, genetic information, or any other status protected by applicable law. We interpret these protected statuses broadly to include both the actual status and also any perceptions and assumptions made regarding these statuses.
BlackRock will consider for employment qualified applicants with arrest or conviction records in a manner consistent with the requirements of the law, including any applicable fair chance law."
235,Associate Data Engineer,Lowe's Inc.,,"Mooresville, NC","DATA ANALYTICS & CUSTOMER INSIGHTS:
Business Intelligence: Responsible for analyzing data across a number of verticals, including marketing / market research, enterprise and Lowe's functions. Helps build a competitive advantage by providing accurate and timely information based on big data that is used to improve decision making, identify inefficient business processes, drive cost savings and identify new business opportunities. Works closely with and provides insight to Lowe's business leaders by providing predictive analysis, data visualization, contextual visualization and insights. Also responsible for analyzing customer and consumer behavior data, which will inform the business on trends across all consumer markets. Builds a competitive advantage by providing accurate and timely information based on consumer data that is used identify new business opportunities, new products, and new strategies. Includes technical marketing positions that direct marketing strategy on product positioning, market penetration and pricing.

PURPOSE OF ROLE:
The primary purpose of this role is to deliver data pipelines and analytics solutions on Hadoop and other platforms. The Associate Data Engineer is responsible for building, maintaining, testing and evaluation solutions in close partnership with solution architects and other analytic partners. This includes translating requirements into new big data solutions, maintenance and execution of existing processes, as well continuous improvement.

RESPONSIBILITY STATEMENTS
Builds technical solutions that turn business requirements into operational processes and inject analytical insights into the business
Aligns technical solutions to corporate governance requirements to ensure data security and maintain data quality standards
Remains on the cutting edge of industry trends to ensure that the Lowe's COE is aligned with industry best practices
Builds automation and self-service consumption tools to operationalize data ingestion and analytic models
Aligns technical solutions to corporate standards to ensure that security and privacy requirements are met
Communicates clearly and concisely to key leadership and stakeholders to ensure alignment on project status and deliverables
Partners with Big Data architects and other key stakeholders to ensure that project deliverables align with cost and timing standards

QUALIFICATIONS
3 years of progressive, post-bachelor’s experience in Data Engineering/Management, or Analytics.
3 years of experience in Big Data Solutions using technologies including one or more of the followings: Hadoop, Hive, HBase, MapReduce, Spark, Sqoop, Oozie, Java.
1 year of experience with REST API development
1 Years of experience in Spark/Scala.
6-12 months of experience of Cloud platforms such as Azure or GCP
1 years of experience in troubleshooting application issues & analyzing production issues.

REQUIRED EDUCATION/EXPERIENCE
Bachelor’s Degree in Computer Science, Engineering or related field
2-3 years’ Experience in Data Management

PREFERRED EDUCATION/EXPERIENCE
Master’s Degree in Computer Science, Engineering or related field"
236,Data Engineer II,"RetailMeNot, Inc.",,"Austin, TX 78701","At RetailMeNot, we believe that gaining valuable insights using our data is core to our future success. The Data team at RetailMeNot is responsible for developing core datasets and for exposing data services consumed by product, data science and business teams. Daily, we collect approximately a terabyte of analytics events and process hundreds of terabytes of data. Our team works efficiently to deliver new features for real-time and batch processing services. We use primarily AWS cloud services and Kubernetes to build and deploy services quickly, at scale and with no downtime.

This team is integral to the RetailMeNot business, so we need engineers who can deliver results while understanding the structure of a large system. We provide cross-team leadership that ensures that RetailMeNot code meets a consistently high standard while building the platform to support the future of the company. Your daily activities will involve oversight, mentoring, delivering key pieces of functionality, and collaborating with technology leadership to plan the technical roadmap for RMN.

We are constantly evolving both the software and the teams that deliver it. If you’re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, then we want you to be a part of the team.
WHO YOU ARE
You have 3+ years work experience
You are skilled using Python, Linux, Docker, Git, and Amazon Web Services (or have translatable experience with similar toolsets)
You have extensive SQL experience on a variety of RDBMS, and enjoy optimizing queries as well as designing efficient data models
You have developed scalable solutions using both SQL and NoSQL (Hadoop) databases. Working with data sets comprised of millions or billions of records is comfortable
You strive to identify simple solutions to complex problems, can identify a minimal viable product and enjoy iterative development
You are able to accurately estimate tasks, identify dependencies and dedicatedly solve problems to ensure commitments are met
You recognize that your success depends upon enabling your fellow team members to succeed; taking time to help others energizes you
You enjoy gathering requirements from non-technical coworkers and delivering solutions that meet their needs and exceed their expectations
You derive satisfaction from enabling the business to succeed and delighting coworkers, not building technology for its own sake
You have a work ethic that inspires your fellow team members to give their best
WHAT YOU'LL DO
Implement data system for both real-time and warehouse applications
Develop ETL processes that ensure data is accurate and available within SLAs
Enhance data models by developing integrations with business partners
Seek opportunities for performance improvement and implement optimizations
Create dashboards that provide insight into the health of data integrations, ETL processes and data sets
WHO WE ARE
We give our engineers a lot of responsibility and the freedom to make a huge impact
We have an open environment that encourages people to collaborate and learn from each other
We work on large scale challenges with a variety of technologies
We have a phenomenal open vacation policy
We'll provide you with food, food, and more food
We believe in giving prizes, bonuses, and recognition for doing what you enjoy

Rewards*
We offer an opportunity to be an integral part of a company that eagerly pursues disruption in its space to continue to drive innovation and lead the competition. Benefits of being an employee of RetailMeNot, Inc. include, but are not limited to the following:
Competitive base & bonus packages; salary negotiableLong Term Incentive PlanPerformance based rewards & recognition for your hard work and serviceVery competitive benefits packages, including best-in-class parental leaveOpen & flexible PTOCell phone & gym membership reimbursementsFully stocked break room & onsite catered breakfasts & lunches multiple days/week
Some rewards do not apply to contract workers or interns.

About Us
RetailMeNot, Inc. is a leading savings destination bringing people and the things they love together through savings with retailers, brands, restaurants and pharmacies. RetailMeNot makes everyday life more affordable through online and in-store coupon codes, cash back offers, discount gift cards, and the RetailMeNot Genie browser extension. Savings are also provided in consumers’ mailboxes through the RetailMeNot Everyday™ direct mail package, and at the pharmacy with RxSaver by RetailMeNot.

RetailMeNot is a wholly owned subsidiary of Harland Clarke Holdings. http://www.retailmenot.com/corp or follow @RetailMeNot on social media.

U.S. Equal Employment Opportunity/Affirmative Action Information
Individuals seeking employment at RetailMeNot, Inc. are considered without regards to race, color, creed, religion, gender, gender identity, national origin, citizenship, age, sex, marital status, ancestry, physical or mental disability, veteran status, sexual orientation, or any other protected classification. You are being given the opportunity to provide the following information in order to help us align with federal and state Equal Employment Opportunity/Affirmative Action record keeping, reporting, and other legal requirements."
237,Data Engineer,Life.Church,,"Edmond, OK 73034","The Data Engineer will design, develop, and maintain various pipelines to improve the quality and cycle time of data analytics in the organization.

Did you ever think God could use your talent at a church? We might not see how that’s possible, but God doesn’t work within our parameters. He creates opportunities we never could have imagined, but it’s up to us to embrace His plan. Your God-given skills and passion to serve Him could intersect right here, right now. When you bring your skills to Life.Church, there’s no telling where God could take your future.

Life.Church embraces the use of technology in the pursuit of this calling and is always looking for ways to improve the systems and programs that help share God’s love with the world. The Life.Church IT Team is looking for a Data Engineer who will improve the quality and reduce the cycle time of data analytics in the organization. Are you the type of person who processes life as one giant, highly complex SQL query? Continue reading to see if you might be the right fit for this role.
You are:
Comfortable being known as a data nerd by all your friends
Excited to work in a team environment and want to help those around you
Experienced in ETL for data warehousing including cloud technologies
Skilled in one or more scripting languages, such as Python, to consume data from APIs
Willing to do whatever it takes to help your team be successful
A wizard in SQL and at least one database platform, such as Google BigQuery or MSSQL
You will:
Be part of a team who cares about you and your personal growth and development
Ensure data architecture supports the requirements of the organization
Design, develop, and maintain various data analytics pipelines from their conception to production and maintenance
Automate and streamline the process to ingest third-party data into our data warehouse
Employ a variety of coding languages and tools to ingest, integrate and model data
The cherry on top:
You function as a software engineer but have a love for data analysis
You develop strategies to win spontaneous Nerf wars"
238,Data Engineer,Cognizant Technology Solutions,,"Missoula, MT 59801","This position may be filled at these alternative locations: Kansas City, MO / St. Louis, MO / Cincinnati, OH / Missoula, MT / Virtual

What
does a Data Engineer do for ATG?

The ATG Data Engineer primary role is to
strategize, implement, solve, and execute on data related solutions to support
ATG initiatives on a variety of platforms.
Common activities may include, creating data strategies, data
migration/conversions, data mapping, and data profiling. The role will interact with the latest
enterprise SaaS platforms within the monetization ecosystem. Individuals in the role will leverage their
business, analytical, and technical skills to provide expert guidance on data
strategies to our customers that are critical in solution implementations.

What are some of the
responsibilities of a Data Engineer?

Participate
in discovery sessions with project teams to understand data requirements needed
to support solution implementations.
Work collaboratively
with a project and client team in an agile environment to support our cloud
initiatives
Leverage
system and process analysis skills to produce data strategies that create a
seamless and uninterrupted experience for our clients.
Collaborate
with clients to generate data mapping documents between target and source
systems.
Leverage
industry leading tools to extract, transform and load data into source systems
in an automated fashion to support data migrations.
Perform data
profiling tasks to collect statistics, trends, impacts, and summaries that lead
to a structured and successful implementation
Provide
key insights based on data analytics to aid the client in drawing conclusions
as to the business value and impact of their data and best approaches for
migrating and transforming data to support new system architectures.
Work with
project management to provide regular status updates to internal and external
stakeholders.

What are the minimum requirements to
be a Data Engineer?

Bachelor’s
degree and 1-5 years’ experience in a data related position that includes the
use of common database tool, ETL products, and interacting with data within
relational databases.

What other skills, knowledge and qualifications are needed to be
a successful Data Engineer?

Demonstrated knowledge of ETL (Extract-Transform-Load)
concepts
In-depth experience with SQL and PL/SQL
Proficiency
in object-oriented programming (Java, C++, C#) and/or scripting (Python,
Perl, PHP)
Experience working in cloud transformation projects within
CRM, CPQ, and/or Finance systems
Ability to effectively manage time and prioritize tasks
Capability to take on tasks with little direction, and
produce high-value deliverables in a fast-paced environment
Excellent verbal and written communication skills. Ability to
work with technical and non-technical users, other departments, and
clients
Experience working in an Agile (Scrum) environment desired
Organization
skills - ability to work in a highly dynamic environment with shifting
priorities

The Advanced Technology Group (ATG) is a privately held
professional management and consulting firm founded in 2000 and headquartered
in Overland Park, Kansas. Our niche service offering centers on customer care
and billing/CRM expertise supporting our communications industry
(telecom/cable/conferencing) based clients. Additionally, ATG’s management
consulting practice focuses on working with our targeted clients from a program
delivery perspective in providing business solutions, through strategy,
process, and program/project implementation assisting our clients meet their
business critical program/project initiatives.

Employee Status : Full Time Employee
Shift : Day Job
Job Posting : Aug 14 2019

About Cognizant Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 193 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @Cognizant.
Cognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service."
239,Data Engineer,DISQO,,"Glendale, CA","DISQO is a next generation consumer insights platform. We provide the highest quality consumer data to the world's largest market research agencies, analytics companies, and brands. We operate one of the world's largest true consumer insights panels. This data helps our clients understand user behavior, build better experiences, and make better decisions. We utilize cutting-edge technology and innovative, out-of-the-box strategies to collect and analyze insights which help shape the products and services of tomorrow.

This is a great opportunity to join a fun, exciting & highly motivated team and upgrade your skills while creating real impact. We use a modern tech stack and cloud infrastructure. We are not only looking for work experience, but rather the willingness to step up to challenges and the ability to learn quickly.

We believe the best software is written and managed by small teams that know how to make the impossible possible. We use agile software development techniques and modern tools to focus our efforts on solving our business goals. We use OKRs to track everything we do. We deliver early and often. We obsess over our code, architecture and infrastructure. And we believe that these practices lead to higher quality products.

Responsibilities:

Leverage your software development and data engineering skills to impact our business by taking ownership of key projects requiring coding and data pipelines
Collaborate with product managers, software engineers and data engineers to design, implement, and deliver successful data solutions
Define technical requirements and implementation details for data solutions
Design, build and optimize performant databases, data models, integrations and ETL pipelines in RDBMS and NoSQL environments
Maintain detailed documentation of your work and changes to support data quality and governance
Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to the customers
Be an active participant and advocate of agile/scrum practices to ensure health and process improvements for your team

Requirements:

5+ years of experience designing and delivering large scale, 24/7, mission-critical data pipelines and features using modern big data architectures
3+ years of Scala
3+ years of Python
3+ years of Spark
2+ years of experience with AWS data ecosystem (Redshift, EMR, Glue, Athena, ...)
Deep knowledge in various ETL/ELT tools and concepts, data modeling, SQL, query performance optimization
Experience with building stream processing applications using Kinesis or Kafka
Experience with workflow management tools (Airflow, Oozie, Azkaban, Luigi, etc.)
Comfortable working in Linux environment
Ability to thrive in an agile, entrepreneurial start-up environment

DISQO is an equal opportunity employer


Recruiting firms that submit resumes to DISQO without first entering into a written contract will not be entitled to any compensation on candidates referred by that firm.

"
240,Data Engineer,Tredence,,"Sunnyvale, CA","As a Data Engineer with Tredence, you will demonstrate ability to transform business requirements to code, specific analytical reports and tools. You will also provide business insights, while leveraging internal tools and systems, databases and industry data.

THE IDEAL CANDIDATE WILL

Very Strong engineering skills. Should have an analytical approach and have good programming skills.
Provide business insights, while leveraging internal tools and systems, databases and industry data
Minimum of 5+ years’ experience. Experience in retail business will be a plus.
Excellent written and verbal communication skills for varied audiences on engineering subject matter
Ability to document requirements, data lineage, subject matter in both business and technical terminology.
Guide and learn from other team members.
Demonstrated ability to transform business requirements to code, specific analytical reports and tools
This role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering team.
Must Have

Strong analytical background
Self-starter
Must be able to reach out to others and thrive in a fast-paced environment.
Strong background in transforming big data into business insights
ELIGIBILITY CRITERIA

Technical Requirements

Knowledge/experience on Teradata Physical Design and Implementation, Teradata SQL Performance Optimization
Experience with Teradata Tools and Utilities (FastLoad, MultiLoad, BTEQ, FastExport)
Advanced SQL (preferably Teradata)
Experience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.).
Strong Hadoop scripting skills to process petabytes of data
Experience in Unix/Linux shell scripting or similar programming/scripting knowledge
Experience in ETL/ processes
Real time data ingestion (Kafka)
Nice to Have

Development experience with Java, Scala, Flume, Python
Cassandra
Automic scheduler
R/R studio, SAS experience a plus
Presto
Hbase
Tableau or similar reporting/dash boarding tool
Modeling and Data Science background
Retail industry background
Education

BS degree in specific technical fields like computer science, math, statistics preferred

SEND YOUR CV TO CAREERS@TREDENCE.COM"
241,Data Engineer,Object Partners,,"St. Louis, MO","Why consider OPI, and why do people dig working here?

Variety of consulting; new technologies, projects, and people on a regular basis.
Stability; we’ve been around since 1996 and have a diverse mix of clients and technologies to keep us busy, very busy. And we keep a bench. If you’re not on a project, you’re writing software for our internal business functions or you’re learning new technologies. It’s in our benefit to make our consultants as marketable as possible. That’s good for your career.
No politics or management; we don’t get in the way. Why sit in meetings all day when you can code and be productive?
Awesome benefits; free healthcare for your entire family (yes, free), 24 days of PTO + 10 days of sick time, quarterly profit sharing bonuses, you get paid OT, company trips (to Mexico), 3 company lake cabins/homes, various quarterly company events, new Macbook Pro’s, free beer/soda, chips, candy, and so much more.
You work with the best. Do an Object Partners search on LinkedIn and see the types of talent we hire. You truly get to work with intelligent, passionate engineers that share the same goal of building great software the right way.
Low company overhead. It all means more money back into our consultants pockets (profit sharing) or company trips and events to share in the financial success.
Qualifications
Architecture and design of highly available/scalable distributed systems.
Design and development of streaming data platforms using Kafka Streams, Spark, Flink, Storm, Beam or Cloud Dataflow.
Experience with functional/event-driven programming.
NoSQL technologies such as ElasticSearch or Cassandra.
Messaging technologies such as RabbitMQ, Kafka or Kinesis.
SQL on Hadoop technologies such as Hive, Impala or Presto.
Cloud services such as AWS, Azure, GCP or OpenStack.
JVM languages such as Java, Groovy, Kotlin or Scala.
Service frameworks such as Spring Boot, Ratpack, Vert.x, or Play.
Knowledge of data analytics, visualization and governance.
Knowledge of operating big data production solutions at scale.
Knowledge of CI/CD pipelines and DevOps culture.
Passion for software development. Someone that loves what they do, that aren’t just in it for a paycheck. Do you have dev projects going on at home at all times?
New projects, new teams, new technologies means having to adjust and learn all the time. You might get thrown on a DevOps project as well, so having the ability to jump in and get your feet wet without hesitation is important. If you don’t want to grow and learn, we’re not a fit.
Positive, can-do attitude. We can teach the technology, but we can’t teach how to approach client challenges with a positive, helpful demeanor. The best consultants aren’t the most technical (although that sure helps), it’s the consultant that will do whatever it takes to see a client be successful, no matter what they throw at you."
242,"Data Engineer, Corporate FP&A","Amazon.com-Amazon.com Services, Inc.",,"Seattle, WA","5+ years of ETL experience with intermediate to advanced skills
. 3+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets
2+ years of experience in functional/programming languages such as Python, Spark, Scala and Java
. Knowledge of distributed systems as it pertains to data storage and computing
Quick learner with a positive attitude, professional demeanor, and strong analytical and troubleshooting skills.Excellent communication (verbal and written) and interpersonal skills, and an ability to effectively communicate with both business and technical teams.Bachelor's Degree in Information Systems, Computer Science, Engineering, Math, Finance/Accounting, or a related discipline

We are looking for Data Engineers to join our application team! Within Amazon’s Corporate Financial Planning & Analysis team (FP&A), we enjoy a unique vantage point into everything happening within Amazon. As part of that, this role would be part of a team that is responsible for Company’s enterprise-wide Financial Planning & Analytics solutions.

Do you have solid data engineering skills, but are eager for an opportunity to gain skills in other aspects of building & maintaining an enterprise-scale analytics system, such as exposure to Amazon Web Services (AWS), data lake, data modeling, and master dimension management?

In addition to requiring a high level of technical expertise in ETL software, you will work closely with peer financial and business teams to continuously optimize timelines, processes, and system automation. Ideal candidates will have expertise in all phases of the data life-cycle management process and successful experience working in a fast-paced, results-oriented environment.

The data flowing through our platform directly contributes to decision-making by our CFO and all levels of finance leadership. If you’re passionate about building tools that enhance productivity, improve financial accuracy, and improve work-life harmony for a large and rapidly growing finance user base, come join us!

Key Responsibilities:
Design, development, enhancement, and support of data acquisition from various source systems.Design Data Warehouse including new subject area development and integration with existing data modelsWorks closely with key stakeholders, business representatives and technical resources to gather requirements.
. Creates/modifies data extraction pipelines into a standardized approach that can be repeatable and reusable.
Implement best practices around ETL development, data validation, and data security.Establish controls around the processes and addresses performance issues with appropriate technical solutions.Implements a robust enterprise wide repository designed to ensure analysts will be able to access the data for final presentation to end users.Provides technical leadership and mentoring to other members of the team to assist in their technical growth.
.Serves as member of on-call rotation for critical off-hour support requests.
Collaborate with 3rd party software vendors on issue resolution and product enhancements.

Familiarity with AWS solutions such as S3, Redshift, Aurora, Dynamo DB and EMR.
. Familiarity with IBM Cognos Product Suite -BI/TM1 OLAP cube modeling
. Knowledge of related BI technologies (Tableau, QlikView, MicroStrategy, AWS Quicksight).
. Metadata design and implementation
. Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.
Master’s degree in Information Systems, Computer Science, Engineering, Math, Finance/Accounting, or a related discipline.
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation"
243,Jr. Data Engineer,St Edwards University,,"Austin, TX 78704","Jr. Data Engineer

The Institutional Effectiveness and Planning Division is seeking a Jr. Data Engineer dedicated to actively providing innovative and actionable insights in support of strategic priorities, regulatory reporting, and external surveys to move St. Edward's University forward. As a key member of the Analytics and Planning team and in collaboration with the Office of Institutional Research and Assessment (OIRA), the Jr. Data Engineer is responsible for developing data solutions in support of the implementation and analysis of assigned projects.

For consideration, a Bachelor's degree required, Master's degree preferred. One to three years of practical experience with delivery of data engineering/science solutions; including predictive modeling, statistical analyses, and programming required. Prior higher education environment experience preferred. Proficient with Alteryx, Python/R, or other predictive analytics tools/languages, preferred. Ability to research and develop statistical learning models for data analysis. Experience with longitudinal data sets (e.g., National Center for Education Statistics, Bureau of Labor Statistics, Data.gov) and related data collection (e.g., IPEDS) preferred. Familiarity with Oracle and Amazon Redshift technologies and experience with data visualization products (Tableau, PowerBI, PowerPivot, or similar) preferred. Ability to use SQL programming language in a relational database environment to edit and maintain SQL queries required. Demonstrated experience with data warehouses, data lakes, and/or operational data stores required. Must possess an unencumbered driver's license, and the ability to successfully pass a criminal background check.
For detailed information, please visit our direct page and scroll to the bottom of the page to download the job description.


The University offers an excellent TOTAL REWARDS package!
Medical & Rx Coverage Blue Cross Blue Shield (HSA & FSA Available)

Dental Guardian Dental

Vision Guardian/Davis Vision Plan

Short Term Disability (STD) Insurance
Long Term Disability (LTD) Insurance
Life & Accidental Death & Dismemberment (AD&D) Insurance

Employee Assistance Program (EAP)

Annual Leave & Paid Sick Leave

Retirement Plan (TIAA) Employee 5%/Employer Match 7%

Tuition Benefits

Paid Holidays

Services & Discounts


HOW TO APPLY

Interested applicants should submit an online application at; https://stedwards.applicantpro.com. Please include resume, cover letter, and three employment references. No Calls Please. Applications will not be considered if it is missing any of these three items.

In your cover letter, please describe your lived experiences that prepare you to contribute to diversity and inclusion at St. Edward's University?


EQUAL OPPORTUNITY EMPLOYER:
St. Edward's University, as an equal opportunity/affirmative action employer, complies with all applicable federal and state laws regarding nondiscrimination and affirmative action. The University is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, national origin, age, marital status, sex, sexual orientation, gender identity, gender expression, disability, religion, or veteran status in employment, educational programs and activities, and admissions.

ABOUT ST. EDWARD'S UNIVERSITY

Founded in 1885 by the Congregation of Holy Cross, St. Edward's University is a private, Catholic liberal arts institution of more than 4,600 diverse students located in Austin, Texas. St. Edward's emphasizes critical thinking and ethical practices, as well as small classes, personalized learning and exciting internship opportunities. The community atmosphere extends to the approximately 800 faculty and staff who work together to make the university a welcoming yet challenging environment for students. An overview of St. Edward's University employee benefits is available at; https://www.stedwards.edu/human-resources/benefits-summary


OTHER ITEMS TO KNOW

Sponsorship:

We are not offering sponsorship at this time.


Background Checks:
A criminal history background check is required for finalist(s) under consideration for this position.
Reference #: J19051

Posted: 6/24/2019
Available: Immediately"
244,Data Engineer,Lovepop,,"Boston, MA","Lovepop is on a mission to create one billion magical moments and the best place in the world for hungry, creative problem solvers. We were first introduced to America on Shark Tank in December 2015 and we have been on a rocket ship ride ever since. At Lovepop, we feel there is not enough human connection in the ever-growing digital world.

We are hiring a Data Engineer to join our engineering team to maintain and expand our data analytics platform, and ensure that our database infrastructure is reliable and scalable. You’ll also have an opportunity to work on backend microservice development.
What you'll do here:
Connect new data sources (in-house and third-party) to our data analytics platform.
Keep our database and data analytics platforms healthy, monitored and up-to-date.
Work closely with product, marketing, and business stakeholders to understand what data and data sources are important for gaining new insights about our customers, and help prioritize that work with the team.
Work collaboratively with an agile, cross-functional team to implement new eCommerce and backend fulfillment features.
What you'll bring to the table:
3-4 years of experience as a data engineer or BI developer, working with modern data analytics tools and services.
Experience with management of at least one RDBMS (PostgreSQL preferred).
Experience with a modern OO programming language, such as C#, Java or Python (C# preferred).
Good collaboration skills. Experience working with an agile development team is a plus.
A passion for learning about our business, and about new technologies.
A desire to make an impact and help us create one billion magical moments - and ensure we can measure how far we’ve come on the journey."
245,Data Engineer,Cooler Screens,,"Chicago, IL 60601","Cooler Screens is backed and led by some of the most prominent Chicago & Silicon Valley leaders, advisers, and investors and has developed a digital solution to create and transform a multi-billion dollar industry and positively affect the buying experiences of consumers in the US and beyond. If you're looking to get on the ground floor of the next digital revolution and Chicago tech success story – this opportunity may be for you.
As the Data Engineer at Cooler Screens, you will design and maintain an enterprise data warehouse platform architecture that structures data for analytical processes to support BI and data integration management, which will be foundational in building out our data processing pipeline. You will be responsible for constructing how we identify, absorb, store, and query data across the Cooler Screens platform that scales to the rising growth of the company.
Who You Are:
You're excited to contribute to building a robust scalable and robust data platform and shaping an early stage startup
You're passionate about building scalable data processing systems and turning data into insights
You know how to develop solutions to transform and optimize data and have expertise in solutions to build robust data pipelines and repositories
You're comfortable working with a variety of BI tools and helping users to develop solutions to gain key data insights
You're a critical thinker and enjoy measuring, analyzing, and solving complex problems
You're experienced and comfortable in managing data from a variety of sources and formats and using best practices to ensure consistency and quality
You thrive working both independently and in a team environment
What You Need to Succeed
You have 4+ years of data engineering development experience
You have a deep understanding of the data architecture and relevant technologies
You have hands-on experience with Big Data technologies such as Spark and Hadoop
You have an advanced understanding of cloud data warehousing solutions
You have extensive experience with database modeling and design
About Cooler Screens
We are transforming retail cooler surfaces into IoT-enabled screens. Our media platform reimagines the brick-and-mortar shopping experience for consumers in the cooler and freezer aisle while delivering new marketing opportunities and smart merchandising for brands and retailers.
We have an excellent benefits package that includes medical, dental, vision, 401(k), life insurance, paid time off, and many other perks. Come join our fast-growing team at our headquarters in the heart of Chicago.
We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, native origin, sexual orientation, age, citizenship, marital status, disability, gender identity or veteran status.

Tpgl3Av2Le"
246,Data Engineer Lead,AETNA,,"Hartford, CT 06156","Description:
Manages and responsible for successful delivery of large scale data structures and Pipelines and efficient Extract/Load/Transform (ETL) workflows. Acts as the data engineering team lead for large and complex projects involving multiple resources and tasks, providing individual mentoring in support of company objectives.62884

Fundamental Components:
Designs and develops complex and large scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs. Writes complex ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing. Develop frameworks, standards & reference material for architecture and associated products. Designs data marts and data models to support Data Science and other internal customers. Behaves as mentor to junior team members to provide technical advice. Applies knowledge of Aetna systems and products to consult and advise on additional efforts across multiple domains spanning broader enterprise. Collaborates with data science team to transform data and integrate algorithms and models into highly available, production systems. Uses in-depth knowledge on Hadoop architecture, HDFS commands and experience designing & optimizing queries to build scalable, modular, and efficient data pipelines. Uses advanced programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.

Background Experience:
Strong collaboration and communication skills within and across teams.Ability to communicate technical ideas and results to non-technical clients in written and verbal form.Proven ability to create innovative solutions to highly complex technical problems.Ability to leverage multiple tools and programming languages to analyze and manipulate large data sets from disparate data sources.Ability to understand and build complex systems and solve challenging analytical problems.Advanced knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.Advanced knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment.Experience building and implementing data transformation and processing solutions.Has in-depth knowledge of large scale search applications and building high volume data pipelines.Experience with bash shell scripts, UNIX utilities & UNIX Commands.7 or more years of progressively complex related experience. Master’s degree or PhD preferred.Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.

Potential Telework Position:
No

Percent of Travel Required:
0 - 10%

EEO Statement:
Aetna is an Equal Opportunity, Affirmative Action Employer

Benefit Eligibility:
Benefit eligibility may vary by position. Click here to review the benefits associated with this position.

Candidate Privacy Information:
Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately."
247,Data Engineer,Esports One,,"Santa Monica, CA 90401","Esports One Intro
Esports One is a ground-breaking esports data and analytics company that changes the way fans watch and engage with esports. Combining historical esports information with our state of the art computer vision technology, real-time analytics, and deep learning technology allows us to produce innovative experiences for the esports industry, from broadcaster to viewer, team to the fan and beyond. On top of building our own products, we also package our data solutions and provide them to partners, enabling them to create their own real-time esports applications - powered by Esports One.

We’re looking for highly motivated, humble individuals that have a passion for the gaming industry and above all else, Esports. We want you to come in and teach us a thing or two, but also be receptive to learning and growing alongside the company. You’re a self-starter, take work seriously, but you love games and will kick back to play with your co-workers. Quality is key, but as a startup in a competitive and evolving industry, we also embrace the “move fast and break things” mantra. Don’t be afraid to experiment and take initiative, you miss 100% of the shots you don’t take.

Engineering Team Intro

The engineering team at Esports One has a few values that are core to how we go about our work.

We are a team of learners, continuous personal improvement is crucial.

We engineer solutions, we don’t throw stuff at the wall and hope they stick.

We challenge conventional thinking, but we do not reinvent the wheel either.

We understand the balance between perfection and shipping products.

We communicate candidly and honestly because we win and fail as a team.

As an Esports One Data Engineer, you’ll play a critical role in helping us architect and develop our data pipelines and analytics leveraging AWS solutions and others like Couchbase. To this end you will partner with product owners to deliver killer analytics across a variety of applications. All the while, working on a cross-disciplinary team to deliver scalable ETL and database solutions that support millions of users all across the globe.

You are:

Experienced: you have a BS degree (or equivalent professional experience) in Computer Science or related engineering field with 7+ years of hands-on experience.

Data oriented: you have 5+ years experience with ETL and the do-es and don'ts of data transformation, and you are comfortable developing data models for NoSQL, RDBMS and BigData.

You are Big on data: Hadoop, BigQuery, Apache Spark, MapR, Apache Flink, CouchBase, MongoDB are technologies you’ve used in real world scenarios at scale.

Passionate about esports: you are passionate about esports and have a keen understanding of the kind of data a coach, broadcaster or die-hard fans want to have.

A coding champion: you are proficient in a multitude of programming languages such as SQL, Golang, Scala/Elixir.

Ambitious: you are eager to learn new technologies, ElasticSearch, Redis, AWS Kinesis, Consul.io, HashiVault, Nomad, Computer Vision, Machine Learning etc. These might not be technologies you’ve used, but you are keen to learn and grow.
Adaptable: you are capable of adjusting to new challenges, and experimentation is in your blood.


You will:

Architect and deliver first-class ETL services leveraging the best of bread AWS solutions.

Create, influence and improve our overall data modelling share best practices with the team.

Learn new technologies and programming languages on the job.

Contribute to open source projects that are used by us.

Work in a distributed team, with members across the US and in Europe.

While we are growing, you may be asked to help development efforts in other areas than just the data engineering.



You’ll love:

Opportunity to join a pre-series A startup with equity to show for it

Environment built by and for gamers

Unlimited vacation policy

Health, Dental and Vision coverage

Experience the growth of a company from up close and be an instrumental part in molding who we are

Leave your mark on an industry that is still in its infancy

Opportunity to broaden your skill set beyond data science"
248,Data Engineer,Pac-12 Networks,,"San Francisco, CA","Pac-12’s Data Engineer position is an exciting opportunity to shape the future of data and analytics at the Conference of Champions. Our goal is to enable business intelligence analysis for the Pac-12 Networks and Pac-12’s member schools, in order to better understand and serve our fans, and create the best fan experiences in sports.
 As a Data Engineer on our team, you will play a key role in engineering one of the most sophisticated data platforms in college sports. You will help build and maintain our data infrastructure by supporting business intelligence at school athletic departments as well as across all business functions at the Pac-12 Networks. This position can be based in San Francisco, CA or within the Pac-12 footprint: Oregon, Washington, California, Arizona, Colorado or Utah.
Responsibilities
Create and maintain data pipeline architecture
Create system monitoring and alerts
Ensure data integrity and security
Improve and optimize ETL processes
Facilitate data loading into BI tools
Create documentation and best practices for our data platform
Communicating with stakeholders
Desired Qualifications
A love of sports!
Ability to build and maintain our data warehouse and ETL pipeline.
Excellent Python programming skills.
Expert-level experience with SQL, query performance tuning, data modeling on the following or equivalent platforms: MySQL, Postgres, Redshift, SQL Server, etc.
Extensive, hands-on experience with cloud data warehouse/data lake technologies such as BigQuery, Redshift, RDS, S3/Athena or similar.
Experience with cloud computing technologies such as AWS Lambda, ECS, Google Compute Engine, Kubernetes, etc.
System management experience with monitoring, backup, and disaster recovery, automated schema migration, automated testing, and continuous deployment.
Experience with Machine Learning
Experience with Tableau, Domo or other BI tools.
Strong verbal and written communication skills.
BA/BS degree in Computer Science or equivalent practical experience."
249,Senior Data Engineer,AETNA,,"Hartford, CT 06156","Description:
This Sr. Data Engineer role will be working with a group of Data Scientists and Data Engineers developing a Next Best Action recommendation system. They will be responsible for creating and managing data pipelines that support core system functionality and reporting. They will work within the team’s Agile processes to create a high-quality systems and processes, and they will collaborate with other teams to achieve the best possible results. 63450

Fundamental Components:
Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.
Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.
Collaborates with data science team to transform data and integrate algorithms and models into automated processes.
Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines.
Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems.
Builds data marts and data models to support Data Science and other internal customers.
Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions.
Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.

Background Experience:
7+ years writing SQL queries, understanding logic of existing queries, and developing relational databases.
5+ year with languages used for data engineering (Java, R, Python, H2O, etc.)
5+ years with Linux via CLI and Shell Scripting
3+ years of experience data engineering on Hadoop (HDFS, Hive, Pig, HBase, etc.)
#LI-DT1

Potential Telework Position:
No

Percent of Travel Required:
0 - 10%

EEO Statement:
Aetna is an Equal Opportunity, Affirmative Action Employer

Benefit Eligibility:
Benefit eligibility may vary by position. Click here to review the benefits associated with this position.

Candidate Privacy Information:
Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately."
250,Data Engineer,Pie Insurance,,"Denver, CO","Your Mission

Establish Pie as the preeminent commercial insurance among small business owners by establishing a best in class data architecture as a data engineer in this startup environment.

How You’ll Do It

As a data engineer with Pie, you will work with our data architect to Pie-oneer our data environment. This individual will be a key member that will work directly data architect to define the future state of our data architecture. This role will work in data architecture, data analytics, ETL development, and data reporting.

Success in this position will be establishing how data comes into and flows through the Pie insurance platform. This data will be used to help our organization quote customers based on data on best policy and prices for their workers compensation insurance.
The Day to Day
Work with current data architect to establish the Pie Insurance data platform as a best in class offering.Manage various priorities and not be afraid when priorities change.
Work fast, but have an attention to detail so key pieces are not missed

Whether It’s Right For You

You will be joining our team in downtown Denver, specifically LoDo within walking distance from Union Station. Everything we do is connected, but we each have different roles. That means we need you to be an analytical thinker with a strong data background. We are a start-up. All hands and minds are needed.
Our team is looking for an experienced data engineer. We expect you’ll have spent at least 3 years in the data warehouse and/or data analytics space. Of course, you’ll also need certain skills and abilities to do the work.
The Right Stuff
 3-5 years working in data as an engineer. Building data solutions for a company who uses data as a primary part of their business.
Experience in data warehouse and/or data analytics. Qualified candidates may also come from a strong database skillset involved in analytics architecture
Strong experience in writing complex SQL queries
Strong experience in ETL/ELT platforms is strongly preferred
Exposure to one major SQL RDBMS or analytics database. (Snowflake, Redshift, MySQL, Postgres, Oracle, SQL Server, etc.)
Big Data and Business Intelligence exposure would help in the success of this role.
Working at Pie
You expect to challenge others and to be challenged.
Your communication style is candid and compassionate.
You share your knowledge. We aren’t here to compete with one another; we are in it to win it, together.
You share our passion for the small business community.
Keywords
Data Engineer, Software Engineer – Data, Informatica, Redshift, Snowflake, data platform, insurance technology, insurtech, analytics, business intelligence, reporting, Tableau, Qlik, SQL queries, Shell Scripts, Database, Amazon, report visualization
Why Pie
We're building Pie for the next generation of small businesses. Our team is on a mission to make workers’ compensation less expensive, simpler, and more transparent. Easy as pie, in fact.

Founded in 2017, Pie is a fast-growing insurtech startup with home offices in Washington, DC, and Denver, CO. Pie offers an incredible career opportunity where you can use your expertise to disrupt an industry and serve the SMB community. Invest your time doing meaningful work in a fast-paced environment where your contribution will have an immediate impact.

Pie, backed by trusted global insurer Sirius Group, continues to expand nationwide in 2019 as we scale our team and operations. Want a slice? In return for your commitment to us, you’ll get a piece of Pie in the form of stock options. At Pie, we’re invested in you, and you in us."
251,Data Engineer,Saint-Gobain,,"Faribault, MN 55021","SageGlass® is the pioneer of the world’s smartest electrochromic glass and is transforming the indoor experience for people by connecting the built and natural environments. Electronically tintable SageGlass controls sunlight to optimize daylight, outdoor views and comfort while preventing glare, fading and overheating without the need for blinds or shades. SageGlass dramatically reduces energy demand and the need for HVAC by blocking up to 91 percent of solar heat. As a wholly owned subsidiary of Saint-Gobain, SageGlass is backed by more than 350 years of building science expertise that only the world leader in sustainable environments can provide.

SAGE is all about its people, its products and its company culture. The vision of the company is to deliver a durable, reliable and high-performance energy-saving electrochromic product for buildings and to provide a healthier indoor environment for their occupants. Its award winning electronically tintable glass solution is second-to-none and recognized by Green Building, Inc. as one of the top ten green building products available on the market place.

SageGlass R&D group works with customers, Saint-Gobain R&D team, and manufacturing colleagues to develop next generation products and process for Sage electrochromic glass. You will join our new data analytics team and drive the change through the creation of innovative data analytics algorithm/software and data-driven services. You will be responsible for working with customers and SageGlass R&D and engineering team to identify opportunities for data analytics.

Responsibilities Include:
Build infrastructure required for optimal Extraction, Transformation, and Loading (ETL) of data from a wide variety of data sources;
Identify, design, and implement data processing improvement: automating manual processes and optimizing data delivery, re-designing infrastructure for greater efficiency and scalability;
Design, implement and test data analytics algorithms: filtering, modeling, classification, and etc.;
Data visualization: build user interfaces per design specifications
Perform debugging, troubleshooting, modifications and unit testing of integration solutions;
Documentation and Reporting

Minimum Qualifications:
B.S. with 2+ years of experience in Computer Science, Computer Engineering, Data Engineering, Data Science, Informatics, or Engineering related fields OR M.S. with 0+ years of experience in Computer Science, Computer Engineering, Data Engineering, Data Science, Informatics, or Engineering related fields

Competencies Required:
Good communication skills in English, both written and oral;
Passionate, self-driven team player with ""can-do"" attitude;
Work in an agile environment accountable to deliver results;
Interpersonal skills with the ability to work effectively in a cross functional team

Tools & Technicques:
Strong experience with Python and SQL use for developing custom ETL solutions
Experience with advanced signal/image processing and optimization algorithm development
Experience working with revision control systems (Git, etc.)
Understanding of machine learning, deep learning and cloud-based platform

Saint-Gobain provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws. Saint-Gobain is an equal opportunity employer of individuals with disabilities and supports the hiring of veterans."
252,Data Engineer,SPR,,"Chicago, IL 60606","JOB DESCRIPTION
DATA ENGINEER

WHO IS SPR?

SPR is a digital technology consultancy that develops elegant solutions to transform the way people do business. We’re 300+ strategists, developers, designers, architects, consultants, thinkers, and doers in Chicago and Milwaukee. We work with 160 clients in 10 unique industries – everything from corporate finance and global logistics to local breweries and Chicago startups.

We think about the end users and rigorously apply the latest technologies and frameworks to address our clients’ needs. We enable companies to do more with data, engage with other people, build disruptive solutions, and operate productively. To do this, we hire smart technologists and sharp business leaders who are excellent communicators and have an interest in working on multiple projects across industries.

SPR offers a great environment for employees to learn, to build systems that make an impact, and to tackle exciting challenges. With our office’s “Maker Space”, you can explore your IoT side and develop fun projects with 3D printing and CNC machining. We operate in a fun, casual work environment and have great benefits including: competitive salary, bonuses, generous vacation time, big fitness incentives, and medical/dental/vision insurance.

By joining the SPR team, you’ll be using your brain, working hard and making an impact through your projects – and you’ll be rewarded for it.

WHAT IS THE POSITION?

As a Data Engineer at SPR, you must have experience building and operating data pipelines (both streaming and batch, utilizing both ETL and ELT architectures). You will be building data pipeline solutions by designing, adopting and applying big data strategies and architectures. You must be experienced in large-scale system implementations with a focus on complex data processing and analytics pipelines. You must demonstrate an understanding of data integration best practices, and expertise in data integration, data transformation, data modeling and data cleansing. The Data Engineer must be able to demonstrate innovative approaches to complex problems which deliver industry-leading experiences for our clients.

PROFESSIONAL QUALIFICATIONS

Experience in designing and implementing innovative data integration solutions, utilizing Python with Spark clusters.
Familiarity with architectural patterns for data-intensive solutions
Expertise in real-time streaming and migrating batch-style data processing to streaming and micro-batch solutions
Knowledge of the RDBMS core principles; set up, tune, design, as well as newer unstructured data tools
Familiarity with consulting and traditional application design
Excellent written and verbal communication skills
Display solid problem-solving abilities in the face of ambiguity
Must be a hands-on individual who is comfortable leading by example
Experience with Agile Methodology
Possess excellent interpersonal and organizational skills
Able to manage your own time and work well both independently and as part of a team

TECHNOLOGIES WE USE

Cloud (Azure, AWS, Cloud Foundry, Heroku, Mesos, DC/OS) / / RDBMS (SQL Server, PostgreSQL, Oracle, DB2) /NoSQL (Mongo, Raven, DocumentDB, Cassandra, Maria, Riak) / Python (including Databricks) / / Big Data (Cloudera & Hortonworks Hadoop distributions, including Hive, Pig, Sqoop, Spark) / Integration Tools (Apache Nifi, Cloudera Streamsets, Azure Data Factory, AWS Glue, Talend) / ELK (ElasticSearch, Logstash, Kibana) / Machine Learning (Azure ML tooling, TensorFlow, AWS Sagemaker, scikit-learn) / Data Visualization (Grafana, Kibana) / Microsoft PowerShell / AWS SDK / Fast Data (Apache Ignite / Gridgain, Apache Geode/Pivotal Gemfire)

EDUCATION & EXPERIENCE

3-5 years of professional experience
BA or BS, preferably in Computer Science, Engineering or Science/Technology-based discipline

If this sounds like the kind of challenge you would be up for every day, we would love to hear from you."
253,Data Engineer,Nestle Nespresso,,"New York, NY",".
Nestle Nespresso USA is searching for an experienced, detail oriented Data Engineer to join the Nespresso Marketing team within our Nespresso USA Headquarters in New York, NY. This position will play a key role in collaborating with reporting teams, business areas, and technology teams to enhance the advanced analytics user experience at Nespresso USA. Successful candidates will have a comprehensive knowledge of Microsoft BI suite, Azure Cloud Tools/Technologies, and familiarity with data mining techniques. This role will design and maintain technical infrastructure that meets Nespresso USA business requirements and follow system architecture best practices. They will need to ensure security of data by maintaining the integrity and performance of Nespresso data feeds and guaranteeing that data is stored securely and optimally. This position will work closely with the Manager of Marketing Analytics at Nespresso USA and counterparts on the Nestle GLOBE team.
Responsibilities:
Partner with global data teams to build and maintain a localized, compliant, Azure-based datalake for Nespresso USATranslate business needs into detailed requirements for technical teamsTake end-to-end responsibility for design, build, and maintenance of batch and real-time data pipelines from a variety of data sourcesAcquire and synthesize the data from different external sources into usable and consistent formatCreate data models to enable reporting, advanced analytics, and marketing activation based on understanding of business needsCollaborate with analysts and other key stakeholders to bring models into productionBuild project timelines and project plans to manage key stakeholders and interim deadlines for on-time deliveryDesign, develop, Quality Assurance (QA) and maintain application codeProvide thought-leadership and dependable execution on diverse projects

Benefits and Perks:
– Annual bonus
– Paid vacation time
– Transit assistance
– Seamless lunch allowance
– Tuition reimbursement program
– Free Machine and coffee allowance
– 401K savings match
– Full medical benefits

Requirements:
Bachelor’s Degree required. Studies in Computer Science, Statistics or similar field preferred.3+ years of experience in an data engineer (or similar) roleExpertise working with structured data, applying methods, technologies and techniques that address data architecture, integration and governance of data. Additional experience working with and analyzing unstructured data (such as text analytics) preferredExperience in database concepts, data modelling; data integration including design and architectureAdvanced knowledge of Azure and datalake buildsAdvanced skills and experience using SQL, Python, DAX, Java (JSON), and other query and programming languagesWorking knowledge of managing IaaS and SaaS systemsExperience with Big Data technologies (BigQuery, Hadoop, Spark, etc.)Experience with BI and data visualization tools. Power BI experience preferredExperience with SQL Server Management Studio, Azure Dev Ops, and Visual StudioKnowledge of data mining and predictive modeling. Familiarity with machine learning, and/or natural language processing preferredAble to effectively manage competing & parallel project demands. Strong project management skills required.

The Nestlé Companies are equal employment and affirmative action employers and looking for diversity in qualified candidates for employment."
254,Snowflake Developer - Data Engineer,Capgemini,,"Atlanta, GA","The Snowflake Developer will be responsible for designing and implementing Snowflake data warehouse infrastructure and pipelines.
Strong experience and comfort with relational database concepts (Databases, Schemas, Tabular/Semi-structured Data, Primary/Foreign keys, etc).At least 2 years of experience in data warehouse build projects including data loading, processing and transformationExpert-level Python and SQL (Scala and Java are a plus). At least 2 years of experience is requiredExperience and comfort with ETL/ELT concepts and frameworks and data modelling.Knowledge and hands on experience with Snowflake, familiarity with Snowflake's features and use cases is a plus.Exposure to either AWS or Azure cloud environments in a production settingHands on experience with Apache Airflow is a plus.


Candidates should be flexible / willing to work across this delivery landscape which includes and not limited to Agile Applications Development, Support and Deployment.
Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.

Qualifications

Responsible for programming and software development using various programming languages and related tools and frameworks, reviewing code written by other programmers, requirement gathering, bug fixing, testing, documenting and implementing software systems. Experienced programmers are also responsible for interpreting architecture and design, code reviews, mentoring, guiding and monitoring programmers, ensuring adherence to programming and documentation policies, software development, testing and release.
Required Skills and Experience:
Write software programs using specific programming languages/platforms such as Java or MS .NET, and related tools, platform and environment. Write, update, and maintain computer programs or software packages to handle specific jobs, such as tracking inventory, storing or retrieving data, or controlling other equipment. Consult with managerial, engineering, and technical personnel to clarify program intent, identify problems, and suggest changes. Perform or direct revision, repair, or expansion of existing programs to increase operating efficiency or adapt to new requirements. Write, analyze, review, and rewrite programs, using workflow chart and diagram, and applying knowledge of computer capabilities, subject matter, and symbolic logic. Write or contribute to instructions or manuals to guide end users. Correct errors by making appropriate changes and then rechecking the program to ensure that the desired results are produced. Conduct trial runs of programs and software applications to be sure they will produce the desired information and that the instructions are correct. Compile and write documentation of program development and subsequent revisions, inserting comments in the coded instructions so others can understand the program. Investigate whether networks, workstations, the central processing unit of the system, and/or peripheral equipment are responding to a program's instructions. Prepare detailed workflow charts and diagrams that describe input, output, and logical operation, and convert them into a series of instructions coded in a computer language. Perform systems analysis and programming tasks to maintain and control the use of computer systems software as a systems programmer. Consult with and assist computer operators or system analysts to define and resolve problems in running computer programs. Perform unit testing Assist in system and user testing Fix errors and bugs that are identified in the course of testing.
Qualifications: 3-7 years (2 years min relevant experience in the role) experience; Bachelor’s degreeShould be proficient in Software Engineering Techniques, Software Engineering Architecture, Software Engineering Lifecycle and Data Management.Should have progressing skills on Business Analysis, Business Knowledge, Software Engineering Leadership, Architecture Knowledge and Technical Solution Design.

Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.

This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Click the following link for more information on your rights as an Applicant - http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law

About Capgemini
A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion.

Visit us at www.capgemini.com. People matter, results count."
255,"Data Engineer, Newsroom",Dow Jones,,"New York, NY 10176","Job Description:
The Wall Street Journal seeks a data engineer who will be responsible for developing tools to help the newsroom in its data science work. The Journal is expanding its use of data in both editorial and audience-related projects, and this engineer will be an important force in bolstering the newsroom’s data capacities.
The Journal is seeking a full-stack data engineer who will be responsible for (1) acquiring new datasets, (2) creating and maintaining data pipelines, (3) deploying data and insights to editors in the newsroom, and (4) building prototypes of tools for editors and newsroom staffers.
This role is responsible for making and maintaining a data pipeline for all the data sets we want, have and need. This is a function tied to the newsroom’s top-level strategy, working in collaboration with the Audience group, the R&D Lab and the broader newsroom. The engineer will collaborate with data scientists and work directly with a number of highly sophisticated audience and content data sets. The engineer will also help with rapid prototyping and testing of newsroom data tools as well as help maintain ones that are successful.
We are looking for someone with deep knowledge of audience behavior around common journalism types, like breaking news and enterprise journalism, as well as experience in newsroom tools and data dashboards. The data engineer should have strong background in A/B testing as well as managing data processes that inform content optimization. This role is suited for a talented engineer with a strong understanding of newsroom workflow and a passion for helping journalists connect with their audiences.
Skills:
Experience running and supporting production of enterprise data platforms
Experience creating internal tools that combine content and audience data
Experience in building infrastructure required for optimal extraction, transformation and loading of data from various resources
Knowledge of JavaScript, Python, Bash and SQL
Build data pipelines with tools and cloud-based data services like Google’s BigQuery, AWS, Dataproc and Pub/Sub
2+ years of data engineering experience
Strong statistics skills
This position reports to The Wall Street Journal’s Head of Data Solutions.
LI-JA1-WSJ
Dow Jones , Making Careers Newsworthy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, or disability status. EEO/AA/M/F/Disabled/Vets .
Dow Jones is committed to providing reasonable accommodation for qualified individuals with disabilities, in our job application and/or interview process. If you need assistance or accommodation in completing your application, due to a disability, please reach out to us at TalentResourceTeam@dowjones.com . Please put “Reasonable Accommodation"" in the subject line.
Business Area: NEWS/WSJ
Job Category: IT Development Group
About Us
The Wall Street Journal is a global news organization that provides leading news, information, commentary and analysis. The Wall Street Journal engages readers across print, digital, mobile, social, and video. Building on its heritage as the preeminent source of global business and financial news, the Journal includes coverage of U.S. and world news, politics, arts, culture, lifestyle, sports, and health. It holds 38 Pulitzer Prizes for outstanding journalism. The Wall Street Journal is published by Dow Jones, a division of News Corp (NASDAQ: NWS, NWSA; ASX: NWS, NWSLV).
If you are a current employee at Dow Jones, do not apply here. Please go to the Career section on your Workday homepage and view ""Find Jobs - Dow Jones."" Thank you.
Req ID: 14570"
256,Data Engineer - Associate Development Program,FIS,,"Milwaukee, WI 53223","Position Responsibilities/Requirements
WHO ARE WE?
FIS connects consumers to their money through leading-edge payments solutions. We're the development engine of FIS: the largest, Fortune 500, Fintech company in the world. Our solution is branded by our customers - banks and retailers - so you might not have heard of us. But, if you've used a mobile banking app, interacted with an ATM, or used a debit card to complete a financial transaction, you're most likely using FIS software. Our product is one that touches the average consumer on a regular basis, and makes the financial world go 'round.
Headquartered in Jacksonville, Florida, FIS serves more than 20,000 clients in over 130 countries, and our technology powers billions of transactions annually that move over $9 trillion around the globe. FIS is a Fortune 500 company, ranked 392 on the Fortune 500 list and is a member of Standard & Poor's 500® Index. Named a 2016 ""World's Most Admired Company"" by Fortune Magazine.
WHAT IS THE POSITION?
**This position is slated to start in January 2019**

FIS is looking for a Data Engineer to join our Data Analytics team in Milwaukee, WI. This is an exciting opportunity for someone with a solid background in Statistics or Data Engineering who likes to contribute new ideas and is willing to put in the work to build their career.
The FIS 2019 Associate Development Program:
As a 2019 Associate Development Program participant with FIS, you will benefit from a two-year on the job training program. As a program participant you will have the opportunity to be mentored and participate in team projects, social events, community service activities work, and professional development seminars.
The two-year Associate Development Program participants start in January and June of each year; Upon their program start each ADP participant will complete a one-week orientation program where participants:
Learn how to work within a corporate environment
Learn the structure of FIS and options for growth within the organization
Hear from FIS leadership about their journey and participate in senior leader networking opportunities
Build a network of your peers from across the organization
The ADP program participants will participate in monthly virtual trainings with their manager and while receiving bi-weekly performance feedback from their managers they will also receive accelerated performance reviews and merit increases every six-month throughout the two-year development program.
General Duties and Responsibilities:
Works with Senior Analysts to build data models
Builds predictive models for machine learning
Builds distribution and statistical models for raw data
Develops statistical models. Selects and applies appropriate statistical techniques to analyze clinical and/or other related data
Develops data visualization models
Performs statistical analyses, under supervision on complex projects, including; analytical methodologies, appropriate statistical sampling, statistical data interpretations, and reporting format and content.
Contributes to project dissemination efforts. This includes writing and reviewing content related to methodologies and results in drafts of reports, posters, and manuscripts as well as creating graphics and tables.
Develops and implements strategies as well as documents and maintains consistent procedures for data processing, validation, and evaluation. Ensures data quality for documentation and dissemination.

WHAT WE EXPECT FROM YOU?
You have a passion for data and numbers
You understand how to work in a collaborative environment
You know how to program in S, Java and/or Python
You take good notes and you are willing to ask for help when needed - this is important!
If you enjoy learning from a variety of team members, developing new ways to enhance existing processes, are driven by real-world applications and are looking for a great place to grow your career - we want to hear from you! We are looking for smart, driven and dedicated team members to join our team.
Required Skills:
Bachelor's degree in applied math, statistics, computer science, business analytics or related field.
Proficiency in one or more of the following programming languages: S, Java, Python
Experience building statistical models of data
Interest and ability to work in AI/Machine Learning
WHAT CAN YOU EXPECT FROM US?
At FIS we value new ideas and we pride ourselves on providing a wide-range of opportunities for professional growth in a face-paced environment. FIS offers an open minded collaborative culture with enthusiastic technologists. A true partner. We are dedicated to our team members and our customers equally. We strive to create an environment to help you succeed.
FIS Benefits and Perks:
A generous paid time off program in which the benefits increase along with your tenure with the company
Health coverage offered for you and your family through Health/Vision/Dental/Insurance plans
Tax advantage flexible spending accounts and health savings accounts that allow you to pay for specific health-care and dependent care expenses with pre-tax dollars
To help you manage your work and life needs we offer a health advocate program, tuition reimbursement and a group legal service plan
401K match and Employee Stock Purchase Program
Company Paid Volunteer Day
Onsite café
Onsite gym
Casual dress code (jeans every day if desired)
FIS Gives Back Program - supporting our local community
**Current and future sponsorship not available for this position**

With a 50-year history rooted in the financial services industry, FIS™ is the world's largest global provider dedicated to financial technology solutions. We champion clients from banking to capital markets, retail to corporate and everything touched by financial services. Headquartered in Jacksonville, Florida, our 53,000 worldwide employees help serve more than 20,000 clients in over 130 countries. Our technology powers billions of transactions annually that move over $9 trillion around the globe. FIS is a Fortune 500 company and is a member of Standard & Poor's 500® Index.

FIS is committed to protecting the privacy and security of all personal information that we process in order to provide services to our clients. For specific information on how FIS protects personal information online, please see the FIS Online Privacy Notice.

FIS is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, genetic information, national origin, disability, veteran status, and other protected characteristics. The EEO is the Law poster is available here: www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf and here: www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf

For positions located in the US, the following conditions apply. If you are made a conditional offer of employment, you will be required to undergo a drug test. ADA Disclaimer: In developing this job description care was taken to include all competencies needed to successfully perform in this position. However, for Americans with Disabilities Act (ADA) purposes, the essential functions of the job may or may not have been described for purposes of ADA reasonable accommodation. All reasonable accommodation requests will be reviewed and evaluated on a case-by-case basis.

As part of the selection process this role may require an assessment to determine suitability

Recruitment at FIS works primarily on a direct sourcing model; a relatively small portion of our hiring is through recruitment agencies. FIS does not accept resumes from recruitment agencies which are not on the preferred supplier list and is not responsible for any related fees for resumes submitted to job postings, our employees, or any other part of our company."
257,Data Engineer,Engage3,,"Davis, CA","As a Sr Data Architect at Engage3, you will lead a technical team that architects, builds, maintains, scales, monitors, administrates and secures Engage3's retail pricing platform. You will actively work in a multi-disciplinary fast-paced environment. Your ultimate goal is to create a solid, flexible, stable system that enables us to deliver best-in-class analytics products to retailers and brands in the face of massive growth.
This role requires a broad range of skills and abilities; you will be the functional lead, manage staff and do the work. Your primary responsibility is to enable data access, data processing and data products by architecting, maintaining, scaling, monitoring & securing.
ML production system (AWS, Python)
Data Warehouse (Snowflake)
ETL system & data pipelines
BI system (Tableau Online)
As a qualified applicant:
You have planned, built & managed data infrastructures in a public cloud
You have strong experience with working with tools & platforms within the AWS ecosystem (EC2, S3, Aurora, Lambda, API Gateway, etc)
You have in-depth experience with MySQL databases and Snowflake's data warehouse
You have managed a business intelligence system
You have demonstrated experience of ETL developments
You are proficient in at least one programming language like Python, Scala and Java
You have familiarity with big data technologies like Hadoop, Spark, Hive
You are comfortable with setting and meeting SLAs for data availability and quality
You have an understanding of Machine Learning / AI principles in data engineering
You are a mentor to your team & colleagues and have passion in sharing your knowledge
You've worked in an Agile environment. You thrive on iteration. You make opportunities to bring value sooner rather than later.
You value data-driven decisions. You are always looking for opportunities to quickly produce the right data to make decisions quickly. You keep cool under pressure.
You are a self-driven, highly motivated technologist who can work with a high degree of autonomy, is able to prioritize effectively and drive the data architecture vision."
258,Senior Data Engineer,Kognetics,"$101,000 a year","Gahanna, OH","Job Responsibility:
Based on business strategy and knowledge of emerging technologies, drive the architecture and design of the analytical platform (Big Data, Semantic Computing, Graph Database, Graph Analytics)
Use data mining & Text mining algorithms using open source tool like R, Gate & NLP (Stanford, Berkley, UIMA, etc … ) to provide the solution to various business problems.
Use data visualizations tools such as D3J, to tell compelling business stories, via complex mashups
4+ years of experience in core Java/J2EE, Python.
2+ Year of experience in Struts/Spring,
Experience in Hibernate, Django will be added advantage.
Experience in MySQL / PostgreSQL
Good knowledge of JavaScript, JQuery.
Experience in Open Source search technology Like SOLR / Elasticsearch is a plus
Experience in Hadoop, HBase, Graph database and MongoDB will be added advantage.
Experience in Product development background will be a plus.

Basic Qualifications: Passion,commitment, resourcefulness, and a drive to continue learning are essentialprerequisites. For this role, we’re also looking for someone who meets thefollowing criteria:
 B. Tech/PhD/ Master's Degree inStatistics, Mathematics, Computer Science, or equivalent; 5+ years of datascience mining experience;"
259,Big Data Engineer,Integral Ad Science,,"Chicago, IL","Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions for the advertising industry and we're looking for a Data Engineer to join our Data Engineering team. If you are excited by technology that has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day; and evaluate thousands of data-points in real-time all while responding in just a few milliseconds, then IAS is the place for you!

As a Data Engineer you will build and expand upon the testing framework and testing infrastructure of IAS' core ad verification, analytics and anti ad fraud software products.

The ideal candidate is naturally curious, dedicated, detailed-oriented with a strong desire to work with awesome people in a highly collaborative environment. You should be able to not take yourself too seriously as well.

What you'll do:

Working on Big Data technologies such as Hadoop, MapReduce, Kafka, and/or Spark in columnar databases
Architect, design, code and maintain components for aggregating tens of billions of daily transactions
Lead the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming and batch ETL's and RESTful API's
Mentor junior team members

Who you are and what you have:

5+ years of recent hands-on in object oriented language (Java, Scala, Python)
Strong knowledge of collections, multi-threading, JVM memory model, etc.
Great understanding of designing for performance, scalability, and reliability
Superb understanding of algorithms, data structures, scalability and various tradeoffs in a Big Data setting
In-depth understanding of object oriented programming concepts
Excellent interpersonal and communication skills
Understanding of full software development life cycle, agile development and continuous integration
Good knowledge of Linux command line tools
Experience with Hadoop MapReduce, Spark, Pig
Good understanding of database fundamentals, good knowledge of SQL

What puts you over the top:

Exposure to messaging frameworks like Kafka or RabbitMQ
Some exposure to functional programming languages like Scala
Experience with Spark

About Integral Ad Science

Integral Ad Science (IAS) is a global technology company that offers data and solutions to establish a safer, more effective advertising ecosystem. We partner with advertisers and publishers to protect their investments, capture consumer attention, and drive business impact. Founded in 2009, IAS is headquartered in New York with global operations in 13 countries. Our growth and innovation have been recognized in the Inc. 5000, Crain's Fast 50, Forbes America's Most Promising Companies, and I-COM's Smart Data Marketing Technology Company. Learn more at www.integralads.com ( http://www.integralads.com ).

Equal Opportunity Employer:
IAS is an equal opportunity employer, committed to our diversity and inclusiveness. We will consider all qualified applicants without regard to race, color, nationality, gender, gender identity or expression, sexual orientation, religion, disability or age. We strongly encourage women, people of color, members of the LGBTQIA community, people with disabilities and veterans to apply.

To learn more about us, please visit http://integralads.com/ ( http://integralads.com/ ) and https://muse.cm/2t8eGlN ( https://muse.cm/2t8eGlN )

Attention agency/3rd party recruiters: IAS does not accept any unsolicited resumes or candidate profiles. If you are interested in becoming an IAS recruiting partner, please send an email introducing your company to recruitingagencies@integralads.com. We will get back to you if there's interest in a partnership."
260,Data Engineer,Principal Financial Group,,"Des Moines, IA 50309","What you'll do:
As a Data Engineer in our Retirement and Income Solutions (RIS) business area, you'll own and advance the Data Governance initiatives! We are seeking a highly motivated and skilled individual to join our organization. If you're looking for a role where you can make an impact immediately, look no further!
Here are few examples of the kinds of things you’ll do:
Set standards and coordinate technical aspects of data management functions with an emphasis on data governance, quality, and privacy
Participate in Enterprise level Data Architecture and Data Governance collaboration groups as well as participation in local and industry groups.
Provide technical advice and weigh in on technical decisions that have an effect on other teams or the company at large. Research, influence and propose new technologies
Facilitate strategic discussions and execute tactical plans to build and mature a Data Store Inventory and Data Lineage tracking
Conduct data management maturity assessments to identify gaps and issues for capabilities including data quality, governance, analytics, metadata management, and master data management. Recommend ways to improve data reliability, efficiency, and quality
Facilitation and participation in architecture and design reviews
Maintaining an awareness of emerging technology trends in the data management area and sharing this knowledge with necessary partners. Conducting the necessary research and making recommendations around data technology


What you'll get from us:
Competitive pay, benefits, perks and more. We’ll reward you for the skills and experience you have. Find out more.
A great place to live, work and play. Greater Des Moines is one of the fastest-growing metro areas in the country. Given its vibrant and welcoming culture, it’s no wonder Des Moines has garnered so much national recognition—including #3 Best Affordable Place to Live (U.S. News & World Report, 2019).
Respect for your unique perspective. Diversity, inclusion and empowerment are at the core of our culture.
A career, not just a job. Principal is a place where you can learn and innovate. Do important work. Make an impact. And achieve your professional goals.
The ability to have a great job and a great life. Sure, work is important. But so is your family. And your friends. And your community. That’s why we provide the flexibility needed to find the right balance between your job and the rest of your life.


Qualifications:
Associates or Bachelor's degree in a science, technology, engineering, or math related field or equivalent work experience (6 years of experience equates to an Associate’s degree when defining “equivalent work experience”) and 6+ years of work related experience
Leadership and presentation skills required. Must be able to effectively communicate strategies and designs to all levels of the company
Data experience and a technical background including being able to work directly with data and datastores through programming languages such as SQL, Python, or JAVA
Familiarity with data governance practices and data modeling or governance software is preferred
Prior architect or data architecture experience is a plus.


Job Level:
Willing to consider applicants at multiple job levels.


How we hire:
Once you apply, your application is hand reviewed by our talent team. Generally within a few weeks, the team makes interview selection decisions and communicates those via email. If selected, you’ll receive an email from Principal Talent Team to complete a pre-recorded interview on your own time. Be sure to check your email frequently and follow the steps shared to submit timely.
Learn more about our hiring steps and find answers to frequently asked questions.


Additional Information:
Work Authorization/Sponsorship
At this time, we're not able to consider candidates needing sponsorship now or in the future or those needing work authorization for this role. (This includes students on F1-OPT, F1-CPT, J-1, etc.) However, we’d hope you continue to keep us in mind for future opportunities.

Principal is an Affirmative Action and Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to of age, race, color, religion, sex, gender identity, gender expression, pregnancy, national origin, citizenship status, disability, genetic characteristics, sexual orientation, marital status, domestic partner status, military status, protected veteran status, disability status or any other characteristic protected by law.

Investment Code of Ethics
For positions with Principal Global Investors, you will be required to follow a Code of Ethics which covers a number of obligations related to personal and business conduct as well as personal trading activities for you and members of your household. These same requirements may also apply to other positions across the organization.


Who we are:
We help people build the kinds of lives they dream about—empowered by financial security and stability. We started in 1879 (yes, 1879) as an insurance company. Today, we’re a member of the FORTUNE 500® and a global investment management leader.
So that’s what we do. As far as who we are, well, we believe that how we do things is just as important as what we do.
Doing what’s right has been a part of who we are for more than 140 years. We focus on taking care of not only our employees, customers and advisors, but also the people who live in the communities where we do business.
Learn more about our values in the workplace."
261,Data Engineer,BP,,"Denver, CO","Job profile summary:
Responsible for supporting the delivery of business analysis and consulting processes and procedures for the defined specialism using basic technical capabilities, developing working relationships to provide support with queries, issues and ad-hoc requests and assisting with quality assurance services.Specialisms: Business Analysis; Data Management and Data Science; Digital Innovation.
Job Advert:
Role synopsis
The Data Engineer is responsible for the maintenance, improvement, cleaning, and manipulation of data in L48’s operational and analytics databases. The Data Engineer works with the L48’s IT platform owners, data analytics teams, data scientists, and data warehouse architect in order to understand and aid in the implementation of database requirements, analyze performance, and troubleshoot any existent issues.
The Data Engineer has to be an expert in SQL development further providing support to the Data and Analytics team in database design, data flow and analysis activities. The position of the Data Engineer also plays a key role in the development and deployment of innovative big data platforms for advanced analytics and data processing.
The Data Engineer defines and builds the data rules and pipelines that will enable faster, better, data-informed decision-making within the business
The Data Engineer ensures stable solutions, stable infrastructure and participates in the design and build of services, integrations, and ETL solutions
Key accountabilities
Creating databases optimized for performance, implementing schema changes, and maintaining data architecture standards across all of L48’s databases.
Leads innovation through exploration, benchmarking, making recommendations, and implementing data technologies
Leads the development and implementation of scripts for database maintenance, monitoring, performance tuning, and so forth – in collaboration with IT DBAs and support groups.
Designing and developing scalable ETL packages from the business source systems and the development of ETL routines in order to populate databases from sources and also to create aggregates
Enabling and running data migrations across different databases and different servers, for example, data migration from SQL servers to MySQL; or on-premises to cloud;
Responsible for performing thorough testing and validation in order to support the accuracy of data transformations and data verification used in any service. E.g reporting, visualizations, machine learning models
Ensure proper data governance and quality across the Data and Analytics department and the business as a whole
Implement Master Data Management solutions, processes and governance to support Data Management team
Performs ad-hoc analyses of data stored in the L48’s databases and designs, develops and tests SQL scripts, stored procedures, functions, and views
Troubleshoots data issues within the business and across the business and presents solutions to these issues
Analyze complex data elements and systems, data flow, dependencies, and relationships in order to contribute to & define conceptual physical and logical data models
Collaboratively works with the entire Data and Analytics team, providing support to the entire department for its data centric needs

Key technologies
5-7 years of experience with SQL Server On-Prem and/or Azure cloud, required
5-7 years of SSIS experience utilizing SQL Agent jobs and Integration Services Catalog, required
3-4 years of experience running ETL/ELT SSIS projects independently from end to end, required
2+ years of Python programming experience, preferred
Experience with DevOps Repository and Git, preferred
Azure cloud experience, preferred
AWS cloud platform experience, a plus
Azure Data Lake storage experience, a plus
Azure Data Factory and/or Databricks experience, a plus
About BP:
BP's BPX Energy business (formerly known as the Lower 48) operates across a vast US geography, from Texas north through the Rocky Mountains. The business manages a diverse portfolio which includes an extensive unconventional resource base of about 7.5 billion barrels of oil equivalent across 5.5 million gross acres in some of the largest and most well-known basins in the US. Headquartered in Denver (Colorado), BPX Energy employs about 1,700 people across six states, operates more than 9,600 producing wells and has 70,000 royalty owners. Our vision is to be the premier, high return, onshore exploration and production company that consistently increases asset value. Our Wyoming operations are anchored on the giant Wamsutter tight gas field in the south central part of the state. In the San Juan area of Colorado and New Mexico we produce from tight gas sands and operate the largest coal-bed methane field in the US. Our Mid-Continent operations cover the prolific Anadarko, and is home to the famed East Texas basin, along with the Woodford shale gas play and Arkoma basin. We also have non-operating interests in over 10,000 wells across the US with substantial positions in both the Eagle Ford and Fayetteville shale basins. In 2018, BP completed a $10.5 billion acquisition of BHP's world-class unconventional oil and gas assets in the Permian-Delaware basin in Texas, along with two premium positions in the Eagle Ford and Haynesville basins in Texas and Louisiana. These assets currently produce 190,000 barrels of oil equivalent per day, of which about 45 percent are liquid hydrocarbons. The deal represents BP’s largest purchase since buying ARCO in 1999. It is a transformational acquisition for our BPX Energy business which gives the BPX Energy team access to some of the best acreage in some of the best basins in the onshore U.S."
262,Data Engineer,Prokarma Inc.,,"Seattle, WA","ProKarma currently has exciting opportunities for Data Engineers that would like to join our team as a full-time employee in Seattle, WA.

Responsibilities
Expand and optimize data availability for end users and develop new features for storage, analytics and availability management
Create and maintain the data pipeline architecture
Build the infrastructure required for processing of data based on defined schema using appropriate AWS technologies
Build analytics tools to monitor and manage the data stack to maintain reliability and reduce time to fix in cases of issues or outages
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Assemble large, complex data sets that meet business needs
Identify, design and implement internal process improvements like automating manual processes, optimizing data delivery and re-designing infrastructure for greater scalability
Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs
Keep data properly secure (encryption, managing permission levels, handling PII information, ect.)
Work with data and analytics experts to strive for greater functionality in data systems
Qualifications
Bachelor’s degree in Computer Science or other relevant program preferred
Experience building and optimizing ‘big data’ pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with structured and unstructured data-sets
Working knowledge of message queuing, stream processing and highly scalable ‘big data’ stores
Solid experience with big data tools: Hadoop, Spark, Kafka, etc.
Previous experience with relational SQL and NoSQL databases (as well as column-based storage)
Experience with data pipeline and workflow management tools
Experience with AWS cloud services: S3, EMR, Redshift
Experience with relevant scripting languages: Python, Java, Scala, etc.
Experience with JSON file formatting and querying
DBA or AWS certifications are a plus
Additional Required Skills: Parquet; Redshift; SQS/JMS; SQL/NoSQL; MongoDB; Hadoop; Automation; Kafka; JSON; Schema; Anomaly Detection and Presto

~ In order to provide equal employment and advancement opportunities to all individuals, employment decisions at ProKarma are based exclusively on merit. ProKarma does not discriminate in employment opportunities or practices on the basis of race, color, religion, sex, including gender identity and identity expression, national origin, age, or any other characteristic protected by law."
263,"Data Engineer, Promise","Amazon.com Services, Inc.",,"Seattle, WA","Bachelor’s degree in Computer Science, MIS, related technical field, or equivalent work experience.At least 2 years of relevant work experience in analytics, data engineering, business intelligence, or related fieldDemonstrable ability in data modeling, ETL development, and data warehousingProven skills and experience using SQL with large data sets (e.g. Oracle, SQL Server, and Redshift)Experience with AWS technologies including Redshift, RDS, and S3Effective communication and strong collaboration skills with research, engineering, and product management teams

Do you want to play a crucial role in the future of Amazon's retail business by building data systems that enable data driven decision making at the largest scale possible?

The Promise team sets the expectation of when customers’ items will arrive. The delivery promises we make are key drivers of customer behavior and influence the short and long-term profitability of Amazon’s global retail business. We serve as the interface between customer expectations and Amazon's industry-leading fulfillment capabilities. We are part of the Supply Chain Optimization Technology (SCOT) Group that develops and manages systems that optimize inventory acquisition and placement so products are available for fast delivery. (For more information on SCOT, see this short video: http://bit.ly/amazon-scot.) Successful members of the Promise team are customer obsessed, flexible, and collaborative team players who enjoy working across functions and organizations to solve problems and get results. They ask hard questions and build solutions that provide the critical business insights needed to influence decision making across retail and operations teams. They find timely answers buried in large data sets and complex systems, identify root causes, and get their hands dirty building data systems and sharing insight.

The Promise team is seeking a Data Engineer with broad technical skills and world-class data management skills to develop the data systems we use to optimize delivery promises for Amazon's customers worldwide. As an Amazon Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. You will work on analytics, visualization solutions, and other Business Intelligence applications used by researchers, engineers, and management across multiple departments. You should be reliable at designing, implementing, and operating stable, scalable, low-cost solutions that persist data from production systems into the data warehouse and end-user facing applications. You should have deep expertise in the design, creation, management, and business use of extremely large datasets. You will need excellent business and communication skills to work with researchers and business owners in a fast paced environment to develop and define key business questions and requirements. Above all, you should be passionate about working with huge datasets and analytics tools to provide insight into how the business is running and enable business decisions to be data driven.

Responsibilities include:
Designing, implementing, and supporting an analytical platform that provides ad hoc access to large data sets and computing powerManaging AWS resources including EC2, S3, and RedshiftWorking with other technology teams to extract, transform, and load data from a variety of data sources using SQL and AWS big data technologiesEvaluating and implementing various big-data technologies and solutions to optimize processing of extremely large data sets in an accurate and timely fashionRecognizing and adopting best practices in reporting, analysis, data integrity, test design, validation, and documentation
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.

Graduate degree in computer science, business, mathematics, statistics, economics, or other quantitative field3+ years prior experience in a Data Engineer role with a technology company or financial institution.Knowledge of Advanced SQL and scripting for automation (e.g. Python, Perl or Ruby)Oracle, Redshift, Linux, OBIEE experienceExperience with multiple database platformsFamiliarity with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complexity analysisFamiliarity with statistical concepts and terms and data mining algorithmsExperience with Hadoop or other map/reduce ""big data"" systems and services"
264,Data Engineer,Clearcover,,"Chicago, IL","Who's Clearcover?
Clearcover is the smarter car insurance company. We use powerful technology to offer everyday drivers better coverage for less money. We're proud to be one of the fastest growing startups in Chicago, and we're currently looking to add a few more extraordinary people to our team.

What is a Data Engineer?
Reporting to Braun Reyes, the Data Engineer is responsible for building, owning, monitoring, and maintaining our data and analytics ecosystem. This platform powers our business decisions and data product initiatives by enabling our analysts, engineers, and data scientists with the data access and tools they need to deliver value. Core responsibilities include extracting and ingesting raw data into our data warehouse/data lake and providing frameworks and services for operating on that data. Our data comes from multiple sources, including clickstream, change data capture, webhooks, sftp, and API in formats that include tabular, json, and xml.

What will you do?


Build batch and real-time data pipelines into our Snowflake data warehouse using AWS serverless, open-source ELT (extract-load-transform) frameworks, and ETL as a service technologies.
Create workflows to monitor and alert on service/job health and data quality.
Deploy and own full set of cloud infrastructure created to support data management workflows.
Develop custom utilities and components as needed to compliment existing tooling.
Implement automated deployment of data pipelines using CI/CD best practices.
Ensure privacy, compliance and security of data
Participate in integrating our core data assets into the rest of the technical organization and Clearcover product suite.
Provide technical mentorship to fellow Engineers and Analysts
Advise and assist on application side data modeling and data science workflow design

What do you need?


Fluency in Python for data extraction, transformation, and cloud service api interaction.
Ability to write SQL for reporting, transformations, and data loading/extraction.
Experience implementing, testing, debugging and deploying batch data pipelines using workflow management tools like Apache Airflow, Apache Azkaban, DBT, and/or Step Functions.
Experience with managing and loading data into cloud data warehouse services like Snowflake and/or Redshift.
Experience deploying containerized applications and workflows using Docker on platforms like Kubernetes and/or ECS/Fargate.
Experience building real-time data pipelines using services like Alooma, Fivetran and/or technologies like Kinesis, Apache Kafka.
Experience deploying and administering cloud DBAAS (Database as a Service) like RDS(Postgres, MySQL, Aurora) and/or Dynamodb.
Ability to automate cloud infrastructure provisioning via Terraform and/or Cloudformation.
Solid understanding of CI/CD workflows for data pipelines/services.

Nice to haves?


Interest in writing about technical solutions developed by Clearcover Data Engineering.
Experience with deploying data science workflows and/or services using frameworks like AWS Sagemaker leveraging Machine Learning, Deep Learning, and Computer Vision.
Experience with the AWS Serverless stack (Lambda, SNS, SQS, Kinesis, Step Functions, Fargate, Dynamodb).
Familiarity with BI tools such as: Periscope, Tableau, and Looker

What's in it for you?


Unlimited vacation, we hire adults
Equity for all employees, so you own a piece of the pie too
Dental and Vision, we've got you covered 100%
Medical, we cover 90% of your premium, 75% of your dependents and contribute to your HSA and HRA (cha-ching)
We invest in your future by contributing 3% of your salary to a 401(K), even if you don't
Come to work pre-taxed through our FSA commuter benefits
and yes, we have unlimited LaCroix, beer, snacks and the occasional ice cream social

"
265,Big Data Engineer,Alt Shift USA,,"Dallas, TX","Multiple Bigdata Job Positions across Dallas, TX and Tampa, FL
Position 1: Big Data Engineer
Location: Dallas TX
Experience Required - 6+ years
Job Description
Hadoop/HDFS.
Spark is a must. Scala preferred but Java is ok too.
Experience Spark core, Spark SQL, Spark Streaming ( these 3 are a must) and Spark ML is good to have.
Position 2: Sr. Big Data Engineer
Experience Required - 8-15 years
Location: Tampa, FL
Job Description
The client is looking for a senior engineer who can drive things rather than being managed by the client.
Hadoop/Hive and Kafka.
Spark streaming using Java is a must.
Position 3: Lead Big Data Engineer
Location: Dallas TX
Experience Required - 8-15 years
Job Description
Primary requirement - Spark, Scala developer with knowledge of Kafka.
Good to have exposure to other NoSQL databases like Casandra.
AWS experience will be a definite plus."
266,Data Engineer,Bills.com,,"Tempe, AZ","WHO WE ARE:

Freedom Financial Network is a family of companies that takes a people-first approach to financial services, using technology to empower consumers to overcome debt and create a brighter financial future. The company was founded in 2002 by Brad Stroh and Andrew Housser on the belief that by staying committed to helping people, you can ensure better financial outcomes for both the customer and the business. This Heart + $ philosophy still guides the vision of our growing company, which has helped millions of people find solutions for their financial needs.
What began with 2 people in a spare bedroom has now rapidly expanded to a vibrant business that employs over 2,200 employees (known internally as The Freedom Family) in two locations: San Mateo, CA and Tempe, AZ. When you visit either of our offices, you’ll understand why our employees have voted us the Best Place to Work for the last several years. It’s a place where the Heart + $ philosophy continues to thrive, where we believe that success is only achieved by doing what’s right for our customers, our employees, and our communities.

In order to create brighter futures for our clients, employees, and businesses, Freedom Financial Network holds itself to four core values that have grown out of our Heart + $ philosophy: to care for everyone around us, act with integrity every time, collaborate with everybody we work with, and get better at what we do every day.

THE OPPORTUNITY:
As a BI Engineer with Freedom Debt Relief in its Tempe, AZ office, this position will report to the BI Engineer Manager, but will work closely with a strong Engineering team comprised of BI Engineers, Data Engineers, and the Director of Engineering. We are building a BI platform to provide a single and consistent source of truth in a fast-growing organization (1,800 employees, adding 100+ every month), and will do so with:
Best in breed technology – Google Big Query & Tableau
Small, smart team of engineers and analysts
Well executed dashboards, self-service modules, alerting and real-time monitoring tools
Evangelizing the incremental business value to aid platform adoption
THE ROLE:
Work with BI Product and Analysts on a daily basis to understand reporting requirements
Design and model data structures to support daily, operational and ad hoc reporting
Own and support production data warehouse objects
Participate in daily scrums and work with data engineers to ensure data availability
Effectively manage your time and lead platform development efforts for the line of business
REQUIREMENTS/CHARACTERISTICS:
4-5 years’ experience in developing and maintaining BI solutions.
2-3 years’ experience in building and enhancing data warehouses.
Strong data modeling skills. Good understanding and experience in building star schema and denormalized data structures.
Report authoring experience. Working knowledge of reporting tools like Tableau, Looker, SSRS, Business Objects etc.
Strong SQL skills.
1-2 years’ experience implementing ETL.
Ability to translate business questions to technical specifications.
Strong communication skills
CULTURAL FIT (Our Core Values):

Care (for everyone): We show compassion and contribute to the well-being and growth of those around us. We only pursue products that improve the financial lives of our clients.
Act with Integrity (every time): We take the right action even when it is hard and even when no one is watching. We treat our employees, clients, and communities the way they wish to be treated.
Get Better (every day): We innovate, iterate, and improve each day. We are creative, take thoughtful risks, and ultimately learn and recover from failures.
Collaborate (with everybody): We strive to work together toward a common purpose by proactively sharing information and inviting participation. We recognize the perspective of various groups and embrace healthy, constructive debate.
WHY JOIN THE FREEDOM FAMILY?

Fast, continued growth – there’s a lot of opportunity for advancement
Voted a Phoenix Best Place to Work 9 times by our employees including the #1 spot for 2 years in a row!
Benefits start within 30 days
401k with employer match
3 weeks’ paid vacation (increased with tenure)
9 paid holidays & 5 sick days
Paid time off for volunteer work and on your birthday

Attention Agencies & Search Firms: We do not accept unsolicited candidate resumes or profiles. Please do not reach out to anyone within Freedom Financial Network (FFN) to market your services or candidates. All inquiries should be directed to Talent Acquisition only. We reserve the right to hire any candidates sent unsolicited and will not pay any fees without a contract signed by FFN’s Talent Acquisition leader."
267,Data Engineer,EmployIndy,,"Indianapolis, IN 46204","EmployIndy has partnered with a local employer who helps companies build medical devices that improve and extend the lives of millions of people. Engineers use our Software as a Service platform to build quality into their device development process. Think: Github + JIRA for medical devices.
We're experiencing rapid customer growth. We need your help to build a data pipeline that enables our machine learning team to deliver unique insights to our customers about how to build and deliver better medical devices. We have dozens of data sources with complex relationships. You'll help us wrangle this data using a mix of tools like standard relational databases, graph databases (e.g. Neo4j), noSQL databases (MongoDB), Kafka, natural language processing (NLP), and web scraping.
On a given workday you will:Work closely with our machine learning, backend engineering, and product teams to normalize, transform, and map disparate data sources in a way that enables insightStay up to date with state-of-the-art data engineering and development tools and participate in professional development opportunitiesCollaborate with the entire development and management team to advance company-wide Quality initiatives, including automated testing, continuous integration, and continuous deployment
Great-fit candidates will have:
2+ years of professional development experienceA deep appetite for learning new tools and services. We don't expect you to have already used all of the tools you'll be using in this roleExperience and interest doing backend web product development in a language like Java or C# / .NET to understand how this data and insights will work its way into our productExperience with one or more of these data engineering tools, such as Neo4j, ElasticSearch, GraphQL, Cassandra, FlockDB, Kafka, MongoDB, and Redis
We'll be extra excited to talk to you if you have:
Experience working with data science or machine learning teamExperience using JavaScript to interact with APIs
Benefits:
Co-workers who care deeply about our mission to spur medical device innovation and about helping each other become better engineersFlexible hoursWork from home optionsPTO"
268,"Data Engineer, Digital Innovation Team",TSYS,,"Alpharetta, GA","Summary
At TSYS, We are creatively and disruptively solving some of the most complex database problems related to immense growth, scaling, extremely high performance and high availability requirements. Do these types of challenges excite you?
TSYS is seeking talented database engineers who will innovate and engineer solutions in the area of database technology. We need individuals who are enthusiastic about applying bold new ideas to solving real-world data problems. The Data Engineering team is actively engaged in the ongoing database engineering process, partnering with development groups and providing deep subject matter expertise input as stakeholders to design reviews, and as an advocate for bringing forward and resolving customer issues.
Candidates must have:
Deep knowledge and experience designing and maintaining relational databases including Oracle, MySQL, Postgres or SqlServer
Experience developing and supporting complex mission-critical production database systems
Broad awareness of customer workloads and use cases, including performance, availability and scalability
Experience analyzing issues holistically, from the application tier through the database, down to the storage
Awareness of emerging technologies and approaches in IT
Working knowledge of relational database internals (locking, consistency, serialization, recovery paths)
Working knowledge of at least one scripting language (shell, Python, Perl)
Coding skills in the procedural language for at least one database engine a must (PL/SQL, T-SQL etc.)
Proven track record of automating tasks
Root cause analysis of production-related database issues
On-call for production databases - daily maintenance, monitoring, problem resolution and internal customer and dev support
Excellent SQL and DB performance tuning skills
Experience with other Data Persistence platforms: RDBMS (Postgres, Oracle, MySQL), Distributed SQL Engines (Cassandra, Foundation DB, Cockroach DB), NoSQL (Mongo DB), Hadoop, Data and Software Migration off/to RDBMS, AWS DBs (RDS, DynamoDB, EMR, Redshift, Aurora, etc.), Data Information Lifecycle Management, Data Security, Big Data....
Working with one or more streaming platforms, such as Apache Kafka, Spark Streaming, Storm, or AWS Kinesis
Track record of engineering performance and availability solutions
Exposure to cloud environments and architectures
experienced in various Clustering and Sharding architectures
Experience in Kubernetes will be a plus
What Are We Looking For in This Role?
Minimum Qualifications
BS in Computer Science, Information Technology, Business / Management Information Systems or related field
No experience required. Typically has a basic knowledge and use of one or more languages / technologies from the following but not limited to; two or more modern programming languages, experience working with various APIs, external Services, experience with both relational and NoSQL Databases.
Preferred Qualifications
BS in Computer Science, Information Technology, Business / Management Information Systems or related field
Professional experience in coding, designing, developing and analyzing data
Level: Or Above
Not Ready to Apply? Join Our Talent Community!!
US Applicants:

TSYS is an equal opportunity employer (EOE) committed to employing a diverse workforce and sustaining an inclusive culture."
269,Data Engineer,Zazzle,,"Redwood City, CA 94063","We're looking for highly motivated individuals for an engineering position on the Zazzle core development team. As part of a fast paced organization, this position requires that the candidate be able to work closely with our search and growth engineering teams, product management, data scientists, site operations, and quality assurance. The candidate is expected to be a self-starter, a team player, and a strong driver for results and continuous improvement. We're looking for superstar software engineers with a (1) profound passion to innovate, (2) who are motivated by tackling great technological challenges, (3) who want to work beside those just as passionate, and (4) are itching to do the best work of their lives.
Responsibilities:
Design and architect our workflow and pipelines for relational and non-relational data.
Implement such systems to be flexible, reliable, real time and reproducible.
Employ the use of the latest data management systems such as Kafka, Airflow, Spark and Hadoop HDFS, or any tech as see fit.
Evaluate vendors solutions in the area of ETL and real time data delivery.
Work with our data scientists, machine learning engineers, search engineers and growth engineers to design data systems to serve their needs.
Preferred Qualifications:
Experience with data workflow/ETL tech such as Kafka, Airflow, Spark, Hadoop or related cloud based tech.
Top-notch analytical and problem solving skills.
Strong Java/C#, or other modern OO programming language skills.
Proficiency in SQL.
Experience building any facet of large-scale systems a big plus.
Strong production debugging and performance tuning skills.
Comfortable technically reviewing code and participating in design/architecture reviews.
Great communication skills.
 Education Requirement:
Bachelor of Science or above in Computer Science or a related field.
Career Development And Training:
Lots of internal tech talks by engineering team members.
Many opportunities to join various industry conferences.
Project rotations with lots of opportunity to work on the latest technologies."
270,Junior Data Engineer,SpareFoot,,"Austin, TX 78705","We are looking for a Junior Data Engineer to join our Data Engineering team in Austin. This team member will be responsible for working with the team to expand and optimize our cloud-native big data pipeline architecture, as well as optimize data flow and collection for the teams we support. The ideal candidate is excited to work closely with a team of engineers and analysts and to learn new tools and technologies in the data and data engineering space and already has an understanding of SQL, database structures, and software development processes. Curiosity, a great attitude, and an aptitude for data and software are highly valued.

About Us
We are the leading technology provider for the $38 billion self-storage industry. Our solutions, which includes more than 15,000 facilities, offers consumers the ability to find, compare and book self-storage, manage the operations of their storage business, and a host of other capabilities that make them better business managers . We provide a suite of industry-leading web marketing tools for self-storage operators and have been chosen as a preferred partner by more major online brands than any other self-storage company.

In 2018, SpareFoot acquired SiteLink and storEDGE, global leaders in self-storage management software and in-house payment processing. The deal allows the combined business to accelerate investment, drive innovation and generate value for both consumers and facility operators.

We are now relaunching the combined company under one name and consolidating the way that all of the merged and acquired entities go to market.

For three consecutive years, SpareFoot has been recognized by Entrepreneur Magazine and the Austin American-Statesman for an exceptional company culture.

What you’ll do everyday:
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet both functional and non-functional requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologies
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needs
Create data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our product
Work with data and analytics experts to strive for greater functionality in our data systems
What you need to bring to the table:
Computer science degree or equivalent experience
2+ years experience in software development, data engineering, BI development, and / or data architecture
Experience with Python, SQL, AWS, RESTful APIs, and Tableau or other data visualization tools"
271,Data Engineer - Prime Video,"Amazon.com-Amazon.com Services, Inc.",,"Seattle, WA","Bachelor's Degree in Computer Science or a related technical disciplineExpertise in the design, creation and management of large datasets/data modelsExperience working with business owners to define key business requirements and convert to technical specificationsAbility to write high quality, maintainable, and robust code, often in SQL, PL/SQL and Python.Expertise in data modeling - understanding of 3NF, Star schema, etc.Expertise in designing systems and workflows for handling Big data volumes

Amazon Video is changing the way millions of customers interact with video content. The Amazon Video team delivers high-quality instant video to Amazon customers through subscriptions (Amazon Prime) as well as purchases and rentals. Amazon believes so deeply in the mission of Video that we've launched our own studio to create original and exclusive content.

Amazon is looking for a Data Engineer to join the Prime Video Data Engineering team. There are millions of videos watched each day by customers. In order to evaluate the performance of the business and make the best forward looking decisions, we need to store and process Bigdata volumes related to Playback activity of digital content. Prime Video Data Engineering team presents exciting opportunities to work on very large data sets in one of the world's largest and most complex data warehouse environments. Our data warehouse is built on AWS cloud technology like EMR, S3 and Redshift for performing ETL processing on over 100+ TB of relational data in a matter of hours. Our team is serious about great design and redefining best practices with a cloud-based approach to scalability and automation.

As a data engineer in this team, you will take a leadership role in the data platform and you will solve big data warehousing problems on a massive scale. You will apply cloud-based AWS services to solve challenging problems around: big data processing, data warehouse design, and BI self-service. You will be part of a data engineering team that focuses on automation and optimization for all areas of DW/ETL maintenance and deployment. You will work closely with the business and technical teams in analysis on many non-standard and unique business problems and use creative problem solving to deliver actionable output. The role of data engineer in Amazon requires excellent technical skills in order to develop systems and tools to process data as well as, but not limited to, the ability to analyse data. Your work will have a direct impact on the day-to-day decision making in the Amazon Video team.

Experience with Amazon RedshiftExperience handling Bigdata volumesExperience in AWS technologiesExposure/Experience in Bigdata Technologies (hadoop, spark, presto, etc.Experience working in a UNIX/LINUX environmentStrong analytical and problem solving skillsExcellent verbal and written communication skills"
272,"Data Engineer, Shipping Palo Alto, CA",AI Fund,,"Palo Alto, CA","Dogpatch Shipping is an early-stage startup at the forefront of bringing AI to the maritime shipping industry. This is a trillion dollar industry that moves 90% of the goods we interact with on a daily basis, but has traditionally lagged far behind other industries in adopting new technologies. At Dogpatch Shipping, we’re changing that. We’re building an AI-enabled product that solves the shipping industry’s biggest pain point and we already have some of the world’s biggest shippers as our partners.

We are currently looking for an exceptionally talented Data Engineer to add to our small team. This person will be responsible for leading our Data Engineering efforts in addition to guiding our initial Dev Ops efforts and contributing to the team’s Machine Learning model development.

Since this person will be joining an early-stage startup at the ground level they’ll need to be able to wear multiple hats and thrive while working in a dynamic environment.
Primary responsibilities:
Assess data quality coming from our partners, develop a data quality framework and work with our partners to optimize data quality moving forward
Architect, develop and optimize ongoing ETL pipelines for that data. These pipelines must be built to support scale
Help develop and validate complex machine learning models
Design and implement overall data security strategy for Dogpatch Shipping Mentor/support other technical staff on data modeling and ETL related issues
Effectively communicate data strategy with a wide range of stakeholders
Help interview candidates and further build out the team
What You Must Bring:
Must Haves:
3+ years of experience with data warehouse technical architecture
3+ years of experience with Python, Spark, ETL/Data engineering, Docker/Kubernetes, automation/devops in AWS
S3, EC2, EMR, RDS, Redshift, Glue, Lambda, EKS, Sagemaker experience
Experience setting up security for Redshift, S3, and EC2
Basic knowledge in machine learning model development
Entrepreneurial grit - You are ready to roll up your sleeves and willing to be scrappy in order to carry out plans under time and resource constraints
Bachelor’s or Master’s degree in computer science or software engineering
Nice to Haves:
Experience with data visualization tools such as TableauKnowledge of the shipping industry or experience with logistics products
What We Have to Offer:
Free snacks, drinks, lunches and dinners daily
401(k) plan
Untracked PTO including a week off between Christmas and New Year’s
Variety of medical, dental and vision coverage to choose from
Collegial atmosphere
Direct and transparent communication styleInnovative environment
Work with global AI Leaders
Be part of a great cause to improve lives everywhere through artificial intelligence
This is a full-time position based in Palo Alto, California. You must already have, or be able to obtain, authorization to work in the United States."
273,Data Engineer,US Department of Defense,,"St. Louis, MO","Duties
Summary
JOB DESCRIPTION: Data Engineers develop, construct, test, and maintain architectures such as databases and large-scale data processing systems. They clean, prepare, and optimize data for consumption through the design and construction of massive reservoirs for big data. They solve problems associated with database access and integration and unstructured data sets to provide clean, usable data for customers and IT

Responsibilities
ADDITIONAL INFORMATION: The National Geospatial-Intelligence Agency (NGA) serves as the world leader in providing timely, relevant, accurate and actionable geospatial intelligence (GEOINT). NGA's civilian, military and contract personnel evaluate imagery, maps, charts, multiple layers of foundation data - such as terrain, elevation and gravity - and the full spectrum of visible and invisible light in order to help users visualize what is happening at a particular place and time. Our intelligence officers go beyond describing 'what, where and when' to revealing 'how and why.' Our work enables decision advantage for our policymakers, warfighters, intelligence professionals and first responders. NGA is in search of Data Engineers who understand data, data architectures, data pipeline infrastructure, cloud solutions, and security best practices. Data Engineers lead efforts to create data infrastructure and integrated data pipelines; transforming, enriching, and delivering data for consumers. They may work independently and/or as part of a team of data and non-data professionals on projects of varying complexities. They will lead efforts to design logical and physical data models for data warehousing; develop robust data models, data dictionaries, and data flow diagrams. In addition, these engineers share expertise within their team and with their data science and IT counterparts and provide guidance and mentoring to junior staff. Data Engineers must be able to operate at the tactical and strategic level while working in a dynamic and sometimes ambiguous environment to deliver impactful results. They will leverage their strong interpersonal skills to effectively communicate with mission owners at varying levels inside and outside the Agency. As a Data Engineer, you will be tasked against the Agency's biggest data challenges in support of missions such as National Intelligence, National Security, Military Operations, and Disaster Relief and Preparedness, for a diverse audience including the Department of Defense, the Intelligence Community, and senior government officials up to and including the United States Congress and the President of the United States. Other duties may include: * Utilize a variety of languages and tools (e.g., scripting languages) to build data pipelines to pull together information from different source systems. * Lead efforts to design, construct, install, test, and maintain highly scalable data management systems; * Develop data set processes for data discovery, modeling, mining, and production. * Integrate and prepare large, complex data sets that meet functional /non-functional business requirements. * Design and implement high performance data pipelines for distributed systems and data analytics for customers. * Build, deploy, operate, and maintain big data analytics infrastructure. * Orchestrate large PB sized data storage and compute clusters across bare-metal and cloud. * Deploy and manage infrastructures based on Docker, Kubernetes, or OpenStack, and public Clouds such as Azure, AWS or Google Cloud Platform. * Create tool-chains for analytics and data scientist team members that assist them in building and optimizing AI workflows. * Work with Agency data and machine learning experts to improve functionality in our data and model life cycle management capabilities. * Lead efforts to develop enterprise interoperable coding and data standards and create processes to ensure use of enterprise-wide data architecture capabilities. * Solve problems and deliver solutions with database access and data integration, define data conflation, fusion, and de-confliction rules. * Collaborate with data architects, data scientists, data stewards, and customers to enable more efficient and effective data-informed decision-making. * Apply expert knowledge of scripting, tools, programming languages, standards, and software packages to build databases and data pipelines. * Accomplish data cleansing, preparation, storage, and security. * Web-based data creation and editing through iD editor or JDSM. * Data conflation activities through Hootenanny. * Upgrades to the UI and system capabilities to enhance user experience. * Use business analytics and data validation to improve campaign focus and output. * Establish and sustain a NOME User Forum. * Create and brief presentations to leadership, around the community and conferences. This position may be eligible for an INCENTIVE: The selected candidates may be offered an incentive as part of the offer of employment, based on budget availability. To receive the incentive, the selected candidate must sign a service agreement depending on the approved amount or duration of the incentive. If the employee leaves before the end of the service agreement, the employee may be required to repay a pro rata share amount of the incentive to the government.
Travel Required
25% or less - Travel Required 25% or Less
Supervisory status
No
Promotion Potential
4
Job family (Series)
2210 Information Technology Management
Similar jobs
Data Engineer
Engineers, Software Systems
Software Systems Engineers
Requirements

Requirements
Conditions of Employment
US Citizenship is required.
Designated or Random Drug Testing required.
Security Investigation
SPECIAL INFO:

As a condition of employment at NGA, persons being considered for employment must meet NGA fitness for employment standards.


U.S. Citizenship Required
Security Clearance (Top Secret/Sensitive Compartmented Information)
Polygraph Test Required
Position Subject to Drug Testing
Two Year Probationary Period
Direct Deposit Required
SPECIAL REQUIREMENTS:

You must be able to obtain and retain a Top Secret security clearance with access to Sensitive Compartmented Information. In addition, you are subject to a Counterintelligence Polygraph examination in order to maintain access to Top Secret information. All employees are subject to a periodic examination on a random basis in order to determine continued eligibility. Refusal to take the examination may result in denial of access to Top Secret information, SAP, and/or unescorted access to SCIFs.


Employees with SCI access and who are under NGA cognizance are required to submit a Security Financial Disclosure Report, SF-714, on an annual basis in order to determine continued eligibility. Failure to comply may negatively impact continued access to Top Secret information, Information Systems, SAP, and/or unescorted access to SCIFs.

NGA utilizes all processes and procedures of the Defense Civilian Intelligence Personnel System (DCIPS). Non-executive NGA employees are assigned to five distinct pay bands based on the type and scope of work performed. The employee's base salary is established within their assigned pay band based on their unique qualifications. A performance pay process is conducted each year to determine a potential base pay salary increase and/or bonus. An employee's annual performance evaluation is a key factor in the performance pay process. Employees on term or temporary appointments are not eligible to apply for internal assignment opportunity notices.


This position is a DCIPS position in the Excepted Service under 10 U.S.C. 1601. DoD Components with DCIPS positions apply Veterans' Preference to preference eligible candidates as defined by Section 2108 of Title 5 USC, in accordance with the procedures provided in DoD Instruction 1400.25, Volume 2005, DCIPS Employment and Placement. If you are an external applicant claiming veterans' preference, as defined by Section 2108 of Title 5 U.S.C., you must self-identify your eligibility in our ERecruit application.
Qualifications
MANDATORY QUALIFICATION CRITERIA: For this particular job, applicants must meet all competencies reflected under the Mandatory Qualification Criteria to include education (if required). Online resumes must demonstrate qualification by providing specific examples and associated results, in response to the announcement's mandatory criteria specified in this vacancy announcement:


1. Demonstrated proficiency in applied programming and/or manipulation of data with a programing language such as Python, R or Java. 2. Demonstrated experience enabling access to data by way of databases or dashboards. 3. Demonstrated experience cleaning, filtering, transforming data, and/or enriching data. EDUCATION REQUIREMENT: A. Education: Bachelor's degree in Computer Science, Applied Mathematics, Management Information Systems, Engineering, Physical Sciences, or any other technology related field. -OR- B. Combination of Education and Experience: A combination of education and experience that demonstrates the ability to successfully perform the tasks associated with this work. As a rule, every 30 semester (45 quarter) hours of college work is equivalent to one year of experience. Candidates should show that their combination of education and experience totals to 4 years. -OR- C. Experience: Three years of experience or training in data engineering and related technologies.
DESIRABLE QUALIFICATION CRITERIA: In addition to the mandatory qualifications, experience in the following is desired:


1. Experience as a leader, proven innovator and strategic problem solver. 2. Experience working with one or more database structures (e.g. relational, noSQL, graph) to include experience with database retrieval methods (e.g. SQL, database specific queries, APIs). 3. Familiarity with cloud services as applied in the DoD and IC. 4. Experience working with a range of data storage/access options (e.g. S3 buckets, FTP sites, APIs). 5. Knowledge of simplification and optimization of data automation workflows; streamlining of extract-transform-load (ETL) operations. 6. Knowledge of emerging technical skills, tools, and best practices.
Education
Additional information

How You Will Be Evaluated
You will be evaluated for this job based on how well you meet the qualifications above.
Applicants are not required to submit a cover letter. The entire cover letter cannot exceed the specified limits provided in the Cover Letter field (3,000 characters). Pages exceeding this limit will not be considered. THE COVER LETTER IS RECOMMENDED BUT IS NOT REQUIRED FOR EMPLOYMENT CONSIDERATION WITH THE NATIONAL GEOSPATIAL-INTELLIGENCE AGENCY.
APPLICANT EVALUATION PROCESS: Applicants will be evaluated for this job opportunity in three stages: 1) All applicants will be evaluated using the Mandatory Qualification Criteria, 2) Qualified applicants will then be evaluated by an expert or panel of experts using a combination of qualification criteria to determine the best-qualified candidates, 3) Best-qualified applicants may then be further evaluated through an interview process. Military retiree applicants, if selected, may be impacted by the 180-day appointment restrictions of DODI 1402.01. HD personnel will provide additional information if applicable. Applicants are encouraged to carefully review the Assignment Description, Additional Information Provided By the Selecting Official, and the Qualification Requirements; and then construct their resumes to highlight their most relevant and significant experience and education for this job opportunity. This description should include examples that detail the level and complexity of the performed work. Applicants are encouraged to provide any education information referenced in the announcement. If education is listed as a mandatory requirement, only degrees obtained from an institution accredited by an accrediting organization recognized by the Secretary, US Department of Education will be accepted. As a condition of employment at NGA, persons being considered for employment must meet NGA fitness for employment standards. In accordance with section 9902(h) of title 5, United States Code, annuitants reemployed in the Department of Defense shall receive full annuity and salary upon appointment. They shall not be eligible for retirement contributions, participation in the Thrift Savings Plan, or a supplemental or redetermined annuity for the reemployment period. Discontinued service retirement annuitants (i.e., retired under section 8336(d)(1) or 8414(b)(1)(A) of title 5, United States Code) appointed to the Department of Defense may elect to be subject to retirement provisions of the new appointment as appropriate. (See DoD Instruction 1400.25, Volume 300, at http://www.dtic.mil/whs/directives.) All candidates will be considered without regard to race, color, religion, sex, national origin, age, marital status, disability, or sexual orientation. NGA provides reasonable accommodations to applicants with disabilities. Applications will only be accepted online. If you need a reasonable accommodation for any part of the application and hiring process, please notify us at recruitment@nga.mil. The decision on granting reasonable accommodation will be on a case-by-case basis.

Background checks and security clearance
Security clearance
Sensitive Compartmented Information
Drug test required
Yes
Required Documents

Required Documents
None
If you are relying on your education to meet qualification requirements:
Education must be accredited by an accrediting institution recognized by the U.S. Department of Education in order for it to be credited towards qualifications. Therefore, provide only the attendance and/or degrees from schools accredited by accrediting institutions recognized by the U.S. Department of Education .
Failure to provide all of the required information as stated in this vacancy announcement may result in an ineligible rating or may affect the overall rating.
Benefits

Benefits
Review our benefits"
274,Biomedical Data Engineer - Health Technologies,Apple,,"Santa Clara Valley, CA 95014","Summary
Posted: Aug 21, 2019
Weekly Hours: 40
Role Number: 200092872
The Health Technologies Team conceives and proves out innovative technology for Apple’s future products and features in health.
We are seeking a highly capable Biomedical Data Engineer to join a multi-disciplinary team. Successful candidates will be able to integrate with our research study leads, data scientists, and engineers to develop and support effective data analysis and machine learning workflows.
Key Qualifications
Experience with software engineering frameworks:
Excellent coding skills in Python (e.g. Spark, Pandas, Jupyter )
Web Service APIs (e.g., AWS, REDCap, XNAT)
Designing and maintaining (non-)relational databases (e.g. Postgres, Cassandra, MongoDB)
Linux, MacOS based development frameworks
Version control frameworks (Git, virtualenv)
Experience using task scheduling system (eg: Airflow, Luigi or equivalent)
Familiarity with best practices for information security, including safe harbor privacy principles for sensitive data
Experience with biomedical sensors/platforms for measuring physiological signals (time-series data) in the health, wellness and/or fitness realms
Description
• Work closely with team members and study staff to design, build, launch and maintain systems for storing, aggregating and analyzing large amounts of data
• Process, troubleshoot, and clean incoming data from human studies
• Create ETL pipelines to automate data ingestion and transformation, with hooks for QA, auditing, redaction and compliance checks per data management specifications
• Create and populate databases with existing and incoming clinical data
• Architect data models and create tools to harmonize disparate data sources
• Incorporate and comply with regulations as they pertain to electronic and clinical data and databases
Education & Experience
BS/MS in Computer Science, Engineering, Informatics, or equivalent with relevant 4+ years industry experience with biomedical, health or sensitive data.
Additional Requirements
You will thrive in our fast-paced environment if you are highly organized and able to multitask.
Flexible thinking, adaptability to change and comfort with ambiguity are hallmarks of successful people on our team.
We look forward to witnessing your excellent communication and interpersonal skills during the interview process.
Apple is known for heterogeneous and cohesive teams. A proven ability to work seamlessly with others is required to acclimate quickly to our culture.
We highly value your analytical mind and problem-solving skills, and expect a stellar attention to detail.
You will let the customer experience guide your decision making. You will design with Apple’s culture and values, inclusion for all and privacy, as fundamental requirements."
275,Data Engineer (Medical Claims),"Leavitt Partners, LLC",,"Salt Lake City, UT 84111","Data Engineer (Medical Claims)
Leavitt Partners, LLC is seeking to hire a Data Engineer to work with Leavitt Partners Insight in our Salt Lake City, UT office. As a Data Engineer, you will be responsible to accelerate the build of our existing data warehouse, and to lead the development of our claims analysis/modeling for various new products. You will provide direction on profiling, analyzing, and building models from one of the largest sets of claims data in the nation. If you enjoy complex data challenges, building systems, analytic models, you will appreciate the work. If you are fascinated with SQL, python, and AWS, you will love our stack. If you enjoy autonomy, responsibility, and a culture of trust, you may be the perfect addition to our team.
We have a passionate and hardworking team dedicated to the vision for Torch Insight. Our team looks for high caliber candidates who are adaptable, collaborative, professional, and who thrive in fast-paced environments. We seek individual qualities such as entrepreneurial spirit, self-efficacy humility and a strong desire to learn. Compensation is commensurate with experience. Interested candidates should apply online at www.leavittpartners.com/careers.
Main Responsibilities
Build claims data analysis strategy in conjunction with the product team to understand, design, and build warehouse models to fit user stories and requirements for existing and new product
Lead analysis and manipulation of healthcare claims data via web portals for extraction
Work with other database engineers to design, build and support data warehouse components including: the database, ETL processes, reporting environment and reports.
Analyzes data for trends, insights, accuracy
Conceive, design, develop, and deploy data models from scratch
Build production systems to store, access, and use claims data-derived findings via web API
Guide and create appropriate data solutions (dashboards, reports, business intelligence tools, etc.) and data analysis for the measurement of processes and outcomes
Maintain data governance, data mining, data warehousing, data modeling and analytics
Education and Experience
Bachelor’s degree required; graduate degree a plus
2+ years of experience working with SQL, python or a statistical programming package
2+ years of analyst/modeling experience
Experience working with healthcare claims

Skills and Competencies
Subject matter expertise in a wide variety of health care data assets
Solid knowledge of designing, maintaining, and enhancing data warehousing systems
Experience developing ETL processes, data cleansing, creation of metadata and use of various reporting tools
Ability to work as a self-starter in an ambiguous environment
Excellent verbal and written communications skills
High collaborative IQ and ability to work as a team player

About Us
We are a SaaS healthcare data and analytics company founded and incubated at Leavitt Partners. We've spent years splicing siloed data sets together, investing thousands of hours cleaning and validating data in preparation for analysis, and integration with business intelligence platforms. We excel at blending population health data with quality, financial, and performance metrics from stakeholders across key industries. Now we offer our raw data and curated insights to you.

We provide access to our cloud-based database and key analytic tools for strategists thirsting for intelligence, and for marketing teams hungry to expand. We specialize in understanding ACOs, value-based care, and how markets are transforming. Built with 21st century technology, we bring together the consulting expertise of Leavitt Partners with cloud technology, to turn healthcare data into actionable insights."
276,Data Engineer Intern,Internship,,"Manhattan Beach, CA 90266","Job Description

Are you excited about high performance data implementation? Then great! the Skechers USA Data Engineering team is growing, and we need Data Engineering Interns who are thorough and agile, willing to learn and adapt and capable of breaking down and solving problems. In the Data Engineering team, you will work on real-world problems working on OnPrem or multi cloud tech stack where reliability, accuracy and speed are paramount, take responsibility for your systems end-to-end and influence the direction of our technology that impacts customers around the world.

We are looking for an Intern Data Engineer with an understanding of structured/semi structured/Complex data processing and streaming frameworks; RDBMS and NoSQL data stores. As a member of our Data Services team you will be mentored by peers who are responsible for continuing organizational expansion of our data processing projects.

ESSENTIAL JOB RESULTS
The successful candidate will be both self-motivated and collaborative as is necessary to operate in the realm of enterprise technology projects, interacting with team members and vendors to resolve issues quickly.
Team collaboration by building existing knowledge base in Confluence and creating LucidChart flows.Involved in the development of ETL/ELT to deliver data for reporting, troubleshoot and resolve production data integrity and performance issues.Develop SQL scripts work with script languages (Python, shell etc.) to wrap applications execution.Support various Data Platform Applications and Data Warehouse Applications

JOB REQUIREMENTSUnderstanding of NoSQL data stores with experience in working with SQL, script languages (Python, shell etc.)Understanding of AWS managed services for data ingestion, processing with hands on experience working in AWS environment, and have a strong knowledge of S3, Glue and Athena.Writing tech specs and documentationLinux KSH/bash scriptingUnderstanding of how an ETL/ELT toolsets works.Experience with any of the following message / file formats: Parquet, AvroExpertise in Python, pySpark or similar programming languages

EDUCATION AND EXPERIENCE
Bachelor of Science in Computer Science or equivalent experience.
The preferred candidate will have 2+ years experience as an Engineer

Qualifications

null

Additional Information

PHYSICAL DEMANDS
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
All your information will be kept confidential according to EEO guidelines."
277,Big Data Engineer,Verizon,,"Alpharetta, GA 30022","What you’ll be doing...
The position performs a dual role, of Technical Lead and Subject Matter Expert as focal point and Key Team Member to deliver multiple Data Ware House Projects.
You’ll be responsible for:
Architecture and Design: As senior developer for EDW handling Unified Data Enablement Architecture delivering the integrated foundation for EDW/Big Data and Advanced Technical solutions. Architecture involves expansion of data flows, design of data models, and decision on the right development tools. As an design example of Analytic solutions to resolve complex business problems of VZ 2.0 Consumer (VZW+VCM) Campaign Strategy for the New Customer with competitive accusation offer to find out the most effectiveness of the channel to awake up the sleeper cell of the customer segment, while analyzing/designing interacting with third party vender and Verizon Internal Channel for approx. 100M prospect.
Development and performance tuning: Consolidated Verizon EDW - Implement best practices in EDW and Big Data software development, especially in the areas of ETL and data integration. Drive efficiencies across lines of business by performance tuning the application from design decisions and development methods. As an example efficient way to build Tooling capability to increase productivity of other developer for any repetitive initiative.
Coaching and technology advancement: Maintain expertise in advanced technologies, including real-time, active EDW, Big Data (new data sources), Hadoop, No SQL databases and improved interfaces to BI and Data Science tools and applications. Provide coaching, mentoring and knowledge transfer to other team members. This responsibility involves constant learning of the tools, presentation to external and and internal teams and coaching.
Improve system efficiency in ETL: Provides critical ETL SME support for ETL Data/Tool/Teradata – Development, Design and Best Practice. This responsibility uses the advanced technical skills in the ETL tools to develop efficient systems and review and enhance implementation from junior peer developers.
What we’re looking for...
You'll need to have:
Bachelor’s degree or four or more years of work experience.
Four or more years of relevant work experience.
Experience in of Big Data ETL Tools.
Even better if you have:
Bachelor’s degree in Computer Science or in a related filed.
Knowledge of Spark.
Five or more years of Teradata Utilities and Unix Shell Scripting.
When you join Verizon...
You’ll be doing work that matters alongside other talented people, transforming the way people, businesses and things connect with each other. Beyond powering America’s fastest and most reliable network, we’re leading the way in broadband, cloud and security solutions, Internet of Things and innovating in areas such as, video entertainment. Of course, we will offer you great pay and benefits, but we’re about more than that. Verizon is a place where you can craft your own path to greatness. Whether you think in code, words, pictures or numbers, find your future at Verizon.
Equal Employment Opportunity
We're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better."
278,Data Engineer II,Oath Inc,,"Sunnyvale, CA","It takes powerful technology to connect our brands and partners with an audience of 1 billion. Nearly half of Verizon Media employees are building the code and platforms that help us achieve that. Whether you’re looking to write mobile app code, engineer the servers behind our massive ad tech stacks, or develop algorithms to help us process 4 trillion data points a day, what you do here will have a huge impact on our business—and the world. Want in? As Verizon’s media unit, our brands like Yahoo, TechCrunch and HuffPost help people stay informed and entertained, communicate and transact, while creating new ways for advertisers and partners to connect. With technologies like XR, AI, machine-learning, and 5G, we’re transforming media for tomorrow, too. We're creators and coders, dreamers and doers creating what's next in content, advertising and technology.
A Little About Us
Verizon Media has the most trafficked destinations on internet, including Yahoo, Aol, Huffington Post, TechCrunch, and many other well known brands. The Oath Advertising team is responsible for delivering market-leading advertising and audience products, solutions, and services. Our advertising products deliver billions of ad impressions to hundreds of millions of users everyday, enabling hundreds of thousands of advertisers to effectively connect with the right audience at the right time across devices and across the globe. Our audience analytics products analyze multi-terabytes of data everyday and deliver insight on the preference, intent, interest, and behavior of users to help Yahoo and its customers drive user growth, engagement and value.
A Lot About You
You are an exceptional data engineer with rich experience in big data processing, excellent analytical skills, sound knowledge about statistics and experimental design, and proven ability to communicate analytical findings. You are always curious, self-motivated, and eager to learn learning new domain knowledge and analytics tools. You are an out-of-the-box thinker, a believer in data driven approaches, and a quick learner. You enjoy to work with a strong, cross-functional, and sometimes cross-geography team of engineers and data scientists, and you are passionate in working with our business and product teams to turn data into actionable insights. You always think positive, have the can-do attitude, and focus on “getting stuff done” with quality. People like to work with you because you’re a valuable contributor, and a responsible team player.
Your Day
• Investigate and analyze different types of creatives, their coding patterns, and potential anomalies
• Build offline models for creative fraud score, with considerations of features from creatives, advertisers, delivery data, and many other external signals
• Work with engineers to build automated pipelines to score creative risk in real time, and conduct online testing
• Work closely with engineers, data scientists, product managers and ad operations to track, monitor, and report on ad platform health, in term of potential creative fraud
• Provide insights to product managers and engineers on what data gaps should be addressed for improving fraud detection
Requirements
• Advanced degree in Statistics, Applied Mathematics, Computer Science, or a related field
• Proficient programming skills in at least one of the following languages: Python, R, and Java, and familiar with Hadoop and SQL
• Excel in statistical thinking while deeply grounded in solid business judgment
• Results driven, great attention to detail, and a team player
• Excellent communication and presentation skills
• Skilled in interpersonal communication and relationship. Ability to clearly explain analytical findings to business partners, researchers and engineers
Verizon Media is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Verizon Media is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. If you need accessibility assistance and/or a reasonable accommodation due to a disability, please email ApplicantAccommodation@verizonmedia.com or call 408-336-1409. Emails/calls received for non-disability related issues, such as following up on an application, will not receive a response.
Currently work for Verizon Media? Please apply on our internal career site."
279,Data Engineer,Social Interest Solutions,,"Oakland, CA 94612","What We Do:
Social Interest Solutions is a non-profit on a bold mission: to remove the barriers between people and the services and support that matter most. We are policy experts and technologists, determined leaders and compassionate innovators, united by our desire to do whatever it takes to solve complex problems in access to health and social services. We create technology solutions that put people first.
How We Work:
Social Interest Solutions employees dig deep into how our clients work and the challenges faced by the people they serve, so we can streamline complicated processes and design smart solutions that work better for everyone. At SIS, every employee has a role in ensuring that nobody falls through the cracks. We put people and their unique needs at the center of what we do. Each client we work with is different, and each community they aim to serve unique, so our technology must flex to each opportunity.
Position Description:
We are looking for a Data Engineer who can help create a new model of data architecture to support business units with better access to data, drive the design and use of a new data warehouse and build automated ETL/ELT processes to connect data across the organization. We’re building a data practice from the ground up, you’ll get to shape this work and help drive our data-informed culture.
Essential Functions & Summary of Responsibilities:
Implement and oversee the Data Warehousing (possibly Snowflake) and ETL infrastructureDefine and establish processes to maintain the integrity of data within our data pipeline and warehouseEnsure quality of data and completeness of event logging across enterprise codebaseIntegrate data from 3rd party services such as SalesForce, New Relic, Splunk.Develop ETL jobs and tests to process, validate, transport, collate, aggregate, and distribute dataTransform raw event logs into higher-order tables to make existing analysis easier and new analysis possibleBe part of a new team championing a data-informed cultureBuild workflows that empower analysts and scientists to efficiently use data in Tableau and open source data visualization toolsOther duties and tasks as assigned

Desired Qualifications (Knowledge, Skills, Abilities):
To succeed in this role, you will need to possess the following minimum characteristics:
Expertise building data pipelines with streaming and batch ETL patterns on complex datasets using Python/Docker/SQLAbility to architect the entire data ecosystem with flows and dependencies across numerous internal systems and new cloud applicationsSoftware Engineering experience with expertise in at least one high-level programming language (preferably Python)Strong SQL writing skillsExperience with orchestration tools such as Airflow (preferred) or LuigiExceptional communication skills and desire to share your knowledge with clarity, patience, and empathyThe ability to work independently and set your own priorities with careful attention to detail and deadlinesFamiliarity with Data Visualization tools such as Tableau, Looker, SupersetFamiliarity with data warehousing; Specific familiarity with Azure data warehousing infrastructure a plus

Education/Training:
Bachelor’s Degree (B.A. /B.S.) in Computer Science, Engineering or equivalent experience in a related field preferred

Work Environment and Conditions:
Physical requirements: The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential duties.

While performing the duties of this job, the employee is regularly required sit, stand, talk and hear, use hands to finger, handle, or feel objects, tools, or controls; repetitive motion; reach with hands and arms. The position is primarily sedentary work. The employee may occasionally lift and/or move up to 10 pounds. The worker is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading; Other vision abilities required by this job include close vision, distance vision, peripheral vision, and the ability to adjust focus.

Work environment: The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is working in an office environment. The noise level in the work environment is usually moderate.

SIS is an Equal Opportunity Employer. In keeping with our beliefs and goals, no employee or applicant will face discrimination/harassment based on: race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status."
280,Data Engineer II - AMZ3401,"A9.com, Inc.",,"Palo Alto, CA","Position Requirements:
Bachelor's degree or foreign equivalent in Computer Science, Engineering, Statistics, Mathematics or a related field and two years of experience in the job offered, or as a Data Scientist, Database Developer, Technical Data Analyst, or a related occupation. Must have two years of experience in the following skill(s): developing and operating large-scale data structures for business intelligence analytics using each of the following: ETL/ELT processes; data modeling; and SQL.

MULTIPLE POSITIONS AVAILABLE
Entity: A9.com, Inc., an Amazon.com Company
Title: Data Engineer II
Worksite: Palo Alto, CA

Position Responsibilities:
Design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for business intelligence analytics. Implement data structures using best practices in data modeling, ETL/ELT processes, SQL, Hadoop/Hive, and AWS technologies. Provide on-line reporting and analysis using business intelligence tools such as Tableau and QuickSight and a logical abstraction layer against large, multidimensional datasets and multiple sources. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture. Analyze source data systems and drive best practices in source teams. Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance. Produce comprehensive, usable dataset documentation and metadata. Evaluate and make decisions around dataset implementations designed and proposed by peer data engineers. Evaluate and make decisions around the use of new or existing software products and tools. Mentor junior data engineers.

Applicants must meet all of the above listed requirements for this position."
281,Data Engineer,"Clover Network, Inc.",,"Sunnyvale, CA","About Clover:
Join the Fintech and SaaS revolution that is helping small businesses grow faster and get the technology and insights previously available only to the ""big guys."" At Clover, you will be part of an entrepreneurial team working in a fast-paced and high growth environment. Clover delivers the leading Point of Sale system with an elegant end-to-end solution that incorporates beautiful devices, cloud-based POS software, payments processing, platform API's for third-party developers, and an ecosystem with over 220 apps. The Clover platform delivers solutions in a scalable and modular fashion that powers tiny merchants through large football stadiums, supporting millions of transactions daily.

About the role:
Do you love data? Does the thought of parsing through a new data feed or optimizing a SQL query captivate you? If so, then the data team is looking for you! The right candidate is a passionate, quality-centric, data maven who enjoys harvesting and curating data for analysts and data scientists. You go beyond just ""going through the motions"" and really care about where data comes from, how it will be used, and the best way to surface it for use. You are a highly skilled, experienced, professional with excellent scripting, coding, SQL, and problem solving abilities. You dive head first into challenging and complex problems, because you love solving puzzles. You enjoy building tools for non-technical audiences to help them self-serve their data needs. If this sounds like you and you are interested in joining a team of passionate, high-performing data professionals, then read the details below and apply today!

Responsibilities:

Design, develop, own, and maintain ETL/ELT data flows across a constellation of data sources and systems
Aggregate and store quality data in an efficient and transparent manner for reporting, analytics, and data science uses
Implement tools for monitoring and ensuring data quality and consistency
Build, support, and improve custom tools necessary for data and analytics self-serve initiatives
Work closely with Security and Operations teams to develop and enforce proper data security and privacy practices
Work cross-functionally and communicate in an effective manner
Investigate, advocate for, and proactively obtain new data sets

Requirements:

Proven experience creating and maintaining fault-tolerant data pipelines using relational, non-relational, and cloud-based data warehouse systems
Data modeling and data architecture experience
Initiate and drive projects to completion with minimal guidance in a fast-paced, dynamic environment
Detail-oriented, inquisitive by nature, with a can-do attitude and a passion for quality results and an interest in learning new technologies
Strong coding skills and working experience with SQL, Python, and Java

Nice to have:

Familiarity with content version control, code reviews, and agile development methodology
Experience with technologies such as Kafka, Docker, Kubernetes, and Spark
Experience with Linux

Clover is an equal opportunity employer and we value diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
282,Finance Data Engineer,"Rodan and Fields, LLC",,"San Ramon, CA","The Opportunity
This role has significant cross functional responsibility in the Finance System team driving the resolution of technical issues in business workflows of financial transaction processing.
Responsibilities:
Follow SQL best practices in the development of stored procedures, tables, views, and jobs used to create and maintain medium to complex data structures.
Use indexes, common table expressions, temp tables to optimize large SQL queries
Design, develop, test, optimize, and deploy SQL Server Integration Services (SSIS) packages to perform all data integration requirements. Ability to work independently and hit the ground running with SSIS development. Previous experience with code repositories and deployment pipelines required.
Gather business requirements from users
Resolving issues raised by business users and finance management. Production support as needed.
Ensuring system integrity across financial systems and perform reconciliations
Maintain and ensure relevant system and process documentation is kept current.
Coordinating and performing user acceptance testing.
Familiarity with report development in tools like MicroStrategy and /or SSRS
Qualifications:
Bachelor's Degree in Technology or Finance related fields
2 Years SQL/SSIS Developer experience required
Knowledge of business intelligence practices, Skills in data warehousing and data modeling techniques in SQL Server, Skilled in the use of Extract, Transformation, and Load (ETL) tools (SSIS), Knowledge of data warehouse concepts and principles.
Ability to code in Python a definite plus
Proven track record of delivery and problem solving skills
Ability to demonstrate attention to detail and work to tight deadlines where required
Excellent verbal and written communication skills and proven experience of building and maintaining relationships with key stakeholders
Effective and pro-active in identifying investigating and resolving issues

The Company
Rodan + Fields was founded in 2000 by Stanford-trained Dermatologists Dr. Katie Rodan and Dr. Kathy Fields with a passion for giving people the best skin of their lives — and the confidence that comes with it. Today, Rodan + Fields is the #1 Skincare Brand in North America[1] in 2018 and the #1 Skincare Brand in the US in 2016, 2017 and 2018[2]! The company has grown its innovative line of products and expanded into Canada and Australia. Headquartered in San Francisco, CA, R+F now employs approximately 700 people, has more than 300,000 enrolled Independent Consultants and over two million Preferred Customers.
At Rodan + Fields, you will be challenged to make an impact, inspired to do more, and rewarded for your contributions. We are transforming skincare, and we welcome your big ideas to fuel our ambitious growth plans! If you are looking for a life-changing career opportunity, we've got your prescription. You’ll become part of a positive, passionate movement that celebrates greatness and encourages employees to be catalysts for change. We provide a creative, vibrant workplace outfitted with all of the technology, tools and training you'll need to learn, grow and thrive! We create life-changing impact in our communities through our non-profit, Prescription for Change®, the heart of Rodan + Fields. By funding empowerment programs for students, we teach them how to use their skills to make life-changing differences in their lives and the lives of others. Join us and share your talents as we develop innovative solutions for your skin and empower entrepreneurs. In addition to working arm-in-arm with industry leaders, employees at Rodan + Fields enjoy rich benefits plans and perks.

[1] Source Euromonitor International Limited; Beauty and Personal Care 2019 Edition, retail value RSP terms; all channels, Skin Care includes Sets and Kits; North America defined as Canada and the United States
[2] Source Euromonitor International Limited; Beauty and Personal Care 2019 Edition, retail value RSP terms; all channels, Skincare includes Sets & Kits."
283,Data Engineer,IdentityMind,,"Palo Alto, CA 94301","IdentityMind is the Trusted Digital Identity (TDIs) company. We offer a SaaS Platform that builds, maintains and analyzes digital identities worldwide. Our Platform allows companies to perform identity proofing, risk-based authentication, regulatory identification, and ultimate detect and prevent synthetic identities, and stolen identities. We extend our Platform with transaction monitoring for e-commerce fraud prevention, anti-money laundering, and the counter financing of terrorism (CFT). We are expanding internationally and are also building Big Data analytics and reporting tools to cope with the financial crime prevention requirements of our clients.

IdentityMind has a diverse workforce and strongly believes in a non-political, just get the job done work environment. We are a highly distributed organization with offices in Palo Alto, Spokane and Mexico City, plus remote-work employees in another 5 states and 4 countries.

This is a highly visible role within a small team reporting directly to the CTO. If you are a world-class data engineer wanting your work to have an outsized impact, we want to hear from you and explore how you can make our product better and even more successful.


Key Qualifications
Computer Science degree or another highly quantitative degree such as engineering, physics, or mathematics
2+ years professional experience implementing ETL, and real time data pipelines
Familiarity with cloud computing (AWS or equivalent) and big data technologies (Kafka, NoSQL databases, Spark)
Strong Python and SQL skills
A software engineering mindset
Documentation
Source control and release cycles
Repeatability and sharing
Ability to work within a small high achieving team, as well as independently
Self-driven, highly motivated, innovative
Strong communication skills, both written and verbal



Desirable
Cassandra
Graph Databases such as Janus or Neo4J
Java
Linux
Machine learning tools
Position Based in Palo Alto; CA, Spokane, WA; or anywhere in the US.

Benefits include:
Medical, dental, vision
401k Plan"
284,Data Engineer,StrategyWise,,"Birmingham, AL 35233","Position Summary:

The Data Engineer is responsible for the maintenance, improvement, cleaning and manipulation of internal and external data. The Data Engineer will also play a key role in expanding and optimizing our data architecture, flow and collection on big data platforms.

Responsibilities:

Prepare raw data for manipulation by Data Scientists and Analysts and ensure optimal and consistent data delivery architecture throughout projects
Build infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources on Azure using related “big data” technologies
Create databases optimized for performance, implement schema changes and maintain data architecture standards
Develop and implement scripts for database maintenance, monitoring and performance tuning
Perform thorough testing and validation in order to support the accuracy of data transformations and data verification to use in machine learning models
Research industry trends and best practices; advise senior management on new and improved data engineering strategies that will drive departmental performance
Detect and correct errors in work, monitor and troubleshoot operational or data issues in the data pipelines
Ensure work remains backed up and readily accessible to coworkers
Required Skills / Experience:

2+ years’ experience in data wrangling and data pipeline building, working with cross-functional teams in a dynamic environment
Comfortable supporting the data needs of multiple teams, systems and products
Working SQL knowledge and experience working with relational databases, performing root cause analyses to answer specific business questions
Experience working with web APIs and their related file formats (json, csv, parquet, etc)
Hands on experience with any of the following public clouds (Azure, AWS, or GCP)
Experience working on both Linux and Windows servers and systems.
Bachelor’s degree in Computer Science or related field
Strong conceptual knowledge of Power BI, Tableau, or other BI dashboarding products
Strong ability to effectively communicate with both business and technical teams
Hands on experience coding and working within the Python language
Experience working with code version-control systems like Git
Preferred Qualifications:

Proven real-world experience working on data engineering projects.
Passion for about continued education in Data Engineer or Comp Sci space.
Experience with any of the following tech products considered a plus; MS SQL, Hadoop, Docker, Kubernetes"
285,Data Engineer - San Francisco,Keyrus,,"San Francisco, CA","WHO WE ARE:

Keyrus Group is a trusted leader in Data Intelligence with offices across the globe in 17 countries. Our expert team provides strategic data engineering, data discovery, and data science solutions for our clients, primarily in the retail, non-profit, and financial services industries.

The Keyrus US team is currently expanding rapidly! Our team has doubled in the last two years and is continuing to grow! If you are looking to join an innovative startup-style company that has the support of an internationally recognized brand, we encourage you to apply to our Data Strategy Consultant position and join our Management & Transformation practice!

THE ROLE:
Keyrus is hiring an analytical minded and highly technical data engineer to join us in San Francisco! As a data engineer, you will take ownership of building critical data pipelines and modern data warehousing solutions that will be deployed on the cloud. Using some of the industry’s most bleeding-edge data technologies, you will work directly with business stakeholders to help define architecture and take part in the entire development lifecycle of our client’s projects.
You will be an integral part of a team here at Keyrus that pushes the adoption of cloud and big data technologies. We are a language agnostic shop and are excited by the idea of working with new technologies that can deliver innovative solutions for our clients!
Responsibilities:
Design and develop scalable data pipelines for ingestion, transformation, and computation using big data technologies
Building and testing data ingestion and ETL programs from both structured and unstructured sources
Work with modern data storage, processing, and messaging tools and help continually grow and improve client’s data infrastructure
Requirements:
At least 2+ years of experience working as a Java, Python, C++ developer OR data engineer
At least 1+ years of experience working directly with big data technologies including:
Hadoop, Spark, Hive, Kafka, Scala, Big Query, Redshift, Cassandra etc.
Strong background working with cloud platforms such as AWS, GCP, Azure
Minimum of a Bachelor’s degree in Computer Science or related engineering field
Please send an updated version of your resume to recruitment@keyrus.us."
286,Data Engineer,Blueprint Technologies,,"Bellevue, WA","Who is Blueprint?

Blueprint Technologies is a group of solution minded thinkers changing the face of Technology in Bellevue, WA. We follow a Mission, Vision, and Core Values that allow us to function as a collaborative unit.

What are our Solutions?

Blueprint is a technology solutions firm that connects strategy, product and delivery. We help companies digitally transform. We have a special focus in cloud and infrastructure, data platform and engineering, data science and analytics, organizational modernization and customer experience optimization.

Why you want to be a part of Blueprint?

We are innovators. Motivators. Thought provokers. And coffee drinkers. Our collective backgrounds bring diverse perspectives that enable us to consistently think differently. Our people are our solutions. We want you to bring your biggest and best ideas to help positively impact our culture, clients and the community around us. We believe in the importance of a healthy and happy team, which is why our benefits include full medical, dental and vision coverage, as well as paid time off, 401k, paid volunteer hours and tuition reimbursement.

Blueprint is looking for Data Engineer to join us as we build cutting-edge technology solutions!

Qualifications:
At least 5-years of experience as a software development or data engineer

At least 3-years of experience with SQL, Python and/or other data collection tools & reporting

Experience with Pyspark or Scala is necessity (Databricks or Spark).

Advanced knowledge and skills with Azure, or similar cloud platforms.

Excellent collaboration skills to work on a team as well as independently (be self-reliant and resourceful)

Excellent organization skills and able to multi-task and detailed oriented

Excellent verbal and written communication skills (must be able to write clear and concise emails for any audience, etc.

Nice to have

Experience working with Business Intelligence BI tools such as PowerBI

Bachelor's Degree

**We are not able to sponsor Visa's at this time or do Corp-to-Corp arrangements. Must be able work on a W2 basis please!"
287,Data Engineer,Texas State Technical College,,"Abilene, TX","About TSTC :
TSTC is the only state-supported technical college system in Texas. TSTC’s statewide role and mission is to efficiently and effectively help Texas meet the high-tech challenges of today’s global economy, in partnership with business and industry, government agencies, and other educational institutions. If this is a mission you could support and help TSTC achieve, please complete our application process.
Position Overview :
This role focuses on designing, reconstructing and maintaining relationship structures between data sets. The Data Engineer will ensure that data models are clean and precisely constructed as well as manage data integrity and accuracy. This role requires energy, ability to solve complex problems, and comfort working in a fast-paced, self-initiated environment.
Essential Function :
Design, organize and implement opportunities to optimize performance within the division through the use of a variety of data tools and resources.
Find anomalies in complex data sets and prescribe corrections.
Build the structure for data, maintain architecture of data structures and manage data integrity.
Knowledge and application of analytical tools and methods, mining techniques; ability to construct and analyze data sets to answer business questions.
Create and maintain data structure models and cleanse data efficiently
Apply reconciliations of various data sets to ensure accuracy of data elements and files utilized within the division for various reporting.
Support ad-hoc projects, analysis and other duties as assigned.
Minimum Qualifications :
Bachelor’s degree in relevant field
3-5 Years of experience in a data intensive environment.
Proficient experience in SQL and Microsoft Excel, Access required
Familiarity with data visualization tools (e.g. Tableau, SAP) at least one required
Experience with complex database architecture
Must possess good problem-solving skills.
Be able to practice organizational techniques
Ability to perform effectively with minimum supervision, exercising independent judgment and decision-making abilities.
Because of the sensitive nature of the job, complete confidentiality is required.
Maintain a positive and respectful attitude.
Demonstrate a sense of urgency and be flexible and efficient time management with the ability to work under stressful.
Provide exceptional customer service when communicating with the public and with employees at all levels in the organization.
Demonstrate a sense of urgency and be flexible and efficient time management with the ability to work under stressful.
Application Requirements:

Your application must be submitted with the following documents attached
College/University Transcript
Resume/ Curriculum Vitae (CV)
Cover Letter
 Salary : Dependent on qualifications
Additional Comments:
Security Sensitive Position
Appropriate background check and drug screening.
Department: Business Analytics & Reporting
Location(s): Abilene; Austin; Breckenridge; Brownwood; Harlingen; Hutto; Marshall; Red Oak; Rosenberg (Fort Bend); Statewide; Sweetwater; Waco
All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal law.
Requisition ID: 35779"
288,"Data Engineer, Data Acquisition",Walmart,,"Hoboken, NJ 07030","Position Description
As part of the newly created Data Strategy and Enablement Team (DS&E), this role will be an enabler of our journey to be the world’s leading data-driven retailer. As part of this transformation, we are seeking an individual who will be responsible for establish robust data pipelines and services – including both in house developed data enabling services and systems integrations across the DS&E team to ensure we our technical deliverables meet and exceed the quality expectations.

We are looking for a highly motivated, resourceful, team-oriented individual to drive the data engineering process. You are exceptionally talented Data engineer with an outstanding track record of working with very large data sets and building robust ETL pipelines for data acquisition for internal systems and external data sources. You will be modernizing and improving the data acquisition infrastructure from the ground up. You will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.

The Data Engineer role will report up to the Lead Data Engineer/Senior Manager Data Engineering.
Minimum Qualifications
Play a pivotal design and hands on implementation role in improving the Data infrastructure in a project-oriented work environment.Influence cross functional architecture in sprint planningGather and process raw data at scale from internal and external data sources and expose mechanisms for large scale parallel processingDesign, implement and manage a near real-time ingestion pipeline into a data warehouse and Hadoop data lake.Process unstructured data into a form suitable for analysis and then empower state-of-the-art analysis for analysts, scientists, and APIsSolve complex SQL and Big Data Performance challenges.Mitigate Risks in our data infrastructure by developing the best in class tools and processes.Implement controls, policies, processes and best practices in the Data Engineering space.Evangelize an extremely high standard of code quality, system reliability, and performance.Help us improve our database deployment and change management process.Provide reliable and efficient Data services as part of the global data team.Work closely with the team on development best practices and standards.Be a mentor.
Who you are:
You have prior experience with leading data engineering efforts across a variety of data systemsYou have deep understanding of commercial data sources and understand database concepts and terminologyYou have a demonstrated track record of handling multiple complex sourcing projects and delivering results in the data engineering areaYou have strong SQL experience and the ability to work on multiple aspects of a data projects including ETL, tools integrations, data results and APIs.You are a team player, with the courage to drive change through disruption while maintaining a respect for the team
Requirements:
Very Strong engineering skills. Should have an analytical approach and have good programming skills.Provide business insights, while leveraging internal tools and systems, databases and industry dataMinimum of 5+ years’ experience. Experience in retail business will be a plus.Excellent written and verbal communication skills for varied audiences on engineering subject matterAbility to document requirements, data lineage, subject matter in both business and technical terminology.Guide and learn from other team members.Demonstrated ability to transform business requirements to code, specific analytical reports and toolsThis role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering teamExperience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.) and platforms such as HDP, Cloudera etc.Strong Hadoop scripting skills to process petabytes of dataExperience in Unix/Linux shell scripting or similar programming/scripting knowledgeReal time data ingestion (Kafka)Experience in ETL/ processes with exposure to one or more tools such as Nifi, Talend, Informatica, SSIS etc.
Additional Preferred Qualifications

Company Summary
The Walmart eCommerce team is rapidly innovating to evolve and define the future state of shopping. As the world’s largest retailer, we are on a mission to help people save money and live better. With the help of some of the brightest minds in technology, merchandising, marketing, supply chain, talent and more, we are reimagining the intersection of digital and physical shopping to help achieve that mission.
Position Summary
As part of the newly created Data Strategy and Enablement Team (DS&E), this role will be an enabler of our journey to be the world’s leading data-driven retailer. As part of this transformation, we are seeking an individual who will be responsible for establish robust data pipelines and services – including both in house developed data enabling services and systems integrations across the DS&E team to ensure we our technical deliverables meet and exceed the quality expectations.

We are looking for a highly motivated, resourceful, team-oriented individual to drive the data engineering process. You are exceptionally talented Data engineer with an outstanding track record of working with very large data sets and building robust ETL pipelines for data acquisition for internal systems and external data sources. You will be modernizing and improving the data acquisition infrastructure from the ground up. You will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.

The Data Engineer role will report up to the Lead Data Engineer/Senior Manager Data Engineering."
289,Master Data Engineer (Machine Learning Integrations),Capital One - US,,"McLean, VA","McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Master Data Engineer (Machine Learning Integrations)

Job Description
Card ML (Card Machine Learning) is focused to set the standard for data quality to power real-time, automated intelligence, centered on a longitudinal, human-centric view. Lead an enterprise alliance to deliver a breakthrough ML platform. World class thought leadership on NLP (Natural Language Processing), voice science, proactive engagement, active influence, fairness and explainability in AI and more!

Responsibilities & Expectations:
Lead a team of software and data engineers to apply lean development practices to develop software products that help us build and run machine learning applications.
Design and implement application frameworks, APIs, libraries and services that perform at scale using existing and emerging technology platforms and standards like Kubernetes, Docker, Kafka, gRPC.
Lead design and development of automation workflows, set and expect high standards for code reviews to make sure the software is rigorously designed, elegantly coded, and effectively tuned for platform performance, and assess the overall quality of delivered components.
Work as a partner with product owners to prioritize features. Work with other tech teams to integrate solutions across the organizations. Lead the tech team to make decisions for the program.
Demonstrate strong verbal and written communication skills, to address the dynamic nature of collaboration with customers, vendors, and other engineering teams.

We’re looking for self-starters, those who can work independently, with, and across, teams.

About the candidate:
Ability to research, select and propose the right technology or tool for the task
Influencing organization’s leaders to help resolve critical architecture and data challenges
Is a problem solver with a curiosity about technology. Highly creative and curious technologist, one with excellent research skills
Is a leader that team members can lean on
Continues to learn through iterative delivery
Is not intimidated by challenges; the candidate will be expected to research and develop cutting-edge solutions
Values continuous integration and deployment. Knows how to automate processes to quickly generate value for our customers
Loves learning new technologies and mentoring other engineers

Basic Qualifications:
Bachelor’s Degree
At least 6 years of combined experience using any combination of the following programming languages: Java, Python, Golang, Scala or JavaScript
At least 3 years of experience developing applications using Agile principles
At least 1 year of experience in developing software products for cloud platforms

Preferred Qualifications:
Master’s Degree in STEM
10+ years of combined experience using any combination of the following programming languages: Java, Python, Golang, Scala or JavaScript
2+ years of experience in developing software products for cloud platforms like AWS, Azure or Google Cloud Platform
2+ years of experience with containers (e.g.: Docker)
2+ years of experience in CI/CD - DevOps practices
1+ years of experience using streaming data platforms like Kafka, RabbitMQ
1+ years of experience as a tech lead

At this time, Capital One will NOT sponsor a new applicant for employment authorization for this position."
