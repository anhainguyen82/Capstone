,Title,Location,City,State,Zip,Country,Qualifications,Skills,Responsibilities,Education,Requirement,FullDescriptions
0,Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"CirrusMD helps health plans create happier, healthier, and more engaged members by giving them access to on-demand virtual care solutions that they love to engage with. Our chat-powered care delivery platform connects members to a dedicated, board-certified physician in under 90 seconds from any web-enabled device, with no cost and no time limits attached. CirrusMD enables stress-free, human care conversation that doesn’t end until members get the answers (and peace of mind) they need to manage their wellness.
CirrusMD has partnered with over a dozen major national payers and healthcare systems to deliver extraordinary virtual care to millions of lives across the nation. The company was founded in 2012 and is headquartered in Denver, CO.
Why Work Here?
Join our team and help us deliver Care Without Barriers. Our company offers significant opportunity for motivated self-starters who thrive in a fast-paced environment that is quickly transitioning from a startup to a highly recognized healthcare industry disruptor. We offer an exceptional benefits package including health, dental and vision, 401k savings, flexible vacation and working policies, competitive salaries and stock options and an EcoPass. CirrusMD is located in the Catalyst HTI building in Denver’s RiNo neighborhood, a newly built office space, with access to open-air shared workspaces and community areas, and a highly engaged community of healthcare and tech innovation leaders. Subsidized parking, on-site gym and shower facilities are also available to our team. Join us and see for yourself!
Who We’re Looking For:
An experienced engineer who can confidently contribute to our mission of redefining the healthcare experience for patients and providers. You are comfortable working independently as well as pairing with other engineer(s). You are comfortable breaking larger tasks down into an executable action plan for yourself, and perhaps others. We seek a motivated, self-starter who knows how to buckle-down and deliver; yet knows when to push back if the demand is too high or the directed path unclear.
What You’ll Be Responsible For Achieving:
Building and optimizing data pipelines, architectures and data sets
Automating and optimizing ingestion of outside data from customers
A reputation for reliably delivering well considered, well tested and performant code
Contributions to our evolving development and testing standards and best practices
High collaboration with Analytics + Product
What Will Make You Successful:
Advanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.
Advanced programming skills in Python, Java, and/or Go (Golang)
Experience designing and managing complex relational database structures
Proficient with AWS especially S3, EC2, RDS (Postgres), Redshift, Athena (Presto) or EMR (Spark)
Your philosophy aligns with Agile methodologies and processes
Confidence pairing with other engineers of all levels
What Will Make You Stand Out:
Significant experience with data warehouses like Snowflake and ETL tools like Fivetran
Experience with 1 or more additional backend core technologies (Scala / Rust / Ruby / Elixir)
A reputation for superb communication skills with other engineers and teammates
You have a reputation for a high level of craftsmanship about your work
Strong analytic skills related to working with structured and unstructured datasets
Backend experience designing REST & gRPC APIs

To Apply:
Please submit resume including your salary requirements as PDF to EngineeringJobs@CirrusMD.com indicating ""Data Engineer"" in the subject line.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Notice to recruiters and placement agencies:
If you are a recruiter or placement agency, please do not submit résumés to any person or email address at CirrusMD prior to having a signed agreement with Human Resources. CirrusMD is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Also, any résumés sent to us without an agreement in place will be considered your company's gift to CirrusMD and may be forwarded to our Talent Acquisition team."
1,Sr. Data Engineer,"Boulder, CO",Boulder,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and seek to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun and most importantly to each other’s success. Learn more about Splunk careers and how you can become a part of our journey!
Splunk is located in Boulder, CO. This team is made up of a strong mix of very talented and driven individuals, with a proven senior leadership team with a strong technical background.
The office layout is open and the floor plan helps to foster the casual, collaborative environment that has played a key role in the company’s innovation of its product. In addition, you will find everything you would expect in the office of a progressive and fun company – stocked kitchen with snacks and drinks to keep you nourished, rooftop patio, and no less than two craft beers on tap for your enjoyment.
Our Team's Mission and Methods
The Data Analytics Team at Splunk focuses on delivering actionable data on a silver platter to both external customers and internal data scientists, data analysts and users.
Our new Data Analytics Platform was designed to satisfy the data quality requirements of our high-value and high-complexity data, while also providing fast query performance, ease of maintenance, automatic self-healing, immutable data, and fully automated CI/CD code deployment. Our target technology stack includes Kinesis, S3, Lambdas, Kubernetes, Athena (Presto), Postgres, Python, and Scala.
The team's influences and backgrounds come from data warehousing, data lakes, big data, analytical data engineering, business intelligence, software engineering, and devops.
We're currently building and transitioning into the Data Analytics Platform - and so are continuing to experiment, explore and evaluate these methods as we move forward.
And we are committed to making the work fun, interesting, and exciting while collaborating, mentoring, and supporting one another.
Who You Are
Over the last five or more years you've been working as a data engineer to build and maintain custom data pipelines and data at rest in support of reporting and data analysis using a number of elements from our technology stack described above.
You think of data as one of the most valuable resources an organization has, not just an inconvenient by-product of a process, but an opportunity-rich source of future features and capabilities. You understand that the success of a data engineering team is measured by the success of our data consumers and so think deeply and creatively about ways to deliver high-quality, high-functionality and high-performance data for these users.
You are a software engineer for whom automated testing is as important as clean, performant and reusable code.
You are an enthusiast - of analyzing data, of helping people make a difference with data, of building great analytical solutions, and of automated testing and software delivery.
And as a Senior Data Engineer, you will be responsible for designing, developing and supporting data pipelines, data at rest, and machine learning applications. Software will be written in Scala, Python, SQL, and Spark using an Agile development process.

BS in Computer Science or related plus 8 years of related professional experience
What We Offer You:
A constant stream of new things for you to learn. We're always expanding into new areas, bringing in open source projects and contributing back, and exploring new technologies.
A set of exceptionally talented and dedicated peers, all the way from engineering and QA to product management and user experience.
A stable, collaborative and supportive work environment within a team that is fully committed to everyone's success and joy.
We take work-life balance seriously: we prioritize a sustainable engineering culture over heroism, prioritize work that makes your life better, and many of us work from home two days a week.
We value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying.
For job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records."
2,"Senior Data Engineer, Data Informatics","Englewood, CO",Englewood,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Senior Data Engineer, Data Informatics
CPAADSC2.Analyst, Data Science

ttec Digital Consulting’s Insights Practice is looking for a Data Engineer who is passionate about solving complex data problems and intricate engineering issues with large systems.

We combine human insight and the speed of technology to transform our clients’ interactions with their customers. Advanced analytics of the customer experience and customer interactions is our expertise.

Check out our website below to learn more about what we do and how we help our clients.
https://www.ttec.com/ttec-digital
What you’ll be doing:

This person will be a part of our Humanify Insights Platform focused on helping our clients and coordinating with internal business consulting, analyst, data science, and technology teams.
Work closely with data scientists to create impactful insights from complex data
Drive and participate in the definition, the direction, and the development of our analytics platform
Improve platform features and functionality by keeping up with the latest innovations in data technology
Design, test, and maintain robust, scalable, secure, and fault-tolerant data management systems
Assist with process improvement with a customer focused, progressive mindset
Communication of project status/issues to clients and internal management
Partner with various internal teams
Document implementation requirements and expected effort
Research and recommend data management best practices
Aggressively and continuously advance skill set

What you’ll bring to us:
Bachelor’s degree qualifying under STEM with 2-3 years of relevant experience
Extensive knowledge of various databases
Proficiency with Azure or AWS ecosystem
Experience with Big Data ETL/technologies – Hadoop, Hive, Spark
Experience working with structured, semi-structured, and unstructured data sources
Proficiency with SQL
Experience with NoSQL solutions such as MongoDB is preferred
Proficient in Python or Scala
Experienced in consuming third-party REST APIs (JSON) and SDKs
Proficiency in Linux environment
Understanding of complex data flows, identification of data processing bottlenecks and designing and implementing solutions
A broad set of technical skills and knowledge across hardware, software, systems and solutions development and across more than one technical domain
Passion for innovation, delivering quality results, self-driven discovery, outside the box thinking, and complex problem solving

What skills you’ll need:
Proven ability to balance and manage multiple, competing priorities.
Collaborative interpersonal skills and ability to work within cross-functional teams.
Self-starter who relies on experience and judgment to plan and accomplish goals in complex fast-paced environment to ensure quality of all data integration points.
Excellent customer service skills.
Creative problem-solving and analysis skills.
Ability to handle problem situations quickly, inventively, and resourcefully.
Project management skills including:
Ability to prioritize and manage tasks
Ability to plan, commit, and deliver to schedules
Ability to identify, escalate, and manage project issues
Willingness to work extended hours on an as-needed basis
Some travel is necessary
What We Offer:
Variable incentive bonus plan, 401K company match, tuition reimbursement
Global career mobility, employee recognition programs, professional development
State of the art technology which allows for seamless global connectivity
Rich wellness program and health incentives
Lead Everyday w Do the Right Thing w Reach for Amazing w Seek First to Understand w Act as One w Live life Passionately


Primary Location: US-CO-Englewood
Job: Consulting"
3,Principal Data Engineer & DBA,"Broomfield, CO",Broomfield,CO,None Found,None Found,"
Over five years of experience as a DBA
Working knowledge of Oracle Autonomous Data Warehouse environments in Oracle’s OCI (Oracle Cloud Infrastructure)
Experience in tuning and optimizing DB performance
Knowledge of DB security best practices, and back and recovery processes
Experience in developing data architectures
Years of experience in ETL (Extract, Load, and Transform) from various data sources
Knowledge of Big Data and Time-series databases
Data Cleansing expertise
Working knowledge of data analysis / data science",None Found,"
Over five years of experience as a DBA
Working knowledge of Oracle Autonomous Data Warehouse environments in Oracle’s OCI (Oracle Cloud Infrastructure)
Experience in tuning and optimizing DB performance
Knowledge of DB security best practices, and back and recovery processes
Experience in developing data architectures
Years of experience in ETL (Extract, Load, and Transform) from various data sources
Knowledge of Big Data and Time-series databases
Data Cleansing expertise
Working knowledge of data analysis / data science",None Found,None Found,"Principal Data Engineer & DBA-19000YP6


Preferred Qualifications

Position Responsibilities

The Oracle Cloud development is one of the biggest initiatives in the history of Oracle. We are introducing revolutionary SAAS, IAAS and PAAS products to the market place, and because many of these products are being built new, we are experiencing tremendous growth in our cloud storage product development team. Our systems move a huge amount of data and we need you to help build both the internal and external infrastructure to support it.

We are looking for an experienced data engineer / Database Administrator to help ETL, manage, and administrate our growing data warehouse. This person will be responsible for all aspects of a DBA role, managing our Cloud based Autonomous Data Warehouse on a daily basis. The person will also be responsible for structuring new DB schemas and working with stakeholders to understand they needs and requirements. Proposals for how to structure the data will need to be created and vetted with upper management. Part of duties includes ETL’ing data from various sources into our ADW instance for easy access and analysis. If this opportunity sounds exciting, look no further and join a growing and dynamic team to help develop a data and analytics based infrastructure in the Oracle Cloud organization. Our team has a start-up feel, but with the stability Oracle gives.

DESIRED QUALIFICATIONS:
The ideal candidate will have experience working with large dataset from multiple sources. They should have years of experience as a DBA and familiar with DB tuning. In addition, they should work well in teams, and understand how to create/optimize database schemas. The candidate will be expected to create simple and complex ETL jobs. Specific experience the ideal candidate will demonstrate includes:
Over five years of experience as a DBA
Working knowledge of Oracle Autonomous Data Warehouse environments in Oracle’s OCI (Oracle Cloud Infrastructure)
Experience in tuning and optimizing DB performance
Knowledge of DB security best practices, and back and recovery processes
Experience in developing data architectures
Years of experience in ETL (Extract, Load, and Transform) from various data sources
Knowledge of Big Data and Time-series databases
Data Cleansing expertise
Working knowledge of data analysis / data science

ADDITIONAL SKILLS SOUGHT:
The successful candidate is expected to demonstrate the following additional requirements:

MS in Computer Science or Data Engineering
5+ years of experience working in as a DBA & Data Engineer
Strong knowledge of data structures, algorithms
Highly skilled in Python and SQL
Experience with OCI, AWS, and/or other IaaS environments.

Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.


Detailed Description and Job Requirements

Design, develop, troubleshoot and debug software programs for databases, applications, tools, networks etc.

As a member of the software engineering division, you will assist in defining and developing software for tasks associated with the developing, debugging or designing of software applications or operating systems. Provide technical leadership to other software developers. Specify, design and implement modest changes to existing software architecture to meet changing needs.

Duties and tasks are varied and complex needing independent judgment. Fully competent in own area of expertise. May have project lead role and or supervise lower level personnel. BS or MS degree or equivalent experience relevant to functional area. 4 years of software engineering or related experience.

Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law."
4,"Data Strategy Specialist - Business & Data Analysis, Cloud, AWS, Azure, Big Data","Denver, CO 80203",Denver,CO,80203,None Found,None Found, 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:,None Found,None Found,None Found,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The North America Data Strategy & Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.

 Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model

Required Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:
o Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling
o Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.
o Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel
 Able to travel up to 100% (Mon-Thu)

Optional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model
 Experience in compiling business cases and roadmaps for data, analytics and technology investments

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
5,Data Engineer,"Centennial, CO 80112",Centennial,CO,80112,None Found,"3+ years working with SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing and stream processing
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 3+ years of experience in a Data Engineer or Database Developer role
Experience with big data tools: Azure SQL Server Analysis Services, Azure Data Warehouse
Experience with relational SQL and NoSQL databases
Experience with SQL Server Enterprise and Microsoft Master Data Services would be a plus
Experience with data pipeline and workflow management tools: Azure Service Bus, Azure Data Pipelines, Dell Boomi or similar technologies
Experience with Azure cloud services
Experience with object-oriented/object function scripting languages: C#, C++, JavaScript, etc.",None Found,"Create and maintain master data solutions for critical business data
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure or similar cloud ‘big data’ technologies.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with data and analytics experts to strive for greater functionality in our data systems.
",None Found,None Found,"By joining the Bio-Techne team you will have an impact on future cutting-edge research. Bio-Techne, and all of its brands, provides tools for researchers in Life Sciences and Clinical Diagnostics.
Position Summary
We are looking for a Data Engineer to join our growing team. As a Data Engineer you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The core function will be driving the creation of master data repositories for critical business data. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.
Key Responsibilities
Create and maintain master data solutions for critical business data
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure or similar cloud ‘big data’ technologies.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications
Qualifications
3+ years working with SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing and stream processing
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 3+ years of experience in a Data Engineer or Database Developer role
Experience with big data tools: Azure SQL Server Analysis Services, Azure Data Warehouse
Experience with relational SQL and NoSQL databases
Experience with SQL Server Enterprise and Microsoft Master Data Services would be a plus
Experience with data pipeline and workflow management tools: Azure Service Bus, Azure Data Pipelines, Dell Boomi or similar technologies
Experience with Azure cloud services
Experience with object-oriented/object function scripting languages: C#, C++, JavaScript, etc."
6,Data Engineer - Population Health,"Denver, CO 80202",Denver,CO,80202,None Found,None Found,None Found,None Found,None Found,None Found,"Our Population Health Data Integration Engineer will deliver top notch data design and integrations to drive meaningful patient interventions. He/she will work with a high functioning team and have a bias towards action. He/She will be able to collaborate with cross-functional teams, and is obsessive about breaking down complex data problems to provide our clinicians with a holistic view of a patient.
All About DaVita IKC.
DaVita Integrated Kidney Care (DaVita IKC) is an the integrated care division of DaVita Inc. working on DaVita’s vision to provide integrated care to all ESRD patients, who are some of the most medically complex and vulnerable patient populations in the US. We provide services to help improve the lives of our patients by working with them to prevent complications, reduce the number of avoidable hospitalizations and improve overall health.

Let's face it…the world of healthcare is dynamic and moves at the speed of light. Unlike many healthcare companies in the market today, we acknowledge that our teammates are our most important asset and we want to help them to feel fulfilled in their careers.

When you join the DaVita Village, you're joining a winning team. Through our commitment to training, growth and quality we consistently achieve superior clinical outcomes while giving teammates the opportunity to excel in an award-winning environment that enables them to thrive both professionally and personally.
Essential Duties & Responsibilities:
Develops overall data strategy for implementing population health data and integrations platform following SDLC to manage development, testing and deployment
Identifies and partners with integrating systems both internal and external. Creating relationships with other system owners to organize work around data integration
Builds expertise on data sources, types of data, and challenges
Strong relational database concepts and a demonstrated ability to comprehend complex technical systems and/or processes
Ability to create data mapping to standardize data to a single data model
Ability to use multiple technical tools to answer questions and conduct analysis
Qualifications:
Bachelors in Computer Science, MIS, or Information Management
Ability to leverage multiple tools and technologies to analyze and manipulate large data sets from disparate data sources
Strong leadership and stakeholder communication skills. Strong logical, analytical and presentation skills.
Knowledge of population health concepts, data related government and regulatory requirements and emerging trends and issues (e.g. information privacy and data protection laws).
Excellent problem-solving, organizational and analytical skills with the ability to evolve product strategy based on research, data and industry trends
Demonstrated ability to easily build expertise in unfamiliar domains, and achieve stretch goals in an innovative environment
Experience working in an Agile environment
Database modeling experience preferred
Advanced MS Excel skills required
Advanced experience with SQL/PL SQL
Here is what you can expect when you join our Village:
A ""community first, company second"" culture based on Core Values that really matter.Clinical outcomes consistently ranked above the national average.Award-winning education and training across multiple career paths to help you reach your potential.Performance-based rewards based on stellar individual and team contributions.A comprehensive benefits package designed to enhance your health, your financial well-being and your future.Dedication, above all, to caring for patients suffering from chronic kidney failure across the nation. Join us as we pursue our vision ""To Build the Greatest Healthcare Community the World has Ever Seen.""
Why wait? Explore a career with DaVita today.

Go to http://careers.davita.com to learn more or apply."
7,Data Engineer,"Broomfield, CO 80021",Broomfield,CO,80021,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for a Data Engineer with a passion for transforming data architecture and building, new, cloud-based (AWS) solutions. This role is important in building functional and scalable data infrastructure to provide the organization the ability to make informed decisions. The ideal candidate has a strong background in the end-to-end data architecture, SQL and data warehousing. You will add your knowledge in the “how” vision for data engineering and will help to drive leading-edge data warehousing and business intelligence practices at a company which is high on collaboration and innovation, but low on egos. This is an opportunity to have a definitive impact on a company's data practice. Apply today!

In this role you will:
Work with internal business customers in understanding the business requirements and develop prototypes and proof of concepts for selected projects
Implement complex data solutions with a focus on collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into insights using multiple platforms
Work with our global team and provide technical direction in building solutions and architecting distributed systems
Actively participate in knowledge sharing sessions, code and design reviews etc.
Develop scalable data lake for data acquisition, storage, access, and analysis required for structured, semi-structured, and unstructured data in a cloud (AWS) environment
Enhance the data infrastructure framework by evaluating new and existing technologies and techniques to create efficient processes around data extraction, aggregation, and analytics
Assist in establishing and maintaining standards and guidelines for the design, development, tuning, deployment, and maintenance of information and advanced data analytics
You bring to the table:
3+ years in Data Engineering
Hands on experience building Data Lake in AWS (preferably) while sourcing data from heterogeneous sources
Experience with AWS technologies, such as S3, Aurora, Athena, Redshift, Kinesis, EMR, etc.
Experience with distributed systems and NoSQL databases
Hands on experience with Spark, Python and PySpark
Hands on experience with Data Modeling
You enjoy learning new technologies, data analysis, and identifying data patterns and trends
Ability to recommend innovative solutions to solve business and technical problems in a straight forward manner to business partners
Extensive experience with SQL programming (TSQL, PLSQL)
Strong communication skills
Strong problem solving and analytical skills
At Webroot, we do more than secure our customers' personal computers, mobile devices and networks. We also nurture our employees' most critical assets – their talents, experience, and career aspirations. Webroot has the energy of a start-up with the strength and stability of an Internet security market leader. We foster the innovative culture you'd expect of a company that's making a statement. Webroot is a company in which you can invest yourself fully, knowing that you're not only protecting our customers around the world, but also that your talents and innovation will be recognized and rewarded. We encourage you to learn more about us and explore our job openings. Secure your future. Ensure the same for your career. Principals only - no third parties, please. Webroot Inc. is an Equal Opportunity Employer. At Webroot, we believe in rewarding success. Our total rewards philosophy includes highly competitive salaries and a robust benefits package that you can view here.

Job Information
Job Id: WEBR-DataEngineerCO1
Company: Webroot
Location: Broomfield, CO, United States
Position: Data Engineer - AWS
Type: Full Time"
8,Supply Chain Data Engineer,"Englewood, CO",Englewood,CO,None Found,None Found,"Education - Sys/Div/Mkt/Local Manager - Bachelor's Degree and minimum of 3 years leadership experience OR minimum of 5 years leadership experience in the discipline OR Master's Degree and no experience. Bachelor's Degree in Information Systems, Business, Engineering or closely related field
Experience -
5+ years of experience in data engineering and/or devops
1 years of production experience with the SQL Server ecosystem
2 years of production experience with cloud or distributed platforms and architectures such as Azure, AWS, Spark, and/or Docker
Skills -
Python or Scala; advanced SQL
Knowledge of and experience with DevOps concepts, tools, and architectures
Comfort with Linux and Windows server environments, web APIs
Experience with Airflow or similar orchestration tools. SSIS experience helpful.
",None Found,"Design, build, test, and maintain data pipelines integrating data from source systems into a cohesive data warehouse and other repositories for reporting and analysis.
Expand and maintain source control, build scripts, and other elements supporting increasingly continuous deployment.
Institute a testing framework for data pipelines and schemas and write tests.
Coordinate the automated transfer of data between local and remote systems.
Work with data science and analytics team to deploy machine learning models.
Implement tools to monitor data quality, availability, and pipeline performance.
Work with IT to plan and execute further improvements to Supply Chain's data platform as needed, potentially including cloud (e.g, Azure or AWS), containerization (e.g., Docker), or other (e.g., Spark) components.
",None Found,None Found,"Dignity Health is one of the nation's largest health care systems. As of June 30, 2017, Dignity Health operated more than 400 care centers, including hospitals, urgent and occupational care, imaging and surgery centers, home health, and primary care clinics in 22 states, through its network of more than 9,000 physicians and more than 60,000 employees. Headquartered in San Francisco, CA, Dignity Health is dedicated to providing compassionate, high-quality, and affordable patient-centered care with special attention to those who are poor and underserved. In its fiscal year ended June 30, 2017, Dignity Health provided $2.6 billion in charitable care and services. More information on Dignity Health is available at www.dignityhealth.org .

Responsibilities
Job Summary:

The supply chain data engineer builds and maintains the data pipelines that make data accessible to supply chain staff, our partners, and the larger organization. This includes identifying and deploying best-in-class tools and infrastructure to improve the reliability and efficiency of those pipelines, as well as writing ELT code, tests, and documentation to democratize access to data. The data engineer will work with analytics, business, and IT stakeholders to identify, plan, and execute projects to radically improve the availability, quality, and scope of our data.

Core Duties:
Design, build, test, and maintain data pipelines integrating data from source systems into a cohesive data warehouse and other repositories for reporting and analysis.
Expand and maintain source control, build scripts, and other elements supporting increasingly continuous deployment.
Institute a testing framework for data pipelines and schemas and write tests.
Coordinate the automated transfer of data between local and remote systems.
Work with data science and analytics team to deploy machine learning models.
Implement tools to monitor data quality, availability, and pipeline performance.
Work with IT to plan and execute further improvements to Supply Chain's data platform as needed, potentially including cloud (e.g, Azure or AWS), containerization (e.g., Docker), or other (e.g., Spark) components.
Non-Essential Job Responsibilities:
Other duties as assigned by management
~LI-DH
tb91119

Qualifications
Minimum Qualifications:

Education - Sys/Div/Mkt/Local Manager - Bachelor's Degree and minimum of 3 years leadership experience OR minimum of 5 years leadership experience in the discipline OR Master's Degree and no experience. Bachelor's Degree in Information Systems, Business, Engineering or closely related field
Experience -
5+ years of experience in data engineering and/or devops
1 years of production experience with the SQL Server ecosystem
2 years of production experience with cloud or distributed platforms and architectures such as Azure, AWS, Spark, and/or Docker
Skills -
Python or Scala; advanced SQL
Knowledge of and experience with DevOps concepts, tools, and architectures
Comfort with Linux and Windows server environments, web APIs
Experience with Airflow or similar orchestration tools. SSIS experience helpful.

Preferred Qualifications:

Familiarity with dbt, R, Tableau, or related tools helpful
Relevant experience in a healthcare environment and computer systems preferred"
9,"Consultant, Cloud Data Engineer","Denver, CO 80202",Denver,CO,80202,None Found,"
3+ years previous consulting experience preferred
3+ years architecting and implementing Microsoft Azure, Amazon Web Services, and/or Google Cloud Platform infrastructure and topologies
Experience implementing core infrastructure, networking, and cloud-based services for business teams or consumers
Experience implementing Lambda architecture-based data designs
Deep product knowledge and understanding of Azure, AWS, and/or GCP
Experience configuring and tuning virtual private clouds
Practical experience sizing hardware and storage needs
Strong analytical problem solving ability
Good written and verbal communication + presentation skills
Self-starter with the ability to work independently or as part of a project team
Capability to execute performance analysis, troubleshooting, and remediation
Knowledge of High Availability and Disaster Recovery principles, patterns, and usage
Understanding of the cloud ecosystem and leading emerging technologies/interdependencies",None Found,"
Work as part of a team to design and develop cloud data solutions
Gather technical requirements, assess client capabilities, and analyze findings to provide appropriate cloud solution recommendations and adoption strategy
Define Cloud Data strategies, including designing multi-phased implementation roadmaps
Lead analysis, architecture, design, and development of data warehouse and business intelligence solutions
Be versed in Amazon Web Services, Google Cloud Platform, and/or Microsoft Azure cloud solutions, architecture, related technologies, and their interdependencies
Research, analyze, recommend, and select technical approaches for solving development and integration problems
Learn and adopt new tools/techniques to increase performance, automation, and scalability
Assist business development teams with pre-sales activities and RFPs
Understand how to translate business goals and drivers into a technical solution
Provide technical direction and oversight to cloud implementation teams",None Found,None Found,"Slalom is a modern consulting firm focused on strategy, technology, and business transformation. In 30 markets across the U.S., U.K., and Canada, Slalom's teams have autonomy to move fast and do what's right. They're backed by regional innovation hubs, a global culture of collaboration, and partnerships with the world's top technology providers.
Founded in 2001 and headquartered in Seattle, Slalom has organically grown to over 7,000 employees. Slalom was named one of Fortune's 100 Best Companies to Work For in 2019 and is regularly recognized by employees as a best place to work.

Job Title:
Cloud Data Engineer
As a Cloud Data Engineer in our Data & Analytics practice at Slalom, you will analyze, design, and architect cloud-based solutions to address our clients’ needs for infrastructure-as-a-service, platform-as-a-service, and software-as-a-service. We are looking for sharp, disciplined, and self-motivated individuals who have a passion for utilizing cloud solutions from Amazon Web Services, Microsoft Azure, and Google Cloud Platform to solve real business problems for our clients.
Responsibilities:
Work as part of a team to design and develop cloud data solutions
Gather technical requirements, assess client capabilities, and analyze findings to provide appropriate cloud solution recommendations and adoption strategy
Define Cloud Data strategies, including designing multi-phased implementation roadmaps
Lead analysis, architecture, design, and development of data warehouse and business intelligence solutions
Be versed in Amazon Web Services, Google Cloud Platform, and/or Microsoft Azure cloud solutions, architecture, related technologies, and their interdependencies
Research, analyze, recommend, and select technical approaches for solving development and integration problems
Learn and adopt new tools/techniques to increase performance, automation, and scalability
Assist business development teams with pre-sales activities and RFPs
Understand how to translate business goals and drivers into a technical solution
Provide technical direction and oversight to cloud implementation teams
Qualifications:
3+ years previous consulting experience preferred
3+ years architecting and implementing Microsoft Azure, Amazon Web Services, and/or Google Cloud Platform infrastructure and topologies
Experience implementing core infrastructure, networking, and cloud-based services for business teams or consumers
Experience implementing Lambda architecture-based data designs
Deep product knowledge and understanding of Azure, AWS, and/or GCP
Experience configuring and tuning virtual private clouds
Practical experience sizing hardware and storage needs
Strong analytical problem solving ability
Good written and verbal communication + presentation skills
Self-starter with the ability to work independently or as part of a project team
Capability to execute performance analysis, troubleshooting, and remediation
Knowledge of High Availability and Disaster Recovery principles, patterns, and usage
Understanding of the cloud ecosystem and leading emerging technologies/interdependencies

Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."
10,Senior Big Data Engineer,"Englewood, CO",Englewood,CO,None Found,None Found,"Bachelor’s in computer science or related field is required (masters preferred)You have a minimum of 7 years of experience in the design, development, and deployment of large-scale, distributed, and cloud-deployed software services.Expected to be an expert in SQL and RDBMS. Good at modeling data for relational, analytical and big data workloadsYou have a minimum of 4 years of experience in Big Data software development technologies (e.g., Hadoop, Hive, Spark, Kafka) and exposure to resource/cluster management technologies.Minimum of 3 year of experience with AWS (e.g., EC2, S3, EMR, SNS, SQS, Aurora, Redshift).Experience with various software technologies/solutions and understand where to use them.2 years of experience with Data Virtualization like Denodo",None Found,"Ability to quickly identify an opportunity and recommend possible technical solutions.Utilize multiple development languages/tools such as Python, SPARK, Hive to build prototypes and evaluate results for effectiveness and feasibility.Operationalize data ingestion and data-analytic tools for enterprise use.Utilize tools available to you across AWS ServicesDevelop real-time data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, NIFI, Python, HBase and Hadoop.Custom Data pipeline development (Cloud and locally hosted)Work heavily within AWS and Hadoop ecosystemsProvide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc",None Found,None Found,"The Company

Hitachi Vantara combines technology, intellectual property and industry knowledge to deliver data-managing solutions that help enterprises improve their customers’ experiences, develop new revenue streams, and lower the costs of business. Hitachi Vantara elevates your innovation advantage by combining IT, operational technology (OT) and domain expertise. Come join our team and our employee-focused culture and help drive our customers’ data to meaningful customer outcomes.

The Role

As a Sr. Big Data Engineer you will provide engineering knowledge to create and enhance data solutions enabling seamless delivery of data across our enterprise. You will be on the cutting edge of finding and integrating new technology and tools for data centric projects. Additionally, you will provide technical consulting to peer data engineers during design and development of highly complex and critical data projects. Some of these projects will include designing and developing data ingestion and processing/transformation frameworks leveraging open source tools such as NiFi, EMR, Java, Scala, Spark APIs, AWS Glue, etc. Additionally, you will work on real time processing solutions using tools such as Spark Streaming, MQ, Kafka, and AWS Kinesis. You will deploy application code using CI/CD tools and techniques.

Responsibilities:

 Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.
Ability to quickly identify an opportunity and recommend possible technical solutions.Utilize multiple development languages/tools such as Python, SPARK, Hive to build prototypes and evaluate results for effectiveness and feasibility.Operationalize data ingestion and data-analytic tools for enterprise use.Utilize tools available to you across AWS ServicesDevelop real-time data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, NIFI, Python, HBase and Hadoop.Custom Data pipeline development (Cloud and locally hosted)Work heavily within AWS and Hadoop ecosystemsProvide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc

Qualifications
Bachelor’s in computer science or related field is required (masters preferred)You have a minimum of 7 years of experience in the design, development, and deployment of large-scale, distributed, and cloud-deployed software services.Expected to be an expert in SQL and RDBMS. Good at modeling data for relational, analytical and big data workloadsYou have a minimum of 4 years of experience in Big Data software development technologies (e.g., Hadoop, Hive, Spark, Kafka) and exposure to resource/cluster management technologies.Minimum of 3 year of experience with AWS (e.g., EC2, S3, EMR, SNS, SQS, Aurora, Redshift).Experience with various software technologies/solutions and understand where to use them.2 years of experience with Data Virtualization like Denodo

We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.

#LI-DNI"
11,Sr. Data Engineer,"Greenwood Village, CO",Greenwood Village,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Innovar Group is looking for a Sr. Data Engineer in Greenwood Village, CO

CLICK APPLY NOW TO LEARN MORE ABOUT THIS JOB
Innovar Group is seeking a talented, driven, and motivated Sr. Data Engineer for an exciting Contract-to-hire opportunity in the Denver Tech Center. This is an outstanding opportunity to work for a cutting-edge software team and help design and develop the new iteration of their product.
Experience: Contract
About Us: Innovar Group is comprised of senior talent agents who deliver top recruitment services to clients throughout the United States. We bring a new era of recruiting to the industry by aligning state-of-the-art technology with outstanding talent.

Innovar - derived from the Latin “to innovate” - this is what we set out to do each and every day. We strive to go beyond the norm to utilize innovative next-gen recruiting tools connecting us to the world at large in order to uncover high-impact TECHNOLOGY talent for our valued clients. Our goal is straightforward: the unmitigated satisfaction of each client. The difference is significant; we provide consummate service and foster long-term, thriving relationships with our clients. In essence, we work as a vital member of your talent acquisition team.

Keywords: Ideal Candidate / Analysis / Analysis / Teamwork / Design / Implementation / Support / Write / Experience / Knowledge / Communication /"
12,Jr. Data Engineer,"Lone Tree, CO 80124",Lone Tree,CO,80124,None Found,None Found,None Found,None Found,None Found,None Found,"Your Opportunity
Do you want to be part of a Data Solutions Delivery team managing over 150+ terabytes of data and building the next generation analytics platform for a leading financial firm with over $3.2 trillion in assets under management? At Schwab, the Global Data Technology (GDT) organization governs the strategy and implementation of the enterprise data warehouse and emerging data platforms. We help Marketing, Finance and executive leadership make fact-based decisions by integrating and analyzing data.
We are looking for a Data Engineer who has passion for data and comes with data engineering background. Someone who has experience in designing and coding batch as well as real time ETL and one who wants to be part of a team that is actively designing and implementing the big data lake and analytical architecture on Hadoop. You will have the opportunity to grow in responsibility, work on exciting and challenging projects, train on emerging technologies and help set the future of the Data Solution Delivery team.
What you’re good at
Designing schemas, data models and data architecture for Hadoop and HBase environments
Building and maintaining code for real time data ingestion using Java, MapR-Streams (Kafka) and STORM.
Implementing data flow scripts using Unix / Hive QL / Pig scripting
Designing, building and support data processing pipelines to transform data using Hadoop technologies
Designing, building data assets in MapR-DB (HBASE), and HIVE
Developing and executing quality assurance and test scripts
Working with business analysts to understand business requirements and use cases
What you have
Minimum of 2 years of experience in understanding of best practices for building and designing ETL code Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required
Knowledge in schema design, developing data models and proven ability to work with complex data is preferred
Hands-on experience in Java object oriented programming (At least 2 years)
Hands-on experience with Hadoop, MapReduce, Hive, Pig, Flume, STORM, SPARK, Kafka and HBASE is preferred
Understanding Hadoop file format and compressions is preferred
Familiarity with MapR distribution of Hadoop is preferred
Understanding of best practices for building Data Lake and analytical architecture on Hadoop is preferred
Scripting / programming with UNIX, Java, Python, Scala etc. is preferred
Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required
Knowledge in real time data ingestion into Hadoop is preferred
Experience in working in large environments such as RDBMS, EDW, NoSQL, etc. is preferred
Knowledge of Big Data ETL such as Informatica BDM and Talend tools is preferred
Understanding security, encryption and masking using Kerberos, MapR-tickets, Vormetric and Voltage is preferred
Experience with Test Driven Code Development, SCM tools such as GIT, Jenkins is preferred
Experience with Graph database is preferred"
13,Sr Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Responsibilities
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Collaboration with the Data Center SMEs, Data Scientists, and Program Managers
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data
Minimum Qualification
Bachelor’s degree or Maser degree in Computer Science, Software Engineering, or related field
5 + years of SQL (Oracle, Vertica, Hive, etc.) experience and relational databases experience (Oracle, MySQL)
5 + years of experience in custom or structured (i.e. Informatica/Talent/Pentaho) ETL design, implementation and maintenance
5 + years’ experience in data engineering, experience in applying DWH/ETL best practic
5 + years of Java and/or Python development experience
5 + years of experience in LAMP and the Big Data stack environments (Hadoop, MapReduce, Hive)
5 + years of experience working with enterprise DE tools and experience learning in-house DE tools
3+ years exp in AWS data solutions stack – EMR, S3, redshift, Kinesis, ECS, Docker
3+ years exp in CI/CD stack – Jenkins , Git
Preferred Qualifications
Master’s degree or Bachelor degree in a related field.
Cloudera Administrator certification.
Working Conditions
Office environment."
14,Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,None Found,"
Experience working in a Microsoft SQL Server environment, especially with SSIS, stored procedures and query development.
Knowledge of data warehousing best practices, concepts and processes.
Strong analytical and problem-solving skills, with demonstrated change management experience.
Demonstrated ability to set and meet project timelines and deliverables.
Effective interpersonal and communications skills with the ability to interact with various levels of personnel.
Must be flexible, organized, self-directed, able to prioritize multiple tasks, and able to manage a full workload.
Experienced in Microsoft Office and Microsoft Operating Systems.
Must be able to work in CCMCN's main office and travel to all required meetings
Fluency in written and spoken English.",None Found,None Found,None Found,"Location: 1212 S Broadway, Denver, CO 80210

Category: Full time, Exempt

Salary Range: (DOE)

Reports To: CEO
Colorado Community Managed Care Network (CCMCN) is comprised of Colorado’s Community Health Centers with over 190 clinic sites (including school based clinics, pharmacies, and mobile units). CCMCN was founded as a non-profit organization in 1994 to respond pro-actively to the advent of mandatory Medicaid managed care, and has evolved to a multi-faceted organization that serves its members in areas where a network solution optimizes collaborative endeavors. Areas of focus include population health, accountable care, shared services, health information technology and clinical quality improvement programming. CCMCN’s vision is that all Coloradoans have access to high quality, integrated, accountable health care.

CCMCN is a one-third founding partner in Colorado Access, a health plan focusing on Medicaid, Medicare and Child Health Plan Plus (CHP+) and CCMCN has been a HRSA-supported Health Center Controlled Network (HCCN) since 1995. CCMCN is governed by a Board of Directors comprised of organizational representatives from each of its health center members as well as representation from Colorado Community Health Network (CCHN). The Board membership includes clinician representatives, elected at-large, and carries out policy and decision-making duties.

Position Description:
The Data Operations department provides data management, integration, and reporting services for multiple external and internal consumers. The Data Engineer will be responsible for all aspects of data management that support CCMCN’s production services. Candidate must have experience in Microsoft SQL database development, data integration, ETL (extract, transform and load) tools and methods, analytics, reporting, and documentation.

Essential Functions:
Provide development and maintenance support for data integrations between systems.
Provide development and maintenance support for the EDW and supporting databases.
Provide automated solutions whenever possible and proactively suggest alternative solutions.
Assist in the development of new databases and associated processes as necessary.
Provide data analytics report development for specific projects as needed.
Develop data validation reports and analysis where applicable.
Develop technical documentation of data integrations and processes.
Communicate and collaborate with other team members and clients to develop innovative data solutions.
Utilize up-to-date knowledge of database and data quality best practices to produce effective solutions.
Remain knowledgeable in healthcare data standards, measures and code sets as well as applicable data privacy practices and legal requirements.
Required Skills and Experience:
Experience working in a Microsoft SQL Server environment, especially with SSIS, stored procedures and query development.
Knowledge of data warehousing best practices, concepts and processes.
Strong analytical and problem-solving skills, with demonstrated change management experience.
Demonstrated ability to set and meet project timelines and deliverables.
Effective interpersonal and communications skills with the ability to interact with various levels of personnel.
Must be flexible, organized, self-directed, able to prioritize multiple tasks, and able to manage a full workload.
Experienced in Microsoft Office and Microsoft Operating Systems.
Must be able to work in CCMCN's main office and travel to all required meetings
Fluency in written and spoken English.
Additional Preferred Skills:
Strong business and technical writing abilities.
Knowledge of healthcare data standards such as HL7, CCD, CCR and claims data.
Knowledge of standard healthcare code sets like LOINC, ICD9/10, CPT4 and SNOMED.
Knowledge of IHI triple aim, clinical quality improvement, and primary care operations/workflow.
Ability to stay current on healthcare reporting requirements such as UDS, PQRI, NQF, Meaningful Use and Patient Centered Medical Home.
Ability to attend conferences and workshops for further education to expand and improve management skills.
CCMCN is an equal opportunity employer offering a generous benefit package, generous vacation and holiday schedule, casual work environment, and a flexible work schedule.

TO APPLY:
Please submit a resume and cover letter by email to jobs@ccmcn.com.
PLEASE INCLUDE THE JOB TITLE IN THE SUBJECT OF THE EMAIL.
No phone calls please."
15,Data Engineer,"Denver, CO 80221",Denver,CO,80221,None Found,"
Must have a solid understanding of data engineering, integration, and warehousing concepts and patterns.
Must have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem
You’re proficient with Linux operations and development, including basic commands and shell scripting
You can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices
Must be fluent in Python, R, and Java
Must have excellent experience command of SQL
Must have good experience and knowledge with Data Modeling concepts.
You have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills",None Found,None Found,None Found,None Found,"Job Description Summary
We are looking for Data/Machine Learning engineers at all levels to help us build a robust and scalable data platform to support AI/ML data pipelines, reporting and data analysis as our business scales. We use cloud native (AWS) cutting-edge technologies like Spark, Kinesis/Kafka Streaming, Graph , infrastructure as code, CI/CD to deliver high-quality data solutions to analysts, data scientists, and partners. We’re looking for an engineer that takes ownership in their work, has a strong focus on quality, and enjoys working in a collaborative environment. At Transamerica, we believe achieving a secure future requires both smart financial planning and a healthy lifestyle. We’re using data science, machine learning, computer vision, natural language processing, and Iot to revolutionize the way our customers save, invest, protect, and retire and to help them develop better wellness habits. As part of the Data Engineering team in our Analytics Execution group, you will work with data scientists and analytics engagement managers to develop innovative data-based solutions that transform the way we do business.
Job Description
Qualifications:
Must have a solid understanding of data engineering, integration, and warehousing concepts and patterns.
Must have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem
You’re proficient with Linux operations and development, including basic commands and shell scripting
You can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices
Must be fluent in Python, R, and Java
Must have excellent experience command of SQL
Must have good experience and knowledge with Data Modeling concepts.
You have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills
Preferred Qualifications:
Must have 3 -5 years of experience building data productionalized pipelines.
Must have strong experience ingesting huge volumes of structured and unstructured data both in streaming and batch ingestions patterns.
2 - 4 years of Cloud development experience with AWS and or Azure stack.
Exposure with and have solid experience with statistical analysis and machine learning libraries
Must have previous experience with NoSQL database implementations
You understand the fundamentals of lambda architectures and serverless. applications
Must be proficient in Tableau
Must be comfortable with leveraging ETL tools, like Informatica.
You are proficient in Scala or Node.js
You have a master’s degree in a quantitative field
Job Description:
Partner with data scientists, analytics engagement managers, and other data engineers to discover, collect, cleanse, and refine the data needed for analysis and modeling
Analyze large data sets to extract actionable insights and inform experimental design and model development
Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build models using basic statistical and machine learning techniques, partnering with data scientists for education and guidance
We’re looking for an engineer that takes ownership in their work, has a strong focus on quality, and enjoys working in a collaborative environment.
Working Conditions
Office environment"
16,Cloud Based Analytics Data Engineer,"Englewood, CO",Englewood,CO,None Found,None Found,None Found,"
Linux command line experience.
Python experience.
Working knowledge and experience with AWS, S3, RDS/Postgres.
Experience with Php / Laravel framework.
Experience with JIRA.
Experience working in an Agile/SCRUM environment.",None Found,None Found,"
Experience with Relational Database
Experience with query tools like SQL
Experience using a report designer such as Power BI
Ability to work in a team environment.
Strong communication skills.","Whether it’s unlocking the potential of digital content, powering breakthrough innovations, creating entertainment that enriches lives, or keeping nations secure, Quantum works with customers and partners to make the world a happier, safer and smarter place.
Job Summary and Duties:
We are looking for an experienced candidate to be part of an international team of senior engineers that develop a global data warehouse and information system for scalar tape library devices. This is a business critical role that will enable servicing our hyper scale customers.
Specific duties include but are not limited to:
Design and implement data ETL from logfiles, database dumps, etc. into database target to support managed services for global install base. A typical parsing script will be in Python.
Analyze source data and define target data.
Implement ad hoc and scheduled reporting, with Power BI and other reporting tools
Gather, document, and implement requirements for new reporting.

Job Requirements:
Experience with Relational Database
Experience with query tools like SQL
Experience using a report designer such as Power BI
Ability to work in a team environment.
Strong communication skills.

Desired Skills:
Linux command line experience.
Python experience.
Working knowledge and experience with AWS, S3, RDS/Postgres.
Experience with Php / Laravel framework.
Experience with JIRA.
Experience working in an Agile/SCRUM environment.
Experience with containers, AWS cloud, and Azure cloud.
Quantum is proud to be an equal opportunity and affirmative action employer. Female/Minority/Veteran/Disabled/Sexual Orientation/Gender Identity."
17,Sr Data Architect,"Boulder, CO",Boulder,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and seek to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun and most importantly to each other’s success. Learn more about Splunk careers and how you can become a part of our journey!
Splunk is located in Boulder, CO. This team is made up of a strong mix of very talented and driven individuals, with a proven senior leadership team with a strong technical background.
The office layout is open and the floor plan helps to foster the casual, collaborative environment that has played a key role in the company’s innovation of its product. In addition, you will find everything you would expect in the office of a progressive and fun company – stocked kitchen with snacks and drinks to keep you nourished, rooftop patio, and no less than two craft beers on tap for your enjoyment.
Our Team's Mission and Methods
The Data Analytics Team at Splunk focuses on delivering actionable data on a silver platter to both external customers and internal data scientists, data analysts and users.
Our new Data Analytics Platform was designed to satisfy the data quality requirements of our high-value and high-complexity data, while also providing fast query performance, ease of maintenance, automatic self-healing, immutable data, and fully automated CI/CD code deployment. Our target technology stack includes Kinesis, S3, Lambdas, Kubernetes, Athena (Presto), Parquet, Postgres, Python, and Scala.
The team's influences and backgrounds come from data warehousing, data lakes, big data, analytical data engineering, business intelligence, software engineering, and devops.
We're currently building and transitioning into the Data Analytics Platform - and so are continuing to experiment, explore and evaluate these methods as we move forward.
And we are committed to making the work fun, interesting, and exciting while collaborating, mentoring, and supporting one another.
Who You Are
Over the last five or more years you've been working as a data architect or data engineer building and maintaining custom data pipelines and data at rest in support of reporting and data analysis using a number of elements from our technology stack described above.
You think of data as one of the most valuable resources an organization has, not just an inconvenient by-product of a process, but an opportunity-rich source of features and capabilities. You understand that the success of a data engineering team is measured by the success of its data consumers and so think deeply and creatively about ways to deliver high-quality, high-functionality and high-performance data for these users.
While primarily a data architect with exceptional skills in dimensional modeling, parallel query technology, columnar storage, metadata, and data management; you are also a software engineer comfortable writing clean code and automated tests.
You are an enthusiast - of analyzing data, of helping people make a difference with data, of building great analytical solutions, and of automated testing and software delivery.
And as a Senior Data Architect, your focus will be primarily on:
The overall flow of data between applications across our incident-management activity and ultimately through our platform and to users.
Helping to determine the best technology for different data uses and how to implement it.
A deep understanding of all data attributes and business rules within our platform.
Developing over the wire and at-rest data models - involving considerations of partitioning, queries, write-performance, latency, and usability
Collaborating with other teams on our incoming data.
Assisting consumers in working with our data
And since we're a small team, you will also get to occasionally wear the data engineer hat and participate in building, testing and supporting our pipelines.
What We Offer You:
A constant stream of new things for you to learn. We're always expanding into new areas, bringing in open source projects and contributing back, and exploring new technologies.
A set of exceptionally talented and dedicated peers, all the way from engineering and QA to product management and user experience.
A stable, collaborative and supportive work environment within a team that is fully committed to everyone's success and joy.
We take work-life balance seriously: we prioritize a sustainable engineering culture over heroism, prioritize work that makes your life better, and many of us work from home two days a week.
We value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying.
For job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records."
18,BI Data Engineer,"Boulder, CO",Boulder,CO,None Found,None Found,"Seeking of 3-5+ years of experience with data analysis, data warehousing, and delivering reporting
Fluent in at least SQL syntax (Read functions)
Experience with Tableau, Power BI, or Looker
Proficiency with MS Excel
Experience with statistical models and methodologies
Experience with eCommerce analysis (CLV, AOV, Retention Rate, etc) a plus
Experience with Python, Javascript, and R are a plus
Experience with Financial Management, Supply Chain, Warehouse Management, and Manufacturing
Experience working with an eCommerce platform
Bachelor’s Degree in Information Technology or Mathematics or Statistics, or related field preferred
Excellent planning and project management skills
Self-starter with a high level of initiative and strong sense of ownership
Strong communication and interpersonal skills
Experience working in the CPG industry preferred
Experience with both B2C and B2B business models preferred
",None Found,None Found,None Found,None Found,"About Charlotte's Web™:
Charlotte's Web™ products are made from our world-renowned hemp genetics grown 100% in the USA. Founded by the Stanley Brothers of Colorado, Charlotte’s Web™ leads the industry in quality, safety, consistency and social responsibility to improve thousands of lives daily through the use of Charlotte's Web™. At Charlotte’s Web™, we are driven by principles that extend far beyond the bottom line. It is our goal to provide products of the highest possible quality, while contributing to the sustainability of the communities we have the privilege of serving.
Position Summary:
The BI Data Engineer will be responsible for managing and compiling data and metrics to help drive decision support for all functional business units.

Essential Duties:
Develop, implement, and maintain SAAS based data warehouse systems, integrations, and data frameworks.
Provide source to target mappings and data model specifications
Working with the Business Intelligence Manager to interface with the business to establish needs and data standards.
Developing and implementing data collection systems and other tactics that optimize data efficiency and accuracy
Acquiring data from primary or secondary data sources and maintaining (through 3rd party) databases
Scoping actionable analysis projects with functional business units
Prioritizing analysis projects on the basis of actionable outputs and potential business impact
Deliver summarized analysis, implications for action, and recommended use of insights to stakeholders
Complete and deliver large-scale analysis projects pertaining to product selection, customer behavior patterns, and business efficiency
Communicate data-driven insights to functional business groups and ensure a level of understanding that is actionable and impacts the business
Collaborate with the Business Intelligence Reporting Analyst to standardize and operationalize deep-dive analyses into relevant reporting dashboards
Convey analysis methodology to functional team
Ensure that all related documentation (business requirements, technical designs, data dictionary, etc.) are created, maintained and available in a central repository.
Coordinate cross-functionally with other departmental managers and Subject Matter Experts (SMEs) in Accounting / Finance, Marketing, Ecommerce, Sales, Supply Chain, Manufacturing, and Cultivation
Support corporate objectives and global growth strategies
Provides recommendations for improvements
Responsible for escalating support issues to third-parties, as needed
Provide regular project status reports
Predict project risk factors and address proactively
Qualifications:
Seeking of 3-5+ years of experience with data analysis, data warehousing, and delivering reporting
Fluent in at least SQL syntax (Read functions)
Experience with Tableau, Power BI, or Looker
Proficiency with MS Excel
Experience with statistical models and methodologies
Experience with eCommerce analysis (CLV, AOV, Retention Rate, etc) a plus
Experience with Python, Javascript, and R are a plus
Experience with Financial Management, Supply Chain, Warehouse Management, and Manufacturing
Experience working with an eCommerce platform
Bachelor’s Degree in Information Technology or Mathematics or Statistics, or related field preferred
Excellent planning and project management skills
Self-starter with a high level of initiative and strong sense of ownership
Strong communication and interpersonal skills
Experience working in the CPG industry preferred
Experience with both B2C and B2B business models preferred

Benefits: We offer best-in-class benefits, including:
Company-Paid Medical, Dental, and Vision
3 Weeks of Paid Vacation Your First Year
401K Match with Automatic Vesting
Up to 9 Weeks Paid Parental Leave
Self-Tailored Wellness Program
Generous Employee Discount

Charlotte’s Web™ provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Charlotte’s Web™ is an At-Will Employer."
19,Data Engineer,"Greenwood Village, CO",Greenwood Village,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Innovar Group is looking for a Data Engineer in Greenwood Village, CO

CLICK APPLY NOW TO LEARN MORE ABOUT THIS JOB
Innovar Group is seeking a talented, driven, and motivated Data Engineer for an exciting Contract-to-hire opportunity in the Denver Tech Center. This is an outstanding opportunity to work for a cutting-edge software team and help design and develop the new iteration of their product.
Experience: Contract
About Us: Innovar Group is comprised of senior talent agents who deliver top recruitment services to clients throughout the United States. We bring a new era of recruiting to the industry by aligning state-of-the-art technology with outstanding talent.

Innovar - derived from the Latin “to innovate” - this is what we set out to do each and every day. We strive to go beyond the norm to utilize innovative next-gen recruiting tools connecting us to the world at large in order to uncover high-impact TECHNOLOGY talent for our valued clients. Our goal is straightforward: the unmitigated satisfaction of each client. The difference is significant; we provide consummate service and foster long-term, thriving relationships with our clients. In essence, we work as a vital member of your talent acquisition team.

Keywords: Ideal Candidate / Analysis / Create / Support / Teamwork / Implementation / Architecture / Experience / Communication / Knowledge / Technologies /"
20,Data Engineering Analyst,"Broomfield, CO 80020",Broomfield,CO,80020,None Found,None Found,None Found,None Found,None Found,None Found,"Reach Your Peak at Vail Resorts. You're someone who pushes boundaries and challenges the status quo. You're brave, ambitious and passionate in everything you do. And we want you on our team. Pursue your fullest potential and never settle in the quest to deliver extraordinary guest service. Join one of the world's most innovative companies and re-imagine a mountain resort experience with us. Welcome to Vail Resorts. Reach Your Peak.
Welcome to Vail Resorts. Reach Your Peak.
DEPARTMENT PURPOSE
The mission of the Marketing Analytics team is to leverage Vail Resort’s proprietary guest data to generate actionable insights that unlock value for the business. The team is responsible for informing insight-driven strategic decision making across the entire enterprise.

POSITION PURPOSE
As the Data Engineer you will be developing the data products that enable and accelerate data science and analytics at Vail. You will have opportunity to explore new data sources, prepare, clean, and promote data through our analytic ecosystem. The Data Engineer will partner across the organization to create products aligned with our technology stack and explore new tools and methods to enhance the capabilities of our existing systems. This team will be the back bone of our advanced analytics and data science discipline by providing rich, trusted, and diverse sources.

Why this role is special
We are a technology and data-driven business with a proprietary guest database unmatched by any other company in the industry
You will be a critical part in the acceleration of our advanced analytics and data science programs
We are an incredibly fast-growing company at the forefront of the Travel and Tourism industry – redefining the ski industry in the 21st century
We are a group of motivated, ambitious, and forward looking leaders
All the free skiing / riding you and your dependents can handle – free Epic passes, discounted lodging and retail

RESPONSIBILITIES
 [Develop] Create and support an efficient, clean, and valuable set of data products that push forward what is possible with our expanding data ecosystem.
[Innovate] Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
[Build] Employ an array of technological languages and tools to connect systems together
[Quality Assurance] Partner with team members to ensure the quality of every product produced. Due to the complexity of our work and the credibility we are trusted with, every piece we do must receive high levels of scrutiny.
 [Knowledge Base] Collaborate with business and IT resources to facilitate documentation of process, tools, and meta-data via our data governance platform in order to advance our discipline and protect our data assets.
[Policy and Procedure] Adhere to all Vail Data Governance policies and procedures in enriching and protecting our data assets.

JOB REQUIREMENTS
At least 2 years’ experience with Enterprise Data required.
Advanced working SQL knowledge and experience working with relational databases
Previous exposure to development, documentation and maintenance of business data models
Candidates must be highly organized and detail-oriented.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strength in written and verbal communication. Excellent interpersonal, with the ability to communicate difficult concepts effectively.
Highly collaborative approach and open communication style

COMPUTER SKILLS
Proficient with MS Office Suite, Outlook & Internet applications
High level of comfort with technology
Relational database skills/understanding in a MS SQL environment
Experience with a Data Science tool set including: R, Python, Alteryx, SAS
Experience with analytic tools/software such as SQL, SAS, SPSS, Alteryx, MS Excel
Working in on premises and cloud environments

PREFERRED KNOWLEDGE
Familiarity with Vail Resorts and its operations is helpful

We offer a variety of career opportunities at our world-class resorts and corporate headquarters near Boulder, Colorado in fields like Finance & Accounting, Human Resources, Information Technology, Legal, Public Affairs & Sustainability, Marketing, Sales & Communications and more. Our corporate team shares both a passion for the outdoors and a drive to re-imagine the mountain resort experience around the world. Learn more at www.vailresortscareers.com
Vail Resorts is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or any other status protected by applicable law.
VRMRKT
Requisition ID 171722"
21,Sr. Data Engineer,"Greenwood Village, CO 80111",Greenwood Village,CO,80111,None Found,None Found,None Found,None Found,None Found,None Found,"Overview
BryterCX is seeking a Senior Data Engineer to work with our Denver, CO office. Our ideal candidate has extensive experience and working knowledge of a distributed software architectures, RDBMS, and distributed database technologies. Experience with analytics, time series data, optimization, and creating pipelines for analytics tools and data science is required.
Key Responsibilities
Participate in a scrum-based agile delivery team.
Contribute to architecting and designing performant analytical software systems.
Evaluate and tune RDBMS technology and architectures to improve the operational and processing capabilities.
Assist in data modeling and query optimization for RDBMS technologies.
Drive RDBMS and MPP RDBMS design, creation, and implementations.
Identify and implement ETL improvements to automate, validate and audit data ingestion.
Implement software patterns for data retrieval and persistence to support complex analytical queries and storage needs.
Identify technical debt and be pro-active in reducing debt by articulating the value to leadership and project teams.
Write production quality application and database code as required
Participate in driving policies and standards for modeling, structuring, naming, describing, securing, and formatting data
Qualifications
Bachelor's degree in Computer Science or equivalent
Significant experience with RDBMS systems, particularly Postgres is required.
Experience with data extraction and transformation is required.
Experience creating data models and performing query optimization is required.
Exceptional communication and management skills is required.
Working knowledge of MPP concepts and systems is required.
Experience with creating and consuming time series data, data visualization/analytics is preferred.
Understanding of the Hadoop ecosystem and experience with Hadoop based data lakes and solutions is preferred.
Experience in creating technical Proof of Concepts is preferred.
Experience in these technologies is preferred: Postgres, Greenplum.
Experience in some of these technologies is desirable: Hive, Impala, Spark.
Knowledge and experience in these technologies is a plus: Cloud Services, microservices, Python, Map Reduce, Java, Scala, HBase, MapReduce tuning.
Company Overview:
BryterCX pioneered and remains the industry leader in providing the premier Journey Analytics (Fox) solution for optimizing self-service information systems. BryterCX unique software modeling solutions enable its customers to translate complex customer interactions across multiple self-service channels, including Interactive Voice Response, Web/Intranet, Email, Chat, Agent Desktop, Routing, and other enterprise applications. Through ongoing application of the ClickFox Customer Behavior Intelligence system, companies can dramatically reduce operational costs, improve customer satisfaction and revenue generation and enhance the overall interactive customer experience.

sxCLpsnEXp"
22,Data Engineer (Up to 25% Profit Sharing Benefit!),"Denver, CO 80225",Denver,CO,80225,None Found,"
Clearance: Active TS/SCI with current Poly
Education: Bachelor’s degree or equivalent (Master's preferred)
Experience: A minimum of five years of related work experience","
Minimum 5 years experience with Linux/Unix environments
Strong software scripting skills in Python and other scripting languages (Bash, Perl, etc.)
Strong understanding of RDBMS databases (PostgreSQL, Oracle, MySql, etc.)
Prior experience working with the Git version-control system
Strong systems engineering background
Must be certified to meet DoD 8570 level IAT-II qualifications. Security+ certification or CISSP preferred.",None Found,None Found,None Found,"Job Description
BITS, a CACI Company, offers very rewarding and unique benefits, which equates to 50% of compensation on TOP of your base salary! The first part is a tax-qualified profit-sharing retirement plan, to which BITS annually contributes up to 25% of your base salary (not in excess of applicable IRS limits) to your retirement account. The second component consists of BITS' Individual Benefit Account (IBA), which is used for premiums, medical reimbursements, dependent care, education and Paid Time Off (PTO) policy. Both components of the BITS benefit package are paid for by BITS, in addition to your base salary and potential performance bonuses. We believe in a healthy home/work balance and both of our locations offer a wide variety of activities to balance with your work life. Learn more at http://www.caci.com/bit-systems/
We are seeking a passionate Data Engineer. The commitment of our employees to ""Engineering Results"" is the catalyst that has propelled BITS to becoming a leader in software development, R&D, sensor development and signal processing. Our engineering teams are highly adept at solving complex problems with the application of leading-edge technology solutions, empowering decision-makers to make better mission-critical decisions. Our operational team excels at signal collection, processing and analysis. We have operational personnel stationed around the world in support of our customers' missions.
What You’ll Get to Do:
As our Data Engineer you will have the opportunity to:
Work with external data providers and stakeholders to engineer the ingest of and distribution of mission-relevant data feeds into the FADE data ecosystem.
Focus on the operations of COTS and GOTS software to ensure low latency, high data integrity, quality, and availability for over 400 data feeds (and growing).
Work with users to help them troubleshoot and resolve issues with FADE data.
Work in a fast moving environment with many moving parts and must be able to juggle several tasks at once.
Work with a highly qualified team to maintain the integrity of data flow by responding to real-time operational alerts.
Bring a continuous delivery mindset while striving for a 99.9% uptime to the flow of data through the system.
Specific duties include:
Configure and maintain data ingest workflows (ETL) across several production systems
Install, configure, and update a wide array of COTS/GOTS and homegrown software applications
Support and troubleshoot diverse IT infrastructure hardware platforms and protocols
Work with software development and systems administration staff to monitor and troubleshoot production systems
Generate and maintain systems documentation and diagrams
Troubleshoot network issues and establish new connectivity
Monitor and maintain a variety of databases
You’ll Bring These Qualifications:
Clearance: Active TS/SCI with current Poly
Education: Bachelor’s degree or equivalent (Master's preferred)
Experience: A minimum of five years of related work experience
Required Skills:
Minimum 5 years experience with Linux/Unix environments
Strong software scripting skills in Python and other scripting languages (Bash, Perl, etc.)
Strong understanding of RDBMS databases (PostgreSQL, Oracle, MySql, etc.)
Prior experience working with the Git version-control system
Strong systems engineering background
Must be certified to meet DoD 8570 level IAT-II qualifications. Security+ certification or CISSP preferred.
These Qualifications Would be Nice to Have:
Red Hat Enterprise Linux administration experience
Understanding of Amazon Web Services – EC2, RDS, S3
Hadoop, Accumulo and Map Reduce techniques
Familiarity with monitoring tools such as Grafana and Kibana
Understands compiled languages including Java
Familiarity with Software Development Programs
Familiarity with Agile Development methodologies
What We can Offer You:
We’ve been named a Best Place to Work by the Washington Post and one of the Top Workplaces in the Denver, Co by the Denver Post.
Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives.
We offer competitive benefits and learning and development opportunities.
We are mission-oriented and ever vigilant in aligning our solutions with the nation’s highest priorities.
For over 55 years, the principles of CACI’s unique, character-based culture have been the driving force behind our success.
TH-JOBS!BB
HJ-BLITZ-101019!
Job Location
US-Denver-CO-DENVER


CACI employs a diverse range of talent to create an environment that fuels innovation and fosters continuous improvement and success. At CACI, you will have the opportunity to make an immediate impact by providing information solutions and services in support of national security missions and government transformation for Intelligence, Defense, and Federal Civilian customers. CACI is proud to provide dynamic careers for employees worldwide. CACI is an Equal Opportunity Employer - Females/Minorities/Protected Veterans/Individuals with Disabilities."
23,Sr. Data Engineer,"Boulder, CO 80301",Boulder,CO,80301,None Found,None Found,None Found,None Found,None Found,None Found,"Company Overview

Fanatics is the global leader in licensed sports merchandise and changing the way fans purchase their favorite team apparel and jerseys. Through an innovative, tech-infused approach to making and selling fan gear in today's on-demand culture, Fanatics operates more than 300 online and offline stores, including the e-commerce business for all major professional sports leagues (NFL, MLB, NBA, NHL, NASCAR, MLS, PGA), major media brands (NBC Sports, CBS Sports, FOX Sports) and more than 200 collegiate and professional team properties, which include several of the biggest global soccer clubs (Manchester United, Real Madrid, Chelsea, Manchester City). Fanatics offers the largest collection of timeless and timely merchandise whether shopping online, on your phone, in stores, in stadiums or on-site at the world's biggest sporting events.

About the Team

Fanatics is first and foremost a technology company. We are powered by cutting-edge tech created by our small agile teams using the latest tools and technologies under our highly analytical, forward thinking, and open-minded leadership. As the global leader in licensed sports merchandise, we challenge ourselves by improving our new fully responsive NodeJS cloud commerce platform, Elasticsearch engine, and deep data science capabilities while building the best-in-class retail manufacturing and supply chain technologies. Our tech teams work together to revolutionize data science and engineering initiatives, provide highly scalable real-time and streaming platforms, and create secure e-commerce and in-stadium fan experience products. Our own e-commerce platform transacts in over 190 countries, 17 languages, and 14 currencies. Our motto is “#GSD”—get stuff done—and we do just that. If you want to be at the nexus of sports, commerce, and technology, come be a part of our industry-leading team here at Fanatics Tech.

Our inventory intelligence team in close collaboration with data science team has a charter to build data-driven applications & services to develop supply chain & inventory management excellence at Fanatics. The team plays a key role in building data pipelines that extract and process raw data into useful data analytics and aid data scientists to develop predictive models to meet our business’s growing activities and potential. The pipelines are core to inventory replenishment algorithms, pricing optimization, assortment optimization, and deriving key business insights for our merchandising and fulfillment operations. We also build automation tools and monitoring systems to improve our development cycle.

We are seeking for a Senior Data Engineer who has strong architectural skills and upkeeps scalability, availability and excellence when building the next generation of our data pipelines and platform. You are an expert in various data processing technologies and data stores, appreciate the value of clear communication and collaboration, and devote to continual capacity planning and performance fine-tuning for emerging business growth. As the Senior Data Engineer, you will be designing and building inventory intelligence data pipelines and application platform services that power business decisions.
What will you do?
Architect and build inventory intelligence data pipelines and platform that can parse raw data algorithmically from different data sources, and deliver quality real-time analytical reports for all our replenishment team and our business analytics
Develop clean, safe, testable and cost-efficient solutions; Build fast and reliable pipeline, platform with underlying data model that can scale according to business needs and growth
Work with backend engineers to create services that can ingest and supply data to and from external sources, provide data streaming solutions and ensure data quality and timeliness
Work with product manager to translate business requirements into scalable solutions, prioritize workload and deliver quality and functional products on a timely manner that can grow over time
Make well-informed decisions with deep knowledge of both the internal and external impacts to teams and projects
Understand the system you are building, foresee shortcomings ahead of time and be able to resolve or compromise appropriately
What are we looking for?
Excellent understanding of data structures algorithms and at least 4 years of experience in distributed systems
Knowledge of common design patterns used in Big Data processing
Strong development experience using programming languages: Scala, Java, C++, Python
Proficiency in big data technologies: Spark, Hadoop, Flink, Hive
Proficiency in Streaming technologies: Apache Kafka, Kafka Streams, KSQL, Spark, Spark Streaming is desired
Experience with and deep understanding of traditional, NoSQL and columnar databases such as Oracle, MySQL, PostgreSQL, DynamoDB, Redshift, Vertica
Knowledge and experience in designing and developing data modeling & mining, ETL, data warehouse, deployment and infrastructure management, and performance tuning
Experience in partnering with architects, engineers in data environments that are complex, enterprise wide, multi-tenant, and host large scale of data
Ability to build systems that balance scalability, availability and latency while solving different problems
Advocator of continual deployment and automation tools that can help improve the lives of our engineers
A good communicator and team player who has a proven track record of building strong relationships with management, co-workers and customers.
A desire to learn and grow, push yourself and your team, share lessons with others and provide constructive and continuous feedbacks, and receptive to feedback from others
Tryouts are open at Fanatics! Our team is passionate, talented, unified, and charged with creating the fan experience of tomorrow. The ball is in your court now."
24,AWS Systems Engineer,"Englewood, CO 80113",Englewood,CO,80113,None Found,None Found,None Found,None Found,None Found,"
Be available to work onsite out of our American Fork, UT or Englewood, CO offices
Have a 4-year college degree in Computer Science / Information Technology, master’s degree is preferred or equivalent professional experience
3+ years experience in AWS.
5+ years experience in Hadoop.
5+ years experience with Java programming.","Sling TV L.L.C. provides an over-the-top (internet delivered) television experience on TVs, tablets, gaming consoles, computers, smartphones, smart TVs and other streaming devices. Distributed across a variety of strategic device partners, including Google, Amazon, Apple TV, Microsoft, Roku, Samsung, LG, Comcast, and many others, Sling TV offers two primary domestic streaming services that collectively include more than 100 channels of top content. Featured programmers include Disney/ESPN, NBC, AMC, A&E, EPIX, NFL Network, NBA TV, NHL Networks, Pac-12 Networks, Hallmark, Viacom, and more. For Spanish-speaking customers, Sling Latino offers a suite of standalone and extra Spanish-programming packages tailored to the U S. Hispanic market. And for those seeking International content, Sling International currently provides more than 300 channels in 20 languages (available across multiple devices) to U.S. households.

Sling TV is the #1 Live TV Streaming Service Sling TV is a next-generation service that meets the entertainment needs of today’s contemporary viewers. Visit www.Sling.com. We are driven by curiosity, pride, adventure, and a desire to win – it’s in our DNA. We’re looking for people with boundless energy, intelligence, and an overwhelming need to achieve to join our team as we embark on the next chapter of our story.

Opportunity is here. We are Sling.
Basic Requirements:
A successful Big Data Engineer will:
Be available to work onsite out of our American Fork, UT or Englewood, CO offices
Have a 4-year college degree in Computer Science / Information Technology, master’s degree is preferred or equivalent professional experience
3+ years experience in AWS.
5+ years experience in Hadoop.
5+ years experience with Java programming.

Technologies in our environment:
Here are some of the key technologies that make up our environment. While we do not expect you to have a detailed understanding of each, the more of these you are familiar with the better.

GoLang, Java, Python, JavaScript, Type Script
Automated testing of applications & Continuous Integration / TDD / BDD
Confluent Stack / Kafka / ELK Stack / Couchbase / Cassandra / PostGreSQL / Elasticsearch
Cloud Native tools: Kubernetes / Docker / Rancher / Consul / Vault / Salt Stack / Jenkins / Terraform / AWS / Jaeger / gRPC / Istio / Calico / Envoy
Big Data tools: Kestrel / Storm / Spark / Apache Drill / Hive / Phoenix / MapReduce / Yarn / Pig / Hive / HDFS / HBase
CI / CD & DevOps Culture
12 Factor Applications

#LI-SLING2
About the position
Our mission is to build the next generation, web scale platform for SlingTV. Our environment is…

Complex
Highly elastic
Based on some of the latest and greatest cloud native technologies
Very fast paced
Your team will be…

Building the AWS based enterprise data lake for the organization
Driving a customer centric, highly personalized approach to the evolution of our platform
Delivering microservices into a Kubernetes based, web scale environment
Delivering software in a SAFe based agile environment, continuously
In order to be successful in this role, you will need to be…

Highly motivated, driven, hardworking and open to learning new things
Not afraid to fail
Capable of architecting and leading the implementation of an enterprise data lake in AWS
Helping drive the big data teams’ strategic goals
Able to build highly available, best practice based & horizontally scalable solutions
Assisting in performance monitoring and capacity planning
Driving a data driven organizational culture
A team player. We have a great group of diverse folks working together in harmony. Big egos and “super heroes” need not apply."
25,Google Data Engineer,"Denver, CO 80203",Denver,CO,80203,None Found,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
",DevOps on an GCP platform. Multi-cloud experience a plus.,None Found,None Found,"Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet today’s high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on GCP and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security (Cloud IAM, Data Loss Prevention API, etc)Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutionsMinimum of 3 years of hands-on experience in GCP and Big Data technologies such as Java, Node.js, C##, Python, PySpark, Spark/SparkSQL, Hadoop, Hive, Pig, Oozie and streaming technologies such as Kafka, Stream Ingestion API, Unix shell/Perl scripting etc.
Extensive experience providing practical direction with the GCP Native and Hadoop ecosystem
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Experience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps – Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plus
Understanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus


Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
26,Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,"Minimum of a Bachelor’s degree in Computer Science, MIS or related degree and five (5) years of relevant development or engineering experience or combination of education, training and experience.Expert/Advanced level experience with ETL Tools, ODI preferably.Expert Level experience with Oracle as a Database Platform.Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases.Experience or understanding of programming languages like Python, Java, R etc.Experience or understanding of Cloud Data Platforms a plus.Strong understanding of Data Warehousing concepts.Financial Services Industry knowledge is a plus.May occasionally work a non-standard shift including nights and/or weekends and/or have on-call responsibilities.",None Found,"Builds scalable and reliable Data Integration solutions which are flexible, scalable and elastic.Develops low latency Data Integration solutions to provision data near real time for multiple consumers.Collaborates with Data Engineers, Data Architects and Service developers to build optimal and efficient ETL and Database code.Produces dynamic, data driven solutions to support the strategic business goals.Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute to meet business objectives.Help develop an enterprise scale Data WarehouseDesign and develop new systems and tools to enable stakeholders to consume and understand data fasterSupports ETL processing.Provides on-call support of Data Integration processes on a rotating basis and other on-call as required.Produces dynamic, data driven solutions to support the strategic business goals.Performs other duties and responsibilities as assigned.",None Found,None Found,"Description

About the role:
Data Engineer works in the Data Engineering team and has primary responsibility for building Enterprise Data Integration solutions by working on enterprise class data integration initiatives. The Data Engineer will be responsible for building solutions which are flexible, performant and scalable. In this role, you are the primary resource on the most complex or escalated issues and may provide direction, guidance and mentoring to team members. You will apply specialized business knowledge, technical skills and creativity to significant deliverables and projects that involve multiple IT departments and business units which have enterprise scope. The Data Engineer should be able to explore newer technology options, if need be, and must have a high sense of ownership over every deliverable.
Responsibilities:
Builds scalable and reliable Data Integration solutions which are flexible, scalable and elastic.Develops low latency Data Integration solutions to provision data near real time for multiple consumers.Collaborates with Data Engineers, Data Architects and Service developers to build optimal and efficient ETL and Database code.Produces dynamic, data driven solutions to support the strategic business goals.Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute to meet business objectives.Help develop an enterprise scale Data WarehouseDesign and develop new systems and tools to enable stakeholders to consume and understand data fasterSupports ETL processing.Provides on-call support of Data Integration processes on a rotating basis and other on-call as required.Produces dynamic, data driven solutions to support the strategic business goals.Performs other duties and responsibilities as assigned.

Qualifications
Minimum of a Bachelor’s degree in Computer Science, MIS or related degree and five (5) years of relevant development or engineering experience or combination of education, training and experience.Expert/Advanced level experience with ETL Tools, ODI preferably.Expert Level experience with Oracle as a Database Platform.Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases.Experience or understanding of programming languages like Python, Java, R etc.Experience or understanding of Cloud Data Platforms a plus.Strong understanding of Data Warehousing concepts.Financial Services Industry knowledge is a plus.May occasionally work a non-standard shift including nights and/or weekends and/or have on-call responsibilities.
Licenses/Certifications:
None required"
27,Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,"
Must have a solid understanding of data engineering, integration, and warehousing concepts and patterns.
Must have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem
You’re proficient with Linux operations and development, including basic commands and shell scripting
You can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices
Must be fluent in Python, R, and Java
Must have excellent experience command of SQL
Must have good experience and knowledge with Data Modeling concepts.
You have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills",None Found,None Found,None Found,None Found,"Qualifications:
Must have a solid understanding of data engineering, integration, and warehousing concepts and patterns.
Must have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem
You’re proficient with Linux operations and development, including basic commands and shell scripting
You can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices
Must be fluent in Python, R, and Java
Must have excellent experience command of SQL
Must have good experience and knowledge with Data Modeling concepts.
You have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills
Preferred Qualifications:
Must have 3 -5 years of experience building data productionalized pipelines.
Must have strong experience ingesting huge volumes of structured and unstructured data both in streaming and batch ingestions patterns.
2 - 4 years of Cloud development experience with AWS and or Azure stack.
Exposure with and have solid experience with statistical analysis and machine learning libraries
Must have previous experience with NoSQL database implementations
You understand the fundamentals of lambda architectures and serverless. applications
Must be proficient in Tableau
Must be comfortable with leveraging ETL tools, like Informatica.
You are proficient in Scala or Node.js
You have a master’s degree in a quantitative field
Job Description:
Partner with data scientists, analytics engagement managers, and other data engineers to discover, collect, cleanse, and refine the data needed for analysis and modeling
Analyze large data sets to extract actionable insights and inform experimental design and model development
Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build models using basic statistical and machine learning techniques, partnering with data scientists for education and guidance
We’re looking for an engineer that takes ownership in their work, has a strong focus on quality, and enjoys working in a collaborative environment.
Working Conditions
Office environment"
28,Lead Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"As a Senior Data Engineer, here's what we'll be looking for you to bring:


Hands-on Engineering Leadership
Proven track record of Innovation and expertise in Data Engineering
Tenure in coding, architecting and delivering complex projects
Deep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others
Deep understanding of streaming data architectures and technologies for real-time and low-latency data processing
Deep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies
Understanding of how to architect solutions for data science and analytics such as productionizing machine learning models and collaborating with data scientists
Understanding of agile development methods including: core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Passion for software craftsmanship
A rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..
Strong stakeholder management and interaction experience at different levels

There's no typical day or engagement for our Senior Data Engineers. Here's what you'll do:


Be the SME. Develop modern data architectural approaches to meet key business objectives and provide end to end data solutions
You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems.
On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.
It could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.
Whatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.
You have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.
You recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.

A few important things to know:
-------------------------------

Projects are almost exclusively on customer site, so candidates should be flexible and open to extensive travel.

Candidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.

Not quite ready to apply? Or maybe this isn't the right role for you? That's OK, you can stay in touch with AccessThoughtWorks ( https://www.thoughtworks.com/careers/access?utm_source=apply-jobs&utm_medium=jd&utm_campaign=access-thoughtworks ), our learning community (click ""contact me about recruitment opportunities"" to hear about jobs in the future).

It is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.

#LI-NA"
29,Data Engineer - Robotic Systems,"Boulder, CO",Boulder,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"BS/MS/PhD in Engineering or Computer Science.Experience working in Python, C++, or other Object-Oriented languages.Experience with database schema design and database administration (both No-SQL and SQL a plus)Basic linux system administration skills.Excellent communicator able to act as a bridge between multiple engineering groups.Familiarity with fast growing companies and the associated deadlines and high-energy culture that goes along with launching new products.

Recently acquired by Amazon Robotics, Canvas Technology is using spatial AI to provide end-to-end autonomous delivery of goods. By using state-of-the-art cameras and other sensors, the system perceives its surroundings with unrivaled vision and fidelity. Vehicle fleets combine a mix of high-performance sensors with simultaneous localization and mapping software that continuously builds and refines maps in real-time. The vehicles have the capability to ‘see’ and identify different objects, people, other vehicles, and places as they perform their work in a dynamic environment.

Work with a world-class team and help develop one of the most advanced 3D computer vision systems in the world. You'll be a key contributor to our top-tier computer vision team, have a huge impact in a developing sector and see your research come to life building indoor and outdoor autonomous vehicles.

We are seeking a talented and highly self-motivated Data Engineer to help stand up a modern data analytics pipeline focused on autonomous vehicle data. We are passionate about building a highly scalable big data system to support our analytics, machine learning, and data applications. A successful candidate should have a background in business analytics, data science, data visualization, and data engineering.

Responsibilities:
Lead the development of a suite of backend extract-translate-load (ETL) applications to aggregate large amounts of vehicle and ground station data.Define the data infrastructure architecture to store large quantities of data and serve higher level consumers of this data.Define the frontend framework to provide a data visualization user interface to serve the engineering community.Work with the software quality and development teams to build targeted analytics applications to help drive autonomous vehicle development and field deployment
Amazon is an equal opportunity employer and encourages women and minorities to apply.

Proven track record standing up and scaling a backend/frontend data analytics toolchainExperience working with AWS Big Data Technologies (EMR, Redshift, S3)Experience with the Python data stack for data cleansing, analysis, and visualization.Experience working with Open Source Big Data tools (Parquet, Spark, Hadoop, Presto)·Experience with Apache Airflow for implementing complex ETL pipelines.Experience with both SQL and No-SQL database administration.Working knowledge of web frontend technologies including javascript and CSS.Basic statistics, statistical signal processing, and statistical data visualization.
Familiarity with modern CI/CD toolchains including source control, issue tracking, automated test, and automated deploy.
#canvas #canvastech #canvastechnology#canvas #canvastech #canvastechnology"
30,Sr Data Engineer,"Denver, CO 80221",Denver,CO,80221,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description Summary
We are looking for Data/Machine Learning engineers at all levels to help us build a robust and scalable data platform to support AI/ML data pipelines, reporting and data analysis as our business scales. We use cloud native (AWS) cutting-edge technologies like Spark, Kinesis/Kafka Streaming, Graph , infrastructure as code, CI/CD to deliver high-quality data solutions to analysts, data scientists, and partners. We’re looking for an engineer that takes ownership in their work, has a strong focus on quality, and enjoys working in a collaborative environment.
Job Description
Responsibilities
Apply proven expertise and build high-performance scalable data warehouses
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Securely source external data from numerous partners
Intelligently design data models for optimal storage and retrieval
Deploy inclusive data quality checks to ensure high quality of data
Optimize existing pipelines and maintain of all domain-related data pipelines
Ownership of the end-to-end data engineering component of the solution
Collaboration with the Data Center SMEs, Data Scientists, and Program Managers
Support on-call shift as needed to support the team
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data
Minimum Qualification
Bachelor’s degree or Maser degree in Computer Science, Software Engineering, or related field
5 + years of SQL (Oracle, Vertica, Hive, etc.) experience and relational databases experience (Oracle, MySQL)
5 + years of experience in custom or structured (i.e. Informatica/Talent/Pentaho) ETL design, implementation and maintenance
5 + years’ experience in data engineering, experience in applying DWH/ETL best practic
5 + years of Java and/or Python development experience
5 + years of experience in LAMP and the Big Data stack environments (Hadoop, MapReduce, Hive)
5 + years of experience working with enterprise DE tools and experience learning in-house DE tools
3+ years exp in AWS data solutions stack – EMR, S3, redshift, Kinesis, ECS, Docker
3+ years exp in CI/CD stack – Jenkins , Git
Preferred Qualifications
Master’s degree or Bachelor degree in a related field.
Cloudera Administrator certification.
Working Conditions
Office environment."
31,Sr Engineer Data,"Greenwood Village, CO",Greenwood Village,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Looking to make a real difference?
You belong right here.
Come build a rewarding career helping others achieve their financial dream at an organization that values your own long-term success. With your unique talents, you have what it takes to be bold and brilliant in everything you do and reach new heights for a company dedicated to diversity and inclusion, community and you.
If you share that belief, this is where you belong.
Join our team of nearly 6,000 associates across 40 different locations worldwide and start your future today.
Empower-Retirement is looking for an experienced Sr Data Engineer with experience in implementation or leading AWS data ecosystems, pipelines, and transformations. The Sr Data Engineer will be responsible for building, maintaining, and implementing ETL and ELT pipelines and database structures. The Sr Data Engineer will be responsible the data lifecycle and will partner with partner with Architects to complete database design. The Sr Data Engineer works with the business units, internal technology full stack teams, data analytics teams, and data scientists in order to understand and aid in the implementation of database requirements, analyze performance, and troubleshoot any existent issues.
DUTIES / RESPONSIBILITIES / ESSENTIAL FUNCTIONS:
Create content maps (decomposition of Tableau reports and data sources)
Analyze data discrepancies to determine root cause and identify correct course of action
Document data lineage (source to target), data transformations, business logic, calculations, data dictionary
Validate data and transformations throughout the entire data lifecycle
Validate expectations and accuracy of the data with business and development leaders
Provide guidance on automated data QA checks throughout data lifecycle
Revamp/optimize ETL scripts/processes and reporting database
Design, construct, install, test and maintain data management systems.
Build high-performance algorithms, predictive models, and prototypes.
Ensure that all systems meet the business/company requirements as well as industry practices.
Integrate up-and-coming data management and software engineering technologies into existing data structures.
Develop set processes for data mining, data modeling, and data production.
Create custom software components and analytics applications.
Research new uses for existing data.
Employ an array of technological languages and tools to connect systems together.
Collaborate with members of your team (eg, data architects, the IT team, data scientists) on the project’s goals.
Install/update disaster recovery procedures
Recommend different ways to constantly improve data reliability and quality
Train and mentor others on the team on best practices, techniques, and languages
EDUCATION:
Bachelor’s degree in computer science, software/computer engineering, applied mathematics, or physics statistics.
OTHER PREFERRED QUALIFICATIONS:
4+ years working experience in data engineering, data warehousing, data integration or business intelligence
4+ years of experience managing, debugging, and optimizing databases that are critical to the business’s mission.
Strong experience with Python, Postgres, and MySQL writing data transformation jobs
Strong working and conceptual knowledge of building and maintaining physical and logical data models
Excellent communication skills with the ability to collaborate with non-technical partners
Experience in cloud-based data engineering in AWS Cloud, EC2, CloudFrontRoute5, API Gateway, CloudWatch, CloudTrail, CloudFormation, RDS, Redshift, DynamoDB, S3, Aurora
Experience working in AWS: Glue, Lambda, Step, EMR, Data Pipeline, Kenisis, Athena, Quicksight strongly preferred
Experience with Business Intelligence, end-to-end implementation and requirements gathering, in Tableau, Power BI, Looker, Databricks, Alteryx, or Snowflake in strongly desired
Familiarity with Scala, Java, .Net
Experience working in an Agile environment working on a Scrum team
Strong understanding of concepts or experience with ETL and ELT, Data piplines, Data Warehousing and Data Marts, Big Data and Data Lakes, Kimball Vs. Inmon architecture patterns, TDD, Source control, CI/CD, DevOps"
32,Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,None Found,None Found,"
Must have a Data warehouse/Big Data background
3+ years experience with Hadoop
3+ years experience with Java
3+ years SQL experience
2+ years experience in AWS services (EMR, Glue, S3, Lambda, ect.)
The ability to build CI/CD pipelines with large data sets and cloud technologies
","Qwinix Technologies is seeking a Data Engineer to assist our client in leveraging Big Data frameworks, optimizing data pipeline architecture, and assisting with cloud strategy!

Requirements:
Strong candidates will have recent experience with the following:


Must have a Data warehouse/Big Data background
3+ years experience with Hadoop
3+ years experience with Java
3+ years SQL experience
2+ years experience in AWS services (EMR, Glue, S3, Lambda, ect.)
The ability to build CI/CD pipelines with large data sets and cloud technologies

Bonus Points!


MapReduce framework experience!
Oozie workflows experience!
Hbase experience!

[https://www.linkedin.com/in/bethany-delaney-307180153/]"
33,Engineer II - Data Engineer (667467),"Boulder, CO",Boulder,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Who We Are:

Ball Aerospace pioneers discoveries that enable our customers to perform beyond expectation and protect what matters most. We create innovative space solutions, enable more accurate weather forecasts, drive insightful observations of our planet, deliver actionable data and intelligence, and ensure those who defend our freedom go forward bravely and return home safely. For more information, visit http://www.ball.com/aerospace, Facebook or Twitter.


Qualifications:


Engineer II – Data Engineer
Ball Aerospace is looking for an experienced data engineer to join the Mission and Process Analytics (MPA) team.
What you’ll do:

Be a member of a dynamic data analytics team; a team tasked with being a champion for data-driven decisions throughout the enterprise.
Collaborate with other team members on projects that answer key stakeholder questions.
Discover, blend, transform and interpret data from disparate sources.
Work with a variety of systems to integrate data and build out a data model/semantic layer for use in projects.
Design, implement, and deploy dashboards and other reports for consumption by the business community.
Assist in the operationalizing of statistical models, software algorithms, and re-usable components to mature the MPA toolkit.
Partner with internal business stakeholders to gather requirements and develop solutions that provide access to information.
Work closely with the Information Technology organization in the development of data solutions.
Maintain a regular and predictable work schedule.
Establish and maintain effective working relationships within the department, the Strategic Business Units, Strategic Support Units and the Company. Interact appropriately with others in order to maintain a positive and productive work environment.
Perform other duties as necessary.
What you’ll need:

BS degree or higher in Computer Science or a related technical field is required plus 5 or more years related experience.
Each higher-level degree, i.e., Master’s Degree or Ph.D., may substitute for two years of experience. Related technical experience may be considered in lieu of education. Degree must be from a university, college, or school which is accredited by an agency recognized by the US Secretary of Education, US Department of Education.
A passion for solving problems and answering the enterprise’s data questions.
Strong collaboration skills.
Attention to detail, cost conscious mindset, self-motivation and go-getter attitude is required
In-depth knowledge of SQL and data integration tools.
Experience with data virtualization tools and concepts.
Experience with data visualization tools and the development of dashboards and reporting solutions; tools like Tableau, PowerBI, Business Objects Web Intelligence.
Experience with software development – methodologies and writing code in R, Python, JavaScript, or C#
Understanding of data warehousing concepts and ETL tools is desired.
Working Conditions:

Work is performed in an office, laboratory, production floor, or clean room, outdoors or remote research environment.
May occasionally work in production work centers where use of protective equipment and gear is required.
May access other facilities in various weather conditions.
Travel and local commute between Ball campuses and other possible non-Ball locations may be required.

Relocation:

Relocation for this position is Available


EEO Statement:

US CITIZENSHIP OR PERMANENT RESIDENCY MAY BE REQUIRED

Ball Aerospace is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status."
34,Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Your Mission

Establish Pie as the preeminent commercial insurance among small business owners by establishing a best in class data architecture as a data engineer in this startup environment.

How You’ll Do It

As a data engineer with Pie, you will work with our data architect to Pie-oneer our data environment. This individual will be a key member that will work directly data architect to define the future state of our data architecture. This role will work in data architecture, data analytics, ETL development, and data reporting.

Success in this position will be establishing how data comes into and flows through the Pie insurance platform. This data will be used to help our organization quote customers based on data on best policy and prices for their workers compensation insurance.
The Day to Day
Work with current data architect to establish the Pie Insurance data platform as a best in class offering.Manage various priorities and not be afraid when priorities change.
Work fast, but have an attention to detail so key pieces are not missed

Whether It’s Right For You

You will be joining our team in downtown Denver, specifically LoDo within walking distance from Union Station. Everything we do is connected, but we each have different roles. That means we need you to be an analytical thinker with a strong data background. We are a start-up. All hands and minds are needed.

Our team is looking for an experienced data engineer. As our data environment grows, we will utilize machine learning and data analytics to further extend Pie's industry advantage. We expect you’ll have spent at least 3 years in the data warehouse and/or data analytics space. Of course, you’ll also need certain skills and abilities to do the work.
The Right Stuff
3-5 years working in data as an engineer. Building data solutions for a company who uses data as a primary part of their business.
Experience in data warehouse and/or data analytics. Qualified candidates may also come from a strong database skillset involved in analytics architecture
Strong experience in writing complex SQL queries
Strong experience in ETL/ELT platforms is strongly preferred
Exposure to one major SQL RDBMS or analytics database. (Snowflake, Redshift, MySQL, Postgres, Oracle, SQL Server, etc.)
Big Data and Business Intelligence exposure would help in the success of this role.
Experience with machine learning, predictive analytics, or other data science techniques is an added bonus.
Working at Pie
You expect to challenge others and to be challenged.
Your communication style is candid and compassionate.
You share your knowledge. We aren’t here to compete with one another; we are in it to win it, together.
You share our passion for the small business community.
Why Pie
We're building Pie for the next generation of small businesses. Our team is on a mission to make workers’ compensation less expensive, simpler, and more transparent. Easy as pie, in fact.

Founded in 2017, Pie is a fast-growing insurtech startup with home offices in Washington, DC, and Denver, CO. Pie offers an incredible career opportunity where you can use your expertise to disrupt an industry and serve the SMB community. Invest your time doing meaningful work in a fast-paced environment where your contribution will have an immediate impact.

Pie, backed by trusted global insurer Sirius Group, continues to expand nationwide in 2019 as we scale our team and operations. Want a slice? In return for your commitment to us, you’ll get a piece of Pie in the form of stock options. At Pie, we’re invested in you, and you in us."
35,Data Engineer,"Denver, CO 80221",Denver,CO,80221,None Found,"
Bachelor’s degree in a technical field (e.g. Comp Science, Math, Engineering) or related experience
2+ years of collective experience in data engineering, data analysis, data warehousing, data integration or business intelligence, in a similarly sized organization
2+ years of experience architecting, building and administering big data and real-time streaming analytics architectures in both on premises and cloud environments (AWS, Azure, Google) leveraging technologies such as Hadoop, Spark, S3, EMR, Aurora, DynamoDB, Redshift, Neptune, Cosmos DB
1+ years of experience architecting, building and administering large-scale distributed applications
1+ years of experience with Linux operations and development, including basic commands and shell scripting
2+ years of experience with execution of DevOps methodologies and Continuous Integration/Continuous Delivery within a large scale data delivery environment
Software development experience in least two or more of following languages: Java, Python, Scala, Node.js
Expertise in usage of SQL for data profiling, analysis and extraction",None Found,"
Working collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:
Architect, build and support the operation of our Cloud and On-Premises enterprise data infrastructure and tools
Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks and applications required to expand our platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage",None Found,None Found,"Job Description Summary
At Transamerica, we are innovating the next generation of data solutions and capabilities to help our customers achieve a lifetime of financial security. As part of the Data Engineering team in our Enterprise Data Services group, you will apply your engineering skills and passion in developing modern architectures to enable our data-driven digital business.
Job Description
Data Engineers are responsible for the design, architecture and support of the systems, services and applications required for the collection, storage, processing, and analysis of all forms of data in order to enable data-driven decisions and outcomes within the organization.
Responsibilities:
Working collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:
Architect, build and support the operation of our Cloud and On-Premises enterprise data infrastructure and tools
Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in selection and integration of data related tools, frameworks and applications required to expand our platform capabilities
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage
Qualifications:
Bachelor’s degree in a technical field (e.g. Comp Science, Math, Engineering) or related experience
2+ years of collective experience in data engineering, data analysis, data warehousing, data integration or business intelligence, in a similarly sized organization
2+ years of experience architecting, building and administering big data and real-time streaming analytics architectures in both on premises and cloud environments (AWS, Azure, Google) leveraging technologies such as Hadoop, Spark, S3, EMR, Aurora, DynamoDB, Redshift, Neptune, Cosmos DB
1+ years of experience architecting, building and administering large-scale distributed applications
1+ years of experience with Linux operations and development, including basic commands and shell scripting
2+ years of experience with execution of DevOps methodologies and Continuous Integration/Continuous Delivery within a large scale data delivery environment
Software development experience in least two or more of following languages: Java, Python, Scala, Node.js
Expertise in usage of SQL for data profiling, analysis and extraction
Preferred Qualifications:
Master’s Degree in a technical field (e.g. Comp Science, Math, Engineering) or related experience
1+ years of experience with advanced analytics and machine learning concepts and technology implementations (Tensorflow, H20)
2+ years of experience with NoSQL implementations (Mongo, Cassandra, HBase)
3+ years of experience in implementing serverless architecture leveraging AWS Lambda or similar technology
1+ years of experience with data visualization tools such as Tableau and PowerBI
Solid understanding of the Hadoop ecosystem (e.g. HDFS, MapReduce, HBase, Pig, Scoop, Spark, Hive)
Solid understanding of the Hadoop ecosystem (e.g. HDFS, MapReduce, HBase, Pig, Scoop, Spark, Hive)
2+ years of experience with data warehousing architecture and implementation, including hands on experience developing ETL (Informatica, SSIS, etc.)
Relevant technology or platform certification (AWS Certified, Microsoft Certified)
Behavioral & Leadership Competencies:
Attention to detail and results oriented, with a strong customer focus
The ability to work within a team environment
Problem-solving and effective technical communication skills
Meet tight deadlines, multi-task, and prioritize workload
Willing to learn and keep pace with the latest advances in the related field (Grasp new technologies rapidly as needed to progress varied initiatives )
Our Culture:
At Transamerica we promote a Future Fit mindset. What is a Future Fit mindset?
Acting as One fosters an environment of positive collaboration
Accountability allows us to own the problem as well as the solution
Agility inspires new ideas, innovation and challenges the status quo
Customer Centricity encourages an above and beyond approach to our customer
Working Environment:
Office environment
Due to the nature of the role, work outside of normal business hours may be required as needed
Occasional travel less than 10%"
36,Data Engineer (Imagine Analytics),"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,"
You will be responsible for architecting, building and maintaining data pipelines, storage systems, analysis flows, and procedures to support IA’s product objectives and business goals.
You should be comfortable working in a fast-paced environment with quick changes and a high degree of uncertainty.
A high level of proficiency in tools and architectures for building modern data infrastructure is a must, as is a desire to learn quickly to adapt to changing technologies and business challenges.
As a member of the engineering team, you will help guide both the technical and usability aspects of the software we build and will be involved all stages of the cycle of iterative product development.
You’ll work with partners and stakeholders to analyze data sources and gather requirements, with designers to define user flows and look and feel, with other engineers to implement functionality, and with customers to provide support and gather feedback.
You will have a high degree of freedom to pick the tools and frameworks that you feel best address the problems at hand and will be responsible for managing and maintaining those technology investments over their entire lifetime.",None Found,None Found,"Why Join IMA?
Imagine a Company…
…that is embarking on a journey to rethink how 300-year-old processes can be engineered to be simple, easy and inspired by the best of consumer tech. A company that looks for the best athletes but loves the art of winning as a team. A company that knows relationships are the currency of the business, and that digitizing them, to reflect their importance today – will only make the future stronger and the industry better. A company working to limit risk every day, while risking it all.

Imagine being part of a team, where impact is tangible, your voice is part of the story and the work is industry changing. This is not imaginary, it is Imagine Analytics. Explore the frontier of insurance with us!

Working at Imagine Analytics
We are doing highly impactful work in an established industry with a lot of complexity and a rich array of interesting challenges. Learning about insurance markets and working closely with partners to identify opportunities and create efficiencies is what drives us. And we have the best of both worlds, the stability of 40 years in business with the speed and energy of a startup.

Our team is a small group of experienced designers, engineers and entrepreneurs. We believe that software development is a team sport, and that the best products are built by teams with diverse backgrounds who are empowered with a high degree of autonomy. We strive for rapid iteration and continuous improvement in both our products themselves and our approaches to building them. We work at a sustainable pace and know from experience that building a lasting product organization is a marathon, not a sprint.

We are building a greenfield product on a modern technology stack, which currently includes React/Apollo, Scala/Spark/Python, Elasticsearch, Postgres, and various serverless technologies. We iterate quickly and deploy continuously, while striving to keep the quality of our user experience high and our codebases tidy. And we have a hell of a lot of fun doing it, together.
What You’ll Do
Imagine Analytics (IA) is looking for a Data Engineer to build and maintain technology products for our mid-market insurance data platform. They will work in a highly interdisciplinary fashion with other functions within the team to prioritize, scope, design, build, deliver, and manage features and systems within IA’s product portfolio, operating with a high degree of autonomy and flexibility. The ideal candidate will have prior experience building and managing data systems, as well as working in an early-stage company environment.
Roles & Responsibilities
You will be responsible for architecting, building and maintaining data pipelines, storage systems, analysis flows, and procedures to support IA’s product objectives and business goals.
You should be comfortable working in a fast-paced environment with quick changes and a high degree of uncertainty.
A high level of proficiency in tools and architectures for building modern data infrastructure is a must, as is a desire to learn quickly to adapt to changing technologies and business challenges.
As a member of the engineering team, you will help guide both the technical and usability aspects of the software we build and will be involved all stages of the cycle of iterative product development.
You’ll work with partners and stakeholders to analyze data sources and gather requirements, with designers to define user flows and look and feel, with other engineers to implement functionality, and with customers to provide support and gather feedback.
You will have a high degree of freedom to pick the tools and frameworks that you feel best address the problems at hand and will be responsible for managing and maintaining those technology investments over their entire lifetime.
You Should Have:
Bachelor's degree, or related field or equivalent experience
2+ years of experience building data pipelines and storage systems. Experience with data warehousing and/or machine learning a plus.
Design Centered: A high degree of product sense & strong appreciation for UX
Excitement to explore new technologies and platforms that could add system value

This Job Description is not a complete statement of all duties and responsibilities comprising this position.

The IMA Financial Group, Inc. provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, The IMA Financial Group, Inc. complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities."
37,C/C++ Software Developer,"Denver, CO 80221",Denver,CO,80221,None Found,None Found,"2 or more years experience writing code using C/C++ and Unix
Basic understanding of RDBMS databases such SQL Server and Oracle.
Basic understanding of modern software design and development methodologies (e.g., OO).
Basic understanding of modern SCM (software configuration management).
Basic understanding of testing tools and unit test scripting, and testing methodologies.
Experience using (or an understanding of the use of) an Integrated Development Environment (e.g., Eclipse, Visual Studio).
Understanding of basic Database Administration.
Understanding of quality and security standards. Good verbal and written communication skills.","Participates as a member of development team.
Completes development of units with designs prepared by more senior developers.
Participates in code reviews. Prepares and executes unit tests.
Applies growing technical knowledge to maintain a technology area (e.g. Web- site Development).
May perform unit design.
Applies company and 3rd party technologies to software solutions of moderate complexity.
Configures end-user or enterprise systems designed by more senior technologists.",Typically a technical Bachelor's degree or equivalent experience and a minimum of 2 years of related experience or a Master's degree and up to two years of experience.,None Found,"Job Description:
Applies specialized knowledge to conceptualize, design, develop, unit-test, configure, and implement portions of new or enhanced (upgrades or conversions) business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies.
Responsibilities:
Participates as a member of development team.
Completes development of units with designs prepared by more senior developers.
Participates in code reviews. Prepares and executes unit tests.
Applies growing technical knowledge to maintain a technology area (e.g. Web- site Development).
May perform unit design.
Applies company and 3rd party technologies to software solutions of moderate complexity.
Configures end-user or enterprise systems designed by more senior technologists.
Education and Experience Required:
Typically a technical Bachelor's degree or equivalent experience and a minimum of 2 years of related experience or a Master's degree and up to two years of experience.
Knowledge and Skills:
2 or more years experience writing code using C/C++ and Unix
Basic understanding of RDBMS databases such SQL Server and Oracle.
Basic understanding of modern software design and development methodologies (e.g., OO).
Basic understanding of modern SCM (software configuration management).
Basic understanding of testing tools and unit test scripting, and testing methodologies.
Experience using (or an understanding of the use of) an Integrated Development Environment (e.g., Eclipse, Visual Studio).
Understanding of basic Database Administration.
Understanding of quality and security standards. Good verbal and written communication skills.
NOTE: This position can be located in El Paso, Texas, OR Conway, Arkansas."
38,US Contractor - Data Engineer (Big Data),"Louisville, CO",Louisville,CO,None Found,None Found,"
Bachelor’s degree in Computer Science, Computer Engineering or Information Technology
8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills
5 years of experience of ETL development in a big data environment
5 years working in an agile development environment.
Experience developing in an AWS environment using S3, EC2, Redshift, Glue, Athena and RDS
","
Bachelor’s degree in Computer Science, Computer Engineering or Information Technology
8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills
5 years of experience of ETL development in a big data environment
5 years working in an agile development environment.
Experience developing in an AWS environment using S3, EC2, Redshift, Glue, Athena and RDS
",None Found,None Found,None Found,"We work to solve deep technical problems that improve the world of Healthcare. These problems span a variety of core topics in computer science ranging from databases to distributed systems. We are looking for an Experienced Data Engineer for a 6-month contract (possibility to extend or convert to FTE) to join our new Data Organization.

Principle duties and responsibilities:

Leads backend and ETL development effort for the Data Team.
Designs, develops, and performance-tunes extraction, transformation, and load (ETL) processes using SQL, PySpark or Python source-to-target data mappings
Analyzing Use Case requirements and working with teammates to implement defined functionality
Provide architectural guidance and development/build standards for the team
Promoting collaboration through activities such as design sessions, design reviews, pair programming, etc.,
Required Qualifications & Skills:

Bachelor’s degree in Computer Science, Computer Engineering or Information Technology
8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills
5 years of experience of ETL development in a big data environment
5 years working in an agile development environment.
Experience developing in an AWS environment using S3, EC2, Redshift, Glue, Athena and RDS
Strong ownership, urgency, and drive to ship code
Ability to think outside the normal concepts to implement
Ability to communicate technical concepts and designs to cross functional teams and off shore teams with varying degrees of technical experience.
Displays flexibility in adapting to changing conditions.
Strong team player, makes a valuable contribution to team objectives, displays trust and mutual understanding, accepts constructive feedback, and handles confrontation constructively.
Preferred Qualifications & Skills

Application/system architecture experience
Experience with AWS Services RedShift Spectrum, Elastic Search, API Gateway and Lambda"
39,AWS Data Engineer,"Denver, CO 80203",Denver,CO,80203,None Found,"At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.","DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud",None Found,None Found," Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
§ Certified AWS Developer - Associate
§ Certified AWS DevOps – Professional (Nice to have)
§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
40,Senior Data Engineer,"Denver, CO 80202",Denver,CO,80202,None Found,None Found,None Found,None Found,None Found,None Found,"Ibotta is looking for a Senior Data Engineer to build something great with us. As part of the Data Services team, you will work with both Engineering and Analytics to develop and own stable, scalable, and repeatable data-driven features. We're looking for a self-motivated engineer who has a passion for working with an event-based architecture heavily leveraging the AWS cloud data stack & tools. The data engineering team is core to driving and delivering the current and future data, analytics, and decisioning platforms across Ibotta.

Here is what you'll be doing:

Work with engineering, analytics, and product management to implement data-driven features
Be a contributor and architect of distributed systems, frameworks, and design patterns of BI and Data Science/Machine Learning
Use Scala, Java or Python to utilize Hadoop/Spark to collect and analyze large-scale datasets in batch and real-time
Design, implement and maintain distributed messaging systems
Build, monitor, and maintain data ETL pipelines
Manage Data Governance and Security
Administer and maintain our data infrastructure
Mentor junior and mid-level data engineers in principles and best practices
Share relevant knowledge and evangelize Data Engineering with Engineering and Analytics teams

Here is what we're looking for:

Bachelor's degree in Computer Science, Engineering or a related field or equivalent work experience
5+ years of experience in software development, preferably with Scala, Java, or Python
3+ years of experience working in the Hadoop ecosystem, using tools such as Hive, Spark, or Pig
Proven expertise in taking large data projects from conception to implementation
Substantial experience with Event-driven architecture design patterns and practices
Significant experience in database design and architecture principles, and expert-level SQL abilities
Extensive experience with:
AWS DynamoDB, Hive, Cassandra, Bigtable, or other big data stores
Python and Java
Event platforms such as Kafka or Kinesis
ETL tools and processes (Airflow or other similar tools)
Agile (Kanban or Scrum) development experience

Nice to have:

Experience with managed, cloud-based data warehouses; e.g. Snowflake, Vertica, etc
Experience with BI tools; e.g. Looker, Tableau, etc
Experience with data serialization technologies, e.g. Avro, Protobuf, etc
Experience with Qubole

About Us:
Headquartered in Denver, CO, Ibotta (""I bought a..."") is a free app that's transforming the shopping experience by making every purchase rewarding. The company partners with leading brands and retailers to offer real cash back on groceries, travel, electronics, clothing, gifts, home and office supplies, dining out, and much more. Ibotta is the ultimate starting point for savings, and having paid out more than $500 million in cash rewards to its Savers, it's no surprise why Ibotta is one of the most downloaded shopping apps in the United States

Learn more about Ibotta here: https://liferewarded.ibotta.com/press-and-media/ ( https://liferewarded.ibotta.com/press-and-media/ )

Additional Details:

This position is located in Denver, CO and includes competitive pay, benefits package (including medical, dental, vision), 401k, commuter stipend, and equity.
Ibotta provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, and genetics.
Applicants must be currently authorized to work in the United States on a full-time basis.

"
41,Senior Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Responsibilities
Work collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment.
Architect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools.
Design robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data.
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications.
Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities.
Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage.

Qualifications
Bachelor’s degree in computer science, math, engineering, or relevant technical field
Four years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration, and data integration concepts and methodologies
Three years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environments
Two years of experience architecting, building, and administering large-scale distributed applications
Two years of experience with Linux operations and development, including basic commands and shell scripting
Two years of experience with execution of DevOps methodologies and continuous integration/continuous delivery
Expertise in SQL for data profiling, analysis, and extraction
Familiarity with data science techniques and frameworks
Creative thinker with strong analytical skills
Results oriented with a strong customer focus
Ability to work in a team environment
Strong technical communication skills
Ability to prioritize work to meet tight deadlines
Ability to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables
Preferred Qualifications
Master’s degree in a technical field (e.g. computer science, math, engineering)
Solid understanding of big data and real time streaming analytics processing architecture and ecosystems
Practical experience with data warehousing architecture and implementation, including hands on experience with source to target mappings and developing ETL code
Experience with advanced analytics and machine learning concepts and technology implementations
Experience with data analysis and using data visualization tools to describe data
Relevant technology or platform certification (AWS, Microsoft, etc.)
Software development experience in relevant programming languages (i.e. Java, Python, Scala, Node.js)
Working Conditions
Office environment
Occasional travel"
42,Associate Data Engineer,"Boulder, CO 80302",Boulder,CO,80302,None Found,None Found,None Found,"Development of jobs & pipelines from multiple production data sources into Data Lake environments
Engineers production ready solutions, inclusive of alerting and error handling
Works with Cloud based tools (Google GCP, Big Query, Dataproc, Composer, Steamsets, Looker, etc.) to deliver best-in-class cloud based data solutions
Works collaboratively with DBA team for operational execution and reliability of data solutions, both in Oracle and BigQuery
Assists in maintaining data governance through documentation of data solutions, through ERDs, Confluence documentation, or external tools
Engineer & model curated and keyed Data Warehouse solutions that meet business objectives that perform efficiently and effectively
Works in Agile product management method, managing tasks & objectives (user stories) through JIRA and providing updates to SCRUM master
Partners with Product Manager (PO) to understand business requirements across multiple functional areas; Store Operations, Merchandising, Supply Chain, Finance, Digital, Customer & Loyalty, Legal, & Data Science
Support current Data Warehouse ETL jobs, respond to tickets and inquiries from business partners when data quality issues occur
Other projects and duties as assigned
",None Found,None Found,"Come work for us!
We are looking for dedicated employees to join our team to help our customers have the best experience possible every time they enter a Finish Line Store.
Our employees are key to our success.
Job Summary:
Engineer, Enterprise Data Solutions performs activities related to the data foundation of Finish Line/JD Sports; including development & support of pipelines, ETL jobs and tools, data marts, data lake, and data warehouse in multiple environments. The Engineering position partners with the Product Manager to understand business requirements (user stories), and develops solutions to meet business objectives. The Engineering team (EDS) is responsible for data from production system to BI tool, including movement and transformation. This role reports to the Consulting Engineer.
Key Responsibilities and Tasks:
Development of jobs & pipelines from multiple production data sources into Data Lake environments
Engineers production ready solutions, inclusive of alerting and error handling
Works with Cloud based tools (Google GCP, Big Query, Dataproc, Composer, Steamsets, Looker, etc.) to deliver best-in-class cloud based data solutions
Works collaboratively with DBA team for operational execution and reliability of data solutions, both in Oracle and BigQuery
Assists in maintaining data governance through documentation of data solutions, through ERDs, Confluence documentation, or external tools
Engineer & model curated and keyed Data Warehouse solutions that meet business objectives that perform efficiently and effectively
Works in Agile product management method, managing tasks & objectives (user stories) through JIRA and providing updates to SCRUM master
Partners with Product Manager (PO) to understand business requirements across multiple functional areas; Store Operations, Merchandising, Supply Chain, Finance, Digital, Customer & Loyalty, Legal, & Data Science
Support current Data Warehouse ETL jobs, respond to tickets and inquiries from business partners when data quality issues occur
Other projects and duties as assigned
Required Education and/or Experience
Bachelor’s degree (B.A.) in Information Systems or other related field from a four-year college or university, or equivalent combination of education and experience. 1-3 years of proven work ability in data analytics, data engineering, and process documentation required. Must have experience with partnering with stakeholders of all levels of the organization to plan and solve problems.
Physical Demands – The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
EEOC Statement:
The Finish Line, Inc. is an Equal Opportunity Employer and is committed to complying with all federal, state, and local EEO laws. The Finish Line, Inc. prohibits discrimination against employees and applicants for employment based on the individual's race or color, religion or creed, national origin, alienage or citizenship status, marital status, sex, pregnancy status, age, military status, disability, or any other protected characteristic or class protected by law. The Finish Line, Inc. provides reasonable accommodation for disabilities in accordance with applicable laws.

Need accessibility assistance to apply?

Applicants who require accessibility assistance to submit an employment application can either call Finish Line at ( 317) 613-6890 or email us at talentacquisition@finishline.com. A member of our Talent Acquisition team will respond as soon as reasonably possible. ( This email address and phone number is only for individuals seeking accommodation when applying for a job. )"
43,Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,"
Microsoft SQL Server
PostgreSQL
MongoDB
Microsoft SSIS
BIRST BI
Valen InsureRight platform","
2+ years Data Engineering experience with TSQL, python, map reduce or functional programming
Bachelor’s degree in programming or related technical areas
Developing and supporting an end user production system
Reporting/data warehousing experience
Insurance industry knowledge or experience with insurance data a plus",None Found,"Join a high performing and rapidly growing team

Valen Analytics is a rapidly expanding advanced data and predictive analytics company headquartered in downtown Denver. Valen’s state-of-the-art analytics and predictive modeling products are built on Valen’s unique industry-wide consortium data platform, specifically designed for property and casualty insurance carriers.

Valen Analytics is looking for a Data Engineer to expand our growing data processing needs. We are looking for candidates with at least 2 years of experience, who demonstrate a curious analytical mind with ability to understand business objectives, ask insightful questions, and be detail oriented in implementation.

As a Data Engineer, you will work with customers, Valen team members, and 3rd party data providers, to develop, maintain, and enhance our data engineering capabilities in support of our data and predictive analytic offerings to the market.

Responsibilities

This position will be part of an existing team whose primary responsibilities are to identify, acquire, validate, cleanse, and produce data and datasets to be used in advanced analytics and predictive modeling initiatives by our customers and internal teams. This is accomplished by combining data processing experience with software engineering concepts into solutions that are hosted in our cloud-based platform, InsureRight.

This position will leverage the following tools and platforms:

Microsoft SQL Server
PostgreSQL
MongoDB
Microsoft SSIS
BIRST BI
Valen InsureRight platform
This position will make sure of the following skills:

Data extraction, transformation, and cleansing
Data profiling and visualization
Collaborating with data scientists, software engineers, production operations, subject matter experts and customers
Fostering continuous delivery pipelines
Managing and maintaining metadata
This position requires the ability to:

Work in a fast-paced environment as part of a small team
Collaborate with team members in the development and maintenance of our solutions
Identify opportunities to automate data engineering tasks and workflow
Fostering continuous delivery pipelines
Managing and maintaining metadata
Education & Experience

2+ years Data Engineering experience with TSQL, python, map reduce or functional programming
Bachelor’s degree in programming or related technical areas
Developing and supporting an end user production system
Reporting/data warehousing experience
Insurance industry knowledge or experience with insurance data a plus
The Valen Team
Valen’s mission is to help our clients achieve their goals and solve problems by leveraging data to make more informed decisions.

Guide Customer Success: We relentlessly pursue making our customers successful.

Live the Golden Rule: We treat our customers, employees, vendors and shareholders how we expect to be treated as customers, employees, vendors and shareholders…period.

Be Agile: Valen is a test and learn environment. We organize everything we do around our customer’s success to provide something of value quickly. We learn and then adapt. Then, we learn some more.

Have Fun: We have great attitudes and we have fun. We do not take ourselves too seriously, we celebrate our successes and we enjoy our work. Most of all, we live passionately.

Embrace Simplicity: We endeavor to make everything we provide our customers ridiculously easy.

Expect Ownership: At Valen we take responsibility for our actions and we build trusting relationships by making and meeting our commitments."
44,Lead Data Engineer,"Denver, CO 80202",Denver,CO,80202,None Found,"
Data modeling and corporate-level data management experience
Previously worked with Windows Workflow Foundation, Windows Workflow Designer and Windows Presentation Foundation technologies
Experience with Elasticsearch, Aurora, MySQL, Spark, Streamsets, Kafka, Python, and Scala is a plus",None Found,None Found,None Found,None Found,"As a Lead Data Engineer on Healthgrades' Facility Data as a Service team, you will develop and maintain database functionality to support corporate products and services, focused on mastering healthcare claims data for consumption across Healthgrades' products. You will collaborate with multi-functional database and product development teams using Agile / Scrum, SQL Server, .Net and Open Source technology.
In this role, you will implement database technologies and development processes to support database development for a Data Platform that is changing the game. On this product, we are currently transitioning our Microsoft technology stacks to other Open Source technology stacks, so experience or interest in learning those tools is a plus. If you are passionate about growing your expertise in these technologies, this will be a great opportunity for you.

What You Will Do:
Oversee day-to-day operational matters, provide training, and supervise performance related to company and individual OKRs
Lead and manage complex development projects including plans, designs, technical leadership, schedules, and resource allocation while also being a hands-on engineer
Build and maintain complex T-SQL statements that perform efficiently against large data sets
Performance tune large Data Warehousing platforms, with a focus on schema bound views, database partitioning, and etl/query optimization
Build and maintain Windows Workflow Engine Packages and Models using Windows Workflow Foundation
Create and maintain automated ETL processes with special focus on data flow, error recovery, and exception handling and reporting
Create and modify data models and implementations as they relate to RDBMS, DW, and BI
Design of specialized data structures for the purpose of data consumption by a public facing website and/or Business Analytics data visualization
Load, process and migrate incoming data feeds and create outgoing data extracts
Create and maintain documentation to support developed applications

What You Will Bring:
Ability to participate in a culture of communication, collaboration and creativity
Previous experience in a lead or management role with direct reports
Strong RDBMS and Microsoft SQL Server 2014/2016 skills
SQL Programming / ETL and data architecture management experience
Experience building and maintaining database structures, ETL processes, stored procedures, audit reports, data extracts, SSIS, SSAS, SSRS, etc. to meet project objectives
Perform Unit Tests and internal QA checks to insure high quality work
Good collaboration and idea sharing in team environment
A Bachelor’s Degree in related field or equivalent experience

Preferred Qualifications:
Data modeling and corporate-level data management experience
Previously worked with Windows Workflow Foundation, Windows Workflow Designer and Windows Presentation Foundation technologies
Experience with Elasticsearch, Aurora, MySQL, Spark, Streamsets, Kafka, Python, and Scala is a plus

Why Healthgrades?
At Healthgrades, we recognize that our people drive our greatest achievements. We are passionate about maintaining a fulfilling, rewarding and high-energy work environment while setting the stage for your continued success.
Meaningful Work – empowering consumers with data to make the right decisions for themselves and their families
Changing the Game - evolving, dynamic culture with career advancement opportunities
Community Builders- participating in local charity organizations and wellness initiatives
Robust Perks – generous PTO, 401k contributions, tuition assistance, entertainment discounts & more!"
45,Data Engineer - Node,"Denver, CO 80246",Denver,CO,80246,None Found,"
Experience with databases SQL and NoSQL
Deep knowledge of Node, JavaScript,
Experience with scaling backend of large-scale applications
Algorithms
Positive attitude
Curious, eager to learn, and colaborate
",None Found,None Found,"Required: Bachelor’s degree in Computer Science or equivalent work experience
Preferred: Master’s degree",None Found,"Data Engineer – Node [Denver, CO]
Become a part of something great at MeetingOne.

MeetingOne is a rapidly growing software development and services company focused on audio and web conferencing technology solutions. Headquartered in Denver, Colorado, MeetingOne is a full-service audio and web conferencing, e-learning, event solution and consulting services provider. Since 1999, MeetingOne has enabled businesses and educational organizations around the world to communicate more effectively, using innovative virtual meeting and event technologies and services. More info at www.meetingone.com.

This position is a great opportunity to own the core technology stack that is vital to our growing company. Learn, develop, refactor and master the heart of our product portfolio that in turn achieves high velocity in driving value to our beloved clients. Demonstrate the skills and knowledge you collected over the years to help lead our development efforts to success.

As the Software Engineer, you will be reporting to the Software Development Manager. Your focus is on owning key layers in our technology. You will have the opportunity to imagine, create and deliver leading edge solutions that meet the needs of a rapidly expanding user base. You will have the ability to influence people and process to achieve this goal.

You will have the opportunity to mentor other developers, sharing your years of experience with tactics and strategies that lead to quality coding. With senior experience, you are a development team advocate. Ensuring the right tools are available and continuous training opportunities are available.

You will work closely with QA and Product to aid in the dependable release of value. You will be responsible for handling development related items escalated with respective core technology stack.

At the end of the day you love working on the most valuable technology the company owns. You strive at every opportunity to leverage bleeding techniques and technology to express your creative side. You enjoy having an impact on co-workers and the user community a-like.

What does success look like?

You exhibit creative problem solving to achieve effective results.
You prioritize action to drive achievements that delight.
You enjoy independence to define, direct, and/or perform critical thinking to resolve complex issues.
Qualifications/ Experience:

Experience with databases SQL and NoSQL
Deep knowledge of Node, JavaScript,
Experience with scaling backend of large-scale applications
Algorithms
Positive attitude
Curious, eager to learn, and colaborate
Pluses:
Experience with audio, video conferencing, WebRTC, SFU, MCU, WebAssembly
Experience with other languages e.g. C++, LUA
Experience with IM/Presence (XMPP, Other,…)
Experience with Docker, AWS
Telecommunications knowledge
Education OR equal work experience:

Required: Bachelor’s degree in Computer Science or equivalent work experience
Preferred: Master’s degree"
46,Data Engineer,"Lone Tree, CO 80124",Lone Tree,CO,80124,None Found,None Found,None Found,None Found,None Found,None Found,"Your Opportunity
Do you want to be part of a Data Solutions Delivery team managing over 150+ terabytes of data and building the next generation analytics platform for a leading financial firm with over $3.2 trillion in assets under management? At Schwab, the Global Data Technology (GDT) organization governs the strategy and implementation of the enterprise data warehouse and emerging data platforms. We help Marketing, Finance and executive leadership make fact-based decisions by integrating and analyzing data.
We are looking for a Data Engineer who has passion for data and comes with data engineering background. Someone who has experience in designing and coding batch as well as real time ETL and one who wants to be part of a team that is actively designing and implementing the big data lake and analytical architecture on Hadoop. You will have the opportunity to grow in responsibility, work on exciting and challenging projects, train on emerging technologies and help set the future of the Data Solution Delivery team.
What you’re good at
Designing schemas, data models and data architecture for Hadoop and HBase environments
Building and maintaining code for real time data ingestion using Java, MapR-Streams (Kafka) and STORM.
Implementing data flow scripts using Unix / Hive QL / Pig scripting
Designing, building and support data processing pipelines to transform data using Hadoop technologies
Designing, building data assets in MapR-DB (HBASE), and HIVE
Developing and executing quality assurance and test scripts
Working with business analysts to understand business requirements and use cases
What you have
Minimum of 3-5 years of experience in understanding of best practices for building and designing ETL code Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required
Knowledge in schema design, developing data models and proven ability to work with complex data is required
Hands-on experience in object oriented programming (At least 2 years)
Hands-on experience with Hadoop, MapReduce, Hive, Pig, Flume, STORM, SPARK, Kafka and HBASE is required
Understanding Hadoop file format and compressions is required
Familiarity with MapR distribution of Hadoop is preferred
Understanding of best practices for building Data Lake and analytical architecture on Hadoop is preferred
Scripting / programming with UNIX, Java, Python, Scala etc. is preferred
Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required
Knowledge in real time data ingestion into Hadoop is preferred
Experience in working in large environments such as RDBMS, EDW, NoSQL, etc. is required
Knowledge of Big Data ETL such as Informatica BDM and Talend tools is required
Understanding security, encryption and masking using Kerberos, MapR-tickets, Vormetric and Voltage is preferred
Experience with Test Driven Code Development, SCM tools such as GIT, Jenkins is preferred
Experience with Graph database is preferred"
47,Senior Data Engineer,"Lone Tree, CO 80124",Lone Tree,CO,80124,None Found,None Found,None Found,None Found,None Found,None Found,"Your Opportunity
The HR Technology organization is a highly skilled and passionate team that explores new ways to demonstrate our data as a strategic asset. Through hands-on initiatives with our business partners we contribute to the company’s customer experience, revenue and profitability. Our team is looking for a highly-motivated, self-driven individual contributor well versed in database development technologies and fluent in Agile.
What you’re good at
As a member of a group with broad business impact, you will work directly with counterparts in the Incentives Administration organization and HR Technology in support of Schwab’s compensation applications. You will also work to uncover and define new needs, find appropriate solutions, and ensure implementation and delivery within specified timeframes.
Develop and code based on the “How” as defined by the technical team lead.
Work closely with fellow developers to determine sizing and effort of user stories; code user story tasks as defined by Senior Development Team Members.
Be committed to Agile Methodology.
As a developer you will be a self-driven individual contributor; provide efficiently organized and designed logic; be highly-motivated and well versed in developing easily maintainable applications.
What you have
3+ year’s experience as a Sr. Data Engineer
Significant SQL skills with the ability to write code freehand completely from scratch and understand existing complex SQL
Experience with Python and/or Perl
Demonstrated experience with data modeling and performance tuning queries
Teradata experience a strong plus
Track record of solid project execution with deliverables having minimal defects
Must be creative, critical thinker and solutions-oriented
Strong analytical and problem-solving skills
Excellent written and verbal communication skills"
48,Data Engineer - Hux,"Denver, CO 80203",Denver,CO,80203,None Found,"
4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
1+ years of experience on distributed, high throughput and low latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.",None Found,None Found,None Found,None Found,"Hux Data Engineer
Locations: New York, NY – Greensboro, NC - Chicago, IL – Raleigh, Durham, Chapel-Hill, NC - Denver, CO
What is Hux? Hux is the Human Experience Platform by Deloitte Digital.
In today’s world, customers expect companies to know who they are and what they want. Customers want to have products, services or experiences that best suit their needs delivered to them seamlessly across physical and digital channels.
Customers are human first: driven by dynamic wants, needs, and desires. The ability for brands to make personal, meaningful connections on a human level has never been greater and Hux by Deloitte Digital delivers on those experiences in a way that allows companies to own the customer journey end to end. We help companies connect key data sources to understand what matters most to people; connect to advanced technologies like AI and machine learning to sense and respond to those needs at scale; and connect their systems to unlock insights, create collaboration and drive acquisition, engagement and loyalty. Most importantly, we empower companies to connect with customers in personal, meaningful ways that respect them as people, not just customers.
Hux by Deloitte Digital gives companies the ability to build and leverage the connections – between people, systems, data and technologies – so they can deliver personalized, contextual experiences to customers at scale.

Work you’ll do
As a Hux Data Engineer, you’ll design, implement, and maintain a full suite of real-time and batch jobs that fuels our cutting edge AI to provide real-time marketing intelligence to our existing clients.
You’ll develop, test and deliver production grade code to help our clients solve their marketing challenges using cutting-edge big-data tools. You’ll also ensure data integrity, resolve production issues, and assist in the support and maintenance of our overall Platform.
As you grow your capabilities and learn how to build a platform that can ingest, load and process billions of data points, you’ll enjoy new challenges and opportunities to showcase your development skills by joining project teams to build innovative new-client platforms and execute high-value strategic development projects with high visibility.
Your responsibilities will include:
Design, construct, install, test and maintain highly scalable data pipelines with state-of-the-art monitoring and logging practices.
Bring together large, complex and sparse data sets to meet functional and non-functional business requirements.
Design and implements data tools for analytics and data scientist team members to help them in building, optimizing and tuning our product.
Integrate new data management technologies and software engineering tools into existing structures.
Help build high-performance algorithms, prototypes, predictive models and proof of concepts.
Use a variety of languages, tools and frameworks to marry data and systems together.
Recommend ways to improve data reliability, efficiency and quality.
Collaborate with Data Scientists, DevOps and Project Managers on meeting project goals.
Tackle challenges and solve complex problems on a daily basis.
Qualifications
Required:
4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
1+ years of experience on distributed, high throughput and low latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.
Preferred:
Producing high-quality code in Python.
Passionate about testing, and with extensive experience in Agile teams using SCRUM you consider automated build and test to be the norm.
Proven ability to communicate in both verbal and writing in a high performance, collaborative environment.
Follows data development best practices, and enjoy helping others learn to do the same.
An independent thinker who considers the operating context of what he/she is developing.
Believes that the best data pipelines run unattended for weeks and months on end.
Familiar with version control, you believe that code reviews help to catch bugs, improves code base and spread knowledge.
Helpful, but not required:
Knowledge in:
Experience with large consumer data sets used in performance marketing is a major advantage.
Familiarity with machine learning libraries is a plus.
Well-versed in (or contributes to) data-centric open source projects.
Reads Hacker News, blogs, or stays on top of emerging tools in some other way
Data visualization
Industry-specific marketing data
Technologies of Interest:
Languages/Libraries – Python, Java, Scala, Spark, Kafka, Hadoop, HDFS, Parquet.
Cloud – AWS, Azure, Google
The team
Advertising, Marketing & Commerce
Our Advertising, Marketing & Commerce team focuses on delivering marketing and growth objectives aligned with our clients’ brand values for measurable business growth. We do this by creating content, communications, and experiences that engage and inspire their customers to act. We implement and operate the technology platforms that enable personalized content, commerce and marketing user-centric experiences. In doing so, we transform our clients’ marketing and engagement operations into modern, data-driven, creatively focused organizations. Our team brings deep experience in creative and digital marketing capabilities, many from our Digital Studios.

We serve our clients through the following types of work:Cross-channel customer engagement strategy, design and development(web, mobile, social, physical)eCommerce strategy, implementation and operationsMarketing Content and digital asset management solutionsMarketing Technology and Advertising Technology solutionsMarketing analytics implementation and operationsAdvertising campaign ideation, development and executionAcquisition and engagement campaign ideation, development and executionAgile based, design-thinking, user-centric, empirical projects that accelerate results

How you’ll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.
Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitte’s culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.
Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.
Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.
kwhux"
49,Data Engineer (Mid and Senior),"Denver, CO",Denver,CO,None Found,None Found,"2+ years designing and developing data analytics solutions
2+ years with RDBMS such as SQL Server, Oracle, MySQL
2+ years data warehouse, dimensional modeling design and architecture
A passion to learn and improve your skills to deliver the best possible solutions to customers
Experience with cloud based data services offered by Azure, AWS and Google
Experience with data visualization tools such as Power BI and Tableau
Previous consulting experience preferred
Degree in computer science, information technology, engineering or business
Must be authorized to work in the US. We are unable to sponsor H-1B visas at this time.
",None Found,"Hands-on development and serve as technical expert on projects
Develop data solutions leveraging traditional and cloud product offerings from leading vendors
Develop data models to meet client needs
Develop data models to meet client needs, including transactional, third-normal form, dimensional, columnar, distributed and NoSQL
Develop ETL/ELT processes and patterns to efficiently move data
Create data visualizations, dashboards and reports as needed
Develop and scope requirements
Travel as needed (currently less than 5%)
Maintain effective communication with team and customers",None Found,None Found,"Data Engineer, Mid to Senior Level
Datalere team members lead by example, focus on customer needs and have a thirst to learn all they can about data analytics. Successful candidates are self-starters and never shy away from challenges.
We need team members that excel when working directly with clients to meet their goals. They understand the client's needs and requirements and build a collaborative environment to ensure a successful project delivery.
Data Engineers analyze and develop on-premises and/or cloud data and ETL solutions to solve the client's challenges. They enjoy the challenges of consulting and thrive to knock the socks off of clients
Please note that this role is vendor agnostic in regards to what ETL tools are used, so having multi vendor experience would be ideal.
Responsibilities:
Hands-on development and serve as technical expert on projects
Develop data solutions leveraging traditional and cloud product offerings from leading vendors
Develop data models to meet client needs
Develop data models to meet client needs, including transactional, third-normal form, dimensional, columnar, distributed and NoSQL
Develop ETL/ELT processes and patterns to efficiently move data
Create data visualizations, dashboards and reports as needed
Develop and scope requirements
Travel as needed (currently less than 5%)
Maintain effective communication with team and customers
Qualifications
2+ years designing and developing data analytics solutions
2+ years with RDBMS such as SQL Server, Oracle, MySQL
2+ years data warehouse, dimensional modeling design and architecture
A passion to learn and improve your skills to deliver the best possible solutions to customers
Experience with cloud based data services offered by Azure, AWS and Google
Experience with data visualization tools such as Power BI and Tableau
Previous consulting experience preferred
Degree in computer science, information technology, engineering or business
Must be authorized to work in the US. We are unable to sponsor H-1B visas at this time.
About Us
At Datalere, we work with our clients to transform their enterprise through the use of modern compute technologies and proven deployment processes providing cost effective durable solutions for the competitive world.
If you are seeking new challenges, interested in staying up to date with the latest releases and can deliver uncompromised service to our customers, then we'd like to hear from you. If you are interested and meet the above qualifications, please submit your resume and cover letter indicating your interest to join our team."
50,Sr. Data Engineer,"Denver, CO 80202",Denver,CO,80202,None Found,"
Bachelor’s degree in computer science, math, engineering, or relevant technical field
4+ years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration, and data integration concepts and methodologies
3+ years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environments
3+ years of experience with execution of DevOps methodologies and continuous integration/continuous delivery
Object-oriented/object function scripting languages: Python, R, C/C++, Java, Scala, etc.
SQL, relational databases and NoSQL databases
Data integration tools (e.g. Talend, SnapLogic, Informatica) and data warehousing / data lake tools
API based data acquisition and management
MSSQL, PostgreSQL, MySQL, etc. - MemSQL, CrateDB, etc.
Business intelligence tools such as Tableau, PowerBI, Zoomdata, Pentaho, etc.
Data modeling tools such as ERWin, Enterprise Architect, Visio, etc.
Data integration tools such as Boomi, Pentaho, Talend, Informatica, SnapLogic, etc.
Familiarity with cloud-based data engineering (AWS, GCP, or Azure)
Familiarity with data science techniques and frameworks
Creative thinker with strong analytical skills
Ability to work in a team environment
Strong technical communication skills
Ability to prioritize work to meet tight deadlines
Ability to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables
",None Found,"
Architect, build, and support the operation of enterprise data and analytical infrastructure and tools
Design robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in the selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in the management of enterprise data, including master data, reference data, metadata, data quality and lineage
Develop and prepare strategies for Business Intelligence processes for the organization
Manage and customize all ETL processes as per customer requirement and analyze all processes for same
Perform assessment on all reporting requirements and contribute to the development of a long-term strategy for various reporting solutions
Coordinate with data generator and ensure compliance to all enterprise data model according to data standards
",None Found,None Found,"At Ping Identity, we're changing the way people think about enterprise security technology. With our innovative Identity Defined Security platform, we're helping to build a borderless world where people have total freedom to work wherever and however they want. Without friction. Without fear.

We're headquartered in Denver, Colorado, and we have offices and employees around the globe. And we serve the largest, most demanding enterprises worldwide, including over half of the Fortune 100. Because even in the most complex enterprise environments, security shouldn't be a source of anxiety. It should be one of your greatest competitive advantages.

We call this digital freedom. And it's not just something we provide our customers. It's something that drives our company. People don't come here to join a culture that's build on digital freedom. They come to cultivate it.

The Senior Data Engineer will have the opportunity to play a critical role in the early stages of developing Ping’s Business Intelligence and Data Analytics capabilities. This individual will collaborate with other teams across the organization (e.g., Product Management, Engineering, Sales, and Finance) to gain a quick understanding of Ping products, business process areas, and/or technologies, and build the analytical framework to enable the business to better understand and leverage complex data sets. The Senior Data Engineer will be expected to architect and build core datasets, implement efficient ETL processes, and own the design, development and maintenance of critical metrics, reports, analyses, dashboards, etc. to drive key business decisions. In addition, this individual must be able to adapt and thrive in a fast-paced and changing business and technical environment.

Key Responsibilities:

Architect, build, and support the operation of enterprise data and analytical infrastructure and tools
Design robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in the selection and integration of data related tools, frameworks, and applications required to expand platform capabilities
Understand and implement best practices in the management of enterprise data, including master data, reference data, metadata, data quality and lineage
Develop and prepare strategies for Business Intelligence processes for the organization
Manage and customize all ETL processes as per customer requirement and analyze all processes for same
Perform assessment on all reporting requirements and contribute to the development of a long-term strategy for various reporting solutions
Coordinate with data generator and ensure compliance to all enterprise data model according to data standards
Essential Qualifications:

Bachelor’s degree in computer science, math, engineering, or relevant technical field
4+ years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration, and data integration concepts and methodologies
3+ years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environments
3+ years of experience with execution of DevOps methodologies and continuous integration/continuous delivery
Object-oriented/object function scripting languages: Python, R, C/C++, Java, Scala, etc.
SQL, relational databases and NoSQL databases
Data integration tools (e.g. Talend, SnapLogic, Informatica) and data warehousing / data lake tools
API based data acquisition and management
MSSQL, PostgreSQL, MySQL, etc. - MemSQL, CrateDB, etc.
Business intelligence tools such as Tableau, PowerBI, Zoomdata, Pentaho, etc.
Data modeling tools such as ERWin, Enterprise Architect, Visio, etc.
Data integration tools such as Boomi, Pentaho, Talend, Informatica, SnapLogic, etc.
Familiarity with cloud-based data engineering (AWS, GCP, or Azure)
Familiarity with data science techniques and frameworks
Creative thinker with strong analytical skills
Ability to work in a team environment
Strong technical communication skills
Ability to prioritize work to meet tight deadlines
Ability to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables
Desired Qualifications:

Experience with advanced analytics and machine learning concepts and technology implementations
Experience in a fast-paced, ever-changing and growing environment"
51,Senior Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.

As a Senior Data Engineer, here's what we'll be looking for you to bring:


Hands-on Engineering Leadership
Proven track record of Innovation and expertise in Data Engineering
Tenure in coding, architecting and delivering complex projects
Deep understanding and application of modern data processing technology stacks. For example Spark, Kafka, Hadoop, ecosystem technologies, and others
Deep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies
Deep understanding of relational database technologies and database development techniques
Understanding of how to architect solutions for data science and analytics
Data management for reporting and BI experience is a plus
Understanding of ""Agility"", including core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Passion for software craftmanship
A rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..
Strong stakeholder management and interaction experience at different levels
Any experience building and leading an offshore/outsourcing function would be highly beneficial.

There's no typical day or engagement for our Senior Engineers. Here's what you'll do:


Be the SME. Develop Big Data architectural approach to meet key business objectives and provide end to end development solution
You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that Big Data has to solve their most pressing problems.
On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.
It could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.
Whatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.
You have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.
You recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.

Regardless of what you do at ThoughtWorks, you'll always have the opportunity to:


Think through hard problems, and work with a team to make them reality.
Learn something new every day.
Work in a dynamic, collaborative, transparent, non-hierarchal, and ego-free culture where your talent is valued over a role title
Travel the world.
Speak at conferences.
Write blogs and books.
Develop your career outside of the confinements of a traditional career path by focusing on what you're passionate about rather than a predetermined one-size-fits-all plan
Be part of a company with Social and Economic Justice at the heart of its mission.

A few important things to know:
-------------------------------

Projects are almost exclusively on customer site, so candidates should be flexible and open to travel.

Candidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.

Not quite ready to apply? Or maybe this isn't the right role for you? That's OK, you can stay in touch with AccessThoughtWorks ( https://www.thoughtworks.com/careers/access?utm_source=apply-jobs&utm_medium=jd&utm_campaign=access-thoughtworks ), our learning community (click ""contact me about recruitment opportunities"" to hear about jobs in the future).

It is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.

#LI-NA"
52,Data Engineer (ADX-98-19),"Boulder, CO 80301",Boulder,CO,80301,None Found,None Found,None Found,None Found,None Found,None Found,"POSITION SUMMARY:
The Data Engineer will contribute key skills and expertise to our rapidly growing In Vitro Diagnostics program. The candidate will help plan, develop, and build data infrastructure critical to the program’s core data pipeline and will be responsible for ensuring the accuracy and usability of data central to regulatory approval of ArcherDX’s diagnostics. This person will work closely with program technical leads, data scientists, and quality assurance personnel to think creatively and achieve ambitious goals to advance the treatment of genetic diseases.

RESPONSIBILITIES:
Develop understanding of scientific, analytical, and regulatory needs for data infrastructure
Think creatively to plan adaptable data solutions for a growing and quickly evolving program
Integrate custom software and databases with a commercial laboratory information management system
Develop programmatic solutions to collect, parse, transform, and transfer data
Become familiar with and employ industry-leading data standards, striving for findable, accessible, interoperable, and reusable data
Adhere to standard operating procedures and work in a manner compliant with regulatory standards

REQUIRED QUALIFICATIONS:
4+ years of relevant industry experience including working as a Data Engineer or similar
Experience with cloud computing technologies (AWS preferred)
Strong expertise with SQL databases (NoSQL is a plus)
Avid Python programmer and experience working with APIs (experience with R is a plus)
Experience working with scientists (experience in a life science organization preferred)
Ability to work independently and proactively
Excellent communication skills and ability to explain technical concepts to non-experts
Excited to build infrastructure to power innovative biotechnology"
53,"Data Engineer - Broomfield, CO","Broomfield, CO",Broomfield,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Swisslog designs, develops and delivers best-in-class automation solutions for forward-thinking hospitals, warehouses and distribution centers. We offer integrated systems and services from a single source – from consulting to design, implementation and lifetime customer service. Behind the company’s success are 2 300 employees worldwide, supporting customers in more than 50 countries.

The Healthcare Solutions portfolio comprises automated material handling and drug management systems for hospital facilities that increase efficiency and enhance the patient experience in forward-thinking hospitals. Swisslog automated material handling solutions provide quick, flexible and safe transportation of medications, specimens and basic supplies throughout hospitals and across medical center campuses, while its medication management solutions address packaging, labeling, storage and dispensing for inpatient and outpatient pharmacies.
DATA ENGINEER - BROOMFIELD, CO
LOCATION - BROOMFIELD, CO
Swisslog looking for a passionate Data Engineer who loves data and database technologies and has experience bringing new analytics solutions from concept to market. We are building a new series of Products and Services based on Software as a Service (SaaS) model on AWS. Analytics will be our first service offering in that series. You will be joining at very early stage and will have the opportunity to influence the design and implementation. You must have deep knowledge of open source scripting, database and data warehouse technologies and be self-driven and highly motivated.

The Data Engineer is a critical member of the Software Development and Analytics team. You will help in sustaining and improving our current software solutions. Additionally, you will help steer new analytics offerings and establish our next generation data ingestion, transformation, and analytics capabilities and transform our current practices to take advantage of these capabilities.
YOUR RESPONSIBILITIES
Open Source database technologies such as My-SQL and Postgres
NoSQL technologies such as: DynamoDB or MongoDB.
Real time ETL to Data Mart using data streams such as Kafka and Spark ETL process
Data Modeling
Agile Data Warehouse and BI development
Fine tune data flows and data search functions
Experience with AWS or other cloud based platforms
Design the organization of data for data warehousing using the star schema and/or snowflake schema
Design and implement complex queries utilizing SQL Commands, Views, Stored Procedures
Write programming scripts to extract data to end users to make business decisions
Create functions to provide custom functionality per the requirements
Extensive experience in writing database queries
Participate in discussions involving the application creation and understand the requirements and provide the back-end functionality for the applications
Assist with critical analysis of test results and deliver solutions to problem areas
Identify and research data issues; working with teams to resolve problems
Analyze user problems and make suggestions for the prevention of future problems
YOUR PROFILE
Proven experience in bringing an analytics solution from concept to market
10+ Years of experience in data processing and database technologies
Migration Experience with moving Data in/out from open source database like Postgres, and other similar technologies
Experience using scripting languages such as Python and Powershell to source API data
Strong preference for Agile/Scrum experience
Strong work ethic, loyal, achievement oriented, and ethical
Excellent interpersonal and communications skills (both verbal and written)
Possess a high level of professionalism and have excellent follow-through skills
Highly analytical and organized, with a strong attention to detail
Bachelors Degree in Computer Science, Math or Engineering

Desirable but not Essential:
Experience in the Healthcare industry a plus
Develop reports in tools such as SQL, SSRS, Tableau, Qlik View, Power BI
Experience with statistical modelling and exposure to R
WE OFFER
Swisslog offers challenging work in a globally networked environment as well as competitive base salary, comprehensive benefits including health/dental and above-market 401K!

OUR SOLUTIONS DELIVER RESULTS. OUR EMPLOYEES DELIVER SOLUTIONS.
Swisslog is an EEO Employer, Females/Minority/Veterans/Disabled/Sexual Orientation/Gender Identity

Swisslog’s FMLA policy can be found at:
http://www.dol.gov/whd/regs/compliance/posters/fmlaen.pdf

Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. If you require reasonable accommodation to complete the application or to perform your job, please contact Human Resources at jobs.healthcare.us@swisslog.com.
Contact
Andy Levine
Talent Acquisition Manager"
54,Data Engineer,"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description

The Data Engineer, Analytics role falls into the Data Management & Business Intelligence practice area at CapTech, through which our consultants provide a broad spectrum of services to help our clients define and implement a strategy to deliver lasting and mission-critical information capabilities. Our Data Integration consultants bridge the gap between the business and IT side of companies. By partnering with clients to fully understand both their business philosophy and IT strategy, CapTech consultants maintain the vision that data integration should be built to help the organization make better decisions by providing the right data at the right time.
Specific responsibilities for the Data Engineer, Analytics position include:

Design, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, C, etc)
Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insights
Design, develop, and implement data processing pipelines at scale
Present programming documentation and design to team members and convey complex information in a clear and concise manner.
Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.
Write and refine code to ensure performance and reliability of data extraction and processing.
Communicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customers
Participate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.
Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.
Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.
Some of our technologies might include: HDFS, Cassandra, Spark, Java, Scala, Informatica, SQL Server, Oracle, Ab Initio, Kafka.

Qualifications

Specific qualifications for the Data Engineer, Analytics position include:

Development experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system
Strong SQL development skills
Development experience with at least two different programming languages (Python, Java, C, etc.)
Development experience with Unix tools and shell scripts
Development experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.)
Minimum of 3 years experience designing, developing, and testing software aligned with defined requirements
Experience tuning SQL queries to ensure performance and reliability
Software engineering best-practices, including version control (Git, TFS, JIRA, etc.) and test driven development
Exposure to Business Intelligence tools such as Business Objects, Informatica, SSRS, Cognos, MicroStrategy, Tableau, QlikView, SpotFire, etc.
Additional Information

We offer challenging and impactful jobs with professional career paths. All CapTechers can keep their hands-on technology no matter what position they hold. Our employees find their work exciting and rewarding in a culture filled with opportunities to have fun along the way.
At CapTech we offer a competitive and comprehensive benefits package including, but not limited to:
Competitive salary with performance-based bonus opportunities
Single and Family Health Insurance plans, including Dental coverage
Short-Term and Long-Term disability
Matching 401(k)
Competitive Paid Time Off
Training and Certification opportunities eligible for expense reimbursement
Team building and social activities
Mentor program to help you develop your career
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace.
Candidates must be eligible to work in the U.S. for any employer directly (we are not open to contract or “corp to corp” agreements). At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
CapTech is a Drug-Free work place.
Candidates must have the ability to work at CapTech’s client locations.
All positions include the possibility of travel.
CapTech has not contracted/does not contract with any outside vendors in its recruitment process. If you are interested in this position, please apply to CapTech directly."
55,Data Engineer (Green Chef),"Boulder, CO",Boulder,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Green Chef is the leading and first certified organic company in the meal kit space in the US. We are passionate about making it easier to eat healthfully at home. Founded in September 2014, Green Chef is already a national force with hundreds of employees and tens of thousands of customers relying on us to help them eat well.

In 2018 Green Chef and Hello Fresh partnered to become the world's largest meal kit company. Together, we tirelessly work to disrupt the food industry.

Green Chef remains focused on delivering healthy, organic ingredients, designed for busy people. We believe in creating thoughtful food experiences from planting to the plating. We strive to create greater interaction and meaningful connections with the food that nourishes us, the people who provide it, and the ones we share it with. Green Chef offers a variety of specialty lifestyle plans.

As a Data Engineer at Green Chef, you will be a key member of foundational projects in data architecture and business intelligence that drive customer insights and operational efficiency. This is an opportunity to get in on the ground floor of a talented and diverse cross-functional team tackling high visibility data projects. You will work with talented team members to design and implement scalable data solutions that enable Green Chef to be a truly data-driven company. This is a unique chance to be involved in marketing, e-commerce, physical product, operations and supply chain data at a food-tech company where data is key to growth and innovation.

Our ideal candidate will have excellent communication and problem-solving skills as well as a deep passion for learning. You will take full ownership of systems and projects, and enjoy the challenge of navigating a fast-paced start-up environment while being able to have some fun.

Our infrastructure runs on AWS and we have data warehouses in MongoDB, Postgres, and Redshift.

Responsibilities


Architect and build data pipelines that can parse production data from different sources and deliver quality data to our business analytics teams.
Partner with engineering to create services to ingest and supply data and provide the associated streaming solutions.
Ability to deal with large, complex amounts of data that will be used 24/7 by the business as a foundation for all decision making at all levels of the company.
Test, assess, and secure the data generated from the transformations while keeping our data secure with all the privacy policies in place.
Work closely with BI, product, marketing, operations, and engineering teams to understand how data can influence our company's strategic direction and decision-making.
You'll be part of the global BI group of HelloFresh working with many different cultures, backgrounds, and tech stacks as we work with the various HelloFresh brands.
Make well-informed decisions with deep knowledge of both the internal and external impacts on teams and projects.
Evaluate and integrate third-party data platforms, APIs and machine learning services.
Keep track of industry trends in data science and engineering to ensure cutting-edge implementations
Expand and optimize our data and data pipeline architecture and data flow

Requirements


At least 3 years' experience as a Data Engineer.
You are a builder. You not only get excited about data architecture, but you have the technical knowledge and experience building complex pipelines to set us up with the tools we need to provide data to the greater company.
Demonstrated strength in data modeling, ETL development, and data warehousing
Strong fundamentals in data structures, algorithms, data modeling, and database performance on NoSQL/SQL
Proficient in at least one of the scripting languages: Python/PySpark or Ruby
Proficient in streaming services (Kafka or Kinesis)
Experience with automated testing.
Knowledge of software engineering best practices across the development lifecycle (agile methodologies, coding standards, code reviews, build processes, and testing)
Excellent written and oral communication skills.
Bachelors in Computer Science, Engineering or Mathematics.
Experience with:
MongoDB, Postgres, Redshift, AWS RDS
Docker and containerization
Airflow, Luigi, AWS Glue, MoSQL
Kafka or AWS Kinesis
Experience building custom ETL solutions
AWS S3, Athena, EMR

Our team is diverse, high-performing and international, helping us to create a truly inspiring work environment in which you will thrive!

It is the policy of Green Chef and HelloFresh not to discriminate against any employee or applicant for employment because of race, color, religion, sex, sexual orientation, gender identity, national origin, age, marital status, genetic information, disability or because he or she is a protected veteran."
56,ETL Software Engineer,"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Overview

We’re looking for a developer / data engineer to create, debug, and streamline imports of investment data from various financial sources into our SQL database.
We already have a custom data import framework in place, built using C#, and we need to add new data sources and interfaces.
Involves writing C# code, querying data in SQL, and reconciliation of investment finance data.


What qualifications will help me be successful at FinFolio?

Intermediate level SQL required (SQL Server specifically a plus).
Beginner to Intermediate level C# required.
Excellent written and verbal communication skills.
Must be reliable and self starting, with a strong attention to detail.
Familiarty with IT / infrastructure is a plus.
Investment finance knowledge is a plus.


What will you do at this job?

Develop a custom tool to load and transform data into FinFolio’s SQL database.
Work with various financial data from brokers and advisory systems, typically: Accounts, Transactions, Custodial Balances, Tax Lots, Securities, Prices, etc.
Software is written in C# (with WPF), and it may need bug fixes or new features. You will be responsible for both coding these changes and coming up with your own improvements.
You will need to reconcile data import errors by using SQL queries and FinFolio desktop software. This will involve determining why transactions are not impacting balance correctly, why account data does not match, etc.
You may need to interact with clients to request data extracts from their current systems and work with them to make sure the correct data is extracted.
Eventually we hope for this position to branch out and encompass a broader range of duties, including infrastructure and general software development.


Who is FinFolio?

We make wealth management simpler!
A SaaS solution that is the backoffice for professional wealth managers.
Our software reports, trades, and bills investment accounts.
Founded in 2008, we recently re-launched our product to rave reviews.
20+ employees, 50% are remote, grew 100% over 24 months.
Exciting early-stage startup, adding a new client every 12 days.
Passionate and excited about great software and making our clients happy.


Why should you work here?

Work with a team that is excited and passionate about what they do.
Help support an amazing, best-in-class product that is fun to use.
Competitive salary + health/dental + paid time off + 401K match.
Interact with interesting and successful financial advisor clients.
Fun co-working office in Denver RINO: https://www.denver.org/about-denver/neighborhood-guides/river-north-art-district/"
57,Data Engineer with TS/SCI with Poly,"Aurora, CO 80011",Aurora,CO,80011,None Found,"United States’ citizen with current TS/SCI, SSBI and polyBachelor’s Degree with at least 4-8 years of applicable experience or Masters degree with 2-6 years of experience or 4 additional years in lieu of degree",None Found,"Support of production data processing and data distribution systemsWork with data providers and customers to ensure data quality and availabilityGather requirements and work with data providers to enable distribution of new data sourcesSupport software deployments and integration of geographically diverse computing systemsProvide direct support to end usersConfigure and maintain data ingest workflows (ETL) across several production systemsInstall, configure, and update a wide array of COTS/GOTS and homegrown software applicationsSupport and troubleshoot diverse IT infrastructure hardware platforms and protocolsWork with software development and systems administration staff to monitor and troubleshoot production systemsGenerate and maintain systems documentation and diagramsTroubleshoot network issues and establish new connectivityMonitor and maintain a variety of databases",None Found,None Found,"Description
Job Description:
The National Solutions Group at Leidos has an opening for a passionate Data Engineer to provide a variety of software and IT support services to ensure customer satisfaction with production software and systems. The position specifically focuses on the operations of GOTS software to ensure data integrity and availability for a large ETL system, but includes general system troubleshooting and direct customer support. The position is for a mid-senior level engineer with experience in both software development and systems administration. Candidates must be able to work in a fast moving environment with many moving parts and must be able to juggle several tasks at once. In addition, they must be willing to share on-call responsibilities to troubleshoot customer issues during non-business hours.
Primary Roles & Responsibilities:
As a Data Engineer you will have the opportunity to:Support of production data processing and data distribution systemsWork with data providers and customers to ensure data quality and availabilityGather requirements and work with data providers to enable distribution of new data sourcesSupport software deployments and integration of geographically diverse computing systemsProvide direct support to end usersConfigure and maintain data ingest workflows (ETL) across several production systemsInstall, configure, and update a wide array of COTS/GOTS and homegrown software applicationsSupport and troubleshoot diverse IT infrastructure hardware platforms and protocolsWork with software development and systems administration staff to monitor and troubleshoot production systemsGenerate and maintain systems documentation and diagramsTroubleshoot network issues and establish new connectivityMonitor and maintain a variety of databases
Minimum Qualifications:United States’ citizen with current TS/SCI, SSBI and polyBachelor’s Degree with at least 4-8 years of applicable experience or Masters degree with 2-6 years of experience or 4 additional years in lieu of degreeStrong grasp of LinuxAutomating tasks by writing quality codeStrong coding skills (Java, Javascript, shell scripting, Perl, Python)Configuration managements tools (Puppet, Chef)Monitoring complex systems (Nagios, ElasticSearch, Grafana)Automation tools (Jenkins, Bamboo)Source-control systems (Git, SVN)Candidate must be certified to meet DoD 8570 level IAT-II qualifications. A Security+ certification is requiredWilling to share on-call responsibilities to include coming into work during non-business hours to troubleshoot customer issues
Preferred Qualifications:Master’s DegreeRed Hat Enterprise Linux administration experienceUnderstanding of Amazon Web Services – EC2, RDS, S3Hadoop, Accumulo and Map Reduce techniquesUnderstands compiled languages including JAVASoftware versioning control systems – GIT/SVNFamiliarity with Software Development ProgramsFamiliarity with Agile Development methodologiesJava Programming experience
External Referral Bonus:
Ineligible
Potential for Telework:
No
Clearance Level Required:
Top Secret/SCI with Polygraph
Travel:
Yes, 10% of the time
Scheduled Weekly Hours:
40
Shift:
Day
Requisition Category:
Professional
Job Family:
Software Engineering
Leidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.
Pay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.
Leidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.
All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws."
58,DATA ENGINEER,"Denver, CO 80202",Denver,CO,80202,None Found,None Found,None Found,"Design, Develop and Maintain Data Storage and Data Analytics Products, Systems and Solutions for clients in the Media Industry
Develop, Enhance & Configure data transformation and storage solutions in support of our product offerings
Work closely with other internal engineering teams focused on front-end and back-end APIs development and configuration in direct support of various products
Work closely with internal product and project management teams to design and prioritize features and their associated delivery timelines
Diagnose and correct system and product faults, designing & implementing solutions to correct
",None Found,None Found,"Decentrix is offering an exciting opportunity to an individual with the right skill set and background to join our elite team and work with the most advanced Media Advertising Enhancement/Optimization Technologies. You would be using your business, technical & development skills in an extremely fast paced environment providing services to forward thinking media corporations. See www.bianalytix.com for more information.
Position Responsibilities:
Design, Develop and Maintain Data Storage and Data Analytics Products, Systems and Solutions for clients in the Media Industry
Develop, Enhance & Configure data transformation and storage solutions in support of our product offerings
Work closely with other internal engineering teams focused on front-end and back-end APIs development and configuration in direct support of various products
Work closely with internal product and project management teams to design and prioritize features and their associated delivery timelines
Diagnose and correct system and product faults, designing & implementing solutions to correct
Technical Proficiencies
Scala Language development with experience in full stack testing
Apache Spark and distributed data processing methodologies
T-SQL / PL SQL Development
Complex data processing methodologies
Massive scale data processing methodologies
Amazon AWS technologies and products (ECS, EC2, EMR, S3, etc.)
Linux
Docker image development & container execution
Advanced knowledge of RDBMS systems and associated storage optimization techniques
Analytic data structure knowledge and experience
Working knowledge of software engineering and applying database methodologies, techniques, and tools
Personal
The experience in the design of data storage and data transformation processes to leverage and analyze complex data relationships
The skills to quickly diagnose and solve technical problems associated to large data solutions and the associated technologies
Interest and ability to work at the technical installation level with major media corporations
Maturity to manage task associated to products and/or services to an installation contract
Excellent communication skills, including good verbal and written abilities"
59,Cloud Data Engineer,"Denver, CO 80209",Denver,CO,80209,None Found,"
Degree in Computer Engineering/Science or related field, with 4+ years of professional experience in database/data lake development
Proficient with processing data on relational databases like Oracle/SQL Server/MySQL/etc.
Experience with developing on an MPP database Redshift/Teradata/Snowflake
Proficient handling large data sets using SQL and databases in a business and engineering environment
Experience with operations in a Public Cloud Environment (AWS/Azure/GCP)
Experience with ETL and Data Warehouse/Lake processes
Excellent verbal and written communication skills
Strong troubleshooting and problem-solving skills
Thrive in a fast-paced, innovative environment
",None Found,None Found,None Found,None Found,"Businessolver delivers market-changing benefits administration technology supported by an intrinsic and unwavering responsiveness to client needs. Our clients trust Businessolver to take care of them and their employees with a configurable and secure SaaS platform and a culture of service, all aimed at total and measurable success and our clients' complete delight.

We work with some of the most recognizable brands in the U.S. We look to our rock-star employees to help these clients maximize the investment in their benefits program, minimize their exposure to risk, engage their employees with our easy-to-use solution and full suite of communication tools, and empower their employees to use their benefits wisely.

At Businessolver you will have opportunities for individual development through our common language: Trust through transparency. Assume positive intent. Be real. Live a growth attitude. Embrace the reverse golden rule.

The Cloud Data Engineer (CDE) will be responsible for architecting, developing, implementing, and operating stable, scalable, low cost solutions to source data from production systems into the data lake (AWS) and data warehouse (Redshift) and into end-user facing applications (AWS Quicksight). The ideal candidate should be able to work with Infrastructure, Data Analysts, and Machine Learning Engineers in a fast-paced environment, understanding the business requirements, and implementing ETL, machine learning and cloud solutions. This role will serve on the Cloud Data Engineering team.

Qualifications:

Degree in Computer Engineering/Science or related field, with 4+ years of professional experience in database/data lake development
Proficient with processing data on relational databases like Oracle/SQL Server/MySQL/etc.
Experience with developing on an MPP database Redshift/Teradata/Snowflake
Proficient handling large data sets using SQL and databases in a business and engineering environment
Experience with operations in a Public Cloud Environment (AWS/Azure/GCP)
Experience with ETL and Data Warehouse/Lake processes
Excellent verbal and written communication skills
Strong troubleshooting and problem-solving skills
Thrive in a fast-paced, innovative environment

Preferred Qualifications:

Oracle, Postgres, EMR, Redshift, Linux experience
Familiar with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complexity analysis
Experience with Agile Methodologies
Experience with complex/large data sets (Big Data)
Experience operating a Data Lake
Experience with Cloud Architecture/Engineering

The Businessolver Way…

Our team has spent nearly two decades crafting a culture that challenges each employee to perform at the top of their game – and have fun doing it! If you desire to use your skills and experience in an environment where you can make a difference, we want to hear from you! Businessolver employees experience a vibrant work culture with extensive workplace perks including:


Competitive pay, great benefits, and vacation time. We are an equal opportunity employer with competitive benefits including medical, dental, life insurance, disability, 401(k) with company match, among others.
Smart Casual Dress. No need to suit up, but we also have on-site dry cleaning services for those that prefer to dress-up!
Weekly catered meals. Breakfast every other Mondays, lunch Wednesdays, and afternoon appetizers on Fridays encourage collaboration across our teams.
Fully-stocked kitchens. We know it takes fuel to perform, so we provide a kitchen stocked with healthy cereals, fruit, snacks, and beverages to keep you at the top of your game.
Fitnessolver. If you need a boost, visit our on-site fitness facility to clear your head.
Massages. With a ""work hard/play hard"" atmosphere we all need a little stress relief at times.
Charity and community involvement. Participate in a variety of ways to support those around us.
Learning & Development. Continue to learn about the industry through our online and instructor-led classes.
Recognition. Want some swag? Earn tons of it by helping out your co-workers through our employee recognition program.
Culture. Want a culture most dream of? Most companies talk about it, we live it. Come find out for yourself!

Interested? Great, we look forward to reading your application - make sure it includes:


A cover letter that highlights why you think you'd be great for the gig, focusing on how your past work experience has prepared you for this kind of position – or why you think you can rock the job even though you don't have past work experience that's perfectly aligned.
Your resume.

You will receive an auto-reply confirming that we've received your application, and you will hear from us again after we've reviewed your application and decided whether or not to move you forward in our recruiting process.

If you do decide to apply, please know that every complete application will be carefully reviewed. Seriously! We know it is a time commitment to prepare an application. We will respect that effort by thoughtfully reviewing every single complete application and we are truly grateful for your interest.

Thanks for your interest in Businessolver!

Check us out on Twitter ( https://twitter.com/businessolver ), Facebook ( https://www.facebook.com/bsolver ) and LinkedIn ( https://www.linkedin.com/company/232793?trk=tyah&trkInfo=tarId%3A1415406210925%2Ctas%3Abusinessolver%2Cidx%3A2-1-4 ) for a look at our vibrant culture."
60,Sr. Data Engineer,"Westminster, CO",Westminster,CO,None Found,None Found,None Found,None Found,"Collect, store, and aggregate data to support the creation of great data products for the business and our customers
Be creative and cooperative in designing and building data pipelines
Use Cloud-based infrastructure and applications (we use AWS and maintain our own Kubernetes clusters)
Continually learn and seek ways to improve our data flows
Collaborate well in a team environment (we use agile)
Validate data and maintain healthy infrastructure and data flows
",None Found,None Found,"Please review the job details below.
The Data Intelligence Team is a central data engineering and analytics team that supports business decisions and company strategy across all parts of MAXAR. We collect data from a variety of interesting sources and build automated data flows that help enable actionable insights for many different teams and products. We are looking for data professionals that are excited to work with cloud-based tools and build high quality data flows. We are a team of people from a variety of data focused backgrounds. We support each other, and we help each other learn and grow. Come collaborate with us to help guide MAXAR and improve our collective understanding of our planet.
Responsibilities
Collect, store, and aggregate data to support the creation of great data products for the business and our customers
Be creative and cooperative in designing and building data pipelines
Use Cloud-based infrastructure and applications (we use AWS and maintain our own Kubernetes clusters)
Continually learn and seek ways to improve our data flows
Collaborate well in a team environment (we use agile)
Validate data and maintain healthy infrastructure and data flows
Required
Bachelor’s Degree in a technical field or equivalent work experience
Experience building and maintaining ETL pipelines
2 years of experience with Python, SQL, AWS (S3, EC2)
Data visualization or reporting experience
Good verbal and written communication skills
Preferred
Any experience with…
Airflow or other ETL scheduling tools
Building or improving APIs
Collecting data from a variety of sources (APIs, Postgres, Oracle, SAP, Salesforce, etc.)
Thoughtfully storing data in databases (especially Postgres) to support data products and reporting
Any experience with or willingness to learn…
Big data pipelines using Spark and streaming tools
Maintaining data pipelines used in a production environment
Working with geospatial data
Additional coding languages, especially JVM languages or Bash
Any of these or similar tools (Kubernetes, Ansible, Jenkins, PostgreSQL, Tableau)
Additional AWS tools (EFS, Lambda, RDS/Aurora, Glue, Athena)
Perks
Diverse team that works and learns together
Great benefits, flexible time off for family and travel, pet insurance discounts
Working to improve products that helps us understand and protect our planet
Well-lit office space with on-site cafeteria and coffee shop
Private frisbee golf course and very nice game and quiet rooms for mental breaks
Quarterly self-guided work time to build what you want or learn new skills and technology
MAXAR Technologies offers a generous compensation package including a competitive salary; choice of medical plan; dental, life, and disability insurance; a 401(K) plan with competitive company match; paid holidays and paid time off."
61,Tableau Developer Data Engineer (BHJOB22048_585),"Denver, CO",Denver,CO,None Found,None Found,None Found,None Found,None Found,None Found,"
3-5 Years BI Experience developing BI dashboards, reporting, visualizations and data models. Proven BI dashboard, report, and visualization design and delivery.
1-2 years Tableau Server experience. Proven Tableau performance tuning reporting and dashboard solutions.
Tableau Server Experience: Configuration, Administration, Tuning & Performance, Data Connections, APIs
Tableau Desktop & Tableau Prep Experience: Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience – including data extraction, transformation and load (ETL) from many sources; OLAP, OLTP, Datawarehouse’s, Hadoop/Cloudera, and Cloud platforms.
Experience with Tableau and Cognos Development.
Experience in data modeling and data prep (discovery, structuring, cleaning, enriching, validation, publishing). Tableau Prep preferred.
Source to Target mapping. Tableau Prep experience a major plus.
SQL Experience (joins, queries, select statements) – Oracle, SQL Server database back ends.
BI performance tuning across front end, ETL, Data back end optimization.
Solid Agile experience on BI projects – Gathering requirements, translating to use cases, BI stories, design/delivery of BI use cases – using SAFe, SCRUM, Agile, Kanban or similar method.
Exposure Python.","Tableau BI Engineer – ITmPowered Tableau Engineer will design, develop, and deliver high performance Tableau dashboards, workbooks, and visualizations providing Business Intelligence on hundreds of enterprise technology projects. Provide deep Tableau Server knowledge and expertise to the team on how to build and maintain Tableau Server dashboards and reports. Tableau Developer design, develop and implement […]

Tableau BI Engineer – ITmPowered

Tableau Engineer will design, develop, and deliver high performance Tableau dashboards, workbooks, and visualizations providing Business Intelligence on hundreds of enterprise technology projects.
Provide deep Tableau Server knowledge and expertise to the team on how to build and maintain Tableau Server dashboards and reports.
Tableau Developer design, develop and implement BI solutions leveraging Tableau Desktop, Tableau Online, Tableau Prep, and Tableau Server.
Provide Tableau development and support across dozens of systems and data sources. Support existing Tableau server, Tableau reports, and Tableau Dashboard solutions.
Performance tune Tableau dashboards (optimize extracts, limit fields/records, marks, optimize/materialize calculations, query optimization, workbook cleanup).
Engineer BI dashboards for Technology Projects and related Finance information (Burn Rates, Accruals, Milestones, Expenses, budgets, Earned Value, ROI) as well as project KPIs.
Identify BI performance bottlenecks and design optimal solutions at the report / dashboard level (Tableau, Cognos, PowerBI), SQL data munging level (SQL joins, pivots, enrichment, aggregations Oracle, Hadoop, SQL), or ETL / Data movement level (DataStage, Informatica, Sqoop).
Tableau Server Configuration, Administration, Tuning & Performance, Data Connections, APIs
Tableau Desktop & Tableau Prep Experience: Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience – including data extraction, transformation and load (ETL) from many sources; OLAP, OLTP, Datawarehouse’s, Hadoop/Cloudera, and Cloud platforms.
Work with end users to gather BI requirements (use cases, visualization, drill up/down, hierarchies, tables, pivots, outcomes, and data sources, etc.).
Build data models. Prepare, wrangle, and model the Data to derive effective data models supporting performant BI solutions. Understand technical data sources, data structures, data quality and necessary transformations to aggregate, enrich, validate, and publish data.
Source to target mappings and work with ETL Engineers to optimize data flows, and data preparation.
Develop and optimize BI Dashboards, Reports, Data Models, and data flows.
Support existing BI environments Tableau, Cognos, data movement (DataStage, Informatica, sqoop), and backend data repositories (Oracle, SQL Server, Hadoop).
Cloud BI migration – Migrate big data solutions to cloud in PowerBI / Azure – design and delivery.
 Requirements:

3-5 Years BI Experience developing BI dashboards, reporting, visualizations and data models. Proven BI dashboard, report, and visualization design and delivery.
1-2 years Tableau Server experience. Proven Tableau performance tuning reporting and dashboard solutions.
Tableau Server Experience: Configuration, Administration, Tuning & Performance, Data Connections, APIs
Tableau Desktop & Tableau Prep Experience: Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience – including data extraction, transformation and load (ETL) from many sources; OLAP, OLTP, Datawarehouse’s, Hadoop/Cloudera, and Cloud platforms.
Experience with Tableau and Cognos Development.
Experience in data modeling and data prep (discovery, structuring, cleaning, enriching, validation, publishing). Tableau Prep preferred.
Source to Target mapping. Tableau Prep experience a major plus.
SQL Experience (joins, queries, select statements) – Oracle, SQL Server database back ends.
BI performance tuning across front end, ETL, Data back end optimization.
Solid Agile experience on BI projects – Gathering requirements, translating to use cases, BI stories, design/delivery of BI use cases – using SAFe, SCRUM, Agile, Kanban or similar method.
Exposure Python."
62,"Senior Data Engineer, Identity Resolution","Denver, CO 80202",Denver,CO,80202,None Found,None Found,None Found,None Found,None Found,None Found,"About FullContact:
FullContact is the premier provider of SaaS-based identity resolution that empowers brands to improve their customer experience and authentically engage with consumers. Using a consumer-first approach with our product offerings, we aim to make relationships better and that starts with our employees.
We offer excellent benefits for our teammates, including full medical and dental coverage, our famous ""paid, paid vacation"" and a generous stock option plan.
You'll join an innovative, enthusiastic team whose hard work helped us achieve recognition from the API awards, MarTech Breakthrough awards, and inclusion on the Inc. 5000 list of fastest-growing companies.
The Role
Work on the Identity Resolution team as Senior Data Engineer to design and construct performant algorithms, infrastructure and data pipelines to manage our graph database for our identity resolution offering. Our identity graph is constructed from billions of observations leveraging the latest in big data technologies.
The Identity Resolution team is primarily focused on providing identity resolution capabilities to our internal customers enabling identity resolution across the board for our external customers. This involves integrating data sets, developing our patented identity resolution graph algorithms and scaling all this to a very large amount of data. The team works on both live streaming and batch systems and leverages machine learning so that we can be the best in class for enterprise identity resolution.

What You'll Do
Be a senior member on the Identity Resolution team
Design, build, test, deploy and maintain systems using JVM based languages, focusing on Scala and Java
Design, build, test and deploy massively parallel graph algorithms
Create and maintain microservices connected through APIs (1000s requests/sec in some cases)
Process large amounts of data leveraging big data technologies such as Spark, Kafka and more.
Expose data and tools to internal teams through APIs and libraries
Account for quality and security as you build
Your Traits
You are creative and enjoy solving problems
You are curious. You look for the root cause of issues and are a life learner
You are collaborative and love working with people, whiteboarding and designing hard problems
You have got grit and recognize that the harder things in life are more rewarding
You are empathetic for both customers and team members
About You
5+ years of experience in Data Engineering or Software Engineering
Deep understanding and experience developing in the JVM
You have a solid mathematical foundation
Experience with a variety of databases (SQL, NoSQL, In-Memory, Searchable, etc)
Solid Linux experience - CLI tools, scripting
Git and AWS familiarity
Authorized to work in the United States on a full-time basis
Bonus Points for experience with Machine Learning and Graph Theory

vPccapMAd3"
63,Senior Data Engineer,"Boulder, CO 80301",Boulder,CO,80301,None Found,None Found,None Found,None Found,None Found,None Found,"Company Overview

Fanatics is the global leader in licensed sports merchandise and changing the way fans purchase their favorite team apparel and jerseys. Through an innovative, tech-infused approach to making and selling fan gear in today's on-demand culture, Fanatics operates more than 300 online and offline stores, including the e-commerce business for all major professional sports leagues (NFL, MLB, NBA, NHL, NASCAR, MLS, PGA), major media brands (NBC Sports, CBS Sports, FOX Sports) and more than 200 collegiate and professional team properties, which include several of the biggest global soccer clubs (Manchester United, Real Madrid, Chelsea, Manchester City). Fanatics offers the largest collection of timeless and timely merchandise whether shopping online, on your phone, in stores, in stadiums or on-site at the world's biggest sporting events.

About the Team

Fanatics is first and foremost a technology company. We are powered by cutting-edge tech created by our small agile teams using the latest tools and technologies under our highly analytical, forward thinking, and open-minded leadership. As the global leader in licensed sports merchandise, we challenge ourselves by improving our new fully responsive NodeJS cloud commerce platform, Elasticsearch engine, and deep data science capabilities while building the best-in-class retail manufacturing and supply chain technologies. Our tech teams work together to revolutionize data science and engineering initiatives, provide highly scalable real-time and streaming platforms, and create secure e-commerce and in-stadium fan experience products. Our own e-commerce platform transacts in over 190 countries, 17 languages, and 14 currencies. Our motto is “#GSD”—get stuff done—and we do just that. If you want to be at the nexus of sports, commerce, and technology, come be a part of our industry-leading team here at Fanatics Tech.

The FanStreams team at Fanatics has the grand vision of making many of the traditional batch-oriented processes into real time systems and is seeking a Senior Data Engineer with the passion and ingenuity to build streams of data across various domains of Fanatics world. These streams will power Data Science, BI & Site optimizations. The FanStreams team is responsible for building, managing complex stream processing topolgies using the latest open source tech stack, build metrics and visualizations on the generated streams and create varied data sets for different forms of consumption and access patterns. We’re looking for a seasoned engineer to help us build and scale the next generation of streaming platforms and infrastructure at Fanatics.
What will you do?
Build data platforms and streaming engines that are real time in nature
Optimizing existing data platforms and infrastructure while exploring other technologies
Provide technical leadership to the data engineering team on how data should be stored and processed more efficiently and quickly at scale
Build and scale stream & batch processing platforms using the latest open-source technologies
Work with data engineering teams and help with reference implementation for different use cases
Improve existing tools to deliver value to the users of the platform
Work with data engineers to create services that can ingest and supply data to and from external sources and ensure data quality and timeliness
What are we looking for?
5+ years of software development experience with at least 3+ years of experience on open-source big data technologies
Knowledge of common design patterns used in Complex Event Processing
Proficiency in Streaming technologies: Apache Kafka, Kafka Streams, KSQL, Spark, Spark Streaming
Proficiency in Java, Scala, SQL
Experience and deep understanding of traditional, NoSQL and columnar databases
Experience of building scalable infrastructure to support stream, batch and micro-batch data processing
Bonus points
You've worked with Amazon Web Services
Tryouts are open at Fanatics! Our team is passionate, talented, unified, and charged with creating the fan experience of tomorrow. The ball is in your court now."
64,Data Engineer,"Broomfield, CO 80021",Broomfield,CO,80021,None Found,None Found,"
SQL
Python
Cloud Data Warehouse Systems
ETL
Workflow Tools
Batch Processing
Spark
CD/CI tools","
3+ years experience with Data Warehouse Systems and working on an ETL system, either a commercial one like Matillion or Fivetran, an open-source one like Airflow, or a custom one you or your company built
Experience with one major cloud analytics database (Snowflake, Redshift, Google Big Query), Snowflake preferred
Strong familiarity with SQL
Python development experience in production
Familiarity with Spark
A strong desire to show ownership of problems you identify and proven ability to empower others to get more done
Familiarity with modern BI and exploration tools, Looker is preferred.
Familiarity with GitHub or other CD/CI tools
Basic AWS experience (S3, EC2) (1-2 years)
Some familiarity with streaming approaches preferred
CS Degree preferred
Some experience preferred with Jenkins, Docker, Kubernetes",None Found,None Found,"Data Engineer

Position Overview
Validity is looking for a talented Data Engineer with 3+ years of experience in implementing modern data architectures. You will work closely with all areas of the business on engineering and analytical initiatives marked by greater complexity and less structure that will yield substantial product enhancements, uncover insights, and inform business decision making and focus. You will be working on one of the biggest opportunities at Validity: A major build-out of our data architecture. Your first projects will include helping to scale our data infrastructure and build out our data warehouse and analytics footprint. You will collaborate closely with Engineers and Product Managers to inform product decision making with data and to identify opportunities to create more value for our customers. This is a high-impact role that will help shape the future of Validity's products and services.
Company Overview
Validity is a leading global provider of data integrity and compliance offerings that thousands of organizations worldwide rely on to trust their data. We're passionate about our people, our customers, our values and our culture!
Join a passionate, driven team committed to bringing better insights and data-driven decisions to our internal and external customers. We're looking for people with a growth mindset and the insight to solve for today while building for the future. You will be working for a company that truly values the power of data.
Essential Position Duties and Responsibilities
3+ years experience with Data Warehouse Systems and working on an ETL system, either a commercial one like Matillion or Fivetran, an open-source one like Airflow, or a custom one you or your company built
Experience with one major cloud analytics database (Snowflake, Redshift, Google Big Query), Snowflake preferred
Strong familiarity with SQL
Python development experience in production
Familiarity with Spark
A strong desire to show ownership of problems you identify and proven ability to empower others to get more done
Familiarity with modern BI and exploration tools, Looker is preferred.
Familiarity with GitHub or other CD/CI tools
Basic AWS experience (S3, EC2) (1-2 years)
Some familiarity with streaming approaches preferred
CS Degree preferred
Some experience preferred with Jenkins, Docker, Kubernetes
Experience/Skills
SQL
Python
Cloud Data Warehouse Systems
ETL
Workflow Tools
Batch Processing
Spark
CD/CI tools

We are looking for someone to work in our Broomfield office but are open to a remote position depending on the situation.
fCVh3Ty9i9"
