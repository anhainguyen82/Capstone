,Title,Location,City,State,Zip,Country,Qualifications,Skills,Responsibilities,Education,Requirement,FullDescriptions
0,Data Engineer - Operations,"Santa Clara, CA 95051",Santa Clara,CA,95051,None Found,None Found,None Found,"Master the MapR Converged Platform, including MapR-FS, MapR-DB Binary and JSON Tables, MapR-Streams and the Hadoop Eco-System products and maintain proficiency and currency as the technology evolves and advances.Achieve and maintain proficiency with MapR cluster and Hadoop Framework sizing, installation, debugging, performance optimization, cluster migration, security and automation.Achieve proficiency with MapR DB Binary and Json Tables sizing, performance tuning and multi-master replication.Achieve proficiency with MapR Event Stream sizing, performance tuning and multi-master replication.Assist the Sales Team (Sales Rep and Sales Engineer) in positioning and selling MapR products and Service offerings.Work closely with MapR sales in scoping and estimating projects.Write, deliver and present formal SOW’s (Statement of Work).Deliver SOW content in formal, hands-on, onsite customer engagements and ensure the on-time delivery and quality of MapR Professional Service engagements.Be a technical voice to MapR’s customers and community via blogs, Hadoop User Groups (HUG’s) and participation at leading industry conferences.Stay current in best practices, tools, and applications used in the delivery of professional service engagements.",None Found,"5+ years experience administering any flavor of LinuxStrong scripting skills (Bash or Python prefered)Familiarity with commercial IT infrastructures including storage, networking, security,","Job Description:
Senior Data Engineer- Operations
Senior Data Engineers will report into MapR’s Professional Services Organization. MapR Data Engineers are responsible for delivering a variety of engineering services to MapR customers throughout North America. Assignments will vary based on the candidate’s skills and experience. Typical assignments may involve cluster sizing, installation, configuration, health checks, performance tuning, data migration, security and automation. MapR’s growing customer base includes most of the Fortune 50 companies which makes the work assignments technically challenging yet rewarding. This role provides a significant opportunity to learn and apply big data technologies and solve related complex problems. MapR Data Engineers report to the Director of Data Engineering located at company headquarters in San Jose, CA.
Responsibilities:
Master the MapR Converged Platform, including MapR-FS, MapR-DB Binary and JSON Tables, MapR-Streams and the Hadoop Eco-System products and maintain proficiency and currency as the technology evolves and advances.Achieve and maintain proficiency with MapR cluster and Hadoop Framework sizing, installation, debugging, performance optimization, cluster migration, security and automation.Achieve proficiency with MapR DB Binary and Json Tables sizing, performance tuning and multi-master replication.Achieve proficiency with MapR Event Stream sizing, performance tuning and multi-master replication.Assist the Sales Team (Sales Rep and Sales Engineer) in positioning and selling MapR products and Service offerings.Work closely with MapR sales in scoping and estimating projects.Write, deliver and present formal SOW’s (Statement of Work).Deliver SOW content in formal, hands-on, onsite customer engagements and ensure the on-time delivery and quality of MapR Professional Service engagements.Be a technical voice to MapR’s customers and community via blogs, Hadoop User Groups (HUG’s) and participation at leading industry conferences.Stay current in best practices, tools, and applications used in the delivery of professional service engagements.
Requirements:
5+ years experience administering any flavor of LinuxStrong scripting skills (Bash or Python prefered)Familiarity with commercial IT infrastructures including storage, networking, security,
virtualization and systems management
Familiarity with either Ansible, Puppet or ChefProficiency in basic Java or Scala programming preferred but not requiredBachelor's degree in CS or equivalent experienceFamiliarity with Hadoop and the Hadoop Eco-System framework a significant advantage
(MapR is willing to train otherwise promising candidates)
Strong verbal and written communication skills are requiredAbility to professionally manage multiple priorities with minimal supervision and deliver on
schedule
Willingness to travel about 70%
Ideal candidate will have any/all of the following:
RHCE certification, Bash or Python scripting, Automation using Ansible, Puppet or Chef, basic knowledge of Hadoop, Hive, or Spark"
1,Biomedical Data Engineer - Health Technologies,"Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Aug 21, 2019
Weekly Hours: 40
Role Number: 200092872
The Health Technologies Team conceives and proves out innovative technology for Apple’s future products and features in health.
We are seeking a highly capable Biomedical Data Engineer to join a multi-disciplinary team. Successful candidates will be able to integrate with our research study leads, data scientists, and engineers to develop and support effective data analysis and machine learning workflows.
Key Qualifications
Experience with software engineering frameworks:
Excellent coding skills in Python (e.g. Spark, Pandas, Jupyter )
Web Service APIs (e.g., AWS, REDCap, XNAT)
Designing and maintaining (non-)relational databases (e.g. Postgres, Cassandra, MongoDB)
Linux, MacOS based development frameworks
Version control frameworks (Git, virtualenv)
Experience using task scheduling system (eg: Airflow, Luigi or equivalent)
Familiarity with best practices for information security, including safe harbor privacy principles for sensitive data
Experience with biomedical sensors/platforms for measuring physiological signals (time-series data) in the health, wellness and/or fitness realms
Description
• Work closely with team members and study staff to design, build, launch and maintain systems for storing, aggregating and analyzing large amounts of data
• Process, troubleshoot, and clean incoming data from human studies
• Create ETL pipelines to automate data ingestion and transformation, with hooks for QA, auditing, redaction and compliance checks per data management specifications
• Create and populate databases with existing and incoming clinical data
• Architect data models and create tools to harmonize disparate data sources
• Incorporate and comply with regulations as they pertain to electronic and clinical data and databases
Education & Experience
BS/MS in Computer Science, Engineering, Informatics, or equivalent with relevant 4+ years industry experience with biomedical, health or sensitive data.
Additional Requirements
You will thrive in our fast-paced environment if you are highly organized and able to multitask.
Flexible thinking, adaptability to change and comfort with ambiguity are hallmarks of successful people on our team.
We look forward to witnessing your excellent communication and interpersonal skills during the interview process.
Apple is known for heterogeneous and cohesive teams. A proven ability to work seamlessly with others is required to acclimate quickly to our culture.
We highly value your analytical mind and problem-solving skills, and expect a stellar attention to detail.
You will let the customer experience guide your decision making. You will design with Apple’s culture and values, inclusion for all and privacy, as fundamental requirements."
2,Data Engineer,"Sunnyvale, CA",Sunnyvale,CA,None Found,None Found,"We are looking for a candidate with 5+ years of experience in a Data Engineer role, with Bachelor or Master’s degree in Computer Science or related field
2+ years of hands on experience in building and optimizing big data pipelines using Apache Spark in Python programming languages preferred.
2+ years of hands on experience of building & integrating data pipelines with machine learning models preferred.
2 years of experience with any of the cloud platforms such as AWS , Google, Azure etc. for building and deploying data pipelines preferred.
Highly desirable to have experience on data quality analysis and detection.
Experience with big data tools: Kafka, Cassandra, Hadoop, Storm and Spark
Highly Proficient in Python programming skills with Java programming language skills as a huge plus.
",None Found,"Design & Build data pipelines for handling real-time data streams and integrating with machine learning models
Understand and analyze large, complex data sets to identify anomalies and data quality issues
Design and Build Data quality and anomaly detection framework for IoT timeseries data
Build data pipeline with Apache Spark
Work with cross functional stakeholders to assist with their data needs via optimal & reusable data pipelines
",None Found,None Found,"Data Engineer
Equinix is one of the fastest growing data center companies, growing connectivity between clients worldwide. That’s why we're always looking for creative and forward-thinking people who can help us achieve our goal of global interconnection. With 200 data centers in over 24 countries spanning across 5 continents, we are home to the Cloud, supporting over 1000 Cloud and IT services companies that are directly engaged in technological innovation and development. We are passionate about further evolving the specific areas of software development, software and network architecture, network operations and complex cloud and application solutions.
At Equinix, we make the internet work faster, better, and more reliably. We hire talented people who thrive on solving hard problems and give them opportunities to hone new skills, try new approaches, and grow in new directions. Our culture is at the heart of our success and it’s our authentic, humble, gritty people who create The Magic of Equinix. We share a real passion for winning and put the customer at the center of everything we do.
Responsibilities
Design & Build data pipelines for handling real-time data streams and integrating with machine learning models
Understand and analyze large, complex data sets to identify anomalies and data quality issues
Design and Build Data quality and anomaly detection framework for IoT timeseries data
Build data pipeline with Apache Spark
Work with cross functional stakeholders to assist with their data needs via optimal & reusable data pipelines
Qualifications
We are looking for a candidate with 5+ years of experience in a Data Engineer role, with Bachelor or Master’s degree in Computer Science or related field
2+ years of hands on experience in building and optimizing big data pipelines using Apache Spark in Python programming languages preferred.
2+ years of hands on experience of building & integrating data pipelines with machine learning models preferred.
2 years of experience with any of the cloud platforms such as AWS , Google, Azure etc. for building and deploying data pipelines preferred.
Highly desirable to have experience on data quality analysis and detection.
Experience with big data tools: Kafka, Cassandra, Hadoop, Storm and Spark
Highly Proficient in Python programming skills with Java programming language skills as a huge plus.
Equinix is an equal opportunity employer. All applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, or status as a qualified individual with disability."
3,Principal Data Scientist,"Mountain View, CA 94043",Mountain View,CA,94043,None Found,"
Have a passion for working on big data with professional experiences in data mining, statistical analysis, predictive modeling, and data manipulation
Advanced degree (MS or PhD) in science, engineering, or statistics field
5+ years of experience in analyzing large data sets to drive significant business decisions for online/e-commerce domain
5+ years delivery experience in advanced modeling environment using Python, H2O, Spark, Java
Strong understanding of machine learning including supervised learning (Neural Network, Gradient Boosting, Logistics Regression) and unsupervised learning (clustering, entropy analysis, isolation forest, etc.)
Strong problem-solving and communication skills
Data engineer experience is a plus
",None Found,"
Work with business partners and stakeholders to understand the business, formulate the problems, and come up with the solutions
Design, develop and implement advance machine learning models to solve E-commerce Fraud problems
Work with large volumes of data; extract and manipulate large datasets using standard tools such as Python, H2O, Hadoop, Spark, SQL, and Java
Collaborate with engineers to implement machine learning models on one of the best data platforms in the industry
Analyze data to detect new fraud patterns and implement quick rules and solutions
Measure and report the business impact of models and rules
Communicate the concepts and results of the models and analyses to audience with different background
",None Found,None Found,"Coupang is one of the largest and fastest growing e-commerce platforms on the planet. Our mission is to create a world in which Customers ask ""How did I ever live without Coupang?"" We are looking for passionate builders to help us get there. Powered by world-class technology and operations, we have set out to transform the end-to-end Customer experience -- from revolutionizing last-mile delivery to rethinking how Customers search and discover on a truly mobile-first platform. We have been named one of the ""50 Smartest Companies in the World"" by MIT Technology Review and ""30 Global Game Changers"" by Forbes.

Coupang is a global company with offices in Beijing, Los Angeles, Seattle, Seoul, Shanghai, and Silicon Valley.

Job Overview:
The E-commerce Fraud Detection System team at Coupang is looking for a Principle/Senior Data Scientist with a solid background in machine learning, data science, data analysis, and data engineering. You will work closely with a team of data scientists, business analysts and engineers to ensure we detect frauds in e-commerce business and take actions to protect our business.

Key Responsibilities:

Work with business partners and stakeholders to understand the business, formulate the problems, and come up with the solutions
Design, develop and implement advance machine learning models to solve E-commerce Fraud problems
Work with large volumes of data; extract and manipulate large datasets using standard tools such as Python, H2O, Hadoop, Spark, SQL, and Java
Collaborate with engineers to implement machine learning models on one of the best data platforms in the industry
Analyze data to detect new fraud patterns and implement quick rules and solutions
Measure and report the business impact of models and rules
Communicate the concepts and results of the models and analyses to audience with different background

Qualifications:

Have a passion for working on big data with professional experiences in data mining, statistical analysis, predictive modeling, and data manipulation
Advanced degree (MS or PhD) in science, engineering, or statistics field
5+ years of experience in analyzing large data sets to drive significant business decisions for online/e-commerce domain
5+ years delivery experience in advanced modeling environment using Python, H2O, Spark, Java
Strong understanding of machine learning including supervised learning (Neural Network, Gradient Boosting, Logistics Regression) and unsupervised learning (clustering, entropy analysis, isolation forest, etc.)
Strong problem-solving and communication skills
Data engineer experience is a plus

Perks:

Autonomy to make decisions in a rapidly growing company
15 days PTO + 15 national holidays off
401K matching
Pre-IPO stock options
Mobile & fitness reimbursement
Free Catered Lunch daily

Coupang is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex or gender (including pregnancy, gender identity, gender expression, sexual orientation, transgender status), national origin, age, disability, medical condition, HIV/AIDS or Hepatitis C status, marital status, military or veteran status, use of a trained dog guide or service animal, political activities, affiliations, citizenship, or any other characteristic or class protected by the laws or regulations in the locations where we operate.

If you need assistance and/or a reasonable accommodation in the application or recruiting process due to a disability, please contact us at usrecruiting@coupang.com ( usrecruiting@coupang.com )."
4,Data Engineer,"Campbell, CA",Campbell,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Us
--------

With electric vehicles (EVs) expected to be 25% of vehicle sales by 2025 and more than 50% by 2040, electric mobility is in the midst of a tipping point and becoming reality. ChargePoint is at the center of the e-mobility revolution, powering the world's leading EV charging network and most complete set of hardware, software and mobile solutions for every EV charging need. Whether it's to ride, driver or deliver on electric fuel, we bring together people, vehicle fleets, businesses, automakers, policymakers, utilities and other stakeholders to make e-mobility a reality globally.

Our fanatical focus on charging and 10+ years in business has made us an industry leader. Supported by more than half a billion dollars in funding from investors, including Quantum Energy Partners, GIC, Clearvision, Daimler Trucks & Buses, Daimler, Siemens, Linse Capital, American Electric Power, Canada Pension Plan Investment Board, Chevron Technology Ventures, Rho Capital Partners, BMW i Ventures and Braemar Energy Ventures, ChargePoint offers a once-in-a-lifetime chance to be part of creating an all-electric future and shaping a trillion-dollar market. Join the team that built the EV charging industry and make your mark on how people and goods will get everywhere they need to go, in any context, for generations to come.

Reports To
----------

Software Engineering Manager

What You Will Be Doing
----------------------

Being a leading EV charging network we have a lot of data that is used to enhance end-user experience, optimize network operations and placement of charging stations, and improve internal operations and manufacturing processes. This is just the beginning and we see a huge opportunity to leverage data to predict failures before they happen, build delightful products and enable electrification of all forms of transportation.

A Data Engineer in this role will work with several teams across the company to identify use cases/scenarios for optimization, build a data lake to enable efficient querying and analytics, and use data science and ML to analyze the data and build data solutions.

What You Will Bring to ChargePoint
----------------------------------


Fundamental understanding of data science and ML
Experience building data pipelines, analyzing big data & building data solutions
Bias to action
Be an evangelist for data analysis and data-oriented decision making

Requirements
------------


1-3 years of object-oriented programming & SQL experience
1-3 years of experience with big data technologies (e.g., Hadoop, Spark, etc.) and data viz and ETL tools (e.g., Pentaho, Tableau etc.)
Relevant coursework and experience in data science and ML
Solid software engineering fundamentals
Fundamental understanding of RDBMS, NoSQL databases and big data solutions
Ability to work with ambiguity, create quick PoCs and engineer an E2E solution
Strong scripting or programming skills for automating repetitive tasks

Preferred
---------


Experience with AWS
Experience building data lake or data solutions from scratch

Location
--------

Campbell, CA

We are committed to an inclusive and diverse team. ChargePoint is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status.

If there is a match between your experiences/skills and the Company needs, we will contact you directly.

ChargePoint is an equal opportunity employer.
Applicants only - Recruiting agencies do not contact.

#LI-SH1"
5,Senior Data Engineer,"Sunnyvale, CA 94087",Sunnyvale,CA,94087,None Found,"Strong quantitative background (Computer Science, Math, Physics, and/or Engineering), or at least 5 years of industry experience in a quantitative roleExperience programming with any of the following: Python, Go, C++, Scala, JavaExperience establishing pipelines using distributed processing and no-SQL technologies such as Apache Spark, Kafka, Elasticsearch, Presto, MongoDBEffectively participate in the team’s planning, code reviews, KPI reviews, and design discussions leading to continuous improvement in these areas.You are passionate about working across the entire technology stack, from hardware, to embedded SW, to local software, to cloud-based services to optimize capability and drive reliability.","Strong quantitative background (Computer Science, Math, Physics, and/or Engineering), or at least 5 years of industry experience in a quantitative roleExperience programming with any of the following: Python, Go, C++, Scala, JavaExperience establishing pipelines using distributed processing and no-SQL technologies such as Apache Spark, Kafka, Elasticsearch, Presto, MongoDBEffectively participate in the team’s planning, code reviews, KPI reviews, and design discussions leading to continuous improvement in these areas.You are passionate about working across the entire technology stack, from hardware, to embedded SW, to local software, to cloud-based services to optimize capability and drive reliability.","Build highly scalable data pipelines to handle ingestion and processing of robot, manufacturing, & engineering dataEnable internal users by giving them APIs, services, and applications to allow them to access and interact with their dataWork closely with core engineering teams to consistently evolve data models & data schemas based on growing business and engineering needsBe a leader in the data space and help people understand what they could be getting from their data that they currently are not",None Found,None Found,"Job: Engineering
Primary Location: United States-California-US-CA-Sunnyvale
Schedule: Full-time
Requisition ID: 191586

Description

Company Description:
Who is Intuitive Surgical? The numbers tell an amazing story. Learn more about our company.

Joining Intuitive Surgical means joining a team dedicated to using technology to benefit patients by improving surgical efficacy and decreasing surgical invasiveness, with patient safety as our highest priority.

Patients First. Always. We build the world’s best surgical robotic systems.
Our surgical robots are deployed worldwide and help nearly a million people per year be cured of cancer and other ailments. Those people get back to their families and lives faster with less pain.

Are you passionate about technology? Do you want to have a direct impact on helping people live better lives?

About the role:
We’re looking for a Senior Data Engineer who is passionate and motivated to make an impact in creating a robust and scalable data platform for product analytics. You will have ownership of core engineering data pipelines that power metrics for engineering, manufacturing, post-market, marketing, and business teams with the purpose of driving data discoverability, accessibility, and analysis. You will be a catalyst of change and will be responsible for helping to re-invent the way the Instruments & Accessories business unit uses data. To be successful in the long run you will need to build sophisticated systems and retool antiquated processes to enable the broader engineering teams to move quickly with confidence.

Responsibilities:
Build highly scalable data pipelines to handle ingestion and processing of robot, manufacturing, & engineering dataEnable internal users by giving them APIs, services, and applications to allow them to access and interact with their dataWork closely with core engineering teams to consistently evolve data models & data schemas based on growing business and engineering needsBe a leader in the data space and help people understand what they could be getting from their data that they currently are not
Qualifications

Skills, Characteristics, and Technology:
Strong quantitative background (Computer Science, Math, Physics, and/or Engineering), or at least 5 years of industry experience in a quantitative roleExperience programming with any of the following: Python, Go, C++, Scala, JavaExperience establishing pipelines using distributed processing and no-SQL technologies such as Apache Spark, Kafka, Elasticsearch, Presto, MongoDBEffectively participate in the team’s planning, code reviews, KPI reviews, and design discussions leading to continuous improvement in these areas.You are passionate about working across the entire technology stack, from hardware, to embedded SW, to local software, to cloud-based services to optimize capability and drive reliability.

Bonus points:
Experience with the following: Apache Airflow, Elasticsearch, Presto, Snowflake, Kubernetes, PandasExperience installing, configuring, and managing AWS or on-premises server infrastructureExperience working with Docker development and deployment workflows
About the Advanced Analytics Team:
We are small group looking for strong technical leads that want an opportunity to move fast and help develop data platforms and analytics that will enable broader engineering teams to create better products for our patients. This is a new group at the company and our mandate is to promote significant change that will improve the productivity of our engineering teams in analyzing and understanding our products. We have the potential to overlap with many teams and as such emphasize tearing down organizational silos and a strong culture of collaboration. Nothing is taboo. If you see a high-leverage solution to a long-standing problem, tear it all down and build the right solution provided you have the will and a good plan to execute!

We are an AA/EEO/Veterans/Disabled employer.
We will consider for employment qualified applicants with arrest and conviction records in accordance with fair chance laws.

#RH1"
6,Senior Data Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,"
Cloud solution implementation experience with Azure Data Lake and Spark preferred
Minimum 8 years hands-on experience with SQL
At least one year of experience in scripting languages such as Python
Demonstrated experience in a cloud-based -computing environment such as AWS, Azure, or Google Cloud Platform
Big data processing techniques, preferred
Can work independently in ambiguous environment",None Found,None Found,None Found,None Found,"Lead Data Engineer

San Francisco, CA

About the role. . .
In order to continue and accelerate our growth, we are looking for a Lead Data Engineer with Cloud Solutions background to add to our Bay Area-based team.
Lead Engineer is responsible for building a large-scale data pipeline in cloud platform. This may involve in automation of manual processes to cloud environment. Candidate would direct the initiatives for creation of data sets. Delivering client value and ensuring high client satisfaction.
Core responsibilities for this position include, but are not limited to the following:
Extracts data from various databases; perform exploratory data analysis, cleanses, massages, and aggregates data
Employs scaling & automation to data preparation techniques - Introduces incremental improvements to data analysis, visualization, and presentation techniques to communicate discoveries
Researches relevant emerging empirical methods and quantitative tools
Possesses in-depth business knowledge in order to initiate and drive discussions with business partners to identify business issues needing analytic solutions
Leads innovative packaging and presentation of insights to business and broader analytics community
Develops processes to automate and scale insights operationalization
Develops and drives multiple cross-departmental projects
Establishes brand and team as subject matter experts in advanced analytics across departments.
Mentors data scientists in pioneering techniques and business acumen
Required Qualifications:
Cloud solution implementation experience with Azure Data Lake and Spark preferred
Minimum 8 years hands-on experience with SQL
At least one year of experience in scripting languages such as Python
Demonstrated experience in a cloud-based -computing environment such as AWS, Azure, or Google Cloud Platform
Big data processing techniques, preferred
Can work independently in ambiguous environment
About Logic20/20. . .
Logic20/20 is one of Seattle’s fastest growing full service consulting firms. Our core competency is creating simplicity and efficiency in complex solutions. Although we make it look like magic, we succeed by combining methodical and structured approaches with our substantial experience to design elegant solutions for even the most intricate challenges. Our rapid growth is in response to our ability to deliver consistently for our clients, which is directly related to the quality of the people we hire.
The past four years, we’ve been in the top 10 “Best Companies to Work For” ….. why? Our team members are highly self-motivated, comfortable conceiving strategies on the fly, and enjoy working both individually and as part of a team. Our environment is very high-energy and demanding, and individuals with remarkable enthusiasm and a can-do attitude are joining our team. We have lots of fun, focus on our employees and our clients, and work to bring our best to every opportunity."
7,Senior Big Data Engineer,"San Jose, CA 95134",San Jose,CA,95134,None Found,None Found,None Found,"
You will design and create multi-tenant systems capable of loading and transforming a large volume of structured and semi-structured fast moving data
Build robust and scalable data infrastructure (both batch processing and real-time) to support needs from internal and external users
Build Data Pipelines
Run ETL into Hadoop/Elastic Search",None Found,None Found,"Location: San Jose, CA or Atlanta, GA
For over 10 years, Zscaler has been disrupting and transforming the security industry. Our 100% purpose built cloud platform delivers the entire gateway security stack as a service through 150 global data centers to securely connect users to their applications, regardless of device, location, or network in over 185 countries protecting over 3,500 companies and 100 Million threats detected a day.
We work in a fast paced, dynamic and make it happen culture. Our people are some of the brightest and passionate in the industry that thrive on being the first to solve problems. We are always looking to hire highly passionate, collaborative and humble people that want to make a difference.
As a Big Data Engineer, you will work on building the next generation of Zscaler's security analytics platform. You will play a crucial role in building a platform to collect and ingest several billion (and growing) log events from Zscaler's globally distributed security infrastructure and provide actionable insights to customers and Zscaler's security researchers.
Responsibilities:
You will design and create multi-tenant systems capable of loading and transforming a large volume of structured and semi-structured fast moving data
Build robust and scalable data infrastructure (both batch processing and real-time) to support needs from internal and external users
Build Data Pipelines
Run ETL into Hadoop/Elastic Search
Required:
3+ years of experience in Python or Java development a must (Strong Scala would skills would be acceptable as well)
3+ years experience in application big data development (Spark, Kafka, Storm, Kinesis, & building data pipelines)
Ability to troubleshoot and find complex performance issues with queries on the Spark platform (Spark SQL)
Familiarity with implementing services following REST model
Excellent interpersonal, technical and communication skills
Ability to learn, evaluate and adopt new technologies
Bachelor's Degree in computer science or equivalent experience
Highly Desirable:
Experience working with data processing infrastructure
Experience with data serialization techniques and data stores for persisting events
What You Can Expect From Us:
An environment where you will be working on cutting edge technologies and architectures
A fun, passionate and collaborative workplace
Competitive salary and benefits, including equity
The pace and excitement of working for a Silicon Valley Unicorn
Why Zscaler?
People who excel at Zscaler are smart, motivated and share our values. Ask yourself: Do you want to team with the best talent in the industry? Do you want to work on disruptive technology? Do you thrive in a fluid work environment? Do you appreciate a company culture that enables individual and group success and celebrates achievement?

If you said yes, we’d love to talk to you about joining our award-winning team!
Learn more at zscaler.com or follow us on Twitter @zscaler. Additional information about Zscaler (NASDAQ : ZS ) is available at http://www.zscaler.com. All qualified applicants will receive consideration for employment without regard to race, sex, color, religion, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability.
#LI-JM1"
8,Principal Application Data Engineer - SQL Specialist,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"You are intellectually curious. You like building cool stuff. And you're nice.

At Noodle, we are not just building AI applications. We are going deep into industries that have yet to leverage AI at scale such as steel mills, distribution & logistics companies or consumer packaged goods. Our applications fit and integrate deeply into the supply chain starting with planning to manufacturing to delivery. The applications we build have to not only integrate into the existing software in these industries but have to talk to each other so really drive the value from AI. Turns out, we are one of the pioneers here charting a new course. This means that science behind building the software and the AI behind it has not settled. You will be part of a team that is charting this new course figuring out how to adapt software engineering best practices to delivering AI applications that fit within legacy software in non-tech industries. This is going to be a bumpy ride and we are looking for people who are not afraid of the unknown, are experts at their craft and can adapt and learn as we create a suite of new AI applications.

Responsibilities


Work collaboratively with an interdisciplinary team of management consultants, product managers, data scientists, data engineers, software engineers, UX designers to understand application data engineering requirements & convert requirements into functional & technical specifications.
Drive rapid application prototyping cycles from technical requirements, data-pipeline design, programming, debugging, and performance optimization.
Design, develop and evolve Noodle's AI Application Databases, Data Models & Data Pipelines.
Analyze complex Enterprise data, exploring entity relationships, performing data quality checks & validations.
Champion engineering and operational excellence, establishing metrics and process for regular assessment and improvement as well as mentoring of Junior Data Engineers.
Provide thought leadership and improve Noodle's Data Architecture interactively through innovations and adaptation of best practices & industry standards.

Qualifications


BS/BE/B.Tech or Advanced degree in a relevant field (Computer Science and Engineering, Technology and related fields). Master' s degree a plus.
6-9 years of industry experience in data driven software development.
7+ years of real-world experience in architecting & developing scalable data driven AI applications, data warehousing & business intelligence solutions.
Expert in writing SQL queries, procedures and user-defined functions.
Demonstrated proficiency as a lead with database development, automation, performance tuning, optimization and management projects.
Real life work experience with relational databases: PostgreSQL, MS SQL Server, Oracle etc.
Strong understanding of Big Data concepts and scenarios where it works. This includes understanding of the nature of distributed systems and its pitfalls.
Strong knowledge of database design and data modeling concepts.
Working knowledge of database security and data encryption.
Experience working with version control systems like: bitbucket, github, etc.
Experience working in a hybrid environment involving on-premise infrastructure and cloud databases will be a plus.
Hands-on experience with modern ETL tools, orchestration platform like Airflow and Python data processing libraries will be considered as an added advantage.
Familiarity with agile software development practices and DevOps is desirable.

Passion for learning and a desire to grow – Noodlers are life-long learners!"
9,Integrated Marketing Analytics Manager,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Department: Marketing & Customer Insights (MCI)
Title: Integrated Marketing Analytics Manager
Position Role/Description:
The Marketing & Customer Insights (MCI) organization has a dual mandate of providing objective customer research, analysis, and marketing effectiveness measurement while advancing the use of Adobe Marketing Cloud technologies to enable and track customer experiences across surfaces.
Within MCI, the Advanced Analytics team was established to focus on developing deep customer insights to support integrated marketing planning across channels. The group closely partners with Global and Field Marketing, IT, business unit leaders and other corporate functions to enhance understanding of our customers and their digital journey.
The team maintains a highly visible and strategically important role in delivering and facilitating understanding of insights to inform business strategies, and to track the performance of marketing specific activities against expectations. The work the team delivers is informed by the business needs for strategic customer understanding, and may range from media investment performance analysis, deep customer journey investigations, to customer segmentation or targeting overlays.
The talent mix of the team is part analyst and part data engineer. This role will focus on the data side. We are looking for a self-starter with marketing science experience and a wide range of technical skills to advance our analytics and execution capabilities. This includes advancing our marketing measurement strategy across all of our online and offline marketing channels. You will also develop our next generation of data and analytics stacks to guide our customer insights work.
A balance of technical and analytics skills as well as a working knowledge of marketing processes is key. A team-player, collaborative, mindset is essential.
Responsibilities:
· Ability to structure problems into data and analytics plan and execute.
· Hands-on creation of time-series models
· Ability to partner with a cross-functional team of technical and analytical partners
· Possible engagement with Adobe Marketing Cloud teams to evolve capabilities within the products.
· An understanding of effective visualization techniques for complex concepts or a background in visualizing “big data” use cases.
· Partner with media teams, campaign strategy teams, marketing and web analysts to gather inputs to inform business problems and analytical approaches.
· Provide ad hoc analysis and communicate results and insights with team and executives
What you need to succeed:
· Intellectual curiosity, flexibility, and high attention to detail
· Undergraduate degree in a quantitative field preferred
· Experience with SQL, R, Python
· Project/program management experience
· Good intuition with information and data; experience synthesizing large data sets to generate insights
· Outstanding communication skills to provide actionable reporting via written and in-person presentations. Must be able to clearly communicate results and actionable recommendations to constituencies at various levels of management
· Strong interpersonal skills and the ability to work as part of a diverse team and build cooperative, productive relationships with colleagues and stakeholders around the world
Desirable Skills:
· Understanding in one or more marketing channels (e.g. web, mobile, display, search)
· Experience in machine learning tools and techniques
· Experience developing and working on larger scale analytics / big data implementations
· Track record of consistent delivery and execution
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status."
10,Data Engineer- Python,"San Jose, CA 95113",San Jose,CA,95113,None Found, 5+ years of experience in core JAVA and SQL,None Found,None Found,None Found,None Found,"As a Senior Consultant, you will focus on managing the information supply chain from acquisition to ingestion, storage and the provisioning of data to points of impact by modernizing and enabling new capabilities. Information value is enhanced through enterprise-scale applications that enable visualization, consumption and monetization of both structured and unstructured data. Big data is becoming one of the most important technology trends that has the potential for dramatically changing the way organizations use information to enhance the customer experience and transform their business models.
Work you'll do

Senior Consultants work within an engagement team. Key responsibilities will include:
 Function as integrators between business needs and technology solutions, helping to create technology solutions to meet clients’ business needs.
 Identifying business requirements, requirements management, functional design, prototyping, process design (including scenario design, flow mapping), testing, training, defining support procedures and supporting implementations.

The Team

Analytics & Cognitive

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.


The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.


Analytics & Cognitive will work with our clients to:
 Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
 Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
 Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements


Qualifications

Required:

 5+ years of experience in core JAVA and SQL
 3+ years of experience in Python& Unix Shell Scripting
 3+ years of experience in building scalable and high performance data pipelines using Apache Hadoop, Map Reduce, Pig & Hive
 Experience with bigdata cross platform compatible file formats like Apache Avro & Apache Parquet
 Experience in Apache Spark is a plus
 1+ years of experience with data lake implementations, core modernizations and data ingestion

 1 or more years of hands on experience designing and implementing data ingestion techniques for real time and batch processes for video, voice, weblog, sensor, machine and social media data into Hadoop ecosystems and HDFS clusters.
 2+ years of experience leading workstreams or small teams
 Willingness for weekly client-based travel, up to 80-100% (Monday — Thursday/Friday)
 Bachelor’s Degree or equivalent professional experience

 Preferred:

AWS Certification, Hadoop Certification or Spark Certification
Experience with Cloud using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)
Experience with data integration products like Informatica Power Center Big Data Edition (BDE), IBM BigInsights, Talend etc.
Experience designing and implementing reporting and visualization for unstructured and structured data sets
Experience in designing and implementing scalable, distributed systems leveraging cloud computing technologies like AWS EC2, AWS Elastic Map Reduce and Microsoft Azure
Experience designing and developing data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching
Knowledge of data, master data and metadata related standards, processes and technology
Experience working with multi-Terabyte data sets
Experience with Data Integration on traditional and Hadoop environments
Ability to work independently, manage small engagements or parts of large engagements.
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint).
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment.
Eagerness to mentor junior staff.
An advanced degree in the area of specialization is preferred.

How you’ll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.


Benefits

At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitte’s culture

Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.


Corporate citizenship

Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.


Recruiter tips

We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.

#LI:PTY
#IND:PTY"
11,Senior Data Engineer - ETL Specialist,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"You are intellectually curious. You like building cool stuff. And you're nice.

At Noodle, we are not just building AI applications. We are going deep into industries that have yet to leverage AI at scale such as steel mills, distribution & logistics companies or consumer packaged goods. Our applications fit and integrate deeply into the supply chain starting with planning to manufacturing to delivery. The applications we build have to not only integrate into the existing software in these industries but have to talk to each other so really drive the value from AI. Turns out, we are one of the pioneers here charting a new course. This means that science behind building the software and the AI behind it has not settled. You will be part of a team that is charting this new course figuring out how to adapt software engineering best practices to delivering AI applications that fit within legacy software in non-tech industries. This is going to be a bumpy ride and we are looking for people who are not afraid of the unknown, are experts at their craft and can adapt and learn as we create a suite of new AI applications.

Responsibilities


Work collaboratively with an interdisciplinary team of management consultants, product managers, data scientists, data engineers, software engineers, UX designers to understand and implement application data engineering requirements.
Execute rapid application prototyping cycles from breaking down technical requirements in tasks, data pipeline design, programming and debugging to optimization of database code.
Orchestrate complex Enterprise data processing workflows, performing data quality checks & validations.
Liaise with Customer teams & Internal stakeholders on the Application data pipeline requirements.
Champion engineering and operational excellence, follow best practices and coding guidelines to deliver highly scalable application data processing components.
Help improve Noodle Data Pipeline Architecture through innovations and adaptation of best practices & industry standards.

Qualifications


BS/BE/B.Tech or Advanced degree in a relevant field (Computer Science and Engineering, Technology and related fields). Master' s degree a plus.
6+ years of industry experience in data driven software projects where 4+ years are on developing real-world scalable data pipelines to ingest, including data quality validations.
Demonstrated proficiency with modern ETL tools / frameworks and data pipeline orchestration platforms like Airflow.
Hands-on work experience in proficient querying - relational databases like PostgreSQL, MS SQL Server, Oracle, etc.
Working knowledge of Python (scripting), bitbucket or github (version control.)
Comfortable working with both high performance on-premises database installations and cloud instances (AWS or Azure) is a plus.
Good knowledge of database design and security, data modeling concepts and data encryption, data devops is a huge plus.

Passion for learning and a desire to grow – Noodlers are life-long learners!"
12,Data Engineer,"Santa Clara, CA",Santa Clara,CA,None Found,None Found,"
Experience with Cloud-based service and development environment, such as AWS or GCP
Proficiency with programming languages such as Python and Java
Proficient understanding of distributed computing principles
Good knowledge of Big Data querying databases, such as BigQuery, BigTable and MongoDB","
Experience with Cloud-based service and development environment, such as AWS or GCP
Proficiency with programming languages such as Python and Java
Proficient understanding of distributed computing principles
Good knowledge of Big Data querying databases, such as BigQuery, BigTable and MongoDB","
Selecting and integrating big data frameworks required to provide requested capabilities
Build analysis pipelines as well as visualization dashboards
Monitoring performance and iterate the solution fast",None Found,None Found,"About Shape Security

We are security and web experts, pioneers, evangelists, and elite researchers. We believe in the power of the Internet to be a positive force; our mission is to protect every website and mobile app from cybercriminals. Shape’s founders fought cybercrime at the Pentagon, Google, and other leading security companies. We are backed by some of the most prominent leaders and investors in the technology industry including Kleiner Perkins, Google Ventures, and more. Come be a part of our unparalleled team that is responsible for making the Internet a safer place for everyone.

Position Summary

We are looking for a Big Data Engineer that will work on the storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company.
Responsibilities
Selecting and integrating big data frameworks required to provide requested capabilities
Build analysis pipelines as well as visualization dashboards
Monitoring performance and iterate the solution fast
Skills and Qualifications
Experience with Cloud-based service and development environment, such as AWS or GCP
Proficiency with programming languages such as Python and Java
Proficient understanding of distributed computing principles
Good knowledge of Big Data querying databases, such as BigQuery, BigTable and MongoDB"
13,Sr. Data Engineer,"Pleasanton, CA 94588",Pleasanton,CA,94588,None Found,None Found,None Found,None Found,None Found,None Found,"Join our team and experience Workday!
It's fun to work in a company where people truly believe in what they're doing. At Workday, we're committed to bringing passion and customer focus to the business of enterprise applications. We work hard, and we're serious about what we do. But we like to have a good time, too. In fact, we run our company with that principle in mind every day: One of our core values is fun.
Job Description
Job Responsibilities:
Analyze, Design and Build robust and scalable Data Engineering solutions for structured and unstructured data conducive to Business Insights, Machine Learning, Reporting and Analytics.
Consistently build and manage evolving Data Model & Data Schema based on business and engineering requirements.
Build Scalable and reusable Abstractions/Data Pipelines to create, ingest or access data sets (batch, streaming or low latency APIs).
Improve the data quality and reliability of the pipelines using innovative solutions (monitoring and failure detection).
Evaluate new technologies and build prototypes for continuous improvements in Data Engineering.
Work effectively using scrum with multiple team members to deliver analytical solutions to the business and Engineering functions.
Provide technical leadership, mentor other Engineering team members and manage a small team of engineers.
Maintain high level of technical skills and strive to expand on technical knowledge through self-education or company provided education.
Demonstrated academic or professional Statistics and Mathematics skills.
Job Skills:
Experience with designing complex Data Models and Data Engineering Solutions for large scale Data Warehouse/Data Lakes from various heterogeneous Data Sources (Salesforce, Eloqua, Workday, Marketo, Gainsight, Adobe Analytics or other Sales, Marketing, Finance and Human Capital Management Applications).
Experience with Data Integration, Business Intelligence and Analytics tools like Pentaho Data Integration, Snaplogic Data Integration, Tableau Software and other open source and self-service analytics tool.
Strong Database Management system knowledge; Experience with Microsoft SQL Server, Amazon Redshift, Amazon Sagemaker, PostgreSQL, MySQL, Oracle and NoSQL Databases are Preferred.
Expertise in Object Oriented and/or Functional Programming skills (Java/Scala, Microsoft .Net, Espresso, Python, Unix/Linux/Perl scripting, SQL/PLSQL)
Experience with Agile development methodologies (Scrum, Pair Programming). Certified Scrum Product Owner Certification will be Preferred.
Minimum Education Requirements:
Bachelor’s/Master’s degree in Computer Science, Computer Engineering, Electrical Engineering, or related field of study.
Minimum Experience Requirements:
8+ years of experience with very large scale and complex Data Management Projects
#LI-SB7"
14,Sr Data Engineer - HPC UI,"Sunnyvale, CA 94086",Sunnyvale,CA,94086,None Found,None Found,None Found,None Found,None Found,None Found,"Description:
Senior Data Engineer – High Performance Computing UI
Target Data Sciences is revolutionizing the way Target retail uses data. Located in Sunnyvale, CA, it’s just across the street from the local train station in the heart of Silicon Valley. Originally opened in 2014, the Sunnyvale office is now home to more than 150 team members who work to make Target a more modern data-driven retail company. To learn more, view this article: https://corporate.target.com/article/2016/11/meet-target-sunnyvale
Team Introduction
The High-Performance Computing Group at Target not only aims to enable teams at Target to stream, filter, transform, and analyze high-bandwidth data in real-time, but also provide tools for data analysts and other team members to analyze and take action on their data streams.
What will you be doing:
As a Senior UI Engineer of the High-Performance Computing Research Group at Target, you are to design and implement user experiences that enable data analysts and engineers to use our high-performance streaming platform to its maximum potential. You’ll receive hands-on experience and exposure to designing and building real-time, state dependent, data driven UI applications.
Responsibilities:
Architect new features and reusable UI components using ReactJS
Collaborating with team in coming up with neat and usable user interfaces
Optimize UI applications for scalability, stability, and speed.
What you will learn:
You will architect, build, and maintain our UI application, meaning you’ll be receive exposure to a large variety of cutting edge front-end/data technologies.
You will get exposure to the full-stack of software systems (down to the computer architecture level) while collaborating with our talented engineers to tackle challenging problems.
Requirements:
Bachelor’s degree (preferably in CS, EE or related field)
Experience in developing real-world UI applications
Strong Proficiency in HTML5, CSS3, and JavaScript.
Good understanding of RESTFUL APIs and asynchronous request handling.
Experience or exposure to front-end tooling such as package managers, CSS-preprocessing, file-bundlers such as webpack etc.
Ability to use code version tool such as Git.
Experience with ReactJS.
A natural talent for design and attention to detail
Preferred Requirements:
Some exposure to low level programing languages and system/computer architecture knowledge.
Wire-framing and UI design skills
Americans with Disabilities Act (ADA)
Target will provide reasonable accommodations (such as a qualified sign language interpreter or other personal assistance) with the application process upon your request as required to comply with applicable laws. If you have a disability and require assistance in this application process, please visit your nearest Target store or Distribution Center or reach out to Guest Services at 1-800-440-0680 for additional information.
Qualifications:"
15,Senior Data Engineer,"Newark, CA",Newark,CA,None Found,None Found,"
Bachelor or Masters in Software Engineering or Computer Science
+4 years of experience in Data Engineering and Business Intelligence.
Proficient in IoT tools such as MQTT, Kafka, Spark
Proficient with AWS, S3, Redshift
Experience with Presto and Parquet/ORC
Proficient with Apache Spark and data frame.
Experienced in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python,
Experience with Columnar database such as Redshift, Vertica
Great verbal and written communication skills.",None Found,None Found,None Found,None Found,"Leading the future of luxury mobility

Lucid’s mission is to inspire the adoption of sustainable energy by creating the most captivating luxury electric vehicles, centered around the human experience. Working at Lucid Motors means having a shared vision to power the future in revolutionary ways. Be part of a once-in-a-lifetime opportunity to transform the automotive industry.

We are looking for a Data Engineer, Streaming who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If building large and building fast, working with a very talented team of engineers and collaborating with the brightest mind in the Automotive industry is what you like, Lucid is the best to experience it.
The Role
Hands-on design and develop streaming and IoT data pipelines.
Developing streaming pipeline using MQTT, Kafka, Spark Structure Streaming
Orchestrate and monitor pipelines using Prometheus and Kubernetes
Deploy and maintain streaming jobs in CI/CD and relevant tools.
Python scripting for automation and application development
Design and implement Apache Airflow and other dependency enforcement and scheduling tools.
Hands-on data modeling and data warehousing
Deploy solution using AWS, S3, Redshift and Docker/Kubernetes
Develop storage and retrieval system using Presto and Parquet/ORC
Scripting with Apache Spark and data frame.
Qualifications
Bachelor or Masters in Software Engineering or Computer Science
+4 years of experience in Data Engineering and Business Intelligence.
Proficient in IoT tools such as MQTT, Kafka, Spark
Proficient with AWS, S3, Redshift
Experience with Presto and Parquet/ORC
Proficient with Apache Spark and data frame.
Experienced in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python,
Experience with Columnar database such as Redshift, Vertica
Great verbal and written communication skills.
Be part of something amazing

Come work alongside some of the most accomplished minds in the industry. Beyond providing competitive salaries, we’re providing a community for innovators who want to make an immediate and significant impact. If you are driven to create a better, more sustainable future, then this is the right place for you.

At Lucid, we don’t just welcome diversity - we celebrate it! Lucid Motors is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, national or ethnic origin, age, religion, disability, sexual orientation, gender, gender identity and expression, marital status, and any other characteristic protected under applicable State or Federal laws and regulations.

To all recruitment agencies: Lucid Motors does not accept agency resumes. Please do not forward resumes to our careers alias or other Lucid Motors employees. Lucid Motors is not responsible for any fees related to unsolicited resumes."
16,Data Engineer,"Sunnyvale, CA",Sunnyvale,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"As Verizon’s media unit, our brands like Yahoo, TechCrunch and HuffPost help people stay informed and entertained, communicate and transact, while creating new ways for advertisers and partners to connect. With technologies like XR, AI, machine-learning, and 5G, we’re transforming media for tomorrow, too. We're creators and coders, dreamers and doers creating what's next in content, advertising and technology.
About the role:
Flurry is a leader in mobile application analytics, empowering companies of all sizes to unlock mobile growth. Flurry empowers app makers to improve acquisition, engagement, retention and revenue with the world’s most adopted mobile app analytics solution. Over 40,000 companies - including Angry Birds, McDonald’s, Microsoft, Samsung and Baidu - use Flurry across a quarter of a million applications. Flurry operates a high-scale, high performance big data operation, delivering behavioral analytics insights on 10 billion daily end-user app sessions.
What we’re looking for:
Flurry is looking for a senior content marketing manager to join and lead our content efforts. You'll develop a deep understanding of our target audiences, the mobile landscape, and Flurry technology—and use that knowledge to tell compelling stories that build brand awareness and thought leadership.
What you’ll do:
Ideate, research, write and edit content for the industry-leading Flurry blog with a focus on educating the market, increasing brand awareness and driving demand
Develop and execute a content strategy to create and promote long-form offerings including ebooks, guides, infographics, etc. that increase engagement and drive leads
Identify, own, and track key content metrics, including content-driven lead creation
Work with our analysts, data scientists and other collaborators to identify relevant and timely industry topics, influencers, and subject-matter experts to aid in content creation
Develop and maintain an editorial style guide to ensure all copy is consistent with Flurry’s messaging, voice, and tone
Cross-team collaboration on marketing programs, including campaigns and larger initiatives that may span coordination across other Verizon Media groups
What would make you a strong fit:
Bachelor’s degree with 4-6 years of content marketing, writing, and/or editorial experience in SaaS, data and analytics, or the mobile application space
Excellent writer and researcher—you can tell stories and communicate technical concepts to a wide variety of audiences without using jargon
Organized and self-motivated with the ability to define goals and prioritize work based on what’s most impactful to the business and our target audience
Familiarity with SEO best practices, content categorization, distribution, promotion, and measurement
Verizon Media is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Verizon Media is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. If you need accessibility assistance and/or a reasonable accommodation due to a disability, please email ApplicantAccommodation@verizonmedia.com or call 408-336-1409. Emails/calls received for non-disability related issues, such as following up on an application, will not receive a response.
Currently work for Verizon Media? Please apply on our internal career site."
17,Machine Learning Lead,"Sunnyvale, CA 94087",Sunnyvale,CA,94087,None Found,"Masters or equivalent degree in a computational science or engineering with 4+ years of experience in machine learning/data engineer role;Familiarity with distributed computing frameworks (e.g., Hadoop/Spark) and relational data base (e.g., Oracle, MySQL), and knowledge of NoSQL database;Strong implementation experience with a programming language (e.g., Jave/C++/Scala) and a scripting language (e.g., Python/Perl/Ruby), and familiarity with Linux/Unix/Shell environments;Strong hands-on skills in building scalable and reliable data pipelines for sourcing, cleaning and manipulating large volumes of data;Basic understanding of machine learning/statistics.",None Found,None Found,None Found,None Found,"Position Description
We are looking for outstanding machine learning/data engineers with skills in distributed computing system and experience working with very large scale of data, who will work closely with machine learning/data scientists in the team, and contribute to Advertising Technology in driving the future of ad targeting, personalization, relevance, ranking, and campaign optimization.

Join us if you want to be spending your time on:
Building, maintaining and monitoring scalable data pipelines to support modeling and optimization products;Analyzing, identifying, and debugging data related issues to ensure quality and stability of models and applications;Working with machine learning/data scientists to build prototypes and perform experiments;Working with engineering team to productize new features and products;Performing ad hoc data related analysis.
Minimum Qualifications
Masters or equivalent degree in a computational science or engineering with 4+ years of experience in machine learning/data engineer role;Familiarity with distributed computing frameworks (e.g., Hadoop/Spark) and relational data base (e.g., Oracle, MySQL), and knowledge of NoSQL database;Strong implementation experience with a programming language (e.g., Jave/C++/Scala) and a scripting language (e.g., Python/Perl/Ruby), and familiarity with Linux/Unix/Shell environments;Strong hands-on skills in building scalable and reliable data pipelines for sourcing, cleaning and manipulating large volumes of data;Basic understanding of machine learning/statistics.
Additional Preferred Qualifications
Experience building and maintaining large scale data pipelines in online advertising, recommender system, ecommerce or relevant areas;Experience building and/or maintaining machine learning models and pipelines;Familiarity with job scheduler (e.g., Jenkins/Azkaban/Airflow);Experience with Elastic Search/Solr.
Company Summary
The Walmart eCommerce team is rapidly innovating to evolve and define the future state of shopping. As the world’s largest retailer, we are on a mission to help people save money and live better. With the help of some of the brightest minds in technology, merchandising, marketing, supply chain, talent and more, we are reimagining the intersection of digital and physical shopping to help achieve that mission.
Position Summary
This position is in the data science team under the Advertising Technology organization. The mission of the Advertising Technology organization is to advance Walmart eCommerce by driving higher value for our customers and vendor partners. Walmart is investing in building a world class advertising platform and the Ads team is responsible for defining and performance advertising products that drive discovery, sales and profits. The team operates an end to end advertising platform that includes a scalable ad service that serves hundreds of millions of impressions each day, sophisticated ad matching algorithms, real-time reports, self-service interface for end to end program management etc.

We are a highly motivated group of Big Data Geeks, Data Scientists and Applications Engineers, working in small agile group to solve sophisticated and high impact problems. We are building smart data systems that ingest, model and analyze massive flow of data from online and offline user activity. We use cutting edge machine learning, data mining and optimization algorithms on ad relevance, ranking and campaign optimization.

Join us if you want to be spending your time on:
Gathering and analyzing data, identifying modeling and optimization problems, devising solutions and building prototypes;Formulating machine learning/statistical approaches while paying attention to business metrics, designing features from the rich data available from many sources, training, evaluating, and deploying models;Researching and implementing methodologies to measure the impact of the technologies;Initiating and proposing unique and promising modeling projects, developing new and innovative algorithms and technologies, pursuing patents where appropriate;Developing high-performance algorithms for precision targeting, user engagement prediction, and ad relevance/ranking; testing and implementing these algorithms in scalable, product-ready code; interacting with other teams to define interfaces and understanding and resolving dependencies;Staying current on published data mining, machine learning and modeling techniques and competing technologies and sharing these findings with scientists and engineers in the organization;Maintaining world-class academic credentials through publications, presentations, external collaborations and service to the research community."
18,Data Engineer,"San Mateo, CA",San Mateo,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are changing the way healthcare uses data. Currently, about 80% of healthcare data is underused because it is too messy and unstructured for humans to efficiently analyze. The healthcare industry needed an intelligent technology that could extract insights quickly and accurately. And that is where we came in. Apixio augments the ability to read, decipher, and understand patient information. This ultimately translates into better care delivery, lower costs, and streamlined processes.

The opportunity at Apixio:
Apixio is a mission-driven data science company passionate to make the practice of medicine more science than art. We are looking for a high-level Data Engineer, who shares our vision, to join our winning team and help us make a difference. This role will be filled by someone who can gain a clear understanding of the current infrastructure and be able to understand how to align with the needs and objectives of the science team. Abilities for training and testing will come into play with their motivation to make constant improvements.

We will be asking you to:

Build and maintain Scala spark / Hadoop pipelines at Apixio that are involved in our training / testing infrastructure for machine learning models
Build and maintain Spark pipelines for ETL of data that write to our data warehouse and maintain the Apixio data warehouse
Contribute to design and implementation of software pipelines that enable Apixio to update its internal proprietary algorithms in a scalable and reliable manner
Contribute to our core code base that performs machine learning in a production setting

To be effective at this role you will need to be:

Curious, willing to learn new things and try new things
Data Driven
Communicative - (willing to stand by decisions / ideas that they have)

We are assuming that you have:
A degree in computer science


Experience working with Scala / Java (min 2 years) in a professional setting
Experience working with Big Data Technologies (Hadoop, spark, etc… )

About Apixio:
We are a mission-driven company developing insights from data for a healthier world. We are helping people make better decisions using artificial intelligence to deliver quality healthcare. We are the technology leader in our space and growing at a 60% year over year rate and show no signs of slowing down. Our success is driven by a team of experienced, passionate and fun engineers, data scientists and business professionals that are working to solve some of the most complex problems, with some of the most innovative technologies and techniques in AI and machine learning. We are well funded by leading organizations such as Bain Capital and SSM, and serve more than 35 national and regional health plan and provider clients, and growing.

What Apixio can offer you:

Meaningful work to improve the healthcare industry
Competitive compensation, including pre-IPO equity
Exceptional benefits, including medical, dental and vision, FSA
401k
Catered, free lunches
Parties, picnics and Friday Happy Hour
Generous Vacation Policy
Free Parking
Subsidized Gym membership/Wellness Program
Modern open office in beautiful San Mateo, CA

"
19,Senior Big Data Engineer / Big Data Engineering Manager,"San Mateo, CA",San Mateo,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Title: Big Data Engineering Manager / Senior Big Data Engineer

Location: San Mateo, CA

Reporting to: Senior Director of Data

Headquartered in the Bay Area with offices in Toronto, Canada; Jaipur, India and Austin, Texas, venture-funded Punchh is the world leader in innovative digital marketing products for brick and mortar retailers, combining AI technologies, mobile-first expertise, and omnichannel communications designed to dramatically increase customer lifetime value. Leading global chains in the restaurant, health and beauty sectors rely on Punchh to grow revenue by building customer relationships at every stage to becoming brand loyalists, including more than 100 different chains representing more than $12B in annual spend, 30,000 locations globally, 26M+ consumers, and 1M+ transactions daily. Punchh boasts a customer list that includes Pizza Hut, Quiznos, Coffee Bean & Tea Leaf and many more.

About this role

Reporting to the Head of Data Science, the data eng manager plays a critical role in leading Punchh's data innovations. He/she will help create cutting-edge big data solutions by leveraging his/her prior industrial experience.

This role requires close collaborations with machine learning, software engineering and product, serving not only internal teams but also our business clients.

What You'll Do


Become company's domain expert in Big Data and related technologies. Continuously improve our internal data infrastructure.
An inspirational hands-on mentor to the other functions of the data team, guiding team members on establishing the best industrial practices.
Own and project-manage Punchh's internal data pipeline supporting machine learning, BI products, and analytics.
Represent Punchh's expertise in advanced technologies in a variety of media outlets.
Work with large data sets and implement sophisticated data pipelines with both raw and structured data.
Manage and optimize our internal data pipeline that supports marketing, customer success and data science to name a few.
Work with the senior leadership to define Punchh's long term strategies in advanced technologies.
Occasional business travels are required.

What You'll Need


7+ years of experience as a big data engineering professional, developing scalable big data solutions.
2+ years of experience as either people manager or technical lead manager of a big data team.
Advanced degree in computer science, engineering or other related fields.
Extensive knowledge with cloud technology, e.g. AWS and Azure.
Excellent programming background in Python and Java, with hands-on experience with Kafka and Spark.
Extensive experience in using big data technologies to build data products.
Exceptional communication skills and ability to articulate a complex concept with thoughtful, actionable recommendations.
Strong problem solving skills with demonstrated rigor in maintaining a complex data pipeline.
Excellent software engineering background. High familiarity with software development life cycle.

Advanced knowledge of big data technologies, such as programming language (Python, Java), relational (Postgres, mysql), NoSQL (Mongodb), Hadoop (EMR) and streaming (Kafka, Spark).

Benefits


Healthcare coverage, FSA, HSA
Life and AD&D insurance
401K
Competitive salaries, bonus and stock options
Professional development
EAP
Maternity and Paternity (Bonding) Leave
PTO
Paid holidays
Free lunch every single day, social events, plus a well stocked refrigerator

Punchh is proud to provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics.

We also provide reasonable accommodations to individuals with disabilities in accordance with applicable laws.

Notice to recruiters and placement agencies: If you are a recruiter or placement agency, please do not submit résumés to any person or email address at Punchh prior to having a signed agreement with Human Resources. Punchh is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Also, any résumés sent to us without an agreement in place will be considered your company's gift to Punchh and may be forwarded to our Talent Acquisition team."
20,"Big Data Engineer, Catalog DataWorks","Cupertino, CA",Cupertino,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Bachelor's degree or higher in computer science or math is required.Data engineering Knowledge including ETL, Machine learning Models, NoSQL, Hive/SparkSQL/Athena, NoSQLStrong computer science fundamentals - algorithms, data structures , design patterns and programming (Java , Scala / Python)At least 5 years of software development experience.At least 3 years of experience of using Big Data systems.

Amazon's Catalog DataWorks team is looking for highly motivated data engineers. We are embarking on multiple new initiatives to re-organize Amazon's catalog of billions of products, in new and interesting views, that drive several features Amazon's customers love. Today, these views drive hundreds of popular features like product recommendations, clustering of similar products, and shopping with Alexa. We will build a new near real-time Catalog Data Lake on AWS, to enable engineers and scientists across Amazon to solve customer problems faster. Come join us on this exciting journey!

As a data engineer on this team, you will own the Catalog Data Lake end-to-end. You will work closely with business partners to synthesize technical requirements.Your focus is at the team level on a major portion of existing or new data architecture (e.g., large or significant dataset, mid-size data solutions). You create coherent Logical Data Models that drive physical design.Your responsibilities may range from optimizing operational data storage to processing semi-structured data streams to building self-service business intelligence infrastructure for analysts. You take on projects and make enhancements that improve data processes (e.g., data auditing solutions, management of manually maintained tables, automating, ad-hoc or manual operation steps). You will use industry technologies like Spark, MapReduce, NoSQL, Parquet as well as modern AWS offerings like EMR, Glue, Athena, and Redshift. We are fortunate to be at the cusp of innovation in both the e-commerce business as well as cloud technology. As a key stakeholder, you will constantly develop ETLs, Queries, new patterns, algorithms, models for ranking, anomaly, pricing, etc.

Experience in building/working on large scale distributed systems like Hadoop, Spark in the cloud.Experience with industry standards big data technologies (Spark, Kafka, Hive, Presto, NoSQL, or AWS equivalent)Data modeling experience with columnar data formats (Parquet, ORC etc.)ETL patterns"
21,Senior Data Engineer (SWE),"Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,"
Design and implement product features
Translate business requirements into data models. Augment the data warehouse to incorporate new data and support new analytics requirements
Craft optimal data processing architecture and systems for new data pipelines
Implement instrumentation to measure query performance
Rewrite and optimize relational queries (SQL) to improve product performance
Work with structured and semi-structured data
",None Found,"
5+ years of experience in the data warehouse and data analytics space
MUST: experience with SQL query rewrite, query optimization
Experience working in data architecture and relational data modeling
Experience with large scale analytic databases (e.g., Redshift, Snowflake, Teradata, Vertica)
Strong experience in scaling and optimizing schemas, performance tuning SQL, OLAP and Data Warehouse environments
Strong in relational algebra
Proficiency with Java or related programming language
BS in computer science or related fields. MS/PhD a plus
","As more companies adopt public cloud infrastructure and the increase sophistication and harm caused by cyber attacks, the ability to safeguard companies from these threats have never been more urgent.

Lacework's novel approach to security fundamentally converts cyber security into a big data problem. We are a startup based in Silicon Valley that applies large scale data mining and machine learning to public cloud security. Within a cloud environment (AWS, GCP, Azure), our technology captures all communication between processes/users/external machines and uses advanced data analytics and machine learning techniques to detect anomalies that indicate potential security threats and vulnerabilities.

The company is led by an experienced team who have built large scale systems at Google, Paraccel (Amazon Redshift), Pure Storage, Oracle, and Juniper networks. Lacework is well funded by a tier one VC firm and is based in Mountain View, CA.
In this role, you are a software engineer building security features into our product in Java and Go. Because you have expertise in databases, you will design data models that forms the basis of our anomaly detection algorithms and event investigation tools. You will extend our existing data models to incorporate new data sets and to support new use cases.

Responsibilities:

Design and implement product features
Translate business requirements into data models. Augment the data warehouse to incorporate new data and support new analytics requirements
Craft optimal data processing architecture and systems for new data pipelines
Implement instrumentation to measure query performance
Rewrite and optimize relational queries (SQL) to improve product performance
Work with structured and semi-structured data

Requirements:

5+ years of experience in the data warehouse and data analytics space
MUST: experience with SQL query rewrite, query optimization
Experience working in data architecture and relational data modeling
Experience with large scale analytic databases (e.g., Redshift, Snowflake, Teradata, Vertica)
Strong experience in scaling and optimizing schemas, performance tuning SQL, OLAP and Data Warehouse environments
Strong in relational algebra
Proficiency with Java or related programming language
BS in computer science or related fields. MS/PhD a plus

Lacework supports Green Card and H-1B sponsorship. We also have an office in Vancouver and sponsor Canadian work permits/permanent residence.

Lacework is an Equal Opportunity Employer. It is the policy of Lacework to provide equal employment opportunity to all persons, regardless of age, race, religion, color, national origin, sex, political affiliations, marital status, non-disqualifying physical or mental disability, age, sexual orientation, membership or non-membership in an employee organization, or on the basis of personal favoritism or other non-merit factors, except where otherwise provided by law."
22,Senior Data Engineer,"Sunnyvale, CA 94085",Sunnyvale,CA,94085,None Found,None Found,None Found,None Found,None Found,None Found,"Senior Data Engineer
Sunnyvale, US- Full Time
LotusFlare is a product company with engineering offices in Silicon Valley, Belgrade and Kiev. Our solution is adopted by leading telecom providers and lifts their product infrastructure into the digital age. We are replacing traditional business backends with an engagement-centric dynamic product stack. Our cloud-native SaaS platform running on leading public clouds as well as supporting on-premise private and hybrid clouds is based on latest technologies picked from the CNCF stack.
LotusFlare, founded by early product and growth staff from Facebook, is backed by leading investors including Social Capital and Google Ventures. LotusFlare provides a platform for telecom operators to create a 100% digital customer experience where subscribers can choose and port a mobile number, order SIMs, and devices, track shipping, choose and purchase plans, discover and consume content, pay bills, receive loyalty rewards, and access customer service. LotusFlare also provides a Growth platform to drive user acquisition, engagement, and monetization on digital products. LotusFlare's clients include leading companies such as Verizon Wireless, Telenor, Ooredoo, Singtel, Maxis, Globe, LinkedIn, and Skype.
RESPONSIBILITIES
As Senior Data Lead Engineer on our team, you would be responsible for designing, building a shipping and maintaining our critical data platform. In this role, you will lead the development of our Data Warehouse/Data Lake mentoring our cross-functional team of engineers and Data Scientists. Besides architecting and implementing you would also be responsible for collaborating with our Growth/Marketing teams to provide useful insights using various ML driven models.
REQUIREMENTS
BA/BS in Computer Science or other equivalent technical disciplines
You have strong programming skills in Java/Scala or equivalent
5+ Years of experience creating ETL pipelines using Spark/Hadoop/Kafka/ClickHouse
2+ Years experience leading and/or mentoring junior engineers
Experience writing complex SQL and ETL processes
Hands-on Experience with AWS
Knowledge of ClickHouse, Apache Spark, Kafka, and similar technology stacks
Machine learning expertise is a plus
Benefits
Competitive base salary and stock options
Full medical/dental/vision package
Generous vacation time and paid holidays
Perks
High impact work; influence the strategic direction of the company
Access to the latest stacks, modernized tools
Accelerated career growth and opportunity
Sharp, motivated co-workers in a flexible office environment
LUmFcmxiw1"
23,"Bearing.ai - Data Engineer Palo Alto, CA","Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"AI is the new electricity: Just as electricity transformed numerous industries starting 100 years ago, AI is now poised to do the same. AI Fund is a startup studio building new AI companies from the ground up. Our companies bridge AI technology and applications, focusing on industries and problems that we believe move the world forward. Founded by Dr. Andrew Ng, our team merges a deep understanding of AI technology with a proven ability to start new ventures. We have a unique business model that prepares founding members of a portfolio company with technical validation of the idea, experienced back office operational support and our team’s AI know how as well as connections within the AI community. This is an excellent opportunity for you to get in on the ground floor of an exciting AI Venture Firm and through it, grow its portfolio.


Dogpatch Shipping is an early-stage startup at the forefront of bringing AI to the maritime shipping industry. This is a trillion dollar industry that moves 90% of the goods we interact with on a daily basis, but has traditionally lagged far behind other industries in adopting new technologies. At Dogpatch Shipping, we’re changing that. We’re building an AI-enabled product that solves the shipping industry’s biggest pain point and we already have some of the world’s biggest shippers as our partners.

We are currently looking for an exceptionally talented Data Engineer to add to our small team. This person will be responsible for leading our Data Engineering efforts in addition to guiding our initial Dev Ops efforts and contributing to the team’s Machine Learning model development.

Since this person will be joining an early-stage startup at the ground level they’ll need to be able to wear multiple hats and thrive while working in a dynamic environment.
Primary responsibilities:
Assess data quality coming from our partners, develop a data quality framework and work with our partners to optimize data quality moving forward
Architect, develop and optimize ongoing ETL pipelines for that data. These pipelines must be built to support scale
Help develop and validate complex machine learning models
Design and implement overall data security strategy for Dogpatch Shipping Mentor/support other technical staff on data modeling and ETL related issues
Effectively communicate data strategy with a wide range of stakeholders
Help interview candidates and further build out the team
What You Must Bring:
Must Haves:
3+ years of experience with data warehouse technical architecture
3+ years of experience with Python, Spark, ETL/Data engineering, Docker/Kubernetes, automation/devops in AWS
S3, EC2, EMR, RDS, Redshift, Glue, Lambda, EKS, Sagemaker experience
Experience setting up security for Redshift, S3, and EC2
Basic knowledge in machine learning model development
Entrepreneurial grit - You are ready to roll up your sleeves and willing to be scrappy in order to carry out plans under time and resource constraints
Bachelor’s or Master’s degree in computer science or software engineering
Nice to Haves:
Experience with data visualization tools such as TableauKnowledge of the shipping industry or experience with logistics products
What We Have to Offer:
Free snacks, drinks, lunches and dinners daily
401(k) plan
Untracked PTO including a week off between Christmas and New Year’s
Variety of medical, dental and vision coverage to choose from
Collegial atmosphere
Direct and transparent communication styleInnovative environment
Work with global AI Leaders
Be part of a great cause to improve lives everywhere through artificial intelligence
This is a full-time position based in Palo Alto, California. You must already have, or be able to obtain, authorization to work in the United States.

At AI Fund, we are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants without regard to race, color, religion, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, gender expression, genetic information (including characteristics and testing), military and veteran status, and any other characteristic protected by applicable law. AI Fund believes that diversity and inclusion among our employees is critical to our success as a company, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool. Selection for employment is decided on the basis of qualifications, merit, and business need.

This is a full-time position based in Palo Alto, California. You must already have, or be able to obtain, authorization to work in the United States."
24,Senior Data Engineer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.
Job Description
About Arm

Arm® technology is at the heart of a computing and connectivity revolution that is transforming the way people live and businesses operate. Our advanced, energy-efficient processor designs are enabling the intelligence in 86 billion silicon chips and securely powering products from the sensor to the smartphone to the supercomputer. Our large footprint in the stack of technologies that enables IoT applications, creates an unparalleled opportunity to be at the forefront of the Machine Learning and data driven revolution.
We are a fast growing, diverse and dynamic team of enthusiastic professionals who share a vision and real passion for building a technology foundation for this emerging industry. We are looking for a hardworking and hands-on data engineer as part of our data science team within ISG organization.
The role will be based in San Jose with occasional travel to other Arm sites, customers and partners.

 What will I be accountable for?

You work closely with internal/external key partners across the globe to understand and document requirements to implement and optimize data workflow
You take lead in designing pipelines for ETL and data access (e.g. scrape, clean, prepare, catalogue data)
You will consolidate and govern data from different sources
You appreciate operating in agile environment and being the interface to the product team
You will successfully maintain data process and model deployment by applying best software practices
You are the database wizard and enjoy enabling the data science with relevant and consolidated data
You will be curious and open-minded about different tools and frameworks, and you can adopt the right one for the task at hand
Job Requirements
What skills, experience, and qualifications do I need?

Bachelor’s degree in Computer Science, Engineering or related field, with at least four years of experience in a professional setting
You are experienced and passionate to design, develop, unit test, code review, build and produce the deployment artifacts.
Proven coding experience with at least one scripting language like Python and another language like Java, Node; JavaScript will be a plus
Deep understanding of trade-offs for different methods to scale and stabilize the scheme of database engines
Nice to have:
Built pipeline to harvest data and put them in a format that is easily accessible by different part of the organization
Hands-on experience with cloud compute experience such as AWS
Demonstrable strong interpersonal skills, excellent written and spoken English to communicate openly and effectively

At Arm, we are guided by our core beliefs that reflect our rare culture and guide our decisions, defining how we work together to defy ordinary and shape extraordinary:
We not I
Take daily responsibility to make the Global Arm community thrive.
No individual owns the right answer. Brilliance is collective.
Information is crucial, share it.
Realise that we win when we collaborate — and that everyone misses out when we don’t.
Passion for progress
Our differences are our strength. Widen and mix up your network of connections.
Difficult things can take unexpected directions. Stick with it.
Make feedback positive and expansive, not negative and narrow.
The essence of progress is that it can’t stop. Grow with it and own your own progress.
Be your brilliant self
Be quirky not egocentric.
Recognise the power in saying ‘I don’t know’.
Make trust our default position.
Hold strong opinions lightly.

#LI-PW1

Benefits
Your particular benefits package will depend on position and type of employment and may be subject to change. Your package will be confirmed on offer of employment. Arm’s benefits program provides permanent employees with the opportunity to stay innovative and healthy, ensure the wellness of their families, and create a positive working environment.

Annual Bonus Plan
Discretionary Cash Awards
401(k), 100% matching on first 6% eligible earnings
Medical, Dental & Vision, 100% coverage for employee only, shared cost for dependents
Basic Life and Accidental Death and Dismemberment Insurance (AD&D)
Short Term (STD) and Long Term (LTD) Disability Insurance
Vacation, 20 days per year with option to buy 5 more.
Holidays, 13 days per year
Sabbatical, 20 paid days every four-years of service
Sick Leave, 7 days per year
Volunteering, four hours per month (TeamARM)
Office location dependent: café on site, fitness facilities, team and social events
Additional benefits include: Flexible Spending Accounts for health and dependent care, EAP, Health Advocate, Business Travel Accident Program & Commuter programs.
ARM, Inc. (USA) participates in E-Verify. For more information, please refer to www.dhs.gov/E-Verify
About Arm
Arm® technology is at the heart of a computing and connectivity revolution that is transforming the way people live and businesses operate. From the unmissable to the invisible; our advanced, energy-efficient processor designs are enabling the intelligence in 86 billion silicon chips and securely powering products from the sensor to the smartphone to the supercomputer. With more than 1,000 technology partners including the world’s most famous business and consumer brands, we are driving Arm innovation into all areas compute is happening inside the chip, the network and the cloud.

With offices around the world, Arm is a diverse community of dedicated, innovative and highly talented professionals. By enabling an inclusive, meritocratic and open workplace where all our people can grow and succeed, we encourage our people to share their unique contributions to Arm's success in the global marketplace.

About the office
The Arm San Jose office is nestled in California's Silicon Valley, just south of San Francisco. Serving as Arm’s North America headquarters, the office houses employees serving all divisions of Arm. World-renowned Stanford University is seen as a hub of innovation, helping to make San Jose one of the US’s fastest growing cities.

San Jose, California USA

Arm Inc.

150 Rose Orchard Way

San Jose, CA 95134-1358"
25,Intern - IT Big Data Engineer,"Santa Clara, CA",Santa Clara,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Description
Our Mission
At Palo Alto Networks® everything starts and ends with our mission:

Being the cybersecurity partner of choice, protecting our digital way of life.

We have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish – but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
Your Career
Our AI & Analytics group is responsible for working with various business owners/stakeholders from Sales, Marketing, GCS, Infosec, Operations, and Finance to solve difficult business problems which will have a direct impact on the metrics defined to showcase the progress of Palo Alto Networks.
We leverage the latest technologies from Cloud & Big Data ecosystem to improve business outcomes and create through prototyping, Proof-of-Concept projects, and application development. We are seeking an intern for our team who will work closely with other Principal Engineers, Product Managers, Data Engineers & Data Scientists. In this role, you will work in the AI & A team to design, and deliver our next generation data services.
Our Summer Internship Program from May-August or June-September provides you:
1:1 mentorship
Fun and engaging events that inspire your intellectual curiosity
Opportunities to expand your knowledge and work on challenging projects
Connections to other interns, recent grads, and employees across the company as well as our leaders
Your Impact
Build Internal or external customer facing applications in Cloud & Hadoop environments.
Develop low latency, high throughput data pipelines in Hadoop and Cloud using leading edge technologies like Spark, Kafka, Beam, and other cloud specific technologies.
Partner with data analysts, product owners to gather business and system requirements
Build and Integrate applications with other external applications
Develop rich front-end experiences using technologies such as Tableau, Angular JS, JavaScript, Node.js, React framework, etc.
Assist with design, development of features to perform predictive analytics on big data sets
Your Experience
Currently working towards a B.A./B. S/B.E./MS in a technical area (Computer Science, Engineering, etc.)
Basic understanding of software design and development
Basic understanding of developing cloud-based services (GCP, AWS, etc.)
Good working knowledge of at least one contemporary programming language such as Python, Java, Scala, and JavaScript.
Basic understanding of data analysis methods and approaches
Excellent oral and written communication skills
Requirements – All applicants must be pursuing a 4-year Undergraduate Degree, a 2-year Master’s Degree or a Doctorate degree and returning to school in the fall. You must have authorization to work within the United States.

*Please note that we will not sponsor applicants for work visas for this position*
The Team
Our Data engineering team is responsible for deep diving into data, figuring out issues with the quality of data and coming up with advanced techniques to bring insights out of the data to solve business problems.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.
We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental."
26,"Director, Business Analytics","Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Code: 4806
Grade: L

Director, Business Analytics

Improvement, Analytics, and Innovation Services
We are a newly formed department within Stanford tasked with providing management and operational consulting services to a highly complex university administration with hundreds of business processes and dozens of major systems. Our team of internal consultants strives to create, instill, and sustain a business improvement discipline that delivers transformed and efficient processes through a data-driven approach.

The Business Analytics team is instrumental in executing our vision by providing essential data services both internally within our department as well as to our various clients. This team is composed of data professionals that cover a broad spectrum of skills such as data visualization, data engineering, statistics, machine learning, and optimization. We are in search of a seasoned director who will lead a current team of three (data engineer, senior data scientist, and business intelligence analyst) on a multitude of projects as well as identify new opportunities with a diverse internal clientele that span across university functions from administration to research. In additional to the technical data skills you bring, we also value your ability to manage and develop a successful and productive team.

The team will support projects across Stanford that range from identifying the impact of collaborative research, building an automated way to classify university expenditures, to identifying opportunities to improve the student services staff onboarding experience.

Take a closer look at what we’re up to https://www.youtube.com/user/StanfordUniversity and dive deeper into our vision: https://ourvision.stanford.edu/

Your responsibilities include:
Manage, coach, and lead a team of data professionals with varying skill levels and backgrounds
Build lasting relationships with our clients by providing data-driven solutions to their most impactful business problems
Make strategic decisions on our project portfolio, establish practical project timelines, and allocate resources in order to deliver solutions to our clients
Create an environment for your team to succeed and remove any barriers that are detrimental to productivity
Manage the performance and career development of the team
Market our services to Stanford by speaking at various outlets and community meet-ups
Implement guidelines, policies, and standards for the team to follow
Perform business analysis and capture requirements from clients regarding their various data needs
As necessary, get hands-on with various aspects of work done by the team such as writing scripts for data analysis or building a data visualization or interactive dashboard

Knowledge, skills, and abilities you bring:
Broad knowledge in the data analytics space from data pipeline development and business intelligences to statistics and machine learning.
Demonstrates rigorous attention to detail when conducting analysis on a business problem as well as when delivering a solution
Impeccable ability to find the most appropriate communication style depending on the audience, e.g. technical level with developers, strategic level with business managers
Fearlessness to dive into the gritty technical details of a particular solution
Keen ability to translate complex concepts around data into layman terms
Solid understanding of scripting languages such as Python, R including relevant packages used in data analysis
Solid understanding of systems architecture and various forms of database technologies such as Oracle, PostgreSQL, MongoDB, etc.
Hands-on experience with business intelligence tools such as Tableau and Oracle BI
Ability to design reporting data models that are precisely aligned with the business questions being asked
An eye for good data visualization design
Understanding of UI/UX as it relates to building interactive dashboards
Experience using git to track source code
Experience working closely with data scientists and report developers
Experience eliciting, interpreting, and documenting user requirements in the context of data analysis
Ability to lead multiple activities in a diverse environment

Working at Stanford:
Imagine a world without search engines or social platforms. Consider lives saved through first-ever organ transplants and research to cure illnesses. Stanford University has revolutionized the way we live and enrich the world. Supporting this mission is our diverse and dedicated 17,000 staff. We seek talent driven to impact the future of our legacy. Our culture and unique perks empower you with:
Freedom to grow. We offer career development programs, tuition reimbursement, audit a course. Join a TedTalk, film screening, or listen to a renowned author or global leader speak.
A caring culture. We provide superb retirement plans, generous time-off, and family care resources. http://stanfordcareers.stanford.edu/pay-and-rewards/a-competitive-edge
Legacy and Innovation. Our new state-of-the-art campus in Redwood City is a vibrant workplace with many great amenities to take in across our 35–acre site. Attend a meeting in our new multi–faceted conference rooms. Or get in a great workout at our impressive 28,000 square–foot Recreation & Wellness Center complete with a six–lane rooftop pool and indoor basketball court.
Enviable resources. Enjoy free commuter programs, ridesharing incentives, discounts, and more!

Consistent with its obligations under the law, the University will provide reasonable accommodation to any employee with a disability who requires accommodation to perform the essential functions of the job."
27,"Data Engineer, Apple Pay Security","Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: May 28, 2019
Role Number: 200063186
Imagine what you could do here. At Apple, new ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Do you have a passion for Security technologies? Do you love significant challenges? Join the team that provides software security technologies to help users protect their accounts. Our vision is to transform your smartphone into a device that secures your digital life without sacrificing your privacy. We are seeking a talented engineer who will develop libraries and platforms to empower data scientists to rapidly build and deploy robust models in production. You will also craft robust and scalable software. The role requires handling multiple assignments, communicating across functional areas, and driving projects to completion. We need you to show initiative and be able to take ownership of a problem area to find workable solutions.
Key Qualifications
Strong software design and development skills
Excellent analytical, programming and debugging skills
Experience with Scala or Java
Experience building scalable, distributed data-intensive applications with modern data tools. Knowledge of Hive, Spark, Cassandra, Kafka is a plus
Experience building data pipelines is a plus
Highly motivated and organized, with the ability to accept ambiguity and deliver exceptional results on tight schedules
Knowledge of SQL and/or NoSQL databases is a plus
Experience with security and cryptography is a plus
Description
We are looking for an expert data engineer who will work on our data science libraries and platforms to support new products. You will work hand in hand with data scientists, security engineers, program managers and business partners to identify problems, define scalable solutions, execute plans and results on a regular basis.
Education & Experience
BS or MS in Math, Computer Science or equivalent industry experience"
28,"Data Engineer (Outward, Inc.)","San Jose, CA 95112",San Jose,CA,95112,None Found,"
Bachelor's degree or equivalent work experience
2+ years of Python development experience
2+ years of SQL (Hive, Oracle, MySQL, PostgreSQL) experience
Professional experience using XML
",None Found,"
Work with Engineers, Product Owners, and Designers to understand their data needs
Automate frequently requested analyses using Python
Evaluate and define critical business metrics and identify new levers to help move these metrics
Design and evaluate A/B experiments
Monitor key product metrics and identify root causes behind anomalies
Build and analyze dashboards and reports
Influence product teams through a presentation of data-based recommendations
Communicate state of business and experiment results to product teams",None Found,None Found,"Location: San Jose, CA

About Outward, Inc.

Outward, Inc. is based in San Jose, CA and is a wholly owned subsidiary of Williams Sonoma, Inc. ( www.outwardinc.com )

At Outward Inc. our vision is to 'lower the friction' with regards to all aspects of the customer journey for our parent company and our retail customers. We do this by offering new technology solutions that enable new experiences and top-notch visualizations of their products. We are continuously pushing the boundaries of how 3D and AR/ VR technologies will drive the next generation shopping experience.

Through our portfolio of premium lifestyle brands - our mission is to deepen consumer connections with the products that matter and deliver an innovative experience.

We are positioned as a technology leader in the visual merchandising space for retail, with a focus on improving customer experiences with next-generation product visualizations.

Come and join a growing team of engineers as we solve technological riddles and push the envelope of what can be done on the web!


Responsibilities

Work with Engineers, Product Owners, and Designers to understand their data needs
Automate frequently requested analyses using Python
Evaluate and define critical business metrics and identify new levers to help move these metrics
Design and evaluate A/B experiments
Monitor key product metrics and identify root causes behind anomalies
Build and analyze dashboards and reports
Influence product teams through a presentation of data-based recommendations
Communicate state of business and experiment results to product teams
Minimum Qualifications

Bachelor's degree or equivalent work experience
2+ years of Python development experience
2+ years of SQL (Hive, Oracle, MySQL, PostgreSQL) experience
Professional experience using XML


Nice-to-have Qualifications

2+ years of experience with data visualization and data-mining
Experience analyzing data to identify deliverables, gaps and inconsistencies
Experience initiating and driving projects to completion with minimal guidance
Experience communicating the results of analyses
Experience with AWS services like lambda, Cloud Formation, RDS, EC2, IAM
Outward's Benefits & Perks:

Medical, Dental, Vision
401K
Paid time off
Company-sponsored team events such as regular staff parties
Fantastic new headquarters
Fully stocked kitchens with catered lunches twice a week
On-site gym & game room
Office dogs! Bring your furry pal with you to work
Flexible working hours
Friendly, caring co-workers and management


This position will not offer relocation assistance or remote work.
Outward, Inc. is an Equal Opportunity Employer.
Outward, Inc. will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the California Fair Employment Act (AB 1008), or other applicable state or local laws and ordinances.

#LI-JQ1 | rev 8.9.19

-"
29,Senior Splunk Architect,"Pleasanton, CA 94588",Pleasanton,CA,94588,None Found,None Found,None Found,"Provide guidance on how to best deploy and maintain our Splunk environment
Design and support the automated deployment of security infrastructure
Install, build and maintain Splunk and related Apps and add-ons
Lead cross-team efforts with other internal organizations
Support On-call duties for a variety of security tools
Lead key deliverables and objectives for the Security Data Engineering team
","Provide guidance on how to best deploy and maintain our Splunk environment
Design and support the automated deployment of security infrastructure
Install, build and maintain Splunk and related Apps and add-ons
Lead cross-team efforts with other internal organizations
Support On-call duties for a variety of security tools
Lead key deliverables and objectives for the Security Data Engineering team
",None Found,"Join our team and experience Workday!
It's fun to work in a company where people truly believe in what they're doing. At Workday, we're committed to bringing passion and customer focus to the business of enterprise applications. We work hard, and we're serious about what we do. But we like to have a good time, too. In fact, we run our company with that principle in mind every day: One of our core values is fun.
Job Description
Join our team and experience Workday!
It's fun to work in a company where people truly believe in what they're doing. At Workday, we're committed to bringing passion and customer focus to the business of enterprise applications. We work hard, and we're serious about what we do. But we like to have a good time, too. In fact, we run our company with that principle in mind every day: One of our core values is fun.
Job Description
Are you looking to join one of the hottest cloud companies in the world? Do you enjoy administering enterprise grade, large-scale, security solutions in a rapidly-growing global infrastructure?
Join Workday's Security Engineering team as a Sr. Security Data Engineer. This position is responsible for the administration of security tools with a focus on Splunk. You will be responsible for working closely with the Security Operations, Network, Security, and IT teams in architecting and administering security solutions. This is a technical role with the expectation that you are fluent in Linux system administration and Splunk configurations.
The team that you would be on designs, deploys and manages all internal and external customer facing security services. You'll be an inventive engineer, with a taste for challenging problems that lesser engineers shy away from. You'll revel in deploying and administering elegant solutions using whatever languages, tools and hardware deemed most appropriate.
Your training and development budget will see you mandated to attend at least one major off-site training course annually (SANS, Splunk Educations, etc) and at least one major security conference (Blackhat, Defcon, .conf, RSA, CanSecWest, etc) as well as having budget for local conferences and events. You'll be encouraged to keep your skills up to date with other events such as internal red/blue team events, hackathons, membership of various groups and societies. You'll be provided a budget to grow a reference library for you and your team. You will have a lab and development pipeline to run proof of concept projects in. We also reserve one afternoon a week for side projects.
Responsibilities
Provide guidance on how to best deploy and maintain our Splunk environment
Design and support the automated deployment of security infrastructure
Install, build and maintain Splunk and related Apps and add-ons
Lead cross-team efforts with other internal organizations
Support On-call duties for a variety of security tools
Lead key deliverables and objectives for the Security Data Engineering team
Requirements
Minimum of B.S. Degree in STEM field required.
6+ Years of experience in IT/Security
Expert in Splunk Architecture (ideally certified)
Proficiency in Linux administration (ideally RHEL/CentOS)
Proficient skills in at least 3 of the following:
Version Control (ideally Git)
Experience with Automated deployment (ideally Chef, Ansible)
Experience with Continuous integration tools (ideally Jenkins, Bamboo)
Experience with Python scripting
Experience open-sourcing and supporting home-grown tools
Experience in Security Operations
Host Based IDS"
30,Big Data Engineer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Adobe IDS Data Engineering team is looking for innovative data and software engineers to build and evolve our industry leading data products and applications that empower Adobe’s Data Driven Operating Model and next generation customer experiences.
What you’ll do
Design, develop & tune data products, applications and integrations on large scale data platforms (Hadoop, Kafka Streaming, Spark, Hana, SQL server etc.) with an emphasis on performance, reliability and scalability and most of all quality.
Analyze the business needs, profile large data sets and build custom data models and applications to drive the Adobe business decision making and customers experience
Develop and extend design patterns, processes, standards, frameworks and reusable components for various data engineering functions/areas.
Collaborate with key stakeholders including business team, engineering leads, architects, BSA's & program managers.
Working at Adobe you have the opportunity to extend your network and collaborate with engineers, architects and leaders across the Adobe data management space, Adobe product engineering and Business leadership teams. We are looking to win our next CIO 100 award and we need you!
The ideal candidate will have:
BS or MS in Computer Science / related technical field with 3 – 8 years of strong hands-on experience in enterprise data warehousing / big data implementations & complex data solutions and frameworks
Experience designing, developing, and tuning data products on large scale data platforms (Hadoop, Kafka Streaming, Spark, SQL server)
Strong SQL, ETL, scripting and or programming skills with a preference towards Python, Java, Scala, shell scripting
Demonstrated ability to clearly form and communicate ideas to both technical and non-technical audiences.
Strong problem-solving skills with an ability to isolate, deconstruct and resolve complex data / engineering challenges
Results driven with attention to detail, strong sense of ownership, and a commitment to up-leveling the broader IDS engineering team through mentoring, innovation and thought leadership
Nice to have :
Familiarity with streaming applications
Experience in development methodologies like Agile / Scrum
Knowledge and experience with NoSQL technologies
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status."
31,Big Data Engineer,"Fremont, CA",Fremont,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer
6 Months+
Santa Clara CA

5+ years of experience with Hadoop or Cloud TechnologiesExpert level building pipelines using Apache Beam or Spark using Dataflow or DataProcExperience in building ETL using data from Big Query. Should have prior projects experience on GCP projects.Expert in reading and writing Scala, Python or Java code.Familiarity with GCP services, preferably having supported deployments on one or more of GCP servicesExperience in Managing code in GithubExperience with all aspects of DevOps (source control, continuous integration, deployments, etc.)Advanced knowledge of the Hadoop ecosystem and Big Data technologiesHands-on experience with the Hadoop ecosystem (HDFS, Hive, Impala, Spark, Kafka, Kudu, Solr)Knowledge of agile(scrum) development methodologyStrong development/automation skillsSystem level understanding - Data structures, algorithms, distributed storage & computeCan-do attitude on solving complex business problems, good interpersonal and teamwork skills


Primary Location: US-CA-Fremont
Schedule: Full Time
Job Type: Experienced
Travel: No
Job Posting: 05/06/2019, 3:54:04 PM"
32,Lead Data Engineer - Demand Forecasting Engine,"Sunnyvale, CA 94086",Sunnyvale,CA,94086,None Found,None Found,None Found,None Found,None Found,None Found,"Description:
The AI Engineering team at Target is a fast growing, dynamic team. AI Engineers work closely with Scientists to create valuable retail insights through state-of-the-art mathematical models powered by cutting edge AI technologies. Business teams, empowered with these insights, provide joy of everyday shopping to each Target guest.
As a Lead AI Engineer working on Demand Forecasting, you will be expected to create software solutions that forecast item demand. You will create solutions using Agile practices and DevOps principles. Responsibilities will include designing, programming, testing, debugging and supporting high quality, distributed, large scale software solutions.
We are looking for a highly-motivated engineering professional who is a team player and can engage with both technical and business team members.
Key Responsibilities
Develop software systems using test driven development employing continuous integration practices
Partner with other engineers and team members to develop software that meets business needs
Follow agile methodology for software development and technical documentation
Innovate constantly and stay current with latest technologies while staying focused on solving problems at hand.
Requirements
4+ years of experience in developing code for production or equivalent experience
Proficient in one or more of C++, Java,Scala, Pythonor equivalent programming language
Deployed computersystems into production
Significant understanding of advanced data structures and algorithms
Familiarity with linear algebra, statistics, information theory or related mathematical domains
Understand deep learning, dynamicprogramming, reinforcementlearning or related areas
Ability to develop and execute processes for complete CI/CD pipeline
Excellent written and verbal communication skills
Preferred Requirements
1+ years’ experience working on Bigdata technologies like Hadoop, Spark and Hive.
Familiar with statistical models like GAMM.
BS degree in Computer Science or related field.
Understand application/software development and design.
Experience developing software for automating the development, deployment and operation of complex distributed systems
Experience with one of more standard automation toolkits (e.g. ansible, chef, puppet, salt)
Experience with one or more cloud deployment automation tools (e.g. AWS, kubernetes, OpenStack) is a plus
Worked on building and supporting Web Services. Experience with REST services preferred.
Experience/knowledge of streaming solutions is a plus.
Why work with us at Target?
We are using great tools. Not only are we using programming languages like Haskell, Rust, Scala, and Python, but we are also using modern tooling and infrastructure like Nix, Kubernetes, and Kafka.
We love open source! Many of our team member's contribute to open source communities and get to do it during work time. We try to contribute back to our communities where we can and are grateful to be able to open source some of our own projects!
We value diversity. We believe that diversity and inclusion is of core importance when try to create positive in-store experiences for our guests, and we think it is also critically important when building our teams. Read more about our commitment to diversity and inclusion
We value our team members. We treat out team members like people, not like cogs in a machine. We value our team members for who they are, not just what they can get done. We are parents, hobbyists, enthusiasts, family members, and community members, and can offer flexibility to our team members' schedules and work arrangements so that they can flourish both inside and outside of work.
Americans with Disabilities Act (ADA)
Target will provide reasonable accommodations (such as a qualified sign language interpreter or other personal assistance) with the application process upon your request as required to comply with applicable laws. If you have a disability and require assistance in this application process, please visit your nearest Target store or Distribution Center or reach out to Guest Services at 1-800-440-0680 for additional information.
Qualifications:"
33,Data Engineer – Logs,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,"
Degree in Computer Science, Engineering, or related fields
Strong in data structures, algorithms, and systems
Deep understanding of systems behavior and affinity for logs
Experience with building infrastructure to collect, prepare and analyze logs, telemetry, and monitoring data
Experience with building and using tools like Dtrace, BTrace, etc.
Experience with log analytics tools like Splunk, ELK, etc.
Knowledge of time series models and the use of systems like OpenTSDB, Graphite, etc.
Experience with handling complex production and/or support escalation issues
Heuristic problem solving with incomplete information
Deep domain knowledge and hands-on experience across a very broad spectrum of backend, frontend, cloud, AI and data infrastructure platforms,
",None Found,"
Degree in Computer Science, Engineering, or related fields
Strong in data structures, algorithms, and systems
Deep understanding of systems behavior and affinity for logs
Experience with building infrastructure to collect, prepare and analyze logs, telemetry, and monitoring data
Experience with building and using tools like Dtrace, BTrace, etc.
Experience with log analytics tools like Splunk, ELK, etc.
Knowledge of time series models and the use of systems like OpenTSDB, Graphite, etc.
Experience with handling complex production and/or support escalation issues
Heuristic problem solving with incomplete information
Deep domain knowledge and hands-on experience across a very broad spectrum of backend, frontend, cloud, AI and data infrastructure platforms,
",None Found,None Found,"About Peritus
Peritus enables self-healing autonomous datacenters with automated, cognitive support for infrastructure software and hardware. It is a funded startup co-created at The Hive in Palo Alto, CA that delivers artificial intelligence-based virtual support expert systems for data center service fulfillment and incident resolution.

As datacenter vendors move from on-premise to the cloud their existing support system lacks the agility and cost-effectiveness for the cloud. Peritus significantly enhances operational efficiencies of existing support services and enables managed service providers & system vendors to offer new business continuity entitlements. Peritus assists & automates a wide spectrum of decisions in system support including incident classification, routing, contract coverage, incident resolution recipes and orchestration of incident management between subject matter experts (SMEs).

Peritus’ unique vectorization of system log data drives predictive modeling with highly granular feature extraction for early detection of system events. The platform’s advanced natural language processing (NLP) capabilities drive Peritus’ incident modeling and predictive capabilities. The core service fulfillment engine uses a combination of supervised and unsupervised methods to predict incident features from system log data. Peritus delivers automated orchestration of incident resolution through its close integration with existing incident management platforms.

Job Description
We are building a product that helps customers fulfill service requests as well as troubleshoot and diagnose infrastructure issues that cut across domains. The product needs to collect, collate and analyze logs, telemetry and monitoring data emanating from multi-vendor and multi-datacenter/cloud systems. The analysis should help with timestamp-based correlation of events across systems, extraction of inferences derived from performance counters from multiple systems and trace the sequence of events across layers of the data center stack.

Responsibilities
We are looking to hire an engineering technical leader with deep systems knowledge. The role entails a deeper understanding of how data center systems operate requiring familiarity with computing, storage, networking and virtualization products. Think DTrace but with a scope that spans across systems and not just within a system. The logs infrastructure needs to scale horizontally and enable machine learning models to learn systems behavior. Over time, the learning translates to automatic application of resolutions and thereby reducing human involvement.

Can you help fill in the blanks for users handling complex systems issues that affect performance and/or bring systems down?

Qualifications & Expertise
The successful engineer would have a proven track record of building complex log analysis platforms:

Degree in Computer Science, Engineering, or related fields
Strong in data structures, algorithms, and systems
Deep understanding of systems behavior and affinity for logs
Experience with building infrastructure to collect, prepare and analyze logs, telemetry, and monitoring data
Experience with building and using tools like Dtrace, BTrace, etc.
Experience with log analytics tools like Splunk, ELK, etc.
Knowledge of time series models and the use of systems like OpenTSDB, Graphite, etc.
Experience with handling complex production and/or support escalation issues
Heuristic problem solving with incomplete information
Deep domain knowledge and hands-on experience across a very broad spectrum of backend, frontend, cloud, AI and data infrastructure platforms,
Please send your resumes to jobs@peritus.ai"
34,Sr Analyst/ Manager - Integrated Marketing Analytics,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Department: Marketing & Customer Insights (MCI)
Title: Senior Analyst / Manager - Integrated Marketing Analytics
The Challenge
The Marketing & Customer Insights (MCI) organization has a dual mandate of providing objective customer research, analysis, and marketing effectiveness measurement while advancing the use of Adobe Marketing Cloud technologies to enable and track customer experiences across surfaces.
Within MCI, the Advanced Analytics team was established to focus specifically on developing deeper customer insights and deliver analytical tools to support integrated marketing planning across channels. The group closely partners with Global and Field Marketing, IT, business unit leaders and other corporate functions to enhance understanding of our customers and their digital journey.
In this role, the team maintains a highly visible and strategically important role in delivering and facilitating understanding of insights to inform business strategies, and to track the performance of marketing specific activities against expectations.
The ideal candidate for this position is part analyst and part data engineer. We are looking for someone who is obsessed with the manipulation of very large data through tools and techniques to advance our analytics and execution capabilities. This includes developing our next generation of data and analytics tech stacks to guide our customer and marketing insights work. The work will also involve looking for more effective methods of executing the analytics stacks quicker/faster/bigger to get more value from them across the organization.
The role will also play a key role in the integration of our techniques and capabilities into the product roadmaps for our Adobe Marketing Cloud solutions.
A balance of technical and analytics skills as well as a working knowledge of marketing processes is key. A team-player, collaborative, mindset is essential. In addition to development of customer journey predictive models, this role will help develop visualization approaches to drive better understanding across key Adobe teams.
What you’ll do
Structure problems into a data / analytics plan and execute.
Synthesize complex data sets into actionable insights and build analytics stories that culminate in accurate business decisions
Build visualizations
Hands-on creation of the next gen analytics tools and tech stack.
Ability to partner with a cross-functional team of technical and analytical partners
Work on cutting edge data science and analytics problems to help solve real world challenges
Possible engagement with Adobe Marketing Cloud teams to evolve capabilities within the products.
Provide ad hoc analysis and work with data scientists as needed;
Communicate results and insights with team and executives
Obsess with leveraging technology to make partner analytics teams more efficient and effective.
Employ effective visualization techniques for complex concepts visualizing “big data” use cases.
What you need to succeed
6-8 years demonstrated experience across the analytics spectrum of Prescriptive, Descriptive and Predictive Analytics
Ability to synthesize large scale data sets to generate insights a must
Intellectual curiosity, flexibility, and high attention to detail
Undergraduate degree in a qualitative field preferred
Expertise and Experience using analytics tools like SQL, R or Python a must
Expertise and Experience with building large scale visualization tools like Tableau / Spotfire / D3 a must
Data Engineering Expertise and Experience building pipelines is a plus
Understanding of marketing processes
Experience in machine learning tools and techniques a plus
Experience developing and working on larger scale analytics / big data implementations
Track record of consistent delivery and execution
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status."
35,SW Big Data Systems Engineer,"Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Aug 28, 2019
Weekly Hours: 40
Role Number: 200038890
The SWE Data Analytics team at Apple collects, processes, and analyzes diagnostics and usage data from Apple devices across the world. Our data is used to generate insights that informs and drives product strategies across all of software and hardware development. We develop batch and streaming analytics solutions using Kafka, Flume, Hadoop, Spark, Jenkins, and other state of the art technologies in a large scale infrastructure.
We are looking for a passionate and results-oriented senior big data engineer to join our team and work on some of the highly visible data projects in software engineering organization. You will be collaborating with data analysts, device engineers and engineering teams. You will drive the development of data pipelines and services with high degree of ownership.
Key Qualifications
Experience developing large scale distributed computing systems
In-depth knowledge and experience in one or more of the following technologies: Hadoop ecosystem, Kafka, Samza, Flume, HBase, Cassandra, Redshift, Vertica, Spark.
Deep understanding of key algorithms and tools for developing high efficiency data processing systems
Validated software engineering experience and discipline in design, test, source code management and CI/CD practices
Experience in data modeling and developing SQL database solutions
Proficient in working with Linux or other Posix operating systems, shell scripting, and networking technologies
Strong software development, problem-solving and debugging skills with experience in one or more of the following languages: Java, Python, Scala, or Ruby
Ambitious, passionate about software development, especially in data technologies, you love working in a fast-paced and dynamic environment
You are deeply organized, detail oriented, and thorough in every undertaking. You are able to multi-task and change focus quickly
Excellent interpersonal skills.
Description
As part of a small team of highly skilled data engineers, you will own significant responsibility in crafting, developing and maintaining our large-scale ETL pipelines, storage, and processing services.
You will build self-service analytics tools to help engineering teams derive actionable metrics out of large volumes of raw data.
You will partner with data science and engineering teams and develop algorithms to answer complex questions on usage of our products
You will work closely with the DevOps team and develop monitoring and alerting scripts on various data pipelines and jobs
You will have the opportunity to learn and work on the latest Big Data technologies, lead POCs to demonstrate new ideas and influence the future direction of our technology stack
Education & Experience
Bachelors in Computer Science or equivalent experience.
Additional Requirements
Experience using data storage technologies such as Apache Parquet or Avro
Experience in machine learning algorithms is a plus
Testing tools and methodologies to test large scale distributed computing systems"
36,"Data Engineer, Hardware Consumer Care","Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,"
Build, own and maintain end to end consumer hardware support operations backend analytics infrastructure.
Provide thought leadership around designing the backend data architecture that scales well with the growing needs of the business.
Work closely with Product Management, Software Engineering and other cross-functional teams to further improve existing customer relationship management tools for all key channels of customer support.
Build new reporting tools and technologies to help measure quality and efficiency of customer care operations.
",None Found,None Found,"Minimum qualifications:

Bachelor's degree in Data Science, CS, Engineering, Mathematics/Statistics, Operations Research, or equivalent practical experience.
9 years work experience. Experience designing and developing data models and data warehouses.
Experience in one or more programming languages (e.g., Python, Java, etc.).
Experience managing projects, troubleshooting technical issues and working with Engineering/Sales/Services teams and customers.

Preferred qualifications:

Master's degree or PhD in a technical or scientific field of study.
Experience with Unix or GNU/Linux systems.
Experience developing reporting portals for users with various security access levels.
Experience in operational analytics and new evolving machine learning space.
Expertise in data management and writing/maintaining ETL for structured and unstructured data sources.
Ability to communicate complex findings in a structured and clear manner to a non-technical audience.
About the job
We are a part of Hardware Customer Care team that engages with dedicated customers across the globe through our online resources and communities, social outreach and 1:1 care. We represent the voice of the consumer and work closely with cross-functional partners across Google to make our products and policies better.
As a data analytics and insights group within the Customer Support organization, we are responsible for providing strategic insights, analysis and reporting to various stakeholders in support organization. As a Data Engineer, you’ll be responsible for building and maintaining data analytics infrastructure. This role requires deep technical expertise, shaping discussions on relevant metrics and data signals to define an excellent user support experience.

Google's mission is to organize the world's information and make it universally accessible and useful. Our Devices & Services team combines the best of Google AI, Software, and Hardware to create radically helpful experiences for users. We research, design, and develop new technologies and hardware to make our user's interaction with computing faster, seamless, and more powerful. Whether finding new ways to capture and sense the world around us, advancing form factors, or improving interaction methods, the Devices & Services team is making people's lives better through technology.
Responsibilities
Build, own and maintain end to end consumer hardware support operations backend analytics infrastructure.
Provide thought leadership around designing the backend data architecture that scales well with the growing needs of the business.
Work closely with Product Management, Software Engineering and other cross-functional teams to further improve existing customer relationship management tools for all key channels of customer support.
Build new reporting tools and technologies to help measure quality and efficiency of customer care operations.
At Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form."
37,Senior Data Engineer,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Us:
---------

We are The Selling App. The fast and easy way to sell or buy almost anything, from fashion to toys, sporting goods to electronics, jewelry to shoes. Launched in 2013, Mercari quickly became the #1 shopping app in Japan. Now we're on a global mission to build a future where people everywhere feel empowered to sell the things they don't use, a future where all useful things are used. And with a fast-growing user base in the U.S. of over 45 million downloads, we are on our way to doing just that.

The ideal candidate is eager to take responsibilities on data products and machine learning platform spontaneously and has a passion to make our systems reliable and sustainable with the modern technologies. The candidate will take responsibility for not only data engineering, but also machine learning platform. We expect the candidate is able to design and implement data pipelines at scale and is deeply familiar with container technologies and related topics to offer intelligent system as microservices.

What You'll Be Doing:
---------------------


Design, build and operate ETL pipeline at scale.
Automation of processes related to data products and machine learning products.
Support stakeholders with designing data structure for data products.
Development and operation of API/tools related to data products and machine learning products.

What You'll Need:
-----------------


Industry experience building and productionizing data pipelines.
Solid understanding / experience in the machine learning space.
Industry experience building and productionizing system to serve machine learning.
Ability and desire to take full ownership of projects, driving them forward to completion.

Nice-to-haves:
--------------


Experience with Apache Spark, Apache Beam or related distributed processing frameworks.
Experience with Google Cloud Platform or related cloud services.
Experience with Apache Airflow or related workflow scheduling products.
Experience with developing microservices with docker and kubernetes to serve machine learning models.

Technologies We Use:
--------------------


ETL: Apache Airflow
Container: Docker/Kubernetes
API: gRPC/Tensorflow Serving/Flask(REST)
Database: Google Datastore/MySQL/Google Spanner
Distributed Processing: Apache Beam/Apache Spark, Hadoop
Cloud: Google Cloud(BigQuery/ML Engine/Google Dataflow/Google Dataproc, etc.)

Why Mercari?
------------

Mercari nurtures an all for one environment where teamwork and innovative thinking is the priority.

Perks:

Competitive ‌medical, dental, and vision insurance options
401k match
Life & disability insurance
Employee Assistant Program
New parent paid leave
Rocket Lawyer legal services
Fond perks and rewards
Commuter reimbursement
Time when you need it - unlimited vacation days
Catered lunches everyday
Team outings and events

"
38,Machine Learning Lead,"Sunnyvale, CA 94087",Sunnyvale,CA,94087,None Found,"Masters or equivalent degree in a computational science or engineering with 4+ years of experience in machine learning/data engineer role;Familiarity with distributed computing frameworks (e.g., Hadoop/Spark) and relational data base (e.g., Oracle, MySQL), and knowledge of NoSQL database;Strong implementation experience with a programming language (e.g., Jave/C++/Scala) and a scripting language (e.g., Python/Perl/Ruby), and familiarity with Linux/Unix/Shell environments;Strong hands-on skills in building scalable and reliable data pipelines for sourcing, cleaning and manipulating large volumes of data;Basic understanding of machine learning/statistics.",None Found,None Found,None Found,None Found,"Position Description
We are looking for outstanding machine learning/data engineers with skills in distributed computing system and experience working with very large scale of data, who will work closely with machine learning/data scientists in the team, and contribute to Advertising Technology in driving the future of ad targeting, personalization, relevance, ranking, and campaign optimization.

Join us if you want to be spending your time on:
Building, maintaining and monitoring scalable data pipelines to support modeling and optimization products;Analyzing, identifying, and debugging data related issues to ensure quality and stability of models and applications;Working with machine learning/data scientists to build prototypes and perform experiments;Working with engineering team to productize new features and products;Performing ad hoc data related analysis.
Minimum Qualifications
Masters or equivalent degree in a computational science or engineering with 4+ years of experience in machine learning/data engineer role;Familiarity with distributed computing frameworks (e.g., Hadoop/Spark) and relational data base (e.g., Oracle, MySQL), and knowledge of NoSQL database;Strong implementation experience with a programming language (e.g., Jave/C++/Scala) and a scripting language (e.g., Python/Perl/Ruby), and familiarity with Linux/Unix/Shell environments;Strong hands-on skills in building scalable and reliable data pipelines for sourcing, cleaning and manipulating large volumes of data;Basic understanding of machine learning/statistics.
Additional Preferred Qualifications
Experience building and maintaining large scale data pipelines in online advertising, recommender system, ecommerce or relevant areas;Experience building and/or maintaining machine learning models and pipelines;Familiarity with job scheduler (e.g., Jenkins/Azkaban/Airflow);Experience with Elastic Search/Solr.
Company Summary
The Walmart eCommerce team is rapidly innovating to evolve and define the future state of shopping. As the world’s largest retailer, we are on a mission to help people save money and live better. With the help of some of the brightest minds in technology, merchandising, marketing, supply chain, talent and more, we are reimagining the intersection of digital and physical shopping to help achieve that mission.
Position Summary
This position is in the data science team under the Advertising Technology organization. The mission of the Advertising Technology organization is to advance Walmart eCommerce by driving higher value for our customers and vendor partners. Walmart is investing in building a world class advertising platform and the Ads team is responsible for defining and performance advertising products that drive discovery, sales and profits. The team operates an end to end advertising platform that includes a scalable ad service that serves hundreds of millions of impressions each day, sophisticated ad matching algorithms, real-time reports, self-service interface for end to end program management etc.

We are a highly motivated group of Big Data Geeks, Data Scientists and Applications Engineers, working in small agile group to solve sophisticated and high impact problems. We are building smart data systems that ingest, model and analyze massive flow of data from online and offline user activity. We use cutting edge machine learning, data mining and optimization algorithms on ad relevance, ranking and campaign optimization.

Join us if you want to be spending your time on:
Gathering and analyzing data, identifying modeling and optimization problems, devising solutions and building prototypes;Formulating machine learning/statistical approaches while paying attention to business metrics, designing features from the rich data available from many sources, training, evaluating, and deploying models;Researching and implementing methodologies to measure the impact of the technologies;Initiating and proposing unique and promising modeling projects, developing new and innovative algorithms and technologies, pursuing patents where appropriate;Developing high-performance algorithms for precision targeting, user engagement prediction, and ad relevance/ranking; testing and implementing these algorithms in scalable, product-ready code; interacting with other teams to define interfaces and understanding and resolving dependencies;Staying current on published data mining, machine learning and modeling techniques and competing technologies and sharing these findings with scientists and engineers in the organization;Maintaining world-class academic credentials through publications, presentations, external collaborations and service to the research community."
39,Data Engineer,"Sunnyvale, CA 94085",Sunnyvale,CA,94085,None Found,None Found,None Found,None Found,None Found,None Found,"If you thrive on working with big data in high performance teams then this is the place for you. You would work on data and build some of the tools that are critical to moving & transforming this data into valuable and insightful information. Creating reliable, scalable, and high performance products requires exceptional technical expertise and practical experience working with large-scale distributed systems. Finally, you will tackle challenging issues of scale, reliability and security while delivering a delightful, simple user experience to a global user base.
RESPONSIBILITIES
You will manage data warehouse plans for a product or a group of products. You will interface with engineers, product managers and product analysts to understand data needs. In addition, you will design, build and launch new data extraction, transformation and loading processes in production. You will work with data infrastructure to triage infra issues and drive to resolution. Be prepared to build and launch new data models that provide intuitive analytics to your customers as well as design and extremely efficient & reliable data pipelines to move data to our Data Warehouse. You will use your expert coding skills across a number of languages from Python, Scala, Java and PHP and work across multiple teams in high visibility roles.
REQUIREMENTS
2+ years of Scala and/or Python development experience is necessary
2+ years of SQL (Oracle, Vertica, Hive, etc) experience is required
2+ years of experience in custom or structured (ie. Informatica/Talend/Pentaho) ETL design, implementation and maintenance
2+ years or experience applying statistical data analysis to real-life problems
Experience working with either a Map Reduce or a MPP system on any size/scale
BS or MS degree in Computer Science or a related technical field
Previous experience with Data ingestion and IR (information retrieval) is highly desirable
Industry experience as a Data Engineer or related specialty
6DOeUlsRZn"
40,Data Engineer- Data Platform & Architecture,"Mountain View, CA 94043",Mountain View,CA,94043,None Found,"
4+ years of experience in Big Data & Data Warehousing technologies
Strong cloud and non-cloud data management experience
Strong experience with big data tool sets in UNIX environment
Good knowledge in Snowflake or MPP, Azure, GCP and AWS preferred
Strong experience developing in programming languages such as Java, Python or Scala
Experience with advertising, media data platforms in large scale preferred
Experience working with data science engineering, advanced data insights engineering preferred
Strong quality proponent and thrives to impress with his/her work
Strong problem solving skills and ability to navigate complicated database relationships
Good written and verbal communication skills
Demonstrated ability to work with product management and/or business users to understand their needs
Self-starter, able to work in a fast-paced, deadline driven environment with multiple priorities",None Found,"
Individual contributor
Design and implement frameworks for future needs in Hadoop, Snowflake, Data Bricks and Cloud
Managing the quality of big data platform(s) such that end users have trust and confidence in data
Forward looking, strategic thinker and executes with impeccable results
Development must support data completeness, data transformation and data quality
Work with diverse teams to make complex architecture decisions
Work with multiple teams to manage goals, objectives and measure the results
Learn, evaluate and adapt new tools/technologies to meet business and technical objectives",None Found,None Found,"Quotient Technology Inc. is currently seeking an Engineer for the Data Platform & Architecture Team to drive high quality data products and services to the market. This person will be responsible for design, develop and support products and services on top of our big data platform. This candidate demonstrates architecture, data and analytics quality as his/her passion, does not shy away from challenges. His/Her experience is filled with taking initiatives, patience, thoroughness, hands on working experience, willing to negotiate and work in a diverse, distributed data culture. Possesses strong technical skills and prior experience in working with highly talented engineers is expected.


Position Responsibilities
Individual contributor
Design and implement frameworks for future needs in Hadoop, Snowflake, Data Bricks and Cloud
Managing the quality of big data platform(s) such that end users have trust and confidence in data
Forward looking, strategic thinker and executes with impeccable results
Development must support data completeness, data transformation and data quality
Work with diverse teams to make complex architecture decisions
Work with multiple teams to manage goals, objectives and measure the results
Learn, evaluate and adapt new tools/technologies to meet business and technical objectives

Qualifications
4+ years of experience in Big Data & Data Warehousing technologies
Strong cloud and non-cloud data management experience
Strong experience with big data tool sets in UNIX environment
Good knowledge in Snowflake or MPP, Azure, GCP and AWS preferred
Strong experience developing in programming languages such as Java, Python or Scala
Experience with advertising, media data platforms in large scale preferred
Experience working with data science engineering, advanced data insights engineering preferred
Strong quality proponent and thrives to impress with his/her work
Strong problem solving skills and ability to navigate complicated database relationships
Good written and verbal communication skills
Demonstrated ability to work with product management and/or business users to understand their needs
Self-starter, able to work in a fast-paced, deadline driven environment with multiple priorities
About Quotient Technology:
Quotient Technology Inc. (NYSE: QUOT) is a leading digital promotions, media and analytics company that delivers personalized digital coupons and ads—informed by proprietary shopper and online engagement data—to millions of shoppers daily. Founded in 1998, Quotient is based in Mountain View, California, and has offices across the U.S., in Bangalore, India; Paris and London. Learn more at Quotient.com, and follow us on Twitter @Quotient"
41,Sr. Big Data Developer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"What You'll Do
As a Senior Data Engineer of the Portals and Data Analytics Engineering team, you will have a unique opportunity to use your creativity in cloud infrastructure to develop innovative big data analytics platform. We are seeking a talented, self-motivated, and highly creative senior software engineer to join our team to develop new big data platform project.
Leads team to develop big data platform, propose solutions and technical evolution, establish Cisco credibility platform for a Cisco Collaboration solution.
Works closely with global team on engagements by estimating the complexity and duration of technical tasks; and providing business value justification and risk evaluations.
Adopt new advanced data analytic technical skills and knowledge in the cloud architecture, particularly for collaboration based big data service
Who You'll Work With
The Cloud Collaboration Technology Group (CCTG) is a $1B business unit that develops primarily cloud based collaboration software solutions, and is an integral part of the $4.4B Collaboration portfolio at Cisco, which is investing heavily to transform the future of collaboration experiences for our end users. CCTG's vision is to be the recognized industry leader in the Cloud Collaboration market as well as to be the industry beacon that attracts top talent. Our strategy is to sustain innovation in our current $1B product line, and to develop innovative new products for the future that expand and positively disrupt our market. We will do all of this, and build a next generation cloud platform with APIs and an ecosystem that deliver tangible business outcomes enabling our customers to get more jobs done!
Who You Are
Bring a strong perspective that drives change and motivates engineers to develop simple solutions to complex problems. Have deep understanding of data analytics best practices, including skills such as data modeling, data cleaning, data mining, machine learning and data virtualization. Be an expert in below two or more areas
A BS/CS/EE Bachelors Degree in addition to a minimum of 8-10 years of distributed systems development experience with strong focus on the design and development of cloud and data analytic applications * Previous experience as hands-on technical lead * Data storage - experienced in implementing Big Data technologies successfully in an enterprise;
Knowledge discovery - advanced knowledge in entity and relationship extraction from unstructured data;
Data governance - experienced in developing and integrating software allowing for flexible and scalable data transformation with data quality controls.
Data analytics - strong backgrounds in statistics, machine learning and similar technologies.
Data visualization - knowledge of tools that are cost-effective and make it easy for end users to better understand and produce reports and graphs.
Hadoop Administration - Contribute to the evolving architecture to meet growth requirements for scaling, reliability, performance and security. * 4-8 years hands on experience with big data tools such as Hadoop, Cassandra, Kafka, Storm, Spark etc * Experienced Java/Scala engineer.
Flexible, self-motivated, problem solver who can work both individually as well as an effective team player with excellent attention to detail and great interpersonal skills (both verbal and written)
* Excellent communications skills * Be willing/able to work crossing boundary to reach best fit solution in real world cloud service
Cisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.
Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.
We Are Cisco
#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool."
42,Siri - Data Engineer,"Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Sep 20, 2019
Weekly Hours: 40
Role Number: 200073619
Play a part in the next revolution in human-computer interaction. Contribute to a product that is redefining mobile computing. Create groundbreaking technology for large-scale systems, spoken language, big data, and artificial intelligence. And work with the people who built the intelligent assistant that helps millions of people get things done — just by asking. Join the Siri team!
Key Qualifications
5+ years of industry experience
Strong data analytical skills
Excellent coding skills in one or more of following languages — Python, Shell scripts, java etc
Experienced in big data processing, tools development and/or web crawling
Outstanding communication skills
Gradings tasks and tools preferred: e.g. for transcribing speech audio, labeling tasks, language translation, etc.
Knowledge in the field of language technologies/NLP, such as language modeling, machine translation, text mining, parsing, etc.
Multilingual speaker
Description
We are looking for highly motivated engineers to fulfill our data needs and drive the product quality to the next level. You will be working closely with a group of talented researchers and engineers on the groundbreaking machine learning technologies. Your responsibilities include data creation, data management, processing pipeline and tools development. The role also involves the collaborations with other multi-functional teams on data related projects.
Education & Experience
MS/BS in Computer Science or related field, preferably with 5+ years of experience"
43,Senior Data Engineer,"San Mateo, CA",San Mateo,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Guideline is looking for a Senior Data Engineer to join the Data team.

What will you be working on?

If you are experienced with state-of-the-art data technologies and passionate about designing and building sophisticated data infrastructure from the ground up that underpins analytics and data product, this is the team you want to join.


Design, develop and maintain data infrastructure which is complex yet reliable, scalable and operable. Including databases / data warehouses, ETL / derived data jobs in batch and streaming mode, and integration with product components and third-party solutions
Design, develop and maintain in-product instrumentation for downstream analysis
Design, develop and maintain data product, in collaboration with data scientists and product teams
Identify and adopt state-of-the-art data technology solutions to meet company's growing data needs
Identify and adopt frameworks, policies and tools to enhance the team's productivity
Facilitate cross-functional collaboration
Coach junior members

The Team

Data Team is comprised of data engineers and data scientists, and tasked to improve our 401(K) product and business efficiency by leveraging the power of proprietary data and public data.

Required Qualifications


Bachelor degree in engineering
6+ year professional experience in a similar role
Extensive experience of 2+ database technologies (e.g. transactional database, analytical data warehouse, key-value data store, document store)
Extensive experience of 2+ backend programming languages (e.g. SQL, Python, Java, Scala)
Extensive experience of 1+ distributed computation frameworks (e.g. Hadoop, Spark), including batch mode and streaming mode
Extensive experience of ETL development and frameworks
Experience of API design and development
Experience of 2+ third-party analytics solutions (e.g. BI solution, marketing analytics solution, product funnel analytics solution, CRM solution)
Empathy, as manifested by excellent people skills, communication skills and collaboration skills
Intellectual curiosity and lifetime learner
Proactive and driven to deliver high-quality work and make impact

What is Guideline?

At Guideline, we're helping people save for a better future. Guideline is the only 401(k) provider that doesn't charge participants any fees on investments, regardless of the value of their assets or retirement account balance. And for employers, Guideline charges a low, flat rate fee per participant, in contrast to the asset-based fee model predominant in the industry.

With over 6,000 clients and hundreds of millions of dollars in assets under management, in just a couple of years of operations, we're well on our way to making smart retirement planning easy for everyone.

We have raised $59M to date with top-tier investors.

Employee Benefits


Flexible Vacation Policy
401(k) Matching
100% coverage of Health / Vision / Dental
Generous parental leave policy
Daily lunch
Commuter benefit

Guideline provides equal employment opportunities to all employees and applicants for employment without regard to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
44,Sr. Data Engineer,"Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,None Found,None Found,"
Advance degree (MS/PhD) in the field of Computer Science, Information Systems, Mathematics, Statistics, etc.
3+ years of experience in data, web or mobile services
Data architecture skills
Experience in SQL or similar languages and development experience in at least one scripting language (Python, Perl, etc.)
Experience with RDBMS, Big data technologies.
3+ years of experience working with large data sets and distributed computing tools (Map/Reduce, Hadoop, Hive, Spark etc.)
3+ years utilizing Object Oriented design and programming with Scala/Python skills to design, develop, and maintain large-scale web applications.
","Title: Sr. Data Engineer

Company: Samsung Research America (SRA)

Lab: VDI

Location: Mountain View, CA

Lab Summary:
The Digital Media Solutions Lab at Samsung Research America is building a next-generation data platform to support Smart TV products and services. With offices in Mountain View and Irvine, CA, we are close to a number of our collaboration partners. Our research and development include TV analytics, ads targeting, and personalized services. We ingest and process billions of records daily from millions of TVs in the field, and we are looking for an experienced professional to join our team on the development of an integrated data platform.

Position Summary:
As an Data Engineer you will have a focus on Analytics and work directly with Big data/AI/ML professionals and have opportunities to show off your expert analytical abilities as well as to grow your skills and career further. You will help design and build data marts, help manage the modern data warehousing infrastructure, analyze key metrics for decision makers to provide visual insights that will change the company direction and benefit consumers. The ideal candidate will be someone who comes from a strong analytical background that is now looking to strengthen their engineering skillset.

Experience Requirements:

Advance degree (MS/PhD) in the field of Computer Science, Information Systems, Mathematics, Statistics, etc.
3+ years of experience in data, web or mobile services
Data architecture skills
Experience in SQL or similar languages and development experience in at least one scripting language (Python, Perl, etc.)
Experience with RDBMS, Big data technologies.
3+ years of experience working with large data sets and distributed computing tools (Map/Reduce, Hadoop, Hive, Spark etc.)
3+ years utilizing Object Oriented design and programming with Scala/Python skills to design, develop, and maintain large-scale web applications.

Additional bonus:

Experience with open-source Data visualization tools or technologies
Proven track record driving rapid prototyping and designs

Samsung is an EEO/Veterans/Disabled/LGBT employer. We welcome and encourage diversity as we strive to create an inclusive workplace."
45,Big Data Engineer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Experian is seeking a Big Data Engineer to join our CIS Platform Engineering team. This is a great opportunity for someone who has experience and specializes in building large-scale, cloud-based platform to access timely, accurate, and relevant data. An ideal candidate will have built scalable platform that can easily accommodate and integrate different data sources and provide comprehensive data capabilities to all Experian products and delivery channels.

About Experian

Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. For five years in a row, we have been named in the Top 100 “World’s Most Innovative Companies” by Forbes Magazine. With a focus on our employees, we were rated the #1 Top Workplace by the Orange County Register. Experian Consumer Information Services is redefining the way our clients do business within all aspects of the customer credit lifecycle. Fueled by best-in-class data and innovative technology we help businesses make smarter decisions, identify consumers, make decisions on loans, market to prospects and collect.

About this role

As a Big Data Engineer, you must be a software development expert with extensive, well-rounded background in a diverse set of big data solutions (commercial, open source, in-house). Main responsibilities for this position include:
Assist in building out the DevOps strategy for hosting and managing our SDP microservice and connector infrastructure in AWS cloud
Strong track record of design/implementing big data technologies around Apache Hadoop, No SQL, Java/J2EE and distributed computing platforms in large enterprises where scale and complexity have been tackled.
Proven experience participating in agile development projects for enterprise-level systems component design and implementation
Deep understanding of application and enterprise software design for implementation of data services and middleware.


What your background looks like
5+ years of experience working with large cloud-based distributed computing platforms
Bachelor’s degree in Technical discipline; Masters preferred
Experience in monitoring the health of distributed systems and strategy for error detection and recovery
Proficient with Python and scripting languages
Experience with Elastic Search and Databases (RDBMS or NoSQL)
Experience in Spark stream processing is a plus
Experience in RDBMS change log streaming is a plus
Systems integration experience, including design and development of APIs, Adapters, and Connectors and Integration with Hadoop/HDFS, Real-Time Systems, Data Warehouses, and Analytics solutions.
Financial Industry experience preferred
Perks
You’ll be working in our San Jose office, which is in the heart Santana Row
You’ll be working in a big data analytic incubator with talented, passionate individuals who are focused on architecting world-class cutting-edge analytics solutions
Four weeks of vacation to start, five sick days and two volunteer days (plus eleven paid holidays)
This is a bonus eligible position with a 15% bonus target
Employee stock purchase program and 401K matching
Wellness initiatives, online discounts, employee discounts, pet insurance and more
EOE including disability/veterans"
46,Data Engineer,"Redwood City, CA 94065",Redwood City,CA,94065,None Found,"2+ years of industry experience in a Data Engineer role.
A Bachelor degree in a quantitative field, such as Computer Science, Applied Mathematics, or Statistics, or equivalent professional experience.
Working experience with SQL, Python (3.x) and Scala is a plus.
Working experience on an ETL system.
Strong communication skills, both written and oral, and an ability to convey complex results in a clear manner.
",None Found,"Work with the team to manage the data warehouse and ETL for all of Perfect World Entertainment products.
Design, build and launch new data models in production.
Interface with engineers from other products to ensure proper data collection.
Implement new requests from product managers and data analysts to fulfill their data needs.
Ensure data quality by implementing data detection mechanisms.
Support existing processes running in production and optimize it when possible.
",None Found,None Found,"Join our Data Engineering team and help build a scalable real-time analytics platform that processes streaming data to make our product even more intelligent! Own and extend our data pipeline, perform data modeling, and improve data reliability and quality. Become part of a team focused on creating innovative real-time analytics and machine learning feedback loops.

Responsibilities
Work with the team to manage the data warehouse and ETL for all of Perfect World Entertainment products.
Design, build and launch new data models in production.
Interface with engineers from other products to ensure proper data collection.
Implement new requests from product managers and data analysts to fulfill their data needs.
Ensure data quality by implementing data detection mechanisms.
Support existing processes running in production and optimize it when possible.



Required Qualifications
2+ years of industry experience in a Data Engineer role.
A Bachelor degree in a quantitative field, such as Computer Science, Applied Mathematics, or Statistics, or equivalent professional experience.
Working experience with SQL, Python (3.x) and Scala is a plus.
Working experience on an ETL system.
Strong communication skills, both written and oral, and an ability to convey complex results in a clear manner.

Desired Qualifications
Working experience with Machine Learning and predictive analytics.
Familiarity with Hadoop framework.
Experience with Spark is highly desirable.
Familiar with data visualization through Tableau, or other tools."
47,Data Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Location: Redwood City

Job Description:
The data team as Invoice2go is dedicated to supporting the rest of the company by transforming our raw data into high quality and high fidelity insights. We're building data sets for an array of uses including performance analysis, personalization, growth and marketing communications. Engineers on this team are critical to making sure Invoice2go continues to be one of the top business apps in fintech.

What You'll Be Doing


Using Databricks and Spark to transform our largest data feeds into a columnar storage format for easy, cost-effective access in S3
Experimenting with Spectrum to allow queries that run on both Redshift and S3
Optimizing our ETL process to make sure we can provide faster data without sacrificing quality

Who We Are Looking For


Someone who can collaborate with product owners to experiment, iterate and deliver new tools
Can learn new languages as needed, depending on the best framework or tool for the job
Strong understanding of schema design and modeling for large datasets.
Someone who loves what they do, is passionate about software development and wants to contribute to the company culture everyday
Experience with SQL and Python is helpful
Someone that owns their work, from conception to release and beyond.

------

------

-----------
ABOUT US...
-----------

Invoice2go is the world's top selling invoicing app, but we haven't stopped there. Equipping business owners with the most straightforward way to run a business, Invoice2go brings together all the tools needed to get the job done: From winning jobs, tracking estimates and payments, and offering the ability to pay any way.

We strive to give small businesses control over their time and their business.

Invoice2go was founded in Australia by Chris Strode, a small business owner who came from a family of tradespeople, and wanted to help them streamline their invoicing.

Today, we are backed by $60 million in funding from Accel, Ribbit Capital and OCV Partners, and trusted by business owners across 160 countries to send $24 billion in invoicing every year. The company employs a world-class team from it's offices in Redwood City, California and Sydney, Australia.

We're working hard to solve big challenges for the smallest of businesses, and we're always looking for talented people to join our team!!

Invoice2go is an equal opportunity employer. In accordance with applicable law, we prohibit discrimination against any applicant or employee based on any legally-recognized basis, including, but not limited to: veteran status, uniformed service member status, race, color, religion, sex, age (40 and over), pregnancy (including childbirth, lactation and related medical conditions), national origin or ancestry, physical or mental disability, genetic information (including testing and characteristics) or any other consideration protected by federal, state or local law. Our commitment to equal opportunity employment applies to all persons involved in our operations and prohibits unlawful discrimination by any employee, including supervisors and co-workers."
48,Data Engineer,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Based in our Global Headquarters located in Palo Alto, you will be working with the data team to help build infrastructure, tooling, and data pipelines to power our data-driven organization and support our rapidly evolving and growing data needs. The ideal candidate will be extremely curious and will their data skills and business mindset to make a difference every day. We are looking for people who can operate at a company that grows as fast as ours, by being able to deal with multiple moving pieces while still holding up quality, long term thinking and delivering value to our customers.

What You'll Do:

Design, build, and maintain data integration pipelines for ingress and egress through the data team infrastructure
Help design and implement data structures in data pipes and data warehouse to allow accurate, efficient reporting, analysis, and machine learning
Develop and implement tooling in the form of Python libraries and deployed systems to allow analysts and scientists to work efficiently and consistently
Provide technical best practice in the team via training and documentation to ensure all disciplines are working in the correct manner
Manage and develop our data persistence environments (data lakes, storage, etc) to ensure that data is properly available to users and secure
Implement and maintain monitoring, alerting, logging, and data quality controls to ensure the accuracy and reliability of our data ecosystem

What We're Looking For:

Ph.D. or Masters in computer science or equivalent
3+ years of work experience in data engineering
Highly experienced and proficient with data infrastructure tooling using Python
Highly experienced and proficient with data modeling methodologies and implementation
Demonstrable experience of SQL
Proficient across the entire stack of technologies used for data management including:
Data pipelines (Kafka, Kinesis)
Structured big data stores (Redshift, Snowflake, Vertica)
Semi-structured big data stores (Hadoop, Presto)
ETL systems
Experienced with large scale, distributed systems and pipelines for data management

"
49,Senior-Level Streaming/Query Software Engineer (Cortex Platform),"Santa Clara, CA",Santa Clara,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Our Mission
At Palo Alto Networks® everything starts and ends with our mission:
Being the cybersecurity partner of choice, protecting our digital way of life.
We have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish – but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
Your Career
With Cortex, the industry's only open and integrated, AI-based continuous security platform. Cortex is a significant evolution of the Application Framework designed to simplify security operations and considerably improve outcomes. Deployed on a global, scalable public cloud platform, Cortex allows security operations teams to speed the analysis of massive data sets. Cortex is enabled by the Cortex Data Lake, where customers can securely and privately store and analyze large amounts of data that is normalized for advanced AI and machine learning to find threats and orchestrate responses quickly. We’re building the foundations of the next inflection point in the security industry by combining our deep expertise in security with the state of the art in large-scale infrastructure for data processing and machine learning (Think exabytes of data).
As a Senior-Level Big Data Engineer, you will be working with highly motivated software developers that thrive on solving challenging problems. You're responsible for building large-scale distributed software systems in Java, Python, and other languages using open source technologies like Hadoop, Kafka, Redis, Spark, Flink (and other proprietary technologies).

For our Cortex Data Lake, Streaming and Query-related big-data experience is a plus.
Your Impact
Provide technical engineering in small teams working on the Application Framework
Collaborate with the broader Cortex Platform team to help develop a long-term technology roadmap
Be both a a hands-on expert for Opensource development tools being used to build the Cortex Platform
Project management, tracking, and delivery
Your Experience
BA/BS Degree in Computer Science or related technical discipline, or related practical experience
6+ year’s experience in software engineering management with an advanced level of experience in software development
Validated experience in hiring, mentoring, coaching and developing engineering talent and small teams
Strong management skills for planning and executing complex projects
Experience with industry, open-source projects and/or academic research in large-data distributed systems
Built complex software systems that have been successfully delivered to customers
Deep expertise is helpful in one or more of the following areas: Real-time stream processing, Columnar databases, Indexing and search, Query processing, Hadoop, Flink, Spark
The Team
To stay ahead of the curve, it’s critical to know where the curve is, and how to anticipate the changes we’re facing. For the fastest growing cybersecurity company, the curve is the evolution of cyberattacks and the products and services that proactively address them. Our engineering team is at the core of our products – connected directly to the mission of preventing cyberattacks. They are constantly innovating – challenging the way we, and the industry, think about cybersecurity. These engineers aren’t shy about building products to solve problems no one has pursued before. They define the industry, instead of waiting for directions. We need individuals who feel comfortable in ambiguity, excited by the prospect of a challenge, and empowered by the unknown risks facing our everyday lives that are only enabled by a secure digital environment.
Our engineering team is provided with an unrivaled chance to create the products and practices that will support our company growth over the next decade, defining the cybersecurity industry as we know it. If you see the potential of how incredible people products can transform a business, this is the team for you. If you don’t wait for directions, instead, identifying new features and opportunities we have to just get better, this is your new career.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together. To learn more about our dedication to inclusion and innovation, visit our Life at Palo Alto Networks page and our diversity website.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.
Additionally, we are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or an accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com."
50,Senior Big Data Engineer,"Santa Clara, CA",Santa Clara,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Our Mission
At Palo Alto Networks® everything starts and ends with our mission:
Being the cybersecurity partner of choice, protecting our digital way of life.
We have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish – but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
Your Career
Our daily fight with cyber bad guys requires us to collect and analyze a lot of data…. A LOT of data. And, as our customer base continues its rapid growth, we need to look at faster and more robust tools to help us and our customers make the best decisions possible.
With your knowledge of Hadoop and Big Data technologies, you will add your tools-building superpowers to a small team tasked with building out a DevOps automation environment, one that will step up our Business Analytics game and help us protect our customers from cyber intruders.
We offer the chance to be part of an important mission: ending breaches and protecting our way of digital life. If you are a motivated, intelligent, creative, and hardworking individual, then this job is for you!
Your Impact
As a Senior Engineer, you will be an integral member of our Artificial Intelligence and Analytics team responsible for the design and development of distributed data applications on Cloud
Partner with data analyst, product owners, and data scientists, to better understand requirements, finding bottlenecks, resolutions, etc
Design, develop and support Platform-as-a-Service (PaaS) frameworks, tools, Micro Services leveraging public cloud infrastructure
Be an SME for all things ‘Cloud and Big Data’ as well as mentor other team members
Design and develop different architectural models for our scalable data processing and data serving systems
Build data pipelines and ETL using heterogeneous sources
Build data ingestion from various source systems to Google Cloud using Kafka, Dataflow, Dataproc Streaming, etc
Responsible to ensure that the platform goes through Continuous Integration (CI) and Continuous Deployment (CD) with DevOps automation
Expands and grows data platform capabilities to tackle new data problems and challenges
Supports Big Data and batch/real-time analytical solutions using groundbreaking technologies like Apache Beam
Have the ability to research and assess open source technologies and components to recommend and integrate into the design and implementation
Your Experience
Degree in Bachelor of Science in Computer Science or equivalent
7+ years of experience with the Hadoop ecosystem, Cloud and other Distributed Systems and Technologies
Strong hands-on experience in developing applications in more than one language stacks: Java, Python, Scala
Expert-level software development experience - practicing strong software development principles and best practices: Test-driven development, CI/CD, coding standards
Ability to dynamically adapt to conventional big data and Cloud frameworks and tools with the use-cases required by the project
Experience with building stream-processing systems using solutions such as Dataflow, spark-streaming, or Flink, etc
Experience in Google Cloud Technologies is a plus
Experience in other open-sources technologies like Elastic Search, Logstash, JanusGraph is a plus
Knowledge of design strategies for developing a scalable, resilient, always-on data lake
Some knowledge of agile(scrum) development methodology is a plus
Strong development/automation skills
Excellent interpersonal and teamwork skills
Can-do attitude on problem-solving, quality and ability to execute
The Team
Working at a high-tech cybersecurity company within Information Technology is a once in a lifetime opportunity. You’ll be joined with the brightest minds in technology, creating, building, and supporting tools and that enable our global teams on the front line of defense against cyberattacks.
We’re joined by one mission – but driven by the impact of that mission and what it means to protect our way of life in the digital age. Join a dynamic and fast-paced team that feels excitement at the prospect of a challenge and feels a thrill at resolving technical gaps that inhibit productivity.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.
We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.
#LI-AH1"
51,Data Engineer (Contract),"San Mateo, CA 94404",San Mateo,CA,94404,None Found,"
Experience with business processes and data around Customer 360, CRM and Revenue Recognition
In-depth knowledge of Database design, SQL and NoSQL, SaaS Data Lake, Time series analysis, OLAP, Data Warehouses and Metadata management
Hands on Experience with AWS services, security framework, events and notifications
Expert in SQL, Python/Java/PHP specifically as they relate to data management
Experience in ETL design, implementation and maintenance and using Mulesoft / Boomi / SnapLogic
General experience with BI tools and working with business users on analytics and reporting
Hands on experience in large scale high volume database design and optimization
Excellent problem solving skills with troubleshooting and root cause analysis
Good communication skills, both oral and written with experience working in a distributed, collaborative team
Ability to work without supervision
Hands on experience with Zuora is a plus
Bachelors/Masters in Computer Science or equivalent with 3-5 years of experience in data processing","
Experience with business processes and data around Customer 360, CRM and Revenue Recognition
In-depth knowledge of Database design, SQL and NoSQL, SaaS Data Lake, Time series analysis, OLAP, Data Warehouses and Metadata management
Hands on Experience with AWS services, security framework, events and notifications
Expert in SQL, Python/Java/PHP specifically as they relate to data management
Experience in ETL design, implementation and maintenance and using Mulesoft / Boomi / SnapLogic
General experience with BI tools and working with business users on analytics and reporting
Hands on experience in large scale high volume database design and optimization
Excellent problem solving skills with troubleshooting and root cause analysis
Good communication skills, both oral and written with experience working in a distributed, collaborative team
Ability to work without supervision
Hands on experience with Zuora is a plus
Bachelors/Masters in Computer Science or equivalent with 3-5 years of experience in data processing","
Perform business data analysis (licensing, telemetry, sales hierarchies, visibilities, support tickets etc.) to drive decision making
Create data dictionary and metadata catalog to ensure standardization of data vocabulary
Design, Build, launch data pipelines optimized for SaaS Data Lake and own data quality for the pipelines
Monitor and remediate data quality issues with closed feedback loop; create daily anomaly reports
Enhance and maintain data pipelines to be extremely efficient & reliable for large amounts of data from source systems (Salesforce, Zendesk etc.), pre-processing, loading into derived Data Lake for downstream processing and analysis
Work with multiple teams in high visibility roles and own the solution end-to-end
Enable self-service. Develop new systems, tools, interfaces, dashboards to enable teams to consume and understand data & gain insights
Maintain custom transformations/calculations to meet specific business requirements (e.g. utilization model)
Implement and maintain Inbound/outbound integrations with Salesforce, Marketo, Zuora or Revpo etc.
Participate in sprint demonstrations with executive and customer stakeholders
Create design specs and ensure traceability from Functional, technical requirements to solutions
",None Found,None Found,"We are a SaaS company and the world’s foremost evangelist of the Subscription Economy®. Zuora’s leading subscription relationship management platform helps enable businesses in any industry to launch or shift products to subscription, implement new pay-as-you-go pricing and packaging models, gain new insights into subscriber behavior, open new revenue streams, and disrupt market segments to gain competitive advantage. Zuora serves more than 800 companies around the world in every industry. The Subscription Economy Index (SEI) demonstrates that SEI companies are growing revenues approximately nine times faster than the S&P 500. Headquartered in Silicon Valley, Zuora also operates offices in Atlanta, Boston, Denver, San Francisco, London, Paris, Beijing, Sydney and Tokyo.

To learn more visit www.zuora.com

Job Description

The Business Technology team is looking for a talented Data Engineer to join the team. You will be responsible for making sure that we are our own best user of Zuora Analytics. This data engineer will be the SME for anything “Data” in business and technical context. S/he will work with various teams within the company to perform data analysis around Customer 360 (Marketing, Sales, Quote to Cash, Revenue Recognition and other 3rd party managed data sources) and Bookings. This position requires constant interaction with the Products, Operations, Finance and BT teams to analyze data anomalies. The candidate must be able to deal with constant change in the user requirements, multiple parallel assignments and collaborate across groups as necessary.

Responsibilities

Perform business data analysis (licensing, telemetry, sales hierarchies, visibilities, support tickets etc.) to drive decision making
Create data dictionary and metadata catalog to ensure standardization of data vocabulary
Design, Build, launch data pipelines optimized for SaaS Data Lake and own data quality for the pipelines
Monitor and remediate data quality issues with closed feedback loop; create daily anomaly reports
Enhance and maintain data pipelines to be extremely efficient & reliable for large amounts of data from source systems (Salesforce, Zendesk etc.), pre-processing, loading into derived Data Lake for downstream processing and analysis
Work with multiple teams in high visibility roles and own the solution end-to-end
Enable self-service. Develop new systems, tools, interfaces, dashboards to enable teams to consume and understand data & gain insights
Maintain custom transformations/calculations to meet specific business requirements (e.g. utilization model)
Implement and maintain Inbound/outbound integrations with Salesforce, Marketo, Zuora or Revpo etc.
Participate in sprint demonstrations with executive and customer stakeholders
Create design specs and ensure traceability from Functional, technical requirements to solutions


Skills & Qualifications

Experience with business processes and data around Customer 360, CRM and Revenue Recognition
In-depth knowledge of Database design, SQL and NoSQL, SaaS Data Lake, Time series analysis, OLAP, Data Warehouses and Metadata management
Hands on Experience with AWS services, security framework, events and notifications
Expert in SQL, Python/Java/PHP specifically as they relate to data management
Experience in ETL design, implementation and maintenance and using Mulesoft / Boomi / SnapLogic
General experience with BI tools and working with business users on analytics and reporting
Hands on experience in large scale high volume database design and optimization
Excellent problem solving skills with troubleshooting and root cause analysis
Good communication skills, both oral and written with experience working in a distributed, collaborative team
Ability to work without supervision
Hands on experience with Zuora is a plus
Bachelors/Masters in Computer Science or equivalent with 3-5 years of experience in data processing"
52,Big Data Engineer,"Santa Clara, CA",Santa Clara,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Our Mission
At Palo Alto Networks® everything starts and ends with our mission: protecting our way of life in the digital age by preventing successful cyberattacks. It’s not a small goal. It isn’t simple either, but we aren’t in this for the easy answer. As a company with a foundation in challenging the way things are done, we’re looking for innovators with a dedication to best. In return, your career will have a tangible impact – one that's working toward technology that affects every level of society.
Our mission doesn’t happen by treading softly – no, it happens by defining an industry. It means building products that haven't been thought of. It means selling products with a solutions mindset. It means supporting the infrastructure of a company that moves at an incredible speed – intentionally – to stay ahead of the world’s next cyberthreat.

Your Career
Our daily fight with cyber bad guys requires us to collect and analyze a lot of data…. A LOT of data. And, as our customer base continues its rapid growth, we need to look at faster and more robust tools to help us and our customers make the best decisions possible.
With your knowledge of Hadoop and Big Data technologies, you will add your tools-building superpowers to a small team tasked with building out a DevOps automation environment, one that will step up our Business Analytics game and help us protect our customers from cyber intruders.
We offer the chance to be part of an important mission: ending breaches and protecting our way of digital life. If you are a motivated, intelligent, creative, and hardworking individual, then this job is for you!

Overview
You will be responsible for leading technical projects to build custom applications and enhance business systems. We are looking for someone with solid project management and organization skills complemented by a strong technical background. You will work with a team of senior level managers and highly technical engineers to lead complex IT projects. The role will collaborate with multiple engineering and business organizations, understand and align with their needs, and take independent end-to-end responsibility for release of new business capabilities to production.
Your Impact
As a Big Data Engineer, you will be an integral member of our Big Data and Analytics team responsible for design and development
Partner with data analyst, product owners and data scientists, to better understand requirements, finding bottlenecks, resolutions, etc.
Design and develop Big Data solutions both in Cloud & OnPrem
Design and develop different architectural models for our scalable data processing as well as scalable data storage
Build data pipelines and ETL using heterogeneous sources using Dataflow or DataProc
Build data ingestion from various source systems to Hadoop or GCP using Kafka, Flume, Sqoop, Spark Streaming etc.
Transform data, using data mapping and data processing in Apache Beam or Spark
Responsible to ensure that the platform goes through Continuous Integration (CI) and Continuous Deployment (CD) with DevOps automation
Expands and grows data platform capabilities to tackle new data problems and challenges
Supports Big Data and batch/real time analytical solutions using groundbreaking technologies like Apache Beam
Have the ability to research and assess open source technologies and components to recommend and integrate into the design and implementation
Work with development and QA teams to design Ingestion Pipelines, Integration APIs, and provide Hadoop ecosystem services
Your Experience
Degree in Bachelor of Science in Computer Science or equivalent
5+ years of experience with the Hadoop ecosystem and Big Data technologies
2+ year of experience in Cloud computing
Competent in writing Scala, Python or Java code.
Development experience in Dataflow or DataProc is a Plus
Ability to dynamically adapt to conventional big data frameworks and tools with the use-cases required by the project
Experience with building stream-processing systems using solutions such as spark-streaming, Storm or Flink etc.
Experience in other open-sources like Druid, Elastic Search, Logstash etc. is a plus
Knowledge of design strategies for developing scalable, resilient, always-on data lake
Some knowledge of agile(scrum) development methodology is a plus
Strong development/automation skills
Excellent inter-personal and teamwork skills
Can-do attitude on problem solving, quality and ability to execute
The Team
Working at a high-tech cybersecurity company within Information Technology is a once in a lifetime opportunity. You’ll be joined with the brightest minds in technology, creating, building, and supporting tools and that enable our global teams on the front line of defense against cyberattacks. We’re joined by one mission – but driven by the impact of that mission and what it means to protect our way of life in the digital age. Join a dynamic and fast-paced team that feels excitement at the prospect of a challenge and feels a thrill at resolving technical gaps that inhibit productivity.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together. To learn more about our dedication to inclusion and innovation, visit our Life at Palo Alto Networks page and our diversity website.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.
Additionally, we are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or an accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.
#LI-AH1"
53,Data Engineer Bachelor's (Intern) - United States,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Who You Are:
Data Engineers are focused on enabling a data driven approach to optimization by sourcing, maintaining and ensuring the availability of data used to drive full life cycle marketing insights to optimize CX’s marketing investments and the customer experience. They integrate both batch and streaming approaches to support standard business intelligence, as well as decision automation and machine learning requirements. Data engineers help make data much easier to understand and consume for others and have led to pivot to new technologies rapidly.
What you’ll do:
Provide leadership and support for development of an integrated Customer Experience data foundation that will enable extensive business intelligence and machine learning for CX Platform and extended user communities.
Work with CX business and IT teams to ensure high quality, on-time deliverables that meet usability, scalability, quality and performance standards.
Provide hands-on technical support for development, research and quality assurance testing.
Apply a variety of technologies to ingest, transform, index, aggregate, correlate, provide API's, visualize and enable a spectrum of organizations across Cisco.
Analyze structural requirements for data storage solutions and software
Design architectures for technical systems as to ensure robustness, scalability, and completeness
We are seeking high energy and qualified candidates who possess the following skills and experience:
You're actively pursuing a bachelor’s or master’s degree in computer science, Mathematics, Engineering, Statistics, or related field
Experience working with large datasets
Who you'll work with:
You will work cross functionally with the Customer Experience, Sales, Marketing, and IT organizations, playing a leadership role in transforming Cisco by developing and implementing analytic models and intelligent automation to drive us toward a data-driven digital organization.
Digital Lifecycle Journey’s (DLJ) digital expertise makes us uniquely qualified to address the evolving expectations of today’s connected customers and partners, along with Cisco sellers. Using real-time connected data, machine learning, and automation; the team enables Cisco sellers and partners to deliver a powerful, personalized experience—throughout the entire customer lifecycle. DLJ is focused on providing customers with an immersive digital experience with Cisco. This in turn drives improvements in recurring revenue, cost savings and sales effectiveness for Cisco and its partners.
Why Cisco
#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!"
54,Data Engineer,"Sunnyvale, CA",Sunnyvale,CA,None Found,None Found,None Found,None Found,None Found,None Found,"
ETL experience.
Data Warehouse experience.
SQL experience.
Programming experience.
Comfortable in a CLI.
Experience in quantitative data analysis.
Ability to analyze an enormous amount of complex data and deliver meaningful conclusions that can be turned into actionable decisions.
Knowledge of statistics.
Great grasp of numbers with love for data and analytics.
Excellent communication (oral and written), attention to detail, time management and organizational skills.","Mission to Our Employees:
Teladoc Health is the global virtual care leader, offering the only comprehensive virtual care solution spanning telehealth, expert medical, and licensed platform services. Teladoc Health serves the world's leading insurers, employers, and health systems and helps millions of people around the world resolve their healthcare needs with confidence. We are committed to provide our Employees a stable work environment with equal opportunity for learning and personal growth. Creativity and innovation are encouraged for improving the effectiveness of Teladoc Health. Above all, Employees will be provided the same concern, respect, values and caring attitude within the organization that they are expected to share with our clients.
What we are looking for:
We are seeking a smart, ambitious and analytical person to be responsible for our data that is used for business, product, and marketing analytics. You will own the entire data life cycle. You will develop systems to gather, transform, and surface the data to other data, marketing, and product analysts. You will also be involved in the analysis yourself and drive real business decisions based on that analysis. If you are looking to work in the intersection of data engineering and data analysis, this is the position for you.
What you will do:
Manage, develop, maintain our ETL.
Manage, develop, maintain our data warehouse.
Manage, develop, maintain our BI tools.
Work with hundreds of millions of rows of data.
Analyze our data to gather actionable insights to drive decisions in product and marketing.
Work with product managers and marketers so they have the right data.
Enjoy great teamwork, have lots of fun, and take pride in building a world-class product that makes a difference on people's lives.
Requirements:
ETL experience.
Data Warehouse experience.
SQL experience.
Programming experience.
Comfortable in a CLI.
Experience in quantitative data analysis.
Ability to analyze an enormous amount of complex data and deliver meaningful conclusions that can be turned into actionable decisions.
Knowledge of statistics.
Great grasp of numbers with love for data and analytics.
Excellent communication (oral and written), attention to detail, time management and organizational skills.
Company Benefits:
Very competitive salary & equity compensation.
Excellent health, dental, vision coverage.
401k benefits with employer matching contribution; immediately vested.
Awesome people to work with.
Becoming a part of a movement in telemedicine and building something that matters.

For a more detailed look at our company and values, visit our website at https://teladochealth.com/en/about/

At Teladoc Health we thrive on difference and individuality. Teladoc Health is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status."
55,Senior Data Engineer,"Sunnyvale, CA 94085",Sunnyvale,CA,94085,None Found,"
MS in Computer Science
3+ years industry experience building and operating distributed data systems in production
3+ years of programming experience in Scala or Java",None Found,"
Design and implement fault-tolerant data pipelines to integrate large amounts of data from many diverse storage systems.
Promote a culture of self-serve data analytics by minimizing technical barriers to data access and understanding.
Execute complex data engineering projects that have a significant impact on Bosch global business.
Share knowledge by clearly articulating results and ideas to customers, managers, and key decision makers.
Stay current with the latest research and technology and communicate your knowledge throughout the enterprise
Take responsibility for preparing data for analysis and provide critical feedback on issues of data integrity
Up to 10% travel may be required.",None Found,None Found,"Job Description

Primary Responsibilities:
Design and implement fault-tolerant data pipelines to integrate large amounts of data from many diverse storage systems.
Promote a culture of self-serve data analytics by minimizing technical barriers to data access and understanding.
Execute complex data engineering projects that have a significant impact on Bosch global business.
Share knowledge by clearly articulating results and ideas to customers, managers, and key decision makers.
Stay current with the latest research and technology and communicate your knowledge throughout the enterprise
Take responsibility for preparing data for analysis and provide critical feedback on issues of data integrity
Up to 10% travel may be required.

Qualifications

Basic Qualifications:
MS in Computer Science
3+ years industry experience building and operating distributed data systems in production
3+ years of programming experience in Scala or Java
Preferred Qualifications:
Proficient in tuning and performance optimization of Apache Spark jobs
Experience with integration of data from multiple data sources
Experience with various messaging systems, such as Kafka or RabbitMQ
Experience managing and solving ongoing issues with a Spark/Hadoop cluster
Familiarity with distributed machine learning frameworks like Spark MLlib
Familiarity of machine learning / deep learning methods
Additional Information

BOSCH is a proud supporter of STEM (Science, Technology, Engineering & Mathematics) Initiatives
FIRST Robotics (For Inspiration and Recognition of Science and Technology)
AWIM (A World In Motion)
By choice, we are committed to a diverse workforce – EOE/Protected Veteran/Disabled."
56,Apple Media Products - Commerce Data Engineer,"Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Mar 18, 2019
Weekly Hours: 40
Role Number: 200040350
The Apple Media Products Commerce Engineering team has a proud tradition of delivering state of the art products in a competitive marketplace. We seek to maintain a meaningful and rewarding environment where the best engineers and scientists can collaborate and produce real-world improvements in customers' online experience. You will solve problems unrivaled in scale and concept in the pursuit of new and creative features. We seeking a talented, experienced Applied Researcher/Data Scientist to work on high visibility projects that affect millions of customers globally. At Apple, great ideas have a way of becoming great products, services, and customer experiences very quickly. If you are a self-motivated, high-energy individual who is not afraid of challenges, we're looking for you.
Key Qualifications
Strong knowledge of coding practices and experience.
Deep understanding of the full software development lifecycle.
Experience of object oriented programming languages like Scala/Java.
Experience of processing large-scale production-level data sets.
Experience working in a large code base.
Experience of building and running large-scale data pipelines.
Experience with large scale data processing frameworks like Spark/Hadoop.
Experience with low-latency big data stores, e.g. Cassandra, Voldemort or HBase.
Highly self-motivated, results driven and data driven.
Ability to stay focused and prioritize a heavy workload while achieving exceptional quality.
Ability to work in a fast-paced dynamic environment.
Attention to detail, data accuracy and quality of output.
Familiarity with scalability and performance issues.
Excellent judgment and integrity with the ability to make timely and sound decisions.
Experience in payment science is a plus.
Description
The Apple Media Products Commerce Engineering team is looking for someone with a love for data. This position involves working on very large scale data mining, cleaning, analysis, deep level processing, machine learning or statistic modeling, metrics tracking and evaluation. The goal is to improve Apps Store/iTunes Store payment experience and engagement. You should have a passion for quality and an ability to understand complex systems while being able to iterate quickly on all stages of data understanding and modeling to solve the real-world problems.
Education & Experience
MS in Computer Science or related field. PhD preferred."
57,Data Engineer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,"
Basic knowledge of data science methodologies.
Basic understanding of business requirements and data science objectives.
Basic data mapping, data transfer and data migration skills. Strong understanding of analytics software, tools and methodologies.
Basic knowledge of machine learning, data integration, and modeling skills and ETL tools .
Basic communication and presentation skills.
Basic data knowledge of relevant data programming languages.
Basic knowledge of data visualization techniques.","
Designs, develops and applies programs, methodologies and systems based on advanced analytic models (e.g. advanced statistics, operations research, computer science, process) to transform structured and unstructured data into meaningful and actionable information insights that drive decision making.
Uses visualization techniques to translate analytic insights into understandable business stories (eg. descriptive, inferential and predictive insights).
Embeds analytics into client’s business processes and applications. Combines business acumen and scientific methods to solve business problems. • Participates in the analysis and validation of data sets/solutions/user experience.
Aids in the development, enhancement and maintenance of a client's metadata based on analytic objectives. May load data into the infrastructure and contributes to the creation of the hypothesis matrix. Prepares a portion of the data for the Exploratory Data Analysis (EDA) / hypotheses.
Contributes to building models for the overall solution, validates results and performance. Contributes to the selection of the model that supports the overall solution.
Supports the research, identification and delivery of data science solutions to problems.
Supports visualization of the model's insights, user experience and configuration tools for the analytics model.","
Master's degree in Statistics, Operations Research, Computer Science or equivalent.
2-4 years relevant experience",None Found,"At HPE, we bring together the brightest minds to create breakthrough technology solutions and advance the way people live and work. Our legacy inspires us as we forge ahead dedicated to helping our customers make their mark on the world.

Learning does not only happen through training. Relationships are among the most powerful ways for people to learn and grow, and this is part of our HPE culture. In addition to working alongside talented colleagues, you will have many opportunities to learn through coaching and stretch assignment opportunities. You’ll be guided by feedback and support to accelerate your learning and maximize your knowledge. We also have a “reverse mentoring” program which allows us to share our knowledge and strengths across our multi-generation workforce.
Responsibilities:
Designs, develops and applies programs, methodologies and systems based on advanced analytic models (e.g. advanced statistics, operations research, computer science, process) to transform structured and unstructured data into meaningful and actionable information insights that drive decision making.
Uses visualization techniques to translate analytic insights into understandable business stories (eg. descriptive, inferential and predictive insights).
Embeds analytics into client’s business processes and applications. Combines business acumen and scientific methods to solve business problems. • Participates in the analysis and validation of data sets/solutions/user experience.
Aids in the development, enhancement and maintenance of a client's metadata based on analytic objectives. May load data into the infrastructure and contributes to the creation of the hypothesis matrix. Prepares a portion of the data for the Exploratory Data Analysis (EDA) / hypotheses.
Contributes to building models for the overall solution, validates results and performance. Contributes to the selection of the model that supports the overall solution.
Supports the research, identification and delivery of data science solutions to problems.
Supports visualization of the model's insights, user experience and configuration tools for the analytics model.
Education and Experience Required:
Master's degree in Statistics, Operations Research, Computer Science or equivalent.
2-4 years relevant experience
Knowledge and Skills:
Basic knowledge of data science methodologies.
Basic understanding of business requirements and data science objectives.
Basic data mapping, data transfer and data migration skills. Strong understanding of analytics software, tools and methodologies.
Basic knowledge of machine learning, data integration, and modeling skills and ETL tools .
Basic communication and presentation skills.
Basic data knowledge of relevant data programming languages.
Basic knowledge of data visualization techniques.
We offer:
A competitive salary and extensive social benefits
Diverse and dynamic work environment
Work-life balance and support for career development
An amazing life inside the element!
Want to know more about it? Then let’s stay connected!
https://www.facebook.com/HPECareers
https://twitter.com/HPE_Careers
1053795"
58,"Senior Data Engineer, Membership","Los Gatos, CA 95032",Los Gatos,CA,95032,None Found,None Found,None Found,None Found,None Found,None Found,"Los Gatos, California
Infrastructure and Tooling
Netflix is revolutionizing entertainment as well as pushing the limits of what it means to be a subscription business. We are one of the planet’s largest recurring subscription services and our members are rightfully the cornerstone of what we do. From ensuring a smooth membership lifecycle and regular billings to enabling our business and finance teams to meet regulations and create new business opportunities, the data is truly mission critical.

This space requires folks who enjoy the complexity that emerges as we manage the member lifecycle which includes reasoning with membership, billing and invoicing systems as well as reporting on revenue. As part of the Membership Data Engineering team you will play a vital role in creating reliable, distributed data pipelines and building intuitive data products that allow our stakeholders to easily leverage data in a self-service manner. You will be expected to show thought leadership and partner effectively with our science and engineering teams to push the business towards better metrics and more efficient systems.
Who are you:
You LOVE data of all sorts, big and small! You enjoy helping teams push the boundaries of what business insights can be extracted out of data
You have a strong background in distributed data processing, software engineering design, and data modeling concepts
You are always on the lookout for opportunities to automate tasks and deliver generic frameworks which can be leveraged by multiple data engineering teams
You like to spark joy in internal partners with high-quality data products that are well documented, modeled and easy to understand
You have strong communication skills to effectively partner with data scientists and engineering stakeholders. You love to innovate and push partners to deliver on novel metrics and solutions
You are proficient in at least one major language (e.g. Java, Scala, Python). You strive to write beautiful code and you're comfortable working with and picking up new technologies
You can relate to many of the aspects of the Netflix Culture and love to operate independently while collaborating and giving/receiving strong, candid feedback to your team members.
Learn more about the team, technologies and the immediate team members you’d get to work with! If you’d like us to makes changes to the interview process to improve the odds of your sailing through it with flying colors, please share your thoughts with us - we promise to do whatever is feasible to accommodate.

APPLY NOW
Share this listing:
LINK COPIED"
59,Sr. Big Data Software Engineer,"San Jose, CA 95134",San Jose,CA,95134,None Found,None Found,None Found,None Found,None Found,None Found,"JOB TITLE
Sr. Big Data Engineer

Requisition ID
DSA32833

OVERVIEW

Samsung Semiconductor, Inc. is a world leader in Memory, System LSI and LCD technologies. We are currently looking for software talent to join our team in San Jose, CA.
The Memory Solutions Labs (MSL) is part of Samsung’s Memory Business Unit, the industry's technology and volume leader in DRAM, NAND Flash, SRAM memory. MSL’s vision is to solve key problems & optimize architecture solutions for Cloud Computing, Big Data and High Performance data center environments. We are an integral part of Samsung’s strong R&D focus & lab innovation engine. We work closely with development teams to bring feature innovation to product roadmaps.
Specifically, we have openings for Cloud Computing, Big Data and High Performance Storage system experts who have expertise in internals of Hadoop ecosystem and No-SQL solutions and can architect new solutions. He or she will join a team of experts in researching and developing innovative solutions that utilize existing and emerging technologies to add substantial value to Samsung.
The team covers a broad spectrum of topics and is building an ecosystem surrounding SSD and Non-volatile memory. You will be working with the-state-of-art technologies in the context of vertical integration and optimization across H/W and S/W. You will participate in a project for improving current Hadoop software stacks and No-SQL database where you will have chance to understand how these systems can be enhanced by exploiting NVMe over Fabrics and Flash/SSD technologies. Our performance research will enable you to develop broad and cutting-edge expertise in cloud computing and big data infrastructure.

JOB RESPONSIBILITIES

The major role of this position is to conduct key research and development activities for next generation cloud computing/big data attached storage systems/solutions to leverage our new flash and non-volatile technology.
The engineer is required to analyze the capabilities and performance of Hadoop and database (No-SQL and SQL) software stacks in such context, identify issues, and propose and prototype new solutions across hardware and software
Create key IPs, implement prototypes, write or publish papers, and do standardization.
Collaborate with universities and international teams
Design & implement new features for innovative storage products targeted to Enterprise, Hyperscale & Cloud environments.
Design, implement and validate state-of-the art solutions that meet or exceed current and future products.

REQUIRED SKILLS

A PhD/Master’s in Computer Science, Computer Engineering, Machine Learning or Statistics, Mathematics or related field.
3-10 years of experience in Big Data/Storage or related field, debug, tuning and performance analysis.
Deep knowledge of Hadoop ecosystem (e.g., HDFS, MapReduce, Hive, Tez/Llap, Spark) and ability to modify the cores
Deep knowledge of software architecture and its internals in at least one of areas and ability to modify the cores: No-SQL database, Storage I/O subsystem, File system, NVMe/NVMeoF Drivers, etc.
Knowledge of one of more relational and no-SQL databases: MySQL, PostgreSQL, Oracle, MongoDB, Cassandra, DynamoDB, etc…
Strong knowledge and experience in analyzing large data sets
Knowledge in Machine Learning, Pattern Recognition, and Statistical Modeling
Good understanding of x86, File Systems, SSDs and general storage stacks
Good working knowledge of the Linux, Unix, VMware
Proven track record of delivering high quality software and solutions
Prior knowledge/experience in Xen/KVM/ESX/Hyper-V hypervisor and virtualization software architecture, development and debug is a plus
Strong software engineering skills with efficient, maintainable and testable programming in one or more of C/ C++/Python/Java/Scala is required.
Exposure to storage networking and systems (Fiber Channel, iSCSI, NVMe over Fabric, NAND flash, SSD, HDD etc.)
Working knowledge of data center tiers and the server system trade-offs involved is beneficial.

PREFERRED SKILLS

Must be highly motivated with excellent presentation, verbal and written communication skills.
Good knowledge of computer architecture, network and data center architecture
Ability to quickly develop proof of concept prototypes based on requirements
Good understanding of SMP/Multi-Threaded programming
Demonstrated attention to deliver SW project deliverables on schedule.
Ability to meet aggressive project deadlines in a team environment
Ability to work successfully with cross-functional teams, including coordinating across organizational boundaries and geographies.

*********************************************************************************************************************
Samsung Semiconductor Inc (SSI), an equal opportunity employer, is a world leader in Memory, System LSI, and LCD technologies. Headquartered in San Jose, California, SSI is a wholly-owned U.S. subsidiary of Samsung Electronics Co., Ltd.- the second largest semiconductor manufacturer in the world and the industry's volume and technology leader in DRAM, NAND Flash, SSDs, mobile DRAM and graphics memory. It is one of the largest providers of system logic, imaging and LED lighting solutions, as well as providing advanced process design and manufacturing for fabless companies. Samsung Semiconductor, Inc. also has a research and innovation center with numerous labs providing product design and research in: logic, memory, image sensors, displays and mobile technologies. In addition, the company supports Samsung Display Company, the largest producer of LCD and OLED displays.
*********************************************************************************************************************

A day in the life Samsung Video: http://bit.ly/1saHOGu
Click here to visit our Samsung Semiconductor Career Page"
60,Data Engineer,"Sunnyvale, CA",Sunnyvale,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"**Please Read**

Local candidates only. This opportunity does not provide Visa sponsorship. No corp to corp applicants please. Candidate must be available to work on our W2.

Data Engineer

Data applications are critical to our success, powering many aspects of our marketplace and supporting products. We are looking for data engineers who will build, migrate and maintain data pipelines. In this role, you’ll expand and refactor the data sets that generate and transform data into applications, insights, and experiences for our users.

The work includes:
? Refactoring existing and build new data pipelines
? Migrating existing data sets into next-gen reporting frameworks and tools
? Using existing data tools and frameworks to configure reports and metrics
? Developing and automating large scale, high-performance data processing systems to drive our business growth and improve the product experience
? Building and refactoring scalable data pipelines on top of Hive and Spark leveraging Airflow scheduler/executor framework

We are looking for engineers with:
? Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and drive effective product solutions
? Experience designing and deploying high performance systems with robust monitoring and logging practices
? Experience building high performance data pipelines
? Nice to have: proven ability to think critically about team direction and use analysis to inform that
? Experience using machine learning is a plus, but not required.
? Excellent communication skills, both written and verbal"
61,New Graduate - IT Data Analytics - Data Engineer,"Palo Alto, CA 94304",Palo Alto,CA,94304,None Found,None Found,"
Currently pursuing a Bachelors or Masters degree in Computer Science, Computer Engineering, Data Analytics or related field
Excellent knowledge of Data Architecture and BI concepts
Expertise in SQL and Python
Working knowledge of Hadoop, SAP HANA
Working knowledge of Tableau and SAP BOBJ is a plus
Familiarity with Machine Learning models.","
Create data pipelines using SQL, Python, Informatica, SDI
Work with structured and unstructured data and loading into Hadoop and SAP HANA
Create the BI & Analytical reports using SAP BOBJ & Tableau based on business needs
Create Machine learning models for predictive analytics.",None Found,None Found,"98% of Fortune 500 Companies use VMware Technology!
The most advanced companies in the world turn to VMware to manage, grow and transform their business. When you work here, you’re connected to a global community of innovative, empowered employees working together to solve the most critical technology challenges.
We believe that creativity sparks innovation and inspires our employees to think differently and challenge the status quo. Whether it’s the kind of products we develop, our approach to sustainability, or how we give back to our communities, VMware finds unique ways to bring people together to fuel creative thinking. Want to know more, check out our website https://careers.vmware.com/
The IT Data & Analytics team is responsible for building the Data pipelines using different corporate data assets as source, working with both structured and unstructured data and transforming the data based on the business needs and maintaining the data in the Enterprise Datawarehouse with the architecture & Infrastructure that supports Near Real time reporting, Big Data and Machine Learning. The team supports Data, reporting and analytical needs of all the business groups across the company and is spread across Palo Alto, Costa Rica & India.
As a New Graduate, you will have the opportunity to work with Structured & unstructured data, building data pipelines, working with SQL, Python and creating the Machine learning modes and building the BI reports.
Responsibilities:
As a New Graduate at VMware, you’ll create innovative solutions and solve complex problems. You’ll take ownership of meaningful, big-picture work and springboard an impactful career. Become immersed in all aspects of our innovative and collaborative culture, and ensure you get the full VMware experience. You’ll interact with industry thought leaders at one of our world class campuses and enjoy networking, community service, and career development events. Some of your responsibilities will include:
Create data pipelines using SQL, Python, Informatica, SDI
Work with structured and unstructured data and loading into Hadoop and SAP HANA
Create the BI & Analytical reports using SAP BOBJ & Tableau based on business needs
Create Machine learning models for predictive analytics.
Required Skills:
Currently pursuing a Bachelors or Masters degree in Computer Science, Computer Engineering, Data Analytics or related field
Excellent knowledge of Data Architecture and BI concepts
Expertise in SQL and Python
Working knowledge of Hadoop, SAP HANA
Working knowledge of Tableau and SAP BOBJ is a plus
Familiarity with Machine Learning models.
Preferred Skills:
Working knowledge of SQL and Python
Working knowledge of Big Data Infrastructure
Familiarity with Machine learning models
Good Analytical and trouble shooting skills
This job opportunity is not eligible for employment-based immigration sponsorship by VMware.

VMware Company Overview: VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com.

Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law."
62,Data Engineer - Retail Business Intelligence,"Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Oct 7, 2019
Weekly Hours: 40
Role Number: 200102503
At Apple, extraordinary ideas have a way of becoming phenomenal products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a dynamic environment where your technical abilities will be challenged on a day-to-day basis? If so, Apple's Retail Business Intelligence team is seeking an expert Data Engineer to support marketing campaign automation and data analysis for Retail.
In this role you will interface with both business and IT partners, using their passion to expose the operational value of data to drive value in both domains.
This role requires you to architect easy-to-use data objects, create email campaigns, and build reporting assets that end users can utilize for data driven decisions. Are you ready to tackle on an exciting new challenge?
Key Qualifications
7+ years of data extraction and front end report building (PHP or similar)
Proficient with data access/preparation methods using Teradata SQL and relational databases
Proficient with scripting/glue languages like PHP or Python and web-technologies like HTML, CSS, and Javascript
Strong interpersonal skills, both verbal and written
Ability to work on multiple assignments with excellent attention to detail
Flexibility to handle directional changes and ability to support multiple deadline-specific projects while maintaining day-to-day business support
Driven, Self-motivated individual who is experienced working in a global, matrixed, fast-paced environment
Ability to comprehensively understand data elements, sources and relationships
Ability to establish and manage relationships in a cross-functional team environment
Unix programming/scripting experience a plus
Description
Define how data is structured, organized and interpreted for consistent executive reporting.
Define and automate campaigns using tools like UNICA
Develop and implement reporting for data quality, data capture rates and monitor data quality
Partner with database developers to have designs for database objects implemented and validated
Perform ad-hoc analytics to support incoming requests from business partners
Engage with IT and project teams to ensure reporting requirements are covered
Define, build and deploy impactful metrics
Develop and implement web reports and analytical tools based on business requirements
Provide consultation and training to users of analytical tools
Additional responsibilities will include extracting, cleaning and manipulating data from multiple systems fo research & advanced analytics.
Education & Experience
BS required, advanced degree in Statistics and/or Computer Science a plus
Additional Requirements
Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants."
63,Data Engineer,"Palo Alto, CA 94301",Palo Alto,CA,94301,None Found,None Found,None Found,None Found,None Found,None Found,"Nyansa is a fast-growing innovator of advanced IT infrastructure analytics software based in Palo Alto, California. Founded in September 2013 by technology professionals from MIT, Meraki, Aruba Networks and Google, Nyansa is credited with developing the first cloud sourced, vendor-agnostic network analytics and IoT security platform, called Voyance.


We embrace simplicity and take following to heart on everything we do:
""Any intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius - and a lot of courage - to move in the opposite direction."" - Einstein

Nyansa is looking for a data engineer to join the team that is building a new, vendor-agnostic IT network analytics service purpose built for CIOs, network operations and helpdesk personnel managing heterogeneous enterprise environments. Our product is focused on the end user experience by helping IT staff gain new insights into client access conditions, network service behavior and enterprise applications issues that impact user performance.

Our current big data analytics system analyzes billions of streaming events per day using advanced algorithms. Going forward, we aim to scale the system extensively and are looking for radical ideas to achieve this.
The company is well funded and provides competitive compensation package, stock options, benefits, catered lunch, and a fun work environment.

We’re located within a 1 min walk from the Palo Alto Caltrain station.

Responsibilities:Design and develop highly scalable and available real time analytics platform using Spark, Kafka, Cassandra, and Elasticsearch for large data input streamsWork closely with data science and UI teams to define and implement various analytics features related to productConfigure, monitor, and optimize Spark and related infrastructure

Requirements:Strong desire to work for an early stage startup and be a part of its successStrong in Map-Reduce, parallelizing computations, and identifying bottleneck computationsStrong in Scala and PythonExperience in configuring and tuning Spark and Kafka systemsGood understanding on Spark UI to extract useful information on application stages, and identify bottlenecksB.S. or higher degree in Computer Science or equivalent

Pluses:Experience with Cassandra, Elasticsearch, MongoExperience with Ganglia and able to correlate information from various UIs to diagnose efficiency issuesExperience working with AWSExperience with Spray to build REST endpoints"
64,"Senior Data Engineer, AI Infrastructure","Santa Clara, CA 95050",Santa Clara,CA,95050,None Found,None Found,None Found,None Found,None Found,None Found,"We are now looking for a Senior Data Engineer - AI Data Platform
NVIDIA is hiring senior distributed systems and data engineers to scale up its AI infrastructure and deep learning platforms! You will need to have strong programming skills and a deep understanding of data science technologies. You should have production grade experience working with heterogeneous data types at scale, cloud technologies, distributed storage & compute systems, and distributed services architecture. You will require excellent communication and planning skills. Together, we will help advance NVIDIA's capacity to build and deploy leading solutions for a broad range of AI based applications such as autonomous vehicles, healthcare, virtual reality, graphics engines and visual computing.
What you'll be doing:
Orchestrate large PB sized data storage and compute clusters across bare metal and cloud that support distributed AI infrastructure.
Design and program scalable data lake interfaces, microservices, and web technologies that support ingesting and querying structured data.
Architect and program high performance compute and data pipelines that support efficient queries. Enabling efficient data selection is a key ingredient to successful machine learning!
Build and implement support for versioned, traceable, and immutable datasets in a data lake in a distributed and scalable manner.
Stand up distributed systems for mining and analyzing data to help AI leaders and researchers make data driven decisions for data collection, diversity, training, and evaluation.
We will spend a majority of the time writing and peer reviewing high performance, high quality, and well tested and well architected code.
What we need to see:
You have a BS or MS in Computer Architecture, Computer Science, Electrical Engineering or a related data intensive Engineering Degree with 7+ years of relevant experience in a programming intensive role.
Strong programming background that incorporates methodologies like data structures, design patterns, OOP, and test driven development.
A technical authority with strong experience in traditional big data technologies, databases, analytics, and common architectures.
Built and orchestrated massive business critical data clusters, infrastructure, services, and ETL pipelines in a cloud environment.
Proven experience in collaborating with multiple teams to collect and process large amounts of data, building microservices, and RESTful APIs.
An expert programmer in Go, C/C++, Scala, and SQL.
Advanced expertise in MapReduce, Hadoop, Hive, Presto, Spark.
Highly motivated with strong interpersonal skills, you have the ability to work successfully with multi-functional teams, principles and architects and coordinate effectively across organizational boundaries and geographies.
Ways to stand out from the crowd:
Experience with structured data such as Avro, Parquet, Protobuf, Thrift, and concepts like schema evolution.
Experience with full-stack web based visualization technologies to help provide data insights.
Strong understanding of Docker and orchestration systems such as Kubernetes.
Do you have a go getter attitude to dive deeper and understand technical requirements?
NVIDIA is widely considered to be one of the technology industry's most desirable employers. We have some of the most brilliant and talented people in the world working with us and our engineering teams are growing fast in some of the hottest state of the art fields: Deep Learning, Artificial Intelligence, and Autonomous Vehicles. If you're a creative and autonomous computer scientist with a real passion for distributed systems and parallel computing, we want to hear from you.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression , sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.
#deeplearning"
65,Senior Cloud Data Engineer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,"
Design and develop framework to automate data ingestion and integration of structured data from a wide variety of enterprise data sources, at scale.
Design and develop data pipeline components and integrate them with the Splunk and other ETL Platforms.
Design data quality monitoring and automated data cleaning.
Assist the business liaison and ETL function with data related issues such as assessing data quality, data consolidation, evaluating existing data sources, etc.
Experience with handling large data infrastructure platform and driving stability through automated monitoring, alerting, and actions.
Experience developing for, configuring, and supporting Cloud computing solutions","
Bachelor degree in Computer Science or related field","
Experience with building scalable and reliable data pipelines using Data engine technologies like APIs, AWS Redshift, Snowflake, Talend.
8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.
4+ years experience designing and developing complex ETL/ELT programs with Python, Visual ETL Tools etc
3+ years experience developing complex SQL
Experience using Cloud Storage and computing technologies such as RedShift, Snowflake
3+ years experience programming in Python
2+ years experience with Bitbucket
2+ years experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues
2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)
Experience with API based integration from multiple SaaS data sources
Experience developing and implementing streaming data ingestion solutions
Experience in Agile methodology (2+ years)","Job Description: That’s a cool job - I want it!
Ready to shake things up? Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and strive to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun, and most significantly to each other’s success. We continue to be on a tear while enjoying incredible growth year over year.
As a Cloud Data Engineer, you should be an expert with data warehousing technical components (e.g., ETL, ELT, Cloud Databases and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data lake solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions.
Responsibilities: I want to and can do that!
Design and develop framework to automate data ingestion and integration of structured data from a wide variety of enterprise data sources, at scale.
Design and develop data pipeline components and integrate them with the Splunk and other ETL Platforms.
Design data quality monitoring and automated data cleaning.
Assist the business liaison and ETL function with data related issues such as assessing data quality, data consolidation, evaluating existing data sources, etc.
Experience with handling large data infrastructure platform and driving stability through automated monitoring, alerting, and actions.
Experience developing for, configuring, and supporting Cloud computing solutions
Requirements: I’ve already done that or have that!
Experience with building scalable and reliable data pipelines using Data engine technologies like APIs, AWS Redshift, Snowflake, Talend.
8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.
4+ years experience designing and developing complex ETL/ELT programs with Python, Visual ETL Tools etc
3+ years experience developing complex SQL
Experience using Cloud Storage and computing technologies such as RedShift, Snowflake
3+ years experience programming in Python
2+ years experience with Bitbucket
2+ years experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues
2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)
Experience with API based integration from multiple SaaS data sources
Experience developing and implementing streaming data ingestion solutions
Experience in Agile methodology (2+ years)
Education: Got it!
Bachelor degree in Computer Science or related field
We value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which you are applying.
For job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records."
66,Manager,"Fremont, CA",Fremont,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Role : Tableau + Sales Analytics
JD : 65533 Dashboards: Build new dashboards and make adjustments to existing dashboards, based on requirements received from stakeholders and Sales Ops Analysts on the team.

65533 Data Models: Deliver requirements to Business Technology team to incorporate new fields, objects, and business views into the enterprise data warehouse. Document logic used to build all business views for future reference. Analyze any upcoming systems changes, determine impact on the data warehouse and dashboards, and manage that transition seamlessly.
65533 Data Analysis: Perform data analysis and/or gap analysis to ensure it can support Dashboard solutions
65533 Data Inputs: Own and maintain nonsystems data inputs, such as Excel files and Google Sheets. Optimize and automate updates where possible.
65533 Troubleshoot: Be the first stop for troubleshooting and resolving access or data quality issues reported by users. Help Sales Ops Analysts troubleshoot issues in dashboards they are building.
65533 Optimization: Optimize performance (load time) of dashboards using best practices.

Qualifications:
65533 Bachelor65533s degree in Information Systems, Computer Science, or related field of study.
65533 5+ years professional experience as a Business Intelligence Analyst, Business Intelligence Engineer, Data Engineer, or related roles.
65533 Extensive experience working in Tableau Desktop and publishing to Tableau Server required.
65533 Strong experience in SQL, data modeling, and data preparation required. Experience with Alteryx preferred.
65533 Experience with Sales Operations analytics preferred. Experience with Salesforce strongly preferred.
65533 Excellent problem solving, analytical, and project management skills.
65533 Strong interpersonal skills.
65533 Takes initiative and tackles challenges with enthusiasm.
65533 Ability to multitask and adapt quickly in a changing environment.


Primary Location: US-CA-Fremont
Schedule: Full Time
Job Type: Experienced
Travel: No
Job Posting: 04/10/2019, 1:14:06 PM"
67,Data Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,"Use data engineering expertise to design and build solutions/products for analyzing data collection from various data sources.
Create data tools for analytics and technical team members to assist them in building and optimizing the product.
Conduct advanced statistical analysis to determine trends and significant data relationships.
Work with data and machine learning experts to strive for greater functionality in our data and model life cycle management systems.
Other duties as assigned
",None Found,"Experience with relational SQL and NoSQL databases required
BS degree in Computer Science, Informatics, Information Systems or another related field and a minimum of 2 years’ experience in AWS, Python required
Years of experience can be substitute for the education","About Nevro
Nevro (NYSE: NVRO) is a public multinational medical technology company headquartered in Redwood City, California. We have developed HF10™ therapy, an innovative, evidence-based neuromodulation platform. We started with a simple mission to help more patients suffering from chronic pain. At each stage of development, our research was subject to the highest levels of scientific rigor, resulting in a new therapy that has impacted the lives of over 45,000 patients around the world. The Nevro® Senza® SCS System received CE mark in 2010, TGA approval in 2011, FDA approval in 2015, and is commercially available in Europe, Australia, and the United States.
Job Summary & Responsibilities
The Data Engineer shall be a foundational member in research and product development team to collaborate on the design and implementation of improving and automating clinical data to analyze, identify and report data and trends.
Use data engineering expertise to design and build solutions/products for analyzing data collection from various data sources.
Create data tools for analytics and technical team members to assist them in building and optimizing the product.
Conduct advanced statistical analysis to determine trends and significant data relationships.
Work with data and machine learning experts to strive for greater functionality in our data and model life cycle management systems.
Other duties as assigned
Role Requirements
Experience with relational SQL and NoSQL databases required
BS degree in Computer Science, Informatics, Information Systems or another related field and a minimum of 2 years’ experience in AWS, Python required
Years of experience can be substitute for the education
Skills and Knowledge
Proficient in data visualization skills with technologies such as PowerBI, Tableau
Experience with Data and Model pipeline and workflow management tools
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Have built processes supporting data transformation, data structures, metadata, dependency and workload management.
Nice to have skill would include: experience with RESTful APIs and cross-platform development
Attention to detail and organization/ documentation skills
Able to work effectively under pressure, independently, and within a collaborative team-oriented environment using sound judgment in decision making
Strong analytic skills related to working with unstructured datasets.
Project management and interpersonal skills.
Experience supporting and working with cross-functional teams in a dynamic environment
LI-TS1"
68,Data Engineer Intern,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"At Hewlett Packard Enterprise (HPE), we live by three core values that drive our business: Partner. Innovate. Act. These values combine to help us create important work all over the world to advance how people live and work.
Learning does not only happen through training. Relationships are among the most powerful ways for people to learn and grow, and this is part of our HPE culture. In addition to working alongside talented colleagues, you will have many opportunities to learn through coaching and stretch assignment opportunities. You’ll be guided by feedback and support to accelerate your learning and maximize your knowledge. We also have a “reverse mentoring” program which allows us to share our knowledge and strengths across our multi-generation workforce.
Hewlett Packard Enterprise is seeking a highly motivated Data Scientist Intern to work in the San Jose, CA office.
Responsibilities:
Identify, analyze and interpret trends or pattern in complex data sets, including telemetry from storage arrays and other IT infrastructure components
Use advanced machine learning techniques to predict or alert in case of failures, and recommend actions that can mitigate or resolve them.
Automate data collection, pre-processing and/or analysis
Communicate findings clearly and succinctly to technical and non-technical audience
Education/Experience:
Currently pursuing a Masters degree or working towards completion of PhD program in Computer Science or Computer Engineering.
R or scala programming, SQL experience
We offer:Competitive salary and extensive social benefitsDiverse and dynamic work environmentWork-life balance and support for career developmentWant to know more about HPE? Then let’s stay connected!
https://www.facebook.com/HPECareers
https://twitter.com/HPE_Careers
HPE is an EOE / Female / Minority / Individual with Disabilities / Protected Veteran Status
1053049"
69,Big Data Engineer - Bachelor's (Full Time) - United States,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"What You’ll Do
Design and deliver automated transformation of large data sets leveraging MapReduce, streaming, and other emerging technologies
Leverage HBase, Elasticsearch, etc. to ingest transformed data at scale
Collaborate with security experts to deliver high-impact web-based APIs
Implement high-volume data integration solutions
Analyze, monitor, and optimize for performance
Produce and maintain high-quality technical documentation

Who You'll Work With
Join us as we transform the world of tomorrow. Develop creative ideas on how to work better and smarter. Influence and participate in top-priority projects that have a real impact.

Who You Are
Recent graduate or on your final year of studies toward a Bachelor's degree in Computer Science or a related technical field
Minimum of a 3.0 GPA or equivalent
Track record of developing technology to enable large scale data transformation
Strong Java experience and hands-on Hadoop ecosystem experience – HBase, Hive, Spark, etc.
Possess knowledge of software engineering best practices
Passion for solving hard problems and exploring new technologies
Excellent communication and technical documentation skills


Why Cisco

#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!"
70,Sr. Data Engineer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,"Experience working with at least one other big data platform like Apache Spark, Redshift, Teradata or SAP HANA.
Familiarity with streaming platforms like Apache Kafka, Amazon Kinesis etc.
Knowledge of Data Science, Machine Learning and Statistical Models is desirable.
Familiarity with Microsoft SSAS and Cubes and advanced Excel skills.
",None Found,None Found,None Found,"The Creative Cloud Engagement Analytics team is looking for a passionate Data Engineer to join our growing team of analytics experts. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Tapping into our massive product usage data sets, you will architect, build and optimize analytics platform and pipelines to harness our data, derive actionable insights to help guide the business with data. One must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing the data architecture to support our next generation of products and data initiatives.
What you’ll do
Architect, build and maintain scalable automated data pipelines ground up. Be an expert of stitching and calibrating data across various data sources.
Work with Adobe’s data ingestion, data platform and product teams to understand and validate instrumentation and data flow.
Develop data set processes for data modeling, mining and production.
Integrate new data management technologies and software engineering tools into existing structures
Support regular ad-hoc data querying and analysis to better understand customer behaviors.
Understand, monitor, QA, translate, collaborate with business teams to ensure ongoing data quality.
What you need to succeed
Bachelor’s degree in Computer Science, Information Systems or a related field is required, master’s preferred.
8+ years of experience building and maintaining big data pipelines and/or analytical or reporting systems at scale.
Expert level skills working with Apache Hadoop and related technology stack like Pig, Hive, Oozie etc.
A strong proficiency in querying, manipulating and analyzing large data sets using SQL and/or SQL-like languages.
Approaching data organization challenges with a clear eye on what is important; employing the right approach/methods to make the maximum use of time and human resources.
Good attention to detail and ability to stitch and QA multiple data sources. Exploring new territories and finding creative and unusual ways to solve data management problems.
Be a self-starter.
Good interpersonal skills.
Preferred (but not required) Skills:
Experience working with at least one other big data platform like Apache Spark, Redshift, Teradata or SAP HANA.
Familiarity with streaming platforms like Apache Kafka, Amazon Kinesis etc.
Knowledge of Data Science, Machine Learning and Statistical Models is desirable.
Familiarity with Microsoft SSAS and Cubes and advanced Excel skills.
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status."
71,Senior Data Engineer,"San Mateo, CA 94401",San Mateo,CA,94401,None Found,None Found,None Found,None Found,None Found,"
A BS or MS degree in Computer Science or Computer Engineering, or equivalent experience, with a strong record of academic achievement
5+ years experience with data engineering in Java and / or Scala
Experience with Spark and other big data technologies, large-scale data processing and ETL pipelines
Experience with SQL and relational databases, such as Postgres
Experience with developing and deploying applications on Linux infrastructure in AWS
Enthusiasm about working in a small, entrepreneurial organization","Motif Capital is modernizing fundamental investment research. We combine traditional and non-traditional data sources, containing structured and unstructured data, deriving quantitative insights to power our clients' long-term investments. We are a small team, dedicated to scaling through automation in order to keep our clients' money invested in their portfolios, not their advisor.
As a Senior Data Engineer, you will be a key driver of our data and analytics platform. Not only will you develop and operate our data infrastructure, you will work closely with analysts to gain understanding of the underlying data and structure it for ease of analysis and use. You will search out new and interesting data sources, which when combined with our existing, proprietary research will generate unique investment insights. This data will feed our analytics and machine learning pipelines, to continuously update and adapt our portfolios to changing micro- and macro-economic conditions.
Job Requirements:
A BS or MS degree in Computer Science or Computer Engineering, or equivalent experience, with a strong record of academic achievement
5+ years experience with data engineering in Java and / or Scala
Experience with Spark and other big data technologies, large-scale data processing and ETL pipelines
Experience with SQL and relational databases, such as Postgres
Experience with developing and deploying applications on Linux infrastructure in AWS
Enthusiasm about working in a small, entrepreneurial organization
Bonus points:
Experience with financial datasets, knowledge of markets and investments
Firm grasp of applied statistical concepts and techniques including probability, regression, and time series analysis
Experience with Numerical / Scientific Python (NumPy, SciPy, Pandas)
Knowledge of machine learning pipelines using Scala / Spark or Python / scikit-learn
About Motif Capital:
Motif Capital Management is a next-generation global equity investment manager that specializes in the management of thematic investment strategies for financial institutions such as private wealth management, investment companies, endowments, and family offices. Our unique disciplined, scientific, and transparent approach to thematic investing relies on combining data-driven insights with objective fundamental research, algorithmic portfolio design and cutting-edge technology & analytics. Our goal is to work with our institutional partners to act on the economic, socio-political, and technological forces that are shaping the global economy for the benefit of their clients’ portfolios. Learn more at www.motifcapital.com.
Motif Capital Management, Inc. is an SEC registered investment advisory firm located in San Mateo, California. The company is privately held and a wholly owned subsidiary of Motif Investing Inc.

About Motif Investing:
Motif Investing is an online broker-dealer that is transforming the way retail investors invest and manage wealth. Motif Investing offers self-directed investors a concept-driven trading platform that enables them to trade “motifs”—intelligently-weighted basket of stocks and ETFs built around themes, investing styles or multi-asset models— for low fees. Based in San Mateo, the company's investors include Goldman Sachs, JPMorgan Chase, Balderton Capital, Renren, Foundation Capital, Ignition Partners, Norwest Venture Partners, and Wicklow Capital, with notable board members including former SEC Chairman Arthur Levitt and former Boston Consulting Group CEO Carl Stern. Learn more at www.motifinvesting.com."
72,Big Data Engineer,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,"7+ years of experience with object oriented programming language such as Java, and scripting languages such as Python
5+ years of experience with big data and distributed NoSql technologies such as Spark, Hbase or Map-reduce
3+ years of experience designing and implementing large, scalable distributed systems
3+ years of production experience designing and implementing mission critical and metrics-driven ETL’s
",None Found,None Found,None Found,"Squadex is a fast growing technology consulting and engineering company headquartered in Silicon Valley with the large R&D center in Ukraine. We offer DevOps & Data Science expertise to emerging startups and well-established enterprises at the US market.

The Role
We are looking for a seasoned data/algorithm engineer who has experience building and maintaining large scale ETL’s and data warehousing to build the next generation mission critical data platform. If you are comfortable with swimming in Petabytes of data to define quality metrics, explore trending and to write scalable code for data pipelines, come talk to us.

In this role, you will
Responsible for designing, implementing, and maintaining LeadGenius’s big data pipeline infrastructure — including data ingestions, stream processing, data warehousing, data pipelines, visualization, quality metrics and exposing data to applications
Data modeling and metadata management
Ensure scalable, highly available, and robust big data platform architecture to meet service level agreements
End-to-end data processing, troubleshooting, problem diagnosis, performance benchmark, load balance, and code reviews
Your KPIs would be
Building large-scale infrastructural data systems using open-source technologies
Designing high-performance RESTful web services to expose data from SQL and NoSql
Cross team collaboration and effective communication
Skills & Experience
7+ years of experience with object oriented programming language such as Java, and scripting languages such as Python
5+ years of experience with big data and distributed NoSql technologies such as Spark, Hbase or Map-reduce
3+ years of experience designing and implementing large, scalable distributed systems
3+ years of production experience designing and implementing mission critical and metrics-driven ETL’s
Good to have
Experience with AKKA-based webservice is a plus
Strong in algorithms and CS fundamentals."
73,Cocoa Framework SW Engineer,"Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Aug 20, 2019
Weekly Hours: 40
Role Number: 200012970
The Cocoa Frameworks engineering group is seeking creative and highly motivated engineer to join the Core Data team and build the next generation of iOS, macOS, watchOS and tvOS technologies. Our team works on a broad range of projects that focus on improving developer experiences for all of Apple's platforms including CloudKit and SwiftUI enhancements.
Key Qualifications
Development experience with Objective-C or Swift.
Strong engineering and debugging skills and a deep understanding of OS fundamentals
Excellent problem solving and critical thinking skills.
Ability to work in a diverse group, including interpersonal skills.
Ideal candidates will have strong knowledge of Apple development tools and framework design as well as systems programming experience
Description
The Core Data team specializes in designing, building, and maintaining application development and data storage functionality used by all of Apple's operating systems. We work in C, Objective-C, and Swift. In this position you would have the opportunity to work with other engineering teams including CloudKit, Swift, and SwiftUI. As a Core Data engineer, you will have the opportunity to work at the intersection of computer science theory and practical everyday engineering. The code that you write will become an important building block for a wide variety of features and a critical part of shipping software on a regular schedule. We are trusted to deliver high quality results on difficult tasks. You will collaborate with many other teams to deliver broadly scoped features, quality improvements and performance enhancements. You will craft new APIs. You will diagnose and fix complex bugs, and provide advice to clients about application performance, data management and storage technologies. We encourage curiosity and initiative, value diversity, and depend on the ability to learn and grow when approaching new features and technology.
Education & Experience
BS or M.S. in Computer Science or equivalent industry experience"
74,Data Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,"
4-6 years of hands-on database skills with such technologies as PostgresSQL/RedShift, Glue, or equivalent
3+ years experience with Spark/pySpark, Pandas, numpy, pysql
4-6 years coding in Python with solid understanding of Python best practices (PEP8)
2+ years experience with cloud services like AWS/GCP
Comfortable supporting analytics tools such as Tableau, PowerBI, etc
Excellent detail-oriented, problem solving skills and the ability to quickly learn and apply new concepts, principles and solutions
Must have excellent communication skills (verbal and written)
Ability to balance competing priorities in a very dynamic and fast-paced environment
",None Found,None Found,None Found,"About the Job
As a rapidly growing start-up, we are implementing the tools and processes we use to run the company and our IoT product base. As a Data Engineer, you will roll up your sleeves to integrate and expand the core systems that run our platform.

About You
Candidates need to have 5+ years of hands-on experience, with proven results in designing, building, integrating and rolling out impactful data infrastructure in support of a growing company.

What You’ll Do

Design and support the application integration between various applications and systems.
Provide analytical rigor to thoroughly analyze business requirements and translate the results into good technical designs.
Design and develop systems automation with future expansion in mind.
Establish the documentation of integration, develops, and maintains technical specification documentation for all processes.

Required Skills and Experience:

4-6 years of hands-on database skills with such technologies as PostgresSQL/RedShift, Glue, or equivalent
3+ years experience with Spark/pySpark, Pandas, numpy, pysql
4-6 years coding in Python with solid understanding of Python best practices (PEP8)
2+ years experience with cloud services like AWS/GCP
Comfortable supporting analytics tools such as Tableau, PowerBI, etc
Excellent detail-oriented, problem solving skills and the ability to quickly learn and apply new concepts, principles and solutions
Must have excellent communication skills (verbal and written)
Ability to balance competing priorities in a very dynamic and fast-paced environment

Preferred Skills:

Experience with GIS services and datasets
Implementation and integration of ticketing and workflow systems like ZenDesk, Jira or PagerDuty.
Implementation and integration of CRM systems like Salesforce, HubSpot, or Copper
Experience with data aggregation services such as Snowflake, Segment, or equivalent
Experience with developing user authentication and authorization solutions and knowledge of security compliance is a plus
Knowledge in Transportation or Logistics domain.

"
75,Data Engineer - ETL/Data Pipeline,"Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"ABOUT US
Lark is the world's largest A.I. healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with A.I. Nurses. We're on a mission to improve people's health and happiness through our digital health coach. We are the only A.I. nurse ever to become fully medically reimbursed to 100% replace a live nurse because we achieved equivalent health outcomes to live healthcare professionals - which allows for infinitely scalable healthcare. Since launch, Lark has continued to receive awards and accolades for both our product, and our leadership.

✦Apple's Top 10 Apps in the World
✧Business Insider's most innovative companies in the world along with Uber and Snapchat
✦Biz Journal's 100 Women of Influence

We are looking for a talented Data Engineer to join our growing team in Mountain View, CA, where you'll be building our next generation data pipelines.

ABOUT THE ROLE

What You'll Do:

Build our next generation data pipelines into a fast and efficient big-data system
You'll be the first fully dedicated data engineer on the team, and will be able to call the shots on strategy

What You'll Need:

A love of data, and the make-or-break effect it has on startups
Default to coding efficient systems from large databases, both RDBMS and noSQL.
Familiarity with the following key technologies (or similar):
Spark
Yarn
Kafka
Python
AWS

JOIN US
Our team works with cutting edge tools and technology related to Artificial Intelligence and Machine Learning. We are using NLP to process millions of meals, and accelerometer data to compute activity and sleep amounts from users' phones. Our chat AI is the most sophisticated digital health engagement tool in the world. Join us and make it even better!

Lark is an Equal Opportunity Employer. Lark does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need."
76,Cloud Data Engineer,"Santa Clara, CA",Santa Clara,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Our Mission


At Palo Alto Networks® everything starts and ends with our mission:
Being the cybersecurity partner of choice, protecting our digital way of life.
We have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish – but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
Your Career
We are the fastest growing company in one of the fastest moving industries, it’s absolutely critical that all of our businesses and product units have all the information needed to make the fastest and smartest decisions. Our Big Data Analytics team is constantly growing and stretching as they rise to the challenges created by our meteoric growth and adding new technologies that make sense is a daily occurrence. We have recently introduced GCP into our Big Data Analytics environment and we are looking for a ‘groundbreaking’ senior level Big Data Engineer to join our team and help drive these efforts.

Your Impact
Integral team member of our AI and Analytics team responsible for design and development of Big data solutions
Partner with domain experts, product managers, analyst, and data scientists to develop Big Data pipelines in Hadoop or Google Cloud Platform
Responsible for delivering data as a service framework from Google Cloud Platform
Responsible for moving all legacy workloads to cloud platform
Work with data scientist to build ML pipelines using heterogeneous sources and provide engineering services for data science applications
Ensure automation through CI/CD across platforms both in cloud and on-premises
Ability to research and assess open source technologies and components to recommend and integrate into the design and implementation
Be the technical expert and mentor other team members on Big Data and Cloud Tech stacks
Your Experience
5+ years of experience with Hadoop or Cloud Technologies
Expert level building pipelines using Apache Beam or Spark
Familiarity with core provider services from AWS, Azure or GCP, preferably having supported deployments on one or more of these platforms
Experience with all aspects of DevOps (source control, continuous integration, deployments, etc.)
You have experience with containerization and related technologies (e.g. Docker, Kubernetes)
Experience in other open-sources like Druid, Elastic Search, Logstash etc is a plus
Advanced knowledge of the Hadoop ecosystem and Big Data technologies
Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Impala, Spark, Kafka, Kudu, Solr)
Knowledge of agile(scrum) development methodology is a plus
Strong development/automation skills
Competent reading and writing Scala, Python or Java code.
System level understanding - Data structures, algorithms, distributed storage & compute
Can-do attitude on solving complex business problems, good interpersonal and teamwork skills
Degree in Bachelor of Science in Computer Science or equivalent
The Team

Working at a high-tech cybersecurity company within Information Technology is a once in a lifetime opportunity. You’ll be joined with the brightest minds in technology, creating, building, and supporting tools and that enable our global teams on the front line of defense against cyberattacks. We’re joined by one mission – but driven by the impact of that mission and what it means to protect our way of life in the digital age. Join a dynamic and fast-paced team that feels excitement at the prospect of a challenge and feels a thrill at resolving technical gaps that inhibit productivity.

Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together. To learn more about our dedication to inclusion and innovation, visit our Life at Palo Alto Networks page and our diversity website.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.
Additionally, we are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or an accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.

 #LI-AH1"
77,Data engineer,"Fremont, CA",Fremont,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Qualification:
Master's Degree in Statistical Analytics, Data Science, or Bachelor's Degree in computer science engineering will be considered with at least three - five years of applicable work experience
Preferred Proficiency:

Web development experience (AngularJS, D3).
Experience in a statistical programming language like R or Python; applied machine learning techniques including dimensionality reduction strategies, supervised/unsupervised classification and natural language processing frameworks.
Experience in at least one data visualization tools (e.g. Tableau, QlikView) and data warehousing tools (e.g. Informatica) is preferred
Huge Advantage:
Building and scaling Machine Learning frameworks
Hadoop (Hive, Spark, UDF's)
Definite Plus:

Web development experience (AngularJS, D3).
Experience:
5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.
5+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.
2+ years of experience in scripting languages like Python etc."
78,Data Engineer - SQL / Big Data / Cloud,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,"
BS or higher degree in Computer Science.
Solid understanding of SQL and good grasp of relational and analytical database management theory and practice.
Knowledge in Java software development, especially in the database field.
Good knowledge of JDBC, XML and Web Services APIs.
Excellent verbal and written communication skills to be able to interact with technical and business counterparts.
Active listener.
Strong analytical and problem solving abilities.
Lots of curiosity. You never stop learning new things.
Creativity. We love to be surprised with innovative solutions.
Willingness to travel around 50%.
Be a team worker with positive attitude.","
Obtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical pitch, including overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning.
Constantly learn new things and maintain an overview of modern technologies.
Be able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature / functionality of our product.
Capable of building and/or leading the development of custom deployments based and beyond client’s requirements.
Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues.
Train and engage clients in the product architecture, configuration, and use of the Denodo Platform.
Promote knowledge and best practices while managing deliverables and client expectations.
Manage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues.
Provide technical consulting, training and support.
Develop white papers, presentations, training materials or documentation on related topics.",None Found,None Found,"Job Description

Your Opportunity
Denodo is always looking for technical, passionate people to join our Services Engineering team. We want a professional who will travel, consult, develop, train and troubleshoot to enhance our clients’ journey around Data Virtualization.

Your mission: to help people realize their full potential through accelerated adoption and productive use of Denodo solutions.

In this role you will successfully employ a combination of high technical expertise and client management skills to conduct on-site and off-site consulting, product implementation and solutions development in either short or long-term engagements being critical point of contact for getting things done among Denodo, partners and client teams.

Duties & Responsibilities

Obtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical pitch, including overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning.
Constantly learn new things and maintain an overview of modern technologies.
Be able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature / functionality of our product.
Capable of building and/or leading the development of custom deployments based and beyond client’s requirements.
Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues.
Train and engage clients in the product architecture, configuration, and use of the Denodo Platform.
Promote knowledge and best practices while managing deliverables and client expectations.
Manage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues.
Provide technical consulting, training and support.
Develop white papers, presentations, training materials or documentation on related topics.

Qualifications

Required Skills
BS or higher degree in Computer Science.
Solid understanding of SQL and good grasp of relational and analytical database management theory and practice.
Knowledge in Java software development, especially in the database field.
Good knowledge of JDBC, XML and Web Services APIs.
Excellent verbal and written communication skills to be able to interact with technical and business counterparts.
Active listener.
Strong analytical and problem solving abilities.
Lots of curiosity. You never stop learning new things.
Creativity. We love to be surprised with innovative solutions.
Willingness to travel around 50%.
Be a team worker with positive attitude.
We Value
Experience working with Java frameworks.
Experience working with GIT or other version control systems.
Experience working with BigData and/or noSQL environments like Hadoop, mongoDB, ...
Experience working with caching approaches and technologies such as JCS.
Experience in Windows & Linux (and UNIX) operating systems in server environments.
Business software implementation and integration projects (e.g. ETL/Data Warehouse architectures, CEP, BPM).
Integration with packaged applications (e.g. relational databases, SAP, Siebel, Oracle Financials, Business Intelligence tools, …).
Industry experience in supporting mission critical software components.
Experience in attending customer meetings and writing technical documentation.
Foreign language skills are a plus.
Additional Information

Employment Practices
We are committed to equal employment opportunity.
We respect, value and welcome diversity in our workforce.
We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee."
79,Data Engineer - Business Growth,"Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer - Business Growth: JD.com

JD.com is China's largest online retailer and its biggest overall retailer, as well as the country's biggest Internet company by revenue. JD.com sets the standard for online shopping through its commitment to quality, authenticity, and its vast product offering covering everything from fresh food and apparel to electronics and cosmetics. Its unrivaled nationwide fulfillment network provides standard same- and next-day delivery covering a population of more than 1 billion - a level of service and speed that is unmatched globally. In 2017, JD had 292.5M annual active consumers and net revenue of US$55.7B. JD.com is currently on pace to become China's largest B2C e-commerce platform by 2021. JD.com is listed on the NASDAQ under the ticker ""JD"". In 2014, the year of its listing, this was the largest IPO on the NASDAQ. JD.com was the first Chinese Internet company to be included on the Fortune Global 500 List.

JD plans to strengthen its leadership position as a technology-driven company over the next 10+ years, with a focus on big data, AI technology and smart logistics, and to become the dominant e-commerce platform in China. To achieve these goals, we aim to more deeply understand consumer behavior in China, better understand what drives purchases, how to forecast demand, and how to target promotions, advertising and other efforts across the whole range of technologies available to sophisticated Chinese consumers today. As global brands enter China and as China in turn becomes more globalized, we will provide innovative ways for these brands to deliver new products, help drive new monetization strategies and provide solutions for brand safety and product authenticity. In addition, we are constantly evolving our already state-of-the-art ad-serving platform to serve digital ads across all of China. For this platform, we are developing sophisticated analytics around attribution, return and exposure. To drive this exciting agenda forward, we are looking for people with strong social science, quantitative, statistical and machine learning skills who can partner with our team in Mountain View and in Beijing to leverage the vast data and computational assets of JD (which now involves collaboration and data sharing with Baidu, NetEase, Quihoo, Sogou, Toutiao and WeChat. This powerful consortium combines the largest search, gaming, mobile-security, news-aggregation and social network companies in China).

The ideal candidate for this position will be quantitatively trained (advanced master's or PhD) with expertise in data science. Individuals who are interested in using computing and data to more deeply understanding consumer behavior, marketplace behavior, competition and who have a passion for solving complex business problems through the combined use of data, technology and strategy will thrive in our environment. They will also be comfortable working cross-functionally and thrive in a fast-paced organization. Interest in e-commerce and in economics/quantitative marketing/business-analytics is a plus.

Responsibilities


Design, build and launch new data extraction, transformation and loading pipelines to develop data-based tools for analyzing consumer behaviors.
Produce high quality code with reliability and scalability; efficiently collaborate across different teams to ensure smooth transition of POC stage code stacks to productization.
Analyze large-scale structured and unstructured data; develop new machine learning, statistical models and visualization tools to help answer JD advertising and marketing related business questions.

Minimum Qualifications


Master's or PhD degree in Computer Science, Statistics, Mathematics or related fields.
Proficiency in SQL and a Unix/Linux environment for automating processes with shell scripting.
Deep understanding of big data technologies (some subset of MapReduce, Hadoop, Pig, Spark, Hive, Kafka, etc.).
Proficiency in at least one of the following programming languages: Java, C++, Scala, Python, R.
Solid foundation in data structures and algorithms, with excellent debugging and troubleshooting skills.
Excellent communication skills and a strong team player.
Interest in consumer behavior, e-commerce and related business questions.
Training and experience in ML and building statistical models a plus.

About JD-Business Growth

The Business Growth Business Unit manages JD's ad-business. JD's ad-tech comprises a set of large scale publishing, programmatic ad-exchange, ad-network and data management platforms for serving digital ads on JD-owned and affiliated properties across China. JD Business Group also works on JD's marketing technology platform efforts aimed at helping brands leverage JD and its partners' large-scale data and computing assets to improve their marketing in China.

JD Business Growth research group in Silicon Valley is focused on using science to suggest, support and shape new and existing data-driven advertising, e-commerce, and marketing products. We combine large-scale experimentation, statistical-econometric, machine learning and artificial intelligence tools, with social-science, economics and computer science to find innovative ways to drive growth and monetization. Some of the problems we work on include advertising attribution, recommendation systems, advertising targeting, using applied economics, causal inference, deep learning, and reinforcement learning methods.

JD.com is an Equal Opportunity Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.

------

------"
80,Sr. Data Engineer,"Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Jun 17, 2019
Weekly Hours: 40
Role Number: 200068165
We are seeking an energetic and self-driven software engineer to help design, implement, and deliver our data platform and pipelines for Health research and development. As a member of our fast-paced group in Apple Health, you will have the unique and rewarding opportunity to shape the systems that allow our products to delight and inspire millions of Apple’s customers every day.
Key Qualifications
5+ years of industry experience
Excellent programming skills in at least one of Scala, Java, or Python
Experience with Spark
Ability to communicate technical ideas clearly within the team
Experience with ETL, data warehousing environments helpful but not required
Description
Apple's Health team is seeking a Software Engineer to build out our data pipelines. We believe researchers and data scientists should be able to focus on the hardest problems from our sensors to healthcare: to enable this, the platform we are building enables high performance computing, while maintaining the highest degree of security and privacy. You will be building on top of existing secure environments, which provide the underlying infrastructure; but your work will directly influence its evolution. In addition to using technical knowledge to drive features and improvements, you must be comfortable with rapidly evolving requirements, and have excellent interpersonal skills. Success depends on working with cross-functional teams.
Responsibilities: Build software to enable data scientists, researchers to produce performant, robust, and reproducible analyses, models, and data products. Work with cross functional teams to better capture requirements. Participate in architecture and code reviews. Maintain relentless focus on performance, usability, security & quality.
Education & Experience
MS in Computer Science (or equivalent experience) preferred.
BS degree in Computer Science or related field, or equivalent work experience."
81,Senior Data Engineer,"Mountain View, CA 94040",Mountain View,CA,94040,None Found,"
BS or MS in Computer Science , MIS or related degrees
Experience with at least one scripting language (e.g. Python)
2+ years of experience with relational databases and SQL
Extensive experience in designing OLAP data warehouses
Great knowledge of dimensional modeling, SCD concepts
Prior experience with data cleaning and modeling
Prior experience with product behavior data preferred
Working knowledge of Spark is preferred
Advanced SQL skills is a must
Expressive personality who is willing to reach out to folks, brainstorm possible solutions and come up with an action plan
",None Found,"
Design and develop ""fact stores"" for the core data sets that makes up the entire company business flows
Architect and develop data models and schemas for our core data using Spark and SQL
Work across multiple data analyst to brainstorm the data points behind their dashboard and generalize it in form of ""fact stores""
Produce high quality readable code in an agile setting.
Work with the data scientists to make sure we can build feature engineering data sets from
",None Found,None Found,"Udacity's mission is to democratize education. We're an online learning platform offering groundbreaking education in fields such as artificial intelligence, machine learning, robotics, and more. Focused on self-empowerment through learning, Udacity is making innovative technologies such as self-driving cars available to a global community of aspiring technologists, while also enabling learners at all levels to skill up with essentials like programming, web and app development. If you love a challenge, and truly want to make a difference in the world, read on!

The Data Team is looking for a Senior Data Engineer to help us architect, design and develop fact tables for managing all of the data that drives our products across consumer and enterprise. The ideal candidate will be comfortable talking to data analysts, understand the data that power their product, generalize it and implement common fact tables across the company.

This is an opportunity to be a part of a team which includes Data Engineers, Data Analysts and Data Scientists. Some of the tools we use are Python, Spark, Docker, Postgres, Git, Airflow and Redshift.

Responsibilities:
-----------------


Design and develop ""fact stores"" for the core data sets that makes up the entire company business flows
Architect and develop data models and schemas for our core data using Spark and SQL
Work across multiple data analyst to brainstorm the data points behind their dashboard and generalize it in form of ""fact stores""
Produce high quality readable code in an agile setting.
Work with the data scientists to make sure we can build feature engineering data sets from

Fact stores


Develop tools to monitor, debug and analyze the generic ""Fact stores""
Come with a Quality check framework for fact stores accuracy
Build a process to come up with feature changes/additions, refactoring to fact stores

Qualifications:
---------------


BS or MS in Computer Science , MIS or related degrees
Experience with at least one scripting language (e.g. Python)
2+ years of experience with relational databases and SQL
Extensive experience in designing OLAP data warehouses
Great knowledge of dimensional modeling, SCD concepts
Prior experience with data cleaning and modeling
Prior experience with product behavior data preferred
Working knowledge of Spark is preferred
Advanced SQL skills is a must
Expressive personality who is willing to reach out to folks, brainstorm possible solutions and come up with an action plan

We are an equal opportunity employer and value diversity at our company. Women, people of color, members of the LGBTQ community, individuals with disabilities, and veterans are strongly encouraged to apply.

Udacity's Terms of Use and Privacy Policy ( https://www.udacity.com/legal/privacy )"
82,Data Engineer / Scientist (Spark + AI 2019),"Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Apr 23, 2019
Role Number: 200044728
We are looking for excellent software engineers with experience in big data engineering. You will have the opportunity to engage with exciting new-product teams around Apple, and use your data science, systems, machine learning and artificial intelligence skills to tackle challenging technical problems in our next generation products that will delight millions of people.
We are hiring in Cupertino, Seattle, and Pittsburgh.
Key Qualifications
Strong programming skills in C++, Java, Scala, or Python
Deep understanding of basic data structures and algorithms
Experience with scaling data platforms to hundreds of terabytes or petabytes using Spark or Hadoop
Familiarity with modern machine learning techniques
Creative, collaborative, & product focused
Curious about new technologies and passionate about exploring new use cases
Description
At Apple, you will design, develop and deploy large scale services and platforms. You will also collaborate with teams across Apple, who are building the newest, most compelling intelligent applications in the world. You will have strong engineering and communication skills, as well as a belief that data driven processes lead to great products.
Education & Experience
B.S., M.S., or PhD in Computer Science, Computer Engineering, Statistics, Bioinformatics, Applied Mathematics, or equivalent practical experience"
83,Staff Back-End Developer - Marketing Analytics,"San Bruno, CA 94066",San Bruno,CA,94066,None Found,"6+ years of experience in a Data Engineering and/or Software Engineering role.Bachelor's or equivalent in Computer Science, any Engineering discipline, Statistics, Information Systems or another quantitative field.",None Found,None Found,None Found,None Found,"Position Description
As a Back-End Data Engineer, you will help the World’s largest omni-channel retailer structure, collect, organize and leverage robust data sets that will be used to manage and optimize the business. The Data Engineer will support database architects, channel and category BI analysts, the campaign measurement team, and data scientists on data initiatives and will ensure data is consistent and accurate at scale.

Responsibilities:

Maintain a robust data pipeline to support Marketing Vehicle performance, Category performance, Customer cohort activity, and Financial Plan/Forecast reporting.Define taxonomy, data flow, and data formatting for various internal and external partners.Provide data architecture that is flexible, scalable, and consistent for cross-functional use, and aligned to stakeholder business requirements.Deliver ongoing and accurate reporting of KPIs for Weekly Business Review reporting.Work with machine learning/data science team on implementation of data with AI models.Publish KPI definitions and ensure they are consistent with Marketing Analytics, Marketing Finance, and Retail Operations expectations.Collect and be responsible for the accuracy of all data to be used in several ROAS models, including MM (Marketing Mix) and MTA (Multi Touch Attribution).Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, address speed/performance issues.Work with BI and data science team members to assist them in accessing and leveraging key data sets.Provide analytics tools and training to drive actionable insights into key business performance metrics, including campaign and category performance, marketing vehicle performance, customer funnel metrics, and operational efficiency.Define SLA and acceptable time lags by data source, define QA process, and socialize resolution process to ensure data accuracy and consistency.
Minimum Qualifications
6+ years of experience in a Data Engineering and/or Software Engineering role.Bachelor's or equivalent in Computer Science, any Engineering discipline, Statistics, Information Systems or another quantitative field.
Additional Requirements:
Experience with big data tools such as Hadoop, Hive, HDFS, Oozie etc.Experience with Relational SQL and NoSQL databasesExperience with Java, Apache Spark or Flink and Spring FrameworkStrong skills in communicating technical material to range of audiences.Exposure to building ‘big data’ data pipelines, architectures and data sets.Experience performing root cause analysisStrong analytic skills related to working with unstructured datasets.Project management and organizational skills.
Additional Preferred Qualifications
Experience working with Marketing channel data, such as Mobile, Social, Email, SEM, SEO, Affiliates, and offline channels like TV and print.Experience visualization tools including Tableau, Thoughtspot, Looker or Domo.Passion for working in a fast-paced agile environment.
Company Summary
Walmart Global eCommerce is comprised of Walmart.com, VUDU, SamsClub.com, and our technical powerhouse @WalmartLabs. Here, innovators incubate next gen e-commerce solutions in real-time. We integrate online, physical, and mobile shopping experiences for billions of customers around the globe. How do we do it? We continuously build and invest in new technology including open source tools and big data innovations. Data scientists, front and back-end engineers, product managers, and web and UX/UI teams collaborate alongside e-commerce experts to envision, prototype, and bring revolutionary ideas to life in a dynamic, flexible and fun work culture.
Position Summary
As a Back-End Data Engineer, you will help the World’s largest omni-channel retailer structure, collect, organize and leverage robust data sets that will be used to manage and optimize the business. The Data Engineer will support database architects, channel and category BI analysts, the campaign measurement team, and data scientists on data initiatives and will ensure data is consistent and accurate at scale."
84,"Big Data Engineer, Apple Media Products Analytics","Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Nov 1, 2018
Role Number: 200007010
The iTunes Store is looking for a top-notch Big Data engineer to develop an analytics infrastructure that will generate insights into customer experiences on products such as the iTunes Store, App Store, and iBookstore. Our products reach hundreds of millions of customers around the world, and have revolutionized how people interact with their music, movies, TV shows, apps, books, and podcasts.
Key Qualifications
Language: Java or Scala
Working knowledge on the following distributed data processing platforms: Spark, Hadoop
Great if you also know: HBase, Kafka, Java Map Reduce
Algorithms: You will be working on developing new algorithms to process large scale data efficiently.
We expect you to know: basic computer science algorithms, data structures and distributed algorithms to process and mine data, e.g. Map Reduce Algorithm
Great, but not required, if you also know about how to develop: graph, data classification and clustering algorithms in distributed environment
Good debugging, critical thinking, and communication skills
Knowledge in engineering machine learning, feature engineering systems is a plus.
Able to gather cross-functional requirements and translate them into practical engineering tasks
5+ years of programming experience
Description
The iTunes Store Analytics team is responsible for collecting, analyzing, and reporting on customer experience data. From this data we generate insights into how customers interact with our products, and use these insights to drive improvements to user-facing features.
You will be working on a small team and will be responsible for processing large amounts of data and developing platforms to process, analyze and mine that data to extract intelligence. Prepare data for visualization, ad-hoc exploration, reporting, and further analysis. We are looking for a well-rounded data engineer who has good design sense.
The ideal candidate pays close attention to details - caring about the quality of the input data as well as how the processed data is ultimately interpreted and used. You are also a team player - ready to contribute during design sessions, and able to give and receive constructive code reviews. Your curiosity drives you to explore new technologies and apply creative solutions to problems.
Education & Experience
BS degree in Computer Science or a related field
Additional RequirementsBuild large scale data processing, mining and analysis projects and features, ensuring robust & maintainable solutions are implemented with special attention to data quality, performance and usability details.Effectively demonstrate feature prototypes to executivesDevelop, advocate for, and build consensus on, coding best practices.Ability to effectively work with cross functional teams to understand requirements and identify design and engineering impactsExperience with architecting big data and analytical applications that scale to petabytes highly preferred."
85,Senior Data Engineer,"Palo Alto, CA 94301",Palo Alto,CA,94301,None Found,None Found,None Found,None Found,None Found,None Found,"Based in Palo Alto, California, Cardinal Analytx Solutions is a predictive analytics company that leverages machine learning and clinical expertise to identify future cost and rising risk. We were founded with the vision of helping patients obtain better care, sooner. Most of healthcare today is very reactive – with care being delivered when someone is already ill. We believe that healthcare should be about keeping individuals and their families well – to prevent them from ever becoming sick. Through machine learning and the extensive experience of our founders, team, and advisors in healthcare, we are committed to helping healthcare organizations see around the corner so that they can take a proactive approach to helping patients.
The engineering team at Cardinal Analytx is responsible for building and supporting the data, software, infrastructure, security, and operational capabilities of the company. We are looking for technologists to help design, build, and manage the processes and tools necessary to craft the industry leading insights and capabilities that our clients will use to help those in need.
As a data engineer, you will be joining a small team of high talented and experienced software engineers in defining and building out the infrastructure and processes around our data pipeline. You'll be working on a wide variety of data problems, including, but not limited to, data modeling, data ETL, data warehousing, analytics, feature generation, and data operations. The ideal candidate will be an experienced software engineer with a solid data engineering background that has moved beyond a SQL database and scripts as the amount of data and real-time processing exceeded the capability of existing tools.
As a Senior Data Engineer, You Will:
Evolve a data pipeline to meet a startup's constantly changing demand
Build data stores/marts/warehouses for our data science and analytics teams
Support pilot proof of concept programs with clients and design partners
Strike a balance between moving fast and engineering resilient solutions
Be flexible! You may not always be working on data – we're a team of generalists, and love to work on different types of problems
Desired Experience:
2+ years of experience in software engineering, with 5+ years of experience with data engineering (mid to senior level)
Experience with building, operationalizing, and managing ETL processes using both home grown and external tools
Experience with modern data approaches
Experience in SQL, Python, Airflow, and Spark
Good understanding of NoSQL technologies and capabilities
Nice to Have, But Not Necessary:
AWS/Cloud experience
Healthcare related development experience
Experience with EHR data and EHR-claims data integration
Experience with data operations
*No recruiter submissions without prior agreement/consent*

Follow us on LinkedIn!
L5BpVnBaqf"
86,Data Engineer,"Los Altos, CA 94022",Los Altos,CA,94022,None Found,None Found,"Proficient using Python / SQL or equivalent technologies for complex data manipulations and transformations
Comfortable with shell scripting, preferably Linux
Familiarity with stream processing (Kafka) is desirable
Ability to drive and implement ideas in a fast-paced environment.
Effective project management, interpersonal and organizational skills.
Developed communication, problem-solving and analytical skills.","Build, maintain and enhance a data pipeline and analytics warehouse used to serve reporting and insights.
Implement batch and streaming ETL processes for ingesting data from heterogeneous sources
Deploy capabilities to allow internal and external customers to access the data for insights to guide customer success initiatives, product development, and corporate marketing efforts.
Build capabilities to enable the organization to develop and deploy machine learning applications at scale.
Involvement in all phases of software development from review of functional specification through assisting with test plans and final QA cycle. Actively participates in monitoring and troubleshooting of production platform related issues
Perform day-to-day tasks that ensure technology platform remains stable and available to users. Closely work with cross functional team to enhance systems, trouble-shoot data issues, etc.","
Bachelor's degree in Computer Engineering or equivalent is desired",None Found,"FinTech is a fast-paced, rapidly growing, and engaging space. Jemstep is a market-leading FinTech provider of digital advice solutions to investment advisory firms including banks, broker dealers, and independent advisors. Our Advisor Pro solution enables firms and their advisors to connect with investors digitally, delivering investment advice to help them achieve their financial goals. We are helping banks and financial advisors transform their business, making it more efficient for firms to serve their clients and onboard new ones. While most 'robo' advice solutions in market have been designed to replace advisors, our solution is designed to help advisors by extending their reach and enriching their clients' experience. We firmly stand behind our platform's flexibility, ease of integration and adoption, and value in delivering measurable results with a full suite of technology solutions.

In this position, you'll be joining a high-growth company brimming with intelligent people, optimism, collaboration, and passion for progress, with ties to Invesco, a globally recognized and established asset management firm with a long history of success. Invesco has presence in over 25 countries and Jemstep has offices throughout the United States, South Africa, and India. As we further our mission and expand our presence and position, we're seeking individuals of demonstrable success, with energetic, motivating personalities to join our team.

Job Purpose (Job Summary):

As a Data Engineer you will be responsible for building, expanding, and optimizing Jemstep's data and data pipeline architecture. You will contribute to and support Software Development, Product Management, and Customer Success initiatives that enable the company to make data-based decisions. You will need strong strategic, collaboration and communication skills, as well as an entrepreneurial mindset.

Key Responsibilities / Duties:
Build, maintain and enhance a data pipeline and analytics warehouse used to serve reporting and insights.
Implement batch and streaming ETL processes for ingesting data from heterogeneous sources
Deploy capabilities to allow internal and external customers to access the data for insights to guide customer success initiatives, product development, and corporate marketing efforts.
Build capabilities to enable the organization to develop and deploy machine learning applications at scale.
Involvement in all phases of software development from review of functional specification through assisting with test plans and final QA cycle. Actively participates in monitoring and troubleshooting of production platform related issues
Perform day-to-day tasks that ensure technology platform remains stable and available to users. Closely work with cross functional team to enhance systems, trouble-shoot data issues, etc.

Work Experience / Knowledge:
At least 5 years of hands on experience in a data engineering capacity
Experience with digital marketing technologies (e.g. web analytics, email marketing platforms)
Experience implementing BI solutions (e.g. Tableau, Qlik, Looker) and scaling to large user audiences

Skills / Other Personal Attributes Required:
Proficient using Python / SQL or equivalent technologies for complex data manipulations and transformations
Comfortable with shell scripting, preferably Linux
Familiarity with stream processing (Kafka) is desirable
Ability to drive and implement ideas in a fast-paced environment.
Effective project management, interpersonal and organizational skills.
Developed communication, problem-solving and analytical skills.

Formal Education: (minimum requirement to perform job duties)
Bachelor's degree in Computer Engineering or equivalent is desired

FLSA (US Only): Exempt

The above information on this description has been designed to indicate the general nature and level of work performed by employees within this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The job holder may be required to perform other duties as deemed appropriate by their manager from time to time.

Invesco's culture of inclusivity and its commitment to diversity in the workplace are demonstrated through our people practices. We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender, gender identity, sexual orientation, marital status, national origin, citizenship status, disability, age, or veteran status. Our equal opportunity employment efforts comply with all applicable U.S. state and federal laws governing non-discrimination in employment."
87,Senior Data/Server Engineer,"Sunnyvale, CA",Sunnyvale,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Niantic, the developer behind popular games like Pokemon GO and Harry Potter: Wizards Unite is searching for a Senior Data Engineer with an extensive server infrastructure background. Join a group of experienced engineers to help build and scale Niantic's core data infrastructure. This is a nimble, motivated team responsible for building trust, owning data integrity, and supporting data-driven decision-making at Niantic.
Responsibilities
Architect the data stack responsible for storing and processing enormous volumes of analytics data.
Design efficient, extensible data models for use with Niantic's data pipelines and analytics systems.
Improve and extend core data competency for the User Acquisition pipeline to facilitate tools and reporting in support of growing UA efforts.
Organize and secure data drawn from diverse sources and build streamlined ETL pipelines to transform and validate it.
Mentor and offer technical guidance to data engineers, data scientists, and infrastructure engineers.
Work with the Product Team and Management to define a shared vision, an execution strategy, and communicate timeline and trade-offs.
Qualifications
4+ years of experience developing and deploying robust, large-scale data pipelines.
A high degree of attention to detail and clear aptitude for finding and resolving data integrity issues.
Proven success in securing and auditing data stores and implementing legal compliance, e.g. GDPR.
Deep knowledge of available data storage technologies such as Hadoop, Cassandra, Druid, and their trade-offs.
Excel in developing general purpose solutions to difficult problems and building elegant solutions.
Strong communicator to both technical and non-technical people and demonstrated ability to document technical design decisions.
Expert in Java or Scala, Python and SQL.
BS, MS, or PhD in Computer Science or a related technical field.
Plus If...
Familiarity with mobile advertising, user acquisition and associated data processing and metrics (e.g. attribution, retention, CPI, ROAS).
Detailed knowledge of and experience with the large advertising networks, e.g. Google, Facebook, Twitter, Apple.
Knowledge of the Google data stack (e.g. Dataflow, BigQuery, BigTable).
Proficient in the use of Airflow, Composer.
Join the Niantic team!
Niantic is the world’s leading AR technology company, sparking creative and engaging journeys in the real world. Our products inspire outdoor exploration, exercise, and meaningful social interaction.
Originally formed at Google in 2011, we became an independent company in 2015 with a strong group of investors including Nintendo, The Pokémon Company, and Alsop Louie Partners. Our current titles include pioneering global-control game Ingress, record-breaking AR game Pokémon GO, and recently released third title, Harry Potter: Wizards Unite.
Niantic is an Equal Opportunity and Affirmative Action employer. We believe that cultivating a workplace where our people are supported and included is essential to creating great products our community will love. Our mission emphasizes seeking and hiring diverse voices, including those who are traditionally underrepresented in the technology industry, and we consider this to be one of the most important values we hold close.
We're a hard-working, fun, and exciting group who value intellectual curiosity and a passion for problem-solving! We have growing offices located in San Francisco, Sunnyvale, Bellevue, Los Angeles, London, Tokyo, Hamburg, and Zurich."
88,Sr. Data Engineer,"Santa Clara, CA",Santa Clara,CA,None Found,None Found,None Found,"
Computer Science/Engineering Bachelor’s degree or equivalent such as: Mathematics or statistics
Solid understanding of distributed programming and experience with EMR or Hadoop
Expert at SQL and query performance tuning
Understanding of Cloud technologies (preferably AWS services), and security and how they can be combined to design scalable cloud solutions
Experience with columnar storage and MPP/Analytical DBs (Redshift, Netezza, GreenPlum, Teradata).
Strong Python skills. Experience with bash scripting and one or more of Java/Scala/node.js
Solid understanding of software engineering and operations
Detail oriented. Strong prioritization skills and sense of urgency
Strong analytical and problem solving skills. Takes pride in efficient designs and accurate results
Objectively analyzes the pros, cons, and the trade-offs of a design path and helps the team to arrive at the most optimal solution, which may not be their own
Loves to learn and experiment with new technologies and shares findings with the team
Effective team player. Honest and respectful of others

#LI-KK1","
Define data processing patterns that will be needed in the data platform
Define needed capabilities in the platform and then lead engineering teams to deliver these capabilities
Design, develop, and deliver building blocks for products based on data and analytics
Design, develop and orchestrate data pipelines for real-time and batch data processing
Design optimal storage, data structures, security, and retrieval mechanisms for data at rest in Data lake and Analytics data store or data in motion for real-time processing requirements
Design and develop reusable components and frameworks for ingestion, cleansing, and data quality
Collaborate with upstream sources and downstream consumers to come up with expandable data contracts
Design and develop Rest APIs/ or web-service consuming clients for data push/pull for both upstream and downstream applications
Help guide and grow the technical depth of junior members of the team.
Collaborate with data team, product owners, Scrum-master to refine and estimate stories/epics
Be integral part of scrum team to deliver on commitments on time and with good quality","
Computer Science/Engineering Bachelor’s degree or equivalent such as: Mathematics or statistics
Solid understanding of distributed programming and experience with EMR or Hadoop
Expert at SQL and query performance tuning
Understanding of Cloud technologies (preferably AWS services), and security and how they can be combined to design scalable cloud solutions
Experience with columnar storage and MPP/Analytical DBs (Redshift, Netezza, GreenPlum, Teradata).
Strong Python skills. Experience with bash scripting and one or more of Java/Scala/node.js
Solid understanding of software engineering and operations
Detail oriented. Strong prioritization skills and sense of urgency
Strong analytical and problem solving skills. Takes pride in efficient designs and accurate results
Objectively analyzes the pros, cons, and the trade-offs of a design path and helps the team to arrive at the most optimal solution, which may not be their own
Loves to learn and experiment with new technologies and shares findings with the team
Effective team player. Honest and respectful of others

#LI-KK1",None Found,"At realtor.com data is very important to us and we have lots of it, in motion and at rest. To keep up with our business and data growth, the realtor.com Data Engineering team is seeking senior-level data engineers to join our team.
This impactful role will promote the build-out and usage of the data platform, data-driven decisions and data products for realtor.com, it's customers, partners, and consumers.

Duties and Responsibilities:
Define data processing patterns that will be needed in the data platform
Define needed capabilities in the platform and then lead engineering teams to deliver these capabilities
Design, develop, and deliver building blocks for products based on data and analytics
Design, develop and orchestrate data pipelines for real-time and batch data processing
Design optimal storage, data structures, security, and retrieval mechanisms for data at rest in Data lake and Analytics data store or data in motion for real-time processing requirements
Design and develop reusable components and frameworks for ingestion, cleansing, and data quality
Collaborate with upstream sources and downstream consumers to come up with expandable data contracts
Design and develop Rest APIs/ or web-service consuming clients for data push/pull for both upstream and downstream applications
Help guide and grow the technical depth of junior members of the team.
Collaborate with data team, product owners, Scrum-master to refine and estimate stories/epics
Be integral part of scrum team to deliver on commitments on time and with good quality
Education, Skills and Experience:
Computer Science/Engineering Bachelor’s degree or equivalent such as: Mathematics or statistics
Solid understanding of distributed programming and experience with EMR or Hadoop
Expert at SQL and query performance tuning
Understanding of Cloud technologies (preferably AWS services), and security and how they can be combined to design scalable cloud solutions
Experience with columnar storage and MPP/Analytical DBs (Redshift, Netezza, GreenPlum, Teradata).
Strong Python skills. Experience with bash scripting and one or more of Java/Scala/node.js
Solid understanding of software engineering and operations
Detail oriented. Strong prioritization skills and sense of urgency
Strong analytical and problem solving skills. Takes pride in efficient designs and accurate results
Objectively analyzes the pros, cons, and the trade-offs of a design path and helps the team to arrive at the most optimal solution, which may not be their own
Loves to learn and experiment with new technologies and shares findings with the team
Effective team player. Honest and respectful of others

#LI-KK1
Diversity is important to us, therefore, realtor.com is an Equal Opportunity Employer regardless of age, color, national origin, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, marital status, status as a disabled veteran and/or veteran of the Vietnam Era or any other characteristic protected by federal, state or local law. In addition, realtor.com will provide reasonable accommodations for otherwise qualified disabled individuals."
89,Data Software Engineer,"Pleasanton, CA",Pleasanton,CA,None Found,None Found,None Found,None Found,"
Own product data set features from the development phase through to production deployment.
Support building distributed, scalable, and reliable data pipelines that ingest and process data at scale and in real-time.
Deploy and maintain Hadoop/Big Data/Spark and database storage Infrastructures in AWS cloud.
Support managing data environments and/or data sets to serve a wide range of data users, including but not limited to Data Scientists, Data Analysts, Business Analysts etc.
Perform offline analysis of large data sets using components of a big data software ecosystem.
Evaluate big data technologies and prototype solutions to improve data processing architecture.
Monitor installation of HDFS/Hadoop/Spark and related software releases, third-party utilities with emphasis on overall system performance.
Support the activity of troubleshoot and determine root cause of complex data provenance, metadata issues and engineering questions that may involve interfacing with various technical staff in multiple organizations and with differing levels of expertise.
Develop tools and procedures to monitor and automate system tasks on servers and clusters
Author extract, transform and load (ETL) scripts for moving and curating data into data sets for storage and use by a datalake, data warehouse and datamart.","Bachelor’s degree in computer science, computer engineering, or a related field, or the equivalent combination of education and related experience.
5 years of professional experience as a data software engineer.
1 years of experience with AWS/Cloud Big Data computing design, provisioning, and tuning.
Previous experience as a Data Engineer / Database Administrator and/or Business Intelligence Analyst.","Knowledge of database concepts, object and data modeling techniques and design principles.
Detailed knowledge of database architectures, software, and facilities.
Successful history of manipulating, processing, and extracting value from large disconnected data sets.
Experience with programming languages – Python (required), Scala, Ruby, R.
Database technologies - SQL, performance tuning concepts, AWS RDS, RedShift, MySQL.
Experience with big data batch processing tools: Hadoop MapReduce, ElasticSearch, PIG, Hive, Cascading/Scalding, Apache Spark, AWS EMR.
Experience with stream-processing systems: Kinesis, Kafika, MQTT.
Experience with relational NoSQL databases including DyanamoDB.
Ability to write JSON, XML, YAML and other data definition schemas.
Good verbal and written communication skills necessary to effectively collaborate in a team environment and present and explain technical information and provide advice to management.
A seasoned, experienced professional with a full understanding of area of specialization; resolves a wide range of issues in creative ways.
Work is independent and collaborative in nature.
Contributes to moderately complex aspects of a project. Provides regular updates to manager on project status.","Job Summary:

Responsible for all aspects of data acquisition, data transformation, analytics scheduling and operationalization to drive high-visibility, cross-division outcomes. Investigate, evaluate, test and recommend technical solutions for future systems. Support software developers, database architects, data scientists on data initiatives and will ensure optimal data delivery architecture.

Major Responsibilities:
Data Design Management
Own product data set features from the development phase through to production deployment.
Support building distributed, scalable, and reliable data pipelines that ingest and process data at scale and in real-time.
Deploy and maintain Hadoop/Big Data/Spark and database storage Infrastructures in AWS cloud.
Support managing data environments and/or data sets to serve a wide range of data users, including but not limited to Data Scientists, Data Analysts, Business Analysts etc.
Perform offline analysis of large data sets using components of a big data software ecosystem.
Evaluate big data technologies and prototype solutions to improve data processing architecture.
Monitor installation of HDFS/Hadoop/Spark and related software releases, third-party utilities with emphasis on overall system performance.
Support the activity of troubleshoot and determine root cause of complex data provenance, metadata issues and engineering questions that may involve interfacing with various technical staff in multiple organizations and with differing levels of expertise.
Develop tools and procedures to monitor and automate system tasks on servers and clusters
Author extract, transform and load (ETL) scripts for moving and curating data into data sets for storage and use by a datalake, data warehouse and datamart.
Technical Advisor
Collaborate with other teams to deploy data tools and data sets that support both operations and product use cases.
Evaluate and advise on technical aspects of open work requests in the product backlog for other projects as assigned.
Knowledge/Skill Requirements:

Knowledge of database concepts, object and data modeling techniques and design principles.
Detailed knowledge of database architectures, software, and facilities.
Successful history of manipulating, processing, and extracting value from large disconnected data sets.
Experience with programming languages – Python (required), Scala, Ruby, R.
Database technologies - SQL, performance tuning concepts, AWS RDS, RedShift, MySQL.
Experience with big data batch processing tools: Hadoop MapReduce, ElasticSearch, PIG, Hive, Cascading/Scalding, Apache Spark, AWS EMR.
Experience with stream-processing systems: Kinesis, Kafika, MQTT.
Experience with relational NoSQL databases including DyanamoDB.
Ability to write JSON, XML, YAML and other data definition schemas.
Good verbal and written communication skills necessary to effectively collaborate in a team environment and present and explain technical information and provide advice to management.
A seasoned, experienced professional with a full understanding of area of specialization; resolves a wide range of issues in creative ways.
Work is independent and collaborative in nature.
Contributes to moderately complex aspects of a project. Provides regular updates to manager on project status.
Education/Experience Requirements:
Bachelor’s degree in computer science, computer engineering, or a related field, or the equivalent combination of education and related experience.
5 years of professional experience as a data software engineer.
1 years of experience with AWS/Cloud Big Data computing design, provisioning, and tuning.
Previous experience as a Data Engineer / Database Administrator and/or Business Intelligence Analyst.
Panasonic is proud to be an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, disability status, protected veteran status, and any other characteristic protected by law or company policy. All qualified individuals are required to perform the essential functions of the job with or without reasonable accommodation. Pre-employment drug testing is required for safety sensitive positions or as may otherwise be required by contract or law. Due to the high volume of responses, we will only be able to respond to candidates of interest. All candidates must have valid authorization to work in the U.S. Thank you for your interest in Panasonic Corporation of North America.
#LI-SR2"
90,Hadoop Data Engineer,"San Jose, CA 95131",San Jose,CA,95131,None Found,"Data Engineer with experience in creating data pipeline
Should have hands-on scripting experience on Hadoop Eco System of 5+ years
Skill Sets: Hadoop, Hive, Spark, Python and Unix scripting, Snowflake/GCP
Cisco experience preferred
Exposure to Data Visualization tools",None Found,None Found,None Found,None Found,"Hadoop Data Engineer - (0044130)
Description

Zensar is a leading digital solutions and technology services company that specializes in partnering with global organizations across industries on their Digital Transformation journey.

Zensar’s comprehensive range of digital and technology services and solutions enables its customers to achieve new thresholds of business performance. Zensar helps clients deliberate not only on executing Digital initiatives but on realizing the Return on Digital®.

For enterprises to be resilient and successful in the long run, they will need to focus on three aspects - Digital Agility, Cross-over IT and fundamentally the Stability of Core Enterprise Systems.

We practice Digital internally as we promote externally – Zensar runs on Digital. Through Engaging, Operating, Partnering, and Managing Digital, we believe in Living Digital.

Qualifications:
Data Engineer with experience in creating data pipeline
Should have hands-on scripting experience on Hadoop Eco System of 5+ years
Skill Sets: Hadoop, Hive, Spark, Python and Unix scripting, Snowflake/GCP
Cisco experience preferred
Exposure to Data Visualization tools

Primary Location: United States of America-California-San Jose
Job Posting: Sep 11, 2019, 4:00:00 AM
 5 To 7"
91,Hadoop Data Engineer,"Sunnyvale, CA",Sunnyvale,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"NON LOCAL IS FINE TOO
NO OPT or CPT
Big Data/Hadoop, Scala/Spark query - MUST HAVE
Overall years of experience 9+ years of experience
Experience in developing Big Data/Hadoop applications
Extensive expertise in batch jobs creation in Scala/Spark query/process batches of data from Hive store
Excellent understanding of development processes and agile methodologies
Strong analytical and interpersonal skills
Self-driven, highly motivated and ability to learn quick
Job Skills:
Big Data/Hadoop
Scala/Spark query
Minimum Experience: 9 Yrs
Education:
Bachelor's Degree"
92,Big Data Engineer,"Sunnyvale, CA",Sunnyvale,CA,None Found,None Found,"Hands-on experience in developing scalable data solutions
Proficiency in Hadoop, Spark, Hive, HBase, and Oozie technologies
Hands-on experience in real-time query engines (Druid, Presto) preferred
Solid understanding of data structures and algorithms
",None Found,None Found,None Found,None Found,"It takes powerful technology to connect our brands and partners with an audience of 1 billion. Nearly half of Verizon Media employees are building the code and platforms that help us achieve that. Whether you’re looking to write mobile app code, engineer the servers behind our massive ad tech stacks, or develop algorithms to help us process 4 trillion data points a day, what you do here will have a huge impact on our business—and the world. Want in? As Verizon’s media unit, our brands like Yahoo, TechCrunch and HuffPost help people stay informed and entertained, communicate and transact, while creating new ways for advertisers and partners to connect. With technologies like XR, AI, machine-learning, and 5G, we’re transforming media for tomorrow, too. We're creators and coders, dreamers and doers creating what's next in content, advertising and technology.
The Yahoo Gemini team is developing next-generation technologies to enrich advertiser and user experience with ever growing and interesting data challenges. We work on all Yahoo user data - building the data pipelines and statistical data models to process billions of events, and making machine learning work.
We are looking for world-class, fun-loving Big Data engineers to join our team where you will have the opportunity to help develop low latency and large scale data systems. You will analyze requirements; investigate optimal software solutions; architect, design, implement and test those solutions.
Minimum Job Qualifications:
Hands-on experience in developing scalable data solutions
Proficiency in Hadoop, Spark, Hive, HBase, and Oozie technologies
Hands-on experience in real-time query engines (Druid, Presto) preferred
Solid understanding of data structures and algorithms
Strong in Java and PIG
Good skills and experience in Linux, XML, JSON, REST
Experience with fault-tolerant system design and high-performance engineering
Experience with machine learning algorithms and/or statistical methods is preferred
Able focus and deliver results in a fast paced and entrepreneurial environment
Strong analytical and problem solving skills
BS/MS degree in Computer Science or industry relevant field
Verizon Media is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Verizon Media is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. If you need accessibility assistance and/or a reasonable accommodation due to a disability, please email ApplicantAccommodation@verizonmedia.com or call 408-336-1409. Emails/calls received for non-disability related issues, such as following up on an application, will not receive a response.
Currently work for Verizon Media? Please apply on our internal career site."
93,Senior Data Engineer - Cloud,"Santa Clara, CA 95050",Santa Clara,CA,95050,None Found,None Found,None Found,None Found,None Found,None Found,"NVIDIA is looking for passionate data engineers to join our Cloud Software Engineering Team in GeForce NOW. In this role, you will play a significant part in helping to craft and guide the future of Cloud Gaming!

GeForce NOW is NVIDIA's Cloud Gaming service streaming games at the highest quality to any and every user regardless of their device types and capabilities – low-end PCs, Macs, or mobile devices. Using the most advanced GPUs and software, GeForce NOW transforms the gaming experience with always up-to-date games on always latest hardware, a streaming experience rivaling that of a local PC, and near-instant launch – just click and play! For more details see https://www.nvidia.com/en-us/geforce/products/geforce-now/
As a data engineer at NVIDIA, you design, implement and maintain our cloud data systems for performance and reliability. You're responsible for tuning and configuring our data platforms as well as building tools to automate and provide disaster recovery, backup systems and monitor the data platforms. You will interact with other engineering teams who want to build services on your platform supporting them in the design and build phases.
What you'll be doing:
Build, design and implement scalable data solutions.
Work closely with engineering teams to build and design data solutions to meet their needs.
Ensure that data is secure and handled responsibly.
Design disaster recovery procedures.
Creating Analytic Reporting interfaces.
Providing public facing system health dashboards.
Cost modeling with infrastructure and suppliers.
Drive performance tuning.
Support, maintain and document solutions.
What we need to see:
Bachelor’s Degree in Computer Science or relevant work experience.
3+ years experience with Cassandra and PostgreSQL.
2+ years of experience designing large-scale data platforms.
Experience with Grafana and Prometheus
Experience building cloud scale data solutions in AWS or like provider.
Experience with high throughput transactional systems.
Ability to code in shell scripting and Python.
Excellent interpersonal, and written communication skills required.
Comfortable working in a dynamic, highly collaborative environment.
Ways to stand out from the crowd:
Show experience in building/managing metric systems, data lakes and transactional systems.
Show a passion for crafting complex SQL Queries.
Familiarity with logging solutions such as Splunk and Sumo Logic.
Experience building SaaS/PaaS enterprise applications.
Experience supporting 24/7 services.
A track record of solving complex problems with elegant solutions.
NVIDIA is widely considered to be one of the technology world’s most desirable employers. We have some of the most brilliant and talented people in the world working for us. If you're creative, self-motivated and enjoy having fun, then what are you waiting for apply today.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression , sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law."
94,Cloud Data Engineer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Req ID: 63856

At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here.

NTT DATA Services currently seeks a Cloud Data Engineer to join our team in San Jose, California (US-CA), United States (US).

Job Description

Integral team member of our AI and Analytics team responsible for design and development of Big data solutions
Partner with domain experts, product managers, analyst, and data scientists to develop Big Data pipelines in Hadoop or Google Cloud Platform
Responsible for delivering data as a service framework from Google Cloud Platform
Responsible for moving all legacy workloads to cloud platform
Work with data scientist to build ML pipelines using heterogeneous sources and provide engineering services for data science applications
Ensure automation through CI/CD across platforms both in cloud and on-premises
Ability to research and assess open source technologies and components to recommend and integrate into the design and implementation
Be the technical expert and mentor other team members on Big Data and Cloud Tech stacks

Experience

5+ years of experience with Hadoop or Cloud Technologies
Expert level building pipelines using Apache Beam or Spark
Familiarity with core provider services from AWS, Azure or GCP, preferably having supported deployments on one or more of these platforms
Experience with all aspects of DevOps (source control, continuous integration, deployments, etc.)
You have experience with containerization and related technologies (e.g. Docker, Kubernetes)
Experience in other open-sources like Druid, Elastic Search, Logstash etc is a plus
Advanced knowledge of the Hadoop ecosystem and Big Data technologies
Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Impala, Spark, Kafka, Kudu, Solr)
Knowledge of agile(scrum) development methodology is a plus
Strong development/automation skills
Competent reading and writing Scala, Python or Java code.
System level understanding - Data structures, algorithms, distributed storage & compute
Can-do attitude on solving complex business problems, good interpersonal and teamwork skills
Degree in Bachelor of Science in Computer Science or equivalent.

This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment.
About NTT DATA Services

NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services.

NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more.

NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.

INDHCLSMC"
95,"Staff Data Engineer, Big Data","Newark, CA",Newark,CA,None Found,None Found,"
Bachelor or Masters in Software Engineering and Computer Science
8+ years of experience in design and development of large scale data platforms
Expert in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Proficient in Spark development with PySpark or Scala
Expert in Data streaming platforms such as Apache Kafka
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python, and/or C++
Experience with running large-scale distributed computing infrastructure such as load balancing, Zookeeper, Micro service architecture
Experienced in security and access management
Experience with managing distributed databases like Elasticsearch, Cassandra
Experience with Columnar database such as Redshift, Vertica
Great verbal and written communication skills.",None Found,None Found,None Found,None Found,"Leading the future of luxury mobility

Lucid’s mission is to inspire the adoption of sustainable energy by creating the most captivating luxury electric vehicles, centered around the human experience. Working at Lucid Motors means having a shared vision to power the future in revolutionary ways. Be part of a once-in-a-lifetime opportunity to transform the automotive industry.

We are looking for a Staff Data Engineer, Big Data who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If building large and building fast, working with a young and very talented team of engineers and collaborating with the brightest mind in the Automotive industry is what you like, Lucid is the best to experience it.
The Role
Lead Data Engineer and architect to design, implement a highly scalable system to ingest and process Petabytes of data per day.
Hands-on design and develop applications for data pipeline and data management.
Set processes and policies for data governance and data pipeline
Architect and implement best practices of big data tools such as Spark, Airflow, Kafka, Presto and Cassandra
Lead and mentor junior Data and BI engineers.
Set and define the standards and best practices in data team
Be the point of reference for solving challenging technical problem.
Architect and implement Machine Learning Pipelines for Data Science team.
Qualifications
Bachelor or Masters in Software Engineering and Computer Science
8+ years of experience in design and development of large scale data platforms
Expert in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Proficient in Spark development with PySpark or Scala
Expert in Data streaming platforms such as Apache Kafka
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python, and/or C++
Experience with running large-scale distributed computing infrastructure such as load balancing, Zookeeper, Micro service architecture
Experienced in security and access management
Experience with managing distributed databases like Elasticsearch, Cassandra
Experience with Columnar database such as Redshift, Vertica
Great verbal and written communication skills.
Be part of something amazing

Come work alongside some of the most accomplished minds in the industry. Beyond providing competitive salaries, we’re providing a community for innovators who want to make an immediate and significant impact. If you are driven to create a better, more sustainable future, then this is the right place for you.

At Lucid, we don’t just welcome diversity - we celebrate it! Lucid Motors is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, national or ethnic origin, age, religion, disability, sexual orientation, gender, gender identity and expression, marital status, and any other characteristic protected under applicable State or Federal laws and regulations.

To all recruitment agencies: Lucid Motors does not accept agency resumes. Please do not forward resumes to our careers alias or other Lucid Motors employees. Lucid Motors is not responsible for any fees related to unsolicited resumes."
96,Lead Data Engineer,"Mountain View, CA 94043",Mountain View,CA,94043,None Found,None Found,None Found,None Found,None Found,None Found,"You are curious, persistent, logical and clever a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Lead Data Enigneer. Scroll down to learn more about the position’s responsibilities and requirements.
What You’ll Do
Architecture design of holistic Cloud data ecosystem with a focus on Google Cloud Platform capabilities and features
Architecture design of Production, Staging/QA, and Development infrastructures running is 24/7 environments
Robust and consistent Cloud Strategy design aligned with business objectives
Provide guidelines for data migration approaches and techniques including ingest, store, process, analyze and explore/visualize data
Assistance with data migration and transformation
Evangelize Cloud computing expertise internally and externally to drive Cloud Adoption
What You Have
A degree in an associated field and/or other advanced certification along with significant experience
In-depth cloud professional, competent of quickly establishing connections and credibility in how to address the business needs via design and operate cloud-based solutions
Experience in Agile or PMI methodology managed projects
Experience in enterprise applications, and big data solutions
Experience in platform and cloud migrations, including migration factory
In-depth experience with databases and tools analysis
In-depth experience with ETL tools
Processes design and development for the data modeling, mining, and analysis
Extensive experience in methodologies and processes for large-scale databases management on-premises and cloud environment
In-depth understanding and knowledge of distributed version control systems like Git
Strong understanding of concepts and experience with StackDriver and other cloud-based monitoring tools including application level and logging
Nice to have
Google Cloud Certified Professional Data Engineer
Experience Creating automated tooling for cloud platforms
Experience with architecting and handling large datasets, structured and semi-structured data formats
Experience with streaming processing
Experience with messaging platforms
Experience with performance testing and tuning
Experience with GCP based security hardening including IAM, ACL, firewall rules, data traffic encryption
What We Offer
Medical, Dental and Vision Insurance (Subsidized)
Health Savings Account
Flexible Spending Accounts (Healthcare, Dependent Care, Commuter)
Short-Term and Long-Term Disability (Company Provided)
Life and AD&D Insurance (Company Provided)
Employee Assistance Program
Unlimited access to LinkedIn learning solutions
Matched 401(k) Retirement Savings Plan
Paid Time Off
Legal Plan and Identity Theft Protection
Accident Insurance
Employee Discounts
Pet Insurance
EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring"
97,"Cloud Engineer (Data) - Santa Clara, CA","Santa Clara, CA",Santa Clara,CA,None Found,None Found,None Found,None Found,"Act as a trusted technical advisor to customers and solve complex Big Data challenges.
Create and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders.
Travel up to 30% of the time.
Communicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.",None Found,"BA/BS degree in Computer Science, Mathematics or related technical field, or equivalent practical experience.
Experience with data processing software (such as Hadoop, Spark, Pig, Hive) and with data processing algorithms (MapReduce, Spark).
Experience in writing software in one or more languages such as Java, C++, Python, Go and/or R.
Experience working data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments.
Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments such as Amazon Web Services, Azure and Google Cloud.
Experience working with big data, information retrieval, data mining or machine learning as well as experience in building multi-tier high availability applications with modern web technologies (such as NoSQL, MongoDB, SparkML, Tensorflow)","As a Cloud Data Engineer, you will guide customers on how to ingest, store, process, analyze and explore/visualize data on the Google Cloud. You will work on data migrations and transformational tasks, and with customers to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential platform issues.
In this role you are the engineer working with our most strategic Google Cloud customers. Together with the team you will support customer implementation of Google Cloud products through: architecture guidance, best practices, data migration, capacity planning, implementation, troubleshooting, monitoring and much more.
The Google Cloud team helps customers transform and evolve their business through the use of Google's global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you will help shape the future of businesses of all sizes use technology to connect with customers.
Responsibilities:
Act as a trusted technical advisor to customers and solve complex Big Data challenges.
Create and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders.
Travel up to 30% of the time.
Communicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.
Requirements:
BA/BS degree in Computer Science, Mathematics or related technical field, or equivalent practical experience.
Experience with data processing software (such as Hadoop, Spark, Pig, Hive) and with data processing algorithms (MapReduce, Spark).
Experience in writing software in one or more languages such as Java, C++, Python, Go and/or R.
Experience working data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments.
Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments such as Amazon Web Services, Azure and Google Cloud.
Experience working with big data, information retrieval, data mining or machine learning as well as experience in building multi-tier high availability applications with modern web technologies (such as NoSQL, MongoDB, SparkML, Tensorflow)"
98,Data Engineer,"Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"What We're About

Telaria (NYSE: TLRA), (formerly Tremor Video), is the leading independent data-driven software platform built to monetize and manage premium video inventory with the greatest speed, control, and transparency, wherever and however audiences are watching.

We are looking for Data Engineers to help us build tools, enhance our platform, and leverage our vast amounts of advertising data to make informed decisions around business optimizations and efficiencies. We're close to the customers and have the reward of seeing our work being used immediately. We take pride in the reliability and scalability of our platform, as well as our pace of implementation. We are a small and efficient team building out a solution in a new space with lots of green field ahead of it.

Why You'll Be Excited


Having a large stake and impact on the product and business direction and bottom-line
Collaborating with innovative and goal-focused engineering and business teams
Working with data scientists, data analysts, and product managers to identify and use the data that is most relevant to the problem at hand
Building systems that can effectively stream, store, and crunch vast amounts of data to help inform customers and power business analytics
Solving complex problems revolving around real-time strategic decision-making and large data systems
Developing, deploying, and maintaining robust and high-performance systems and features

Why We'll Be Excited About You


You have strong verbal and written communication skills that help you express your work in meaningful ways to cross functional teams
You are passionate about learning different technologies, exploring engineering challenges, and working in a dynamic and collaborative environment
You have working experience and skills designing and coding in Java/Scala and/or Python
You are proficient in writing efficient and well-structured SQL queries and have experience with database schemas and design
You have experience with big data technologies (Spark, Presto, Druid, etc.)
You have knowledge of UNIX/Linux and scripting with Perl, Shell, etc.
Degree in Computer Science or a related field
Bonus: Experience working in a data science / machine learning environment
Bonus: Experience working with AWS Services (Redshift, Kinesis, Glue, etc.)

Why We (and You'll) Love It Here


We are a technology and data-driven business
We embrace analytical thinking, kind, and results driven people
We have a plethora of challenging and interesting problems to solve
We help and support each other in creating a productive work/life balance

At Telaria we place an emphasis and importance on ensuring our total rewards are competitive, aligned with industry and to help you create a productive work/life balance. Benefits are highly subsidized and include medical, company paid dental, vision, employer contributed Health Savings Account, 401k matching, corporate gym discounts, pre-tax health and commuter savings, life insurance, 5 and 10-year Sabbatical programs, Discretionary Time Off (a.k.a. open vacation policy!), Paid Parental Leave, an Employee Referral Program, Employee Stock Purchase Plan (ESPP), and much more! All this is within a collaborative work environment you can personalize and topped with engaging programs like Micro-Mentorship, and Team Sports.

Telaria values diversity and is proud to be an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
99,"Data Strategy Specialist - Business & Data Analysis, Cloud, AWS, Azure, Big Data","San Jose, CA 95113",San Jose,CA,95113,None Found,None Found, 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:,None Found,None Found,None Found,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The North America Data Strategy & Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.

 Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model

Required Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:
o Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling
o Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.
o Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel
 Able to travel up to 100% (Mon-Thu)

Optional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model
 Experience in compiling business cases and roadmaps for data, analytics and technology investments

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
100,Senior Software Data Engineer,"Campbell, CA",Campbell,CA,None Found,None Found,"Knowledge of various persistence (RDBMS, noSQL, HDFS, Cassandra, Redis)
Understanding Data Catalog, Data Governance, Data Lineage
Experience with security, authentication in data platform
Bachelor’s Degree or above (Computer Science, Bio Engineering, Electronics and Electrical Engineering or any related field)
5+ years of experience in data engineering
Proficient understanding and experience in one or more programming languages (Java, JavaScript, Python etc.)
Experience with building stream-processing systems, using solutions such as Spark-Streaming or Flink
Experience with integration of data from multiple data sources
Experience with building data lakes and data warehouses by leveraging any of the major cloud providers (GCP, AWS or Azure) is highly desirable
Familiar with Hadoop ecosystem (HDFS, HBase etc.), especially Spark
Good knowledge of Big Data querying tools
Knowledge of various ETL techniques
Knowledge of messaging systems, such as Kafka or RabbitMQ
",None Found,"Knowledge of various persistence (RDBMS, noSQL, HDFS, Cassandra, Redis)
Understanding Data Catalog, Data Governance, Data Lineage
Experience with security, authentication in data platform
Bachelor’s Degree or above (Computer Science, Bio Engineering, Electronics and Electrical Engineering or any related field)
5+ years of experience in data engineering
Proficient understanding and experience in one or more programming languages (Java, JavaScript, Python etc.)
Experience with building stream-processing systems, using solutions such as Spark-Streaming or Flink
Experience with integration of data from multiple data sources
Experience with building data lakes and data warehouses by leveraging any of the major cloud providers (GCP, AWS or Azure) is highly desirable
Familiar with Hadoop ecosystem (HDFS, HBase etc.), especially Spark
Good knowledge of Big Data querying tools
Knowledge of various ETL techniques
Knowledge of messaging systems, such as Kafka or RabbitMQ
",None Found,None Found,"Description
The Senior Software Data Engineer codes software applications based on business requirements. The Senior Software Data Engineer work assignments involve moderately complex to complex issues where the analysis of situations or data requires an in-depth evaluation of variable factors.
Responsibilities
We are looking for a Data Engineer that will work on the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across teams in our group.
The Senior Software Data Engineer standardizes the quality assurance procedure for software. Oversees testing and debugging and develops fixes. Researches complaints and makes necessary adjustments and/or recommendations to resolve complex software related issues. Begins to influence department’s strategy. Makes decisions on moderately complex to complex issues regarding technical approach for project components, and work is performed without direction. Exercises considerable latitude in determining objectives and approaches to assignments.
Required Qualifications
Knowledge of various persistence (RDBMS, noSQL, HDFS, Cassandra, Redis)
Understanding Data Catalog, Data Governance, Data Lineage
Experience with security, authentication in data platform
Bachelor’s Degree or above (Computer Science, Bio Engineering, Electronics and Electrical Engineering or any related field)
5+ years of experience in data engineering
Proficient understanding and experience in one or more programming languages (Java, JavaScript, Python etc.)
Experience with building stream-processing systems, using solutions such as Spark-Streaming or Flink
Experience with integration of data from multiple data sources
Experience with building data lakes and data warehouses by leveraging any of the major cloud providers (GCP, AWS or Azure) is highly desirable
Familiar with Hadoop ecosystem (HDFS, HBase etc.), especially Spark
Good knowledge of Big Data querying tools
Knowledge of various ETL techniques
Knowledge of messaging systems, such as Kafka or RabbitMQ
Preferred Qualifications
Master's Degree
Additional Information
Scheduled Weekly Hours
40"
101,Business Analyst,"Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"ABOUT US
Lark is the world's largest A.I. healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with A.I. Nurses. We're on a mission to improve people's health and happiness through our digital health coach. We are the only A.I. nurse ever to become fully medically reimbursed to 100% replace a live nurse because we achieved equivalent health outcomes to live healthcare professionals - which allows for infinitely scalable healthcare. Since launch, Lark has continued to receive awards and accolades for both our product, and our leadership.

✦Apple's Top 10 Apps in the World
✧Business Insider's most innovative companies in the world along with Uber and Snapchat
✦Biz Journal's 100 Women of Influence

We are looking for a talented Data Engineer to join our growing team in Mountain View, CA, where you'll be building our next generation data pipelines.

ABOUT THE ROLE

What You'll Do:

Build our next generation data pipelines into a fast and efficient big-data system
You'll be the first fully dedicated data engineer on the team, and will be able to call the shots on strategy

What You'll Need:

A love of data, and the make-or-break effect it has on startups
Default to coding efficient systems from large databases, both RDBMS and noSQL.
Familiarity with the following key technologies (or similar):
Spark
Yarn
Kafka
Python
AWS

JOIN US
Our team works with cutting edge tools and technology related to Artificial Intelligence and Machine Learning. We are using NLP to process millions of meals, and accelerometer data to compute activity and sleep amounts from users' phones. Our chat AI is the most sophisticated digital health engagement tool in the world. Join us and make it even better!

Lark is an Equal Opportunity Employer. Lark does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need."
102,Data Engineer,"Sunnyvale, CA",Sunnyvale,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for a Data Engineer who is excited about building products that wrangle AV data to supercharge our customers. You will drive the design and development of data infrastructure across our products and internal tools. At Applied, we encourage all engineers to take ownership over technical and product decisions, closely interact with users to collect feedback, and contribute to a thoughtful, dynamic team culture.

At Applied, you will
Design powerful data pipelines that process fast sensor streams, leverage appropriate data stores, and offer easy-to-use APIs
Develop and deploy high-quality software using modern tooling and frameworks
Work with products and teams across Applied Intuition
Work with customers across the AV ecosystem to understand their needs and the innards of their data systems

We’re looking for someone who
Has 1.5+ years experience building scalable big data pipelines
Has experience with open source data processing frameworks (Spark, Kafka, etc.)
Has experience with different data storages (e.g., relational and NoSQL)
Has experience with containerization and other modern software development workflows
Takes initiative and ownership in a fast-paced environment

Nice to have
Expertise with multiple modern programming languages (Python, C++, Go, etc.)
Prior work in enterprise software, including on-prem and/or cloud deployments
Prior work in either autonomy or simulation products

Autonomy is one of the leading technological advances of this century that will come to impact our lives. The work you’ll do at Applied will meaningfully accelerate the efforts of the top autonomy teams in the world. At Applied, you will have a unique perspective on the development of cutting edge technology while working with major players across the industry and the globe."
103,Senior Data Engineer,"Menlo Park, CA 94025",Menlo Park,CA,94025,None Found,None Found,"MS in Computer Sciences
5+ years of experience in a relevant software engineering position
5+ years hands-on experience with Java/J2EE, Relational databases and NoSQL data stores
In-depth experience building scalable, distributed Analytical Systems
Proficient with Predictive Analytics, AI and Machine Learning
Experience using AWS or GCP
Comfortable with Linux environments
","
Design and develop a highly performant and highly scalable analytical system to collect, store, process and analyze genomic and food sample metadata, utilizing modern technologies.
Use GCP's features and apply AI/ML functionality to enhance our analytical capabilities.
Utilize DDD, Event Sourcing and CQRS to build Microservices and API's for the analytical system.
Suggest, assess and translate system requirements into implementation designs and data models.
Ensure excellent quality and test coverage, as well as effective performance.
Work on timely resolution of issues and other tasks relevant to the position.
",None Found,"
Design and develop a highly performant and highly scalable analytical system to collect, store, process and analyze genomic and food sample metadata, utilizing modern technologies.
Use GCP's features and apply AI/ML functionality to enhance our analytical capabilities.
Utilize DDD, Event Sourcing and CQRS to build Microservices and API's for the analytical system.
Suggest, assess and translate system requirements into implementation designs and data models.
Ensure excellent quality and test coverage, as well as effective performance.
Work on timely resolution of issues and other tasks relevant to the position.
","The Senior Data Engineer, focused on Software and Analytics, will report to the VP of Engineering and be part of a team responsible for the development of our analytical platform. You will help drive the architecture and implementation of a robust, highly performant, and scalable analytical system. The role will require close collaboration with our Bioinformatics team, and involves building data pipelines and relevant backend services. This is a hands-on position, providing the opportunity to build an intelligent, next generation genetic processing and analytical platform that will reimagine how Food Safety is done across the industry!

We offer great opportunities to grow your career! Be part of an exiting, fast growing startup in the biotech sector! You can learn more about us at https://www.clearlabs.com/
Requirements
Responsibilities
Design and develop a highly performant and highly scalable analytical system to collect, store, process and analyze genomic and food sample metadata, utilizing modern technologies.
Use GCP's features and apply AI/ML functionality to enhance our analytical capabilities.
Utilize DDD, Event Sourcing and CQRS to build Microservices and API's for the analytical system.
Suggest, assess and translate system requirements into implementation designs and data models.
Ensure excellent quality and test coverage, as well as effective performance.
Work on timely resolution of issues and other tasks relevant to the position.
Desired Skills and Experience Requirements:
MS in Computer Sciences
5+ years of experience in a relevant software engineering position
5+ years hands-on experience with Java/J2EE, Relational databases and NoSQL data stores
In-depth experience building scalable, distributed Analytical Systems
Proficient with Predictive Analytics, AI and Machine Learning
Experience using AWS or GCP
Comfortable with Linux environments
Preferred:
Experience building Microservices and API's
Experience with LIMS or related software systems
Understanding of Bioinfomatic Pipelines
Exposure to Genomics and NGS
Benefits
Clear Labs offers solid Health Care, a 401k Plan, Food Catering, Massages, and an easy-going, open work environment."
104,Data Engineer,"Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Hiretual is an AI-powered sourcing platform, and has been recognized as one of the best recruiting tools on the market (
https://goo.gl/ERiQgY [https://goo.gl/ERiQgY]). During 2018, Hiretual achieves 500% growth with minimal focus on sales to date (
https://goo.gl/ZTA1D9 [https://goo.gl/ZTA1D9]).

As a data engineer engineer, you will join the core engineering team to build scalable and robust data engine towards an AI and data-driven recruiting SaaS. You will be working with top AI researchers and infrastructure gurus to explore unlimited career space.

The core technical skills you should have:

Strong computer science fundamentals: algorithms, data structures, and object-oriented programming
Strong coding capability with Python
must have 2+ years of experience in data processing pipeline, including data crawling, cleaning, processing, ETL
Proficient in working variant databases: MySQL, Redis, Cassandra, Elasticsearch, graph databases.
Proficient with big data processing frameworks: Spark, Hadoop, Hive, Kafka, EMR
Writing scalable REST APIs for web services

Benefits

Unlimited growth/promotion space
Competitive salary and options
401k matching program
Free and nice food
Comprehensive medical, dental, and life insurance
PTO policy
Commuter benefits
Fun, collaborative, and energetic team environment with nice office environment
Fun events for family!

"
105,Principal Data Engineer,"Santa Clara, CA 95054",Santa Clara,CA,95054,None Found,None Found,None Found,"
Architect and design secure, robust, fault-tolerant and highly scalable Master Data Management solution leveraging Informatica product suite
Performing hands-on development work with Informatica MDM products
Identifying opportunities to reuse strategic components and reduce over-customization
Maintaining the health of the MDM system, support invest and maintenance activities for MDM platform
Responsible for estimating the effort for work activities and assisting peers and matrix team for successful execution of work activities
Identify, document and communicate technical risks, issues and alternative solutions discovered during project
Develop and unit test data solutions, data integrations, data services
Use agile engineering practices and various data development technologies to rapidly develop creative and efficient data products
Align and integrate well with architects, data analysts, data modelers and other stakeholders
Communicate with other developers across teams, both as ad hoc problem solving, and check-ins and discussions with other initiatives
Support non-technical team members in understanding the technical implications of design decisions
Mentor and provide direction to data engineers and quality engineers, both on-site and offshore",None Found,"
Minimum 15 years of data engineering experience, with hands-on experience implementing Informatica MDM.
Minimum of 5+ years is must with implementing Informatica MDM products suite. Completed at least two Informatica MDM implementations from the scratch.
Related work experience in the areas of Master Data Management, Data Governance, deep experience in data warehousing and analysis.
MDM technical expertise across multiple domains, multiple technical areas: matching/merging/survivorship, data cleansing, workflows, UI, etc.
Excellent data integration skills, understanding solutioning Informatica ETL and Data Quality components of Informatica MDM solutions
Experience developing large scale MDM, data warehousing, Data Lake and data integration ‎projects.
Experience designing and developing complex multi-tiered application with Services Oriented Architecture (SOA) and Data Services
Hands-on implementation experience in Big Data technologies is preferred
Experience managing offshore development teams
Experience in performance monitoring and performance tuning
Experience in financial domain is a plus","Make Next Happen Now. For over 30 years, Silicon Valley Bank (SVB) has helped innovative companies and their investors move bold ideas forward, fast. SVB provides targeted banking services to companies of all sizes in innovation centers around the world.
The Information Management team at Silicon Valley Bank is responsible for delivering data solutions that support all lines of business across the organization. This includes providing data integration services for all batch data and real-time movement; managing and enhancing the data warehouse, Data Lake and dependent data marts; and providing support for analytics and business intelligence consumers.

SVB is seeking an experienced Master Data Management Professional who will partner with business leaders to help lead complex, multi-disciplinary projects. The incumbent will participate in design and lead development of technology solutions to address Master Data Management business needs. The successful candidate will help develop the next generation solutions using Informatica MDM products. The position requires in-depth experience and understanding of data management, master data management tools, data warehousing and data integration skillsets. This position requires close interaction and collaboration with Project Managers, Lead Business Analysts, application and enterprise architects, application technical leads, vendor partners (both onsite and offshore), SVB offshore developments teams and other data and application development teams.

Responsibilities include, but are not limited to:

Architect and design secure, robust, fault-tolerant and highly scalable Master Data Management solution leveraging Informatica product suite
Performing hands-on development work with Informatica MDM products
Identifying opportunities to reuse strategic components and reduce over-customization
Maintaining the health of the MDM system, support invest and maintenance activities for MDM platform
Responsible for estimating the effort for work activities and assisting peers and matrix team for successful execution of work activities
Identify, document and communicate technical risks, issues and alternative solutions discovered during project
Develop and unit test data solutions, data integrations, data services
Use agile engineering practices and various data development technologies to rapidly develop creative and efficient data products
Align and integrate well with architects, data analysts, data modelers and other stakeholders
Communicate with other developers across teams, both as ad hoc problem solving, and check-ins and discussions with other initiatives
Support non-technical team members in understanding the technical implications of design decisions
Mentor and provide direction to data engineers and quality engineers, both on-site and offshore

Core Technical Requirements of the role
Minimum 15 years of data engineering experience, with hands-on experience implementing Informatica MDM.
Minimum of 5+ years is must with implementing Informatica MDM products suite. Completed at least two Informatica MDM implementations from the scratch.
Related work experience in the areas of Master Data Management, Data Governance, deep experience in data warehousing and analysis.
MDM technical expertise across multiple domains, multiple technical areas: matching/merging/survivorship, data cleansing, workflows, UI, etc.
Excellent data integration skills, understanding solutioning Informatica ETL and Data Quality components of Informatica MDM solutions
Experience developing large scale MDM, data warehousing, Data Lake and data integration ‎projects.
Experience designing and developing complex multi-tiered application with Services Oriented Architecture (SOA) and Data Services
Hands-on implementation experience in Big Data technologies is preferred
Experience managing offshore development teams
Experience in performance monitoring and performance tuning
Experience in financial domain is a plus


Qualifications

BS (Computer Science) or equivalent
15 years relevant industry experience"
106,"(Sr.) Manager, Data Management","San Carlos, CA",San Carlos,CA,None Found,None Found,"Bachelor’s or Master’s degree in computer science/data processing or equivalent
5+ years of experience with Java programming and developing frameworks
2+ years’ experience with Hadoop and Spark
2+ years’ experience with Amazon EMR/EC2 (or equivalent)
2+ years’ experience with Python
Experience with Bitbucket and a solid understanding of core concepts with Git
Familiarity with Linux
Familiarity with Jenkins and CI/CD
A solid understanding of basic core computer science concepts
Experience with AWS technologies such as Aurora, Athena, EMR, Redshift, S3
Experience with Postgres and MySql
Excellent Organizational and Project Management skills
Outstanding communication skills
Spark - Machine Learning library experience a plus
Experience with Scala, is a plus
Experience with Talend Data Integration (Big Data) platform a plus",None Found,"
In data management, data access (Big Data, traditional Data Marts, ...).
In Advanced programming (python, Shell scripting, and Java)
With interactive and batch processing using Spark SQL and spark scripting.
In applied data technologies:
Hadoop
Spark
Kafka, Spark Streaming
Pig
Hive
MongoDB
Oozie
EMR
Lambda
SQL
Current data warehousing concepts (using technologies like Redshift, Spark, Hadoop, web services, etc to support business-driven decisioning)
In data architecture and data assembly
In Data Governance and Data Security",None Found,None Found,"Overview
ABOUT OPORTUN

Oportun is a mission-driven, technology-powered provider of inclusive, affordable financial services and a certified Community Development Financial Institution (CDFI).
We seek to serve the 100 million people in the US who are shut out of the financial mainstream because they are credit invisible or are mis-scored because they have limited credit history. By lending money to hardworking, low-to-moderate income individuals, we help them move forward in their lives, demonstrate their creditworthiness, and establish the credit history they need to access new opportunities.
Since 2006, we have lent over $6.8 billion through over 3.1 million affordable small dollar loans and have helped over 730,000 people start establishing credit. In recognition of inventive approach, we were recognized by Time Magazine as one of 50 Genius Companies inventing the future.

The Bay Area News Group recognized Oportun as a Top Workplace in 2019. Come and be a part of our community of employees, partners, and customers who are devoted to expanding financial opportunity for millions. When we work together, we can make life better.

SUMMARY
Do you want to be part of a BIG data transformation journey? Do you love exploring new avenues and pioneer things in the technology space? Do you love designing and implementing business critical data management & engineering solutions using emerging technologies? Do you enjoy solving complex business problems in a fast-paced, collaborative, and iterative delivery environment? If this excites you, then keep reading!

We're seeking a hands-on Data Engineer that can design, code and provide architecture solutions for the team. The right candidate for this role is passionate about technology, can interact with product owners and technical stakeholders, thrives under pressure, and is hyper-focused on delivering exceptional results with good teamwork skills. The candidate will have the opportunity to influence and interact with fellow technologists beyond his team and influence technology partners across the enterprise.

Essential Functions:
Design and Develop scalable Big Data solutions across the entire data supply chain.
Create or implement solutions for metadata management.
Create and review technical and user-focused documentation for data solutions (data models, data dictionaries, business glossaries, process and data flows, architecture diagrams, etc.).
Extend and enhance the business Data Warehouse and Data Lake
Solve for complex data integrations across multiple systems.
Design and execute strategies for real-time data analysis and decisioning.
Collaborate with management, business partners, analysts, developers, architects, and engineers to support data quality efforts.
Work closely with the Data Science team to improve actionable data
Be open and willing to learn new skills!
Responsibilities
Expertise (can teach/instruct others):
In data management, data access (Big Data, traditional Data Marts, ...).
In Advanced programming (python, Shell scripting, and Java)
With interactive and batch processing using Spark SQL and spark scripting.
In applied data technologies:
Hadoop
Spark
Kafka, Spark Streaming
Pig
Hive
MongoDB
Oozie
EMR
Lambda
SQL
Current data warehousing concepts (using technologies like Redshift, Spark, Hadoop, web services, etc to support business-driven decisioning)
In data architecture and data assembly
In Data Governance and Data Security
Experience (requires little direction):
with functional requirements, detailed technical specifications, and test cases for new or modified projects
with and understanding of data sources (e.g., 3rd party RDBMS, MS access, SQL server, Oracle, and MySQL)
with data integration tools (e.g., Talend, SIS, Cascading)
with data manipulation scripting languages
with Business Intelligence, MDM, XML, SOA/WebServices
with executing deliverables using Agile
with Data Science toolsets and technology
Qualifications
Bachelor’s or Master’s degree in computer science/data processing or equivalent
5+ years of experience with Java programming and developing frameworks
2+ years’ experience with Hadoop and Spark
2+ years’ experience with Amazon EMR/EC2 (or equivalent)
2+ years’ experience with Python
Experience with Bitbucket and a solid understanding of core concepts with Git
Familiarity with Linux
Familiarity with Jenkins and CI/CD
A solid understanding of basic core computer science concepts
Experience with AWS technologies such as Aurora, Athena, EMR, Redshift, S3
Experience with Postgres and MySql
Excellent Organizational and Project Management skills
Outstanding communication skills
Spark - Machine Learning library experience a plus
Experience with Scala, is a plus
Experience with Talend Data Integration (Big Data) platform a plus"
107,Big Data & Cloud (AWS) Technology Manager,"San Jose, CA 95113",San Jose,CA,95113,None Found,"
Minimum 5 plus years of hands-on technical experience implementing & architecting Big Data solutions utilizing Hadoop.
Minimum 5 plus years of experience with a full life cycle development from functional design to deployment
Minimum 5 plus years of hands-on technical experience with delivering Big Data Solutions in the cloud with AWS or Azure. As it relates to data in the cloud, candidates should have demonstrated past experience of: Streaming Data, Batch Data, Data Lakes and Big Data Cloud Computing.
Minimum of 2 years of hands on experience with serverless architecture
Bachelor's degree or equivalent years of work experience
Ability to travel 100%, Monday- Thursday throughout North America
Certification: AWS Solutions Architect - Professional (preferred) or AWS Solutions Architect - Associate.",None Found,None Found,None Found,"Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills","Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements and the way we collaborate, operate and deliver value provides an unparalleled opportunity to grow and advance. Choose Accenture, and make delivering innovative work part of your extraordinary career.

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward.

Business & Technology Integration professionals advise upon, design, develop and/or deliver technology solutions that support best practice business changes

The Bus&Industry Integration Arch Mgr aligning technology with business strategy and goals they working directly with the client gathering requirements to analyze, design and/or implement technology best practice business changes. They are sought out as experts internally and externally for their deep functional or industry expertise, domain knowledge, or offering expertise. They enhance Accenture's marketplace reputation.

Job Description

Data and Analytics professionals define strategies, develop and deliver solutions that enable the collection, processing and management of information from one or more sources, and the subsequent delivery of information to audiences in support of key business processes.

Data Management professionals define strategies and develop/deliver solutions and processes for managing enterprise-wide data throughout the data lifecycle from capture to processing to usage across all layers of the application architecture.

A professional at this position level within Accenture has the following responsibilities:
Identifies, assesses and solves complex business problems for area of responsibility, where analysis of situations or data requires an in-depth evaluation of variable factors.
Closely follows the strategic direction set by senior management when establishing near term goals.
Interacts with senior management at a client and/or within Accenture on matters where they may need to gain acceptance on an alternate approach.
Has some latitude in decision-making. Acts independently to determine methods and procedures on new assignments.
Decisions have a major day to day impact on area of responsibility.
Manages large - medium sized teams and/or work efforts (if in an individual contributor role) at a client or within Accenture.

This position requires 100% travel, Monday – Thursday

Basic Qualifications

Minimum 5 plus years of hands-on technical experience implementing & architecting Big Data solutions utilizing Hadoop.
Minimum 5 plus years of experience with a full life cycle development from functional design to deployment
Minimum 5 plus years of hands-on technical experience with delivering Big Data Solutions in the cloud with AWS or Azure. As it relates to data in the cloud, candidates should have demonstrated past experience of: Streaming Data, Batch Data, Data Lakes and Big Data Cloud Computing.
Minimum of 2 years of hands on experience with serverless architecture
Bachelor's degree or equivalent years of work experience
Ability to travel 100%, Monday- Thursday throughout North America
Certification: AWS Solutions Architect - Professional (preferred) or AWS Solutions Architect - Associate.
Professional Skill Requirements

Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills
All of our consulting professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a federal contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process."
108,Senior Data Engineer,"San Mateo, CA",San Mateo,CA,None Found,None Found,"
Master's or Bachelor's Degree in Computer Science or equivalent experience
Self-motivated individual with strong problem solving skills
Comfortable and adaptable in a fast-paced and informal environment
Excellent verbal and written communication skills with the ability to interact at all levels of the organization
Detail-oriented, analytical, methodical
Creative thinker committed to driving quality forward
Passionate about data and what we can extract from it
Strong interest in functional programming
Familiar with distributed systems (sharding, lambda arch., streaming arch., etc.)
Experience with big data (Cassandra, Hbase, Hadoop, Redshift, etc.)
Excellent grasp of statistics
Comfortable working in a distributed team
2+ year of experience with Apache Kafka
1+ year of experience with distributed streaming architecture
2+ years of experience building analytics systems
",None Found,None Found,None Found,None Found,"Rakuten Ready (formerly Curbside) builds a next generation cloud platform to connect stores and restaurants for mobile ordering. Rakuten Ready platform bridges the divide between online and bricks-and-mortar stores and restaurants, delivering mobile convenience for shoppers in the communities in which they live, work and play. Customers include Nordstrom, CVS, Pizza Hut and Boston Market. Rakuten Ready fosters a work environment that is respectful, fair and welcoming and we hire according to these values.

We are searching for a Senior Data Engineer to join our team! This position requires a self-motivated individual with excellent problem solving skills who can think of, put forward, design and implement efficient and scalable solutions to challenging problems. The prime directive is to be a key contributor in the development of our platform and its analytics. Moreover, significant contribution to the architecture of the system along with coaching of more junior members of the team is expected. Responsibilities may include, but are not limited to the following:


Follow the evidence to track down / root cause issues arising from a real world location-based service
Develop and maintain tools for discovering and analyzing issues and reporting of location based services problems
Prepare detailed and concise technical reports on a weekly basis documenting top issues and progress toward determining root cause and/or corrective action implementation/verification
Design and carry out experiments/simulations to study factors that influence the accuracy of a location based service;
Aggregate location data to study factors that influence the accuracy of a location based service;
Develop appropriate test plans and production monitoring plans

Qualifications:

Master's or Bachelor's Degree in Computer Science or equivalent experience
Self-motivated individual with strong problem solving skills
Comfortable and adaptable in a fast-paced and informal environment
Excellent verbal and written communication skills with the ability to interact at all levels of the organization
Detail-oriented, analytical, methodical
Creative thinker committed to driving quality forward
Passionate about data and what we can extract from it
Strong interest in functional programming
Familiar with distributed systems (sharding, lambda arch., streaming arch., etc.)
Experience with big data (Cassandra, Hbase, Hadoop, Redshift, etc.)
Excellent grasp of statistics
Comfortable working in a distributed team
2+ year of experience with Apache Kafka
1+ year of experience with distributed streaming architecture
2+ years of experience building analytics systems

Preferred Qualifications


1+ year of experience in measurement, data processing or data analysis
1+ year of experience with Clojure

"
109,"Senior Data Engineer (Outward, Inc.)","San Jose, CA 95112",San Jose,CA,95112,None Found,"
Bachelor's degree or equivalent work experience
6+ years of experience with software engineering, data engineering, data visualization, and data-mining
Strong Python and/or Scala development experience
4+ years of SQL (Hive, Oracle, MySQL, PostgreSQL) experience
Professional experience using XML
Experience with visualization frameworks
Experience analyzing data to identify deliverables, gaps and inconsistencies
Experience initiating and driving projects to completion
Experience communicating the results of analyses to stakeholders
Experience with AWS services like lambda, Cloud Formation, RDS, EC2, IAM
",None Found,"
Work with Engineers, Product Owners, and Designers to understand their data needs
Automate frequently requested analyses using Python
Evaluate and define critical business metrics and identify new levers to help move these metrics
Design and evaluate A/B experiments
Monitor key product metrics and identify root causes behind anomalies
Build and analyze dashboards and reports
Influence product teams through a presentation of data-based recommendations
Communicate state of business and experiment results to product teams",None Found,None Found,"Location: San Jose, CA

About Outward, Inc.

Outward, Inc. is based in San Jose, CA and is a wholly owned subsidiary of Williams Sonoma, Inc. ( www.outwardinc.com )

At Outward Inc. our vision is to 'lower the friction' with regards to all aspects of the customer journey for our parent company and our retail customers. We do this by offering new technology solutions that enable new experiences and top-notch visualizations of their products. We are continuously pushing the boundaries of how 3D and AR/ VR technologies will drive the next generation shopping experience.

Through our portfolio of premium lifestyle brands - our mission is to deepen consumer connections with the products that matter and deliver an innovative experience.

We are positioned as a technology leader in the visual merchandising space for retail, with a focus on improving customer experiences with next-generation product visualizations.

Come and join a growing team of engineers as we solve technological riddles and push the envelope of what can be done on the web!


Responsibilities:

Work with Engineers, Product Owners, and Designers to understand their data needs
Automate frequently requested analyses using Python
Evaluate and define critical business metrics and identify new levers to help move these metrics
Design and evaluate A/B experiments
Monitor key product metrics and identify root causes behind anomalies
Build and analyze dashboards and reports
Influence product teams through a presentation of data-based recommendations
Communicate state of business and experiment results to product teams
Qualifications:

Bachelor's degree or equivalent work experience
6+ years of experience with software engineering, data engineering, data visualization, and data-mining
Strong Python and/or Scala development experience
4+ years of SQL (Hive, Oracle, MySQL, PostgreSQL) experience
Professional experience using XML
Experience with visualization frameworks
Experience analyzing data to identify deliverables, gaps and inconsistencies
Experience initiating and driving projects to completion
Experience communicating the results of analyses to stakeholders
Experience with AWS services like lambda, Cloud Formation, RDS, EC2, IAM


Outward's Benefits & Perks

Medical, Dental, Vision, & 401K
Floating holidays, PTO, and game nights
Company-Sponsored Team Events & Staff Parties
Recently Relocated Headquarters, Great for Commuters!
Twice Weekly Catered Lunch
Onsite gym & showers (San Jose office)
Free healthy snacks
Dog-friendly, bring your furry pal with you to work
Tax-free commuter benefits
A wellness program that supports your physical, financial and emotional health
A smart casual work environment
Time off to volunteer
Additional discounts on nearby gyms and other local businesses


This position will not offer relocation assistance or remote work.

Outward, Inc. is an Equal Opportunity Employer.

Outward, Inc. will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the California Fair Employment Act (AB 1008), or other applicable state or local laws and ordinances.

#LI-JQ1

-"
110,Network Data Engineer -- Data Modelling,"San Jose, CA 95131",San Jose,CA,95131,None Found,None Found,None Found,None Found,None Found,None Found,"What you’ll be doing...
SDN Planning group under Verizon Network Technology and Planning organization is looking for self-motivated and innovative engineer in support of Verizon SDN architecture and technology development.As a senior level member of technical staff, you will be interacting with software engineers, network/system operations staff, network/system architects and vendor partners to provide SDN technology evolution strategies and solutions to keep our service relevance in the market place in the fast moving and quickly evolving networking industry. Proactive technology research, industry trend analysis, and developing next generation network architecture using modern networking technology (e.g., Software Defined Networking) and providing production deployable solutions are key functions of the team.
Responsibilities include:
New technology validation and prototyping.
Software Defined Networking (SDN) ecosystem development and technical vendor management.
SDN/NFV platform architecture and development.
Design and develop machine learning and deep learning models for real world, large scale problems in computer networks.
Develop data collections, labeling pipelines, and evaluation pipelines.
Optimize models for on-device and multi-modal intelligence.
Design and implement software applications using Machine Learning and Artificial Intelligence for data verification, transformation, and analytics.
Open API development and verification in support of Verizon SDN platform infrastructure.
Linux application deployment and linux networking lab infrastructure support.
Linux scripting, networking and administration knowledge.
Participate in open-source and open standard industry collaboration activities.
What we’re looking for...
You’ll need to have:
Bachelor’s degree or four or more years of work experience.
Six or more years of relevant work experience.
Experience developing applications using Machine Learning algorithms.
Even better if you have:
Master’s degree or PhD degree in Computer Science or Electrical Engineering with modern data communication technology discipline.
Experience on machine learning algorithms, from supervised and unsupervised to reinforcement learning
Java, C/C++, and/or Python programming skills.
Experience with one or more of the following: artificial neural networks, classification, pattern recognition, recommendation systems, targeting systems, ranking systems or similar.
Experience in analyzing sophisticated and dynamic patterns.
Five or more years of data communication industry experience.
Network (WAN/LAN) Engineering (both Layer 2 and 3) experience in Service Provider Network or Enterprise Network environment.
SDN knowledge and development experience.
Experience on with automation, Machine Learning, and Deep Learning tools and applications (TensorFlow, RPA, etc.).
Experience in understanding the complexity of the algorithms as well as in optimizing algorithms.
Knowledge of Packet, Optical, and Wireless network technologies.
Industry technology leadership skill.
Multi-vendor system integration and technical vendor management experience.
Open source tools development, implementation, and collaboration experience (Robot Framework, Jenkins, Phabricator, Kafka, Screwdriver, etc.).
Open networking, open API, and open platform development experience.
When you join Verizon...
You’ll be doing work that matters alongside other talented people, transforming the way people, businesses and things connect with each other. Beyond powering America’s fastest and most reliable network, we’re leading the way in broadband, cloud and security solutions, Internet of Things and innovating in areas such as, video entertainment. Of course, we will offer you great pay and benefits, but we’re about more than that. Verizon is a place where you can craft your own path to greatness. Whether you think in code, words, pictures or numbers, find your future at Verizon.
Equal Employment Opportunity
We're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better."
111,Senior Software Development Engineer,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"4+ years of professional software development experience3+ years of programming experience with at least one modern language such as Java, C++, or C# including object-oriented design2+ years of experience contributing to the architecture and design (architecture, design patterns, reliability and scaling) of new and current systemsBachelor’s Degree in Computer Science or related field3+ years of experience with a major scripting language (e.g. Python)3+ years of experience with an industry standard relational database and SQL

Amazon Advertising is dedicated to driving measurable outcomes for brand advertisers, agencies, authors, and entrepreneurs. Our ad solutions—including sponsored, display, video, and custom ads—leverage Amazon’s innovations and insights to find, attract, and engage intended audiences throughout their daily journeys. With a range of flexible pricing and buying models, including self-service, managed service, and programmatic ad buying, these solutions help businesses build brand awareness, increase product sales, and more.

The Team:
Forecasting and dynamic pricing team builds end-to-end solutions including data pipelines, machine learning models, large scale data structures and indexes, advertiser recommendations (bids, products) and data visualizations. We match supply (human eyeballs) and demand (advertisers interests) in thousands of audience targeting dimensions, and recommend optimal prices.

The Role:
The team is seeking an experienced backend and data engineer, who will own backend data pipelines, indexing systems and service & API layer application support. These systems are to be used by hundreds of internal and tens of thousands of self-serve external users. To be successful in this role, you will need to have a passion for backend, data and service architecture and engineering.
In this role, you will:
Own and build large-scale data processing and indexing pipelinesOwn development stack of applications and strive to use new technology packagesOwn services and their health, performance and service-level metricsCollaborate with business and product partners to build applications
In this team, you’ll experience the benefits of working in a dynamic, entrepreneurial environment, in our offices located in downtown Palo Alto, California.

Experience with large-scale data processing and management frameworks such as Hadoop and SparkExperience in backend and server-side development concepts such as multi-threading, concurrency, and fleet management.Experience in database technologies such as MySQL, Oracle, and RedshiftExperience with distributed indexing and storage systems such as Roaring Bitmaps, Redis, and ElasticSearchMasters degree in Computer Science or related disciplines preferred
#PABDSDE3

#PABDSDE3ADS"
112,Data Engineer II,"Pleasanton, CA 94588",Pleasanton,CA,94588,None Found,"
2+ years proven experience in developing and deploying in the Cloud; Azure and Snowflake experience is a plus
2+ years proven expertise in creating pipelines for real time and near real time integration
2+ years proven experience with Spark SQL, Spark Streaming and using Core Spark API to explore Spark features to build data pipelines
Databricks and Delta table knowledge is a plus
Extensive experience in data transformations for various business use cases.
Knowledge for handling exceptions and automated re-processing and reconciling
Passion for Data Quality with an ability to integrate these capabilities into the deliverables
Prior use of Big Data components and the ability to rationalize and align their fit for a business case
Experience in working with different data sources - flat files, XML files and databases
Proficiency in techniques for slowly changing dimensions
Knowledge of Jenkins for continuous integration and End-to-End automation for application build and deployments
Proven experience and ability to work with people across the organization
Proven capabilities for strong written and oral communication skills
Ability to integrate into a project team environment and contribute to project planning activities
Experience in developing implementation plans and schedules and preparing documentation for the jobs according to the business requirements",None Found,"
Create re-usable components
Auditability and logging
Governance and Security
Standards for DevOps manageability
Data Quality integrated
Configuration driven",None Found,None Found,"Albertsons-Safeway Company is one of the largest food and drug retailers with 2,300+ stores. The Albertsons-Safeway family of brands includes some of the most prominent brands in food retailing, with a growing base of loyal shoppers. Thanks to the professionalism, diversity, spirit and friendliness of our people, we have locations across the U.S.

The Information Technology Department has an opening for a Data Engineer II. This position is located in Pleasanton, California.


Position Purpose

Work on various projects to optimize data collection, integration, aggregation, analysis, and data visualizations that support business objectives in a BigData Cloud environment.


Key Responsibilities include, but are not limited to:

Work in conjunction with other augmented staff to ingest data from varying technologies including internal and external sources. Incremental and historical data are to be ingested with an emphasis on streaming and a framework - Content Integration Network (CIN) will be developed with a goal to:

Create re-usable components
Auditability and logging
Governance and Security
Standards for DevOps manageability
Data Quality integrated
Configuration driven


Qualifications:

2+ years proven experience in developing and deploying in the Cloud; Azure and Snowflake experience is a plus
2+ years proven expertise in creating pipelines for real time and near real time integration
2+ years proven experience with Spark SQL, Spark Streaming and using Core Spark API to explore Spark features to build data pipelines
Databricks and Delta table knowledge is a plus
Extensive experience in data transformations for various business use cases.
Knowledge for handling exceptions and automated re-processing and reconciling
Passion for Data Quality with an ability to integrate these capabilities into the deliverables
Prior use of Big Data components and the ability to rationalize and align their fit for a business case
Experience in working with different data sources - flat files, XML files and databases
Proficiency in techniques for slowly changing dimensions
Knowledge of Jenkins for continuous integration and End-to-End automation for application build and deployments
Proven experience and ability to work with people across the organization
Proven capabilities for strong written and oral communication skills
Ability to integrate into a project team environment and contribute to project planning activities
Experience in developing implementation plans and schedules and preparing documentation for the jobs according to the business requirements


How to Apply: Interested candidates are encouraged to submit a resume by visiting https://www.albertsonscompanies.com/careers.html


Diversity is fundamental at Albertsons-Safeway. We foster an inclusive working environment where the different strengths and perspectives of each employee is both recognized and valued. We believe that building successful relationships with our customers and our communities is only possible through the diversity of our people. A diverse workforce leads to better teamwork and creative thinking, as well as mutual understanding and respect.


The Albertsons-Safeway policy is to provide employment, training, compensation, promotion and other conditions of employment without regard to race, color, religion, sexual orientation, gender identity, national origin, sex, age, disability, veteran status, medical condition, marital status or any other legally protected status.


We support a drug-free workplace - all applicants offered a position are required to pass a pre-employment drug test before they are hired.


AN EQUAL OPPORTUNITY EMPLOYER"
113,Senior Data Engineer- ML and AI,"Santa Clara, CA 95054",Santa Clara,CA,95054,None Found,None Found,None Found,"
Create and maintain optimal data and model dataOps pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud-based ‘big data’ technologies from AWS, Azure and others.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep data separated and secure across national boundaries through multiple data centers and strategic customers/partners.
Create tool-chains for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and machine learning experts to strive for greater functionality in our data and model life cycle management systems.
Support dataOps competence build-up in Ericsson Businesses and Customer Serving Units",None Found,None Found,"Date: Sep 26, 2019
Ericsson Overview:

Ericsson is world’s leading provider of communications technology and services. Our offerings include services, consulting, software and infrastructure within Information and Communications Technology.
Using innovation to empower people, business and society, Ericsson is working towards the Networked Society: a world connected in real time that will open up opportunities to create freedom, transform society and drive solutions to some of our planet’s greatest challenges.

We are truly a global company, operating across borders in over 180 countries, offering a diverse, performance-driven culture and an innovative and engaging environment. As an Ericsson employee, you will have freedom to think big and the support to turn ideas into achievements. Continuous learning and growth opportunities allow you to acquire the knowledge and skills necessary to progress and reach your career goals. We invite you to join our team.

Artificial Intelligence at Ericsson

Ericsson has setup Global AI Accelerator (GAIA) unit to accelerate our AI and Machine Learning offerings. This central unit is chartered with the responsibility of building a high-end competence center that brings together a team of high-end Data Scientists, Data Engineers, Software Engineers and Machine Learning Architects towards the goal of enhancing Ericsson’s products and services portfolio with AI capabilities.

Ericsson is hiring experienced Senior Data Engineers to significantly expand its Global AI Accelerator team in Santa Clara or San Francisco.

Job Description:

 As a Senior-level Contributor , you will be evolving and optimizing our data and data pipeline architecture, as well as, optimizing data flow and collection for cross functional teams. You are an expert data pipeline builder and data wrangler who enjoys optimizing data systems and evolving them. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data and models devOps (dataOps) architecture is consistent throughout ongoing projects. You are self-directed and comfortable supporting the dataOps needs of multiple teams, systems and products. You will also be responsible for integrating them with the architecture used across the company. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s dataOps architecture to support our existing and next generation of MI-driven products and solutions initiatives.

Your Roles and Responsibilities

Create and maintain optimal data and model dataOps pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud-based ‘big data’ technologies from AWS, Azure and others.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep data separated and secure across national boundaries through multiple data centers and strategic customers/partners.
Create tool-chains for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and machine learning experts to strive for greater functionality in our data and model life cycle management systems.
Support dataOps competence build-up in Ericsson Businesses and Customer Serving Units

What we would like to see

MS or PhD degree in Computer Science, Informatics, Information Systems or another related field.
5+ years experience using the following software/tools:
Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with Data and Model pipeline and workflow management tools: Azkaban, Luigi, Airflow, Dataiku, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
You have experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and seek opportunities for improvement.
You have strong analytic skills related to working with unstructured datasets.
You have built processes supporting data transformation, data structures, metadata, dependency and workload management.
You have advanced SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of other databases/date-sources.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and interpersonal skills.
Experience supporting and working with cross-functional teams in a dynamic environment.

#LI-BF1
#MANAHJ

DISCLAIMER: The above statements are intended to describe the general nature and level of work being performed by employees assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties and skills required of employees assigned to this position. Therefore employees assigned may be required to perform additional job tasks required by the manager.

We are proud to be an EEO/AA employer M/F/Disabled/Veterans. We maintain a drug-free workplace and perform pre-employment substance abuse testing.

Ericsson provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, pregnancy, parental status, national origin, ethnic background, age, disability, political opinion, social status, protected veteran status, union membership or genetics information. Ericsson complies with applicable country, state and all local laws governing nondiscrimination in employment in every location across the world in which the company has facilities. In addition, Ericsson supports the UN Guiding Principles for Business and Human Rights and the United Nations Global Compact.

This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, training and development.

Ericsson expressly prohibits any form of workplace harassment based on race, color, religion, sex, sexual orientation, gender identity, marital status, pregnancy, parental status, national origin, ethnic background, age, disability, political opinion, social status, protected veteran status, union membership or genetic information.

Ericsson will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by Ericsson or (c) consistent with Ericsson’s legal duty to furnish information.

Employee Polygraph Protection Act Notice - Employers are generally prohibited from requiring or requesting any employee or job applicant to take a lie detector test, and from discharging, disciplining, or discriminating against an employee or prospective employee for refusing to take a test or for exercising other rights under the Act. For more information, visit https://www.dol.gov/whd/regs/compliance/posters/eppac.pdf.

Ericsson is an equal opportunity employer and is committed to providing reasonable accommodation for qualified disabled individuals during the application and hiring process. Ericsson will make modifications or adjustments to the job application or interview process that will enable a qualified applicant to be considered for a position. If you require an accommodation due to a disability, please contact Ericsson at hr.direct.dallas@ericsson.com or (866) 374-2272 (US) or (877) 338-9966 (Canada) for further assistance.

Primary country and city: United States (US) || || Santa Clara || R&D"
114,Data Engineer - Data Analytics,"Fremont, CA",Fremont,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for a Data Engineer to be part of our Applications Engineering team. This person will design, develop, maintain and support our Enterprise Data Warehouse & BI platform within Tesla using various data & BI tools, this position offers unique opportunity to make significant impact to the entire organization in developing data tools and driving data driven culture.

Responsibilities:
Work in a time constrained environment to analyze, design, develop and deliver Enterprise Data Warehouse solutions for Tesla’s Sales, Delivery and Logistics Teams
Create ETL pipelines using Python, Airflow
Create real time data streaming and processing using Open source technologies like Kafka , Spark etc
Work on creating data pipelines to maintain Datalake in AWS or Azure Cloud
Work with systems that handle sensitive data with strict SOX controls and change management processes
Develop collaborative relationships with key business sponsors and IT resources for the efficient resolution of work requests.
Provide timely and accurate estimates for newly proposed functionality enhancements
critical situation
Communicate technical and business topics, as appropriate, in a 360 degree fashion, when required; communicate using written, verbal and/or presentation materials as necessary.
Develop, enforce, and recommend enhancements to Applications in the area of standards, methodologies, compliance, and quality assurance practices; participate in design and code walkthroughs.
Utilize technical and domain knowledge to develop and implement effective solutions; provide hands on mentoring to team members through all phases of the Systems Development Life Cycle (SDLC) using Agile practices.

Qualifications:
Minimum Qualifications:
3+ years of experience in Cloud Technologies like AWS or Azure
3+ years of experience in creating data pipelines using Python
3+ years of experience in Data Modelling
Must have strong experience in Data Warehouse ETL design and development, methodologies, tools, processes and best practices
Strong experience in stellar dashboards and reports creation for C-level executives

Preferred Qualifications:
3+ years of development experience in Open Source technologies like Python, Java
Experience in Big Data processing using Apache Hadoop/Spark ecosystem applications like Hadoop, Hive, Spark, Kafka and HDFS preferable
Excellent query writing skill and communication skills
Familiarity with common API’s: REST, SOAP
Apply
Tesla participates in the E-Verify Program"
115,"Senior Data Engineer, Data Infrastructure","Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Founded by Dr. Priscilla Chan and Mark Zuckerberg in 2015, the Chan Zuckerberg Initiative (CZI) is a new kind of philanthropy that's demonstrating technology to help solve some of the world's toughest challenges – from eradicating disease, to improving education, to reforming the criminal justice system. Across three core Initiative focus areas of Science, Education and Justice and Opportunity, we're pairing engineering with grantmaking, impact investing, policy work, and movement building, to help build an inclusive, just and balanced future for everyone.

----------
Our Values
----------

We believe we can help build a future for everyone.


We aim to be daring, but humble: We look for bold ideas — regardless of structure and stage — and help them scale by pairing engineers with domain authorities to build tools that accelerate the pace of social progress.
We want to learn fast, but build for the long-term: We want to iterate fast and help bring new solutions to the table, but we also realize that important breakthroughs often take decades, or even centuries.
Stay close to the real problems: We engage directly in the communities we serve because no one understands our society's challenges like those who live them every day.

Our success is dependent on building teams that include people from different backgrounds and experiences who can challenge each other's assumptions with fresh perspectives. To that end, we look for a diverse pool of applicants including those from historically marginalized groups — women, people with disabilities, people of color, formerly incarcerated people, people who are lesbian, gay, bisexual, transgender, and/or gender nonconforming, first and second generation immigrants, veterans, and people from different socioeconomic backgrounds.

---------------
The Opportunity
---------------

By pairing engineers with leaders in our education, science, and justice and opportunity teams, we can bring technology to the table in new ways to help drive solutions. We are uniquely positioned to design, build, and scale software systems to help educators, scientists, and policy experts better address the myriad challenges they face. Our technology team is already helping schools bring personalized learning tools to teachers and schools across the country and supporting scientists around the world as they develop a comprehensive reference atlas of all cells in the human body.

The Infrastructure organization works on building shared tools and platforms to be used across all of the Chan Zuckerberg Initiative. Members of the data infrastructure engineering team have an impact on all of CZI's initiatives by enabling the technology solutions used by other engineering teams at CZI to scale. A person in this role will build these technology solutions and help to cultivate a culture of shared standard methodologies and knowledge around data engineering.

--------
You will
--------


Analyze and improve efficiency, stability, and security of CZI data warehouses across our Education, Science, and Justice & Opportunity initiatives
Design and build tools and libraries, to help 5+ engineering teams manage and query their data efficiently
Evangelize and educate teams across CZI on standard methodologies around data privacy and security, data operations, as well as improving data operations efficiency
Help teams improve their data operations by automating manual processes, optimizing data storage and delivery, re-designing infrastructure for scalability
Design and build ETL infrastructure used by our initiatives and across the organization
Work with Data Scientists and Data Analysts to optimize deriving insights from our data
Provide operational oversight for our data and optimized Data DevOps for the organization

--------
You have
--------


Experience with a scripting language such as Python, PHP, Ruby, or Perl
Experience with a systems language such as C, C++, C#, Go, Java or Scala
Experience developing data solutions on Snowflake, Redshift, Cosmos DB, or BigQuery
Experience with developing and maintaining custom or structured ETL processes and data pipelines
Experience working with relational SQL and NoSQL databases
Experience working with large disconnected datasets and help teams extract value from them
Experience working with cloud services such as AWS, Google Cloud, Azure

"
116,Sr. Big Data Engineer,"Mountain View, CA 94043",Mountain View,CA,94043,None Found,None Found,"7+ years of Python or Java/J2EE development experience
3+ years of demonstrated technical proficiency with Hadoop and big data projects
5-8 years of demonstrated experience and success in data modeling
Fluent in writing shell scripts [bash, korn]
Writing high-performance, reliable and maintainablecode.
Ability to write MapReduce jobs
Ability to setup, maintain, and implement Kafka topics and processes
",None Found,None Found,None Found,"Résumé du poste
Big Data Engineers serve as the backbone of the Strategic Analytics organization, ensuring both the reliability and applicability of the team’s data products to the entire Samsung organization. They have extensive experience with ETL design, coding, and testing patterns as well as engineering software platforms and large-scale data infrastructures. Big Data Engineers have the capability to architect highly scalable end-to-end pipeline using different open source tools, including building and operationalizing high-performance algorithms.


Big Data Engineers understand how to apply technologies to solve big data problems with expert knowledge in programming languages like Java, Python, Linux, PHP, Hive, Impala, and Spark. Extensive experience working with both 1) big data platforms and 2) real-time / streaming deliver of data is essential.


Big data engineers implement complex big data projects with a focus on collecting, parsing, managing, analyzing, and visualizing large sets of data to turn information into actionable deliverables across customer-facing platforms. They have a strong aptitude to decide on the needed hardware and software design and can guide the development of such designs through both proof of concepts and complete implementations.
Rôle et Responsabilités
Responsibilities include:
Translate complex functional and technical requirements into detailed design.
Design for now and future success
Hadoop technical development and implementation.
Loading from disparate data sets. by leveraging various big data technology e.g. Kafka
Pre-processing using Hive, Impala, Spark, and Pig
Design and implement data modeling
Maintain security and data privacy in an environment secured using Kerberos and LDAP
High-speed querying using in-memory technologies such as Spark.
Following and contributing best engineering practice for source control, release management, deployment etc
Production support, job scheduling/monitoring, ETL data quality, data freshness reporting
Skills Required:
7+ years of Python or Java/J2EE development experience
3+ years of demonstrated technical proficiency with Hadoop and big data projects
5-8 years of demonstrated experience and success in data modeling
Fluent in writing shell scripts [bash, korn]
Writing high-performance, reliable and maintainablecode.
Ability to write MapReduce jobs
Ability to setup, maintain, and implement Kafka topics and processes
Compétences et Qualifications
Samsung Electronics America, Inc. is committed to employing a diverse workforce, and provides Equal Employment Opportunity for all individuals regardless of race, color, religion, gender, age, national origin, marital status, sexual orientation, gender identity, status as a protected veteran, genetic information, status as a qualified individual with a disability, or any other characteristic protected by law.
* Please visit Samsung membership to see Privacy Policy, which defaults according to your location. You can change Country/Language at the bottom of the page. If you are European Economic Resident, please click here ."
117,Data Engineer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,"Experience working with at least one other big data platform like Apache Spark, Redshift, Teradata or SAP HANA.
Familiarity with streaming platforms like Apache Kafka, Amazon Kinesis etc.
Knowledge of Data Science, Machine Learning and Statistical Models is desirable.
Familiarity with Microsoft SSAS and Cubes and advanced Excel skills.
",None Found,None Found,None Found,"The Creative Cloud Engagement Analytics team is looking for a passionate Data Engineer to join our growing team of analytics experts. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Tapping into our massive product usage data sets, you will architect, build and optimize analytics platform and pipelines to harness our data, derive actionable insights to help guide the business with data. One must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing the data architecture to support our next generation of products and data initiatives.
What you’ll do
Architect, build and maintain scalable automated data pipelines ground up. Be an expert of stitching and calibrating data across various data sources.
Work with Adobe’s data ingestion, data platform and product teams to understand and validate instrumentation and data flow.
Develop data set processes for data modeling, mining and production.
Integrate new data management technologies and software engineering tools into existing structures
Support regular ad-hoc data querying and analysis to better understand customer behaviors.
Understand, monitor, QA, translate, collaborate with business teams to ensure ongoing data quality.
What you need to succeed
Bachelor’s degree in Computer Science, Information Systems or a related field is required, master’s preferred.
5-7 years of experience building and maintaining big data pipelines and/or analytical or reporting systems at scale.
Expert level skills working with Apache Hadoop and related technology stack like Pig, Hive, Oozie etc.
A strong proficiency in querying, manipulating and analyzing large data sets using SQL and/or SQL-like languages.
Approaching data organization challenges with a clear eye on what is important; employing the right approach/methods to make the maximum use of time and human resources.
Good attention to detail and ability to stitch and QA multiple data sources. Exploring new territories and finding creative and unusual ways to solve data management problems.
Be a self-starter.
Good interpersonal skills.
Preferred (but not required) Skills:
Experience working with at least one other big data platform like Apache Spark, Redshift, Teradata or SAP HANA.
Familiarity with streaming platforms like Apache Kafka, Amazon Kinesis etc.
Knowledge of Data Science, Machine Learning and Statistical Models is desirable.
Familiarity with Microsoft SSAS and Cubes and advanced Excel skills.
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status."
118,Data Engineer,"Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,"
Collaboration with data scientists, engineers, and business partners to understand data needs to drive key decision making throughout the company
Implementing a solid, robust, extensible data warehousing design that supports key business flows
Performing all of the necessary data transformations to populate data into a warehouse table structure that is optimized for reporting and analysis; Deploy inclusive data quality checks to ensure high quality of data
Developing strong subject matter expertise and manage the SLAs for those data pipelines
Set up and improve BI tooling and platforms to help the team create dynamic tools and reporting
Partnering with data scientists and business partners to develop internal data products to improve operational efficiencies organizationally
Building and growing partnership with cross functional teams, and evangelize data-driven culture
Contributing to innovations that fuel Confluent’s vision and mission",None Found,None Found,"Dubbed an ""open-source unicorn"" by Forbes, Confluent is the fastest-growing enterprise subscription company our investors have ever seen. And how are we growing so fast? By pioneering a new technology category with an event streaming platform, which enables companies to leverage their data as a continually updating stream of events, not as static snapshots. This innovation has led Sequoia Capital, Benchmark, and Index Ventures to recently invest a combined $125 million in our Series D financing. Our product has been adopted by Fortune 100 customers across all industries, and we’re being led by the best in the space—our founders were the original creators of Apache Kafka®. We’re looking for talented and amazing team players who want to accelerate our growth, while doing some of the best work of their careers. Join us as we build the next transformative technology platform!

The mission of the Data Science team at Confluent is to serve as the central nervous system of all things data for the company: we build analytics infrastructure, insights, models and tools, to empower data-driven thinking, and optimize every part of the business. Data Engineers on the team will be the enabler and amplifiers. This position offers limitless opportunities for an ambitious data science engineer to make an immediate and meaningful impact within a hyper growth start-up, and contribute to a highly engaged open source community.

We are looking for a talented and driven individual to build and scale our data analytics infrastructure and tooling. This person will build state of art data warehousing, ETL, and BI platforms, to make data accessible to the entire company. He/she will also partner closely with data scientists and cross functional leaders to develop internal data products. Data engineers are encouraged to think out of the box and play with the latest technologies while exploring their limits. Successful candidates will have strong technical capabilities, a can-do attitude, and are highly collaborative.
Responsibilities:
Collaboration with data scientists, engineers, and business partners to understand data needs to drive key decision making throughout the company
Implementing a solid, robust, extensible data warehousing design that supports key business flows
Performing all of the necessary data transformations to populate data into a warehouse table structure that is optimized for reporting and analysis; Deploy inclusive data quality checks to ensure high quality of data
Developing strong subject matter expertise and manage the SLAs for those data pipelines
Set up and improve BI tooling and platforms to help the team create dynamic tools and reporting
Partnering with data scientists and business partners to develop internal data products to improve operational efficiencies organizationally
Building and growing partnership with cross functional teams, and evangelize data-driven culture
Contributing to innovations that fuel Confluent’s vision and mission
What We're Looking For:
4+ years of experience in a Data Engineering role, with a focus on data warehouse technologies, data pipelines, BI tooling and/or data apps development
Bachelor or advanced degree in Computer Science, Mathematics, Statistics, Engineering, or related technical discipline
Highly proficient in Python and SQL coding
Highly proficient with tuning and optimizing data models and pipelines
Experience in developing data apps with Python, Javascript, high charts etc
The ability to communicate cross-functionally, derive requirements and architect shared datasets; ability to synthesize, simplify and explain complex problems to different types of audiences, including executives
What Gives You An Edge:
Experience with Apache Kafka
Experience with B2B enterprise apps data: Salesforce, Marketo, Zendesk, etc
Experience in developing data apps with Python, Javascript, high charts, etc
#LI-MT1

Come as you are
At Confluent, equality is a core tenant of our culture. We are committed to building an inclusive global team that represents a variety of backgrounds, perspectives, beliefs, and experiences. The more diverse we are, the richer our community and broader our impact."
119,"Data Engineer, Microservices","Palo Alto, CA 94303",Palo Alto,CA,94303,None Found,None Found,None Found,"
Design and deliver scalable solutions in a microservices environment
Troubleshoot, debug production issues and maintain system functionality
Collaborate with data scientists, project managers, internal and external stakeholders, and other engineers to achieve elegant solutions
Research emerging technologies, new features, processes for continuous improvement",None Found,"
3+ years experience building cloud native platforms including Kubernetes, microservices, containers and messaging components (Kafka, PubSub, Mulesoft etc).
4+ years experience developing applications/services in Java or Python
2+ years of working SQL knowledge and experience with relational databases including design, development, and data processing
Experience building distributed, Internet-scale systems.
Experience delivering applications on AWS, Google Cloud, or Microsoft Azure","Data engineers solve large scale problems at Adara from machine learning pipelines processing petabytes of data to managing APIs handling millions of requests per second. We build fast, we learn faster, and we are looking for self-starters to join our team. We are looking for engineers with an interest in solving data management and processing problems at scale in a cloud-native fashion.

As a Data Engineer, you will design, build, and deliver systems to handle billions of real-time events and securely deliver terabytes of data on a daily basis. You will be working with the Product and Data Science to deliver new initiatives as we expand our product solutions
Responsibilities:
Design and deliver scalable solutions in a microservices environment
Troubleshoot, debug production issues and maintain system functionality
Collaborate with data scientists, project managers, internal and external stakeholders, and other engineers to achieve elegant solutions
Research emerging technologies, new features, processes for continuous improvement
Requirements:
3+ years experience building cloud native platforms including Kubernetes, microservices, containers and messaging components (Kafka, PubSub, Mulesoft etc).
4+ years experience developing applications/services in Java or Python
2+ years of working SQL knowledge and experience with relational databases including design, development, and data processing
Experience building distributed, Internet-scale systems.
Experience delivering applications on AWS, Google Cloud, or Microsoft Azure
Preferred:
2+ years experience with either object-oriented programming or functional programming design patterns
Experience on Google Cloud Platform as ADARA Stack runs on GCP
Experience deploying systems into a production Cloud Native Environment
Experience with DevOps and Agile engineering practices
Experience with Java’s Spring or Python’s Flask frameworks
Experience with ETL tools and data quality management
Experience building applications leveraging NoSQL (Graph, Document, Key/Value)Experience designing/developing RESTful APIs (Bonus points for using Swagger)Experience with container-native development"
120,"Data Engineer, Data Center Products","Menlo Park, CA",Menlo Park,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
Are you passionate about data? Do you like working with big data? If yes, we want to talk to you! Data and associated analytics play a huge role in the success of Facebook. We have diverse business needs and very large-scale data. This makes it a wonderful and exciting challenge to provide scalable, actionable, reliable and timely data for our company. The Enterprise mission is to ship people-centric Enterprise solutions that transform and accelerate Facebook's business.

Facebook’s Enterprise Products team is responsible for building integrated, scalable, and robust enterprise applications. We are currently looking for a Data Engineer who has experience in building data products for supply chain, or is willing to learn within the space, to help enable our engineering teams to build smarter and more relevant solutions for Facebook. The Data Center Products team is responsible for building software that underpins operations in Facebook data centers. Our products enable scale, efficiency and quality in the work performed. A Data Engineer in this Data Center Products will help grow a data and metric driven culture by understanding how data center operations work and redefining how we look for and/or solve for operational efficiencies. DCP has 7 main verticals under its umbrella, and we're looking for someone to own one or more of these verticals, from idea conception to implementation. Moving from a more traditional retroactive batch analysis team to more predictive analytical one, gives us a plethora of opportunities to pursue within the organization, where you will work with our Operations Research Scientists and build out scalable models for your verticals. You will enjoy working on cutting edge technology, and see your insights turned into real products on a regular basis.

In this role, you’ll see a direct link between your work, company growth, and user satisfaction. You’ll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with stakeholders/teams and building scalable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness.

This is a full-time position based in our office in Menlo Park, CA.
RESPONSIBILITIES
Manage data warehouse plans for a product or a group of products.
Interface with engineers, product managers and product analysts to understand data needs.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data models in production.
Design, build and launch new data extraction, transformation and loading processes in production.
Support existing processes running in production.
Define and manage SLA for all data sets in allocated areas of ownership.
Work with data infrastructure to triage infra issues and drive to resolution.
MINIMUM QUALIFICATIONS
2+ years experience in the data warehouse space.
2+ years experience in custom ETL design, implementation and maintenance.
2+ years experience working with either a MapReduce or an MPP system.
2+ years experience with object-oriented programming languages.
2+ years experience with schema design and dimensional data modeling.
2+ years experience in creating SQL statements.
Experience analyzing data to identify deliverables, gaps and inconsistencies.
Experience managing and communicating data warehouse plans to internal clients.
PREFERRED QUALIFICATIONS
BS/BA in Technical Field, Computer Science or Mathematics
Knowledge in Python or Java
Facebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com."
121,"Senior Data Engineer, Growth","Los Gatos, CA 95032",Los Gatos,CA,95032,None Found,None Found,None Found,None Found,None Found,None Found,"Los Gatos, California
Infrastructure and Tooling
Netflix is re-imagining entertainment in 190 countries and on millions of devices, and our goal is to use data to optimize for the best customer experience. The growth data engineering team is responsible for acquisition related data that is important to optimize the signup experience and driving engagement. Our work empowers product managers, and business leads to make decisions across areas such as payments, partnerships, signup flows, and messaging.

In this role, you’ll partner closely with software engineers, and data scientists to power analytical data products, experimentation, and machine learning models. The best person will have a strong engineering background and will be able to tie initiatives to business impact.
What you will do?
Partner with engineering teams and internal data consumers on new projects or enhancements
Build highly scalable data pipelines and clean datasets around key business metrics
Enhance our data architecture to balance scale and performance
Build and improve internal tools, and collaborate with the larger data teams on ideas
Here are some examples of our work
Analytic Data Products - Engineer storage layers using Druid, Hive, Redshift to power interactive custom viz applications
Data Pipelines - Create new pipelines or rewrite existing pipelines using Spark (Scala)
Data Quality and Anomaly Detection - Improve existing tools to measure data quality through metrics and automatic alerting
Data Modeling - Partner with data consumers to improve existing data models and build different facets of the business for analytic use cases
Machine Learning - In addition to feature engineering, build feedback loops in a Pub/Sub model between ML models and production applications
Who are you?
Software engineering mindset and ability to write elegant, maintainable code, and follow engineering best practices
Analytical mindset to understand business needs, and come up with engineering solutions
Experience balancing complexity and simplicity in terms of schema design
Expertise building data pipelines (in either Real-time or batch) on large complex datasets using Spark or other open source frameworks
Expertise in one or more programming languages (ideally Scala, or Python)
Strong SQL (Presto, Spark SQL, Hive) skills
Excellent communication skills to collaborate with cross functional partners and independently drive projects and decisions
Knowledge and familiarity with other distributed data stores (Elasticsearch, Druid)
A few more things to know:
Our culture is unique and we live by our values, so it's worth learning more about Netflix at jobs.netflix.com/culture. You will need to be comfortable working in the most agile of environments. Requirements will be vague. Iterations will be rapid. You will need to be nimble and take smart risks.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

APPLY NOW
Share this listing:
LINK COPIED"
122,Software Development Engineer,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Bachelor’s Degree in Computer Science or related field4+ years professional experience in software developmentComputer Science fundamentals in algorithms, system design and object-oriented designProficiency in more than one modern programming language such as Java, C/C++, and Python

Amazon Advertising is dedicated to driving measurable outcomes for brand advertisers, agencies, authors, and entrepreneurs. Our ad solutions—including sponsored, display, video, and custom ads—leverage Amazon’s innovations and insights to find, attract, and engage intended audiences throughout their daily journeys. With a range of flexible pricing and buying models, including self-service, managed service, and programmatic ad buying, these solutions help businesses build brand awareness, increase product sales, and more.

The Team:
Forecasting and dynamic pricing team builds end-to-end solutions including data pipelines, machine learning models, large scale data structures and indexes, advertiser recommendations (bids, products) and data visualizations. We match supply (human eyeballs) and demand (advertisers interests) in thousands of audience targeting dimensions, and recommend optimal prices.

The Role:
The team is seeking an experienced backend and data engineer, who will own backend data pipelines, indexing systems and service & API layer application support. These systems are to be used by hundreds of internal and tens of thousands of self-serve external users. To be successful in this role, you will need to have a passion for backend, data and service architecture and engineering.
In this role, you will:
Own and build large-scale data processing and indexing pipelinesOwn development stack of applications and strive to use new technology packagesOwn services and their health, performance and service-level metricsCollaborate with business and product partners to build applications
In this team, you’ll experience the benefits of working in a dynamic, entrepreneurial environment, in our offices located in downtown Palo Alto, California.

Experience with large-scale data processing and management frameworks such as Hadoop and SparkExperience in backend and server-side development concepts such as multi-threading, concurrency, and fleet management.Experience in database technologies such as MySQL, Oracle, and RedshiftExperience with distributed indexing and storage systems such as Roaring Bitmaps, Redis, and ElasticSearch"
123,Sr. Health Sensing Data Engineer,"Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Mar 25, 2019
Weekly Hours: 40
Role Number: 200009586
Our team is growing! Here is your opportunity to come and join an exciting engineering team responsible for building next-generation health sensors and features. The Human Interface Devices team is looking for talented and passionate engineers with expertise in data pipelines and infrastructure. This is an integral role where you will help design, develop, and support high quality, scalable data platforms and applications for analysis of machine, user, and sensor data.
Key Qualifications
Strong software development skills, with proficiency in relevant languages such as Python, Java
Extensive experience with Spark and/or Hadoop MapReduce
Familiarity with cloud-based infrastructure such as AWS and Docker
Practical experience with SQL and NoSQL databases (MySQL, Postgres, MongoDB, Cassandra, HBase, etc)
Creative and collaborative
Description
As a Senior Data Engineer in this central role you will own data pipelines, work with the data engineering team to develop general use tooling, and collaborate with the algorithm and QA teams to design and validate the pipelines and tooling. Your work will directly impact the development of features across multiple Apple hardware platforms.
Education & Experience
Bachelor/Masters Degree in Computer Science, 3+ years of programming experience, 1+ year distributed computing with Spark
Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.
Additional Requirements
Scaling our existing data pipelines
Parallelization of data processing tools and frameworks and platform virtualization.
Supporting data collection and curation and handling large datasets.
Working closely with data scientists and algorithm engineers"
124,Sr. Systems Analyst,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Sr. Systems Analyst - (SO00050471-1-1-1)


UST Global is a private high growth organization headquartered in Orange County California and is a leading provider of Advanced Computing and Digital Services for Global 1000 companies worldwide. Our next-generation digital consultancy is the ideal place for you to grow your career. Are you up for the challenge? Read on!

Description

Sr.Big Data Developer
Job Description
Customer s Platform Engineering team is looking for motivated and experienced big data engineers, to join a highly skilled team of experienced professionals, which believe in best of breed software craftsmanship, clean and elegant coding, using the right tool for the job, and always exploring and learning new technologies and approaches. This Big Data Engineer will work on a high volume data pipeline, data integration, data transformation with Transaction State Management. This involves working with multiple BUs to ingest data into an Operational Data Store for downstream analytics by multiple stakeholders. There will be following activities in this initiative.
A Real time streaming and Batch enterprise data platformHigh volume data processingNear Real Time Operational Data Storeperformance tuning of big data applications and analytical queriesData Governance and Data tiering

Responsibilities Requirements
As an engineer, you will be responsible for working on applications and services that handle all types of transactions. You will work in a fast paced environment where continuous innovation and experimentation are a given. You will master both established and cutting edge technologies like Spark, Presto, Hadoop, Hive, Oracle, Casandra, Kafka, Druid among others.
Design, development, and testing of features functions delivered via applications and servicesCollaborating with peers and seniors both within their team and across the organizationWorking with operations teams to ensure your applications and services are highly available and reliableSupporting your applications and or services as and when required on a 24x7 basisDesign, develop, test, and debug large scale complex applications using big data technologies.Develop tools and automation for the effective management and operation of Big Data platform.Collaborate with architects, engineers, and business on product design and feature.Apache Hadoop and data ETL extract, transform, load , ingestion, and processing with Hadoop toolsBe proactive and anticipate handle most issues before they blowupExhibit a strong backbone and challenge the status quo when neededShow pride of ownership and strive for excellence in everything they do

Job Requirements
8 years software development experienceBS in Computer Science or related degree required. MS preferredExpert in performance tuning of Spark batch and streaming applicationsExpert on Big Data Technologies such as Kafka, Apache SparkExperience in database storage technologies like Oracle, Cassandra, CouchBase etc.Experience in building real time ETL pipelines using Spark or Apache FlinkExperience in validating the data in streaming and batch processesExpert in SQL, Hive, Spark, PrestoCompetent in design implementation for reliability, availability, scalability and performanceCompetent in software engineering tools and best practices

Qualifications

Big Data

Primary Location: US-CA-San Jose
Employee Type: Regular Employee
Job Type: Full-time
Job Posting: Aug 6, 2019, 10:07:44 AM


 UST-Global is an Equal Opportunity Employer"
125,Data Engineer | Advertising,"Palo Alto, CA 94301",Palo Alto,CA,94301,None Found,None Found,None Found,None Found,None Found,None Found,"About Houzz
Houzz is the leading home renovation and design platform in the world. We have a highly engaged community of more than 40 million unique monthly users who leverage our technologies to find inspiration for their next home project, connect with over 2.3 million home design and remodeling professionals and discover products in the Houzz Shop.


About the Role
Data Engineers bridge the gap between infrastructure and business. As a Houzz Data Engineer, you will work closely with analysts, data scientists, product managers and researchers to internalize every use case of data within the company. You will also work with data infrastructure to build the necessary platform to house the data and intuitive tools to leverage the data. Additionally, you will work with back-end and front-end engineers to implement the proper logging in order to understand our users’ behavior. You will then build efficient ETL processes that produce the core datasets and dashboards that are leveraged to understand how the company is performing and to make data-driven decisions at Houzz.
What You'll Do
Architect and build core datasets
Implement efficient ETL processes
Design and create dashboards
Propose changes to the logging structures or supporting new kinds of data analysis
Translate business requirements into projects and prioritize based on impact
Evangelize data architecture and ETL best practices
At a Minimum, We'd Like You to Have
Strong SQL skills
Proficiency in data architecture and/or data warehouse design patterns
Proficiency in efficient batch and streaming ETL design
Proficiency in Python or Java and an ability to learn the other
Experience with Big Data technologies such as Hadoop, Spark, Hive and Presto/Impala
Experience with workflow management framework such as Airflow, Luigi
Experience with visualization tools such as Tableau, Superset
Excellent communication, interpersonal and cross-functional skills
A passion for understanding analytics use cases for data, architecting intuitive datasets to support these use cases and optimizing our methods of producing these datasets
B.S. or M.S. in Computer Science or a related field
2+ years of experience
Ideally, You'll Also Have
Experience supporting analytics teams studying user behavior and product metrics
Experience building analytics frameworks and tools with broad adoption

__________________

Be Who You Are and Do What You Love at Houzz

We’re a Family
At Houzz, we strive to create and foster a strong family environment in our workplace. We collaborate to accomplish our goals, always working as a team. We aim to build a culture of inclusion — celebrating and leveraging our differences for the betterment of one another, our products and our community.
Houzz team members come from many backgrounds and bring diverse experiences to the company. We take pride in making each person feel at home.

We Build the Future
Join Houzz in revolutionizing the home remodeling and design industry and have an impact on the more than 40 million homeowners who use our platform every month and the 2.3 million-plus home professionals around the world who are active on the site. Houzz has been named one of the most innovative companies in the world by CNBC and others, and is backed by top venture capitalists. At Houzz, you can help drive the future of an industry worth $1.2 trillion in the U.S. and Europe alone.

We Make Things Happen
Our team members play a key role in guiding the direction of our company and are able to work across multiple groups to implement fresh ideas that allow Houzz to be the industry leader. If you are interested in applying your passion to create products that will transform the lives of millions of people who are designing, remodeling and decorating their homes, welcome to Houzz.

Benefits and Perks
Competitive salary
Flexible paid time off
Commuter benefits
Medical, dental, vision and pet insurance
Maternity/paternity leave programEmployee assistance program
401k retirement savings planFlexible spending accounts
Healthy at Houzz programCatered meals, fully stocked kitchens and much more!

Houzz is an Equal Opportunity Employer."
126,Data Analytics Engineer Intern,"San Jose, CA 95134",San Jose,CA,95134,None Found,None Found,None Found,None Found,None Found,None Found,"Data Analytics Engineer Intern
Summer 2020 internship applications are open September 2019 through January 2020. Samsung Semiconductor summer internships start in May/June 2020.
Join us for a unique 12-14 week paid internship that offers personal and professional development. You’ll work with the teams that create new computing system architectures needed to support emerging machine learning applications, internet of things (IoT) and edge computing that benefit millions of users. This program will give you the opportunity work on complex solutions to that address some of the world’s most complex technological challenges.
Samsung Semiconductor, Inc. is a world leader in Memory and Storage technologies. We are currently looking for Data Analytics Engineer (Intern) to join our team in San Jose, CA. The Data Engineer Intern will contribute to memory and storage system research in the Memory Solutions Lab. He or she will join a team of experts in researching and developing innovative memory and storage system solutions that utilize existing and emerging technologies to add substantial value to storage systems. The ideal candidate must have a strong understanding of storage technologies including file systems, Linux I/O Stack, Linux performance profiling and computer architecture.

JOB RESPONSIBILITIES
Implement software and design machine learning or deep learning models using existing big data/AI frameworks to analyze collected server telemetry data for various predictions and insights
Propose possible approaches and methods to solve complex analytics problems related to storage workloads and management
Analyze large sets of collected server telemetry data, create machine learning models and generate verifiable results
Work with team members to contribute towards prototyping efforts.
Create new and useful IP, publish at conferences, and generate whitepapers.
REQUIRED SKILLS
Pursuing an MS or PhD in Computer Science, Computer Engineering or related field, with focus on data modeling and data science.
Good knowledge of popular Big Data frameworks
Research and development experience with Spark, Presto, Hive, Hadoop, and NoSQL Databases
Experience with deploying and debugging applications across server clusters
Prior experience with storage performance analysis and optimization will be a big plus.
Track record of innovation and creativity in problem solving
Must be highly motivated with excellent verbal and written communication skills.
Strong background in C/C++/Python/Java/Scala.
Comfortable working in a multinational environment and understands how to leverage cultural diversity."
127,"Senior Data Engineer, Device Reliability","Los Gatos, CA 95032",Los Gatos,CA,95032,None Found,None Found,None Found,None Found,None Found,None Found,"Los Gatos, California
Data Science and Engineering
Netflix makes up 1/3 of internet traffic, and we're proud to deliver entertainment that over 150 million global customers enjoy. To provide this level of service, we need to know exactly how devices running the Netflix client behave in the field.

Decisions on how to improve performance and compatibility on millions of devices worldwide, are driven by data. We're looking for someone to transform petabytes of incoming telemetry on device characteristics and performance into well-designed, high-quality data structures that empower critical decision-making for teams within Netflix.

We track every customer action and each byte of data transferred, so you'll work with data at incredible scale and collaborate with best-in-class data engineers and analytic experts. You'll become an authority in the world of device instrumentation and performance (no prior knowledge necessary, but curiosity to learn is a must), and the projects you'll work on will be truly impactful.

In the meantime, learn more about the Streaming Data Engineering team.
What you'll do
You’ll take ownership and increase automation and scale of complex data sets that drive use cases by our analytical partners such as understanding device error rates, and monitoring performance of all devices connected to our service.
You’ll build robust data pipelines of high data quality in a scalable fashion (both data and maintainability).
We need to process data more quickly than ever to enable rapid experimentation in an increasingly nimble engineering organization. You’ll help implement our business logic to be compatible with real-time/stream processing frameworks.
Who you are
Have several of the characteristics/skills listed below and have passion and self-drive to quickly learn in areas of less familiarity. We believe the experience in your years is more important than your years of experience.
Enjoy a high level of autonomy in managing cross-functional engineering projects. We enjoy a culture of Freedom & Responsibility.
Have experience building production data pipelines using one or more frameworks such as Spark, Flink or Hive/Hadoop. Have hands on experience with schema design and data modeling.
Have programming proficiency in at least one major language such as Java, Scala or Python. You have a software engineering mindset and strive to write elegant, maintainable code and you're comfortable working in a variety of tech stacks. You may even be a software engineer with a focus or passion for data-driven solutions.
Have strong SQL skills and knowledge and familiarity with other distributed data stores such as ElasticSearch or Druid.
Have excellent communication in sharing context to effectively collaborate with analytical partners, domain experts and other consumers of your work, preferably in supporting an engineering or product function. We like to collaborate across teams and so should you.
Ambitious and willing to take action, but not stubborn. Awareness to recognize when you're wrong and move past your own mistakes. We are humbly confident in ourselves and our work.
Netflix Culture
 Our culture is unique, and we live by our values. You will need to be comfortable working in the most agile of environments. Requirements will be vague, and iterations will be rapid. You will need to be nimble and take smart risks. Learn more about Netflix’s culture.

APPLY NOW
Share this listing:
LINK COPIED"
128,"Staff Software Engineer, Data Team (Big Fast Data)","Sunnyvale, CA 94087",Sunnyvale,CA,94087,None Found,"Collaborating closely with application team architects and engineers to identify technologies and platforms suitable for their big data processing requirements, and then assisting those teams with onboarding, development, deployment, and debugging on those platforms",None Found,None Found,None Found,None Found,"Position Description

Develops Innovation strategies, processes, and best practices
Drives the execution of multiple business plans and projects
Ensures business needs are being met
Leads and participates in medium- to large-scale, complex, cross-functional projects
Leads the discovery phase of medium to large projects to come up with high level design
Leads the work of other small groups of six to ten engineers, including offshore associates, for assigned Engineering projects
Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity
Provides supervision and development opportunities for associates
Supports business objectives
Troubleshoots business and production issues
Utilizes industry research to improve Wal-Mart's technology environment

Minimum Qualifications
Bachelor's Degree in Computer Science or related field and 6 years experience building scalable ecommerce applications or mobile software
Additional Preferred Qualifications
Please add text
Company Summary
The Walmart eCommerce team is rapidly innovating to evolve and define the future state of shopping. As the world’s largest retailer, we are on a mission to help people save money and live better. With the help of some of the brightest minds in technology, merchandising, marketing, supply chain, talent and more, we are reimagining the intersection of digital and physical shopping to help achieve that mission.
Position Summary
The Walmart Labs Big Data Platforms team is seeking a big data engineer to serve in a consulting engineering and support role for the big data components of Walmart’s major application and project initiatives. The Big Data Platforms team operates multi-tenant persistent Hadoop-based platforms based on several Hadoop distributions, and also leads architecture and integration engineering for Walmart’s cloud-based big data platforms using both on-premise and commercially available cloud resource providers.

We’re looking for a software engineer that has extensive experience building and supporting big data applications using the Hadoop ecosystem and related technologies, both on traditional clusters and cloud platforms, to collaborate with Walmart’s internal product development teams to help them construct scalable and performant big data applications, and also help application teams troubleshoot big data problems when things go wrong.

Responsibilities include:

Collaborating closely with application team architects and engineers to identify technologies and platforms suitable for their big data processing requirements, and then assisting those teams with onboarding, development, deployment, and debugging on those platforms
Providing technical engineering and performance tuning assistance to a broad community of big data infrastructure users, such as software application engineers and data scientists, through research, investigation, collaboration, and hands-on debugging when necessary
Investigating new big data tools and technologies for their potential application to common use cases; establishing best practices, developing design patterns, and writing documentation to disseminate new capabilities to a broad technical audience; working with platform engineers and product managers to specify and deliver new major technology features
Ensuring that application big data solutions adhere to best practices and enterprise standards for scalability, availability, efficiency, data lifecycle management, information security, fault tolerance, and disaster recovery"
129,"Software Engineer, Data Infrastructure (SF and Mountain View)","Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"As one of the first dedicated engineers to data infrastructure, you will build and integrate scalable backend systems, data pipelines, platforms and tools that power our data warehouse. You will also help build the data infrastructure needed for internal and customer-facing analytics needs and our machine learning models, as well as build and maintain systems for managing data clusters, handling security, disaster recovery and replication. In doing so, you will play a meaningful role in scaling our data infrastructure as we grow rapidly.

As Moveworks’ data multiplies during a critical time of hypergrowth, your goal is to provide best-in-class data infrastructure, analytics tools, and data components to scale efficiently. You will design and integrate systems that power processing of unstructured datasets to ultimately finetune our product and achieve fully autonomous resolution of enterprise IT.

What will you do?
You will design, develop and deploy our data infrastructure in the cloud
Promote efficient use of data and analytics within the organization
Build the data infrastructure needed for internal and customer-facing analytics needs
You will build data pipelines for machine learning efforts
You will build and maintain systems for managing data clusters, handling security, disaster recovery and replication
Build analytics tools utilizing the data pipeline to provide actionable insights for our product and engineering team
You will be a data expert and champion data quality efforts across the board
You will ensure that all data components are designed and implemented in compliance with our information security requirements
What do you bring to the table?
You have 4+ years of experience as a data engineer, ideally with a cloud-based SaaS company
You have hands-on experience building, scaling, and supporting large-scale ETL data infrastructure systems in production
You have hands on experience working with different teams for their data requirements
You have strong familiarity with ETL systems like Kafka, Spark, Storm, Fluentd, Hadoop, Presto, Hive
You have experience with SQL and NoSQL databases like PostgreSQL, MySQL, Cassandra, DynamoDB or HBase, and cloud management systems like AWS, GCE or AzureBS or higher in Computer Science or a related field
Nice-to-haves:
Experience with large-scale machine learning pipelines is a plus
Who we are:
Moveworks is an AI first company with a singular focus: fully autonomous resolution of all enterprise IT. We are building a state of the art platform that combines natural language understanding, conversational interface, and automation to enable hundreds of millions of knowledge workers get work done faster. Our engineers built foundational systems are companies like Google, Amazon, LinkedIn, and Facebook, and we are now applying our expertise to build a first of its kind enterprise machine learning platform.

While we currently resolve 15-35% of all IT related requests autonomously at companies like Broadcom and Autodesk, we are just getting started. Our vision is to build a single AI platform where employees can come for help whenever they need anything from their respective companies - ranging from HR related help, facilities, legal, and more. This is all delivered through a conversational interface like Slack, Teams, or Google Chat for a fast, delightful, and frictionless user experience.

As we scale globally, there’s plenty of space for you to grow with us. You’ll be part of a team that thinks in terms of we, not I. Together, we're focused on simplifying life for millions of people around the world in their most pressing hours of the day by giving them an effortless, magical way to resolve issues - all powered by a strong form of AI."
130,Data Engineer,"Santa Clara, CA",Santa Clara,CA,None Found,None Found,"Bachelors or Masters in Computer Science, Engineering, or a related quantitative field.
3+ years of experience with building scalable and reliable date pipelines using technologies like Spark, AWS EMR, Kafka, etc.
3+ years of production coding experience with at least one general software development programming language (Java, Scala), one data programming language (Python, R), and scripting languages (Unix shell) as well as solid experience with git.
Expertise with relational databases and experience with schema design and dimensional data modeling.
Mastery of SQL (writing complex, high performance queries in Oracle or MSSQL); experience with distributed querying (Snowflake, Spark SQL, Hive) and NoSQL systems (MongoDB, etc).
Working experience with various ETL technologies and frameworks (Pentaho, Informatica, Matillion, etc.)
Working experience with AWS cloud ecosystem.
Excellent communication skills in written and verbal forms, and an ability to communicate complex issues to a range of audience (management, peers, clients).
Strong attention to detail while excellent time management and prioritization in multitasking.
Highly motivated problem-solver who enjoys working in a fast-paced environment and can also be patient with the pace of highly regulated industries like healthcare.
",None Found,"Become the subject matter expert on our data and its capabilities. Your scope of knowledge will need to include various data systems that are specialized to internal departments and 3rd party data platforms.
Design and build highly scalable data integration / ETL pipelines to improve data accessibility and consumption.
Automate data processing using workflows tools to schedule and manage dependency of various data pipelines.
Work directly with data scientists to develop scalable implementation of statistical and machine learning models in production, and work with software engineers to design, build, and maintain APIs to interact with those models.
Recommend ways to improve data reliability, efficiency, and quality.
Assist eHealth’s data architect with logical and physical data model designs and documentation.
Work with data infrastructure team to triage issues and support issue resolution.
",None Found,None Found,"Get your career started at eHealth
eHealthInsurance has many exciting career opportunities in a number of locations, across various functions. Come join us today!

At eHealth, we are passionate about solving our nation's toughest problems to bring more suitable, accessible, and affordable health insurance to Americans. We are seeking a talented data engineer to join our growing data team, which is already making a valuable impact on the entire company. This person will help us develop cutting-edge data tools and pipelines to drive better and faster decision making within our company and to better serve our customers. This is a fast-paced, collaborative, and iterative environment requiring quick learning, agility, and flexibility.


Responsibilities:
Become the subject matter expert on our data and its capabilities. Your scope of knowledge will need to include various data systems that are specialized to internal departments and 3rd party data platforms.
Design and build highly scalable data integration / ETL pipelines to improve data accessibility and consumption.
Automate data processing using workflows tools to schedule and manage dependency of various data pipelines.
Work directly with data scientists to develop scalable implementation of statistical and machine learning models in production, and work with software engineers to design, build, and maintain APIs to interact with those models.
Recommend ways to improve data reliability, efficiency, and quality.
Assist eHealth’s data architect with logical and physical data model designs and documentation.
Work with data infrastructure team to triage issues and support issue resolution.
Minimum Qualifications:
Bachelors or Masters in Computer Science, Engineering, or a related quantitative field.
3+ years of experience with building scalable and reliable date pipelines using technologies like Spark, AWS EMR, Kafka, etc.
3+ years of production coding experience with at least one general software development programming language (Java, Scala), one data programming language (Python, R), and scripting languages (Unix shell) as well as solid experience with git.
Expertise with relational databases and experience with schema design and dimensional data modeling.
Mastery of SQL (writing complex, high performance queries in Oracle or MSSQL); experience with distributed querying (Snowflake, Spark SQL, Hive) and NoSQL systems (MongoDB, etc).
Working experience with various ETL technologies and frameworks (Pentaho, Informatica, Matillion, etc.)
Working experience with AWS cloud ecosystem.
Excellent communication skills in written and verbal forms, and an ability to communicate complex issues to a range of audience (management, peers, clients).
Strong attention to detail while excellent time management and prioritization in multitasking.
Highly motivated problem-solver who enjoys working in a fast-paced environment and can also be patient with the pace of highly regulated industries like healthcare.
Nice to Have:
Working experience with implementing scalable models using various statistics and machine learning toolkits (Pandas, SciPy, Scikit-learn, MLlib, Spark ML, Tensorflow, Keras, etc.).
Strong experience in designing and implementing data APIs.
Product familiarity with Adobe Analytics, Cisco systems, Snowflake, or Informatica.
Familiarity with workflow management tools (Airflow).
Working experience with data warehousing.
Ability to create beautiful data visualizations using D3, Tableau, or similar tools.
Working experience with large healthcare related datasets, including EHRs, medical claims data, and health population surveys. Experience in building healthcare data pipelines would be a big plus.
Knowledge of healthcare insurance industry, products, systems, business strategies, and products.
Experience working with call center operations.
eHealth is an Equal Employment Opportunity employer. It is our policy to provide equal opportunity to all employees and applicants and to prohibit any discrimination because of race, color, religion, sex, national origin, age, marital status, sexual orientation, genetic information, disability, protected veteran status, or any other consideration made unlawful by applicable federal, state or local laws. The foundation of these policies is our commitment to treat everyone fairly and equally and to have a bias-free work environment."
131,Senior Data Analyst,"Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"ABOUT US
Lark is the world's largest A.I. healthcare provider, servicing more than a million patients suffering from, or at risk of, chronic disease with A.I. Nurses. We're on a mission to improve people's health and happiness through our digital health coach. We are the only A.I. nurse ever to become fully medically reimbursed to 100% replace a live nurse because we achieved equivalent health outcomes to live healthcare professionals - which allows for infinitely scalable healthcare. Since launch, Lark has continued to receive awards and accolades for both our product, and our leadership.

✦Apple's Top 10 Apps in the World
✧Business Insider's most innovative companies in the world along with Uber and Snapchat
✦Biz Journal's 100 Women of Influence

We are looking for a talented Data Engineer to join our growing team in Mountain View, CA, where you'll be building our next generation data pipelines.

ABOUT THE ROLE

What You'll Do:

Build our next generation data pipelines into a fast and efficient big-data system
You'll be the first fully dedicated data engineer on the team, and will be able to call the shots on strategy

What You'll Need:

A love of data, and the make-or-break effect it has on startups
Default to coding efficient systems from large databases, both RDBMS and noSQL.
Familiarity with the following key technologies (or similar):
Spark
Yarn
Kafka
Python
AWS

JOIN US
Our team works with cutting edge tools and technology related to Artificial Intelligence and Machine Learning. We are using NLP to process millions of meals, and accelerometer data to compute activity and sleep amounts from users' phones. Our chat AI is the most sophisticated digital health engagement tool in the world. Join us and make it even better!

Lark is an Equal Opportunity Employer. Lark does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need."
132,Data Engineer,"Sunnyvale, CA 94085",Sunnyvale,CA,94085,None Found,FIRST Robotics (For Inspiration and Recognition of Science and Technology)AWIM (A World In Motion),None Found,"
Design and implement fault-tolerant data pipelines to integrate large amounts of data from many diverse storage systems.
Promote a culture of self-serve data analytics by minimizing technical barriers to data access and understanding.
Execute complex data engineering projects that have a significant impact on Bosch global business.
Share knowledge by clearly articulating results and ideas to customers, managers, and key decision makers.
Stay current with the latest research and technology and communicate your knowledge throughout the enterprise
Take responsibility for preparing data for analysis and provide critical feedback on issues of data integrity
 Up to 10% travel may be required.",None Found,None Found,"Job Description

Primary Responsibilities:
Design and implement fault-tolerant data pipelines to integrate large amounts of data from many diverse storage systems.
Promote a culture of self-serve data analytics by minimizing technical barriers to data access and understanding.
Execute complex data engineering projects that have a significant impact on Bosch global business.
Share knowledge by clearly articulating results and ideas to customers, managers, and key decision makers.
Stay current with the latest research and technology and communicate your knowledge throughout the enterprise
Take responsibility for preparing data for analysis and provide critical feedback on issues of data integrity
 Up to 10% travel may be required.

Qualifications

Basic Qualifications:
MS in Computer Science
2+ years of in-depth knowledge and hands-on experience with distributed systems
2+ years of in-depth knowledge and hands-on programming skills in Scala or Java
Preferred Qualifications:
Strong understanding in tuning and performance optimization of Apache Spark jobs
Experience with integration of data from multiple data sources
Experience with various messaging systems, such as Kafka or RabbitMQ
Experience managing and solving ongoing issues with a Spark/Hadoop cluster
Additional Information

BOSCH is a proud supporter of STEM (Science, Technology, Engineering & Mathematics) InitiativesFIRST Robotics (For Inspiration and Recognition of Science and Technology)AWIM (A World In Motion)
By choice, we are committed to a diverse workforce – EOE/Protected Veteran/Disabled."
133,Big Data Engineer - Master's (Full Time) – United States,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"What You’ll Do
Design and deliver automated transformation of large data sets leveraging MapReduce, streaming, and other emerging technologies
Leverage HBase, Elasticsearch, etc. to ingest transformed data at scale
Collaborate with security experts to deliver high-impact web-based APIs
Implement high-volume data integration solutions
Analyze, monitor, and optimize for performance
Produce and maintain high-quality technical documentation

Who You'll Work With
Join us as we transform the world of tomorrow. Develop creative ideas on how to work better and smarter. Influence and participate in top-priority projects that have a real impact.

Who You Are
Recent graduate or on your final year of studies toward a Master's degree in Computer Science or a related technical field
Minimum of a 3.0 GPA or equivalent
Track record of developing technology to enable large scale data transformation
Strong Java experience and hands-on Hadoop ecosystem experience – HBase, Hive, Spark, etc.
Possess knowledge of software engineering best practices
Passion for solving hard problems and exploring new technologies
Excellent communication and technical documentation skills


Why Cisco
At Cisco, each person brings their own unique talents to work as a team and make a difference.
Yes, our technology changes the way the world works, lives, plays and learns, but our edge comes from our people.We connect everything – people, process, data and things – and we use those connections to change our world for the better.We innovate everywhere - From launching a new era of networking that adapts, learns and protects, to building Cisco Services that accelerate businesses and business results. Our technology powers entertainment, retail, healthcare, education and more – from Smart Cities to your everyday devices.We benefit everyone - We do all of this while striving for a culture that empowers every person to be the difference, at work and in our communities.

Colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Be you, with us! #WeAreCisco


This position is available to Master's level Students. Positions are located East Coast, West Coast and Central US. Not all positions offer sponsorship or are available at all locations. Relocation is available for some locations and or positions."
134,Data Engineer,"Burlingame, CA 94010",Burlingame,CA,94010,None Found,"
A Bachelor’s in Computer Science, Statistics, Mathematics, Operations Research, Economics, Public Health, or related field with quantitative emphasis
Strong organizational, planning, and problem solving skills
Team player with strong interpersonal skills
Excellent written and oral communication skills
Familiarity with one or more computer programming languages
",None Found,None Found,None Found,None Found,"Are you someone who enjoys working with data? Are you a self-motivated thinker who wants to make an impact in the fast-growing healthcare data industry?

We are looking for a Data Engineer who highly values research, wants to work with multifaceted datasets, and craves new challenges in programming. As a Data Engineer, you will have the opportunity to gain first-hand experience integrating and structuring the healthcare data that shapes policy on many key topics, such as the Affordable Care Act, Medicare, and Medicaid. You will also have the chance to work closely with seasoned programmers, developing the skills to work with data management tools and various programming languages. In addition, you will work alongside smart, vibrant people with a passion for the exciting future of healthcare research.

The Data Engineer will:

Extract, transform, and load (ETL) big data.
Develop complex data processing algorithms that combine multiple data sources, while optimizing run-time efficiency.
Develop data structures, databases, and querying programs which facilitate efficient data access.
Develop data structures from claims and enrollment data which support research and analytic activities of in-house analysts as well as congressional and federal agencies.
Ensure data inventory is complete and accurate through application design, including fault analysis and detection, quality control, and the development of tracking systems.
Collaborate with other Data Engineers and in-house researchers to maintain systems, produce documentation, and educate internal and external users about company resources.
Perform validation checks across multiple sources to verify data integrity as needed.
Perform other duties and responsibilities as assigned.


Required Skills
Qualifications Required

A Bachelor’s in Computer Science, Statistics, Mathematics, Operations Research, Economics, Public Health, or related field with quantitative emphasis
Strong organizational, planning, and problem solving skills
Team player with strong interpersonal skills
Excellent written and oral communication skills
Familiarity with one or more computer programming languages
Qualifications Desired

Master’s in Information Management Systems, Statistics, Mathematics, Operations Research, Economics, Public Health, a related field with quantitative emphasis, or 2+ years of work experience in a field with quantitative emphasis
Interest in big data
Interest in making an impact in the field of healthcare policy research
Previous experience in a Data Analyst/Data Engineer position
1+ years of experience working with programming languages such as SAS, SQL, Python, or R
1+ years of experience working with databases or data pipelining tools
Please submit a cover letter and resume to be considered for this position.

Due to the sensitive nature of much of our work, all Acumen employees must undergo a background check. Your employment will be contingent upon your completing, and Acumen reviewing to its satisfaction, a mandatory background check.
Employees who work with particularly sensitive information may be asked to undergo an additional background check after starting work.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, physical or mental handicap, disability, age or status as a disabled veteran, or veteran of the Vietnam era.
Required Experience"
135,Data Engineer,"San Mateo, CA",San Mateo,CA,None Found,None Found,"
Proven expertise in production software development
7+ years of experience programming in Java, Python, SQL, or C/C++
Proficient in SQL, NoSQL, relational database design and methods for efficiently storing & retrieving data
Strong analytical and science skills
Creative problem solver
Excellent verbal and written communications skills
Strong team player capable of working in a demanding start-up like environment building the next generation Analytical Ecosystem..
Experience building complex and non-interactive systems (batch, real-time, distributed, etc.) Strong experience in working with and managing large distributed computing cluster (combining various technologies & frameworks like Hadoop, Nifi, streamsets, spark, kafka etc.)
","
Prior Data Platform Engineering experience
Experience with Hadoop, Hive, Pig, Avro, Thrift, Protobufs and JMS: ActiveMQ, RabbitMQ, JBoss, etc.
Dynamic and/or functional languages (e.g., Python, Ruby, Scala, Clojure)
Experience designing and tuning high performance systems
Prior experience with data warehousing and business intelligence systems
Experience with Elasticsearch, SolrWeb, and Lucene
Experience with Star Schema, fact vs dimensions, updates/restatements and views
Professional or academic background that includes mathematics, statistics, machine learning and data mining for optimizing the data platform
Linux expertise
Prior work and/or research experience with unstructured data and data modeling
Familiarity with different development methodologies (e.g., agile, waterfall, XP, scrum, etc.)
Demonstrate understanding of ""var"" vs. ""val"", use of multi-return methods, ability to write clean, legible scala code that solves a complex problem
Firm understanding on python memory mode, classes, sub classing, designing classes for re-use, static string constants rather than in-line constants
Understanding of various analytic and visualization utilities available in R
Configure a Jenkins build, create/update a Jira ticket, enable Automated Tests in gradle/maven build
Ability to leverage additional security tools like Ranger, Knox, Sentry to further harden a cluster or secure data access
Understanding of how to segregate data based on access control rules, when and how to encrypt data (whole record vs individual fields) when and how to mask fields, etc.
Able to create and deploy a Samza job via YARN or Mesos, read from a streaming source (like Kafka) and produce some filtered or enhanced output
Able to create a storm topology to filter or transform a steam of data. Ability to track state and isolation in Trident or similar
Able to connect DStream to Kafka or Flume (or similar) queue, filter or transform data and write back to DStream on a different topic/queue
Implementation of D3, Tableau or R graphing technologies that produce an intuitive view of the underlying data
Implement a graph (line or pie etc.) backed by a live (changing) data set, something like ""requests per minute"" or similar
Understand basic modeling techniques, tools sets. Implement simple Python or R analytic routines
","
Proven expertise in production software development
7+ years of experience programming in Java, Python, SQL, or C/C++
Proficient in SQL, NoSQL, relational database design and methods for efficiently storing & retrieving data
Strong analytical and science skills
Creative problem solver
Excellent verbal and written communications skills
Strong team player capable of working in a demanding start-up like environment building the next generation Analytical Ecosystem..
Experience building complex and non-interactive systems (batch, real-time, distributed, etc.) Strong experience in working with and managing large distributed computing cluster (combining various technologies & frameworks like Hadoop, Nifi, streamsets, spark, kafka etc.)
",None Found,None Found,"Responsibilities:
As a Data Engineer, you will provide technical leadership to the team that designs and develops path-breaking large scale cluster data processing systems.

Design and develop code, scripts and data pipelines that leverage structured and unstructured data integrated from multiple sources. Software installation and configuration. Participate in and help lead requirements and design workshops with our clients. Develop project deliverable documentation. Mentor junior members of the team in software development best practices. Other duties as assigned.

Additionally, as a senior member of our development & Ops team, you will help establish thought leadership in the big data space by contributing best practices, white papers, technical commentary and representing our section as one Leads.

Job Qualifications:

Proven expertise in production software development
7+ years of experience programming in Java, Python, SQL, or C/C++
Proficient in SQL, NoSQL, relational database design and methods for efficiently storing & retrieving data
Strong analytical and science skills
Creative problem solver
Excellent verbal and written communications skills
Strong team player capable of working in a demanding start-up like environment building the next generation Analytical Ecosystem..
Experience building complex and non-interactive systems (batch, real-time, distributed, etc.) Strong experience in working with and managing large distributed computing cluster (combining various technologies & frameworks like Hadoop, Nifi, streamsets, spark, kafka etc.)

Preferred Knowledge, Skills and Abilities:

Prior Data Platform Engineering experience
Experience with Hadoop, Hive, Pig, Avro, Thrift, Protobufs and JMS: ActiveMQ, RabbitMQ, JBoss, etc.
Dynamic and/or functional languages (e.g., Python, Ruby, Scala, Clojure)
Experience designing and tuning high performance systems
Prior experience with data warehousing and business intelligence systems
Experience with Elasticsearch, SolrWeb, and Lucene
Experience with Star Schema, fact vs dimensions, updates/restatements and views
Professional or academic background that includes mathematics, statistics, machine learning and data mining for optimizing the data platform
Linux expertise
Prior work and/or research experience with unstructured data and data modeling
Familiarity with different development methodologies (e.g., agile, waterfall, XP, scrum, etc.)
Demonstrate understanding of ""var"" vs. ""val"", use of multi-return methods, ability to write clean, legible scala code that solves a complex problem
Firm understanding on python memory mode, classes, sub classing, designing classes for re-use, static string constants rather than in-line constants
Understanding of various analytic and visualization utilities available in R
Configure a Jenkins build, create/update a Jira ticket, enable Automated Tests in gradle/maven build
Ability to leverage additional security tools like Ranger, Knox, Sentry to further harden a cluster or secure data access
Understanding of how to segregate data based on access control rules, when and how to encrypt data (whole record vs individual fields) when and how to mask fields, etc.
Able to create and deploy a Samza job via YARN or Mesos, read from a streaming source (like Kafka) and produce some filtered or enhanced output
Able to create a storm topology to filter or transform a steam of data. Ability to track state and isolation in Trident or similar
Able to connect DStream to Kafka or Flume (or similar) queue, filter or transform data and write back to DStream on a different topic/queue
Implementation of D3, Tableau or R graphing technologies that produce an intuitive view of the underlying data
Implement a graph (line or pie etc.) backed by a live (changing) data set, something like ""requests per minute"" or similar
Understand basic modeling techniques, tools sets. Implement simple Python or R analytic routines

Job Abilities:
Must be able to sit for long periods of time working on computers. Must be able to interact and communicate with the senior management in meetings. Must be able to write programming code in applicable languages and write project documentation in English.

Education:
Bachelor's Degree or foreign equivalent in Computer Science or related technical field followed by six (6-8) years of progressively responsible professional experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).

OR

Master's Degree or foreign equivalent in Computer Science or related technical field. Four (4-5) years of experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).

Employer will accept any suitable combination of education, training, or experience."
136,Data Engineer and Advanced Analytics,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Engineer will develop, construct, test and maintain architecture databases and large scale processing systems.

Responsibilities include but are not limited to the following:

1. Engineering and Architecture
Develop, construct, test and maintain architectures, such as databases and large-scale processing systems.
Design, build and optimize ‘big data’ data pipelines, architectures and data sets.
Ensure architecture will support the requirements of the business.
Discover opportunities for data acquisition.

2. Data Modeling and Reporting
Develop data set processes for data modeling, mining, and productions.
Employ a variety of languages and tools (e.g. scripting languages) to marry systems together.
Recommend ways to improved data reliability, efficiency, and quality.

3. Documentation Support
Document data design and architecture diagrams and supporting directional information.
Provide updates on improvements.
Identify opportunities to standardize data."
137,Lead Data Engineer - Test + Measurement (Sapphire),"Sunnyvale, CA 94086",Sunnyvale,CA,94086,None Found,None Found,None Found,None Found,None Found,None Found,"Description:
About us:
Target is an iconic brand, a Fortune 50 company and one of America’s leading retailers.

Target as a tech company? Absolutely. We’re the behind-the-scenes powerhouse that fuels Target’s passion and commitment to cutting-edge innovation. We anchor every facet of one of the world’s best-loved retailers with a strong technology framework that relies on the latest tools and technologies—and the brightest people—to deliver incredible value to guests online and in stores. Target Technology Services is on a mission to offer the systems, tools and support that guests and team members need and deserve. Our high-performing teams balance independence with collaboration, and we pride ourselves on being versatile, agile and creative. We drive industry-leading technologies in support of every angle of the business, and help ensure that Target operates smoothly, securely and reliably from the inside out.
As a lead engineer, you serve as the technical anchor for the engineering team that supports a product. You create, own and are responsible for the application architecture that best serves the product in its functional and non-functional needs. You identify and drive architectural changes to accelerate feature development or improve the quality of service (or both). You have deep and broad engineering skills and are capable of standing up an architecture in its whole on your own, but you choose to influence a wider team by acting as a “force multiplier”. Core responsibilities of this job are described within this job description. Job duties may change at any time due to business needs.
Use your skills, experience and talents to be a part of groundbreaking thinking and visionary goals. As a Lead Engineer, you’ll take the lead as you…
Use your technology acumen to apply and maintain knowledge of current and emerging technologies within specialized area(s) of the technology domain. Evaluate new technologies and participates in decision-making, accounting for several factors such as viability within Target’s technical environment, maintainability, and cost of ownership. Initiate and execute research and proof-of-concept activities for new technologies. Lead or set strategy for testing and debugging at the platform or enterprise level. In complex and unstructured situations, serve as an expert resource to create and improve standards and best practices to ensure high-performance, scalable, repeatable, and secure deliverables. Lead the design, lifecycle management, and total cost of ownership of services. Provide the team with thought leadership to promote re-use and develop consistent, scalable patterns. Participate in planning services that have enterprise impact. Provide suggestions for handling routine and moderately complex technical problems, escalating issues when appropriate. Gather information, data, and input from a wide variety of sources; identify additional resources when appropriate, engage with appropriate stakeholders, and conduct in-depth analysis of information. Provide suggestions for handling routine and moderately complex technical problems, escalating issues when appropriate. Develop plans and schedules, estimate resource requirements, and define milestones and deliverables. Monitor workflow and risks; play a leadership role in mitigating risks and removing obstacles. Lead and participate in complex construction, automation, and implementation activities, ensuring successful implementation with architectural and operational requirements met. Establish new standards and best practices to monitor, test, automate, and maintain IT components or systems. Serve as an expert resource in disaster recovery and disaster recovery planning. Stay current with Target’s technical capabilities, infrastructure, and technical environment. Develop fully attributed data models, including logical, physical, and canonical. Influence data standards, policies, and procedures. Install, configure, and/or tune data management solutions with minimal guidance. Monitor data management solution(s) and identify optimization opportunities

About you:
4 year degree or equivalent experience7+ years of software development experience with at least one full cycle implementationDemonstrates strong domain-specific knowledge regarding Target’s technology capabilities, and key competitors’ products and differentiating featuresDemonstrates broad and deep expertise in multiple computer languages and frameworks (e.g., open source). Designs, develops, and approves end-to-end functionality of a product line, platform, or infrastructureCommunicates and coordinates with project team, partners, and stakeholdersDemonstrates expertise in analysis and optimization of systems capacity, performance, and operational healthUnderstands and develops solutions to foster data lifecycle managementMaintains deep technical knowledge within areas of expertiseStays current with new and evolving technologies via formal training and self-directed educationExperience in building highly scalable distributed systems
Americans with Disabilities Act (ADA)
Target will provide reasonable accommodations (such as a qualified sign language interpreter or other personal assistance) with the application process upon your request as required to comply with applicable laws. If you have a disability and require assistance in this application process, please visit your nearest Target store or Distribution Center or reach out to Guest Services at 1-800-440-0680 for additional information.
Qualifications:"
138,Sr. Data Engineer,"San Mateo, CA",San Mateo,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Brief Description
Job Advertisement
Senior Data Engineer

Ten-X Commercial is the CRE marketplace that is a force multiplier for sellers, buyers and brokers. Ten-X precision-matches assets, accelerates close rates, and streamlines the entire transaction process with more than $55 billion in sales and increasing daily. Leveraging desktop and mobile technology, Ten-X allows people to safely and easily complete real estate transactions entirely online. We bring quality assets to the market and attract prospective investors from around the world. By virtue of our best-in-class marketing and scalable technology platform, buyers and seller are able to conduct transactions in an efficient manner.

Ten-X empowers consumers, investors and real estate professionals with unprecedented levels of flexibility, control and simplicity – and the convenience of transacting properties whenever and wherever they want. As real estate continues to move online, Ten-X is uniquely positioned at the forefront of this dramatic industry evolution.

The Role:
Data, and our ability to leverage it, is seen and championed as a key competitive advantage from our CEO on down. We are looking for a top tier data engineer to work with our data engineering team on high impact projects that improve data availability and quality, and provide reliable access to data for the rest of the business. You will be working on key projects that have board level visibility.

Responsibilities

Design, architect and support new and existing data and ETL pipelines and recommend improvements and modifications.
Create optimal data pipeline architecture and systems.
Be responsible for ingesting data into our data lake and providing frameworks and services for operating on that data.
Work with Data Scientists to help dedupe and fuzzy match data
Analyze, debug and correct issues with data pipelines
Identify, design, and implement internal process improvements: automating manual processes and optimizing data delivery.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Spark and AWS technologies.

Experience

Undergraduate degree (ideally a Masters) in a relevant quantitative subject (Math, Statistics, Computer Science, Engineering, Economics, etc.)
8+ Years’ Experience in data engineering, data warehousing, business intelligence including: 5+ years in a modern data stack environment, specifically the Hadoop stack, 3+ Years' Python experience relating to data engineering
Experience with iterative Agile methodologies and use of supporting tools like JIRA, Confluence and Git
Experience in the following will be a plus:
Spark
Kafka
Clickstream data
Machine Learning
Streaming Data
Elastic Search
Containers (Docker)
Fuzzy Matching / NLP
Ability to understand business problems and translate them into data engineering requirements
Understanding and Familiarity with:
Hadoop and all the related stack (Pig, Hive, HBase, etc.)
SQL skills and SQL Databases
Strong oral and written communication skills and be able to communicate complex technical knowledge in meaning terms
Ability to work in a fast-paced environment and fluidly adapt to changing priorities
Must be passionate about getting to the root cause of issues and driving to whys
Proven ability to obtain buy-in/ partner with the data science team, including demonstrated ability to partner with functional leaders toward common goals
Well-developed analytical and interpersonal skills with ability to draw conclusions and communicate/present them confidently and effectively to broad audiences, including senior leadership
High energy and passion about solving business needs through data
Organized, structured thinker with ability to handle multiple assignments, remain calm under pressure, and digest information from multiple, disparate parts
Continuous improvement mindset
Not afraid to challenge conventional thinking or analyses

Bonus:

Cloud migration experience from On-Prem to cloud preferably AWS.
Machine Learning experience

"
139,Senior Data Engineer Sandipan,"San Mateo, CA",San Mateo,CA,None Found,None Found,"
Proven expertise in production software development
7+ years of experience programming in Java, Python, SQL, or C/C++
Proficient in SQL, NoSQL, relational database design and methods for efficiently storing & retrieving data
Strong analytical and science skills
Creative problem solver
Excellent verbal and written communications skills
Strong team player capable of working in a demanding start-up like environment building the next generation Analytical Ecosystem..
Experience building complex and non-interactive systems (batch, real-time, distributed, etc.) Strong experience in working with and managing large distributed computing cluster (combining various technologies & frameworks like Hadoop, Nifi, streamsets, spark, kafka etc.)
","
Prior Data Platform Engineering experience
Experience with Hadoop, Hive, Pig, Avro, Thrift, Protobufs and JMS: ActiveMQ, RabbitMQ, JBoss, etc.
Dynamic and/or functional languages (e.g., Python, Ruby, Scala, Clojure)
Experience designing and tuning high performance systems
Prior experience with data warehousing and business intelligence systems
Experience with Elasticsearch, SolrWeb, and Lucene
Experience with Star Schema, fact vs dimensions, updates/restatements and views
Professional or academic background that includes mathematics, statistics, machine learning and data mining for optimizing the data platform
Linux expertise
Prior work and/or research experience with unstructured data and data modeling
Familiarity with different development methodologies (e.g., agile, waterfall, XP, scrum, etc.)
Demonstrate understanding of ""var"" vs. ""val"", use of multi-return methods, ability to write clean, legible scala code that solves a complex problem
Firm understanding on python memory mode, classes, sub classing, designing classes for re-use, static string constants rather than in-line constants
Understanding of various analytic and visualization utilities available in R
Configure a Jenkins build, create/update a Jira ticket, enable Automated Tests in gradle/maven build
Ability to leverage additional security tools like Ranger, Knox, Sentry to further harden a cluster or secure data access
Understanding of how to segregate data based on access control rules, when and how to encrypt data (whole record vs individual fields) when and how to mask fields, etc.
Able to create and deploy a Samza job via YARN or Mesos, read from a streaming source (like Kafka) and produce some filtered or enhanced output
Able to create a storm topology to filter or transform a steam of data. Ability to track state and isolation in Trident or similar
Able to connect DStream to Kafka or Flume (or similar) queue, filter or transform data and write back to DStream on a different topic/queue
Implementation of D3, Tableau or R graphing technologies that produce an intuitive view of the underlying data
Implement a graph (line or pie etc.) backed by a live (changing) data set, something like ""requests per minute"" or similar
Understand basic modeling techniques, tools sets. Implement simple Python or R analytic routines
",None Found,None Found,None Found,"As a Data Engineer, you will provide technical leadership to the team that designs and develops path-breaking large scale cluster data processing systems.

Design and develop code, scripts and data pipelines that leverage structured and unstructured data integrated from multiple sources. Software installation and configuration. Participate in and help lead requirements and design workshops with our clients. Develop project deliverable documentation. Mentor junior members of the team in software development best practices. Other duties as assigned.

Additionally, as a senior member of our development & Ops team, you will help establish thought leadership in the big data space by contributing best practices, white papers, technical commentary and representing our section as one Leads.

Job Qualifications:

Proven expertise in production software development
7+ years of experience programming in Java, Python, SQL, or C/C++
Proficient in SQL, NoSQL, relational database design and methods for efficiently storing & retrieving data
Strong analytical and science skills
Creative problem solver
Excellent verbal and written communications skills
Strong team player capable of working in a demanding start-up like environment building the next generation Analytical Ecosystem..
Experience building complex and non-interactive systems (batch, real-time, distributed, etc.) Strong experience in working with and managing large distributed computing cluster (combining various technologies & frameworks like Hadoop, Nifi, streamsets, spark, kafka etc.)

Preferred Knowledge, Skills and Abilities:

Prior Data Platform Engineering experience
Experience with Hadoop, Hive, Pig, Avro, Thrift, Protobufs and JMS: ActiveMQ, RabbitMQ, JBoss, etc.
Dynamic and/or functional languages (e.g., Python, Ruby, Scala, Clojure)
Experience designing and tuning high performance systems
Prior experience with data warehousing and business intelligence systems
Experience with Elasticsearch, SolrWeb, and Lucene
Experience with Star Schema, fact vs dimensions, updates/restatements and views
Professional or academic background that includes mathematics, statistics, machine learning and data mining for optimizing the data platform
Linux expertise
Prior work and/or research experience with unstructured data and data modeling
Familiarity with different development methodologies (e.g., agile, waterfall, XP, scrum, etc.)
Demonstrate understanding of ""var"" vs. ""val"", use of multi-return methods, ability to write clean, legible scala code that solves a complex problem
Firm understanding on python memory mode, classes, sub classing, designing classes for re-use, static string constants rather than in-line constants
Understanding of various analytic and visualization utilities available in R
Configure a Jenkins build, create/update a Jira ticket, enable Automated Tests in gradle/maven build
Ability to leverage additional security tools like Ranger, Knox, Sentry to further harden a cluster or secure data access
Understanding of how to segregate data based on access control rules, when and how to encrypt data (whole record vs individual fields) when and how to mask fields, etc.
Able to create and deploy a Samza job via YARN or Mesos, read from a streaming source (like Kafka) and produce some filtered or enhanced output
Able to create a storm topology to filter or transform a steam of data. Ability to track state and isolation in Trident or similar
Able to connect DStream to Kafka or Flume (or similar) queue, filter or transform data and write back to DStream on a different topic/queue
Implementation of D3, Tableau or R graphing technologies that produce an intuitive view of the underlying data
Implement a graph (line or pie etc.) backed by a live (changing) data set, something like ""requests per minute"" or similar
Understand basic modeling techniques, tools sets. Implement simple Python or R analytic routines

Job Abilities:
Must be able to sit for long periods of time working on computers. Must be able to interact and communicate with the senior management in meetings. Must be able to write programming code in applicable languages and write project documentation in English.

Education:
Bachelor's Degree or foreign equivalent in Computer Science or related technical field followed by six (6-8) years of progressively responsible professional experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).

OR

Master's Degree or foreign equivalent in Computer Science or related technical field. Four (4-5) years of experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).

Employer will accept any suitable combination of education, training, or experience."
140,Azure Data Engineer,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,"At least 5 years of consulting or client service delivery experience on Azure
",DevOps on an Azure platform,None Found,None Found," Proven ability to build, manage and foster a team-oriented environment
","Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
141,Principal Data Engineer,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Plume develops and deploys cloud based control planes with scale to manage tens of millions of customer homes through some of the world's largest Internet Service Providers. Our cloud applications include WiFi network management and optimization, device access control, network provisioning, IoT security, and end customer user interaction through mobile apps.

We are growing our team and looking for talented individuals to help us define and drive the success of our cloud based service offering. Our focus is on the home market and we support B2B and B2C product offerings.

The Opportunity:
As a Senior Data Engineer at Plume, you will focus on providing actionable insights and build highly available resilient systems that will impact and help make business decisions. You will ensure that the platform service we are delivering will meet both our and our customers' reliability expectations.

What you will do:

Build infrastructure and abstractions that can enable anyone (engineer or data scientist) to craft a scalable ETL pipeline for whatever the purpose is: metrics, analysis, machine learning, dashboard visualizations
Work closely with ops team to monitor and tune existing infrastructure.
Make intuitive decisions about what services, frameworks, and capabilities need to be in place before they are needed.
Build and maintain a data collection system that robustly extracts meaningful data from multiple sources and data stores.
Build analytics and Machine learning platforms to collect, store, process, and analyze huge sets of data

Who you are:

BA/BS in Computer Science, Information Systems or related technical field.
3+ years of experience in Data Infrastructure, with Cloud SW experience a plus.
Experience in crafting and scaling data infrastructure, models, and pipelines
Hands-on experience with a variety of data infrastructures, such as:
Processing: Spark, Flink, Hadoop, Lambda
Messaging: Kafka, Zookeeper, Pulsar
Storage: Hive, Mongo DB, Athena, Phoenix, Splice, Redshift, DynamoDB
Machine Learning: Sagemaker, H2O, Keras, NumPy
Open and active in sharing knowledge as well as excellent communication skills
Programming experience in one or more application or systems languages including Scala, Java, or Python
Have an ability to own a project from inception to completion

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
142,Senior Data Scientist,"Santa Clara, CA",Santa Clara,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Our Mission
At Palo Alto Networks® everything starts and ends with our mission:
Being the cybersecurity partner of choice, protecting our digital way of life.
We have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish – but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
Your Career
You will have the opportunity to build a career solving network security challenges leveraging on advanced data analytics and data driven technologies. You will help create and deliver the most advanced AI and Machine Learning enabled solution to discover threats, provide intelligence and protect IoT devices.
Your Impact:
You will work in a fast paced team to create and deliver new product and mission mission features to the IoT security platform that many customers use on a daily basis. You will actively engage and contribute to the security community through collaborations. You will be part of the team that is leading and driving industry technology evolution in security and creating positive impact on business.
Your Experience:
PhD in mathematics, science or technology
2+ years industry experience as a data scientist
Domain expert knowledge and solid skills in computer science, applied math, statistics, and machine learning
Exposure in networking and security is a plus
Proficient in at least one general programming language such as Python, Java, C++ or Scala
Excellent communication skills with the ability to influence at all levels of the organization
A self driven data scientist and data engineer and an excellent team player
The Team
We’re not your ordinary Information Security team. We’re a diverse group of security professionals that accepts challenging the status quo in order to protect Palo Alto Networks and our customers.
Driving innovation on the Information Security team of the fastest-growing high-tech cybersecurity company is a once in a lifetime opportunity. You’ll be joined by the brightest minds in technology, and our global teams are on the front line of defense against cyberattacks.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.
We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics."
143,Big Data Engineer - Associate Manager,"San Jose, CA 95113",San Jose,CA,95113,None Found,"
Minimum 5 plus years of hands-on technical experience implementing Big Data solutions utilizing Hadoop (or other Data Science and Analytics platforms.)
Minimum 5 plus years of experience with a full life cycle development from functional design to deployment
Minimum 5 plus years of hands-on technical experience with delivering Big Data Solutions in the cloud with AWS or Azure
Minimum 5 plus years of hands-on technical experience in developing solutions utilizing at least two of the following:
Kafka based streaming services
R Studio
Cassandra , MongoDB
MapReduce, Pig, Hive
Scala, Spark
knowledge on Jenkins, Chef, Puppet
Bachelor's degree or equivalent (minimum 12 years) work experience. (If Associate’s Degree, must have minimum 6 years work experience)
Candidates must be able to travel Monday - Thursday on a weekly basis. This is also referred to as 100% travel.
",None Found,None Found,None Found,None Found,"Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions – underpinned by the world’s largest delivery network – Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With approximately 469,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward.
Analytics professionals create new insights from predictive statistical modeling activities that target and deliver value to our clients.
Job Description
A professional at this position level within Accenture has the following responsibilities:
Adapts existing methods and procedures to create alternative solutions to moderately complex problems.
Understands the strategic direction set by senior management as it relates to team goals.
Uses own judgment to determines optimal solution to recommend.
Primary upward interaction is with direct supervisor or teams leads. Generally interacts with peers and/or management levels at a client and/or within Accenture.
Determines methods and procedures on new assignments with minimal guidance.
Decisions often impact the team in which they reside and occasionally impact other teams.
Manages medium-small sized teams and/or work efforts (if in an individual contributor role) at a client or within Accenture.

Basic Qualifications
Minimum 5 plus years of hands-on technical experience implementing Big Data solutions utilizing Hadoop (or other Data Science and Analytics platforms.)
Minimum 5 plus years of experience with a full life cycle development from functional design to deployment
Minimum 5 plus years of hands-on technical experience with delivering Big Data Solutions in the cloud with AWS or Azure
Minimum 5 plus years of hands-on technical experience in developing solutions utilizing at least two of the following:
Kafka based streaming services
R Studio
Cassandra , MongoDB
MapReduce, Pig, Hive
Scala, Spark
knowledge on Jenkins, Chef, Puppet
Bachelor's degree or equivalent (minimum 12 years) work experience. (If Associate’s Degree, must have minimum 6 years work experience)
Candidates must be able to travel Monday - Thursday on a weekly basis. This is also referred to as 100% travel.
Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills
All of our consulting professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a federal contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
144,Data Scientist,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,"Clear and effective English communicator of technical issues
Team player who works effectively with self-managed cross-functional teams
Organized, methodical, and detail oriented
Ability to work with geographically dispersed teams
Committed to the success of the team and the company",None Found,None Found,None Found,"Data Scientist

Job Summary
The data scientist will play a leading role in Machine Learning team and work directly on flagship analytics products, which will be deployed in ThreatMetrics global digital identity network. The focus of deliverables will be on building high performing fraud detection models that will be used by thousands of our customers in risk management for payment, account login, and new account creation. We are the leader in digital identity field, and the candidate will be able to get opportunities to work on innovative digital identity products and continually bring global digital identity intelligence into customer facing machine learning products.
Our data scientists work on end-to-end modeling projects, involving handling extremely large volume transaction data, focusing on exploratory data analysis, feature construction and feature engineering, model building and performance evaluation, and also be responsible for model deployment and post-deployment performance analysis. There are many opportunities in conducting proof-of-concept research and study to explore advanced modeling techniques and bring the most innovative products and best value to our customers.
About You

ThreatMetrix is seeking an outgoing, curious, interdisciplinary senior machine learning data engineer that has a passion for modeling and data analysis.
Bring a combination of mathematical rigor and innovative algorithm design to create advanced recipes that extract intelligence from billions of rows of data to improve our digital identity solutions and fraud detection capabilities.
Build efficient data pipeline and translate unstructured, complex business problems into well-defined feature space, implement elegant and efficient learning algorithms to work at scale.
Experience
MS or PhD in Computer Science, Statistics, Math, Operations Research, etc.
5+ years of quantitative analytics and data engineering experience with large data-sets, from prototyping to modeling and to implementation with business impact.
Expected Personal Skills
Clear and effective English communicator of technical issues
Team player who works effectively with self-managed cross-functional teams
Organized, methodical, and detail oriented
Ability to work with geographically dispersed teams
Committed to the success of the team and the company
Required Technical Skills
Hands-on and effective analysis on large amount of historical data leading to the better understanding of the data implications.
Knowledgeable about statistical modeling, machine learning techniques and working experience from prototyping to production.
Deeper understanding of feature engineering and variable selection. Practical knowledge of regression models, decision trees, anomaly detection, and deep learning.
Fluent with Python/R, Java, and Unix/Linux platform. Experienced with best practices in software design and testing.
Strong experience with latest big data computation and analytics platforms (Hadoop, Spark, Kafka, Impala, etc.).
Experience in computer security, risk management and fraud detection.
 The advertised position is a full-time position located in our office in San Jose.

About ThreatMetrix

ThreatMetrix®, The Digital Identity Company™, and now part of LexisNexis Risk Solutions, is the market-leading cloud solution for authenticating digital identities and transactions on the Internet. Verifying more than 30 billion annual transactions supporting over 5,000 customers globally through the ThreatMetrix® Digital Identity Network, ThreatMetrix secures businesses and end users against account takeover, payment fraud and fraudulent account registrations resulting from malware and data breaches. Key benefits include an improved customer experience, reduced friction, revenue gain, and lower fraud and operational costs. The ThreatMetrix solution is deployed across a variety of industries, including financial services, e-commerce, payments and lending, media, government, and insurance.
About the Team

The ThreatMetrix engineering team is an international team that includes experts in digital identity, device intelligence, fraud detection, real time systems, Software as a Service (SaaS) applications, machine learning, and data analytics. We are an Agile engineering team using concepts such as Scrum, Continuous Integration, and Continuous Improvement. We leverage many powerful open source packages including Hadoop, Impala, Kafka, Spark, Aerospike and many more to build our solutions.


At LexisNexis Risk Solutions, we believe in the power of data and advanced analytics for better risk management. With over 40 years of expertise, we are the trusted data analytics provider for organizations seeking actionable insights to manage risks and improve results while upholding the highest standards for security and privacy. Headquartered in metro Atlanta, LexisNexis Risk Solutions serves customers in more than 100 countries and is part of RELX Group plc, a world-leading provider of information and analytics for professional and business customers across industries. For more information, please visit www.lexisnexisrisk.com. LexisNexis Risk Solutions is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. If a qualified individual with a disability or disabled veteran needs a reasonable accommodation to use or access our online system, that individual should please contact 1.877.734.1938 or accommodations@relx.com."
145,Senior Data Engineer - Apple,"Cupertino, CA 95014",Cupertino,CA,95014,None Found,"
6+ years of hands-on data modeling and data engineering experience
Strong expertise in dimensional modeling and data warehousing
Database design and development experience with relational or MPP databases such as Postgres/ Oracle/ Teradata/ Vertica
Experience in design and development of custom ETL pipelines using SQL and scripting languages (Python/ Shell/ Golang)
Proficiency in advanced SQL, performance tuning
Hands on experience with Big-Data platform like Hadoop, MapReduce, Hive etc
Experience with cloud computing platforms like AWS, Google Cloud
Familiarity with version control and migration tools for database and software
Experience working with APIs will be a plus",None Found,None Found,None Found,None Found,"Job Summary
Apple is seeking an experienced, detail-minded data engineering
consultant to join our worldwide business development and strategy team. If you are someone
who looks forward to solving complex business problems and is excited about this opportunity,
please reach out to us.
Job Description
You will design and build data warehouses on cloud, to provide efficient analytical and reporting capabilities across Apple's global and regional sales and finance teams.
You will develop highly scalable data pipelines to load data from various source systems, use Apache Airflow to orchestrate, schedule and monitor the workflows.
You will be required to understand existing solutions, fine-tune them and support them as needed. Data quality is our goal and we expect you to meet our high standards on data and software quality.
We seek a self starter, who is willing to learn fast, adapt well to changing requirements and work with cross functional teams.
Key Qualifications
6+ years of hands-on data modeling and data engineering experience
Strong expertise in dimensional modeling and data warehousing
Database design and development experience with relational or MPP databases such as Postgres/ Oracle/ Teradata/ Vertica
Experience in design and development of custom ETL pipelines using SQL and scripting languages (Python/ Shell/ Golang)
Proficiency in advanced SQL, performance tuning
Hands on experience with Big-Data platform like Hadoop, MapReduce, Hive etc
Experience with cloud computing platforms like AWS, Google Cloud
Familiarity with version control and migration tools for database and software
Experience working with APIs will be a plus
Education & Experience
BS or MS in Engineering/ Computer Science

Theorem expects employees to be honest, trustworthy, and operate with integrity. Discrimination and all unlawful harassment (including sexual harassment) in employment are not tolerated. We encourage success based on our individual merits and abilities, and all decisions regarding recruitment, hiring, promotion, compensation, employee development decisions such as training, and all other terms and conditions of employment, will be made without regard to race, nationality, national origin, citizenship status, employment status, ethnicity, ethnic origin, color, creed, religion, belief, age, marital status, pregnancy, gender, gender identity, sexual preference, lifestyle, social class, military status, disability, physical features, or any other protected status. We oppose all forms of unlawful or unfair discrimination. Theorem is an equal opportunity employer.

No statements by Theorem are intended to create an offer of employment unless made in writing signed by an officer of the company, and no offer shall become effective unless countersigned by the prospective employee.
4S8o0qylz4"
146,Data Engineer,"Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Hiretual is an AI-powered sourcing platform, and has been recognized as one of the best recruiting tools on the market. During 2018, Hiretual achieves 500% growth with minimal focus on sales to date.

As a data engineer engineer, you will join the core engineering team to build scalable and robust data engine towards an AI and data-driven recruiting SaaS. You will be working with top AI researchers and infrastructure gurus to explore unlimited career space.

The core technical skills you should have:
Strong computer science fundamentals: algorithms, data structures, and object-oriented programmingStrong coding capability with Pythonmust have 2+ years of experience in data processing pipeline, including data crawling, cleaning, processing, ETLProficient in working variant databases: MySQL, Redis, Cassandra, Elasticsearch, graph databases.Proficient with big data processing frameworks: Spark, Hadoop, Hive, Kafka, EMRWriting scalable REST APIs for web services
Benefits
Unlimited growth/promotion spaceCompetitive salary and options401k matching programFree meals, snacks, and drinksComprehensive medical, dental, and life insurancePTO policyCommuter benefitsFun, collaborative, and energetic team environment with nice office environment"
147,Big Data Engineer,"Cupertino, CA",Cupertino,CA,None Found,None Found,None Found,None Found,"
Participate in design and development of Big Data analytical applications
Design, support and continuously enhance the project code base, continuous integration pipeline, etc.
Write complex ETL processes and frameworks for analytics and data management
Implement large-scale near real-time streaming data processing pipelines
Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale",None Found,"
Strong coding experience with Scala, Java, or Python
In-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams)
Understanding of the best practices in data quality and quality engineering
Experience with version control systems, Git in particular
Desire and ability for quick learning of new tools and technologies","We are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in design and implementation of a top-notch Big Data solution to be deployed at massive scale.

Our customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world.

On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.

Responsibilities:
Participate in design and development of Big Data analytical applications
Design, support and continuously enhance the project code base, continuous integration pipeline, etc.
Write complex ETL processes and frameworks for analytics and data management
Implement large-scale near real-time streaming data processing pipelines
Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale
Requirements:
Strong coding experience with Scala, Java, or Python
In-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams)
Understanding of the best practices in data quality and quality engineering
Experience with version control systems, Git in particular
Desire and ability for quick learning of new tools and technologies
What will be a plus:
Knowledge of Unix-based operating systems (bash/ssh/ps/grep etc.)
Experience with Github-based development processes
Experience with JVM build systems (SBT, Maven, Gradle)

What we offer:
Work in the Bay Area with terrific customers on large, innovative projects.

High-energy atmosphere of exponentially & successfully growing company.

A very attractive compensation package with generous benefits (medical, dental, vision and life), 401K and Section 125 pre-tax offerings (POP and FSA plans).

NB:

Placement and Staffing Agencies need not apply. We do not work with C2C at this time.
At this moment, we are not able to process H1B transfers. Applicants with CPT and OPT visas are welcome to apply.

About Us:
Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors.

We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season.

Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, omnichannel services, DevOps, and cloud enablement"
148,Machine Learning Engineer,"Palo Alto, CA 94301",Palo Alto,CA,94301,None Found,None Found,None Found,None Found,None Found,None Found,"At Liftoff, we're solving one of the core problems faced by every mobile app: growth. To do so, we build Machine Learning models and infrastructure that can accurately predict which apps a user will like and how to connect them in a compelling way. Our systems operate at a scale unseen outside of the largest Internet companies -- processing over a million requests per second and interacting with over a billion users. Our technology is creative and we have strong product-market fit; as a result, we've already reached profitability and are seeing tremendous growth.

As a Machine Learning Engineer at Liftoff, you will:

Own both the ML models and the underlying software tooling and infrastructure. Our ML Engineer role combines the classic ""ML Scientist"" and ""Data Engineer"" role at other companies.
Have a closed feedback loop from hypothesis generation to live AB testing, with no cross-team friction and sub-day iteration cycles.
Take on unique modeling challenges not covered in the scientific literature, like extreme positive sample sparsity and labelling delay.
Work with modeling techniques at the state-of-the-art of probability prediction, as well as a multitude of other ML areas from NLP to CNNs.
Become an expert in Clojure, Go, and the many other cutting-edge open source technologies that maximize our development velocity.
Join a nimble, consistently excellent, and experienced engineering team (former Google/LI/Ooyala/etc).

Desired qualities and experiences:

Very strong coding ability (experience in Go and Clojure is a plus).
2+ years of industry experience applying Machine Learning to large scale problems.
Strong core CS fundamentals (data structures, algorithms, architecting systems).
A passion for quality and excellence, and the ability to temper it when necessary to ship.
Sets ego aside in pursuit of finding the best solution, no matter where it comes from.
Self-motivated and a great ability to hustle.
B.S. or higher in Computer Science. PhD a big plus.

We are an equal opportunity employer and value diversity at our company. Come join our team and help us shape the future of mobile growth!"
149,AWS Data Engineer,"San Jose, CA 95113",San Jose,CA,95113,None Found,"At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.","DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud",None Found,None Found," Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
§ Certified AWS Developer - Associate
§ Certified AWS DevOps – Professional (Nice to have)
§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
150,"Senior Software Engineer, Big Data","Mountain View, CA 94039",Mountain View,CA,94039,None Found,"
BS in Computer Science. MS Preferred
Strong CS fundamentals - data structures, algorithms and distributed systems
Strong database fundamentals including SQL, performance and schema design
Strong programming skills in Scala, Java, Python, or similar
8+ years of hands-on software engineering experience
5+ years of experience integrating technical processes and business outcomes – specifically: data architecture and analysis, data quality metrics/monitoring
3+ years DevOps experience including configuration, optimization, backup, high reliability, monitoring and systems
Experience with Hadoop, Hive, Spark, Kafka, Storm, Druid, Cassandra, Columnar Databases and Graph Databases like DSE Graph
Experience with various offerings from AWS - S3, EMR, Redshift, Athena and Kinesis
Strong leadership experience: Technical “go-to” person, cross-functional leader and build strong relationships
History of contributing to open source projects is a plus",None Found,"
Working on Data Warehousing, Personalization, Fraud, Graph Database Solutions
Ability to work in a high velocity agile environment with customer obsession
Design and develop big data and real-time analytics solutions
Implementing best practices for architecture, design, coding standards & CI/CD
Work cross-functionally with product management, business units to drive forward result
Coaching engineers in the team to be a force multiplier",None Found,None Found,"Overview
Intuit’s Small Business Group (SBG) develops and brings to market the products that small businesses and accountants depend on every day for their success. These products include accounting products (QuickBooks), payments, payroll and others.

The SBG Data Engineering team is looking for a Staff Data Engineer with a winning record of accomplishment in Big Data and Distributed Systems. We are using data in groundbreaking ways to discover customer insights, personalize customer experiences, and provide a unified customer view across all SBG products.
Responsibilities
Working on Data Warehousing, Personalization, Fraud, Graph Database Solutions
Ability to work in a high velocity agile environment with customer obsession
Design and develop big data and real-time analytics solutions
Implementing best practices for architecture, design, coding standards & CI/CD
Work cross-functionally with product management, business units to drive forward result
Coaching engineers in the team to be a force multiplier
Qualifications
BS in Computer Science. MS Preferred
Strong CS fundamentals - data structures, algorithms and distributed systems
Strong database fundamentals including SQL, performance and schema design
Strong programming skills in Scala, Java, Python, or similar
8+ years of hands-on software engineering experience
5+ years of experience integrating technical processes and business outcomes – specifically: data architecture and analysis, data quality metrics/monitoring
3+ years DevOps experience including configuration, optimization, backup, high reliability, monitoring and systems
Experience with Hadoop, Hive, Spark, Kafka, Storm, Druid, Cassandra, Columnar Databases and Graph Databases like DSE Graph
Experience with various offerings from AWS - S3, EMR, Redshift, Athena and Kinesis
Strong leadership experience: Technical “go-to” person, cross-functional leader and build strong relationships
History of contributing to open source projects is a plus"
151,Big Data Software Developer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"What You'll Do
As a Data Engineer of the Portals and Data Analytics Engineering team, you will have a unique opportunity to use your creativity in cloud infrastructure to develop innovative big data analytics platform.
We are seeking a talented, self-motivated, and highly creative software engineer to join our team to develop new big data platform project.
Work with team to develop big data platform, deliver task with good quality, establish Cisco credibility platform for a Cisco Collaboration solution.
Work on our large-scale data platform, build, manage or monitor them.
Works closely with global team on engagements by estimating the complexity and duration of technical tasks; and providing business value justification and risk evaluation
Who You'll Work With
The Cloud Collaboration Technology Group (CCTG) is a $1B business unit that develops primarily cloud based collaboration software solutions, and is an integral part of the $4.4B Collaboration portfolio at Cisco, which is investing heavily to transform the future of collaboration experiences for our end users.
CCTG’s vision is to be the recognized industry leader in the Cloud Collaboration market as well as to be the industry beacon that attracts top talent. Our strategy is to sustain innovation in our current $1B product line, and to develop innovative new products for the future that expand and positively disrupt our market. We will do all of this, and build a next generation cloud platform with APIs and an ecosystem that deliver tangible business outcomes enabling our customers to get more jobs done!
Who You Are
Have good understanding of data analytics best practices, including skills such as data modeling, data cleaning, data mining, machine learning and data virtualization. Be an expert in below two or more areas
Data storage – experienced in implementing Big Data technologies successfully in an enterprise;
Knowledge discovery – advanced knowledge in entity and relationship extraction from unstructured data;
Data governance – experienced in developing and integrating software allowing for flexible and scalable data transformation with data quality controls.
Data analytics – strong backgrounds in statistics, machine learning and similar technologies.
Data visualization – knowledge of tools that are cost-effective and make it easy for end users to better understand and produce reports and graphs.
Hadoop Administration - Contribute to the evolving architecture to meet growth requirements for scaling, reliability, performance and security.
3-5 years hands- on experienced in Hadoop, Cassandra, Kafka, Storm, Spark etc big data tools. A qualified java/Scala engineer.
Flexible, Self-motivated, Problem solver who can work both individually as well as an effective Team player with excellent attention to detail and great interpersonal skills (both verbal and written)
Be willing/able to work crossing boundary to reach best fit solution in real world cloud service.
Cisco is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.
Cisco will consider for employment, on a case by case basis, qualified applicants with arrest and conviction records.
We Are Cisco
#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool."
152,Data Engineer,"San Mateo, CA",San Mateo,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Brief Description
Job Advertisement
Data Engineer

Ten-X Commercial is the CRE marketplace that is a force multiplier for sellers, buyers and brokers. Ten-X precision-matches assets, accelerates close rates, and streamlines the entire transaction process with more than $55 billion in sales and increasing daily. Leveraging desktop and mobile technology, Ten-X allows people to safely and easily complete real estate transactions entirely online. We bring quality assets to the market and attract prospective investors from around the world. By virtue of our best-in-class marketing and scalable technology platform, buyers and seller are able to conduct transactions in an efficient manner.
Ten-X empowers consumers, investors and real estate professionals with unprecedented levels of flexibility, control and simplicity – and the convenience of transacting properties whenever and wherever they want. As real estate continues to move online, Ten-X is uniquely positioned at the forefront of this dramatic industry evolution.

https://www.ten-x.com/

The Role:
Data and our ability to leverage it is seen and championed as a key competitive advantage from our CEO on down. We are looking for a top tier data engineer to work with our data science team on building out proprietary tools and models around our customer and asset data (both internal and external sets). You will be working on key projects that have board level visibility.

Responsibilities

Play a leading role in designing, developing and implementing Big Data databases (Hadoop, Graph, MySQL, NoSQL, MongoDB) that contains multiple data sets from both internal and external sources
Lead the setup of data pipelines of new internal and external data sets into the database
Work with Data Scientists to help dedupe and fuzzy match data
Work with software engineers on developing APIs

Experience

Undergraduate degree (ideally a Masters) in a relevant quantitative subject (Math, Statistics, Computer Science, Engineering, Economics, etc.)
5+ Years’ Experience in data engineering, including: 2+ years in a modern data stack environment, specifically the Hadoop stack, 3+ Years' Python experience relating to data engineering
Experience with iterative Agile methodologies and use of supporting tools like JIRA, Confluence and Git
Experience in the following will be a plus:
Spark
Kafka
Clickstream data
Machine Learning
Streaming Data
Elastic Search
Containers (Docker)
Fuzzy Matching / NLP
Ability to understand business problems and translate them into data science requirements
Understanding and Familiarity with:
Hadoop and all the related stack (Pig, Hive, HBase, etc.)
SQL skills and SQL Databases
Strong oral and written communication skills and be able to communicate complex technical knowledge in meaning terms
Ability to work in a fast-paced environment and fluidly adapt to changing priorities
Must be passionate about getting to the root cause of issues and driving to whys
Proven ability to obtain buy-in/ partner with the data science team, including demonstrated ability to partner with functional leaders toward common goals
Well-developed analytical and interpersonal skills with ability to draw conclusions and communicate/present them confidently and effectively to broad audiences, including senior leadership
High energy and passion about solving business needs through data
Organized, structured thinker with ability to handle multiple assignments, remain calm under pressure, and digest information from multiple, disparate parts
Continuous improvement mindset
Not afraid to challenge conventional thinking or analyses

"
153,Data Engineer - AI Labs,"Palo Alto, CA 94301",Palo Alto,CA,94301,None Found,"
3-5+ years of experience in a data engineer role with a BA or MS degree in a quantitative discipline (computer science, mathematics, statistics, data science, economics, physics, engineering or related field)
Experience with building and optimizing ‘big data’ pipelines, architectures, and data sets. Familiarity with data pipeline and workflow management tools Luigi, Airflow
Advanced working SQL knowledge and experience with relational databases.
Experience with Hadoop, Spark, and Kafka
Experience with Amazon AWS and Google Cloud Platforms
Experience with stream-processing systems: Storm, Spark-Streaming
Experience with OO or object scripting language such as Python, Scala, and Java",None Found,"
Contribute to the creation and maintenance of optimized data pipeline architectures on large and complex data sets.
Assemble large, complex data sets that meet BlackRock business requirements.
Act as lead to Identify, design, and implement internal process improvements and relay to relevant technology organization.
Work with stakeholders to assist in the data-related technical issues and support their data infrastructure needs.
Automate manual ingest processes and optimize data delivery subject to service level agreements; work with infrastructure on re-design for greater scalability.
Keep data separated and segregated according to relevant data policies.
Work with data scientists to develop data ready tools to support their job.
Assist in the development of business recommendations with effective presentation of findings at multiple levels of stakeholders using visual analytic displays of quantitative information. Communicate findings with stakeholders as necessary.",None Found,None Found,"About this role
Data Science at BlackRock:
In February 2018, BlackRock announced the creation of a new central Data Science team in order to accelerate innovation and technology in artificial intelligence, and to have firm-wide impact using data science to solve strategic problems. The team is led by two experienced data science leaders, Dr. Sherry Marcus and Dr. Rachel Schutt.
BlackRock manages $6.2+ trillion in assets on behalf of investors worldwide. As such, there is a rich problem space for data scientists and engineers across all areas of the business including investments, sales, marketing, operations, product, UX, etc. and the potential to have large scale impact.
The kinds of problems you’d be working on:
Building a dynamic pricing and auto-bidding engine for the security lending business
Alpha generation: extracting signals from alternative data sets that provide investment opportunities to investors.
Predictive models in sales and marketing applications in order to anticipate client behavior and needs.
Natural language processing in order to extract and correlate n-grams from unstructured text including from financial reports, news, and contracts in order to drive contextual understanding in different business applications across the firm.
Graph Analysis for path generation for data lineage/provenance, ontological development, or network analytics.
Automating repeatable tasks done by humans to free them up to work on the tasks that require their human intelligence
The firm-wide policy on algorithmic accountability and ethics of data science
The team you’d be part of:
In the first year, the team has grown to 30+ data scientists and data engineers. The team works collaboratively; and is a multi-disciplinary team with the following skills and capabilities: machine learning, statistical modeling, exploratory data analysis, natural language processing, data visualization, network/graph modeling, ETL, data pipelines, data architecture, communication, project /product management and strategy. We work with data from a wide variety of sources including text, news feeds, financial reports, time series transactions, user behavior logs, imagery, and real-time data.
We will be hiring a mix of tech leads and individual contributors with deep expertise in certain areas, as well as generalists. All individuals will be expected to have solid statistical/mathematical, and/or algorithmic/computational foundation and writing code is required. Each individual will be expected to contribute and lead based on their experience and expertise.
As this is a new team, we will be in start-up mode, so you would be helping to build and shape the team and culture from the ground up. You should therefore be comfortable with ambiguity and willing to be pro-active in your contributions, and evolve as the team grows.
We are looking for candidates with unique backgrounds and diverse skill sets with fresh perspectives to accelerate and amplify our efforts to make an impact at BlackRock. Data Science Core aims to bring best of class technologies, analytics, and insights to the entirety of the firm and to our clients utilizing data from a wide variety of sources including text, news feeds, financial reports, time series transactions, logs, imagery, and real-time data.
Check this out:
BlackRock in the News
Job Description:
As Data Engineer, you will improve BlackRock’s product and services suite by creating, expanding and optimizing our data and data pipeline architecture. You will act as architecture lead on a multi-discipline, multi-region team of data scientists, engineers, and investment professionals on a corporate-wide set of client, investor, and operational problems. You will create and operationalize data pipelines to enable squads to deliver high quality data-driven product. You will be accountable for managing high-quality datasets exposed for internal and external consumption by downstream users and applications. The successful candidate will be highly motivated to be the lead architect that creates, optimizes, or redesigns data pipelines to support our next generation of products and data initiatives.
Responsibilities:
Contribute to the creation and maintenance of optimized data pipeline architectures on large and complex data sets.
Assemble large, complex data sets that meet BlackRock business requirements.
Act as lead to Identify, design, and implement internal process improvements and relay to relevant technology organization.
Work with stakeholders to assist in the data-related technical issues and support their data infrastructure needs.
Automate manual ingest processes and optimize data delivery subject to service level agreements; work with infrastructure on re-design for greater scalability.
Keep data separated and segregated according to relevant data policies.
Work with data scientists to develop data ready tools to support their job.
Assist in the development of business recommendations with effective presentation of findings at multiple levels of stakeholders using visual analytic displays of quantitative information. Communicate findings with stakeholders as necessary.
Qualifications:
3-5+ years of experience in a data engineer role with a BA or MS degree in a quantitative discipline (computer science, mathematics, statistics, data science, economics, physics, engineering or related field)
Experience with building and optimizing ‘big data’ pipelines, architectures, and data sets. Familiarity with data pipeline and workflow management tools Luigi, Airflow
Advanced working SQL knowledge and experience with relational databases.
Experience with Hadoop, Spark, and Kafka
Experience with Amazon AWS and Google Cloud Platforms
Experience with stream-processing systems: Storm, Spark-Streaming
Experience with OO or object scripting language such as Python, Scala, and Java
About BlackRock
BlackRock’s purpose is to help more and more people experience financial well-being. As a fiduciary to investors and a leading provider of financial technology, our clients turn to us for the solutions they need when planning for their most important goals. As of June 30, 2019, the firm managed approximately $6.84 trillion in assets on behalf of investors worldwide. For additional information on BlackRock, please visit www.blackrock.com/corporate | Twitter: @blackrock | Blog: www.blackrockblog.com | LinkedIn: www.linkedin.com/company/blackrock.
BlackRock is proud to be an Equal Opportunity and Affirmative Action Employer. We evaluate qualified applicants without regard to race, color, national origin, religion, sex, sexual orientation, gender identity, disability, protected veteran status, and other statuses protected by law.
We recruit, hire, train, promote, pay, and administer all personnel actions without regard to race, color, religion, sex (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), sex stereotyping (including assumptions about a person’s appearance or behavior, gender roles, gender expression, or gender identity), gender, gender identity, gender expression, national origin, age, mental or physical disability, ancestry, medical condition, marital status, military or veteran status, citizenship status, sexual orientation, genetic information, or any other status protected by applicable law. We interpret these protected statuses broadly to include both the actual status and also any perceptions and assumptions made regarding these statuses.
BlackRock will consider for employment qualified applicants with arrest or conviction records in a manner consistent with the requirements of the law, including any applicable fair chance law."
154,Sr Java / API Engineer - Data Platform Engineering,"Sunnyvale, CA 94086",Sunnyvale,CA,94086,None Found,None Found,None Found,None Found,None Found,"
BS degree in Computer Science, Information Systems, Applied Mathematics, Statistics or relevant work experience in software development
3+ years of experience in developing software applications including: analysis, design, coding, testing, deploying and supporting of applications","Description:
JOIN US AS A SR DATA ENGINEER – JAVA / APIs for DIGITAL (TARGET.COM) MEASUREMENT AND OPTIMIZATION PLATFORM
About us:
The Target Data Engineering team is a global, dynamic and collaborative team that works closely with data scientists and Digital Measurement/Optimization partners to create valuable insights using voluminous data collected from internal and external systems. Business operations are empowered by these insights to achieve Target’s strategic initiatives while providing world-class shopping experiences!
Use your skills, experience and talents to be a part of groundbreaking thinking and visionary goals with Target’s Data Science and Engineering team in Sunnyvale/Bay area or Minneapolis.
As a Sr. Data Engineer, you will take the lead as you work on:
Designing and Implementation
Ownership and development of tested software systems employing CI/CD best practices.
Partner with other engineers and business to develop software that meets business needs
Follow Agile methodology for software development and technical documentation
Analyze, monitor and optimize for performance
Write code and unit tests, work on API’s, automation and testing
Produce and maintain high-quality technical documentation
Scope of Work
Will gain new knowledge from upcoming research and applying it to design and development
Develop deep understanding of tie-ins with other platforms within the supported domains
Share the gained knowledge with other team members through interactive sessions
Identify opportunities to adopt innovative technologies
Works with Product Owners and Lead Data Engineers to prioritize features for ongoing sprints and managing the list of technical requirements based on new known defects and issues
Provides continuous support for design, development and application availability
Requirements:
BS degree in Computer Science, Information Systems, Applied Mathematics, Statistics or relevant work experience in software development
3+ years of experience in developing software applications including: analysis, design, coding, testing, deploying and supporting of applications
Expert building and operating REST API services using Java (designing, coding, unit and functional testing, logging, monitoring, performance tuning and JVM profiling)
Experience working with SQL and No-SQL databases
Understanding of different programming paradigms like OOPS (SOLID), functional, reactive and synchronous vs asynchronous
Demonstrated understanding of data structures and designing of algorithms
Experience working on projects facilitating true CI/CD using Drone, Docker, Spinnaker etc.
Collaborative personality and constant learner mentality that enjoys solving large scale challenges
Americans with Disabilities Act (ADA)
Target will provide reasonable accommodations (such as a qualified sign language interpreter or other personal assistance) with the application process upon your request as required to comply with applicable laws. If you have a disability and require assistance in this application process, please visit your nearest Target store or Distribution Center or reach out to Guest Services at 1-800-440-0680 for additional information.
Qualifications:"
155,Lead Data Engineer,"Newark, CA 94560",Newark,CA,94560,None Found,"Bachelor or Masters in Software Engineering and Computer Science
8+ years of experience in design and development of large scale data platforms
Expert in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Proficient in Spark development with PySpark or Scala
Expert in Data streaming platforms such as Apache Kafka
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python, and/or C++",None Found,None Found,None Found,None Found,"The Role
Lead Data Engineer and architect to design, implement a highly scalable system to ingest and process Petabytes of data per day.
Hands-on design and develop applications for data pipeline and data management.
Set processes and policies for data governance and data pipeline
Architect and implement best practices of big data tools such as Spark, Airflow, Kafka, Presto and Cassandra
Lead and mentor junior Data and BI engineers.
Set and define the standards and best practices in data team
Be the point of reference for solving challenging technical problem.
Architect and implement Machine Learning Pipelines for Data Science team.

Qualifications
Bachelor or Masters in Software Engineering and Computer Science
8+ years of experience in design and development of large scale data platforms
Expert in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Proficient in Spark development with PySpark or Scala
Expert in Data streaming platforms such as Apache Kafka
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python, and/or C++
Experience with running large-scale distributed computing infrastructure such as load balancing, Zookeeper, Micro service architecture
Experienced in security and access management
Experience with managing distributed databases like Elasticsearch, Cassandra
Experience with Columnar database such as Redshift, Vertica
Great verbal and written communication skills."
156,"Senior Data Engineer, Customer Experience","Los Gatos, CA 95032",Los Gatos,CA,95032,None Found,None Found,None Found,None Found,None Found,None Found,"Los Gatos, California
Data Science and Engineering
Netflix is revolutionizing entertainment and shaping the evolution of storytelling around the world. With over 150 million members in 190 countries, we are focused on delivering an incredible customer experience.

When someone encounters a problem watching their favorite Netflix show, the data products we build and teams we support get members back to streaming as soon as possible. We collect millions of data points via phone, chat, and social media to assess customer feedback in 25 different languages. This allows us to measure the effectiveness of different customer service strategies, detect issues to proactively assist members with personalized support, and test different paths for members to easily discover useful resources.

In this role, you will collaborate with a team of data engineers to build reliable, scalable data pipelines using Apache Spark/Flink. These pipelines will power analytic dashboards, custom viz applications with different storage engines (Snowflake, Druid, Elasticsearch), A/B experiments, feature generation for training production ML models, and NLP driven insights to better understand customer issues.
Who you are:
You have a strong background in distributed data processing (Batch or Streaming).
You have extensive data modeling skills. You design structures that are adaptable to changes in the source data or business processes.
You are a technical thought leader with a perspective on how to build great data products. You can adopt and help evolve our engineering best practices.
You are proficient in at least one major programming language and are passionate about writing clean, supportable code.
You are an advocate for data quality. You have a strong opinion on when data audits, unit tests, and documentation can be most effective.
You have strong SQL skills.
You have strong communication skills to effectively partner with data scientists and engineering stakeholders.
You are curious about the rapidly evolving technologies in this domain. You are eager to learn and master new tech when it can have a big impact on our team.
You can relate to many of the aspects of the Netflix culture and love to operate independently while collaborating and giving/receiving strong, candid feedback to your team members.
APPLY NOW
Share this listing:
LINK COPIED"
157,Siri - Performance and Reliability Team Data Engineer,"Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Sep 20, 2019
Weekly Hours: 40
Role Number: 200067345
Play a part in the next revolution in human-computer interaction. Contribute to a product that is redefining mobile and desktop computing. Create groundbreaking technology for large scale systems, spoken language, big data, and artificial intelligence. And work with the people who created the intelligent assistant that helps millions of people get things done — just by asking. The vision for the Siri Performance and Reliability team is to empower Siri organization to improve Siri quality by using data as the voice of our customers. Join us, and impact hundreds of millions of customers across a plethora of Apple of devices!
Key Qualifications
4+ years of experience in developing jobs in the MapReduce/Hadoop ecosystem, especially with Spark/Scala.
Expert knowledge of one or more object-oriented programming languages (Scala, Java, C++).
Ability to use several scripting languages (Python, Ruby, Bash, etc.).
Thorough understanding of the Hadoop ecosystem (HBase, HDFS, Hive, MapReduce), Spark, Solr, Kafka.
Experience with Batch and Streaming data processing
Experience with SQL and basic database knowledge for modifying queries and tables.
Working knowledge of the fundamentals of probability and basic statistics.
Strong interpersonal skills and experience working on multi-functional projects.
An obsession with quality.
Description
The Siri team is looking for a talented, broadly-skilled developer who is a creative problem-solver, thrives in a fast-paced environment, can work well across teams and organizations, and has a passion for quality. We thrive in automation, designing and implementing frameworks and other infrastructure for data analysis, creating tools with elegant and effective user interfaces.
If you're interested, you're probably a strong programmer with excellent problem-solving and interpersonal skills. You also have a passion to make the best products possible is the key for success in our group. You will create design patterns to ensure your metrics can be easily understood and reused in different contexts. You will also contribute to the design of our full system architecture. Create unit and functional tests to validate that your code continues to work in a fast-paced environment. You will provide technical expertise to other teams, advising best practices and highlighting risks.
As a Data Engineer on the Siri Performance and Reliability, you will have significant responsibility and influence in improving Siri by using data to measure user perceived latency, errors and failures along with user abandonments. You will develop large scale data processing and analytical solutions. You will collaborate with our quality initiative leaders to ensure the system is meeting the needs, and iterate as well as innovate based on observations and requirements gathering.
Our engineers collaborate with many internal sub-teams, such as engineering, design, QA, operations, and project management, and will be working in a heterogeneous environment. A successful candidate will have experience in large-volume data ingestion, processing, and analysis in near real-time. Design, implement, and manage scalable data models and pipelines used by all Siri teams. Build analytical solutions to enable data analysts to perform accurate and consistent analysis efficiently. Thus deep technical capabilities, strong communication skills and a knack to use hard data to triage issues is a must have requirement.
Education & Experience
MS in EE/CS/CE or equivalent experience
Additional Requirements
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
158,SQL Data Analyst,"Mountain View, CA 94039",Mountain View,CA,94039,None Found,None Found,None Found,None Found,None Found,None Found,"What are we looking for?
We are looking for a technically savvy database-oriented Analyst or Data-Engineer with good people skills and the ability to pick up new business concepts and technologies.

The ideal candidate will possess:
A strong to very strong working knowledge of SQL.
An ability to write and troubleshoot complex SQL procedures.

• The desire to understand business events through data.• An understanding of Data Warehousing and ETL techniques.
High level understanding in at least one scripting language such as Ruby, Shell, Python.
An interest in learning large data set processing with MapReduce/Hadoop/Pig/etc.
Linux skills are a plus.
Good client relations skills strongly preferred.

Who are we?
Raybeam, Inc. is a software engineering and consulting company focused on strategic consulting, business intelligence, and online/database marketing for the past twenty years. We have offices near Boston and San Francisco and support a strong list of clients including Google, Facebook, Microsoft, eBay, Disney, One Kings Lane, Beachbody and Hilton Worldwide.

What do we do?
We provide technology solutions by architecting and developing enterprise systems using a variety of programming languages, tools and platforms. This can range from building data warehouses, to web applications to implementing reporting platforms. We work in small teams, own the projects that we work on, and have direct input into the business decisions of our clients.

What do you get from working for Raybeam?
A fun, supportive work environment that promotes camaraderie and growth.
The chance to travel and network with important figures in the industry.
The chance to have input into business decisions of our clients.
The opportunity to learn technologies that you've always wanted but never had the chance.

If you are interested in applying for the position please click on the link below to take a short quiz.

http://careerseval.raybeam.com/sign_in

Please note that Raybeam, Inc. is currently unable to provide sponsorship or OPT extension work and we will only consider local candidates. Recent grads are encouraged to apply, and an MBA is desirable. Thank You."
159,Hadoop – Senior Data Engineer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,"Experience in Hadoop distributions (Apache / Cloudera / Hortonworks)
Ability to deploy and maintain multi-node Hadoop cluster
Experience working with Big Data eco-system including tools such as Hadoop, Map Reduce, Yarn, Hive, Pig, Impala, Spark , Kafka, Hive, Impala and Storm to name a few
Knowledgeable in techniques for designing Hadoop-based file layout optimized to meet business needs
Understands the tradeoffs between different approaches to Hadoop file design
Experience with techniques of performance optimization for both data loading and data retrieval
Experience with NoSQL Databases – HBase, Apache Cassandra, Vertica, or MongoDB
Able to translate business requirements into logical and physical file structure design
Ability to build and test rapidly Map Reduce code in a rapid, iterative manner
Ability to articulate reasons behind the design choices being made","Strong data analysis and SQL skills in Relational/Columnar/Big data environments (Joins, Union, rank, group by, order etc.)
Demonstrate excellent written and verbal communication skills
Experience with Agile implementation methodology and working in a globally distributed team structure
Able to work independently in a fast-paced environment
",None Found,None Found,"Job Details
Job Code
JPSC-7224
Posted Date
04/19/18
Experience
7 Years
Primary Skills
Hadoop,Hive,Pig,Impala,Map Reduce,KAFKA,Spark,Yarn,HIVE SQL
Required Documents
Resume
Overview
Role: Hadoop – Senior Data Engineer
Duration: 6 Months
Location: San Jose, CA

Only W2, No C2C at this moment

Important:
6 months right to hire will go into the work order
prepare your candidate for multiple interviews including a client interview
-local candidates only - rate is all inclusive - if your candidate is not local but willing to relocate, please make a note of this in the summary section of the resume.
-3 - 6 years of experience

Responsibilities and required skills:
Strong data analysis and SQL skills in Relational/Columnar/Big data environments (Joins, Union, rank, group by, order etc.)
Demonstrate excellent written and verbal communication skills
Experience with Agile implementation methodology and working in a globally distributed team structure
Able to work independently in a fast-paced environment
Hadoop Skills:
Key words for Primary – HIVE SQL , SPARK and Sqoop, Rest optional
Experience in Hadoop distributions (Apache / Cloudera / Hortonworks)
Ability to deploy and maintain multi-node Hadoop cluster
Experience working with Big Data eco-system including tools such as Hadoop, Map Reduce, Yarn, Hive, Pig, Impala, Spark , Kafka, Hive, Impala and Storm to name a few
Knowledgeable in techniques for designing Hadoop-based file layout optimized to meet business needs
Understands the tradeoffs between different approaches to Hadoop file design
Experience with techniques of performance optimization for both data loading and data retrieval
Experience with NoSQL Databases – HBase, Apache Cassandra, Vertica, or MongoDB
Able to translate business requirements into logical and physical file structure design
Ability to build and test rapidly Map Reduce code in a rapid, iterative manner
Ability to articulate reasons behind the design choices being made"
160,"Senior Data Engineer, Apple Media Products","Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Nov 1, 2018
Role Number: 200007033
Apple is seeking a highly skilled data engineer to join the Data Engineering team within Apple Media Products. AMP (home to Apple Music, App Store, iTunes and more) has some of the most compelling data in the world. We are looking for a talented engineer who is motivated by challenging problems and well versed with big data technologies. This is a unique opportunity to join a focused team and work collaboratively with other groups to make a significant impact.
Key Qualifications
Experience in high level programming languages such as Java, Scala, or Python.
Proficiency with databases and SQL is required.
Proficiency in data processing using technologies like Spark Streaming, Spark SQL, or Map/Reduce.
Expertise in Hadoop related technologies such as HDFS, Azkaban, Oozie, Impala, Hive, and Pig.
Expertise in developing big data pipelines using technologies like Kafka, Flume, or Storm.
Experience with large scale data warehousing, mining or analytic systems.
Ability to work with analysts to gather requirements and translate them into data engineering tasks
Aptitude to independently learn new technologies.
Description
As a member of the Data Engineering team, you will have significant responsibility and influence in shaping its future direction. This role is inherently cross-functional and the ideal candidate will work across disciplines. We are looking for someone with a love for data and ability to iterate quickly on all stages of data pipeline. This position involves working on a small team to develop large scale data pipelines and analytical solutions using BigData technologies. Successful candidates will have strong engineering skills and communication, as well as, a belief that data driven processes lead to great products. You will need to have a passion for quality and an ability to understand complex systems.
Education & Experience
Bachelor's degree or equivalent work experience in Engineering, Computer Science, Business Information Systems."
161,Software Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Auction.com is the nation’s leading online real estate marketplace focused exclusively on the sale of residential bank-owned and foreclosure properties via online auctions and live trustee sale events. By offering access to exclusive properties and technology designed to seamlessly connect buyers and sellers, Auction.com empowers residential real estate investors and financial institutions to achieve optimal, mutually beneficial results – to go beyond the bid.

Senior Data Engineer

Position Summary
At Auction.com, we are embarking on a journey to transform the real estate market with technological innovations. A critical prerequisite for this transformation is a robust data infrastructure. As a senior data engineer, you will help us design and implement our big data environment that is real-time, stable and scalable. You will work with a talented data engineering team to improve our data processing pipeline and developing new capabilities to support mission-critical initiatives. You will mentor junior engineers and help evaluating new technology along the way. You impact will be felt across the team as well as the entire Auction.com organization.

Responsibilities/Duties

Make major contribution to the implementation of our real time big data initiative
Build and automate productized data processing pipelines in AWS big data platform
Help improve our development process and standards through mentoring and leading-by-example
Help define and implement data ingestion contracts
Create data environment to support our data analytics, reporting and data science teams

Knowledge, Skills and Abilities

In-depth understanding of modern big data technology, including Hadoop and Spark
Knowledge of real time data streaming and aggregation architectural patterns and practice
Proficient in programming languages such as Python, Scala and Java
Familiarity with NoSQL as well as SQL databases
Data modeling and machine learning skill is a plus

Education/Experience

Bachelor's Degree in computer science, data science or related fields
Familiarity with agile developmental process
Previous experience developing data product required
Hands-on experience with real time data streaming, aggregation and presentation strongly preferred
Previous experience with production ETL pipeline development required
At least a years’ experience with AWS cloud or another cloud platform

To all recruitment agencies: Auction.com does not accept agency resumes unless you are part of our preferred partner network. Please do not forward resumes to our jobs alias, Auction.com employees or any other company location. Auction.com is not responsible for any fees related to unsolicited resumes."
162,Data Engineer - Ad Platforms,"Santa Clara Valley, CA 95014",Santa Clara Valley,CA,95014,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: May 30, 2019
Weekly Hours: 40
Role Number: 200063498
At Apple, we work every day to create products that enrich people’s lives. Our Advertising Platforms group makes it possible for people around the world to easily access informative and imaginative content on their devices while helping publishers and developers promote and monetize their work. Today, our technology and services power advertising in Search Ads in the App Store and Apple News. Our platforms are highly-performant, deployed at scale, and setting new standards for enabling effective advertising while protecting user privacy.
We are looking for an ambitious and versatile engineer who will be a key member of our team, delivering data solutions to important business problems. Responsibilities range from core storage and processing capabilities, to mission-critical pipelines, to supporting online serving architectures, and leading edge, privacy preserving machine learning platforms. You will have the opportunity to define, refine, and/or refactor approaches, designs, and architectures to meet the data engineering challenges we must solve. You will join a team of world-class data engineers hungry to apply leading-edge technologies to deliver extraordinary experiences to our customers. You will play a meaningful role building data products that deliver on Apple's privacy commitments and change the way advertising works with data. You will collaborate closely with the business to deliver relevant data and insight to inform our strategy and decisions.
You should have experience in data engineering roles, ideally within the ads or media space. You will have an excellent understanding of scalable approaches and thrive working in Agile environments. The ability to be a good team player under tight deadline constraints is key to success.
Key Qualifications
You are a clear and effective communicator, and enjoy collaborative problem solving
You love working on a shared codebase that supports web-scale, mission critical applications; and the discipline that it requires
You understand modern data engineering approaches, stay on top of developments, and are aware of what leading players are doing
You have a demonstrated ability to implement and extend highly performant, resilient, reliable, and understandable data pipelines
You have experience with Spark, Hadoop, Kafka or other distributed systems
You have deep expertise in Python, Java, Scala, SQL, and/or other relevant languages and frameworks
You have experience with Oracle, Postgres, mySQL, or other relational databases
You have worked in cloud environments and are familiar with object stores, and other common cloud-native data storage and processing frameworks
You have worked in CI/CD environments
You have experience with pipelines and architectures that support machine learning development platforms and production applications
You are familiar with A/B and other online testing applications
You understand statistics and are capable of using data analysis techniques to understand data quality, profile system loads, understand the relationships between business metrics, and similar
Description
At Ad Platforms, we are constantly developing data products to provide amazing user experiences and drive value for publishers and developers. We are looking for a data engineer to deliver our future data systems, by working to:
- Design and implement new pipelines, storage, and processing solutions using modern, Distributed Systems approaches and technologies
- Advocate for the design and implementation approaches you propose
- Follow best practices in storage, processing, copy/synchronization, etc. appropriate to the scale and maturity of our products
- Work in cross-functional teams to prototype new concepts and deliver end-to-end systems in an agile setting
- Produce high quality systems with excellent reliability and scalability
- Work closely with partners across the organization to create and build data-driven products
Education & Experience
Bachelors degree in Computer Science, Distributed Systems, Software Engineering, or related field and experience designing, building, maintaining, and extending web-scale production data systems."
163,Big Data Engineer - Masters (Co-Op) – United States,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"What You’ll Do
Design and deliver automated transformation of large data sets leveraging MapReduce, streaming, and other emerging technologiesLeverage HBase, Elasticsearch, etc. to ingest transformed data at scaleCollaborate with security experts to deliver high-impact web-based APIs
 Implement high-volume data integration solutions Analyze, monitor, and optimize for performance
Produce and maintain high-quality technical documentation
Who You'll Work With
Join us as we transform the world of tomorrow. Develop creative ideas on how to work better and smarter. Influence and participate in top-priority projects that have a real impact.
Who You Are
Currently enrolled in an accredited university co-op program pursuing a Master’s degree in Computer Science, Computer Engineering, Electrical Engineering, or a related major such as Math, PhysicsMinimum of a 3.0 GPA or equivalentTrack record of developing technology to enable large scale data transformationStrong Java experience and hands-on Hadoop ecosystem experience – HBase, Hive, Spark, etc.Possess knowledge of software engineering best practicesPassion for solving hard problems and exploring new technologiesExcellent communication and technical documentation skills
Why Cisco
#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.
We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!
But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)
Day to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.
So, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!

This position is available to Master’s level Students. Positions are located East Coast, West Coast and Central US. Not all positions offer sponsorship or are available at all locations. Relocation is available for some locations and or positions."
164,Principal Data Engineer,"Mountain View, CA 94043",Mountain View,CA,94043,None Found,None Found,None Found,"
Proactively drive the execution of our data engineering, fraud detection roadmap
Drive design and implementation of durable & efficient software solutions that handles massive amount of structured and unstructured data
Develop and scale data infrastructure that powers batch and real-time data processing
Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs
Research, analyze and select technical approaches for solving difficult and challenging development and integration problems
Coach and mentor other engineers in process and methodologies
",None Found,"
Computer Science or engineering degree
8+ years software development experience
3+ years working experience with big data ecosystem(Spark, Hive, HBase, etc.), solid experience with ETL or Data Warehouse
Prior experience working with structured and unstructured dataand Cloud technologies like AWS, Docker, Kubernetes
Solid experience with handling large data volumes in a distributed environment and web data management
Good at one or more of program languages (JAVA, Scala, Python, etc.)
Excellent communication and presentation skills
Ability to think outside-the-box and challenge conventional wisdoms
","Coupang is one of the largest and fastest growing e-commerce platforms on the planet. Our mission is to create a world in which Customers ask ""How did I ever live without Coupang?"" We are looking for passionate builders to help us get there. Powered by world-class technology and operations, we have set out to transform the end-to-end Customer experience -- from revolutionizing last-mile delivery to rethinking how Customers search and discover on a truly mobile-first platform. We have been named one of the ""50 Smartest Companies in the World"" by MIT Technology Review and ""30 Global Game Changers"" by Forbes.

Coupang is a global company with offices in Beijing, Los Angeles, Seattle, Seoul, Shanghai, and Silicon Valley.

Job Overview:
The eCommerce Fraud Detection team is dedicated in making Coupang's eCommerce business fraud free. We are seeking a talented, enthusiastic and technology-proficient Big Data engineer, who is eager to participate in design and implementation of a large-scale, highly efficient data platform, batch and real-time pipelines and tools. You will work closely with a team of data scientists, business analysts and engineers to ensure we detect fraud in e-commerce business and take actions with them to protect our business.

Key Responsibilities:

Proactively drive the execution of our data engineering, fraud detection roadmap
Drive design and implementation of durable & efficient software solutions that handles massive amount of structured and unstructured data
Develop and scale data infrastructure that powers batch and real-time data processing
Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs
Research, analyze and select technical approaches for solving difficult and challenging development and integration problems
Coach and mentor other engineers in process and methodologies

Requirements:

Computer Science or engineering degree
8+ years software development experience
3+ years working experience with big data ecosystem(Spark, Hive, HBase, etc.), solid experience with ETL or Data Warehouse
Prior experience working with structured and unstructured dataand Cloud technologies like AWS, Docker, Kubernetes
Solid experience with handling large data volumes in a distributed environment and web data management
Good at one or more of program languages (JAVA, Scala, Python, etc.)
Excellent communication and presentation skills
Ability to think outside-the-box and challenge conventional wisdoms

Preferred:

Experience in eCommerce or payment related field
Experience in development of Petabyte-Scale NoSQL database and Messaging Platform

Perks:

Autonomy to make decisions in a rapidly growing company
15 days PTO + 15 national holidays off
401K matching
Pre-IPO stock options
Mobile & fitness reimbursement
Catered Lunch

Coupang is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex or gender (including pregnancy, gender identity, gender expression, sexual orientation, transgender status), national origin, age, disability, medical condition, HIV/AIDS or Hepatitis C status, marital status, military or veteran status, use of a trained dog guide or service animal, political activities, affiliations, citizenship, or any other characteristic or class protected by the laws or regulations in the locations where we operate.

If you need assistance and/or a reasonable accommodation in the application or recruiting process due to a disability, please contact us at usrecruiting@coupang.com ( usrecruiting@coupang.com )."
165,"Data Engineer, Partnership Analytics","Menlo Park, CA",Menlo Park,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
Facebook is looking for exceptionally talented and experienced data engineers to join the Partnerships Analytics team. The Partnerships team at Facebook works with leading content creators, publishers, and businesses in entertainment, sports, news, and many other verticals. This role is a unique opportunity to work with one of the most important datasets in the world to create analytics tools and systems that enable field organizations, analysts and clients alike. You will work with some of the brightest minds in the industry, and get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match. The ideal candidate will have a passion for problem solving and a belief in the power of incremental change in a fast-paced environment. The role will report to the Data Infrastructure Lead and is based in Menlo Park.
RESPONSIBILITIES
Develop methods to unlock access to data for stakeholders across the Partnerships Organization
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts)
Optimize and maintain existing pipelines, ensuring that data arrives accurately and on-time
Build and ensure data tables can serve as the source of truth for various high-level priorities
Create scripts to automate operational processes
Work cross-functionally to define problem statements, collect data, and make recommendations
Build data expertise and own data quality for allocated areas of ownership
Define and manage SLA for all data sets in allocated areas of ownership
Work with data infrastructure to triage infra issues and drive to resolution
Support on-call shift as needed to support the team
MINIMUM QUALIFICATIONS
BS/BTech in Computer Science, Math or related field
4+ years experience in the data warehouse space
4+ years experience in custom ETL/data pipeline design, implementation and maintenance
4+ years of SQL (Oracle, Vertica, Hive, etc.) or relational database experience (Oracle, MySQL) writing queries
Experience with programming languages, Python
Experience with data architecture, data modeling, schema design and software development
Experience with large data sets, Hadoop, and data visualization tools
Experience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders
PREFERRED QUALIFICATIONS
Experience with packages such as R, Tableau, SPSS, SAS, STATA, etc.
Familiar with version control systems (git, mercurial, etc.)
Facebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com."
166,Cloud Data Engineer - GCP,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Responsibilities :
Strong Cloud experience and at least 6 months of GCP hands-on project experience in
Cloud Migration (IAAS)BigData/BigQuery/DataProc/CloudStorage/Object Storage etcKnowledge of Containers & Microservices would be a great plusStrong GCP cloud & data migration experience
- Google Cloud Certified - Professional Data Engineer – Certificate is a MUST
Big Data cloud migration to GCP experience is a GREAT PLUSStrong client communication skills & Interpersonal skillsShould have experience with the distributed team

Position :Cloud Data Engineer - GCP

Location :San Jose

Last Date To Apply :September 30, 2019"
167,Big Data Engineer- Contractor,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,"
Responsible for design, implementation, and ongoing support of the Big Data platforms (Hadoop, HBase, HIVE, Spark), ensure high availability and reliability
Migrating large data sets between data centers and the cloud (such as AWS SQL server and Hadoop)
Design, test and implement cloud BI / DW infrastructure
Understand and support AWS native big data / analytics system
Use Streaming, Spark & Big Data technologies to enrich and transform data for real time ingestion and build low latency feeds.
",None Found,"
Thorough understanding of Hadoop ecosystem (HDFS, YARN, Hive, Pig, MapReduce, Spark, Spark2, Sqoop, Solr, kafka, oozie)
Strong experience in setting up, configurating, upgrading and managing security for Hadoop clusters, setting up Ranger policies for HDFS and Hive
Experience in managing Hadoop cluster with Ambari and developing custom tools/scripts to monitor the Hadoop Cluster health
Strong knowledge and hands on experience related to mission critical backup and recovery
Strong experience with load balancing and high volume, high availability environments
Able to automate administrative tasks using scripting languages (Python, Shell, Ansible)
Strong working experiences of implementing Big Data processing using MapReduce algorithms and Hadoop/Spark APIs
Experience building workflow to perform predictive analysis, muilti-dimensional analysis, data enrichments etc
Understanding of software development methodologies and coding standards.
A burning desire to master new technologies and apply them to real world challenges
","Auction.com is the nation’s leading online real estate marketplace focused exclusively on the sale of residential bank-owned and foreclosure properties via online auctions and live trustee sale events. By offering access to exclusive properties and technology designed to seamlessly connect buyers and sellers, Auction.com empowers residential real estate investors and financial institutions to achieve optimal, mutually beneficial results – to go beyond the bid.

Big Data Engineer- short term project.

Are you looking to work on a challenging short term project with a Leading Online Real Estate Organization that has an amazing company culture. If this sounds like a good fit for you, please read on to view the information about this exciting/challenging project.

What you will be working on;

Responsibilities:

Responsible for design, implementation, and ongoing support of the Big Data platforms (Hadoop, HBase, HIVE, Spark), ensure high availability and reliability
Migrating large data sets between data centers and the cloud (such as AWS SQL server and Hadoop)
Design, test and implement cloud BI / DW infrastructure
Understand and support AWS native big data / analytics system
Use Streaming, Spark & Big Data technologies to enrich and transform data for real time ingestion and build low latency feeds.

Requirements:

Thorough understanding of Hadoop ecosystem (HDFS, YARN, Hive, Pig, MapReduce, Spark, Spark2, Sqoop, Solr, kafka, oozie)
Strong experience in setting up, configurating, upgrading and managing security for Hadoop clusters, setting up Ranger policies for HDFS and Hive
Experience in managing Hadoop cluster with Ambari and developing custom tools/scripts to monitor the Hadoop Cluster health
Strong knowledge and hands on experience related to mission critical backup and recovery
Strong experience with load balancing and high volume, high availability environments
Able to automate administrative tasks using scripting languages (Python, Shell, Ansible)
Strong working experiences of implementing Big Data processing using MapReduce algorithms and Hadoop/Spark APIs
Experience building workflow to perform predictive analysis, muilti-dimensional analysis, data enrichments etc
Understanding of software development methodologies and coding standards.
A burning desire to master new technologies and apply them to real world challenges

Nice to have:

Relational design, understand business requirements and perform data design reviews
Data migration ETL concepts, open source ETL tools
Working experience at a web or internet start-up experience
Knowledge of the real estate industry

To all recruitment agencies: Auction.com does not accept agency resumes unless you are part of our preferred partner network. Please do not forward resumes to our jobs alias, Auction.com employees or any other company location. Auction.com is not responsible for any fees related to unsolicited resumes."
168,Senior Application Support Analyst (Temp. 6 months),"Sunnyvale, CA",Sunnyvale,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Named as one of Fortunes’ 100 Fastest Growing Companies for 2019, EPAM is committed to providing our global team of 30,100+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential.

Description

You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Application Support Analyst. Scroll down to learn more about the position’s responsibilities and requirements.

EPAM Systems is seeking a Cloud Platform/Big Data Support Specialist to provide enterprise-level support to customers of a major cloud service provider. As a cloud support specialist, you will work in a team of experienced support engineers to resolve customer's concerns and issues for using cloud platform and big data products.

You would use your technical expertise and communication skills to understand customer's problem, provide technical assistance, then guide them to resolution. You would also participate in discussion with product engineers to share your insights on customer needs and issues to help product improvements. You will be fully exposed to the cutting edge technologies of a prominent cloud service provider, and play a key role in the growth of their cloud computing products.

#LI-DNI
What You’ll Do
Provide technical assistance and support over e-mail, chat and phone as part of a global 24x7-support organization
Provide initial response to customer's inquiry, troubleshoot, provide updates, identify root case, and resolve the issue to the satisfaction of customer
Handle escalation from customer and lead to satisfactory resolution
Co-work with engineers across technical and product domains to resolve complex cross-domain issues
Consult with senior engineers and subject matter experts (SME) to accelerate problem resolution
Hand-off or take-over cases to/from other geographical region to provide around-the-clock issue resolution for premium customers
Follow communication guidelines and security policies when communicating with customer
Categorize support requests for support and service analytics
Produce support documents, perform knowledge sharing and training
Keep technical skills up to date with latest cloud technologies
What You Have
A degree in an associated field and/or other advanced certification along with significant experience
Strong analytical / troubleshooting / problem solving skills
Strong verbal and written communication skills
Excellent customer service skills
Ability to perform job functions under stress and pressure
Commitment to continuous self-learning
Regular, reliable attendance
3+ years of experience as developer or a combination developer + big data engineer
Proficient in at least one of the following development languages: Java, Python, .NET, Ruby, PHP, Go or Javascript (NodeJS)
Hands on experience with RESTful APIs
Experience with relational databases (e.g. MySQL, PostgreSQL, etc.)
Experience with Big Data architectures and technologies and BI solutions
Experience in CI/CD, DevOps and related automation tools (e.g. Jenkins, Chef, Puppet, etc.)
Ability to read and understand code and able to write code samples to reproduce customer issues
Ability to read and understand logs and stack traces to troubleshoot issues
Good oral and written business communication skills in English (CEF Level C1 or above)
Must be able to work on the following shifts:
Early week shift from 7:00 AM to 6:00 PM, Sunday to Wednesday
Late week shift from 7:00 AM to 6:00 PM, Wednesday to Saturday
Yes, you will work 4 days and take 3 days off
May need to work on public holidays. If worked on a public holiday, you will be provided with a day-off in lieu
Nice to have
BA/BS degree preferred
2+ years of customer support experience preferably in Enterprise software support
Experience with PaaS and IaaS technologies
Experience with distributed computing frameworks (e.g. Hadoop, Spark, Flink, Storm, Samza, Beam, Airflow, Google Big Query, etc.)
Experience with distributed data stores (HBase, Cassandra, Riak, Google Bigtable, Amazon Dynamo DB, etc.) and/or distributed message brokers (Kafka, RabbitMQ, ActiveMQ, Google Pub/Sub, Amazon Kinesis, etc.)
Experience with ETL processes and tools (e.g. AWS Glue, Google Dataprep and/or Datafusion, MS SSIS, ODI, IPC, etc.)
Experience with any ML library (scikit-learn, XGBoost, pytorch, tensorflow, Spark mllib) or basic understanding of ML concepts
What We Offer
Medical, Dental and Vision Insurance (Subsidized)
Health Savings Account
Flexible Spending Accounts (Healthcare, Dependent Care, Commuter)
Short-Term and Long-Term Disability (Company Provided)
Life and AD&D Insurance (Company Provided)
Employee Assistance Program
Unlimited access to LinkedIn learning solutions
Matched 401(k) Retirement Savings Plan
Paid Time Off
Legal Plan and Identity Theft Protection
Accident Insurance
Employee Discounts
Pet Insurance
EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring"
169,Senior Data Engineer,"Mountain View, CA",Mountain View,CA,None Found,None Found,None Found,"
Big data tools (Spark, Hadoop)
Relational and NoSQL Databases (Postgres and Mysql)
AWS Cloud Services (EMR, RDS, Redshift, Kinesis, SQS, S3, Glue etc.)
Workflow management tools (Airflow, AWS Data pipeline)
Language Skills (Python o R)
Ability to do analysis by SparkSQL, Athena, Redshift in Zeppelin or Jupyter notebook or Tableau
","
Independently design, maintain, and enhance big data pipeline, including ETL and data analytics.
Build and scale systems that orchestrate and execute complex workflows in big-data pipelines
Support team members to proactively anticipate and resolve issues
Work with marketing and dev teams to build data-driven business
",None Found,"
A Bachelors or Masters in Computer Science, MIS, Statistics, Economics, Mathematics, etc
5+ years of experience in a data engineering role
Must be a problem solver
Must thrive in a fast-paced working environment
Excellent communicator
Self motivated
","Omlet Arcade is the community for mobile game streamers and e-sports. Every month we bring millions of gamers together to show off their gameplay and make new friends. We hatched up Omlet Arcade to serve the next generation of gamers; those who no longer need PCs or consoles due to the overwhelming power of the phones at their fingertips. This year has shown we're on the right path as AAA games like Fortnite and PUBG light up our mobile screens. In 2019, Omlet is taking it to the next level, bringing mobile e-sports to the world by empowering players with the tools they need to strut their skills, build amazing teams, and have a blast competing with each other.

We are looking for a Senior Data Engineer to join our Engineering ream, this person will be responsible for turning data into information, information into insights and insights into key business decisions, that steer the strategic direction of the company.

Responsibilities:

Independently design, maintain, and enhance big data pipeline, including ETL and data analytics.
Build and scale systems that orchestrate and execute complex workflows in big-data pipelines
Support team members to proactively anticipate and resolve issues
Work with marketing and dev teams to build data-driven business

Requirement:

A Bachelors or Masters in Computer Science, MIS, Statistics, Economics, Mathematics, etc
5+ years of experience in a data engineering role
Must be a problem solver
Must thrive in a fast-paced working environment
Excellent communicator
Self motivated

Skills:

Big data tools (Spark, Hadoop)
Relational and NoSQL Databases (Postgres and Mysql)
AWS Cloud Services (EMR, RDS, Redshift, Kinesis, SQS, S3, Glue etc.)
Workflow management tools (Airflow, AWS Data pipeline)
Language Skills (Python o R)
Ability to do analysis by SparkSQL, Athena, Redshift in Zeppelin or Jupyter notebook or Tableau

Perks & Benefits:

Competitive Salary
Medical, Dental, Vision
Unlimited PTO
Free catered lunch through Door Dash 5 days per week
Friendly working environment

"
170,Data Engineer,"San Jose, CA",San Jose,CA,None Found,None Found,Querying and manipulating large data sets for analytical purposes using SQL-like languages (Hive is strongly preferred),None Found,None Found,None Found,None Found,"Seeking Data Engineer


Design, build, and manage complex analytics data models in Hive/Hadoop for Analytics team across all customer journey from Acquisition, Engagement, and Retention. The analytics data marts will be used by data analysts in Analytics and other team to do deep dive analysis, build analytics dashboard, or other data science project. Design, build, deploy, and maintain new data models ETL pipeline with SQL query, Python, Oozie, and other script language. Create/maintain workflow and ensure overall data quality.


Qualifications:
Querying and manipulating large data sets for analytical purposes using SQL-like languages (Hive is strongly preferred)
Experience with Hadoop/big data environments to synthesize and analyze data.
Professional experience in the data warehouse space
Good attention to detail and ability to QA multiple data sources
Experience working on building scalable ETL pipelines, data warehousing and schema modeling
Experience working with Oozie Workflow or others
Experience with script language such as Python
We are a software company focused on emgergin tecnologies - delivering data, analystics, AI and RPA solutions that drive growth & opportunities.


Learn more at www.dataflix.com"
171,"Staff Software Engineer, Data Team (Big Fast Data)","Sunnyvale, CA 94087",Sunnyvale,CA,94087,None Found,"Collaborating closely with application team architects and engineers to identify technologies and platforms suitable for their big data processing requirements, and then assisting those teams with onboarding, development, deployment, and debugging on those platforms",None Found,None Found,None Found,None Found,"Position Description

Develops Innovation strategies, processes, and best practices
Drives the execution of multiple business plans and projects
Ensures business needs are being met
Leads and participates in medium- to large-scale, complex, cross-functional projects
Leads the discovery phase of medium to large projects to come up with high level design
Leads the work of other small groups of six to ten engineers, including offshore associates, for assigned Engineering projects
Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity
Provides supervision and development opportunities for associates
Supports business objectives
Troubleshoots business and production issues
Utilizes industry research to improve Wal-Mart's technology environment

Minimum Qualifications
Bachelor's Degree in Computer Science or related field and 6 years experience building scalable ecommerce applications or mobile software
Additional Preferred Qualifications
Please add text
Company Summary
The Walmart eCommerce team is rapidly innovating to evolve and define the future state of shopping. As the world’s largest retailer, we are on a mission to help people save money and live better. With the help of some of the brightest minds in technology, merchandising, marketing, supply chain, talent and more, we are reimagining the intersection of digital and physical shopping to help achieve that mission.
Position Summary
The Walmart Labs Big Data Platforms team is seeking a big data engineer to serve in a consulting engineering and support role for the big data components of Walmart’s major application and project initiatives. The Big Data Platforms team operates multi-tenant persistent Hadoop-based platforms based on several Hadoop distributions, and also leads architecture and integration engineering for Walmart’s cloud-based big data platforms using both on-premise and commercially available cloud resource providers.

We’re looking for a software engineer that has extensive experience building and supporting big data applications using the Hadoop ecosystem and related technologies, both on traditional clusters and cloud platforms, to collaborate with Walmart’s internal product development teams to help them construct scalable and performant big data applications, and also help application teams troubleshoot big data problems when things go wrong.

Responsibilities include:

Collaborating closely with application team architects and engineers to identify technologies and platforms suitable for their big data processing requirements, and then assisting those teams with onboarding, development, deployment, and debugging on those platforms
Providing technical engineering and performance tuning assistance to a broad community of big data infrastructure users, such as software application engineers and data scientists, through research, investigation, collaboration, and hands-on debugging when necessary
Investigating new big data tools and technologies for their potential application to common use cases; establishing best practices, developing design patterns, and writing documentation to disseminate new capabilities to a broad technical audience; working with platform engineers and product managers to specify and deliver new major technology features
Ensuring that application big data solutions adhere to best practices and enterprise standards for scalability, availability, efficiency, data lifecycle management, information security, fault tolerance, and disaster recovery"
172,Big Data Engineer,"Santa Clara, CA",Santa Clara,CA,None Found,None Found,None Found,None Found,"
Join an agile SaaS team to design, develop and maintain features and iteratively deploy services using Infoblox’s cloud-based architecture
Design and implement components of our Next Generation Platform
Recommend ways to improve data reliability, efficiency and quality
Expand and grow data platform capabilities to solve new data problems and challenges
Build large-scale data processing systems using cloud computing technologies
Build high-performance algorithms, prototypes, and proof of concepts
Apply complex big data concepts with a focus on collecting, parsing, managing, and analyzing large sets of data to turn information into insights
Work closely with various cross functional product teams
Stay current on key trends especially in the area of technologies and frameworks like: Mesos/Marathon, Kubernetes, Docker etc.","
Bachelor’s degree in CS, CE or EE is required
Masters in CS, CE, or EE is preferred","
8+ years experience, 2+ in Big Data Engineering
Proficient in Java, Scala, Golang, or Python
Good understanding of Microservices architecture
Expertise in BigData - MapReduce, HIVE, HBase, Spark streaming, Apache Flink, Storm, Kafka, In memory Database, JMS
Experience with NoSQL databases such as Cassandra/DynamoDB
Good exposure in application performance tuning, memory management, scalability
Ability to design highly scalable distributed systems, using different open source technologies
Experience building high-performance algorithms","Infoblox is the global leader in providing actionable network intelligence through network services, security and threat intelligence. We give companies total control and visibility of their network, allowing them to operate more efficiently and intelligently.
We are looking for a Staff Software Engineer to join our SaaS Next Generation Platform Team in Santa Clara, CA. In this role, you will be responsible for developing, maintaining, evaluating and testing big data technologies. Our organization is extremely data driven where technical innovations happen and you will have an opportunity to use cutting edge technology across all stages of development lifecycle and be part of our exciting and innovative initiatives.
Responsibilities:

Join an agile SaaS team to design, develop and maintain features and iteratively deploy services using Infoblox’s cloud-based architecture
Design and implement components of our Next Generation Platform
Recommend ways to improve data reliability, efficiency and quality
Expand and grow data platform capabilities to solve new data problems and challenges
Build large-scale data processing systems using cloud computing technologies
Build high-performance algorithms, prototypes, and proof of concepts
Apply complex big data concepts with a focus on collecting, parsing, managing, and analyzing large sets of data to turn information into insights
Work closely with various cross functional product teams
Stay current on key trends especially in the area of technologies and frameworks like: Mesos/Marathon, Kubernetes, Docker etc.
Requirements:
8+ years experience, 2+ in Big Data Engineering
Proficient in Java, Scala, Golang, or Python
Good understanding of Microservices architecture
Expertise in BigData - MapReduce, HIVE, HBase, Spark streaming, Apache Flink, Storm, Kafka, In memory Database, JMS
Experience with NoSQL databases such as Cassandra/DynamoDB
Good exposure in application performance tuning, memory management, scalability
Ability to design highly scalable distributed systems, using different open source technologies
Experience building high-performance algorithms
Education
Bachelor’s degree in CS, CE or EE is required
Masters in CS, CE, or EE is preferred
It’s an exciting time to be at Infoblox. We are the market leader in technology for network control. Our success depends on bright, energetic, talented people who share a passion for excellence in building the next generation of networking technologies—and having fun along the way. Infoblox offers a fast-paced, action-oriented environment. We promote a culture that embraces innovation, change, teamwork, and strong partnerships. Join the winning Infoblox team—our future looks bright, and so will yours. To check out what it’s like to be a Bloxer click here."
173,Senior Data/Server Engineer,"Sunnyvale, CA",Sunnyvale,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Niantic, the developer behind popular games like Pokemon GO and Harry Potter: Wizards Unite is searching for a Senior Data Engineer with an extensive server infrastructure background. Join a group of experienced engineers to help build and scale Niantic's core data infrastructure. This is a nimble, motivated team responsible for building trust, owning data integrity, and supporting data-driven decision-making at Niantic.
Responsibilities
Architect the data stack responsible for storing and processing enormous volumes of analytics data.
Design efficient, extensible data models for use with Niantic's data pipelines and analytics systems.
Improve and extend core data competency for the User Acquisition pipeline to facilitate tools and reporting in support of growing UA efforts.
Organize and secure data drawn from diverse sources and build streamlined ETL pipelines to transform and validate it.
Mentor and offer technical guidance to data engineers, data scientists, and infrastructure engineers.
Work with the Product Team and Management to define a shared vision, an execution strategy, and communicate timeline and trade-offs.
Qualifications
4+ years of experience developing and deploying robust, large-scale data pipelines.
A high degree of attention to detail and clear aptitude for finding and resolving data integrity issues.
Proven success in securing and auditing data stores and implementing legal compliance, e.g. GDPR.
Deep knowledge of available data storage technologies such as Hadoop, Cassandra, Druid, and their trade-offs.
Excel in developing general purpose solutions to difficult problems and building elegant solutions.
Strong communicator to both technical and non-technical people and demonstrated ability to document technical design decisions.
Expert in Java or Scala, Python and SQL.
BS, MS, or PhD in Computer Science or a related technical field.
Plus If...
Familiarity with mobile advertising, user acquisition and associated data processing and metrics (e.g. attribution, retention, CPI, ROAS).
Detailed knowledge of and experience with the large advertising networks, e.g. Google, Facebook, Twitter, Apple.
Knowledge of the Google data stack (e.g. Dataflow, BigQuery, BigTable).
Proficient in the use of Airflow, Composer.
Join the Niantic team!
Niantic is the world’s leading AR technology company, sparking creative and engaging journeys in the real world. Our products inspire outdoor exploration, exercise, and meaningful social interaction.
Originally formed at Google in 2011, we became an independent company in 2015 with a strong group of investors including Nintendo, The Pokémon Company, and Alsop Louie Partners. Our current titles include pioneering global-control game Ingress, record-breaking AR game Pokémon GO, and recently released third title, Harry Potter: Wizards Unite.
Niantic is an Equal Opportunity and Affirmative Action employer. We believe that cultivating a workplace where our people are supported and included is essential to creating great products our community will love. Our mission emphasizes seeking and hiring diverse voices, including those who are traditionally underrepresented in the technology industry, and we consider this to be one of the most important values we hold close.
We're a hard-working, fun, and exciting group who value intellectual curiosity and a passion for problem-solving! We have growing offices located in San Francisco, Sunnyvale, Bellevue, Los Angeles, London, Tokyo, Hamburg, and Zurich."
174,Data Engineer,"Santa Clara, CA 95054",Santa Clara,CA,95054,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description
Job Title: Data Engineer
Location: San Francisco, CA, Austin, TX, San Jose, CA
Terms: Full-time
About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.
What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.
As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do
Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:
Cloud
Analytics
Digitization
Infrastructure
Security
Job Description
Overview
Data is the way our clients make decisions. It is the core to their business, helping create an experience for customers and providing insights into the effectiveness of our product launch & features.

As a Data Engineer , you will be a part of an early stage team that builds the data pipelines, collection, and storage, and exposes services that make data a first-class citizen. We are looking for a Data Engineer to build a scalable data platform. You'll have ownership of core data pipelines that powers top line metrics; You will also use data expertise to help evolve data models in several components of the data stack; You will help architect, building, and launching scalable data pipelines to support growing data processing and analytics needs. Your efforts will allow access to business and user behavior insights, using huge amounts of data to fuel several teams such as Analytics, Data Science, Marketplace and many others.

Responsibilities

Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth
Evolve data model and data schema based on business and engineering needs
Implement systems tracking data quality and consistency
Develop tools supporting self-service data pipeline management (ETL)
SQL and MapReduce job tuning to improve data processing performance

Experience

3+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct advanced performance tuning
Strong skills in scripting language (Python, Ruby, Bash)
1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)
Comfortable working directly with data analytics to bridge Lyft's business goals with data engineering

We are Growing Rapidly: 2019 Highlights
Trianz is growing above the average of the professional services industry. Here are some highlights.
Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.
Won the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.
Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.
Featured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.
Achieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.
Talk to us, Join us & Develop into Leaders
Come join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.
We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
Equal Opportunity Employer
Trianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law)."
175,Senior Scala Data Engineer,"San Jose, CA",San Jose,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Scala Data Engineer – HPE InfoSight

HPE is seeking an outstanding software engineer to play a key role in helping the HPE InfoSight team build AI for the datacenter. HPE InfoSight allow HPE partners and customers to optimize, manage, and protect their datacenter infrastructure, while helping customer support, engineering, and sales to deliver more value to our customers.

You will be joining a small start up within HPE, agile, empowered team, focused on analyzing call-home data sent from HPE storage and enterprise products to provide business value through analytics. The team leverages a modern big-data and microservice-based technology stack for our end-to-end data processing, analysis, API, and web application – to provide our users with the insights they need to be successful.

Responsibilities

Technical contributor as a full-stack developer in a small, cross-functional development team, focused on providing data analytics as a service to internal and external HP customers.Contribute to the continuous improvement of our IoT analytics platform, powered by Scala, Spark, Mesos, Akka, Cassandra, Kafka, Elasticsearch, and Vertica.Develop unit, integration, system or any tests that are needed to help the team deliver value quickly, with high quality, to our customers.Leverage big-data technologies for data analytics, including Hadoop/Spark, Vertica (SQL), and Elasticsearch.Develop automation for continuous delivery, testing, and monitoring of our application and infrastructure, using Mesosphere DCOS, Jenkins, Ansible, Kibana, and others.
Education and Experience

Bachelor/Master's in Computer Science/Engineering, or equivalent, and a minimum of 5-7 years’ experience.

Knowledge and Skills
Team player with a passion for learning, programming, automation, and data analytics.Excellent programming skills, with experience or an interest in learning functional programming.Excellent analytical and problem solving skills.Excellent communications skills.
We are looking for a candidate with some or all of the following:

Experience building a data pipeline using Scala, Java, or Python, preferably with Spark and KafkaData analytics experience with SQL, NoSQL, Hadoop, or ideally Spark.Machine learning experienceLinux development or system administration experience, including Python or BASH scripting.Automation experience with Ansible, Chef, Puppet, or other.
1038195"
176,Data Engineer,"Silicon Valley, CA",Silicon Valley,CA,None Found,None Found,"Bachelor’s degree in CS or related discipline
3 - 5 years of experience in using SQL and databases in a business environment
3 - 5 years of experience in data warehouse space
3 - 5 years of experience in custom ETL design, implementation, and maintenance
3 - 5 years of experience with schema design and data modeling
2 plus years of experience with programming or scripting languages (e.g. Python or shell scripting)
Preferred experience working with either a Map Reduce or an MPP system
Preferred experience working with cloud platforms such as AWS, Google Cloud Platform or MS Azure
Experience to analyze data to identify deliverables, gaps, and inconsistencies",None Found,"
Collaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions
Design and build data extraction, transformation, and loading processes by writing custom data pipelines
Design, implement and support a platform that can provide ad-hoc access to large datasets and unstructured data
Model data and metadata to support adhoc and pre-built reporting
Tune application and query performance using performance profiling tools and SQL
Build data expertise and own data quality for allocated areas of ownership
Work with data infrastructure to triage infrastructure issues and drive to resolution",None Found,None Found,"Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of what’s possible—together.

Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to nearly 5,000 employees. We were named one of Fortune’s 100 Best Companies to Work For in 2018 and are regularly recognized by our employees as a best place to work. You can find us in 28 cities across the U.S., U.K., and Canada.

Job Title: Data Engineer

Do you like working with data? Do you want to use data to influence decisions for products and services being used by business and consumers every day? If yes, we want to talk to you. Slalom is seeking a Data Engineer to join our Data & Analytics team.

As a Data Engineer, you should have expertise in the design, creation, management, and business use of extremely large datasets. You know and love working with analytic tools, can write excellent SQL, scripts (e.g. Python, PL/SQL) and ETL code (e.g. Alteryx, Informatica), and can use your technical skills and creative approaches to solve some unique problems in the BI space. In this role, you will be working across industry sectors such as retail, finance, healthcare and high-tech and you'll get an opportunity to solve some of the most challenging business problems.

Responsibilities:
Collaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions
Design and build data extraction, transformation, and loading processes by writing custom data pipelines
Design, implement and support a platform that can provide ad-hoc access to large datasets and unstructured data
Model data and metadata to support adhoc and pre-built reporting
Tune application and query performance using performance profiling tools and SQL
Build data expertise and own data quality for allocated areas of ownership
Work with data infrastructure to triage infrastructure issues and drive to resolution
Qualifications:
﻿Bachelor’s degree in CS or related discipline
3 - 5 years of experience in using SQL and databases in a business environment
3 - 5 years of experience in data warehouse space
3 - 5 years of experience in custom ETL design, implementation, and maintenance
3 - 5 years of experience with schema design and data modeling
2 plus years of experience with programming or scripting languages (e.g. Python or shell scripting)
Preferred experience working with either a Map Reduce or an MPP system
Preferred experience working with cloud platforms such as AWS, Google Cloud Platform or MS Azure
Experience to analyze data to identify deliverables, gaps, and inconsistencies

Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."
177,Senior Data Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Course Hero is scaling! We are looking for a motivated, and progressive Senior Data Engineer that will help build our next-generation Data Platform. We are searching for someone who has a devops mentality and passionate about innovating, optimizing and automating data at scale.

You can expect high impact and wide-ranging responsibilities: As a Senior Data Engineer you will have deep hands-on experience working with Data Scientists, Machine Learning experts, and Search Engineers. The scalable systems you build will enable our product designers to bring powerful data products to the Course Hero platform.

Check out these videos to learn more about our engineering culture ( https://www.youtube.com/watch?v=Hpa0bVeJpIE ), and our company mission ( https://www.youtube.com/watch?v=pmzuj0MW_Dk ).

Here are some ways you'll make an impact:

5+ years relevant data experience
Work with a team of passionate data engineers and scientists to enable data mining, deep learning, statistical modeling, predictive analytics, machine learning, and NLP.
Empower engineers and to innovate and discover insights by removing the technical barriers that come with processing big data.
Ensure compliance with the organization's high bar for data quality and modeling standards, across the product and related business areas.
Provide tools to empower internal teams across the organization (sales, operations, finance, engineering, etc.) to make data-driven decisions.
Institute development best practices to ensure the team produces high quality, well architected and supportable code through a continuous delivery model.
Participate in the on-call rotation and document administration and response procedures through runbooks & playbooks.

Are you our Star Senior Data Engineer?


Worked with Machine Learning experts and Data Scientists to enable self-service data ingestion, transformation, visualization, reporting and advanced analytics (machine learning, AI).
Strategic thinker and thrive operating in a broad scope, from conception through continuous operation.
Robust Ops foundation - you're always thinking ""What happens if this fails"" when you build things.
Bachelor's or Master's degree in computer science, mathematics, economics, engineering, or other related fields.
Expert in Mysql and other relational database technologies.
Hands-on experience working with large scale data ingestion, ETL processing, storage, Hadoop ecosystems, Spark, non-relational databases (NoSQL, MongoDB, Cassandra), and messaging systems (Kafka, Kinesis, RabbitMQ).
Written scripts in one or more languages such as Python or Go.

Bonus Points:

Understand open source software like Kafka, Arvo, ElasticSearch, NGINX, Kubernetes, and Docker.
Familiar with Python analytics libraries or use of R language.
Experience with standard IT security practices such as Access, Authorization, and Key Management.
Performed hands on work using AWS stack (i.e. S3, Redshift, EC2, SNS, SQS, SES, DynamoDB,Kinesis).

About Us:
At Course Hero, we have an awesome team and a truly engaging culture. We are customer-focused, collaborative, responsible, gritty and we love to learn. Our bold mission is to help students graduate confident and prepared!

We are not the only ones that think we're onto something big. Course Hero has been recognized as the 245th Fastest Growing Company in North America ( https://www.prnewswire.com/news-releases/course-hero-ranked-number-245-fastest-growing-company-in-north-america-on-deloittes-2018-technology-fast-500-300751425.html ) on Deloitte's 2018 Technology Fast 500 and also 2018's One of the Best Places to Work in the Bay Area ( https://www.bizjournals.com/sanfrancisco/feature/best-places-to-work/2018/best-places-to-work-bay-area-2018-top-workplaces.html ) by the San Francisco Business Times and the Silicon Valley Business Journal. Read up on some of our recent news coverage ( https://www.coursehero.com/press-room/ ), blog ( https://www.coursehero.com/blog/ ), and learn moreabout us ( https://www.coursehero.com/about-us/ ) to see what it is like to work with our team.

Benefits & Perks!


Competitive salary and stock options
Full medical coverage (medical, dental, vision)
401(k) program with match
Education Reimbursement
Quarterly team events and outings (Sporting Events, Escape Rooms, Go-Kart Racing, Karaoke, Bowling and much more!)
Free lunches twice a week, on-site cafe discount, plus an endless snack and drink supply
Onsite gym – Pacific Shores Center – Classes – Pool – Spa – Rock Wall - Massages!
Commuter benefits, shuttle service from Redwood City, and cell-phone allowance
Local move benefit to move within 10 miles of our office!
8 hours per quarter paid time for volunteering for a cause of your choice
Front row seat to Master Educator lectures – check out the videos on our LinkedIn Career Page

"
178,Big Data Engineer,"Newark, CA 94560",Newark,CA,94560,None Found,"Bachelor or Masters in Software Engineering and Computer Science
8+ years of experience in design and development of large scale data platforms
Expert in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Proficient in Spark development with PySpark or Scala
Expert in Data streaming platforms such as Apache Kafka
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python, and/or C++",None Found,None Found,None Found,None Found,"We are looking for a Staff Data Engineer, Big Data who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If building large and building fast, working with a young and very talented team of engineers and collaborating with the brightest mind in the Automotive industry is what you like, Lucid is the best to experience it.
The Role
Lead Data Engineer and architect to design, implement a highly scalable system to ingest and process Petabytes of data per day.
Hands-on design and develop applications for data pipeline and data management.
Set processes and policies for data governance and data pipeline
Architect and implement best practices of big data tools such as Spark, Airflow, Kafka, Presto and Cassandra
Lead and mentor junior Data and BI engineers.
Set and define the standards and best practices in data team
Be the point of reference for solving challenging technical problem.
Architect and implement Machine Learning Pipelines for Data Science team

Qualifications
Bachelor or Masters in Software Engineering and Computer Science
8+ years of experience in design and development of large scale data platforms
Expert in containerization, including Docker and Kubernetes
Expert in tools such as Apache Spark, Apache Airflow, Presto
Proficient in Spark development with PySpark or Scala
Expert in Data streaming platforms such as Apache Kafka
Expert in design and implement reliable, scalable, and performant distributed systems and data pipelines
Extensive programming and software engineering experience, especially in Java, Python, and/or C++
Experience with running large-scale distributed computing infrastructure such as load balancing, Zookeeper, Micro service architecture
Experienced in security and access management
Experience with managing distributed databases like Elasticsearch, Cassandra
Experience with Columnar database such as Redshift, Vertica
Great verbal and written communication skills."
179,Data Engineer,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Earnin:
Every year, while Americans wait for their paychecks, more than $1 trillion of their hard-earned money is held up in the pay cycle. As a result, we accumulate over $50 billion in late and overdraft fees and turn to high-interest loans. Overdraft charges and bank fees often trap people in a cycle of debt that can lead to unhealthy decisions and falling victim to predatory businesses disguised as helpful services. We don't accept that.

Earnin is an app that creates products that help people gain control of their finances. Cash Out lets people get paid as soon as they leave work, with no fees, interest, or hidden costs. With Health Aid, Earnin negotiates on behalf of community members to lower their total unpaid medical bill and work out a budget-friendly payment plan. Cash Back Rewards is a way for members to earn up to 10% cash back on purchases from over a thousand local and national businesses without needing a credit card or having to reach spend thresholds to earn cash rewards — and they can withdraw the money at any time. We also offer free tools to help avoid overdrafts, to remind people when recurring bills are due, and we're working on more! There is never any required cost to use any of these products or services, users can choose to tip what they think is fair to support the service and pay it forward to keep the movement going.

Earnin is supported by funding partners including Andreessen Horowitz, Matrix Partners, Ribbit Capital, Felicis Venture, Thrive Capital, and others. Join us and help build a new financial system focused on fairness and people's needs.

You can help make a difference.

About the Team:
We are a data driven mobile financial tech company and we're looking for a Data Engineer to join us and help us build out our data infrastructure to aid in our mission of enabling people to gain access to their paycheck on demand.

Data engineers are an important function to interact with every team within Earnin and you will be interfacing heavily with our analytics, engineering, and data science teams to help them advance our product utilizing machine learning intelligence.

As a Data Engineer you will:

Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute for our business
Help us build a world class data lake/data warehouse, by building data pipelines
Design and develop new systems and tools to enable folks to consume and understand data faster
Use your expert coding skills across a number of languages from Python, Java, C++, Go etc.
Work across multiple teams in high visibility roles and own the solution end-to-end
Design, build and launch new data extraction, transformation and loading processes in production
Work with data infrastructure to triage infra issues and drive to resolution.

Some skills we consider critical to being a Data Engineer:

BS or MS degree in Computer Science or a related technical field
Familiarity with Python
Familiarity with Hadoop stack, Spark, AWS Glue, AWS Athena etc
Diverse data storage technologies (RDBMS, Sql Server, Mysql, ElasticSearch, dynamodb, s3 etc.)
Deep familiarity with schemas, metadata catalogs etc.
Ability to manage and communicate data warehouse plans to internal clients
Strong communication skills, including the ability to identify and communicate data driven insight

Earnin does not unlawfully discriminate on the basis of race, color, religion, sex (including pregnancy, childbirth, breastfeeding or related medical conditions), gender identity, gender expression, national origin, ancestry, citizenship, age, physical or mental disability, legally protected medical condition, family care status, military or veteran status, marital status, registered domestic partner status, sexual orientation, genetic information, or any other basis protected by local, state, or federal laws. Earnin is an E-Verify participant."
180,Data Engineer – Systems and Configuration,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,"
Degree in Computer Science, Engineering, or related fields
Strong in data structures, algorithms, and systems
Deep understanding of interconnected systems behavior controlled through configuration
Experience with modeling system configuration and behavior from a manageability perspective
Experience with creating validated solution designs across multiple layers of the data center infrastructure stack in technical collaborations with partners
Experience with building and using configuration management systems and databases (e.g.: Chef, Puppet, BMC Remedy, etc.)
Experience with handling complex service request fulfillment and production issues
Heuristic problem solving with incomplete information
Deep domain knowledge and hands-on experience across a very broad spectrum of the backend, frontend, cloud, AI and data infrastructure platforms.
",None Found,"
Degree in Computer Science, Engineering, or related fields
Strong in data structures, algorithms, and systems
Deep understanding of interconnected systems behavior controlled through configuration
Experience with modeling system configuration and behavior from a manageability perspective
Experience with creating validated solution designs across multiple layers of the data center infrastructure stack in technical collaborations with partners
Experience with building and using configuration management systems and databases (e.g.: Chef, Puppet, BMC Remedy, etc.)
Experience with handling complex service request fulfillment and production issues
Heuristic problem solving with incomplete information
Deep domain knowledge and hands-on experience across a very broad spectrum of the backend, frontend, cloud, AI and data infrastructure platforms.
",None Found,None Found,"About Peritus
Peritus enables self-healing autonomous datacenters with automated, cognitive support for infrastructure software and hardware. It is a funded startup co-created at The Hive in Palo Alto, CA that delivers artificial intelligence based virtual support expert systems for datacenter service fulfillment and incident resolution.

As data center vendors move from on premise to the cloud their existing support system lacks the agility and cost-effectiveness for the cloud. Peritus significantly enhances operational efficiencies of existing support services, and enables managed service providers & system vendors to offer new business continuity entitlements. Peritus assists & automates a wide spectrum of decisions in system support including incident classification, routing, contract coverage, incident resolution recipes and orchestration of incident management between subject matter experts (SMEs).

Peritus’ unique vectorization of system log data drives predictive modeling with highly granular feature extraction for early detection of system events. The platform’s advanced natural language processing (NLP) capabilities drive Peritus’ incident modeling and predictive capabilities. The core service fulfillment engine uses a combination of supervised and unsupervised methods to predict incident features from system log data. Peritus delivers automated orchestration of incident resolution through its close integration with existing incident management platforms.

Job Description
We are building a product that helps customers fulfill service requests as well as troubleshoot and diagnose infrastructure issues that cut across domains. The product needs to interface with configuration management systems/databases to glean insights into how systems are interconnected. Some of the key outcomes relate to the evolution of the interconnected systems over time through timestamped versioning of configuration trees, detection of anomalies via comparative analysis of the system with that of recommended best practices/solutions, and identification of interoperability issues by correlating configuration data across the infrastructure stack.

Responsibilities
We are looking to hire an engineering technical leader with deep systems knowledge. The role entails a deeper understanding of how data center systems interoperate, requiring familiarity with computing, storage, networking and virtualization products within a rack and across racks. The whole system behavior needs to be modeled from a manageability perspective to apply any corrective measures. The configuration infrastructure needs to enable building machine learning models that establish a baseline of a working system. Over time, the learning translates to automatic configuration of systems thereby enabling self-healing.

Can you help connect the dots for users to understand the impact of configuration changes?

Qualifications & Expertise
The successful engineer would have a proven track record of building complex log analysis platforms:

Degree in Computer Science, Engineering, or related fields
Strong in data structures, algorithms, and systems
Deep understanding of interconnected systems behavior controlled through configuration
Experience with modeling system configuration and behavior from a manageability perspective
Experience with creating validated solution designs across multiple layers of the data center infrastructure stack in technical collaborations with partners
Experience with building and using configuration management systems and databases (e.g.: Chef, Puppet, BMC Remedy, etc.)
Experience with handling complex service request fulfillment and production issues
Heuristic problem solving with incomplete information
Deep domain knowledge and hands-on experience across a very broad spectrum of the backend, frontend, cloud, AI and data infrastructure platforms.
Please send your resume to jobs@peritus.ai"
181,"Data Engineer, Analytics","Menlo Park, CA",Menlo Park,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
Do you like working with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. In this role, you will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.


This is a full-time position based in our office in Menlo Park.
RESPONSIBILITIES
Manage data warehouse plans for a product or a group of products.
Interface with engineers, product managers and product analysts to understand data needs.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data models in production.
Design, build and launch new data extraction, transformation and loading processes in production.
Support existing processes running in production.
Define and manage SLA for all data sets in allocated areas of ownership.
Work with data infrastructure to triage infra issues and drive to resolution.
MINIMUM QUALIFICATIONS
2+ years experience in the data warehouse space.
2+ years experience in custom ETL design, implementation and maintenance.
2+ years experience working with either a MapReduce or an MPP system.
2+ years experience with object-oriented programming languages.
2+ years experience with schema design and dimensional data modeling.
2+ years experience in writing SQL statements.
Experience analyzing data to identify deliverables, gaps and inconsistencies.
Experience managing and communicating data warehouse plans to internal clients.
PREFERRED QUALIFICATIONS
BS/BA in Technical Field, Computer Science or Mathematics.
Knowledge in Python or Java.
Facebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com."
182,Senior Application Data Engineer - SQL Specialist,"Palo Alto, CA",Palo Alto,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"You are intellectually curious. You like building cool stuff. And you're nice.

At Noodle, we are not just building AI applications. We are going deep into industries that have yet to leverage AI at scale such as steel mills, distribution & logistics companies or consumer packaged goods. Our applications fit and integrate deeply into the supply chain starting with planning to manufacturing to delivery. The applications we build have to not only integrate into the existing software in these industries but have to talk to each other so really drive the value from AI. Turns out, we are one of the pioneers here charting a new course. This means that science behind building the software and the AI behind it has not settled. You will be part of a team that is charting this new course figuring out how to adapt software engineering best practices to delivering AI applications that fit within legacy software in non-tech industries. This is going to be a bumpy ride and we are looking for people who are not afraid of the unknown, are experts at their craft and can adapt and learn as we create a suite of new AI applications.

Responsibilities


Work collaboratively with an interdisciplinary team of management consultants, product managers, data scientists, data engineers, software engineers, UX designers to understand & implement application data engineering requirements.
Execute rapid application prototyping cycles from breaking down technical requirements in tasks, data-pipeline design, programming, debugging, and optimization of database code.
Design, develop and evolve Noodle's AI Application Databases, Data Models & Data Pipelines.
In charge of database design, data modeling and data pipeline development activities for the Application.
Analyze complex Enterprise data, exploring entity relationships, performing data quality checks & validations.
Champion engineering, operational excellence, follow best practices and coding guidelines to deliver highly scalable application data components.
Help improve Noodle Data Architecture iteratively through innovations and adaptation of best practices & industry standards.

Qualifications


BS/BE/B.Tech or Advanced degree in a relevant field (Computer Science and Engineering, Technology and related fields). Master' s degree a plus.
6+ years of industry experience in data driven software development.
4+ years of real-world experience in architecting & developing scalable data driven AI applications, data warehousing & business intelligence solutions.
Expert in writing SQL queries, procedures and user-defined functions.
Demonstrated proficiency as a lead with database development, automation, performance tuning, optimization and management projects.
Real life work experience with relational databases: PostgreSQL, MS SQL Server, Oracle, etc.
Good understanding of Big Data concepts and scenarios where it works. This includes understanding of the nature of distributed systems and its pitfalls.
Good knowledge of database design and data modeling concepts.
Working knowledge of database security and data encryption.
Experience working with version control systems like: bitbucket, github, etc.
Experience working in a hybrid environment involving on-premise infrastructure and cloud databases will be a plus.
Hands-on experience with modern ETL tools and Orchestration platform like Airflow is desirable.
Familiarity with agile software development practices and DevOps is an added advantage.

Passion for learning and a desire to grow – Noodlers are life-long learners!"
183,Senior Data Engineer,"Sunnyvale, CA",Sunnyvale,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Do you want to be part of a growing data team with a huge impact on products used by millions of people around the world? Niantic, the developer of Ingress, Pokemon Go, and Harry Potter: Wizards Unite, is searching for a Senior Data Engineer. You'll lead efforts to scale and organize the data infrastructure, working closely with Machine Learning scientists and engineers to craft a highly scalable data pipeline capable of data-enabling Niantic’s next generation of augmented reality games. You'll form the cornerstone of a flexible, driven team to build game experiences which enrich millions of lives.
Responsibilities
You'll design and implement elegant data models (tables, partitioning, dependencies ...) for use with Niantic's ML pipelines and analytics.
You'll architect the technical stack for storing and processing large volumes of Niantic’s data.
You'll craft ETL pipelines to ingest and structure data from diverse sources.
You'll mentor and provide technical guidance to junior data engineers, with many opportunities to demonstrate leadership.
You'll work hand-in-hand with the ML Science and ML Engineering teams to provide datasets which can be used in production ML systems.
Qualifications
You have a BS, MS, or PhD in Computer Science, or a related technical field.
You have 4+ years of experience developing and deploying data pipelines.
You've deployed large-scale data extraction pipelines which feed into ML models.
You have extensive knowledge of large-scale data processing concepts and technologies.
You have experience with cloud deployment of pipelines and orchestration tools (Airflow, Composer).
You possess a deep knowledge of data storage and analysis technologies such as Hive, Presto, or Spark, and are comfortable with their trade-offs and optimizations.
You know your way around Java or Scala, Python and SQL.
Plus If...
You have knowledge of the Google data stack (Dataflow, Dataproc, BigTable, BigQuery, etc.).
You have experience with design of data models which serve multiple applications underlying the same model (common schemas across multiple games).
You have knowledge of ML models for classification, regression, and clustering.
You have experience with deploying ETL pipelines on large user bases (10s of millions of users).
Join the Niantic team!
Niantic is the world’s leading AR technology company, sparking creative and engaging journeys in the real world. Our products inspire outdoor exploration, exercise, and meaningful social interaction.
Originally formed at Google in 2011, we became an independent company in 2015 with a strong group of investors including Nintendo, The Pokémon Company, and Alsop Louie Partners. Our current titles include pioneering global-control game Ingress, record-breaking AR game Pokémon GO, and recently released third title, Harry Potter: Wizards Unite. .
Niantic is an Equal Opportunity and Affirmative Action employer. We believe that cultivating a workplace where our people are supported and included is essential to creating great products our community will love. Our mission emphasizes seeking and hiring diverse voices, including those who are traditionally underrepresented in the technology industry, and we consider this to be one of the most important values we hold close.
We're a hard-working, fun, and exciting group who value intellectual curiosity and a passion for problem-solving! We have growing offices located in San Francisco, Sunnyvale, Bellevue, Los Angeles, London, Tokyo, Hamburg, and Zurich."
184,"Senior Data Engineer, Product","Los Gatos, CA 95032",Los Gatos,CA,95032,None Found,"
Creative thinker and strong problem solver with meticulous attention to detail.
An aptitude to learn new technologies.
Passion for solving business problems using data.
Excellent written and verbal communication skills - Ability to communicate in a clear and effective manner with teams of diverse skills & mindsets to influence the overall strategy of the product.
Ability to initiate and drive projects to completion with minimal guidance in a fast-paced dynamic environment.",None Found,None Found,"MS or BS in Computer Science, Engineering, Math or a related field or equivalent experience.
4+ years of industry experience building large-scale distributed systems.
Strong Software Engineering experience with exceptional skills in at least one high-level programming language (Python, Java, Scala or equivalent).
Knowledge & experience with Spark, Hadoop, MapReduce, Kafka.
Experience with building stream-processing applications using Flink, Storm or Spark-Streaming.
Proficiency with NoSQL databases, such as HBase, Cassandra, MongoDB.
Experience with databases and SQL.
Experience with Cloud Computing platforms like Amazon AWS, Google Cloud etc.",None Found,"Los Gatos, California
Data Science and Engineering
Netflix is the world's leading internet entertainment service which has revolutionized how people interact with TV shows and movies. With over a 148M subscribers in 190 countries we strive to connect the world through amazing, award-winning stories which can be viewed on any internet-connected device. As a data-driven company, we use data to deliver delightful experiences to our users.

The “Product-Analytics” data engineering team is responsible for enabling and empowering our partners in Product, Science & Analytics by democratizing access to user interaction data.

As part of this engineering team, you will work on diverse data technologies such as Spark, Flink, Kafka, Cassandra & others to build mission-critical, scalable and robust data pipelines and intuitive data products that power data discovery & analysis in a self-service manner. You will be expected to partner effectively with our Engineering and Analytics teams to enable data-driven decisions which improve the experience of hundreds of millions of users worldwide.

Who are you?
You have a strong background in distributed data processing and software engineering.
You build high-quality data products and frameworks which can be leveraged across multiple teams.
You are knowledgeable about data modeling, data access, and data storage techniques.
You know how to write distributed, high-volume services in Java, Scala or other programming languages.
You appreciate agile software development principles and patterns.
You understand the value of partnership within teams.
You have limitless curiosity and are capable of taking on loosely defined problems.
Education & Experience
MS or BS in Computer Science, Engineering, Math or a related field or equivalent experience.
4+ years of industry experience building large-scale distributed systems.
Strong Software Engineering experience with exceptional skills in at least one high-level programming language (Python, Java, Scala or equivalent).
Knowledge & experience with Spark, Hadoop, MapReduce, Kafka.
Experience with building stream-processing applications using Flink, Storm or Spark-Streaming.
Proficiency with NoSQL databases, such as HBase, Cassandra, MongoDB.
Experience with databases and SQL.
Experience with Cloud Computing platforms like Amazon AWS, Google Cloud etc.
Key Qualifications
Creative thinker and strong problem solver with meticulous attention to detail.
An aptitude to learn new technologies.
Passion for solving business problems using data.
Excellent written and verbal communication skills - Ability to communicate in a clear and effective manner with teams of diverse skills & mindsets to influence the overall strategy of the product.
Ability to initiate and drive projects to completion with minimal guidance in a fast-paced dynamic environment.
APPLY NOW
Share this listing:
LINK COPIED"
185,Big Data Engineer,"Santa Clara, CA",Santa Clara,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"8+ years of back-end engineering experience, preferably Java Maven or Scala or Node.js
5+ years of experience in building IoT Bigdata and Analytics solution utilizing ETL tools, Kafka, Java or Scala, PL/SQL, Pentaho or Big Query SQL and Shell scripting
5+ years of Web development using Django or Angular JS, JavaScript frameworks, EXT JS, MongoDB or NoSQL or RDBMS
3+ years of API design, development and integration experience
2+ years of hands on experience creating applications and tools using Cloud Platform (example: Google Big Query, Cloud Storage, Cloud Functions, Pub/Sub, App Engine, Cloud SQL)
1+ year of experience in deploying AI models in preferably Google Cloud using Google Cloud Platform tools.
1+ year of experience in designing and scaling cloud-based applications (Google Cloud Platform and Amazon Web Services)."
