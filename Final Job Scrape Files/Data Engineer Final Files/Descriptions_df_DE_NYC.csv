,Title,Location,City,State,Zip,Country,Qualifications,Skills,Responsibilities,Education,Requirement,FullDescriptions
0,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"-------------
About Teampay
-------------

Teampay is the first purchasing software built for fast-growing, technology-enabled businesses. The way companies spend money has changed, rendering expense management tools obsolete. Our products empower businesses to request, approve and track spending in real-time.

Every day, more and more finance teams rely on Teampay to scale company purchasing. Led by an experienced team with prior exits, Teampay has raised double-digit millions of capital from prominent venture investors including CrossCut Ventures, Tribe Capital, Precursor Ventures, and CoVenture.

We're an agile team building software that's revolutionizing how companies spend their money. Joining Teampay at this early stage is an opportunity to grow your skill set, build a company, and get paid to do it.

--------------
About the role
--------------

As a data engineer at Teampay, you will architect a data infrastructure that is stable and scalable to support data analytics, reporting, and visualization. Your input and contribution will have a direct impact on our data strategy and technology roadmap.

What you'll do...


Combine and analyze data from various sources to help drive business insights
Work with various team and executive stakeholders to define mission-critical metrics and key performance indicators
Develop and maintain a scalable data infrastructure
Develop tools supporting self-service data pipeline management
Work closely with teams on design and implementation of data solutions
Own and provide BI development tools for internal use

Common Candidate Qualifications include…


3+ years functional experience in a data engineering or business intelligence role
Proficient in data modeling and systems design skills
Proficient in SQL
Proficient in at least one scripting language (ideally Python)
Experience building and maintaining robust ETL pipelines
Experience with version control systems
Experience with BI platforms

Nice to Have...


Experience with AWS
Machine learning experience
You enjoy telling a compelling story with data

You would be the first Data Engineer in a seed stage, fast-growing company. As a core member of the engineering team, you will have a strong impact on the future of Teampay. This is an unusual opportunity to join a business with traction at the ground floor.

----------------------
Apply at Teampay if...
----------------------

You're a builder. You're passionate about crafting things that matter. You're curious and agile in thought and action. You value authenticity and possess a strong work ethic. You're empathetic and look forward to learning from people unlike yourself. You want to make an impact with a strong team. You look for challenges that force you to grow. You rarely miss a detail and always learn from your mistakes. You have diverse interests outside of work, but are ready to pitch in and be responsive when the pressure is on.

-----------
Interested?
-----------

You can learn more about the product at www.teampay.co ( http://www.teampay.co/ ) and if you'd like to apply, please include a cover note and tell us about something you started, whether it's a club, a team, business or blog.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
1,Senior Data Engineer – Datasets,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"Data and information has become an invaluable asset for technology companies that focus on innovation and are dedicated to providing a great user experience. Spotify is taking this concept to heart and driving a “Data as a Product” initiative for its own data consumers within the company. The volume and breadth of data at Spotify is staggering – billions of records of streamed music, app interactions, artist information and user behavior trends flow through our platform on a daily basis. The Vivaldi squad and it’s Tribe are at the center of this initiative. We design and implement new ways and strategies to empower the Spotify community of data scientists, ML, researchers, product designers, fraud investigators, business analysts and the CEO himself.

What you will do
Understanding what fuels many of Spotify’s product features such as Discover Weekly, Daily Mix, Podcast offerings, holiday campaigns and others
Working hand-in-hand with the data science community to understand various user or content trends that influence product changes and customer acquisition strategies
Getting hands-on experience with Google Cloud Platform and technology/languages such as BigQuery, Scala, Scio, Luigi, Styx and Docker
Collaboration on a global scale; our squad offers ongoing opportunities to work in Stockholm with other engineering colleagues
Cross departmental exposure and flexibility to engage with many teams in the company
Gaining technical expertise in building a data platform at scale to solve business, product and technical use cases
Working in a supportive team that offers engineers the flexibility to be creative and chase interesting ideas
Work closely with cross-functional teams of data and backend engineers, analysts, user researchers, product managers and designers
Communicate insights and recommendations to key stakeholders, engineering and product partners
… and of course, having fun! Being passionate about what you do also means celebrating milestones within the team and the tribe!
Who you are
An BS/MS in CS or any other relevant fields of study
At least 3 years of work experience in the Data Engineering and Big Data field
Strong analytical and problem solving ability
Coding skills for analytics and data engineering/manipulation (Scala, Java and Python)
Strong communication and data presentation skills (such as Tableau, PowerPoint, Qlik, etc.)
Experience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Google’s Cloud Platform
You are capable of tackling very loosely defined problems and thrive when working on a team which has autonomy in their day to day decisions
You are a communicative person that values building strong relationships with colleagues and multiple stakeholders, and have the ability to explain complex topics in simple terms
Ideally you have experience working in a large scale, global consumer product company, in an engineering or insights role"
2,Director of Data Science,"New York, NY 10005",New York,NY,10005,None Found,None Found,None Found,None Found,None Found,None Found,"DIRECTOR OF DATA SCIENCE

Who We Are

Murmuration transforms how political campaigns, advocates, and organizers identify, engage, and mobilize people and communities. Our focus is on driving change and accelerating progress toward a future where every child in America has the opportunity to benefit from a high-quality public education. Our partners include the leading practitioners and funders of efforts to ensure access for all children to a high-quality public education.

What We Do

At Murmuration, we provide our partners with the skills, knowledge, and tools they need to drive sustained political change—at all levels of government and aspects of civic life. Through the use of predictive intelligence and easy-to-use tools, Murmuration’s partners make informed decisions about who they need to reach, what they need to say, and how to achieve and sustain impact. Every application of this work, as well as shared knowledge and best practices, further improves our collective ability to organize communities and electoral campaigns.

Murmuration offers a fully featured political campaign platform that allows our partners to execute large scale electoral, advocacy and organizing efforts across the United States. The m{insights toolset includes a web platform that sits on top of an aggregated dataset from a wide variety of sources, including publicly available data, consumer data, voter-file data, and membership data furnished by Murmuration’s partners. Partners are also offered sophisticated targeting and outreach tools to help activate key audiences and expand their base of support. Murmuration’s data science team also provides deep analysis on our partners’ work, helping craft experiments and polls, building predictive models and measuring impact.

About the Position

The Director of Data Science will lead a team of analysts and data scientists charged with managing a suite of analytics products that will position our partners for greater impact. The team’s current main projects include: building machine-learning models to predict voter opinions and behavior, running national scale polls for our sector and partner-specific polls on request, and constructing large scale randomized control trials to determine the effectiveness of various outreach methodologies in local and state elections. The Director will also have the remit and be encouraged to expand the scope of analytic offerings for our partners.

The Analytics team is a highly collaborative, friendly, and hard-working group, and we are looking for a Director that embodies those values. The Director of Data Science will report to the Chief Data Scientist and work in close tandem with the Director of Data Management and our Lead Data Engineer.

The Director will:

Be an excellent personnel manager, with a track record of providing constructive feedback to direct reports and of helping employees develop their skills and careers
Help manage and coordinate Murmuration’s day-to-day analytics projects, including managing the resources of the Analytics team and planning for scalable growth in order to provide continued high-quality analytics and data science to our partners
Interact with our partner organizations to understand their analytic needs, in order to better determine the course of our R&D work
Collaborate with the Chief Data Scientist, to define future research priorities and R&D efforts that streamline the production of our existing analytics projects
Liaise with the VP of Partnerships to determine project delivery timelines and determine the staffing needs and assignments for partner engagements
Identify staffing needs and collaborate with HR to hire team members as we grow
Provide input and recommendations on the future composition and remit of the Analytics team
Candidate Profile

Murmuration attracts employees with distinctive and diverse backgrounds and accomplishments. Integrity, creativity, flexibility, and drive are key attributes of competitive candidates.

The ideal candidate will have most of the following qualities:

Experience managing an analytics team, with a track record of achieving project goals and meeting deadlines while fostering a healthy team environment
Familiarity with modern analytics techniques in statistics and machine learning; it is ideal but not required to be an experienced data scientist, however experience managing data scientists is required
Strong problem solving skills as well as the ability to manage several tasks/projects concurrently and prioritize work effectively
Be an exceptional team player, with strong interpersonal skills
Strong communication skills to interact effectively with various internal and external stakeholders to develop analytics strategy and deepen our partnerships

Location, Compensation and Benefits

The Director of Data Science is a full-time, salaried position based onsite in New York City, with a comprehensive benefits package. Salary for this position is commensurate with experience.

An Equal-Opportunity Employer with a Commitment to Diversity

Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job."
3,Senior Data Engineer,"New York, NY 10017",New York,NY,10017,None Found,None Found,None Found,None Found,None Found,None Found,"Senior Data Engineer
New York, NY
Are you an experienced Data Engineer who designs and implements innovative solutions that leverage modern tools and technologies?
Do you share our passion for enabling positive change within healthcare and helping patients with chronic conditions like diabetes?
If so, you could be a perfect fit for our team of like-minded professionals who share a common mission and passion for helping others and a desire to build a great company. Come and work with us to build our next generation healthcare platform!
Cecilia Health is a high-growth, venture-backed healthcare company based in New York City. We partner with pharmaceutical & device companies, payers and ACOs to deliver personalized, technology-enabled coaching to improve treatment, adherence and health outcomes for people living with diabetes and other chronic conditions. Cecelia Health is a high-energy, results-oriented work place that believes our success, as well as the success of our customers and patients, relies primarily on a fantastic team with the passion, drive and skills to change the face of chronic condition management.
We are hiring a Senior Data Engineer in New York City. This role will report to our Chief Technology Officer and join a strong technology team that is continuously innovating our current solutions that allow clinicians to efficiently serve an increasing volume of patients.
WHO YOU ARE
You have data in your DNA and at least 5 years of relevant work experience in data engineering requiring application of analytic skills to integrate data into business operations. You are an expert in using modern technology stacks to build intelligent ETL and data processing pipelines. You've worked in a variety of data wrangling roles and have strong knowledge of data manipulation using advanced SQL and other tools. You have a solid understanding of engineering best practices, as well as familiarity with scaling DevOps and engineering initiatives. You want to help develop software platforms that have real-world impact on people with chronic diseases like diabetes.
POSITION RESPONSIBILITIES
Define and lead data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage
Apply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification
Improve data sharing, increase data repurposing and improve efficiency associated with data management efforts
Build best practices to support tracking chain of custody of data so it can be easily traced back to the source for accuracy and consistency
Responsible for the design, development, and documentation of data models and ETL processes that will feed various databases including a data warehouse/lake and various analytics/data science components
Support and enhance existing applications and automation processes on a daily basis using a combination of SSIS, C#, .NET Core, Python, PowerShell, SQL Server, and Snowflake databases
Ensure production service levels, performance quality, and resolution of data load failures
Manage multiple projects independently
Evaluate and recommend tools, technologies and processes to ensure the highest quality product platform
QUALIFICATIONS
Required Experience
Bachelor's degree in information science, computer science, engineering or a similar area, with 5+ years of related experience, or comparable real-world development experience
Master Data Management experience including data consolidation, linkage, federation and dissemination
Strong knowledge of Python for Data Engineering
Advanced SQL experience (Nested Queries, Complex Joins, Analytic Functions, Time Series)
Experienced working in Agile/Dev Operations environment with continuous integration and continuous deployment and application lifecycle management
Strong communication skills
Desired Experience
Data integration, application development and secure information management in healthcare, life sciences, or clinical research
Experience with scalable, enterprise-level development on virtualized (AWS, Azure, GCP) infrastructure
Experience with a cloud-based data warehouse such as Snowflake or Redshift
Familiarity with analytics and business intelligence tools (Tableau, Power BI, Cognos).
Experience with Real Time processing and Big Data tools (Hive, Spark, Hadoop, HDFS, Kafka, Lucene, Glue, Airflow…)
D9aAMpL07D"
4,Senior Software Engineer - Data,"New York, NY",New York,NY,None Found,None Found,"
Master's or bachelor's degree in Computer Science
At least 10 years of hands-on software development experience in Python, Golang, Java, C++ or Scala
Strong Object Oriented Programming skills
Deep knowledge in data structures, algorithms, and software design
Experience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources
Highly proficient with relational and non-relational data storages
Strong verbal and written communication skills",None Found,"
Work with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs
Adheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time
Able to define new architectures and improve existing ones
Can be the central focus for code reviews, architecture discussion and bug fixing
Demonstrates code and product ownership in production
Support the business teams and product managers in data extracts and data analysis
Performs as a true agile team leader and exhibits competencies in all layers of the application stack
Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration",None Found,None Found,"Lucid is a market research platform that provides access to authentic, first-party data in over 90 countries. Our products and services enable anyone, in any industry, to ask questions of targeted audiences and find the answers they need – fast. These answers can be used to uncover consumer motivations, increase revenue, and measure the impact of digital advertising. Founded in 2010, Lucid is headquartered in New Orleans, LA with offices in Dallas, New York, London, Sydney, Singapore, Gurgaon, Prague, and Hamburg.


Apply for this Job

The Opportunity

The Senior Data Engineer will be a key part of our development team and work closely with development team peers, Product Management team, and business and other support teams.
Responsibilities
Work with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs
Adheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time
Able to define new architectures and improve existing ones
Can be the central focus for code reviews, architecture discussion and bug fixing
Demonstrates code and product ownership in production
Support the business teams and product managers in data extracts and data analysis
Performs as a true agile team leader and exhibits competencies in all layers of the application stack
Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration
Qualifications
Master's or bachelor's degree in Computer Science
At least 10 years of hands-on software development experience in Python, Golang, Java, C++ or Scala
Strong Object Oriented Programming skills
Deep knowledge in data structures, algorithms, and software design
Experience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources
Highly proficient with relational and non-relational data storages
Strong verbal and written communication skills
Preferred Qualifications
AWS experience with S3, RDS, Redshift, EMR, Kinesis, Lambda, Elastic Beanstalk and Elasticsearch
Experience with Spark, Hadoop and Hive on EMR and non-managed. Can build and run a basic cluster
Experience developing ETLs and running job schedulers (e.g. Airflow)
Experience with PostgreSQL
Experience with NoSQL data sources including Cassandra
Experience with Kafka, Redis
Experience creating Data Lakes


At Lucid we foster a collaborative and inspiring workplace. We pride ourselves in doing this by recruiting, hiring and retaining diverse, passionate, and forward-thinking talent. Lucid is committed to and encourages an inclusive environment and we are dedicated to providing equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please let us know."
5,Senior Data Engineer (Scala/Spark),"New York, NY 10010",New York,NY,10010,None Found,None Found,None Found,None Found,None Found,None Found,"dv01 is the world's first end-to-end data management, reporting and analytics platform offering loan level transparency and insight into lending markets, making them more efficient for institutional investors and safer for the world. In a nutshell, we're doing our part to prevent a repeat of 2008.

As the technological hub between lenders and capital markets, dv01 provides all parties with unprecedented data transparency, insight, and analytics. dv01 has integrated data from 16 marketplace lending platforms, including LendingClub, Prosper and SoFi and multiple mortgage servicers. To date, dv01 has provided reporting and analytics on $105 billion of online lending and mortgage loans and $35 billion of securitization coverage.

To get a better idea of what a year at dv01 looks like, check out our 2018 Year in Review page here: https://dv01.co/2018yearinreview/ ( https://dv01.co/2018yearinreview/ ). If that looks like fun to you, get in touch because we'd love to hear from you.

YOU WILL:
---------

Own our data platform. You will own the infrastructure for the data platform that powers all of dv01's customer offerings. This includes an Apache Spark cluster, SQL Server databases, scheduling/monitoring tools, API crawlers, SFTP servers, and ad-hoc analysis services such as Apache Zeppelin and RStudio—all hosted on the cloud.

Work extensively with open source technology. You'll work heavily within the Spark ecosystem, as well as explore new open source technologies to solve customer needs. The skills you develop here will serve you well beyond dv01.

Interact with a diverse team. You will collaborate closely with other teams at dv01 to expand and improve the capabilities of our data platform. This includes working closely with the structuring team to resolve scaling or functionality pain points in data processing, the modeling team to build out a modern data science platform, and the frontend product engineering team to scale database queries to ship new products.

Gain knowledge of the financial industry. You'll have an exclusive view into the system that enables Americans to afford houses, cars, and college. You will learn about the participants, terminology, and mathematics behind this sector of the financial markets.

QUALIFICATIONS:
---------------

Experienced with all aspects of working with big data. You have 2+ years of professional data engineering experience working with large data sets and are intimately familiar with the construction of scalable ETL pipelines, intricacies of accurate data processing, and infrastructure involved with ensuring the reliability of hundreds of daily processes.

A Spark Enthusiast. You have at least one year of professional programming experience with Apache Spark and a strong understanding of Spark internals and the operational complexities of managing a Spark cluster. You have engaged with the Spark community on mailing lists or on Github and may have contributed source code to the project.

A well-rounded engineer. You have 3+ years of professional programming experience with Scala or Java and a deep appreciation for engineering fundamentals. You understand the importance of writing tests, designing systems for long term maintainability, and evaluating both sides of common engineering considerations.

Enthusiastic about expanding your financial knowledge. You are excited to learn more about the intricacies of the financial system. You are adept at learning on the job and unafraid to dive into technical finance books.

Undergraduate or graduate degree in Finance, Math, or Engineering. Note that we're not anti dropouts if you're a superstar.

Perks and Benefits:
Almost 100% Paid Benefits (medical/dental/vision)
Annual Professional Development Stipend
Monthly Commuter Budget
Daily Lunch and Dinner Allowance
Free Premium Equinox or ClassPass Membership
Unlimited PTO and Remote days
Casual, collaborative culture
Company Outings (Happy Hours, Team Yoga, Book Club, etc.)

dv01 is an equal opportunity employer and all qualified applicants and employees will receive consideration for employment opportunities without regard to race, color, religion, creed, sex, sexual orientation, gender identity or expression, age, national origin or ancestry, citizenship, veteran status, membership in the uniformed services, disability, genetic information or any other basis protected by applicable law."
6,Data Engineer,"New York, NY 10022",New York,NY,10022,None Found,"
BS/MS in Computer Science or a related technical field
Seeking candidates with 1+ years of experience in:
Architecting, building, and maintaining end-to-end, high-throughput data systems and their supporting services
Designing data systems that are secure, testable, and modular, particularly in Python, as well as their support infrastructure (shell scripts, job schedulers, message queues, etc.)
Designing efficient data structures and database schemas
Working with distributed systems architecture
Incorporating data processing and workflow management tools into pipeline design (AWS EMR, Airflow, Kafka, etc.)
Using profiling tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements
Developing for continuous integration and automated deployments
Utilizing a variety of data stores, including data warehouses (ideally Redshift), RDBMSes (ideally MySQL), in-memory caches (ideally Aerospike and Redis), and searchable document DBs (ideally Elasticseach)
Wrangling large-scale data sets",None Found,"
Ship high-quality, well-tested, secure, and maintainable code
Design, develop, and maintain data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions
Manage automated unit and integration test suites
Work collaboratively and communicate effectively with a small, motivated team of engineers and product managers
Experiment with and recommend new technologies that simplify or improve PromoteIQ's stack
Participate in an on-call rotation and work occasional off-hours",None Found,None Found,"PromoteIQ delivers intelligent vendor marketing solutions built for the next generation of e-commerce. Our solutions help retailers implement, automate, and scale brand-funded marketing programs on e-commerce sites. PromoteIQ is a New York City-based technology company that works with the US's largest e-commerce retailers. Learn more about us at https://www.promoteiq.com.

Who we’re looking for
At PromoteIQ, data plays an integral role in our product, and software engineers on our data engineering team build the pipelines that power reporting and analytics for our e-commerce promotions platform. The infrastructure and applications that you'll build on the data engineering team will have broad and critical reach in powering real-time auction decisions, becoming multipliers on our revenues, and forecasting supply and demand for our customers.
Responsibilities
Ship high-quality, well-tested, secure, and maintainable code
Design, develop, and maintain data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions
Manage automated unit and integration test suites
Work collaboratively and communicate effectively with a small, motivated team of engineers and product managers
Experiment with and recommend new technologies that simplify or improve PromoteIQ's stack
Participate in an on-call rotation and work occasional off-hours
Qualifications
BS/MS in Computer Science or a related technical field
Seeking candidates with 1+ years of experience in:
Architecting, building, and maintaining end-to-end, high-throughput data systems and their supporting services
Designing data systems that are secure, testable, and modular, particularly in Python, as well as their support infrastructure (shell scripts, job schedulers, message queues, etc.)
Designing efficient data structures and database schemas
Working with distributed systems architecture
Incorporating data processing and workflow management tools into pipeline design (AWS EMR, Airflow, Kafka, etc.)
Using profiling tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements
Developing for continuous integration and automated deployments
Utilizing a variety of data stores, including data warehouses (ideally Redshift), RDBMSes (ideally MySQL), in-memory caches (ideally Aerospike and Redis), and searchable document DBs (ideally Elasticseach)
Wrangling large-scale data sets

#MicrosoftAdvertising #PromoteIQ

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work."
7,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Us
--------

NS1's mission is to manage the world's application traffic. We are the market leader in DNS and traffic management software and services, and our customers include the biggest properties and largest enterprises on the internet, such as Salesforce, LinkedIn, Squarespace, Pandora, Imgur, Yelp, Dropbox, and many more. Our modern DNS technologies enable optimized application delivery, couple via our APIs into the tooling and processes of today's DevOps organizations, and deliver reliability and performance at global scale. We operate a worldwide, highly tuned Managed DNS network, and also deliver our technologies to customers as single-tenant software deployments. We solve incredibly challenging problems on behalf of our customers, in the most mission critical parts of their stack.

The Role
--------

NS1 is looking to hire an experienced data engineer (or backend developer interested in specializing in data) to join our growing data science team. As an engineer embedded within the data team, you will be responsible for architecting and deploying our big data analytics infrastructure, creating ETL pipelines and helping the team test and deploy machine learning models. We are seeking someone who can leverage existing technical strengths in distributed systems, immerse themselves in our platform quickly, and teach themselves any systems that are new to them.

Your Skills
-----------

Data engineers come from all backgrounds. Even if you do not meet all the requirements, send us a note telling us why you are interested. In general, you would be a strong candidate if you are familiar with more than a few of the following:


Python, Go, Scala, C++, Node.js
Extensive experience of databases and data modeling
Data pipelining, ETL, statistics
AWS: Kinesis, DynamoDB, Lambda, SQS
Spark, Hadoop, MapReduce
Experience with docker
Excellent UNIX and networking knowledge
Strong Linux systems programming skills
Knowledge of distributed systems
Ability to design with modern internet infrastructure operation in mind, including metrics, logging, automation, etc

Our technology stack at NS1, and the many systems you'll have an opportunity to work with here:
-----------------------------------------------------------------------------------------------------


Our globally distributed platform is comprised of many subsystems including:
Custom built DNS software that's deployed on physical hardware and an anycasted network that spans nearly 30 facilities globally
REST API, and Portal
Deployment automation, CI/CD, unit/integration testing
Monitoring, metrics collection and alerting
Traffic load balancing, filtering, and DDoS mitigation tools
Messaging, persistent DB and caching systems
Other technologies and integrations include:
Linux, Ansible, Docker & other container platforms
BGP, BPF/IPTables, SDN, packet analysis
MongoDB, Redis, RabbitMQ, SQL
Python (Twisted), Bash, C, C++14, React, Redux, D3
Hadoop/HDFS/OpenTSDB, Grafana, Bosun
Integrations with third party SaaS, APIs, and libraries, various Open Source projects including REST API clients and integrations

Working @ NS1
-------------

We're a fast-growing, well-funded startup based in the heart of New York City's Financial District with offices and team members around the world. Working at NS1, you'll come to understand our team is unique, both in and out of the workplace. We have PhDs, musicians, artists, and athletes working side by side, dedicated to delivering first class products. We're hardworking, but we're also a compassionate group. We understand that outside of NS1 is a world that places demands on our time. Our leadership team is dedicated to open and honest communication and we continuously strive to foster a culture of transparency, flexibility, and creativity.

We offer:

competitive compensation (salary and stock options)
medical, dental, and vision
commuter benefits
401k
flexible hours and time off
choice of workstation

NS1 is an equal opportunity employer."
8,Data Engineer - Product,"New York, NY 10176",New York,NY,10176,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer - Product
REF#: 35132
CBS BUSINESS UNIT: Showtime
JOB TYPE: Full-Time Staff
JOB SCHEDULE: Full-Time
JOB LOCATION: New York, NY
DESCRIPTION:
The Showtime Product team is looking for a curious and creative Data Engineer to help us pursue answers to our increasingly interesting and complex business questions and empower our team to incorporate data-driven features and machine learning into our products, which include our standalone service SHOWTIME and our TV Everywhere service, Showtime Anytime.
The big data platform at Showtime is relatively new, but is now being used across the company for critical functions and features like: recommendations, analyzing customer experience, understanding programming consumption’s effect on subscriber lifetime, building churn/retention prediction models, and more. In this role, you will work our dedicated Product Analytics team, the Showtime Research and Data Strategy teams, the CRM team and our in-house engineering team, and you will architect and enable technologies, systems and workflows that enable our analysts and data scientists to focus more on algorithms and analyses than on the associated engineering.
Ideal candidates will be innovative, self-motivated, a quick study, and willing to develop new skills while constantly improving existing abilities.
Key Technologies:
Java, Scala, Groovy, Spark, AWS, AWS/EMR, Spring, Mongo, Git, Redis, Bamboo, JIRA etc
Responsibilities
Develop understanding of key business, product and user questions.
Collaborate with other Engineering team members to develop, test and support data-related initiatives. Work with other departments to understand their data needs.
Evolve data-driven feature prototypes into production features that scale; streamline feature engineering, so that the underlying data is efficiently extracted.
Build flexible data pipelines that we can rapidly evolve as our needs change and capabilities grow.
Develop and enhance data warehouse in AWS S3.
Employ data mining, segmentation, and other analytical techniques to capture important trends in our user base.
QUALIFICATIONS:
3+ years of relevant experience in a comparable data engineering role
Expert-level knowledge of SQL/Spark SQL
Experience in pursuing and applying data-backed decisions, such as recommendations, trends etc. to make the core product better
You like to dive-deep on data analysis or technical issues to come up with effective solutions and are comfortable summarizing key insights graphically when needed
You believe in writing code that is easy to understand, test and maintain
You enjoy a workplace that values autonomy, applauds ideas and a enjoys a sense of humor
ABOUT US:
SHOWTIME continues to make its mark across the cultural landscape with one of the most successful programming lineups in television. The SHOWTIME programming slate features original series including Emmy® nominated limited series ESCAPE AT DANNEMORA, BILLIONS, HOMELAND, SHAMELESS, THE CHI, RAY DONOVAN, THE AFFAIR, KIDDING, BLACK MONDAY, THE LOUDEST VOICE, CITY ON A HILL and ON BECOMING A GOD IN CENTRAL FLORIDA. SHOWTIME continues to raise the bar with fresh content including upcoming series THE L WORD: GENERATION Q, BACK TO LIFE, THE GOOD LORD BIRD and WORK IN PROGRESS. The network’s eclectic, brand-defining programming is further distinguished by the captivating offerings of SHOWTIME Documentary Films, including docuseries THE CIRCUS: INSIDE THE WILDEST POLITICAL SHOW ON EARTH, Emmy-nominated WU-TANG CLAN: OF MICS AND MEN and THE FOURTH ESTATE and upcoming documentary films THE KINGMAKER and READY FOR WAR. SHOWTIME Sports continues to dominate with its flagship franchise SHOWTIME CHAMPIONSHIP BOXING® and the Emmy Award-winning series INSIDE THE NFL. SHOWTIME is currently available to subscribers via cable, DBS and telco providers, and as a stand-alone streaming service through Amazon, Apple®, Google, LG Smart TVs, Oculus Go, Roku®, Samsung and Xbox One. Consumers can also subscribe to SHOWTIME via Amazon’s Prime Video Channels, DirecTV Now, FuboTV, Hulu, Sling TV, Sony PlayStation™ Vue and YouTube TV. The network’s authentication service, SHOWTIME ANYTIME, is available at no additional cost to SHOWTIME customers who subscribe to the network through participating providers. Subscribers can also watch on their computers at [1] www.showtime.com and [2] www.showtimeanytime.com.
References
Visible links
http://www.showtime.com
http://www.showtimeanytime.com
EEO STATEMENT:
Equal Opportunity Employer Minorities/Women/Veterans/Disabled"
9,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,"
Experience with data dictionary concepts, data mapping, and data architecture.
Knowledge and experience with a wide range of tools and IT technologies.
Proven ability to research and learn tools, hardware, and languages quickly.
Understanding of data analysis strategies and concepts (e.g. business intelligence, time series analysis, project management).
Excellent interpersonal, organizational, and communication skills required.
Must be a self-starter with the ability to work independently and manage multiple long-term projects.
Strong attention to detail and high concern for data accuracy.
Ability to interact with all levels of staff, with a high regard for confidentiality and diplomacy.
Ability to work efficiently to meet deadlines.
Dependable team player who works collaboratively and cooperatively with staff in a team-oriented environment.
Ability to multi-task in a fast-paced environment, prioritize among competing needs and respond quickly to requests for information.
Ability to follow directions and apply proper policies, procedures and guidelines.
Resourcefulness, initiative, and good judgment essential.
",None Found,"
Develop expertise in the DANY data model.
Develop data dictionaries and best practices for data definitions and use.
Coordinate with network administrative staff to develop, implement, and maintain a secure and agile sandbox environment.
Identify tools and procedures to effectively use data.
Research and promote options for quick development.
Document standards for security and maintainability.
Supervise developers hired for special projects.
Interact with Enterprise projects development team to coordinate data collection, standards, and use.
Perform related tasks as assigned.
","
Bachelor's degree in Information Technology or Computer Science/Engineering, Machine Learning or a related field required.
Master's degree preferred.
","
Bachelor's degree in Information Technology or Computer Science/Engineering, Machine Learning or a related field required.
Master's degree preferred.
","Position Summary:
The New York County District Attorney's Office (DANY) has an opening for a Senior Data Engineer in its Information Technology (IT) Department. DANY's IT Department provides agency-wide IT solutions for investigations, prosecution support, and case management. The Senior Data Engineer will work with key data sets used in the prosecution process and other criminal justice initiatives. The Senior Data Engineer will also be a key force in the development of tools and appropriate environment for the effective and progressive use of the data to further criminal prosecutions and justice initiatives.

Responsibilities include but are not limited to:

Develop expertise in the DANY data model.
Develop data dictionaries and best practices for data definitions and use.
Coordinate with network administrative staff to develop, implement, and maintain a secure and agile sandbox environment.
Identify tools and procedures to effectively use data.
Research and promote options for quick development.
Document standards for security and maintainability.
Supervise developers hired for special projects.
Interact with Enterprise projects development team to coordinate data collection, standards, and use.
Perform related tasks as assigned.

Qualifications:

Experience with data dictionary concepts, data mapping, and data architecture.
Knowledge and experience with a wide range of tools and IT technologies.
Proven ability to research and learn tools, hardware, and languages quickly.
Understanding of data analysis strategies and concepts (e.g. business intelligence, time series analysis, project management).
Excellent interpersonal, organizational, and communication skills required.
Must be a self-starter with the ability to work independently and manage multiple long-term projects.
Strong attention to detail and high concern for data accuracy.
Ability to interact with all levels of staff, with a high regard for confidentiality and diplomacy.
Ability to work efficiently to meet deadlines.
Dependable team player who works collaboratively and cooperatively with staff in a team-oriented environment.
Ability to multi-task in a fast-paced environment, prioritize among competing needs and respond quickly to requests for information.
Ability to follow directions and apply proper policies, procedures and guidelines.
Resourcefulness, initiative, and good judgment essential.

Educational Requirements:

Bachelor's degree in Information Technology or Computer Science/Engineering, Machine Learning or a related field required.
Master's degree preferred.

Commitment:

One (1) year commitment to hiring department.

The New York County District Attorney's Office is an Equal Opportunity Employer"
10,Bioinformatics Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Description
Elysium is seeking a bioinformatics data engineer. In this position, you will be an essential member of our team and support new product development. You will work with multidisciplinary teams to build complex epigenomic data analysis pipelines and cloud infrastructure.

Responsibilities

You will:

Develop and maintain programmatic tools and workflows to automate the analysis of biological data
Design and develop databases and interfaces for the management of biological data
Process, integrate, and analyze biological datasets to reveal insights that enable testable hypotheses
Implement quality control measures and benchmarking methods into our bioinformatics solutions
Stay up-to-date with available bioinformatics tools through literature review and computational experiments
Contribute to a team of translational researchers committed to improving health
Place a priority on getting results with an emphasis on high quality outcomes
Display a willingness to challenge the status quo and take risks
Possess a strong sense of urgency that drives performance beyond expectation

Requirements

You have:

MS or PhD in Bioinformatics, Computational Biology, Computer Science, or a related field
2+ years of experience (preferably industry-based) processing, managing, and integrating biological datasets
2+ years of programming experience in a production environment
Proficiency with R and Bioconductor packages, and one or more programming languages (Python, Perl, etc)
Familiarity with cloud services and cloud management systems
Excellent collaboration and communication skills, both verbal and written

About Elysium
Elysium is building the world's first direct-to-consumer health science company, developing advanced natural products that go beyond nutritional and dietary supplements to address cellular health at the most fundamental levels. Using new technologies, we can now impact wellness at an unprecedented scale - from cellular and metabolic processes to entire systems and pathways. We are growing our operational team to work with over 30 scientists and medical pioneers, as well as several Nobel Laureates, to bring scientific breakthroughs in health directly to the home.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
11,Data Engineer Lead,"New York, NY 10001",New York,NY,10001,None Found,None Found,None Found,None Found,None Found,None Found,"Responsibilities include but not limited:
Manage technical delivery related to Repo, Collateral & Margin platforms for Business initiatives and Front-to-Back programs.
Formulate and execute strategy to mitigate/eliminate the End-of-Life technical stack (components and dependencies).
Manage technical buildouts for critical margin services workflows, straight-through processing & collateral valuations.
Consolidate & standardize technical components supporting business processes (e.g. Margin Issuance, Disputes and Settlements).
Technical lead with a strong focus on delivery – impact analysis, design, hands-on development, and implementation.
 Machine learning/AI/NLP
Expectation/ Responsibilities:
Ability to source, aggregate, mine and analyze data from structured and unstructured sources.
Define, prototype PoC, develop, validate and deploy solutions based on Machine Learning models
Develop processes and tools to monitor and analyze model performance and data accuracy.
Good understanding of Natural Language Processing, Knowledge graph, named entity analysis
Thorough understanding of statistical concepts (distribution, regression techniques, and statistical tests)
 Qualifications:
Strong programming ability with major NLP toolkits – Open NLP/ Stanford etc.
Strong programming skills with Python & packages- Scipy, Scikit-learn, NLTK, Flask etc.
Experience visualizing/presenting data – Pyplot, Tableau/ Power BI.
Experience with distributed data/computing tools: Map/Reduce, Hadoop, & Spark.
Strong experience working with Sybase & Oracle and NOSQL Data sources (Mongo).
Strong experience processing information in multiple formats Excel, CSV, XML, Fixed Length, JSON etc.
Experience working with projects based on machine learning and statistics.
Strong experience in machine learning models Logistic Regression, Linear Regression, Random Forest, etc.
Experience working with cloud services –AWS and Azure- Dynamo, S3, Cosmos, ML Studio.
Exposure to RPA with BluePrism is nice to have.
Experience in real-time distributed messaging, multi-threaded trading, streaming data and elastic search is nice to have.
Bachelor's in Computer Science or Engineering Field
Java Stack
Other Qualifications:
Strong background and hands on experience with server side Java Stack development in Micro services architecture
Experience with HTML5 and Java-script technologies (ExtJs/Angular, React)
Strong focus on design and application architecture with hands on development.
Solid experience in real-time distributed messaging and multi-threaded trading environment.
Strong experience with Web services development and REST framework with XML/JSON as the exchange mechanism.
Strong experience with Object oriented design and programming skills.
C# / .NET experience
Experience with Big Data frameworks Apache Spark, Hadoop, Integration frameworks and messaging systems (ex. IBM MQ, JMS) or memory grids (ex. MongoDB, Redis).
Experience with Financial Services technology environment.
Experience with Repo Trading & Margin Services functions.
Ability to pick up new technologies independently.
Ability to work under pressure in a fast paced environment.
Experience in DevOps.
Delivery focused with proven ability to work under aggressive deadlines.
Strong controls/ compliance culture"
12,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"PromoteIQ delivers intelligent vendor marketing solutions built for the next generation of e-commerce. Our solutions help retailers implement, automate, and scale brand-funded marketing programs on e-commerce sites. PromoteIQ is a New York City-based technology company that works with the US's largest e-commerce retailers. Learn more about us at https://www.promoteiq.com.

Who we're looking for

At PromoteIQ, data plays an integral role in our product, and software engineers on our data engineering team build the pipelines that power reporting and analytics for our e-commerce promotions platform. The infrastructure and applications that you'll build on the data engineering team will have broad and critical reach in powering real-time auction decisions, becoming multipliers on our revenues, and forecasting supply and demand for our customers.

Responsibilities


Ship high-quality, well-tested, secure, and maintainable code
Design, develop, and maintain data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions
Manage automated unit and integration test suites
Work collaboratively and communicate effectively with a small, motivated team of engineers and product managers
Experiment with and recommend new technologies that simplify or improve PromoteIQ's stack
Participate in an on-call rotation and work occasional off-hours

Qualifications


BS/MS in Computer Science or a related technical field
Seeking candidates with 4+ years of experience in:
Architecting, building, and maintaining end-to-end, high-throughput data systems and their supporting services
Designing data systems that are secure, testable, and modular, particularly in Python, as well as their support infrastructure (shell scripts, job schedulers, message queues, etc.)
Designing efficient data structures and database schemas
Working with distributed systems architecture
Incorporating data processing and workflow management tools into pipeline design (AWS EMR, Airflow, Kafka, etc.)
Using profiling tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements
Developing for continuous integration and automated deployments
Utilizing a variety of data stores, including data warehouses (ideally Redshift), RDBMSes (ideally MySQL), in-memory caches (ideally Aerospike and Redis), and searchable document DBs (ideally Elasticseach)
Wrangling large-scale data sets

--

How We Interview & Hire
Our interview process begins with a quick phone call to assess fit with the role and company - and help you decide if PromoteIQ is the right place for you. Then we do several rounds of in-person interviews, where you'll meet the team and participate in a number of interactive interviews. Throughout this process you'll have ample opportunity to ask questions, get to know us as a company, and learn more about our product."
13,Senior Sales Specialist - Azure Data & AI,"New York, NY 10022",New York,NY,10022,None Found,None Found,None Found,"You will work with partners and others at Microsoft, as well as use our core tools, targeted account lists to identify and engage prioritized customers. You will be required to be disciplined in business-management, adaptable to a culture of accountability and build a strong and active business network.You will be the key technical leader, trusted advisor and influencer in shaping customer decisions to buy and adopt Microsoft Data & AI solutions. You will own winning the technical decision at customers for sales opportunities and usage scenarios, through tailoring your message, bringing ideas to customers, engaging with them to show our technology differentiation, and guiding them in decision making. You will lead presentations and solution demonstrations to explain and prove to our largest customers the capabilities of Microsoft's Data & AI solutions, and how we can make their businesses more successful.You will be influencing the Microsoft Data & AI go to market strategies by providing feedback to sales, marketing, and engineering on current and future product requirements and sales blockers. You will be recognized for sharing, learning and driving work that results in business impact for customers, partners and Microsoft. We encourage thought leadership and we encourage all our employees to continuously maintain and enhance their technical, sales, professional skills and competitive readiness. You will therefore be required to attain and maintain required certifications",None Found,None Found,"As an Azure Data & AI Specialist you will be a senior solution sales leader within our enterprise sales organization working with our most important customers. You will lead a virtual team of technical, partner and consulting resources to advance the sales process and achieve SQL Server and Azure Data Services (including our Data Platform, AI and IoT services) revenue and consumption in your assigned accounts. You will be a trusted advisor and a Data & AI subject matter expert. You will help customers evaluate their applications, recommend solutions that meet their requirements and demonstrate these solutions to win the technical decision. You will need to support customers to remove roadblocks to deployment and drive customer satisfaction. You will help our customers take advantage of our unique hybrid data platform to realize the value of digital transformation.

Responsibilities
Primary accountabilities for this role include:You will work with partners and others at Microsoft, as well as use our core tools, targeted account lists to identify and engage prioritized customers. You will be required to be disciplined in business-management, adaptable to a culture of accountability and build a strong and active business network.You will be the key technical leader, trusted advisor and influencer in shaping customer decisions to buy and adopt Microsoft Data & AI solutions. You will own winning the technical decision at customers for sales opportunities and usage scenarios, through tailoring your message, bringing ideas to customers, engaging with them to show our technology differentiation, and guiding them in decision making. You will lead presentations and solution demonstrations to explain and prove to our largest customers the capabilities of Microsoft's Data & AI solutions, and how we can make their businesses more successful.You will be influencing the Microsoft Data & AI go to market strategies by providing feedback to sales, marketing, and engineering on current and future product requirements and sales blockers. You will be recognized for sharing, learning and driving work that results in business impact for customers, partners and Microsoft. We encourage thought leadership and we encourage all our employees to continuously maintain and enhance their technical, sales, professional skills and competitive readiness. You will therefore be required to attain and maintain required certifications
Qualifications
Professional
Experienced. 8+ years’ experience selling business solutions to large/global enterprise customers with a focus on data platform technologies preferred
Account Management. Effective account management: planning, opportunity qualification and creation, stakeholder and executive communication, needs analysis, value engineering, services/partner engagement, opportunity management, pipeline management, large dollar licensing and deal negotiation required
Executive Presence. Experience and expertise selling to senior business decision makers by aligning & reinforcing the value of the solution to the customer’s overall business pain and/or strategic opportunities and decision criteria.
Problem Solver. Ability to solve customer problems through cloud technologies required
Collaborative. Orchestrate and influence virtual teams to pursue sales opportunities and lead v-teams through influence

Technical
Azure Platform. Understanding of Microsoft server products and/or complementing solutions. The position requires the ability to articulate and demonstrate the business value of Microsoft's solutions and have a firm understanding of Microsoft's strategies and products relative to major Microsoft competitors required
Leadership. Experience leading large cloud deals especially those involving Data Platform modernization and migration, AI and related required
Competitive Landscape. Knowledge of enterprise software solutions and platform competitor landscape
Partners. Understanding of partner ecosystems and the ability to leverage partner solutions to solve customer needs.
Certifications. Azure Data Engineer or Azure AI Engineer preferred

Education
Bachelor’s Degree required, MBA preferred, or equivalent experience.

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work."
14,Analytics Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,"
2+ years experience working with data warehouses, data visualization and data analysis
Experience implementing best-practices for data modelling, especially with regards to dimensional modelling for business intelligence. Experience with DBT is a plus.
Experience with SQL-first, self-service visualization tools such as Mode Analytics, Tableau, Looker
High level of comfort with SQL
Proficiency in Python and its scientific libraries (pandas, scipy etc.)
Experience with git and git-based workflows
You understand that analytics and business intelligence should mimic, but not necessarily mirror the software development lifecycle
You’re at least as comfortable with ambiguity as you are with a clearly defined problem
Clear written and verbal communication skills, for both technical and non-technical audiences.
Embodiment of our core company values of motivation, positivity, curiosity, humility and integrity, and buy-in for our company mission of eliminating bias in hiring.
","Using neuroscience-based assessments and machine learning algorithms, pymetrics is reinventing the recruiting industry by matching candidates to jobs and companies where they are most likely to succeed. We are leading the charge in an evolving industry, and growing our amazing team to support the mission of using data to unleash one's full potential and remove bias from hiring.
Are you a hardworking analytics or data engineer who has a curiosity for solving problems and a propensity for exploring data? Do you love understanding people, behavior and believe in making the hiring-process more equitable for everyone?
As our next analytics engineer, you will work within our fast-paced Data Insights & Analytics team to develop and deliver incredible visualization products aimed at making the output of our predictive models both intuitive and actionable.
If this speaks to you and you want to help make hiring more predictive and less biased, we hope you apply!
What you’ll do includes:
Build lasting, scalable solutions to surface data for our suite of external-facing visualization products
Own the analytics layer of our data ecosystem
Support efforts driving machine learning interpretability
Champion automation
Inform product direction using your understanding of the capabilities of our rich datasets
Explore novel use cases for our current tools and future vendors to support both internal and external users
Guide adoption of new vendors, tools and technologies to enrich our analytics stack
Help internal teams define the right metrics to answer the questions at hand
Ensure the right teams and the right people have access to the right data

Requirements

2+ years experience working with data warehouses, data visualization and data analysis
Experience implementing best-practices for data modelling, especially with regards to dimensional modelling for business intelligence. Experience with DBT is a plus.
Experience with SQL-first, self-service visualization tools such as Mode Analytics, Tableau, Looker
High level of comfort with SQL
Proficiency in Python and its scientific libraries (pandas, scipy etc.)
Experience with git and git-based workflows
You understand that analytics and business intelligence should mimic, but not necessarily mirror the software development lifecycle
You’re at least as comfortable with ambiguity as you are with a clearly defined problem
Clear written and verbal communication skills, for both technical and non-technical audiences.
Embodiment of our core company values of motivation, positivity, curiosity, humility and integrity, and buy-in for our company mission of eliminating bias in hiring.
Benefits

Health care plan (medical, dental & vision)
Flexible paid time off
Family leave (maternity, paternity)
401k
Team budget for training & development
Stock option plan
Commuter transportation reimbursement
Dog-friendly workplace
Fun, diverse and intellectually eager coworkers
Finally, we are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
15,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Help us Build the Future of Money
Gemini Trust Company, LLC (Gemini) is a licensed digital asset exchange and custodian. We built the Gemini platform so customers can buy, sell, and store digital assets (e.g., Bitcoin, Ethereum, and Zcash) in a regulated, secure, and compliant manner.


Digital assets and blockchain technology have the power to transform the world for good. This truth, along with our core values, form the bedrock of our company and culture. At Gemini, no job is too small and no project too big as we endeavor to build the future of money. We are a mission-driven, team-based, inclusive, and determined community of thought leaders who invest in each other and the long game. Join us in our mission!

THE DEPARTMENT: DATA ENGINEERING

THE ROLE: DATA ENGINEER

As a member of our data engineering team, you’ll shape the way we approach data at Gemini by using your engineering, analytical and communication skills to work with teams across the business. You know how to ask the right questions and are passionate about using data to support and drive informed business decisions. You are ready to roll up your sleeves and are excited to take on challenging opportunities and projects. You’ll help to build data infrastructure at Gemini for cross-functional teams to leverage the data to improve operational efficiency, products and achieve KPIs. Communicating your insights with leaders across the organization is paramount to success.

RESPONSIBILITIES:

Assist in designing and implementation of best-in-class Data Infrastructure and Business Intelligence solutions
Research new tools and technologies to build new and improve existing processes
Participate in design discussions and meetings
Design, automate, build, and launch scalable, efficient and reliable data pipelines in production
Build data integrations to integrate different data sources into the Data Lake
Assist in building real-time data streaming and reporting solutions
Schedule, monitor and maintain data pipelines using a workflow management tool
Design, build and enhance dimensional models for Data Warehouse and Data Marts
Create test plans, test scripts and build data reconciliation processes
Tune SQL queries, reports and ETL/ELT processes
Partner with engineers, project managers, and analysts to deliver insights to the business
Perform root cause analysis and resolve production and data issues


MINIMUM QUALIFICATIONS:

4+ years experience in data engineering and data warehouse technologies
4+ years experience in custom ETL design, implementation and maintenance
2+ years experience working with AWS cloud technologies
2+ experience working with Airflow or a similar workflow management tool
Skilled in programming languages Python and/or Java
Experience building real-time data streaming solutions using Kafka and/or other tools/technologies
Experience with schema design and dimensional data modeling
Experience with one or more MPP databases(Redshift, Bigquery, Snowflake, etc)
Advanced SQL skills is a must
Experienced in working collaboratively across different teams and departments
Strong technical and business communication


PREFERRED QUALIFICATIONS:

Kafka, Spark, HDFS, Cloud computing experience is a plus
Experience building and integrating web analytics solutions
Experience with one or more ETL tools(dbt, Informatica, Pentaho, SSIS, Alooma, etc)
Experience with continuous integration and deployment
Knowledge and experience of financial markets, banking or exchanges
It Pays to Work Here

We take a holistic approach to compensation at Gemini, which includes:

Competitive base salaries across all departments
Ownership in the company via profit sharing units
Amazing benefits, 401k match contribution, and flexible hours
Snacks, Perks, Wellness Outings & Events


Gemini is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you have a disability or special need that requires accommodation, please let us know."
16,Big-Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description:
Applicant should be at least 8 years of experience in the relevant field (At least 5 years in Bigdata). Must be a hands-on Bigdata developer.
Should have experience in building ETL/ELT pipeline in data technologies like Hadoop, spark, hive, presto, data bricks.
Should have understanding of data warehousing concepts.
Should be able to troubleshoot API integration code.
Should be familiar with Github and other source control tools.
Knowledge of Amazon EC2 would be a plus.
Knowledge of Python, PYspark or any other programming language is a plus.
Red shift or snowflake knowledge is a plus
knowledge of any reporting tool is a plus.
Applicant’s responsibility would be to interface with Business intelligence team and help them build data pipeline to support existing BI platform and data products

What's in it for you?

Excellent benefits plan: medical, dental, vision, life, FSA, & PTO
Roll over vacation days
Commuter benefits
Excellent growth and advancement opportunities
Certification reimbursement
Rewards and recognition programs
Innovative and collaborative company culture

“LTI values diversity and inclusion and is committed to the principles of Equal Employment Opportunity EOE/Minority/Female/Veteran/Disabled/Sexual Orientation/Gender Identity.”"
17,Marquee - Data Engineer,"New York, NY 10282",New York,NY,10282,None Found,None Found,None Found,None Found,None Found,None Found,"MORE ABOUT THIS JOB
The Marquee team at Goldman Sachs is responsible for delivering digital products to our institutional client base. We design and build scalable web platforms that provide access to Goldman Sachs content, portfolio analytics, risk, and execution services. These tools help to transform and simplify client experiences while generating new revenue streams and business models for a leader in global financial markets. Marquee is a product driven team, composed of talented and passionate product managers, designers, and engineers working to change the expectation of institutional finance.

As an engineer within Marquee, you will be working in a close knit team at the forefront of shaping our client's experience using leading open source technologies. Teams develop in an agile environment that is flexible to client request which have need for us to prototype and create high quality tested code.
RESPONSIBILITIES AND QUALIFICATIONS
RESPONSIBILITIES
Design and lead development of a scalable data warehouse containing measurements of client interactions with the Marquee digital platforms via web, mobile and API.
Develop a real-time metrics ingestion pipeline using Kafka.
Develop tools that allow product managers and business sponsors to gain the data insights they need to drive product decisions and strategic direction. This may include either static or dynamic reporting.

WHO WE LOOK FOR
Data engineers with the following qualities:
commitment to understanding the metrics that drive our business
passion for clean, consistent data models
healthy paranoia about data integrity and quality

BASIC QUALIFICATIONS & PREFERRED QUALIFICATIONS
3+ years working with large data warehouses
Demonstrated ability to deliver using some of the following: Python, SQL, Java, Kafka
Experience with digital client engagement analytics
Experience integrating data with tools like Qlikview, Tableau, Jupyter Notebooks
AWS experience preferred

WHY MARQUEE
Work on some of the most complex technical and design challenges in technology and finance
Learn from the foremost experts in finance, technology, and math who are diverse in their academic, ethnic, and social backgrounds
Benefit from ongoing training, development, and mentoring to advance in your career
ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

Â© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet."
18,"Data Strategy Specialist - Business & Data Analysis, Cloud, AWS, Azure, Big Data","New York, NY 10011",New York,NY,10011,None Found,None Found, 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:,None Found,None Found,None Found,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The North America Data Strategy & Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.

 Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model

Required Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:
o Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling
o Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.
o Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel
 Able to travel up to 100% (Mon-Thu)

Optional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model
 Experience in compiling business cases and roadmaps for data, analytics and technology investments

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
19,"Data Engineer, Analytics","New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"OVERVIEW:
GroupM Data & Analytics Services (DAS) is a full-service analytics, data, business, and marketing consultancy. We maximize our clients’ and agencies’ marketing ROI by providing leading edge data management tools, advanced analytics, and consulting to monitor, evaluate, and optimize media investments.

The Data and Analytics organization in GroupM is responsible for managing and analyzing massive amounts of data as well as supporting the whole organization in delivering value from data. We focus on supporting operating our global technology stack with four key layers of Data Marketplace, Data Management, Analytics Workbench and Application Marketplace. These products are hosted on a variety of infrastructure options including but not limited to local servers, public and cloud-based solutions that enable a suit of data-driven media services.
 YOUR IMPACT:
Support development and deployment of GroupM products and services across multiple cloud environments
Collaborate with a cross-functional team of client leads, application developers, operations engineers and architects to translate complex product requirements into technical specs and design requirements
Optimize performance and cost efficiency of cloud based processes across multiple cloud environments (AWS, Azure, GCP)
Maintain a high degree of knowledge in cloud data architecture and ETL best practices especially across Google Cloud Platform products and services
Act as a consultant and subject matter expert for internal stakeholders in GroupM Data & Analytics, GroupM Engineering and agency data science and tech leads
Develop and deploy automated scripts in BigQuery and other Google cloud services to be used by other teams to increase productivity
Facilitate architectural discussions and initiatives to ensure cloud-based products are optimally deployed with maximum availability of design features
Formulate and execute robust UAT protocols to identify and address latent errors in product functions
Design, build and deploy ETL and data management processes with reliable error/exception handling and rollback framework
Provide production support for data load jobs and develop customized query to generate automatic periodic reports
Build applications writing SQL scripts to manipulate data and / or writing specific instructions for an off-shore programmer to write the scripts
 YOUR QUALIFICATIONS:
Essential:
Bachelor’s degree in Computer Science, Engineering, Mathematics or other technical field is highly preferred
5+ years of overall experience with Data Engineering
Minimum 2 years of hands-on experience with Google Cloud Platform
Good communication and written skills
Strong expertise with Data Architecture fundamentals, database design and programming, ETL and custom query development
Experience of the following GCP Services: Cloud Storage, DataProc, Dataflow, CloudSQL, BigQuery
Experience with building data pipes landing large files into GCP for processing, developing/ cleansing data for AI/ML purposes
Experience using the Linux Command Line, especially in conjunction with GCP
Use of Data Profiling Tools, ETL and Data Management Tools
Data warehousing/data modeling experience, with strong understanding of semantic and physical data models
Knowledge of Agile methodology
Experience with the full development life cycle of an application stack - from architecture through test and deployment.
Able to deliver a broad range of data engagements in areas such as Data Architecture, Data Integration, Data Analytics, Data Lineage, Data Governance, Data Quality and BI Reporting
Understanding of data analysis techniques and how they can be applied in the marketing context beneficial
Experience with source code management systems
Desired:
Prior knowledge of advertising ecosystem, understanding of marketing metrics, and analytical products offered as a service is preferred
ABOUT GROUPM:
GroupM is the leading global media investment management operation serving as the parent company to WPP media agencies including Mindshare, MEC, MediaCom, and Maxus, each global operations in their own right with leading market positions. GroupM’s primary purpose is to maximize performance of WPP’s media agencies by operating as leader and collaborator in trading, content creation, sports, digital, finance, proprietary tool development and other business-critical capabilities. GroupM’s focus is to deliver unrivaled marketplace advantage to its clients, stakeholders and people. Discover more about GroupM at www.groupm.com.

Take A Virtual Office Tour: https://roundme.com/tour/368027
GroupM and all its affiliates embrace and celebrate diversity, inclusivity, and equal opportunity. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. We are a worldwide media agency network that represents global clients. The more inclusive we are, the more great work we can create together."
20,"Senior Engineer, Data Engineering","New York, NY",New York,NY,None Found,None Found,"
Bachelor’s degree in Computer Science or related field
3+ years of software development experience, as a developer
Fluency in Scala and/or Java programming languages
Strong OO & FP design patterns, data structure, and algorithm design skills
Extensive experience developing Apache Spark applications
2+ years of experience with both relational database design (SQL), non-relational (NoSQL) databases, big data, real-time technologies
Familiar with various cloud data sources and architectures such as AWS/S3, HDFS, Kafka
Experience with software containerization, such as Docker
Experience developing and/or consuming web interfaces (REST API) and associated skills (HTTP, web services)
Self-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment",None Found,"
Serve as a senior data engineer for AdSmart products.
Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and leadership
Develop and code the software components that are core to Audience Studio, under the leadership of the VP/Chief Architecture
Support product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.
Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership",None Found,"
 Interested candidate must submit a resume/CV through www.nbcunicareers.com to be considered
Must be willing to work in New York, NY","As a member of the Data Engineering Team, the Senior, Data Engineer, AdSmart will be directly responsible for the design, management, and development of parts of the software platform and products for NBCUniversal’s AdSmart. NBCUniversal’s AdSmart products will enable NBCUniversal to better understand it's brand’s audiences such as NBC News, Bravo, The Tonight Show, Saturday Night Live, and USA Network as well as audiences that cross brands. The goal is to ensure we know who is watching what, where and when. In turn, enabling NBCUniversal’s sales teams to properly align our audiences with the market advertisements that can benefit them the most.

You’re a big thinker who can analyze and evangelize a long-range opportunity, architect a groundbreaking solution, and roll-up your sleeves to get code out the door when needed. You are data-driven and analytical. You understand the concept of a value proposition and evaluation criteria, and you know how to align them with low-level milestones to get the work done. You can apply domain knowledge from one technical subject, in order to quickly ramp and deliver on a new one. You know how to learn from failure until you succeed, and you are able to articulate and quantify the reasons for your decisions.

You will be part of the AdSmart Data Engineering team, participating in the data architecture that will drive both current and future data management initiatives within NBCUniversal’s AdSmart group.

Responsibilities:
Serve as a senior data engineer for AdSmart products.
Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and leadership
Develop and code the software components that are core to Audience Studio, under the leadership of the VP/Chief Architecture
Support product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.
Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership

Qualifications:
Bachelor’s degree in Computer Science or related field
3+ years of software development experience, as a developer
Fluency in Scala and/or Java programming languages
Strong OO & FP design patterns, data structure, and algorithm design skills
Extensive experience developing Apache Spark applications
2+ years of experience with both relational database design (SQL), non-relational (NoSQL) databases, big data, real-time technologies
Familiar with various cloud data sources and architectures such as AWS/S3, HDFS, Kafka
Experience with software containerization, such as Docker
Experience developing and/or consuming web interfaces (REST API) and associated skills (HTTP, web services)
Self-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment

Additional Job Requirements:
 Interested candidate must submit a resume/CV through www.nbcunicareers.com to be considered
Must be willing to work in New York, NY

Desired Requirements:
Experience as a development manager (with direct authority over development staff)
Experience with Cluster Management and Container Orchestration technologies such as Mesos, Kubernetes, Hadoop/Yarn
Experience with Apache Kafka or similar streaming technologies
Experience with digital advertising technologies.
Able and eager to learn new technologies
Able to easily transition between high-level strategy and day-to-day implementation
Excellent teamwork and collaboration skills
Results-oriented, high energy, self-motivated
Sub-BusinessTechnology
Career Level
Experienced
CityNew York
State/Province
New York
CountryUnited States
About Us
At NBCUniversal, we believe in the talent of our people. It’s our passion and commitment to excellence that drives NBCU’s vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It’s what makes us uniquely NBCU. Here you can create the extraordinary. Join us.
Notices
NBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable."
21,Engineer - Data,"New York, NY",New York,NY,None Found,None Found,"
Master's or bachelor's degree in Computer Science
At least 5 years of hands-on software development experience in Python, Golang, Java, C++ or Scala
Strong Object Oriented Programming skills
Deep knowledge in data structures, algorithms, and software design
Experience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources
Highly proficient with relational and non-relational data storages
Strong verbal and written communication skills",None Found,"Work with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs
Adheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time
Collaborates with code reviews, architecture discussion and bug fixing
Demonstrates code and product ownership in production
Support the business teams and product managers in data extracts and data analysis
Performs as a true agile team member and exhibits competencies in all layers of the application stack. Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration",None Found,None Found,"Lucid is a market research platform that provides access to authentic, first-party data in over 90 countries. Our products and services enable anyone, in any industry, to ask questions of targeted audiences and find the answers they need – fast. These answers can be used to uncover consumer motivations, increase revenue, and measure the impact of digital advertising. Founded in 2010, Lucid is headquartered in New Orleans, LA with offices in Dallas, New York, London, Sydney, Singapore, Gurgaon, Prague, and Hamburg.


Apply for this Job

The Opportunity

The Data Engineer will be part of our development team and work closely with development team peers, Product Management team, and business and other support teams.
Responsibilities
Work with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs
Adheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time
Collaborates with code reviews, architecture discussion and bug fixing
Demonstrates code and product ownership in production
Support the business teams and product managers in data extracts and data analysis
Performs as a true agile team member and exhibits competencies in all layers of the application stack. Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration
Qualifications
Master's or bachelor's degree in Computer Science
At least 5 years of hands-on software development experience in Python, Golang, Java, C++ or Scala
Strong Object Oriented Programming skills
Deep knowledge in data structures, algorithms, and software design
Experience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources
Highly proficient with relational and non-relational data storages
Strong verbal and written communication skills
Preferred Qualifications
AWS experience with S3, RDS, Redshift, EMR, Kinesis, Lambda, Elastic Beanstalk and Elasticsearch
Experience with Spark, Hadoop and Hive on EMR and non-managed. Can build and run a basic cluster
Experience developing ETLs and running job schedulers (e.g. Airflow)
Experience with PostgreSQL
Experience with NoSQL data sources including Cassandra
Experience with Kafka, Redis
Experience creating Data Lakes


At Lucid we foster a collaborative and inspiring workplace. We pride ourselves in doing this by recruiting, hiring and retaining diverse, passionate, and forward-thinking talent. Lucid is committed to and encourages an inclusive environment and we are dedicated to providing equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please let us know."
22,AI Data Scientist / Data Engineer - Experienced Associate,"New York, NY 10017",New York,NY,10017,None Found,"
Exploring new analytical technologies and evaluating their technical and commercial viability quickly;
Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;
Testing and rejecting hypotheses around data processing and machine learning model building;
Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;
Building machine learning pipelines that ingest, clean data, and make predictions;
Staying abreast of new AI research from leading labs by reading papers and experimenting with code;
Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;
Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;
Applying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);
Understanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;
Building machine learning models and systems, interpreting their output, and communicating the results;
Moving models from development to production is a plus; and,
Conducting research in a lab and publishing work is a plus.","
Exploring new analytical technologies and evaluating their technical and commercial viability quickly;
Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;
Testing and rejecting hypotheses around data processing and machine learning model building;
Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;
Building machine learning pipelines that ingest, clean data, and make predictions;
Staying abreast of new AI research from leading labs by reading papers and experimenting with code;
Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;
Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;
Applying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);
Understanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;
Building machine learning models and systems, interpreting their output, and communicating the results;
Moving models from development to production is a plus; and,
Conducting research in a lab and publishing work is a plus.",None Found,None Found,"
Exploring new analytical technologies and evaluating their technical and commercial viability quickly;
Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;
Testing and rejecting hypotheses around data processing and machine learning model building;
Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;
Building machine learning pipelines that ingest, clean data, and make predictions;
Staying abreast of new AI research from leading labs by reading papers and experimenting with code;
Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;
Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;
Applying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);
Understanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;
Building machine learning models and systems, interpreting their output, and communicating the results;
Moving models from development to production is a plus; and,
Conducting research in a lab and publishing work is a plus.","PwC Labs is focused on standardizing, automating, delivering tools and processes and exploring emerging technologies that drive efficiency and enable our people to reimagine the possible. Process improvement, transformation, effective use of innovative technology and data & analytics, and leveraging alternative delivery solutions are key areas of focus to drive additional value for our firm. The AI Lab focuses on implementing solutions that impact efficiency and effectiveness of our technology functions. Process improvement, transformation, effective use of technology and data & analytics, and leveraging alternative delivery are key areas to drive value and continue to be recognized as the leading professional services firm. AI Lab is focused on identifying and prioritizing emerging technologies to get the most out of our investments.

To really stand out and make us ?t for the future in a constantly changing world, each and every one of us at PwC needs to be an authentic and inclusive leader, at all grades/levels and in all lines of service. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.

As an Associate, you’ll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:

Invite and provide evidence-based feedback in a timely and constructive manner.Share and collaborate effectively with others.Work with existing processes/systems whilst making constructive suggestions for improvements.Validate data and analysis for accuracy and relevance.Follow risk management and compliance procedures.Keep up-to-date with technical developments for business area.
- Communicate confidently in a clear, concise and articulate manner - verbally and in written form.
Seek opportunities to learn about other cultures and other parts of the business across the Network of PwC firms.Uphold the firm’s code of ethics and business conduct.

Our team is capability centric, focusing on AI and machine learning techniques that are broadly applicable across all industries. We work with a variety of data mediums including text, audio, imagery, sensory, and structured data. Our work involves the use of supervised/unsupervised machine learning algorithms, traditional statistical models, deep neural networks, terabyte scale data, and simulation modelling. Our work is having a tremendous impact on how PwC & our clients do business.
Job Requirements and Preferences:

Basic Qualifications:

Minimum Degree Required:
Bachelor Degree

Minimum Years of Experience:
1 year(s)

Preferred Qualifications:

Preferred Fields of Study:
Computer and Information Science, Computer Engineering, Computer and Information Science & Accounting, Economics, Economics and Finance, Economics and Finance & Technology, Engineering, Mathematics, Mathematical Statistics, Statistics

Preferred Knowledge/Skills:
Demonstrates some knowledge and/or a proven record of success in the following areas:
Exploring new analytical technologies and evaluating their technical and commercial viability quickly;
Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;
Testing and rejecting hypotheses around data processing and machine learning model building;
Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;
Building machine learning pipelines that ingest, clean data, and make predictions;
Staying abreast of new AI research from leading labs by reading papers and experimenting with code;
Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;
Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;
Applying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);
Understanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;
Building machine learning models and systems, interpreting their output, and communicating the results;
Moving models from development to production is a plus; and,
Conducting research in a lab and publishing work is a plus.
Demonstrates some abilities and/or a proven record of success learning and applying new skills quickly, including the following areas and technologies:
Programming: Python, R, Java, JavaScript, C++, Unix;
Hardware: sensors, robotics, GPU enabled machine learning, FPGAs, Raspberry Pis, etc.;
Data Storage Technologies: SQL, NoSQL, Hadoop, cloud-based databases such as GCP BigQuery, and different storage formats (e.g. Parquet, etc.);
Data Processing Tools: Python (Numpy, Pandas, etc.), Spark, cloud-based solutions such as GCP DataFlow;
Machine Learning Libraries: Python (scikit-learn, genism, etc.), TensorFlow, Keras, PyTorch, Spark MLlib;
Visualization: Python (Matplotlib, Seaborn, bokeh, etc.), JavaScript (d3); and,
Productionization and containerization technologies: GitHub, Flask, Docker, Kubernetes.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer."
23,Senior Data Engineer,"New York, NY 10011",New York,NY,10011,None Found,"
Commercial experience leading on client-facing projects, including working in close-knit teams
5+ years of experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)
5+ years of experience working on projects within the cloud ideally AWS or Azure
5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent
Experience with open source tools like Apache Airflow and Griffin
Experience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform
Data Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift
Experience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models
Experience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma
Experience building automated data quality and testing into data pipelines
Experience with AI, NLP, Machine Learning, etc. is a plus
Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R
Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time
Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.
A deep personal motivation to always produce outstanding work for your clients and colleagues
Excel in team collaboration and working with others from diverse skill-sets and backgrounds",None Found,None Found,None Found,None Found,"Summary:

You have experience with client projects and in handling vast amounts of data – working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Data Science team. Lead on projects from a data engineering perspective, working with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches
Plan and execute secure, good practice data integration strategies and approaches
Acquire, ingest, and process data from multiple sources and systems into Big Data platforms
Create and manage data environments in the Cloud
Collaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models
Have a strong understanding of Information Security principles to ensure compliant handling and management of client data
This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science
Qualifications:
Commercial experience leading on client-facing projects, including working in close-knit teams
5+ years of experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)
5+ years of experience working on projects within the cloud ideally AWS or Azure
5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent
Experience with open source tools like Apache Airflow and Griffin
Experience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform
Data Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift
Experience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models
Experience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma
Experience building automated data quality and testing into data pipelines
Experience with AI, NLP, Machine Learning, etc. is a plus
Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R
Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time
Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.
A deep personal motivation to always produce outstanding work for your clients and colleagues
Excel in team collaboration and working with others from diverse skill-sets and backgrounds
Cervello is a dynamic technology company that is focused on business analytics and planning. We take an innovative approach to making complex solutions simple so our clients can focus on running their businesses. Our services and applications enable our clients to gain the benefits of a world-class analytics and planning capability without the headaches.

M4ahUGavPU"
24,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"• Hands-on experience in Azure Data Factory, Informatica and good understanding on data obfuscation or data masking techniques • Design, construct, install, test and maintain highly scalable data management systems • Build automated data delivery pipelines and services to integrate data • Build and deliver cloud-based deployment and monitoring capabilities consistent with DevOps models • Develop solutions in agile environment for the overall data domain • Deep experience with developing SQL • Must have Deep experience developing with MS SQL • Must be able to do ETL (SSIS, Azure Data Factory, Informatica) • Understand Data Security"
25,Data Engineer,"New York, NY 10018",New York,NY,10018,None Found,"THE PERSON
Hands on research and development
Find innovative solutions to difficult and unstructured problems that will support and expand our core business
Use analytical rigor to analyze large amounts of data, help extract actionable insights using data analysis, feature engineering, optimization tools and machine learning techniques
Develop and maintain scalable and reliable data pipelines for AI/ML processes
Participate in all aspects of Teach For America’s agile software development cycle
Other duties (research, presentations, communicating with other teams)

THE MUST HAVES
Prior Experience:
Intermediate or higher experience working with machine learning tools in Python (TensorFlow, Keras, fast.ai, etc)
Experience working with data solutions (data ingestion, preprocessing, analysis, predictive analytics)
Experience with Reporting and Advanced Analytics Solutions
Intermediate or higher experience working with Java (Spring, REST, JMS) and SQL
Comfortable working in a Unix environment
Experience with version control, containerization technologies
Experience working with Continuous Integration / Automation architectures
Experience interfacing and working with engineering teams throughout the product development lifecycle (leading projects and/or other developers)
Experience partnering with other teams to test and roll out cognitive & predictive analytics solutions
Optional but desired:
Experience developing applications for and deploying applications in cloud environments
Experience with big data and associated technologies (Hadoop, Databricks, Azure Machine Learning etc.)
Experience with statistical modeling in R
Skills:
Be able to communicate in a clear and effective manner with both technical and non-technical audiences
Be interested in staying up-to-date with recent advances in machine learning and predictive analytics
Be detail oriented, able to work under pressure and effectively manage competing priorities
Education:
At least a four-year degree in Computer Science, BSEE, MIS or a related field, or equivalent experience
Work Demands:
This position is located on site in our New York National Office
Limited travel may be required

THE TEAM
Our team loves to collaborate. We partner with every other team in the organization to create world-class technology solutions that staff and corps members use to more effectively and efficiently get all kids access to educational opportunity. Our team works very hard, but we also have a lot of fun. We enjoy game nights, quarterly trivia outings, and themed potlucks where we get together to eat and explore each other's cultures and favorite recipes.

THE PERKS
By joining staff, you join a network of individuals committed to pursuing equity for all students and developing themselves as professionals in the process. We as an organization value the longevity of our employees and offer a comprehensive and competitive benefits plan. The salary for this position is also competitive and depends on your prior work experience. Please be advised, you will have an opportunity to discuss salary in more detail after you begin the application process.

WE ARE DEEPLY COMMITTED TO DIVERSITY, EQUITY & INCLUSIVENESS
Teach For America encourages individuals of all ethnic, racial, and socioeconomic backgrounds to apply for this position. We are committed to maximizing the diversity of our organization, as we want to engage all those who can contribute to this effort.

Teach For America is committed to providing equal employment opportunities to all qualified individuals and does not discriminate on the basis of race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, parental status, genetic information or characteristics (or those of a family member) or any other basis prohibited by applicable law.

This job description reflects Teach For America's assignment of essential functions and qualifications of the role. Nothing in this herein restricts management's right to assign, reassign or eliminate duties and responsibilities to this role at any time.

NEXT STEPS
Interested in this position? Apply now! Scroll down to the bottom of the page to find the link to the online application. If you still have questions regarding the role, feel free to contact our recruitment team at staffing@teachforamerica.org or visit www.teachforamerica.org/about-us/careers.",None Found,None Found,None Found,None Found,"TEAM: Information Technology
REPORTS TO: MD, AI & Cognitive Services
LOCATION: New York, NY

THE ROLE
Are you passionate about Artificial Intelligence/Machine Learning, have coding experience and are looking for more progression & autonomy in your career? We are seeking an AI/ML Engineer to collaborate with some of tech’s and analytics’ sharpest minds to solve the firm’s ever-changing but exciting challenges and explore unique technical solutions. Working in tech here at Teach for America means that you’ll always be presented with a variety of new possibilities as you continue to enhance your skills and contribute to our mission.

THE ORGANIZATION
There are more than 16 million children growing up in poverty in the U.S., and less than 10 percent of them will graduate from college. These statistics are not a reflection of our children’s potential; we know that children growing up in poverty can and do achieve at the highest levels. Rather, these statistics reflect the systemic lack of access and opportunity for children in low-income communities.

Teach For America’s (TFA) mission is to find, develop, and support a diverse network of leaders committed to expanding opportunity for children from classrooms, schools, and every sector and field that shapes the broader systems in which schools operate. We are seeking individuals who align with our mission, core values and commitment to Diversity Equity & Inclusiveness and are ready to join us in this global movement.


Qualifications:
THE PERSON
Hands on research and development
Find innovative solutions to difficult and unstructured problems that will support and expand our core business
Use analytical rigor to analyze large amounts of data, help extract actionable insights using data analysis, feature engineering, optimization tools and machine learning techniques
Develop and maintain scalable and reliable data pipelines for AI/ML processes
Participate in all aspects of Teach For America’s agile software development cycle
Other duties (research, presentations, communicating with other teams)

THE MUST HAVES
Prior Experience:
Intermediate or higher experience working with machine learning tools in Python (TensorFlow, Keras, fast.ai, etc)
Experience working with data solutions (data ingestion, preprocessing, analysis, predictive analytics)
Experience with Reporting and Advanced Analytics Solutions
Intermediate or higher experience working with Java (Spring, REST, JMS) and SQL
Comfortable working in a Unix environment
Experience with version control, containerization technologies
Experience working with Continuous Integration / Automation architectures
Experience interfacing and working with engineering teams throughout the product development lifecycle (leading projects and/or other developers)
Experience partnering with other teams to test and roll out cognitive & predictive analytics solutions
Optional but desired:
Experience developing applications for and deploying applications in cloud environments
Experience with big data and associated technologies (Hadoop, Databricks, Azure Machine Learning etc.)
Experience with statistical modeling in R
Skills:
Be able to communicate in a clear and effective manner with both technical and non-technical audiences
Be interested in staying up-to-date with recent advances in machine learning and predictive analytics
Be detail oriented, able to work under pressure and effectively manage competing priorities
Education:
At least a four-year degree in Computer Science, BSEE, MIS or a related field, or equivalent experience
Work Demands:
This position is located on site in our New York National Office
Limited travel may be required

THE TEAM
Our team loves to collaborate. We partner with every other team in the organization to create world-class technology solutions that staff and corps members use to more effectively and efficiently get all kids access to educational opportunity. Our team works very hard, but we also have a lot of fun. We enjoy game nights, quarterly trivia outings, and themed potlucks where we get together to eat and explore each other's cultures and favorite recipes.

THE PERKS
By joining staff, you join a network of individuals committed to pursuing equity for all students and developing themselves as professionals in the process. We as an organization value the longevity of our employees and offer a comprehensive and competitive benefits plan. The salary for this position is also competitive and depends on your prior work experience. Please be advised, you will have an opportunity to discuss salary in more detail after you begin the application process.

WE ARE DEEPLY COMMITTED TO DIVERSITY, EQUITY & INCLUSIVENESS
Teach For America encourages individuals of all ethnic, racial, and socioeconomic backgrounds to apply for this position. We are committed to maximizing the diversity of our organization, as we want to engage all those who can contribute to this effort.

Teach For America is committed to providing equal employment opportunities to all qualified individuals and does not discriminate on the basis of race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, parental status, genetic information or characteristics (or those of a family member) or any other basis prohibited by applicable law.

This job description reflects Teach For America's assignment of essential functions and qualifications of the role. Nothing in this herein restricts management's right to assign, reassign or eliminate duties and responsibilities to this role at any time.

NEXT STEPS
Interested in this position? Apply now! Scroll down to the bottom of the page to find the link to the online application. If you still have questions regarding the role, feel free to contact our recruitment team at staffing@teachforamerica.org or visit www.teachforamerica.org/about-us/careers.

Share||"
26,Data Engineer,"New York, NY 10036",New York,NY,10036,None Found,None Found,None Found,None Found,None Found,None Found,"Short Description
About Capgemini

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50-year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion (about $15.6 billion USD at 2018 average rate).
Visit us at www.capgemini.com. People matter, results count.

Title: Python Developer

Expertise in converting SAS code to Python
MAIN Expertise in adding efficiency to models
Knowledge of writing a modular code
Advanced Python R not necessary o
Very strong test driven programming skills o
Strong object oriented programming knowledge o
Software integration API design and integration
DB connection performance tuning exception
Handling Advanced Spark ETL PySpark API rather than Scala SQL Hive Presto AWS packages boto troposphere to be able to script automated full workflows pipelines

Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.

Click the following link for more information on your rights as an Applicant: http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law"
27,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,"
Manage competing priorities across the company
Maintain and automate reporting infrastructure
Manage the design and architecture of our Data Warehouse
Create Scripts to automate and manage ETL processes and Dependencies
Advise on the design of our application DB, machine learning components, and our data infrastructure.
Cleaning and restructuring datasets
Managing and optimizing reporting systems",None Found,"
At least two years experience as a Data Engineer, or related Software or DevOps experience
Fluent in Python and SQL
Experience with Data Warehouses and Schema Design
Strong communication skills and experience communicating across business units
Experience working with Data Scientists and Data Analysts
Ability to create fast solutions to problems introduced in a changing environment with iteration towards optimal solutions
Machine Learning experience a plus
Analytical and BI skills a plus
Postgres and RedShift experience is a plus","Who We Are:
Ocrolus is a Series B venture-backed FinTech company that uses Artificial Intelligence and crowdsourcing to automate financial review processes. The Company transforms e-statements, scans, and cell phone images, regardless of quality, into 99+% accurate digital data. By replacing tedious, imperfect human audits with sharp, AI-driven analyses, Ocrolus modernizes financial assessments in lending and a variety of other industries.





We are seeking a candidate with proven experience working as a Data Engineer, Full Stack Software Engineer with a Data focus, or similar role. The ideal candidate is comfortable in DevOps and Software Engineering.
Responsibilities
Manage competing priorities across the company
Maintain and automate reporting infrastructure
Manage the design and architecture of our Data Warehouse
Create Scripts to automate and manage ETL processes and Dependencies
Advise on the design of our application DB, machine learning components, and our data infrastructure.
Cleaning and restructuring datasets
Managing and optimizing reporting systems
Requirements
At least two years experience as a Data Engineer, or related Software or DevOps experience
Fluent in Python and SQL
Experience with Data Warehouses and Schema Design
Strong communication skills and experience communicating across business units
Experience working with Data Scientists and Data Analysts
Ability to create fast solutions to problems introduced in a changing environment with iteration towards optimal solutions
Machine Learning experience a plus
Analytical and BI skills a plus
Postgres and RedShift experience is a plus

We’re a young and rapidly growing FinTech company - if you have ever wanted to jump on a rocket ship as it’s taking off, now is your chance!"
28,GCP Data Engineer,"New York, NY 10001",New York,NY,10001,None Found,"
5+ years of experience consulting in Data Engineering or Data Warehousing
Hands-on experience with Google Cloud Platform
Experience leading data warehousing, data ingestion, and data profiling activities
Advanced SQL & Python skills
Hands-on experience with Google cloud platform technologies: Google Cloud Platform Pub/Sub, Cloud Functions, DataFlow, DataProc (Hadoop, Spark, Hive), Cloud Machine Learning, Cloud Data Store and BigTable, BigQuery, DataLab, and DataStudio
Migrating Data Pipelines to Google Cloud Platform (GCP)
",None Found,"
Build and Deploy Data Pipelines on Google Cloud to enable AI & ML capabilities.
Drive the development of cloud-based and hybrid data warehouses & business intelligence platforms
Build Data Pipelines to ingest structured and Unstructured Data.
Gain hands-on experience with new data platforms and programming languages
",None Found,None Found,"Position: GCP Data Engineer
Location: New York, United States
Remuneration: $ 115.00 per hour
Who is hiring?
One of our Clients in Austin, TX will be hiring for a GCP Data Engineer to assist in building a recommendation software platform utilizing GCP as the main platform for their Data Visualization solution. This role will work closely with their Cloud Architect and Software Development teams thorughout the engagement and may require up to 25% - 50% travel.
What will you be doing?

Responsibilities will include:

Build and Deploy Data Pipelines on Google Cloud to enable AI & ML capabilities.
Drive the development of cloud-based and hybrid data warehouses & business intelligence platforms
Build Data Pipelines to ingest structured and Unstructured Data.
Gain hands-on experience with new data platforms and programming languages

Qualifications for This Role:

5+ years of experience consulting in Data Engineering or Data Warehousing
Hands-on experience with Google Cloud Platform
Experience leading data warehousing, data ingestion, and data profiling activities
Advanced SQL & Python skills
Hands-on experience with Google cloud platform technologies: Google Cloud Platform Pub/Sub, Cloud Functions, DataFlow, DataProc (Hadoop, Spark, Hive), Cloud Machine Learning, Cloud Data Store and BigTable, BigQuery, DataLab, and DataStudio
Migrating Data Pipelines to Google Cloud Platform (GCP)

Why you shouldn’t miss this opportunity?
As a GCP Data Engineer through Third Republic (Recruitment Agency), you will work in teams to deliver innovative solutions on Google Cloud using core cloud data warehouse tools like Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in the advertisement space.
Data Science(Data Engineer), Google Cloud Platform (GCP), Data Engineer"
29,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Opportunity Overview
OVERALL SUMMARY
The Business Technology Services (BTS) team within the Media Technology & Information Services (MTIS) group at HBO is responsible for the build and support of vital technology solutions supporting the scheduling, acquisition, processing, and distribution of HBO content. The team is a fundamental part of HBO’s innovation and successful Technology team in New York City, Santa Monica and Seattle. The team partners with various business units to enable the services required to deliver HBO’s premium content to domestic and global platforms as well as third party partners by bringing together platform, software, data and business intelligence services. The team is looking to add a Senior Data Engineer to be part of a team dedicated to breaking the norm and pushing the limits of continuous improvement and innovation. The individual will be involved in detailed technical design, development and implementation of applications using Amazon AWS platform. Working within an agile environment, the Senior Data Engineer will provide input into architectural design decisions, develop code to meet business needs and ensure the applications we build are meeting high standards of quality and supportability. The individual will have the opportunity to develop his/her technical knowledge and skills while evolving the overall technical maturity of the team. This position will be based in Seattle.
PRIMARY RESPONSIBILITIES
Partner with architects and business leaders to design and build robust services using streaming and batch data.
Key contributor in building identity services that will enable HBO to share profiles across the organization in support of marketing and analytics.
Work independently and part of teams that will ingest data from a variety of source types including viewership, behavioral, attribution, content metadata etc.
Structure and munge ingested data in support of various use cases including analytics, marketing execution and cross divisional data sharing.
REQUIREMENTS
Bachelor’s degree in Computer Science, Computer Engineering, or equivalent
2-3 years of experience in the AWS Cloud environment.
3-5 years of experience using Python and SQL.
4-6 years of experience working with various database methodologies such relational, columnar, NoSQL.
Experience with developing and maintaining production data pipelines
Nice to have: 1 year of experience working on Snowflake Cloud Datawarehouse on AWS.
About Us
It's HBOSM
America's most successful premium television company, Home Box Office delivers two 24-hour pay television services—HBO®and Cinemax®. HBO continues to take advantage of the latest technological innovations with advancements that include the availability of HBO programming online though HBO GOSM and MAX GOSM, as well as HBO On Demand® and Cinemax On Demand® in HD. Just as HBO is a company noted for its commitment to excellence in the products and services it delivers to consumers, it makes the extra effort to create a work environment in which fairness, equity, trust, and individual responsibility are valued. HBO is committed to retaining and recruiting skilled and motivated employees, placing a priority on qualified team players who contribute to the diversity of their workforce. HBO offers competitive benefits to include medical, dental, vision, a matched 401(k) plan, flexible spending, a commuter benefit program and tuition reimbursement.
HBO is an equal employment opportunity employer. HBO does not discriminate against any applicant or employee based on race, color, religion, national origin, gender, age, sexual orientation, gender identity or expression, marital status, mental or physical disability, and genetic information, or any other basis protected by applicable law. HBO also prohibits harassment of applicants or employees based on any of these protected categories."
30,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Who You Are:
Getty Images is looking for a Data Engineer who enjoys working across the entire lifecycle of Machine Learning projects and takes pride in deploying high-quality ML and data workflows.
The mission of the Data Science team at Getty Images Inc. is to leverage internal and third-party data to inform other groups on how to interact with its customer base. We achieve this goal by 1) building automated solutions that apply best-in-class Machine Learning and Engineering practices and 2) continuous interactions with stakeholders to identify critical needs that deliver results relevant to the business.
As a Data Engineer on the Data Science team, you will have end-to-end autonomy and ownership of your projects, and will work closely with other business units to develop creative solutions to a variety of problems.

Your Next Challenge:
You will join a team of highly-collaborative and curious Data Scientists and Data Analysts that are comfortable working with a diverse set of tools, and willing to take initiative on their ideas. As a member of the team, you will have the chance to define the technical architecture that serves as the foundation for upstream analytical projects, and accelerate the delivery of a robust portfolio of Data Science models.
Your primary goal will be to catalyze the development and deployment of full-stack Machine Learning pipelines. You will have the opportunity to continuously develop and ship code in our production environment, and will be empowered to implement a variety of data-centric architectures that support critical operational initiatives.
You will also interact with the entirety of Getty Images Inc. technology stack, and collaborate with data infrastructure, platform and cloud engineers to design and build a production-level data ecosystem that aligns with business function requirements and capable of handling large-scale structured and unstructured data. You will also have the opportunity to continuously evaluate and provide guidance on the use of new technologies.
We value learning and development, and you will be given every opportunity to work on projects that excite you. You will get to lead and innovate as a thought-leader within Getty Images, and will sit at the intersection of Engineering, Marketing and Leadership to inform, influence, support, and execute on our decisions.

What You'll Need:
You have prior experience working as a Data Engineer, preferably in a product or customer-focused organization.
You are extremely comfortable working with Python and have a working knowledge of Cloud services and Tools, as well as standard engineering tools such as Git, Linux and SQL.
You have experience building streaming and batch data pipelines and are comfortable working within a large-scale distributed environment with open source tools such as Hadoop, Hive, Airflow and Spark.
You can independently execute on a project, from ideation to delivery to stakeholders, and can pro-actively interact with other engineers at Getty Images to access necessary resources or data.
You understand, or have interest learning about, the real-world advantages and drawbacks of various Machine Learning techniques, and have applied those to a variety of datasets.
Nice to Have:
A M.S. or Ph.D. in computer science, statistics, economics/econometrics, natural science or any other equivalent quantitative project is preferred. If you are self-taught and believe you are a good fit for this role, or have significant work experience, we would love to hear from you as well.
Previous experience in an analytical role, or experience working with teams of Data Scientists and Data Analysts.
Experience having managed or contributed to the use of Business Intelligence platforms.

#LI-MM1

Who We Are:

Getty Images is the most trusted and esteemed source of visual content, with over 200 million assets available through its industry-leading sites www.gettyimages.com and www.istockphoto.com. The Getty Images website serves creative, business and media customers in almost every country in the world and is the first place people turn to discover, purchase and share powerful content from the world's best photographers and videographers. Getty Images works with over 200,000 contributors and hundreds of image partners to provide comprehensive coverage of more than 160,000 news, sport and entertainment events, impactful creative imagery to communicate any commercial concept and the world's deepest digital archive of historic photography.
For company news and announcements, visit our Press Room. Find iStock on Facebook, Twitter, Instagram and LinkedIn, or download the iStock app where you can easily search, save and share superior images to create standout visual communications.

Getty Images is an equal opportunity employer and strongly supports diversity in the workplace."
31,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"MongoDB is growing rapidly and seeking a Data Engineer to be a key contributor to the overall internal data platform at MongoDB. You will build data driven solutions to help drive MongoDBs growth as a product and as a company. You will take on complex data-related problems using very diverse data sets.

Who?

You have experience with:

several programming languages (Python, Scala, Java, etc..)
data processing frameworks like Spark
streaming data processing frameworks like Kafka, KSQ, and Spark Streaming
a diverse set of databases like MongoDB, Cassandra, Redshift, Postgres, etc.
different storage format like Parquet, Avro, Arrow, and JSON
AWS services such as EMR, Lambda, S3, Athena, Glue, IAM, RDS, etc.
orchestration tools such as Airflow, Luiji, Azkaban, Cask, etc.
Git and Github
CI/CD Pipelines

Also


Enjoy wrangling huge amounts of data and exploring new data sets
Value code simplicity and performance
Obsess over data: everything needs to be accounted for and be thoroughly tested
Plan effective data storage, security, sharing and publishing within the organization
Are constantly thinking of ways to squeeze better performance out of the pipelines

Bonus Points


You are deeply familiar with Spark and/or Hive
You have expert experience with Airflow
Understand the differences between different storage format like Parquet, Avro, Arrow, and JSON
Understand the tradeoffs between different schema designs like normalization vs denormalization
In addition to data pipelines, you're also quite good with Kubernetes, Drone, and Terraform
You've built end to end production grade data solutions that run on AWS
Have experience building ML pipelines using tools likeSparkML, Tensorflow, Scikit-Learn, etc.

What?

As a Data Engineer, you will:

Build large-scale batch and real-time data pipelines with data processing frameworks like Spark on AWS
Help drive best practices in continuous integration and delivery
Help drive optimization, testing and tooling to improve data quality
Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day
MongoDB, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws*

"
32,EDL Big Data Engineer - Corporate- Information Technology (No Agencies),"New York, NY 10022",New York,NY,10022,None Found, A minimum of 5 years of hands-on technical experience with:,None Found, A minimum of 5 years of hands-on technical experience with:,None Found, A minimum of 5 years of hands-on technical experience with:,"Description


Position:
We are looking for an accomplished big data developer with strong experience in the cloud AWS data implementation to help us build and integrate data-driven intelligent cloud solutions for EDL. This role will involve a close collaboration with our team of passionate and innovative big data specialists, application developers and product managers.

This is a unique opportunity to be a member of our corporate CRM and Analytics Team, tackling our toughest and most exiting data lake challenges across multiple divisions in Jefferies.

CRM & Analytics Team Overview:

The CRM & Analytics team is a highly strategic and cross-functional team responsible for leading the firm’s global digitalization effort. This initiative, spanning all client-facing business units and corporate functions, will drive innovation and strategic change through technology, data science, and deep analytics. The team partners with key business leaders and industry experts to build transformational technology to drive revenue, maximize efficiency, and optimize the allocation of resources. The CRM & Analytics team is at the forefront of Jefferies’ cloud initiative, leveraging best-in-class cloud-based technologies to replace legacy on-premises solutions to provide intelligent trend insights, actionable opportunities, decision support, and transparency into all client and business-related activities. This team is also responsible for Enterprise Data Lake.

Position Overview:
We are looking for an accomplished big data architect with strong experience in the cloud AWS data architecture and implementation.
This role will involve a close collaboration with our team of passionate and innovative big data specialists, application developers and product managers.
This is a unique opportunity to be a member of our corporate CRM and Analytics Team, tackling our toughest and most exiting data lake challenges across multiple divisions in Jefferies.


Enterprise Data Lake (EDL) Team Responsibilities:
The EDL team will oversee and support architecture and implementation of EDL for all Jefferies big data initiatives. It will drive the data governance and facilitate data onboarding. It will approve the design of data and software architecture, perform architecture review to pass EDL tollgates, evaluate and select cloud/AWS/Big Data tools for acceptance, and serve as a vendor liaison with data lake tool vendors and out internal infrastructure teams. Also, it will certify data for consumption, EDL patterns and processes, manage and govern data access controls, and will manage data lake and data governance training initiatives across enterprise.

The EDL team will become the center of excellence for the following EDL components and associated tools:
q EDL architecture and patterns
q EDL Data Stores
q EDL Governance
q EDL Data Discovery
q EDL Data Preparation
q EDL Reporting
q EDL Ingestion tools & other technologies and tools
q Educate teams to migrate and develop new cloud applications


Qualifications

Basic Requirements:
 A minimum of 5 years of hands-on technical experience with:
 big data implementation and technology offerings
 AWS/cloud big data modeling & data management
 analytics and ingestion architecture of big data
 data lake management and data architecture
 data lake design patterns & cloud best enterprise practices
 IoT and streaming, real time processing
 Big data related AWS technologies

Experience in AWS technologies such as Kinesis, Lambda, EC2, Redshift, RDS, Cloud formation, EMR, AWS S3, AWS Analytics, Spark, Databricks
Experience with at least one of the following languages Scala, Python, R and or Java
Experience with designing, developing, and implementing complex integration for end-to-end solutions at a middleware and app level with focus on performance optimization
Strong implementation skill in area of cloud development in AWS
Demonstrated ability in implementing cloud scalable, real time and high-performance data lake solutions (AWS)
Ability to quickly perform proof-of-concepts for validating new technology or approach
Ability to exercise independent judgment and creative problem-solving techniques in a highly complex environment using leading-edge technology and/or integrating with diverse application systems
Ability to lead and drive technology change in a fast-paced, dynamic environment and all phases of the entire software life cycle
Strong experience with data catalog, data governance, Collibra, MDM and/or Data Quality (IDQ) toolset
Strong experience with integration of diverse data sources (batch and real time) in the cloud
Lead the design and sustainment of data pipelines and data storage
Expertise in Structured, unstructured, SQL and No-SQL technologies
Expertise with identifying and understanding source data systems and mapping source system attributes to the target
Experience with design and automation of ETL\ELT processes
AWS and cloud performance tuning and optimization experience
Experience with effort estimation for new projects/proposals on an ongoing basis.
Excellent communication skills across all levels; ability to communicate with ease the complex and technical concepts.
Ability to work effectively in a fast-paced environment

Primary Location: US-NY-New York
Job: Information Technology
Organization: Corporate
Schedule: Full-time
Employee Status: Regular
Job Level: Non-Management
Job Posting: Jun 28, 2019, 2:58:28 PM"
33,Senior Data Software Engineer - Java Map Reduce - Distributed Data Processing,"New York, NY",New York,NY,None Found,None Found,"BA/BS degree and 2+ years of experience in software engineering OR MS degree and 1+ years of experience (Degree in Computer Science or related field preferred)Proficiency with large-scale distributed data processing systems like Hadoop, MR, Hive, Presto, and distributed data stores like Vertica and DruidExperience building large-scale multi-TB data processing systemsExperience supporting production systems",None Found,None Found,None Found,None Found,"Xandr’s mission is to Make Advertising Matter. We develop a technology platform that powers the real-time sale and purchase of digital & TV advertising. Our platform is engineered to provide one of the fastest, most reliable, and massively scaled advertising systems in the industry. As a transparent and independent partner for some of the largest publishers and advertisers, Xandr helps ensure that the Internet stays open and free.
As a Data Engineer on the Publisher Integrations capability, you will be responsible for creative solutions distilling data at a massive scale into insightful, actionable information for online publishers that use Xandr to drive demand.

Consider some of the problems we try to solve:
Getting Insight: Rapidly process data on an immense scale. Create reports that drive business outcomes and optimizations. Balance granularity and business value with resource cost and efficiency.Grow a Mature Platform: Reduce latency and error rates. Increase uptime. Maintain speed, availability, and reliability as our client base grows.

About the job:
Design and develop reliable, scalable and testable Map Reduce jobs and end-to-end data processing for reporting & analytics, real-time decision-making use casesCollaborate and communicate effectively with team members, product management, open source community members, and other stakeholdersBuild and maintain tools to automate regular testing and deploymentConsistently optimize and improve our systems, tools, and testing

QualificationsBA/BS degree and 2+ years of experience in software engineering OR MS degree and 1+ years of experience (Degree in Computer Science or related field preferred)Proficiency with large-scale distributed data processing systems like Hadoop, MR, Hive, Presto, and distributed data stores like Vertica and DruidExperience building large-scale multi-TB data processing systemsExperience supporting production systemsSolid experience in Java/Python/Scala and SQL and proficiency with Unix toolsNice to have: Experience with Spark, Kafka, NoSQL databases

More about you:
You are passionate about a culture of learning and teaching. You love challenging yourself to constantly improve, and sharing your knowledge to empower othersYou like to take risks when looking for novel solutions to complex problems. If faced with roadblocks, you continue to reach higher to make greatness happenYou care about solving big, systemic problems. You look beyond the surface to understand root causes so that you can build long-term solutions for the whole ecosystemYou believe in not only serving customers, but also empowering them by providing knowledge and tools"
34,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Hinge is looking for a Senior Data Engineer to join the team. As a Senior Data Engineer at Hinge, you will create components of a modern data pipeline that will be the foundation of Hinge's decision-making ability. The systems you help create, the problems you help solve, and the support of our analytical minds will be pivotal to the success of Hinge. This role is key to the success of Data Engineering at Hinge. The Senior Data Engineer will be implementing critical pipelines and advancing best practices for not only the data engineering team, but the rest of the organization.

----------------
Responsibilities
----------------


Work with our Engineering teams to ensure data is flowing accurately through data creation to our presentation layers.
Become an advocate for the Data Engineering team by developing and championing Data Engineering practices with the team and with the company at large.
Create foundational Airflow operators.
Improve our Data Engineering stack through containerization, data modeling, developing our ETL pipelines, and more.
Work with stakeholders and translate their needs and expectations into action items and deliverables

---------
About You
---------


Expertise in Python, SQL, shell scripting, and databases.
+5 years of professional/industry experience.
Capability to drive initiatives and articulate their value to Engineering and other stakeholders.
Experience delivering data products from conception to delivery.
Good communication skills (written/verbal).
Passionate about designing elegant ETL pipelines.

Nice-to-Haves:

Used Looker, Redshift, Airflow.
Familiar with data vault modeling, Agile methodologies, Kubernetes, Docker, & data governance.

------------------------------------
Objectives in the first three months
------------------------------------


Learning about the Data Engineering business needs/data needed of Hinge.
Work with internal stakeholders like Marketing, Member Services, and SRE to get a sense of data needs and roadmap.
Delve into a portion of our data pipeline and develop a working beta of a system they identify with the team needs support.

-----------
Our Company
-----------

We believe the quality of your relationships determines the quality of your life. So when it comes to your most important relationship, it makes sense to take a more thoughtful approach. Hinge provides an alternative to swipe culture by creating smart matches and natural conversations among people who are on the same page. That's why 75% of our first dates turn into second dates, and why we're the #1 mobile-first dating app mentioned in the NYTimes Wedding section. Hinge is where the next generation is going when they're over dating games and ready to find meaningful connections.

-----------
Our Culture
-----------


Open: Invite and deeply consider challenges and criticism.
Candid: Share your genuine thoughts and opinions directly, in real time.
Kind: Be empathetic, communitarian and trustworthy.
Engaged: Be self-empowered, purposeful and involved. Lean in and be the solution.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
35,Data Architect,"New York, NY 10017",New York,NY,10017,None Found,None Found,None Found,"
Handling cloud architecture on the Google Cloud Platform (GCP)
Implementing cloud strategy development
Defining technical tasks
Implementing cloud infrastructure solutions
Designing cloud migration and cloud-native solutions
Implementing GCP Cloud environments
Internally and externally to drive Cloud Adoption
Understanding the technical requirements of enterprise customers
",None Found,"
Strong business understanding
Passionate about technology
10+ years of professional experience in the relevant area
Having an experience to work as a technical lead will be an added advantage
Ready to travel 25% of time
Nice if you have certifications
150 - 170 K base salary
Bonus of 6% on base salary
Medical dental, vision related medical facilities
401k
Health Wellness Plan
","Position: Data Architect
Location: New York, United States
Remuneration: $ 100.00 per day
Who is hiring?
We are currently working with a leading company in the US that is working in the area of digital transformation for the cloud environment. It helps companies migrate to cloud environment from Azure, GCP or AWS. My client also works in big data, analytics and AI in the area of warehousing industries. They seek to hire a GCP Architects to work on infrastructure and application.

What will you be doing?

Key Responsibilities:

Handling cloud architecture on the Google Cloud Platform (GCP)
Implementing cloud strategy development
Defining technical tasks
Implementing cloud infrastructure solutions
Designing cloud migration and cloud-native solutions
Implementing GCP Cloud environments
Internally and externally to drive Cloud Adoption
Understanding the technical requirements of enterprise customers

Why you shouldn’t miss this opportunity?

Requirements:

Strong business understanding
Passionate about technology
10+ years of professional experience in the relevant area
Having an experience to work as a technical lead will be an added advantage
Ready to travel 25% of time
Nice if you have certifications
150 - 170 K base salary
Bonus of 6% on base salary
Medical dental, vision related medical facilities
401k
Health Wellness Plan

Third Republic is a new breed of agency helping people skilled in advanced technologies to further their careers. We have a plethora of unique job opportunities for people skilled in Salesforce, Workday, DevOps, AWS, Google Cloud, Azure, Architecture and Software Sales.

Please contact us on 646 979 0247 if you would like to find out more about current market trends and other services, we offer such as CloudCareers, our talent management service helping people access the hidden job market.

Data Science(Data Engineer), Big Data, Google Cloud Platform (GCP)"
36,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,"
Self-starter who can work in a highly demanding environment and maintains a positive attitude
4+ years of data engineering experience
Experienced in Big Data development using AWS EMR, SQOOP, Hadoop, Spark, and HDFS
Expert with one general purpose programming language, including but not limited to: Java, Scala, Python
Awareness of new and emerging Big Data technologies and trends
Advanced degree in computer science, engineering or a related field
",None Found,None Found,None Found,None Found,"Vettery is changing the way people hire and get hired. We use machine learning and real-time data to match talented job-seekers with inspiring companies. Our goal is to enrich and automate the recruiting process, make hiring more rewarding for everyone, and create a happier and more accountable working world.

Since launching in 2015, we've made thousands of matches on our marketplace. We're currently working with over 45,000 job-seekers and 20,000 companies, from Fortune 500 giants to startups based out of co-working spaces. We've built powerful machine learning capabilities, and our matching algorithm is becoming more intelligent with each passing day. With an eye on the future, we're expanding our reach across major cities in the US, and around the globe.

Vettery engineers are working to build a highly innovative platform enabling an efficient experience for our users. Our engineers work across the full stack building highly scalable distributed solutions to enable exponential customer and business growth. Vettery engineers have input into the whole process of the company from business decisions to where our tech stack is going. Our engineering team works on a huge array of projects and are constantly incorporating new technologies into the stack.

Key responsibilities and expectations:

Design, develop and maintain end-to-end data pipelines across multiple data sources and systems of record.
Develop and design data models, data structures and ETL jobs for data science consumption
Manage and maintain cloud based data and analytics platform
Work in a highly agile development environment and practice Agile/Scrum methodology
Participate in hackathons, team outings, lunches and other team building events.

Candidate Qualifications:

Self-starter who can work in a highly demanding environment and maintains a positive attitude
4+ years of data engineering experience
Experienced in Big Data development using AWS EMR, SQOOP, Hadoop, Spark, and HDFS
Expert with one general purpose programming language, including but not limited to: Java, Scala, Python
Awareness of new and emerging Big Data technologies and trends
Advanced degree in computer science, engineering or a related field

Vettery has five key values that are the foundation of our company culture, which every employee embodies:


Positivity - We're positive when things get difficult so we can stay motivated and lift each other up. We're very team focused in everything we do so contagious positive energy is extremely impactful.
Ownership - We take pride in our work and take on a lot of responsibility from day one. All of us have the ability to see how our work and performance impact the success of the business.
Grit - We love getting in the trenches and building from the ground up. Even though we've significantly grown since our tiny startup days, we still have that scrappy mentality and love that there's still a lot to accomplish.
Awareness - We're focused, strategic, and constantly learning from our experiences. Each of us knows what's expected of us as a corporate citizen and within our teams.
Collaboration - We learn from one another and are constantly working with each other, within and across teams. Every team has an impact on others and we take pride in clear lines of communication.

Why you'll love working at Vettery:
We love coming to work on Monday. It's easy to love the work you do when you see the positive impact it has, and helping someone find their dream job can change their life forever. We believe in our mission, love the work, and have fun doing it together. Plus, coming to work in our sunny Flatiron office is easy when there are so many things to look forward to: Flag Football games, Thursday Game Night, Cross-Team lunches, company happy hours, volunteer events, adorable pups, ping-pong, and your favorite snacks.

We know life is about more than just work. We have an open vacation policy so you can take the time you need to relax and rejuvenate, contribute to the cost of insurance coverage (health, vision, and dental), and offer a fully paid 12-week parental leave, 401k, commuter benefits, and gym membership discounts.

We invest in your development. A company is only as strong as its team, and we want to help strengthen every member of our team. We give everyone the opportunities and support they need to reach that next professional level through company-sponsored General Assembly classes and conferences, in-house training, a culture of continuous feedback, and the chance to run with projects.

We're consistently recognized for our culture. We're listed #5 on Fortune's 60 Best Companies to Work ( https://click.api.drift.com/click/f53ee223-f672-4b64-b489-ab2d83a21bcc?u=https%3A%2F%2Ffortune.com%2F2019%2F07%2F16%2Fbest-companies-new-york-2019%2F%23bestcompanies&h=83cf0552f4bb19bbeff9cbafb15704e1 ) for in New York City this year, and have been previously honored at Crain's Best Places to Work Awards and included in Inc Best Workplaces.

Vettery values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status."
37,"Claims Data Engineer, Chubb Overseas General","New York, NY 10036",New York,NY,10036,None Found,None Found,None Found,None Found,None Found,None Found,"Claims Data Engineer, Chubb Overseas General(Job Number: 329062)
Description

The Chubb Overseas General (COG) Claims Business Intelligence Group provides reporting and business insight solutions for the international claims department, allowing Chubb to provide superior claim handling services to our customers in 54 countries around the world.

Scope
The COG Claims Data Engineer will transform and combine large, complex datasets into pragmatic, actionable tables, providing tangible benefits to our organization’s strategy for handling claims.
MAJOR RESPONSIBILITIES:
Partnering with IT and business stakeholders to identify areas of interest and opportunity within data
Sampling, processing, and transforming data as needed
Automating transformation and aggregation tasks for datasets from multiple sources
Utilizing problem solving and critical thinking skills to glean insights from analysis
Sharing insights with business stakeholders and executives
Being a team player and supporting fellow COG Claims BI team as needed

Qualifications

MINIMUM REQUIREMENTS:
2 to 5 years of hands on data engineering experience (insurance industry experience is preferred), machine learning experience a plus
Advanced programming expertise in Python, expertise in R or VBA is also a plus
Experience with GitHub and other collaborative development tools and environments
Advanced knowledge of relational databases and SQL scripting
Advanced expertise with business intelligence reporting and data visualization tools (COGNOS and/or Qlik is preferred, though interactive Python plotting packages or D3 a plus)
Strong statistical background, and ability to learn and utilize new techniques and technologies in a short time
Excellent interpersonal, verbal, and written communication skills
Bachelor’s or Master’s degree (specializing in mathematics, statistics, computer science, economics, or another quantitative discipline is preferred)
Available for occasional international travel


EEO Statement

At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment.

Work Locations - New York-1133 Ave of the Ameri New York 10036

Job - Information Technology

Travel - No

Job Posting - Aug 15, 2019, 3:52:11 PM"
38,"Data Engineer, Data Acquisition","Hoboken, NJ 07030",Hoboken,NJ,07030,None Found,"Play a pivotal design and hands on implementation role in improving the Data infrastructure in a project-oriented work environment.Influence cross functional architecture in sprint planningGather and process raw data at scale from internal and external data sources and expose mechanisms for large scale parallel processingDesign, implement and manage a near real-time ingestion pipeline into a data warehouse and Hadoop data lake.Process unstructured data into a form suitable for analysis and then empower state-of-the-art analysis for analysts, scientists, and APIsSolve complex SQL and Big Data Performance challenges.Mitigate Risks in our data infrastructure by developing the best in class tools and processes.Implement controls, policies, processes and best practices in the Data Engineering space.Evangelize an extremely high standard of code quality, system reliability, and performance.Help us improve our database deployment and change management process.Provide reliable and efficient Data services as part of the global data team.Work closely with the team on development best practices and standards.Be a mentor.",None Found,None Found,None Found,None Found,"Position Description
As part of the newly created Data Strategy and Enablement Team (DS&E), this role will be an enabler of our journey to be the world’s leading data-driven retailer. As part of this transformation, we are seeking an individual who will be responsible for establish robust data pipelines and services – including both in house developed data enabling services and systems integrations across the DS&E team to ensure we our technical deliverables meet and exceed the quality expectations.

We are looking for a highly motivated, resourceful, team-oriented individual to drive the data engineering process. You are exceptionally talented Data engineer with an outstanding track record of working with very large data sets and building robust ETL pipelines for data acquisition for internal systems and external data sources. You will be modernizing and improving the data acquisition infrastructure from the ground up. You will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.

The Data Engineer role will report up to the Lead Data Engineer/Senior Manager Data Engineering
Minimum Qualifications
Play a pivotal design and hands on implementation role in improving the Data infrastructure in a project-oriented work environment.Influence cross functional architecture in sprint planningGather and process raw data at scale from internal and external data sources and expose mechanisms for large scale parallel processingDesign, implement and manage a near real-time ingestion pipeline into a data warehouse and Hadoop data lake.Process unstructured data into a form suitable for analysis and then empower state-of-the-art analysis for analysts, scientists, and APIsSolve complex SQL and Big Data Performance challenges.Mitigate Risks in our data infrastructure by developing the best in class tools and processes.Implement controls, policies, processes and best practices in the Data Engineering space.Evangelize an extremely high standard of code quality, system reliability, and performance.Help us improve our database deployment and change management process.Provide reliable and efficient Data services as part of the global data team.Work closely with the team on development best practices and standards.Be a mentor.
Who you are:
You have prior experience with leading data engineering efforts across a variety of data systemsYou have deep understanding of commercial data sources and understand database concepts and terminologyYou have a demonstrated track record of handling multiple complex sourcing projects and delivering results in the data engineering areaYou have strong SQL experience and the ability to work on multiple aspects of a data projects including ETL, tools integrations, data results and APIs.You are a team player, with the courage to drive change through disruption while maintaining a respect for the team
Requirements:
Very Strong engineering skills. Should have an analytical approach and have good programming skills.Provide business insights, while leveraging internal tools and systems, databases and industry dataMinimum of 5+ years’ experience. Experience in retail business will be a plus.Excellent written and verbal communication skills for varied audiences on engineering subject matterAbility to document requirements, data lineage, subject matter in both business and technical terminology.Guide and learn from other team members.Demonstrated ability to transform business requirements to code, specific analytical reports and toolsThis role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering teamExperience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.) and platforms such as HDP, Cloudera etc.Strong Hadoop scripting skills to process petabytes of dataExperience in Unix/Linux shell scripting or similar programming/scripting knowledgeReal time data ingestion (Kafka)Experience in ETL/ processes with exposure to one or more tools such as Nifi, Talend, Informatica, SSIS etc.
Additional Preferred Qualifications

Company Summary
The Walmart eCommerce team is rapidly innovating to evolve and define the future state of shopping. As the world’s largest retailer, we are on a mission to help people save money and live better. With the help of some of the brightest minds in technology, merchandising, marketing, supply chain, talent and more, we are reimagining the intersection of digital and physical shopping to help achieve that mission.
Position Summary
As part of the newly created Data Strategy and Enablement Team (DS&E), this role will be an enabler of our journey to be the world’s leading data-driven retailer. As part of this transformation, we are seeking an individual who will be responsible for establish robust data pipelines and services – including both in house developed data enabling services and systems integrations across the DS&E team to ensure we our technical deliverables meet and exceed the quality expectations.

We are looking for a highly motivated, resourceful, team-oriented individual to drive the data engineering process. You are exceptionally talented Data engineer with an outstanding track record of working with very large data sets and building robust ETL pipelines for data acquisition for internal systems and external data sources. You will be modernizing and improving the data acquisition infrastructure from the ground up. You will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.

The Data Engineer role will report up to the Lead Data Engineer/Senior Manager Data Engineering."
39,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,"Minimum of a degree in a quantitative subject, ideally containing computer science, software engineering, database, or data analysis modules
3 to 5 years’ experience working as a data engineer or software engineer with a strong focus on data, ideally in financial services (insurance, banking, instech, fintech)
Strong and proven capability in dimensional data modelling: to design, implement and use dimensional database / datamart structures to ensure data is organised, aggregated and indexed effectively and ready for downstream reporting, analysis and statistical modelling
Strong and proven capability in data acquisition: to design data capture systems, collect or stream data from a variety of systems, to creatively transform, manipulate, and methodically describe it
Strong and proven capability with database administration: to maintain transactional and reference data, with migration alongside product development to ensure logical consistency
Strong and proven capability with data validation and verification: to understand the pitfalls in different data sources and to perform necessary provenance and lineage checks, and implement various data testing
A passion for quality software coding, data and analytics and some knowledge of statistics
Experience working in a fast-paced environment and desire for good documentation, exceptional delivery, effective organisation, and high-quality communication with team and customers
Strong and proven technical skills in MS SQL Server, SSIS, SSRS, Python, Git, Bash / Powershell
Desirable technical skills in MS Azure (inc DataFactory), Apache Airflow, PowerBI, Linux (Redhat / CentOS)
","Innovisk is a global underwriting platform with a strategy of growth and diversification. We are building market leading underwriting businesses through the application of modern technology, high quality data management, advanced analytics, and by attracting industry leading underwriting talent.
The Data Science & Actuarial team is part of an international core of integrated advisory, technology and analysis services supporting speciality & commercial insurance underwriters in a wide variety of lines including Commercial EL/PI, D&O, Energy, Environmental, Financial Lines, Inland Marine, M&A, Renewables and Surety.

We are looking for a Data Engineer to join our team of experienced data engineers, reporting engineers, data scientists and actuaries to help our underwriting businesses to evaluate, price and manage their risks & business operations.
The successful candidate will have a passion for creating and managing innovative data assets, and will work closely with the tech team, the underwriters and the operations team. You will also enjoy direct interaction with highly skilled and experienced underwriters, brokers, and insurance experts – the people who are pushing the industry forward.
The Role

Help to acquire, create and manage innovative data assets for the team. These assets draw from various sources – internal underwriting systems, external claims systems, flat files, databases, external APIs etc.
Alongside your UK-based counterpart, you will manage the pipelining (acquisition, ETL, storage, access), engineering (cleansing, feature creation / selection, warehousing, data marts, cubing), and understanding (documentation, exploration, communication) of this data
Work closely with the whole Data Science & Actuarial team to identify patterns, trends, outliers, exceptions in the underwriting businesses operational and performance data: help to understand root causes, predict future patterns and recommend management actions
Work closely with the Tech team to integrate new in-house underwriting systems and data assets, and advise on systems architecture and transactional database design
Help us develop streamlined external-facing commercial data products
As a new venture we have very little technical debt, so you will also have an opportunity to help shape our frameworks and methodologies.
Support the team in stakeholder engagement and business process
The Requirements

Minimum of a degree in a quantitative subject, ideally containing computer science, software engineering, database, or data analysis modules
3 to 5 years’ experience working as a data engineer or software engineer with a strong focus on data, ideally in financial services (insurance, banking, instech, fintech)
Strong and proven capability in dimensional data modelling: to design, implement and use dimensional database / datamart structures to ensure data is organised, aggregated and indexed effectively and ready for downstream reporting, analysis and statistical modelling
Strong and proven capability in data acquisition: to design data capture systems, collect or stream data from a variety of systems, to creatively transform, manipulate, and methodically describe it
Strong and proven capability with database administration: to maintain transactional and reference data, with migration alongside product development to ensure logical consistency
Strong and proven capability with data validation and verification: to understand the pitfalls in different data sources and to perform necessary provenance and lineage checks, and implement various data testing
A passion for quality software coding, data and analytics and some knowledge of statistics
Experience working in a fast-paced environment and desire for good documentation, exceptional delivery, effective organisation, and high-quality communication with team and customers
Strong and proven technical skills in MS SQL Server, SSIS, SSRS, Python, Git, Bash / Powershell
Desirable technical skills in MS Azure (inc DataFactory), Apache Airflow, PowerBI, Linux (Redhat / CentOS)
Equal Opportunity Employer/Vet/Disability"
40,Data Engineer,"New York, NY 10018",New York,NY,10018,None Found,None Found,None Found,None Found,None Found,None Found,"We are an award-winning healthcare technology company deploying a suite of intuitive patient engagement solutions that empowers healthcare organizations to create memorable patient experiences by fostering connections along the healthcare journey to ensure the best possible outcomes for providers, staff, patients, and their loved ones.

As a Data Engineer at CipherHealth, you will take an active role in designing the future of the data engineering practice. A successful candidate will combine strong technical skills and a passion for creative problem solving.

Responsibilities
----------------------


Delivering and maintaining highly available computing platforms
Collect and transform data from a number of sources, storing it in highly optimized database systems
Creating data integration services
Building data products
Maintaining ongoing reliability, performance, and support of the infrastructure--providing solutions based on application needs and anticipated growth

Data Stack
----------


MongoDB, Postgres, Redshift
Ruby, Mongo, and VueJS
Pentaho Kettle
Periscope Data for dashboarding

Qualifications
--------------


Strong programming skills in Ruby/Python and SQL
Great collaboration and team working skills
Practical experience in best practices for developing data pipelining frameworks
Ability to learn autonomously and quickly
Extremely organized and detail-oriented with effective multitasking and prioritization skills
Excellent written and verbal communication skills
3 - 5 years of experience working in software development, data engineering, or related STEM fields
3+ years of working experience with relational databases and data warehousing

"
41,Data Engineer – Growth,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"Spotify is looking for a Data Engineer to join us. You will build data driven solutions to bring music and digital media experiences to our 100 million active users and millions of artists either by working directly on product features, publishing and insight tools for artists, or by improving the quality of our data tools and large scale data infrastructure. You will take on complex data-related problems using some of the most diverse datasets available — user behaviors, acoustical analysis, revenue streams, cultural and contextual data, and other signals across our broad range of mobile and connected platforms. Above all, your work will influence the way people experience music.

What you’ll do
Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.
Use best practices in continuous integration and delivery.
Help drive optimization, testing and tooling to improve data quality.
Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.
Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.
Who you are
You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
You are knowledgeable about data modeling, data access, and data storage techniques.
You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
You understand the value of partnership within teams."
42,Data Engineer,"New York, NY 10036",New York,NY,10036,None Found,"
Bachelor’s Degree

At least 8 years of experience in application development

At least 3 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)
",None Found,None Found,None Found,None Found,"Kinetix Trading is seeking experienced Data Engineers to join our team in New York! This is the perfect opportunity for an experienced Engineer well versed in Big Data Technologies to build on top of a greenfield and deliver valuable solutions to our clients.

Basic Qualifications:

Bachelor’s Degree

At least 8 years of experience in application development

At least 3 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)


Preferred Qualifications:

Bachelors degree in Computer Science or Electrical Engineering

8+ years of experience in application development

3+ year experience working with Hadoop, Spark, Flume, HDFS, Zookeeper, and/or MongoDB

2+ years of experience with Agile engineering practices


3+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)

2+ years of experience developing Java based software solutions

2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)

2+ years of experience developing software solutions to solve complex business problems"
43,Data Engineer / Scientist,"New York, NY",New York,NY,None Found,None Found,"
Proven software development ability, building complex and robust systems - at least 4 years
Proven analytical/research ability - data science, cyber-security, intelligence, academic, etc.
Data engineering experience (data pipelines, ML architecture) - advantage
Web scraping experience - advantage
Python/Golang - advantage
ML libraries and tools (pandas, scikit-learn, jupyter notebook) - advantage
Team player, independent, able to work in an unstructured and fast-paced environment.",None Found,"
Maintain and improve our in-house git-inspired data pipeline orchestration system
Write applications that fetch data by various means, including PDF parsing, HTML scraping, and more.
Perform Real Estate analysis based on the data repository we are continuously growing
Extract insights from our data, and generate predictions using data science techniques
Develop the algorithms which generate the datasets that are used by the whole R&D department",None Found,None Found,"Skyline AI is redefining the world’s approach to the $200 trillion commercial real estate industry by building data technology that will transform the market from the low-tech methods it’s been relying on for decades.
Here at Skyline AI, our data is our fuel. We are constantly collecting new data sources, analyzing our many different types of data and extracting valuable information from it.
We are looking for a versatile software engineer to join our data group. This is one of the broadest and most influential roles in the company - touching our data sources, data processing algorithms, ML research, and being responsible for the orchestration system on which it all relies.

About Skyline AI
Skyline AI is an asset management technology company for commercial real estate. Skyline AI partners with leading commercial real estate firms to establish next-generation investment vehicles augmented by artificial intelligence. Mining data from over 130 different sources and applying advanced AI models to detect and exploit market anomalies, identify superior risk-reward investments, and discover untapped value-add opportunities, Skyline AI is a new, more precise way to invest in real estate. Founded in 2017, Skyline AI is backed by Sequoia Capital, JLL (NYSE: JLL), Nyca Partners and others. The company has offices in New York and Tel Aviv.

Responsibilities
Maintain and improve our in-house git-inspired data pipeline orchestration system
Write applications that fetch data by various means, including PDF parsing, HTML scraping, and more.
Perform Real Estate analysis based on the data repository we are continuously growing
Extract insights from our data, and generate predictions using data science techniques
Develop the algorithms which generate the datasets that are used by the whole R&D department
Qualifications
Proven software development ability, building complex and robust systems - at least 4 years
Proven analytical/research ability - data science, cyber-security, intelligence, academic, etc.
Data engineering experience (data pipelines, ML architecture) - advantage
Web scraping experience - advantage
Python/Golang - advantage
ML libraries and tools (pandas, scikit-learn, jupyter notebook) - advantage
Team player, independent, able to work in an unstructured and fast-paced environment."
44,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Help us Build the Future of Money
Gemini Trust Company, LLC (Gemini) is a licensed digital asset exchange and custodian. We built the Gemini platform so customers can buy, sell, and store digital assets (e.g., Bitcoin, Ethereum, and Zcash) in a regulated, secure, and compliant manner.


Digital assets and blockchain technology have the power to transform the world for good. This truth, along with our core values, form the bedrock of our company and culture. At Gemini, no job is too small and no project too big as we endeavor to build the future of money. We are a mission-driven, team-based, inclusive, and determined community of thought leaders who invest in each other and the long game. Join us in our mission!

THE DEPARTMENT: DATA ENGINEERING

THE ROLE: SENIOR DATA ENGINEER

As a member of our data engineering team, you’ll shape the way we approach data at Gemini by using your engineering, analytical and communication skills to work with teams across the business. You know how to ask the right questions and are passionate about using data to support and drive informed business decisions. You are ready to roll up your sleeves and are excited to take on challenging opportunities and projects. You’ll mentor data engineers and analysts and guide our internal teams to use data to improve the product and achieve KPIs. Communicating your insights with leaders across the organization is paramount to success.

RESPONSIBILITIES:

Design, architect and implement best-in-class Data Warehousing and reporting solutions
Lead and participate in design discussions and meetings
Mentor data engineers and analysts
Design, automate, build, and launch scalable, efficient and reliable data pipelines into production
Build real-time data and reporting solutions
Design, build and enhance dimensional models for Data Warehouse and BI solutions
Research new tools and technologies to improve existing processes
Develop new systems and tools to enable the teams to consume and understand data more intuitively
Partner with engineers, project managers, and analysts to deliver insights to the business
Perform root cause analysis and resolve production and data issues
Create test plans, test scripts and perform data validation
Tune SQL queries, reports and ETL pipelines
Build and maintain data dictionary and process documentation


MINIMUM QUALIFICATIONS:

7+ years experience in data engineering with data warehouse technologies
7+ years experience in custom ETL design, implementation and maintenance
7+ years experience with schema design and dimensional data modeling
Experience building real-time data solutions and processes
Experience building and integrating web analytics solutions
Advanced SQL skills is a must
Skilled in programming languages Python and/or Java
Experience with one or more MPP databases(Redshift, Bigquery, Snowflake, etc)
Experience with one or more ETL tools(Informatica, Pentaho, SSIS, Alooma, etc)
Strong computer science fundamentals including data structures and algorithms
Strong software engineering skills in any server side language, preferable Python
Experienced in working collaboratively across different teams and departments
Strong technical and business communication


PREFERRED QUALIFICATIONS:

Kafka, HDFS, Hive, Cloud computing, machine learning, text analysis, NLP & Web development experience is a plus
NoSQL experience a plus
Experience with Continuous integration and deployment
Knowledge and experience of financial markets, banking or exchanges


It Pays to Work Here

We take a holistic approach to compensation at Gemini, which includes:

Competitive base salaries across all departments
Ownership in the company via profit sharing units
Amazing benefits, 401k match contribution, and flexible hours
Snacks, Perks, Wellness Outings & Events


Gemini is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you have a disability or special need that requires accommodation, please let us know."
45,Financial Services Advisory Senior - Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,"
Strong skills in cloud, data pipelining, data modeling and productionizing AI/ML models.
Ability to multitask and work in a fast-paced, collaborative team environment
Ability to travel in accordance with client and other job requirements
Excellent written and oral communication skills; writing, publishing and conference-level presentation skills a plus","
Understand business and technical requirements
Assess the merits of different technology solutions (e.g., cloud native vs. cloud agnostic) to make recommendations
Study and transform data science prototypes into production-ready systems with large volume data requirements
Select appropriate datasets and data representation methods
Design and implement solutions for data aggregation, improve data foundational procedures, integrate new data management technologies and software into the existing system and build data collection pipelines
Conduct data discovery activities, performing root cause analysis, and make recommendations for the remediation of data quality issues
Run (already existing) machine learning tests and experiments, retraining models when necessary
Develop back-end components to improve responsiveness and overall performance
Expose endpoints to provision model outputs in application frameworks (e.g., Spring Boot for Java, Django or Flask for python.
Perform root cause analysis on data processes and pipelines to answer specific business questions, solve issues, and identify opportunities for improvement.
Make AI/ML insights available to the business and IT through data provisioning and channel integration
Keep abreast of developments in the field; review solutions to ensure they are consistent with best practices in AI/ML architecture and engineering.
Be a team player using an agile delivery methodology while consistently delivering quality client services",None Found,None Found,"EY is the only professional services firm with a separate business unit (“FSO”) that is dedicated to the financial services marketplace. Our FSO teams have been at the forefront of every event that has reshaped and redefined the financial services industry. If you have a passion for rallying together to solve the most complex challenges in the financial services industry, come join our dynamic FSO team!

We help our clients transform. Together we are changing the way they interact with their Customers, Employees and Partners alike. We are modernizing their applications, creating new ones and driving business efficiency across their ever-changing platform and channel infrastructures. Everything we do centers around this simple story.

The opportunity

EY’s FSO-Technology practice is hiring talented data engineers with a passion for designing and implementing leading edge data architectures. Our clients in banking and capital markets, wealth and asset management, and insurance are increasingly turning to cloud-native services. Our clients also are owners and stewards of significant amounts of data and want to use AI/ML insights to realize new business capabilities, reduce costs, and better serve their customers. If you want to bring transformative, cloud-enabled solutions to market by leveraging deep learning and other AI methods, consider joining our team of entrepreneurial and collaborative professionals.
In FSO-T, our work goes end-to-end, and therefore we value systems thinkers. We believe industry has a real challenge when it comes to keeping up with the infuriating pace of change and innovation. In FSO-T we are carrying out the important work that enables our clients to differentiate how they go about delivering technology and helping them with actual execution.

Your Key Responsibilities

As a data engineer you will collaborate with a spectrum of EY professionals to deliver solutions utilizing leading big data platforms and cloud technologies (e.g., Azure, Databricks, GCP, Cloudera, etc.). In any normal day, you’ll be expected to:

Understand business and technical requirements
Assess the merits of different technology solutions (e.g., cloud native vs. cloud agnostic) to make recommendations
Study and transform data science prototypes into production-ready systems with large volume data requirements
Select appropriate datasets and data representation methods
Design and implement solutions for data aggregation, improve data foundational procedures, integrate new data management technologies and software into the existing system and build data collection pipelines
Conduct data discovery activities, performing root cause analysis, and make recommendations for the remediation of data quality issues
Run (already existing) machine learning tests and experiments, retraining models when necessary
Develop back-end components to improve responsiveness and overall performance
Expose endpoints to provision model outputs in application frameworks (e.g., Spring Boot for Java, Django or Flask for python.
Perform root cause analysis on data processes and pipelines to answer specific business questions, solve issues, and identify opportunities for improvement.
Make AI/ML insights available to the business and IT through data provisioning and channel integration
Keep abreast of developments in the field; review solutions to ensure they are consistent with best practices in AI/ML architecture and engineering.
Be a team player using an agile delivery methodology while consistently delivering quality client services
Skills and attributes for success

Strong skills in cloud, data pipelining, data modeling and productionizing AI/ML models.
Ability to multitask and work in a fast-paced, collaborative team environment
Ability to travel in accordance with client and other job requirements
Excellent written and oral communication skills; writing, publishing and conference-level presentation skills a plus
To qualify for the role you must have

Bachelor’s degree in Computer Science, Mathematics or similar field; Master’s degree a plus
A minimum of 5 years’ experience with at least 3+ years in hands-on development
Proven experience as an AI/ML and cloud data engineer, architect or similar role over with minimum of 1 completed projects.
Sound working knowledge of:
Java, Python, and/or Scala
Kafka (or similar), NiFi, Hadoop, and Spark
Big data querying tools such as Pig or Hive or Impala
NoSQL databases such as MongoDB, Cassandra, and Neo4
Data ingestion, data cleaning, ETL, traditional warehousing and data marts, and data & insights provisioning
Hands on experience with GCP, AWS, or Azure.
Understanding of data structures, data modeling, and software architecture
Familiar with at least one Machine Learning library/framework (scikit-learn, Mahoot, MLib, H2o, etc.).
Ideally, you’ll also have

GCP, AWS, or Azure Certifications
Proficiency in a deep learning framework is plus (Tensorflow, Keras, Theano, MXNet, etc.)
Knowledge of data science methods and statistics a plus
What we look for

We’re interested in highly motivated talented individuals with a strong willingness to think outside of the box. You can expect plenty of autonomy in this role, so you’ll need the motivation to take initiative and seek out opportunities to improve our current relationships and expand our business in the evolving market. If you’re serious about consulting and ready to take on some of our clients’ most complex issues, this role is for you.

What working at EY offers

We offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, both pension and 401(k) plans, a minimum of 15 days of vacation plus ten observed holidays and three paid personal days, and a range of programs and benefits designed to support your physical, financial and social well-being. Plus we offer

Support, coaching and feedback from some of the most engaging colleagues around
Opportunities to develop new skills and progress your career
The freedom and flexibility to handle your role in a way that’s right for you
About EY

As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.

Join us in building a better working world.

Apply now.

EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status."
46,Sr. Data Engineer,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"We Are:

Accenture Interactive Operations (AIO) is the global managed services arm of Accenture Interactive. We activate and optimize hyper-relevant, data-driven, brand experiences to deliver breakthrough marketing-led growth, cost effectively, across all digital consumer touchpoints and channels.
Advance your career in a fast-accelerating digital marketing agency on track to transform the industry and dominate the market. Disrupt, innovate, and change the way frictionless marketing strategy is delivered to clients at scale. Deploy new technologies and data-driven insights. Transform the performance of leading brands. Grow alongside our global talent network. Join Accenture Interactive Operations.

For each new employee, we provide guidance, mentoring, encouragement to excel, and top-shelf equipment as well as QA and DevOps support. Our managers will provide you with valuable direction and, together with our team leads, will guide you in your work by delivering meaningful feedback at all points along the way. You can expect to have direct responsibility for complex tasks and engage in research assignments. You will be working alongside motivated, driven, and smart professionals that recognize and appreciate smart work and personal initiative.
You Are:
Detail-oriented, analytical and accuracy driven
Self-starter, problem solver
A master at accomplishing items from your to-do list
Organized and reliable
Enthusiastic and collaborative with can-do mind-set

Accenture has a great opportunity for an ambitious Senior Data Engineer interested in making hands-on contributions to our next generation data platform and unleash the potential of our cross-channel ad management software.

You will be joining a team of world class engineers based in NYC. We write our front-end features in JavaScript and React, with some legacy code written in Coffeescript. Our back-end stack consists of Ruby on Rails, Clojure, Postgres, and Redis, with most new development in Clojure and SQL.


The Work:
You will be our ""resident"" data specialist working on projects including:
Reviewing the data ingested from multiple sources
Building the ingestion logic from S3 into the Snowflake Data Lake
Implementing advanced batch data transformations using DBT
Updating dimensional data into reporting databases for advanced analytics
Designing, building and enhancing dimensional models for fast dashboarding and reporting in Looker

Here’s What You Need:
BS/BA or Advanced degree in a relevant field (Computer Science, Engineering, or other Computational Science)
Minimum of 5 years of hands-on SQL experience in a production environment
Minimum of 5 years of experience designing and implementing data models
Minimum of 2 years designing and implementing star / snowflake schemas and dimensional models
Minimum of 1 year of experience tuning complex SQL queries
Bonus Points:
Minimum of 1 year of experience with batch data transformation frameworks, DBT preferred
Minimum of 1 year of experience working with MPP databases, Snowflake preferred
DBA experience a plus
Able to write code in one modern programming language, Python/Clojure
Can advise on the overall data strategy

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.
Accenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity:
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.
Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.
Accenture is committed to providing veteran employment opportunities to our service men and women.
Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration."
47,AI Data Scientist / Data Engineer - Experienced Associate,"New York, NY",New York,NY,None Found,None Found,"Exploring new analytical technologies and evaluating their technical and commercial viability quickly;
Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;
Testing and rejecting hypotheses around data processing and machine learning model building;
Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;
Building machine learning pipelines that ingest, clean data, and make predictions;
Staying abreast of new AI research from leading labs by reading papers and experimenting with code;
Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;
Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;
Applying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);
Understanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;
Building machine learning models and systems, interpreting their output, and communicating the results;
Moving models from development to production is a plus; and,
Conducting research in a lab and publishing work is a plus.
","Exploring new analytical technologies and evaluating their technical and commercial viability quickly;
Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;
Testing and rejecting hypotheses around data processing and machine learning model building;
Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;
Building machine learning pipelines that ingest, clean data, and make predictions;
Staying abreast of new AI research from leading labs by reading papers and experimenting with code;
Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;
Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;
Applying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);
Understanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;
Building machine learning models and systems, interpreting their output, and communicating the results;
Moving models from development to production is a plus; and,
Conducting research in a lab and publishing work is a plus.
",None Found,None Found,"Exploring new analytical technologies and evaluating their technical and commercial viability quickly;
Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;
Testing and rejecting hypotheses around data processing and machine learning model building;
Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;
Building machine learning pipelines that ingest, clean data, and make predictions;
Staying abreast of new AI research from leading labs by reading papers and experimenting with code;
Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;
Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;
Applying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);
Understanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;
Building machine learning models and systems, interpreting their output, and communicating the results;
Moving models from development to production is a plus; and,
Conducting research in a lab and publishing work is a plus.
","Line of Service
Advisory
Industry/Sector
Not Applicable
Specialism
Data and Analytics Technologies
Management Level
Associate
Job Description & Summary
PwC Labs is focused on standardizing, automating, delivering tools and processes and exploring emerging technologies that drive efficiency and enable our people to reimagine the possible. Process improvement, transformation, effective use of innovative technology and data & analytics, and leveraging alternative delivery solutions are key areas of focus to drive additional value for our firm.


The AI Lab focuses on implementing solutions that impact efficiency and effectiveness of our technology functions. Process improvement, transformation, effective use of technology and data & analytics, and leveraging alternative delivery are key areas to drive value and continue to be recognized as the leading professional services firm. AI Lab is focused on identifying and prioritizing emerging technologies to get the most out of our investments.
To really stand out and make us ﬁt for the future in a constantly changing world, each and every one of us at PwC needs to be an authentic and inclusive leader, at all grades/levels and in all lines of service. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.


As an Associate, you’ll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:


Invite and provide evidence-based feedback in a timely and constructive manner.
Share and collaborate effectively with others.
Work with existing processes/systems whilst making constructive suggestions for improvements.
Validate data and analysis for accuracy and relevance.
Follow risk management and compliance procedures.
Keep up-to-date with technical developments for business area.
- Communicate confidently in a clear, concise and articulate manner - verbally and in written form.

Seek opportunities to learn about other cultures and other parts of the business across the Network of PwC firms.
Uphold the firm’s code of ethics and business conduct.
Our team is capability centric, focusing on AI and machine learning techniques that are broadly applicable across all industries. We work with a variety of data mediums including text, audio, imagery, sensory, and structured data. Our work involves the use of supervised/unsupervised machine learning algorithms, traditional statistical models, deep neural networks, terabyte scale data, and simulation modelling. Our work is having a tremendous impact on how PwC & our clients do business.
Job Requirements and Preferences :


Basic Qualifications :


Minimum Degree Required :

Bachelor Degree


Minimum Years of Experience :

1 year(s)


Preferred Qualifications :


Preferred Fields of Study :

Computer and Information Science, Computer Engineering, Computer and Information Science & Accounting, Economics, Economics and Finance, Economics and Finance & Technology, Engineering, Mathematics, Mathematical Statistics, Statistics


Preferred Knowledge/Skills :
Demonstrates some knowledge and/or a proven record of success in the following areas:
Exploring new analytical technologies and evaluating their technical and commercial viability quickly;
Working in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;
Testing and rejecting hypotheses around data processing and machine learning model building;
Demonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;
Building machine learning pipelines that ingest, clean data, and make predictions;
Staying abreast of new AI research from leading labs by reading papers and experimenting with code;
Developing innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;
Demonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;
Applying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);
Understanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;
Building machine learning models and systems, interpreting their output, and communicating the results;
Moving models from development to production is a plus; and,
Conducting research in a lab and publishing work is a plus.
Demonstrates some abilities and/or a proven record of success learning and applying new skills quickly, including the following areas and technologies:
Programming: Python, R, Java, JavaScript, C++, Unix;
Hardware: sensors, robotics, GPU enabled machine learning, FPGAs, Raspberry Pis, etc.;
Data Storage Technologies: SQL, NoSQL, Hadoop, cloud-based databases such as GCP BigQuery, and different storage formats (e.g. Parquet, etc.);
Data Processing Tools: Python (Numpy, Pandas, etc.), Spark, cloud-based solutions such as GCP DataFlow;
Machine Learning Libraries: Python (scikit-learn, genism, etc.), TensorFlow, Keras, PyTorch, Spark MLlib;
Visualization: Python (Matplotlib, Seaborn, bokeh, etc.), JavaScript (d3); and,
Productionization and containerization technologies: GitHub, Flask, Docker, Kubernetes.
Education (if blank, degree and/or field of study not specified)
Degrees/Field of Study required:
Degrees/Field of Study preferred:
Certifications (if blank, certifications not specified)
Desired Languages (If blank, desired languages not specified)
Travel Requirements
Up to 40%
Available for Work Visa Sponsorship?
Yes
Government Clearance Required?
No
Job Posting End Date"
48,Data Engineering - Associate,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"COMPANY OVERVIEW

At TheMathCompany, we enable viable and valuable data and analytics transformations for our clients. Our mission is to help Fortune 500 organizations build core capabilities that set them on a path to achieve analytics self-sufficiency. We are changing the way companies go about executing enterprise-wide data engineering and data science initiatives, by defining and delivering comprehensive and robust analytics engagements. With a holistic range of services across data engineering, data science and management consulting, we are in the business of disrupting the analytics services and product space. To help us achieve our objectives, we are looking for passionate and experienced practitioners to be part of our US organization, and be part of the growth story of one of the fastest AI/ML startups in the world.

We, as an organization, are committed to your personal success and professional development. TheMathCompany offers and supports our employees' development of their personal brand through professional experiences, best in-class learning opportunities, inclusion, collaboration and personal well-being.

ROLE DESCRIPTION

We are looking for a savvy Big Data Engineer that will be part of our growing team of analytics experts. You will be responsible for collecting, storing, processing, optimizing and analyzing large datasets for our clients. You will have the opportunity to design, implement and monitor optimal data architecture solutions while effectively integrating them with the existing data ecosystem. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Candidates must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

As an Associate, you will:

Create, test, optimize and maintain large-scale data processing systems, data warehouses and data lakes that ingest, aggregate and visualize terabytes of data to meet our clients’ business requirements and solve their toughest challenges
Work with users and other stakeholders to refine analytical requirements for quantitative data, qualitative data and unstructured data
Design and develop schema definitions to enable integration, aggregation and consumption of disparate data sources for analysis
Drive adoption of new technologies, tools, and process improvements to build world class analytical capabilities for our clients
Develop high-performance, scalable implementations of the statistical/machine learning models developed by our Data Scientists
REQUIRED QUALIFICATIONS

Bachelor’s degree in a relevant field or 2 to 4 years’ experience in developing Data Models, DB schemas, ETLs
1+ years’ experience with data ingestion through batch and streaming methodologies using open source or public cloud tools like Kafka, Airflow, Azure Data Factory.
1+ years’ experience working in one of three big public cloud ecosystems (i.e AWS, Azure or GCP)
Experience with databases - both RDBMS and NoSQL (Vertica, Netezza or Oracle and AWS data services tech)
Working knowledge of SQL (any variant)
Good understanding of Data Warehouse methodologies
Hands on experience in any of the programming languages (Shell scripting, Python, Scala, Java, etc
PREFERRED QUALIFICATIONS

Knowledge of Big Data ecosystem like Hadoop M/R, Pig and Hive is a strong plus
Understanding of IN memory distributed computing frameworks like Spark (and/or DataBricks) and its parameter tuning, optimized query writing
Scheduling and Monitoring of Hadoop and Spark jobs
Good understanding of reporting tools, such as Tableau, Pentaho or Jasper
Experience in design, development and deployment of one or more tools - ETL (Informatica, OWB, ODI), reporting (Business Objects, QlikView, Tableau)"
49,Software Data Engineer/Data Science,"New York, NY 10020",New York,NY,10020,None Found,None Found,None Found,"
Create and maintain data pipeline architectures for providing a real time and batch processing platform for all models to run on
Create and maintain APIs for our machine learning models
Coordinating the movement of data between data sources in cloud environments (streaming and batch)
Assemble large, complex data sets that meet functional / non-functional business requirements",None Found,"
4+ years' experience developing, maintaining, and testing APIs & infrastructure for data generation.
Experience with big data processing: Flink, Spark, Kafka, etc.
Experience with different databases, such as Redis, Elasticsearch, Postgres or Cassandra.
Strong understanding of one of: Python, Java, or Scala
Experience with CI/CD infrastructure and a strong supporter of unit / integration testing","(We are not sponsoring for this role or in the future)
At Fareportal, we create technology that is driving innovation in the travel industry - one of the world's fastest-growing sectors. Our employees are the core of our organization and together we're revolutionizing the way people book travel.
Our portfolio of brands including CheapOair and OneTravel receive over 100 million visitors annually and drive over $4 billion in annual revenue.
We are looking for a Software Data engineer to join our team. You will be handling hundreds of millions of events per day, responsible for building out the data tooling and processes to support the creation of machine learning models and data science insights that will drive our business.
The ideal candidate will participate in the design and implementation of the entire data pipeline, from capturing and storing data to streaming and processing the data and making it available to the organization.
We are passionate about making data-driven decisions and you will have the opportunity to shape the team's direction and create large impact.
Our team loves Python and Scala (and is not afraid of Functional Programming) and we strongly encourage DevOps approaches.
Responsibilities:
Create and maintain data pipeline architectures for providing a real time and batch processing platform for all models to run on
Create and maintain APIs for our machine learning models
Coordinating the movement of data between data sources in cloud environments (streaming and batch)
Assemble large, complex data sets that meet functional / non-functional business requirements
Our ideal candidate:
Who You Are
You are smart and love to build systems that are well tested as well as flexible
You like being around smart people who will challenge you on a daily basis.
You love to ramp up on new technologies to build awesome things with us!
Requirements
4+ years' experience developing, maintaining, and testing APIs & infrastructure for data generation.
Experience with big data processing: Flink, Spark, Kafka, etc.
Experience with different databases, such as Redis, Elasticsearch, Postgres or Cassandra.
Strong understanding of one of: Python, Java, or Scala
Experience with CI/CD infrastructure and a strong supporter of unit / integration testing
RxUsEWY7gZ"
50,Data Engineer,"New York, NY 10018",New York,NY,10018,None Found,None Found,None Found,None Found,None Found,None Found,"Crossix is looking for a Data Engineer that will work on collecting, storing, processing, and managing data sets for scalable media measurement product solutions. Your primary focus will be on designing systems for managing solutions to use for these purposes, then maintaining, monitoring, and training operations staff on them. You will also assist with ad hoc data engineering projects and mentoring of more junior members as we build out engineering capabilities in our NYC office.

What You'll Do


Design, implement and improve cloud and on-premise ETL/ELT processes to onboard data into our data lake from a variety of internal and external sources.
Monitor performance and cost, advising any necessary infrastructure/process changes to optimize.
Keep up to date on emerging technology solutions that impact the data and cloud computing domains, in particular on AWS.
Support analytics teams in the NYC office with regards to use of tools, resources and platforms to achieve success and meet delivery commitments.

What You've Done

2+ years of hands-on experience/working knowledge in the following:


Usage of AWS for data-warehousing and processing. In particular, the following services are most relevant:
Redshift
S3
RDS / Aurora
Lambda
SQS
Kinesis
Scripting tasks using Python (strongly preferred) or a similar language.
Complex data structure, data processing, data quality and data lifecycle.
Operational management / DBA of DBMS servers (Oracle, MySQL or MSSQL)
Data platforms and data modeling frameworks.
Superior problem solving and troubleshooting.
Excellent verbal and written communication.
Strong attention to detail and highly organized.

AWS certification, or progress toward, at associate level (Solutions Architect or Developer), or specialty (Big Data) a strong advantage.

About the Team – Crossix delivers hard-to-come-by insights that enable healthcare marketers to plan, measure, and optimize their marketing campaigns with confidence. Using our own proprietary technology and network of health and non-health data, our analyses pinpoint the tactics, programs, and channels that improve performance and boost sales, enabling better healthcare communications. And we do it all while protecting consumer privacy.

Leadership – With decades of combined experience and an unrivaled track record of healthcare innovation, our leadership team sets the standard for us. Their knowledge and expertise continually challenge us and the industry – through their work, their speaking engagements at conferences and their thought leadership published in the top industry publications.

Culture – We know that our employees set us apart. Along with competitive salaries and benefits, we invest in creating compelling opportunities for professional development and career growth. We also believe that diversity is essential to building an environment where everyone can feel they belong. We're continuously building an inclusive company where everyone feels welcome and heard. Come join our rapidly growing team!

We are an equal opportunity employer and welcome all qualified applicants regardless of race, color, religion, sex, gender identity, sexual orientation, marital status, ancestry, national origin, age, disability, genetic information, or veteran status."
51,Software Data Engineer,"New York, NY 10004",New York,NY,10004,None Found,None Found,"
At least 1-2 years of professional experience programming in Python
Exposure to ETL/ELT pipeline automation
Exposure to basic database concepts
",None Found,"
STEM Bachelor's required, graduate degree is a big plus
",None Found,"---------------------

1010data values:
---------------------

Integrity: Doing the right things for the right reasons

Agility: Adapting and thriving in a dynamic environment

Teamwork: Combining our strengths to do amazing things

Passion: Channeling enthusiasm to drive excellence

Creativity: Unleashing curiosity to defy the norm

---------------

About the role:
---------------

As a Software Data Engineer at 1010data, you will be responsible for designing, maintaining, and optimizing large-scale automated ELT processes. Working actively with data scientists and analysts specializing in enterprise data warehousing, you will leverage industry-standard data orchestration tools as well as in-house proprietary scheduling and automation tools to create efficient and reliable ELT jobs which support 1010data's product offerings and data warehousing needs for our customers. As we incorporate more cloud technologies into our processes, you will be at the forefront of exploring and defining best practices, and helping us transition our products to be more scalable.

As part of the onboarding process, you will learn about 1010data's proprietary technology stack. Our query engine, query language, database, and data storage layer were all developed and fine-tuned in-house over the lifetime of the company. ELT processes heavily rely on these components, whether they are written in Python and Airflow , K, or our proprietary data orchestration tools. You will be formally trained in the latter as a new 1010data employee. The concepts should be familiar to anyone with exposure to database techniques like normalization/indexing/partitioning, MapReduce, columnar database architecture and distributed systems.

----------------------

What you will take on:
----------------------


Taking end-to-end ownership of data products and custom solutions for our clients
Coordinating with the systems, core, data science, and analytics teams to build and maintain data products and custom solutions for our clients
Designing and writing automated scripts to preprocess terabytes of data from our partners/clients
Designing and writing new enterprise-scale ELT/ETL workflows from scratch in Python using Airflow, Docker, Kubernetes, AWS, etc.
Modifying/redesigning legacy ELT/ETL processes to leverage cutting-edge open source and proprietary technologies
Ensuring quality, reliability and uptime for critical automated processes
Migrating our products and processes into the cloud while drastically reducing our in-house data center footprint

----------------------

What you already have:
----------------------

Required Skills:

At least 1-2 years of professional experience programming in Python
Exposure to ETL/ELT pipeline automation
Exposure to basic database concepts

Preferred Skills:

Good understanding of Data Engineering, NoSQL databases and database design, distributed systems and/or information retrieval
Knowledge of Apache Airflow
Familiarity with functional/vector programming
DBA experience
Ability to plan and collect requirements for projects, and interact with the analyst and data science teams

Education:

STEM Bachelor's required, graduate degree is a big plus

---------------

About 1010data:
---------------

1010data travels at the speed of thought to make Big Data discovery easy; we power sub-second responses to analyses run on billions of rows of data. 1010data is defining the way the world interacts with data. Come be a part of it. Come do powerful things with data.

An essential tool to more than 850 of the world's top retail, manufacturing, telecom, government and financial services enterprises including The New York Stock Exchange, Dollar General, P&G, and RiteAid; the 1010data platform is a highly differentiated product that is becoming the industry standard for Big Data Discovery and Data Sharing.

With more than 30 trillion rows of data in our private cloud, 1010data is designed to scale to the largest volumes of granular data, the most disparate and varied data sets, and the most complex advanced analytics. All while delivering lightning-quick system performance.

1010data is an equal opportunity employer. We embrace humans of every background, appearance, race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, and disability status."
52,Innovations - Data Engineer,"New York, NY 10154",New York,NY,10154,None Found,"Undergraduate or graduate degree in a technical or scientific field, such as Computer Science, Engineering, Mathematics, or similar
2-5 years professional experience as a data engineer, software engineer, data analyst, data scientist, or related role
Analytically-minded and detail-oriented: you actually like staring at data, looking for patterns and outliers, establishing data models, and rigorously answering questions
Expertise in data engineering languages such as Python, Java, Scala, SQL.
Data modeling and data governance experience; you've designed and implemented a data mart, a data warehouse, or the back-end database of an application
Experience building ETL and data pipelines, especially via code-oriented systems like Spark, Airflow, Luigi, or similar, and with varied data formats
Cloud-oriented but comfortable with on-premises infrastructure
Experience operating in a secure networking environment (e.g. behind a corporate proxy) is a plus
Creative problem-solving skills, especially in situations where ""nobody has tried this before""
Excellent technical documentation and writing skills: you know Markdown syntax cold, and have published API documentation or similar
You're not satisfied with ""close enough"" solutions, and you design long-term solutions that are robust over time
You have a bias towards automation: ""one-time scripts"" eat away at your soul a little bit each time you write one
Proficiency in statistics and machine learning is a nice-to-have, and interest in learning these is even better!
Familiarity with visualizing data with Tableau and similar tools
Great customer service and technical troubleshooting skills
",None Found,"Full-stack design, development, and operation of core data stack including data lake, data warehouse, and data pipelines
Build data flows for data acquisition, aggregation, and modeling, using both batch and streaming paradigms
Consolidate/join datasets to create easily consumable, consistent, holistic information
Design and implement machine learning models and prediction APIs, and ensure their operational performance over time
Empower data scientists and data analysts to be as self-sufficient as possible by building core systems and developing reusable library code
Support and optimize desktop and cloud environments for data scientists and data analysts
Ensure efficiency, quality, resiliency of data science core systems
Work with senior technical staff throughout Blackstone's portfolio companies to develop data flows
",None Found,None Found,"Job Description:
Employer:

Blackstone
Firm Overview:

Blackstone is one of the world’s leading investment firms. We seek to create positive economic impact and long-term value for our investors, the companies we invest in, and the communities in which we work. We do this by using extraordinary people and flexible capital to help companies solve problems. Our asset management businesses, with $512 billion in assets under management, include investment vehicles focused on private equity, real estate, public debt and equity, non-investment grade credit, real assets and secondary funds, all on a global basis. Further information is available at www.blackstone.com. Follow Blackstone on Twitter @Blackstone.
Business Unit:

Innovations
Business Unit Overview:

Blackstone Innovations (BXi) is the technology team at the core of each of Blackstone’s businesses and new growth initiatives. Serving both internal and external clients, we work to build the next generation of systems that manage risk, create efficiency and improve transparency within the firm and across our broad community of investors and portfolio companies.

BXi is nimble and entrepreneurial – our open, iterative design processes and rapid pace of development mean that everyone on the team has the opportunity to make an impact from day one. We are problem solvers who can take projects from idea to implementation. We believe in active mentoring and developing excellence. We collaborate to find the best answers for our customers and for Blackstone. We are critical to the firm maintaining its competitive edge.
Job Title:

Data Engineer
Job Description:

Blackstone is looking to hire a passionate data engineer to be an integral member of the Data Science and Engineering Team within Innovations, the firm's technology team. We are a new and growing team using data science and data engineering to provide a competitive advantage to our investment professionals and management teams, allowing Blackstone to be a more efficient investor and owner of assets.

As a data engineer, you'll design, implement, and extend core systems that enable data science and data visualization at one of the world's leading investment firms. These systems include data lake, data warehouse, and data pipelines, as well as platform tools that help data scientists and data analysts throughout the firm and Blackstone's portfolio companies. The team also owns Blackstone's broad and growing self-service data visualization stack, including the design, development, and curation of golden-copy analytics data sources.

Our data infrastructure is nascent, growing, and constantly improving. We primarily use Python and SQL, AWS cloud services, Snowflake data warehouse, and Airflow for workflow orchestration. We visualize data with Tableau. Our development environment leverages Vagrant and Docker. We practice infrastructure-as-code with Terraform, and we build continuous integration and deployment pipelines with Gitlab and TeamCity.
Key Responsibilities:
Full-stack design, development, and operation of core data stack including data lake, data warehouse, and data pipelines
Build data flows for data acquisition, aggregation, and modeling, using both batch and streaming paradigms
Consolidate/join datasets to create easily consumable, consistent, holistic information
Design and implement machine learning models and prediction APIs, and ensure their operational performance over time
Empower data scientists and data analysts to be as self-sufficient as possible by building core systems and developing reusable library code
Support and optimize desktop and cloud environments for data scientists and data analysts
Ensure efficiency, quality, resiliency of data science core systems
Work with senior technical staff throughout Blackstone's portfolio companies to develop data flows
Qualifications:
Undergraduate or graduate degree in a technical or scientific field, such as Computer Science, Engineering, Mathematics, or similar
2-5 years professional experience as a data engineer, software engineer, data analyst, data scientist, or related role
Analytically-minded and detail-oriented: you actually like staring at data, looking for patterns and outliers, establishing data models, and rigorously answering questions
Expertise in data engineering languages such as Python, Java, Scala, SQL.
Data modeling and data governance experience; you've designed and implemented a data mart, a data warehouse, or the back-end database of an application
Experience building ETL and data pipelines, especially via code-oriented systems like Spark, Airflow, Luigi, or similar, and with varied data formats
Cloud-oriented but comfortable with on-premises infrastructure
Experience operating in a secure networking environment (e.g. behind a corporate proxy) is a plus
Creative problem-solving skills, especially in situations where ""nobody has tried this before""
Excellent technical documentation and writing skills: you know Markdown syntax cold, and have published API documentation or similar
You're not satisfied with ""close enough"" solutions, and you design long-term solutions that are robust over time
You have a bias towards automation: ""one-time scripts"" eat away at your soul a little bit each time you write one
Proficiency in statistics and machine learning is a nice-to-have, and interest in learning these is even better!
Familiarity with visualizing data with Tableau and similar tools
Great customer service and technical troubleshooting skills
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, disability, sexual orientation, national origin or any other category protected by law.
If you need a reasonable accommodation to complete your application, please contact Human Resources at 212-583-5000 (US), +44 (0)20 7451 4000 (EMEA) or +852 3656 8600 (APAC).
The Blackstone Group and its affiliates provide equal employment opportunity to all qualified employees and applicants for employment regardless of race, color, creed, religion, sex, pregnancy, national origin, ancestry, citizenship status, age, marital or partnership status, sexual orientation, gender identity or expression, disability, genetic predisposition, veteran or military status, status as a victim of domestic violence, a sex offense or stalking, or any other classification prohibited by applicable law.
To submit your application please complete the form below. Fields marked with a red asterisk * are required in order to enter into a possible employment contract (although some can be answered "" prefer not to say ""). Failure to provide this information may compromise the follow-up of your application. When you have finished click Submit at the bottom of this form."
53,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"As a Data Engineer, you'll help design and implement our mission-critical data pipeline and warehousing solution that seamlessly enables mass-scale, highly complex and data intensive operational service alongside cutting-edge algorithmic and analytics research for our full-scale, global ride-sharing service.

Responsibilities and Duties


Defining and implementing effective data scaling infrastructure solutions based on best-in-class standard tools
Driving speed, simplicity and robustness in production deployments.
Implementing data-rich system components at the core of system scaling.
Designing the ETL procedures to transform the concepts you design into reality.

Qualifications


Have 2 years of experience as a data/software engineer developing production quality software
Have a strong track-record of building, launching, and interacting with data pipelines at scale in both a streaming and batch fashion
Have experience deploying solutions on cloud-based platforms
Are proficient in Python, SQL, and other scripting languages
Concise: you get stuff done, in a simple, elegant way
Fast learner: you can quickly master concepts, disciplines and methods
Team worker: you understand the goals of the team and work towards them
Pragmatic: you hear about a difficult problem and are able to transform it into a simple one (or more) that you can easily solve

At Via, we're on the cutting edge of mobility. We're building revolutionary technology that's changing the way people get around. It's on-demand transit on a mass scale, a smarter transportation that's friendly to our planet. From on-demand autonomous shuttles in Australia to dynamically routed bus fleets in Singapore, our sophisticated operating system is powering transportation in the world's biggest cities and is sought after by prominent transportation players globally. We've provided more than 50 million shared rides already, and we're growing at an astonishing rate. We have offices in more than 15 countries and deployments in more than 50 markets, with a goal of hundreds of deployments within the next two years. If you're someone who relishes wearing multiple hats, never backs down from a challenge, and loves getting things done, we'd love to hear from you!

Via offers above market compensation packages and benefits, including equity, health insurance, and relocation assistance.

Via is an equal opportunity employer."
54,"Data Scientist, Engineer","New York, NY 10018",New York,NY,10018,None Found,None Found,None Found,None Found,None Found,None Found,"Data Scientist, Engineer - (19000056)
Description

Help us create better connected futures
Make your mark. See the difference. Be recognized.
Work with aligned engaged team members.

About us

At Transit Wireless we create even better connected futures.
Ours is a story of being big enough to deliver and maintain large-scale operations but being nimble enough to make things happen.
Our future-makers thrive on rewriting yesterdays rules. So we put Wi-Fi, fiber and wireless connectivity on subways and we create new services for our customers and communities every day.
We build technology and teams that people want to be part of and we have the courage to not only imagine, but to do what really matters.

About the role
Develop statistical models to drive key core products for an innovative company in a data rich environment.
Combine real world intelligence with sensor data to drive actionable insights for our clients.
Design and develop efficient and scalable data pipelines between enterprise systems and analytics platform
Work closely with team and participate in development and deployment of machine learning models and feature engineering pipelines
Provide technical expertise in the area of design and implementation of Ratings Integrated Data Facility with modern AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and Spark
Build and maintain a data environment for speed, accuracy, consistency and ‘up’ time
Support analytics by building a world-class data lake environment that empowers analysts to determine insights into revenue and power products across the organization
Ensure data governance principles adopted, date quality checks and data lineage implemented in each hop of the data
Be in tune with emerging trends Big data and cloud technologies and participate in evaluation of new technologies
Ensure compliance through the adoption of enterprise standards and promotion of best practice / guiding principles aligned with organization standards

About you
Demonstrated ability to distill data-driven insights through informative visualizations & executive level communications
Experience with geospatial datasets & tool-chains
Experience designing a measurement strategy to evaluate the performance of complex systems against challenging requirements
Deep understanding of statistics and/or applied math as basis for choosing techniques & experiments
Ability to develop D&A related Microservices in mainstream cloud infrastructure
Lead the data science and data engineering team in a cross-functional work environment
Lead/mentor the team to design, develop and maintain D&A solutions
BS or MS degree in Computer Science or Information Technology
8+ years of experience as data engineer at an innovative organization
4+ years of hands-on experience in implementing data lake systems using AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and Spark
Expert managing AWS services (EC2, S3, Route 53, ELB, VPC, cloudwatch, Lambda) in a multi account production environment
Experience With Machine Learning Frameworks, such as TensorFlow, PyTorch, H2O, scikit-learn, Theano, Caffe or Spark MLib is an added advantage
Exposure to R, SparkR, SparklyR or Other R packages is a plus
Experience in constructing fast data staging layers to feed machine learning algorithms
Experience in building data APIs to consume analytic model output
Familiarity with machine learning model training and deployment process is a plus
Experience with development frameworks as well as data and integration technologies such as Python, Scala or Informatica
Expert knowledge of Agile approaches to software development and able to put key Agile principles into practice to deliver solutions incrementally.
Monitors industry trends and directions; develops and presents substantive technical recommendations to senior management
Excellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners
Ability to prioritize and manage work to critical project timelines in a fast-paced environment

Why work with us

We are a courageous culture and we do what really matters, putting people, customers and communities at the heart of every decision.
We believe there is strength in diversity and opportunity through inclusion.
And we provide a learning environment, meaningful recognition for your contribution and competitive compensation.
Help us imagine and create what’s next. Join us.

Primary Location: US-NY-New York
Work Locations: TW - Transit Wireless, LLC 1350 Broadway 3rd Floor New York 10018
Job: Product Development
Organization: Transit Wireless (USD)
Schedule: Full-time
Shift: Day Shift
Employee Status: Regular
Job Type: Standard
Job Level: Individual Contributor
Job Posting: 17-Sep-19, 9:05:14 PM"
55,Senior Data Engineer / Researcher,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,"
Identifying, ingesting, and enriching a wide range of structured and unstructured big data into datasets for analysis;
Operating and extending the data infrastructure platform to deliver production-grade data curation and analysis services;
Thinking and acting as data integrity managers - amplifying data quality and completeness with a process-driven approach and measurement dashboards;
Owning end-to-end data workflows and developing deep domain expertise on the underlying actors and behaviors manifested through data;
Communicating data-driven analysis and insights in the form of “golden triangle” investment insights that supports our clients investment process.",None Found,"
Master’s degree or PHD in Mathematics, Finance, Computer Science, Engineering or related fields.
5+ years of experience working with large structured and unstructured datasets.
Expertise in Python and data analysis tools and languages (PyData, R, Julia, Matlab, Tableau).
Expertise in SQL and relational databases.
Deep intellectual curiosity and passion for data.
Excellent problem solving, communication, and analytical skills.
Eagerness to work in an evolving and fast-paced environment.
While financial industry experience is a plus, we are open-minded in our search for critical thinkers who are passionate about technology and data.
Proven track record of working with and managing junior researchers and developers","Responsibilities:
Identifying, ingesting, and enriching a wide range of structured and unstructured big data into datasets for analysis;
Operating and extending the data infrastructure platform to deliver production-grade data curation and analysis services;
Thinking and acting as data integrity managers - amplifying data quality and completeness with a process-driven approach and measurement dashboards;
Owning end-to-end data workflows and developing deep domain expertise on the underlying actors and behaviors manifested through data;
Communicating data-driven analysis and insights in the form of “golden triangle” investment insights that supports our clients investment process.
Requirements:
Master’s degree or PHD in Mathematics, Finance, Computer Science, Engineering or related fields.
5+ years of experience working with large structured and unstructured datasets.
Expertise in Python and data analysis tools and languages (PyData, R, Julia, Matlab, Tableau).
Expertise in SQL and relational databases.
Deep intellectual curiosity and passion for data.
Excellent problem solving, communication, and analytical skills.
Eagerness to work in an evolving and fast-paced environment.
While financial industry experience is a plus, we are open-minded in our search for critical thinkers who are passionate about technology and data.
Proven track record of working with and managing junior researchers and developers

If you are interested in any of these opportunities, please use the form below to contact our recruiting team or email us at:
careers@altdg.com"
56,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Hello, World! Codecademy has helped over 45 million people from around the world upgrade their careers with engaging, accessible, and flexible education on programming and data skills. We provide over 200 hands-on interactive lessons ranging from Python to R to Javascript and everything in between. Our learners have gone on to start companies, new jobs, and new lives thanks to what they've learned with Codecademy, and we're thrilled to be working to take that impact to the next level.

Codecademy was started in 2011 by two college students in a dorm room at Columbia that were frustrated by the huge gap between education and employment. A few years later, we are a rapidly growing, diverse team of 75+ in SoHo, NYC. We've raised over $40m in venture capital funding from top investors including Union Square Ventures, Kleiner Perkins, Naspers, Y Combinator, and more.

If you want to help build a business that impacts tens of millions of people each year and helps them lead better lives, join us!

Codecademy's Infrastructure & Services team is responsible for the infrastructure and operations to deliver our service to millions of users learning to code. We are looking for a Senior Data Engineer to join this team; in this role you will also work closely with our Data Science team. In this role you will take ownership of our event based architecture and data warehousing. You will also need to assess and devise an effective end to end data infrastructure. The ideal candidate will be comfortable with Amazon Redshift, ETLs, and MongoDB.

--------------
WHAT YOU'LL DO
--------------


Build scalable data infrastructure solutions.
Design and optimize new and existing data pipelines.
Integrate new data sources into our existing data architecture.
Collaborate with a cross-functional team of software engineers and data scientists.

----------------
WHAT YOU'LL NEED
----------------


Hands-on experience building and maintaining large scale ETL systems.
Deep understanding of database design and data structures.
Fluency in one of the following languages: Python, Java, Scala.
Experience working with cloud-based data platforms (we use AWS).
SQL and data warehousing skills -- able to write clean and efficient queries.
Ability to make pragmatic engineering decisions in a short amount of time
Strong project management skills; a proven ability to gather and translate requirements from stakeholders across functions and teams into tangible results.

----------------------------
WHAT WILL MAKE YOU STAND OUT
----------------------------


Experience with tools in our current warehousing stack: Apache Airflow, Redshift, Segment, Kinesis, S3, Looker.
Familiarity with the database technologies we use in production: MongoDB, PostgreSQL
Comfort with containerization technologies: Docker, Kubernetes, etc.
Experience (or interest in learning to) productionizing machine learning models.

At Codecademy, we are committed to teaching people the skills they need to upgrade their careers. Codecademy aims to educate a richly diverse demographic of users with our product and in order to accomplish this, we believe our team should reflect that rich diversity. Our company celebrates diversity in all of its forms-- race, gender, color, national origin, marital status, sexuality, religion, veteran status, age, ability, disability status-- and works to create an inclusive workplace where people of all backgrounds and beliefs are empowered to better their futures."
57,Data Science / Machine Learning / Big Data Engineer - Lead,"New York, NY 10179",New York,NY,10179,None Found,None Found,None Found,None Found,None Found,None Found,"As an experienced member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation & engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You’ll work in a collaborative, trusting, thought-provoking environment—one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally

This role requires a wide variety of strengths and capabilities, including:
BS/BA degree or equivalent experience
Expertise in application, data and infrastructure architecture disciplines
Advanced knowledge of architecture, design across all systems
Proficiency in multiple modern programming languages: Python is a must, Java, Scala, C++; Experience in Big Data Technology with Hadoop, SparkAbility to work in large, collaborative teams to achieve organizational goals, and passionate about building an innovative culture
Invent creative and innovative ways to answer key business questions by leveraging existing data assets or creating new ones. We move 20% of US$ around the globe every day. The possibilities with this data are endless.Collaborate with product managers, data scientists, engineers, end users, and other stakeholders to integrate data discoveries and processes into operational capabilitiesBe a data storyteller, deliver practical data insights in a compelling manner to senior leadership. Articulate findings clearly and concisely including presentations, discussions and visualizations.Experience in writing Machine Learning Algorithms at scale and Time Series Modeling.Experience with data management, manipulating large data sets through statistical software and data warehousing environments processing large volume of transactions.

Highly Desirable Skills:
SparkML, TensorFlow, Panda, NumPy, scikit-learn, NLTK, PySpark, R, Jupyter, Tableau, JPMC Athena, Impala, big data experience, data science and machine learning experience.

CIB (Corporate & Investment Bank)
Our Corporate & Investment Bank relies on innovators like you to build and maintain the technology that helps us safely service the world’s important corporations, governments and institutions. You’ll develop solutions for a bank entrusted with holding $18 trillion of assets and $393 billion in deposits. The Corporate & Investment Bank provides strategic advice, raises capital, manages risk, and extends liquidity in markets spanning over 100 countries around the world.

When you work at JPMorgan Chase & Co., you’re not just working at a global financial institution. You’re an integral part of one of the world’s biggest tech organizations. In our global technology centers, our team of 50,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $11B annual investment in technology enables us to hire people to create innovative solutions that are transforming the financial services industry.

At JPMorgan Chase & Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you’re looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you."
58,Sr Data Engineer,"New York, NY 10017",New York,NY,10017,None Found,"10+ years of experience working in Data Engineering or Data WarehousingHands-on experience with leading commercial Cloud platforms, including AWS, Azure, or GoogleExperience leading data warehousing, data ingestion, and data profiling activitiesAdvanced SQL & Python skillsStrong aptitude for learning new technologies and analytics techniquesHighly self-motivated and able to work independently as well as in a team environmentUnderstanding of agile project approaches and methodologiesExperience working with Business Stakeholders to elicit business requirementsExperience building and migrating complex ETL pipelinesFamiliarity with or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)Bachelor's degree in Business Analytics, Computer Science or a closely related field required",None Found,Lead a team to develop Cloud enabled Data and Analytics solutionsDrive the development of cloud-based and hybrid data warehouses & business intelligence platformsBuild Data Pipelines to ingest structured and Unstructured Data.Gain hands-on experience with new data platforms and programming languages,None Found,None Found,"Caserta is a best-in-class Data Analytics consulting and implementation firm known for its bold solutions to the toughest data challenges. Leading organizations including world-class Financial Services, Healthcare, Media, Ad-tech and Universities, turn to Caserta for the right answer to effect change and realize business goals. We specialize in transformative strategic consulting in the areas of business intelligence, data intelligence and artificial intelligence. Renowned for our advanced technical implementation of data architecture, data engineering and data science, Caserta provides meticulous customer service and is dedicated to solving our clients' toughest data challenges with the right answers.
Description:
As a Sr. Data Engineer at Caserta, you will work in teams to deliver innovative solutions on Amazon Web Services, Azure, and Google Cloud using core cloud data warehouse tools like Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in data and analytics.
Responsibilities:
Lead a team to develop Cloud enabled Data and Analytics solutionsDrive the development of cloud-based and hybrid data warehouses & business intelligence platformsBuild Data Pipelines to ingest structured and Unstructured Data.Gain hands-on experience with new data platforms and programming languages
Qualifications:
10+ years of experience working in Data Engineering or Data WarehousingHands-on experience with leading commercial Cloud platforms, including AWS, Azure, or GoogleExperience leading data warehousing, data ingestion, and data profiling activitiesAdvanced SQL & Python skillsStrong aptitude for learning new technologies and analytics techniquesHighly self-motivated and able to work independently as well as in a team environmentUnderstanding of agile project approaches and methodologiesExperience working with Business Stakeholders to elicit business requirementsExperience building and migrating complex ETL pipelinesFamiliarity with or strong desire to learn quantitative analysis techniques (e.g., predictive modeling, machine learning, segmentation, optimization, clustering, regression)Bachelor's degree in Business Analytics, Computer Science or a closely related field required
CKiDsoZCwF"
59,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"FanDuel Group is an innovative sports-tech entertainment company that is changing the way consumers engage with their favorite sports, teams, and leagues. The premier gaming destination in the United States, FanDuel Group consists of a portfolio of leading brands across gaming, sports betting, daily fantasy sports, advance-deposit wagering, and TV/media, including FanDuel, Betfair US, DRAFT, and TVG. FanDuel Group has a presence across 45 states and 8 million customers. The company is based in New York with offices in California, New Jersey, Florida, Oregon, and Scotland.

Our competitive edge comes from making decisions based on accurate and timely data. As a Data Engineer, you will help us build scalable systems to provide access to that data across the company.

What we're looking for

We are looking for an experienced Data Engineer, ideally well versed in Python, with a deep understanding of large scale data handling and processing best practices in a cloud environment. You should be comfortable building complex yet performant SQL queries on large data sets. Our current stack is built on AWS with Spark and Hive on Amazon EMR for batch processing and Redshift for the data warehouse. Experience working with and tuning these for large scale workloads would be a plus.

Data is a key component of the business used by almost every facet of the company including product development, marketing, operations and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability. We operate a rigorous code review process, so you need to be able to continuously give and take feedback and act on it.

As our data is always growing it is important that we have a cost effective data warehouse with data that is modelled to suit our users needs.

Looking ahead to the next phase of our data platform we are keen to do more more with real time data processing and working with our data scientists to create machine learning pipelines. We would love to hear how you have tackled these before.

What you get in return

Beyond working with such a great team?


An exciting environment with real growth
Contribute to exciting products used by a highly passionate user base
Personal learning and development opportunities
Flexible holiday allowance
401K plan with company match
Attractive health insurance premiums

There's more, but we don't want to go on and on.

FanDuel is an equal opportunities employer. Diversity and inclusion in FanDuel means that we respect and value everyone as individuals. We don't tolerate bias, judgment or harassment. Our focus is on developing employees so that they reach their full potential."
60,Data Engineer,"New York, NY 10036",New York,NY,10036,None Found,None Found,None Found,None Found,None Found,None Found,"About US

ERGO is a marketing technology company that drives the relevance of what’s inside an email with Smart Content. The best way to understand what we do is to imagine that you’re reading your favorite magazine, and with every page you turn, the magazine learns to anticipate your needs and interests and adapts with relevant content.

Our platform leverages big data to drive our Smart Content modules in digital channels. We use our technology platform and professional-services offering to help grow Fortune 1000 brands.

Please apply with your resume and a thoughtful cover letter telling us why you're the best fit for the job.


About the Role

The Data Engineer is responsible for developing, maintaining, testing and evaluating data solutions systems in order to load, transform and query large data sets from a variety of sources.

Design, develop and optimize database queries
Work with our Data Scientists to create data-driven insights and reports for senior management
Assist Data Scientists in developing processes to migrate data from various formats and data sources (Oracle, MySQL, Sybase, flat files, etc.) to database architecture
Design and implement tools to analyze very large data set of raw data
Process unstructured data into a form suitable for analysis
Perform ad hoc data updates and other follow-on data services
Automate recurring analytics reports for clients


What the Role Requires

Bachelor’s degree or higher in Computer Science, Information Systems or related field
Highly proficient in SQL (MySQL, Postgres)
Expertise with relational databases (implementation, queries, modeling)
Working knowledge of distributed data stores (Cassandra, Redshift, Hadoop, HBase)
Proficient in scripting language of choice (Python, R, PHP, Ruby)
Expertise with optimizing query performance
Familiarity with NoSQL technologies (Mongo DB, DynamoDB)
Deep understanding of data structures and schema design
Detail-oriented, proactive problem solving skills


Perks

Choice of Blink Gym or Citibike membership
Bagel Mondays
Working From Home Fridays
Total of 23 PTO package that includes things like: company shut down between Christmas and New Years, three 4-day weekends on the summer holidays, and a 4-day Thanksgiving break. This also includes 10 PTO days to use at your convenience.
Office bar cart, daily produce, healthy and unhealthy snacks, Nespresso
Pool table
Health, vision & dental benefits
401k plan with matching
Pet-friendly office
Referral bonus program
A comprehensive career development program and training
Fun company outings and events
Very cool industrial-style West Village studio space


The best perk? Our people. If you’re ready to join us, apply now!"
61,Data Engineer,"New York, NY",New York,NY,None Found,None Found,"
Bachelor's degree in Computer Science, Information Systems, or related field
5+ years of experience in data engineering or data infrastructure role
3+ years of experience with Spark, Hadoop, Airflow, Kafka, etc
Proficient in data modeling and system design skills
Advanced experience of SQL and Python (data analysis libraries like pandas, numpy, scikit-learn, etc)
Proficient experience with cloud such as Azure, GCP or AWS (Azure is target platform)
Proficient experience in building and maintaining data processing pipelines
Good understanding of SQL/NoSQL databases such as Cosmos DB, Postgres, MongoDB, MySQL etc.
Experience with version control(git), CI/CD (Microsoft Azure Dev Ops, Jenkins)
Experience with supporting Data Science teams on feature engineering, model training and deployment tasks
Comfortable in Windows/Linux environment
Strong business communication skills
Strong drive to constantly learn and keep up to speed with the new technologies
Ability to understand complex business priorities and translate them into clearly defined technical/data specifications for implementation
Ability to deal with ambiguity and work with rapidly changing business data",None Found,None Found,None Found,None Found,"Munich Re is one of the world's leading reinsurance companies. We are working on topics today that will concern the whole society tomorrow, whether that be climate change, major construction projects, gene technology or space travel.

Munich Engine is a strategic innovation initiative of Munich Re which operates in two locations, New York City and Munich. Our goal is to rethink underwriting and create an advanced next generation risk assessment engine. Munich Engine combines traditional actuarial methods with new machine learning capabilities.

Munich Engine is designed and built based on modern software engineering paradigms, has just successfully finished the piloting phase and now starts to scale. We believe that we are building on the future of insurance.

We are looking for a Data Engineer, dedicated to product excellence, experienced in cloud and container implementations, and well adept in handling large data sets, who will impress us with architectural skills as well as with prototyping and problem-solving speed. Speed and agility in developing and delivering data driven solutions will be key to the success of this unit. The successful candidate will join the Munich Engine team located in NY while also having access to all resources of the global IT Infrastructure.

We expect from you a proactive approach to achieving goals, collaborative workstyle and a high level of self-motivation to achieving our ambitious goals of Munich Engine.

Your Job:

Work with cutting-edge technologies(Azure, Spark, Kubernetes, etc.) to build a data platform for a high profile project to uncover new insights
code
Design, build and maintain high-performance, fault-tolerant, and scalable data pipelines to ensure highly accurate and reliable business reporting
Partner with business leaders to understand and synthesize needs into end-to-end technical solution
Partner with data scientists to design and implement advanced statistical models and machine learning pipelines
Qualifications
Requirements:

Bachelor's degree in Computer Science, Information Systems, or related field
5+ years of experience in data engineering or data infrastructure role
3+ years of experience with Spark, Hadoop, Airflow, Kafka, etc
Proficient in data modeling and system design skills
Advanced experience of SQL and Python (data analysis libraries like pandas, numpy, scikit-learn, etc)
Proficient experience with cloud such as Azure, GCP or AWS (Azure is target platform)
Proficient experience in building and maintaining data processing pipelines
Good understanding of SQL/NoSQL databases such as Cosmos DB, Postgres, MongoDB, MySQL etc.
Experience with version control(git), CI/CD (Microsoft Azure Dev Ops, Jenkins)
Experience with supporting Data Science teams on feature engineering, model training and deployment tasks
Comfortable in Windows/Linux environment
Strong business communication skills
Strong drive to constantly learn and keep up to speed with the new technologies
Ability to understand complex business priorities and translate them into clearly defined technical/data specifications for implementation
Ability to deal with ambiguity and work with rapidly changing business data
Preferred

Extensive experience with Microsoft Azure
Familiarity with tools like Informatica is a plus
Experience with containers (Docker, Kubernetes)
Familiarity with web frameworks such as Flask
Extensive experience of Scala
Company NameMunich Re America
Requisition Number
3185BR
CountryUnited States of America
Employment Type
Full Time"
62,Data Engineer – Infrastructure and Operations,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"Spotify is looking for a Software Engineer to join our Core Infra org. You will take on complex data-related problems using some of the most diverse datasets available — user behaviors, acoustical analysis, revenue streams, cultural and contextual data, and other signals across our broad range of mobile and connected platforms. Above all, your work will influence the way people experience music.

What you’ll do
Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.
Use best practices in continuous integration and delivery.
Help drive optimization, testing and tooling to improve data quality.
Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.
Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.
Who you are
You have at least 3 years of professional experience working in a product-driven environment
You are experienced with Java
You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
You are knowledgeable about data modeling, data access, and data storage techniques.
You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
You understand the value of collaboration within teams."
63,"Data Engineer Spring 2020 Internships – New York, NY","New York, NY",New York,NY,None Found,None Found,"In pursuit of a Master’s or PhD in Computer Science, Computer Engineering, Software Engineering, or other relevant quantitative fieldsWorking knowledge of at least two programming languages. Examples: R, Python, Julia, Java, or ScalaWorking knowledge of cloud computing technologies such as AWS, Azure, or GCPFamiliarity with Spark, Hive and/or HadoopCumulative GPA of 3.0 or aboveMust be 18 years of age or olderMust be authorized to work in the United States without visa sponsorship by NBCUniversalNeeds to be able to work on-site in New York, NYInternships at NBCUniversal are paid and do not require course creditDeadline: Interested applicants are encouraged to apply by November 1st, 2019Spring Program Dates: January – May 2020",None Found,None Found,None Found,"In pursuit of a Master’s or PhD in Computer Science, Computer Engineering, Software Engineering, or other relevant quantitative fieldsWorking knowledge of at least two programming languages. Examples: R, Python, Julia, Java, or ScalaWorking knowledge of cloud computing technologies such as AWS, Azure, or GCPFamiliarity with Spark, Hive and/or HadoopCumulative GPA of 3.0 or aboveMust be 18 years of age or olderMust be authorized to work in the United States without visa sponsorship by NBCUniversalNeeds to be able to work on-site in New York, NYInternships at NBCUniversal are paid and do not require course creditDeadline: Interested applicants are encouraged to apply by November 1st, 2019Spring Program Dates: January – May 2020","Unlike a traditional internship, the Decision Sciences Engineering team embeds future Data Engineers in ongoing projects within NBCUniversal. Future Data Engineers become active members of existing teams building data science products, acquiring specific responsibilities, goals and timelines which are interdependent with the goals and timelines of the teams they are embedded in. Upon completing the internship, future Data Engineers will have gained experience in real-world projects in a typical data science environment, using state of the art data engineering tools, and broad exposure to the media industry.
Qualifications/Requirements
In pursuit of a Master’s or PhD in Computer Science, Computer Engineering, Software Engineering, or other relevant quantitative fieldsWorking knowledge of at least two programming languages. Examples: R, Python, Julia, Java, or ScalaWorking knowledge of cloud computing technologies such as AWS, Azure, or GCPFamiliarity with Spark, Hive and/or HadoopCumulative GPA of 3.0 or aboveMust be 18 years of age or olderMust be authorized to work in the United States without visa sponsorship by NBCUniversalNeeds to be able to work on-site in New York, NYInternships at NBCUniversal are paid and do not require course creditDeadline: Interested applicants are encouraged to apply by November 1st, 2019Spring Program Dates: January – May 2020
Desired Characteristics
Prior experience with ML and/or statistical modellingAdvanced working knowledge of statistics and its applications for inferential data analysisFamiliarity with Spark, Hive and/or HadoopExperience with data visualization techniques and concepts
Sub-BusinessCampus Programs Interns
Career Level
Co-op/Intern
CityNew York
State/Province
New York
CountryUnited States
About Us
The NBCUniversal Internship Program is an experience like no other. We offer diversity of opportunities, with unique internships across our iconic portfolio of brands. Through unparalleled access to the best in the business, hands-on training & one-of-a-kind networking events, our interns have the chance to influence change. Our interns are ambitious, innovative and savvy; they shape the way we do things. Here you can contribute as content creators, problem solvers & innovators. Here you can learn the power and possibilities of media and technology. Here you can go far.
Notices
NBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable."
64,Senior Streaming Data Engineer (Flink & Kafka),"New York, NY 10261",New York,NY,10261,None Found,"
6-10 years of relevant experience in Apps Development or systems analysis role
Extensive experience system analysis and in programming of software applications
Experience in managing and implementing successful projects
Subject Matter Expert (SME) in at least one area of Applications Development
Ability to adjust priorities quickly as circumstances dictate
Demonstrated leadership and project management skills
Consistently demonstrates clear and concise written and verbal communication",None Found,"
Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements
Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards
Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint
Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation
Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals
Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions
Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary
Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.","
Bachelor’s degree/University degree or equivalent experience
Master’s degree preferred",None Found,"The Senior Streaming Data Engineer is a senior level position responsible for establishing and implementing new or revised application systems and programs in coordination with the Technology team. The overall objective of this role is to lead applications systems analysis and programming activities.

Responsibilities:
Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements
Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards
Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint
Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation
Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals
Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions
Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary
Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.

Qualifications:
6-10 years of relevant experience in Apps Development or systems analysis role
Extensive experience system analysis and in programming of software applications
Experience in managing and implementing successful projects
Subject Matter Expert (SME) in at least one area of Applications Development
Ability to adjust priorities quickly as circumstances dictate
Demonstrated leadership and project management skills
Consistently demonstrates clear and concise written and verbal communication

Education:
Bachelor’s degree/University degree or equivalent experience
Master’s degree preferred
This job description provides a high-level review of the types of work performed. Other job-related duties may be assigned as required.
Technical Requirements:
Hands-on production experience with distributed stream processing frameworks: Kafka / Spark Streaming / Storm
Experience with deployment platform such as Kubernetes, YARN, Mesos
Production experience of building a robust, fault-tolerant data pipeline that cleans, transforms, and aggregates unorganized and messy data into databases or data sources
Solid design/development background
Experience with micro-services and distributed architecture
Experience with relational databases (SQL Server, Oracle, DB2, or Sybase etc.) and non-relational databases (e.g. Cassandra and MongoDb)
Practical experience in performance tuning and optimization, bottleneck problem analysis
Excellent communication skills to be able to interface with trading and sales functions
Working knowledge of Java and/or Python
-
Grade :All Job Level - All Job FunctionsAll Job Level - All Job Functions - US
-
Time Type :Full time
-
Citi is an equal opportunity and affirmative action employer.
Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity.
Citigroup Inc. and its subsidiaries (""Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity CLICK HERE.
To view the ""EEO is the Law"" poster CLICK HERE. To view the EEO is the Law Supplement CLICK HERE.
To view the EEO Policy Statement CLICK HERE.
To view the Pay Transparency Posting CLICK HERE."
65,Senior Data Engineer - Amazon AI,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering).5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.Demonstrated strength in data management, orchestration, access control, data lake formation, etc.Experience with Redshift, Oracle, etc.Experience with AWS services including S3, Redshift, EMR and RDS.Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)Experience in working and delivering end-to-end projects independently.Knowledge of distributed systems as it pertains to data storage and computing

Come and be part of the Amazon AI Platform team and work on solving cutting edge machine learning problems.

With Machine Learning, businesses now ask our machines to do more than repetitive, strictly-defined tasks. We are taking it one step further and have begun to ask them to not only learn on their own but to also interpret data and report to the customer before they even knew they needed it. It's a step in history for you to be a part of. You will be building a platform that incorporates best practices and runs advanced algorithms at production scale and reliability.

We are a team of applied scientists and machine learning engineers who experiment, research, and turn machine/deep learning and AI research into great products for our customers.

You will work in a fast-paced environment and do everything from determining priorities, designing features, re-architecting as necessary, automating testing, and mentoring others. The best candidates show true end-to-end ownership. In this role, you will be responsible for helping design, build, and manage the data infrastructure that supports our science and research initiatives.

Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation

Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.Experience working with machine learning engineers and scientists on data science problems.Experience providing technical leadership and mentoring other engineers for best practices on data engineeringKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operationsMasters in computer science, mathematics, statistics, economics, or other quantitative field"
66,Data Engineer,"New York, NY",New York,NY,None Found,None Found,"
Knowledge of emerging data integration technologies
Spark Streaming, Kafka Streams, Kafka Connect, etc
Data Science and Modeling pipeline experience
Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools
Strong expertise/background with Linux
Familiarity with MS Sql Server, SSIS, SSAS
",None Found,"
Determine optimal solutions for integrating data from a variety of sources into a common data warehouse
Implement, maintain and monitor batch & stream data pipelines with best practice quality controls
Evaluate relevant new and mature technologies as needs, gaps, and opportunities arise
Work closely and collaboratively in an Agile environment with our analysts, engineers and product teams to analyze issues and find new insights covering our business and operations
Collaborate with data infrastructure team to deploy necessary infra capabilities
Day to day operational support of data infrastructure, and services
",None Found,"
5+ Years of related work experience
Solid command of Python, Java, and/or Scala
Experience in stream processing technology (Kafka, Spark, Storm, Samza, Flink, etc)
In-depth knowledge of SQL, data modeling and data warehousing concepts
Distributed and low latency (streaming) application architecture
Familiarity with API design
CI/CD systems experience (Jenkins, Github, etc)
Experience adhering to robust audit standards
BS or MS degree in Computer Science or Engineering related experience
","The Opportunity:
As a Data Engineer on our Business Intelligence team, you will play a key role in designing and driving our data integration pipelines, ETL/ELT, and analytic strategies across our enterprise. In this role you will be bringing modern data technologies and practices to enable our talented team of researchers, data scientists and analysts to enhance the business with better insights and relevant data products. You will partner with all data teams at Shutterstock to understand requirements and collaborate on providing solutions at scale.

Responsibilities:

Determine optimal solutions for integrating data from a variety of sources into a common data warehouse
Implement, maintain and monitor batch & stream data pipelines with best practice quality controls
Evaluate relevant new and mature technologies as needs, gaps, and opportunities arise
Work closely and collaboratively in an Agile environment with our analysts, engineers and product teams to analyze issues and find new insights covering our business and operations
Collaborate with data infrastructure team to deploy necessary infra capabilities
Day to day operational support of data infrastructure, and services

Requirements:

5+ Years of related work experience
Solid command of Python, Java, and/or Scala
Experience in stream processing technology (Kafka, Spark, Storm, Samza, Flink, etc)
In-depth knowledge of SQL, data modeling and data warehousing concepts
Distributed and low latency (streaming) application architecture
Familiarity with API design
CI/CD systems experience (Jenkins, Github, etc)
Experience adhering to robust audit standards
BS or MS degree in Computer Science or Engineering related experience

Preferred Qualifications:

Knowledge of emerging data integration technologies
Spark Streaming, Kafka Streams, Kafka Connect, etc
Data Science and Modeling pipeline experience
Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools
Strong expertise/background with Linux
Familiarity with MS Sql Server, SSIS, SSAS

About Shutterstock, Inc.

Shutterstock, Inc. (NYSE: SSTK ( https://studio-5.financialcontent.com/prnews?Page=Quote&Ticker=SSTK )), directly and through its group subsidiaries, is a leading global provider ofhigh-quality licensed photographs ( https://www.shutterstock.com/ ),vectors ( https://www.shutterstock.com/vectors ),illustrations ( https://www.shutterstock.com/category/illustrations-clip-art ),videos ( https://www.shutterstock.com/video/ ) andmusic ( https://www.shutterstock.com/music/ ) to businesses, marketing agencies and media organizations around the world. Working with its growing community of over 750,000 contributors, Shutterstock adds hundreds of thousands of images each week, and currently has more than 260 million images and more than 14 million video clips available.

Headquartered in New York City, Shutterstock has offices around the world and customers in more than 150 countries. The company also ownsBigstock ( https://www.bigstockphoto.com/ ), a value-oriented stock media offering; Shutterstock Custom,a custom content creation platform ( https://www.shutterstock.com/custom ), Offset, ahigh-end image collection ( https://offset.com/ ); PremiumBeat a curatedroyalty-free music ( https://www.premiumbeat.com/ ) library; and Shutterstock Editorial, a premier source ofeditorial images ( https://www.shutterstock.com/editorial ) for the world's media.

For more information, please visitwww.shutterstock.com ( https://www.shutterstock.com/ ) and follow Shutterstock onTwitter ( https://twitter.com/shutterstock ) and onFacebook ( https://facebook.com/shutterstock ).

Equal Opportunity Employer, M/F/D/V"
67,DATA ENGINEER,"New York, NY 10038",New York,NY,10038,None Found,None Found,None Found,None Found,None Found,None Found,"JOB OVERVIEW
Do you want to be a part of the team that is reinventing drug research & development? tellic is searching for an experienced, motivated Data Engineer to implement and support our cutting-edge data science tools for the pharmaceutical industry.
Become a member of the core team that is bringing next-generation data technologies to the world's top pharmaceutical companies. At tellic, we are shaping $150 billion of pharmaceutical R&D decisions using cutting-edge data science technology. This is a chance to get in on the ground level at one of NYC's fastest-growing tech startups in the white-hot space of life science AI.
At tellic, we value culture as much as technical craft. Our values include creating a highly collaborative team of A+ players who build cool stuff that drives a real business impact. In fact, the name tellic means ""deliberate, with an outcome, and with impact.""
The Data Engineer will be responsible for scaling Python code, models, and NLP technology to the terabyte level on Google Cloud and our customers’ on-prem environments. This individual will partner with data scientists and DevOps to determine the best strategy for providing our customers with high-performance models, tools, and award-winning service. The ideal candidate will be an innate hacker who loves learning new things, redefining standard practices and is willing to do what it takes to solve problems.
RESPONSIBILITIES
Create and maintain optimal data pipeline architecture
Assist in converting R&D projects to production-ready Python code
Architect scalable solutions that can handle data at the terabyte level
Work with the Data Science, DevOps and Product teams to design product solutions that seamlessly function in our internal environments and customer sites
REQUIREMENTS
BS in computer science or related technical field or equivalent experience (MS is a plus)
5+ years of experience supporting data science pipelines in Python
Cloud experience required (GCP a plus)
Hands-on experience supporting Big Data technologies (e.g., Hadoop, Spark)
Some startup experience required
Experience scaling data science routines for high-throughput processes at the terabyte level
Developing and testing full stack production Python systems
Willingness to assist DevOps as needed
Experience building and optimizing Big Datadata pipelines, architectures, datasets, and machine learning models. Google-certified professional a plus (we will support ongoing GCP training and certifications)
Experience working with large volumes of structured and unstructured data in a machine learning environment
Strong desire to be in a fast-growing startup environment and build next-generation machine learning infrastructure
PERSONAL CHARACTERISTICS
High energy; continuous desire to learn/grow; intellectual curiosity
Relentless problem solver, willing to do whatever it takes
Self-sufficient, resourceful, able to work independently
Not afraid to make mistakes; willing to find creative solutions to difficult problems
Open-minded, hypothesis led, data-driven mindset
Willing to take on anything and try to break the mold
WHAT TELLIC OFFERS
Once in a lifetime opportunity to transform an entire industry with data science, artificial intelligence and analytics
Opportunity to get in early at a self-funded, profitable startup with massive growth potential
Very competitive compensation package including base salary, bonus, and equity plan
Fun, inclusive culture that celebrates diversity and respects individuals and their contributions
Competitive medical/dental/vision coverage
3 weeks of paid vacation + 15 holidays
Discounted corporate gym membership
Convenient NYC location
Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future.
Interested candidates, please send your resume to careers@tellic.com"
68,Data Engineer,"New York, NY 10036",New York,NY,10036,None Found,None Found,None Found,None Found,None Found,None Found,"The New York Times is seeking inventive and motivated data engineers at all levels of experience to join the Data Engineering group. In this role, you will build critical data infrastructure that surfaces data and insights across the company.
About Us
Our Data Engineering teams are at the intersection of business analytics, data warehousing, and software engineering. As Maxime Beauchemin wrote in “The Rise of Data Engineering”, ETL and data modeling have evolved, and the changes are about distributed systems, stream processing, and computation at scale. They’re about working with data using the same practices that guide software engineering at large. A strong data foundation is essential for The New York Times and we’re responsible for it. We use our data infrastructure to power analytics and data products and to deliver relevant experiences to our customers in real-time. We enable our company to validate strategic decisions, make smarter choices, and react to the fast changing world. We are part of a New York based technology organization with a remote-friendly workplace that includes engineers around the world. We value transparency and openness, learning, community, and continuous improvement. Check out the Times Open blog, which is written by engineers and other technical team members, and follow @nytdevs on Twitter to see what we’re up to.
About the Job
We focus on the software engineering related to data replication, storage, centralized computation, and data API’s. We provide customers and partners with data tools, shared frameworks, and data services. These are the foundational core of our group which enables ourselves and others to work with data from a common underpinning. Our tools and services enable our group to scale and avoid blocking others. We reduce data redundancy by creating systems and datasets that serve as sources of record. We enable discovery and governance of our data. We support key business goals like growing our digital subscriber base, understanding how our customers use our products, and retaining our print subscribers.
As a data engineer, you will:
Run and support a production enterprise data platform
Design and develop data models
Work with languages like Java, Python, Go, Bash, and SQL
Build batch and streaming data pipelines with tools such as Spark, Airflow, and cloud-based data services like Google’s BigQuery, Dataproc, and Pub/Sub
Develop processes for automating, testing, and deploying your work
About You
To thrive in this role, you are excited about data and motivated to learn new technologies. You are comfortable collaborating with engineers from other teams, product owners, business teams, and data analysts and data scientists. You are own and shape your technical domain area and move the related business goals forward. You are eager to resolve upstream data issues at the source instead of applying workarounds. You analyze and test changes to our data architectures and processes, and determine what the possible downstream effects and potential impacts to data consumers will be.
Benefits and Perks:
Make an impact by supporting our original, independent and deeply reported journalism.
We provide competitive health, dental, vision and life insurance for employees and their families
We support responsible retirement planning with a generous 401(k) company match.
We offer a generous parental-leave policy, which we recently expanded in response to employee feedback. Birth mothers receive 16 weeks fully paid; non gestational parents receive 10 weeks, also fully paid.
We are committed to career development, supported by a formal mentoring program and $8,000 annual tuition reimbursement.
We have frequent panel discussions and talks by a wide variety of news makers and industry leaders.
Join a community committed to the richness of diversity, experiences and talents in the world we cover, supported by a variety of employee resource groups.
#LI-AM1
The New York Times is committed to a diverse and inclusive workforce, one that reflects the varied global community we serve. Our journalism and the products we build in the service of that journalism greatly benefit from a range of perspectives, which can only come from diversity of all types, across our ranks, at all levels of the organization. Achieving true diversity and inclusion is the right thing to do. It is also the smart thing for our business. So we strongly encourage women, veterans, people with disabilities, people of color and gender nonconforming candidates to apply.
The New York Times Company is an Equal Opportunity Employer and does not discriminate on the basis of an individual's sex, age, race, color, creed, national origin, alienage, religion, marital status, pregnancy, sexual orientation or affectional preference, gender identity and expression, disability, genetic trait or predisposition, carrier status, citizenship, veteran or military status and other personal characteristics protected by law. All applications will receive consideration for employment without regard to legally protected characteristics. The New York Times Company will consider qualified applicants, including those with criminal histories, in a manner consistent with the requirements of applicable state and local ""Fair Chance"" laws."
69,"Senior Engineer, Data","New York, NY 10001",New York,NY,10001,None Found,"
Advanced degree in relevant field of study strongly desirable, particularly in computer science or engineering level programs.
5+ years professional experience working with data extract/manipulation logic.
5+ years professional experience with object-oriented programming, functional programming, and data design.
7+ years experience with Development, Engineering, R&D or Information Technology.
3+ years working with a public cloud big data ecosystem (certification in AWS a plus).
3+ years working with MPP databases, distributed databases, and/or Hadoop.
","Passion for data engineering, able to excite and lead by example and mentoring others.
Hungry and eager to learn new systems and technologies.
Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.
Ability to deliver exceptional results through iterative improvement rather than initial perfection.
Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including: business users, technical staff, senior level colleagues, vendors, and partners.
An extensive track record that demonstrates effectiveness in driving business results through data and analytics.
The ability to develop and articulate a compelling vision and generate necessary consensus.
A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.
A proven ability to influence decision making across large organizations.
A proven ability to hire, develop, and effectively lead deeply technical resources.
Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.
Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.
Create an environment where people from diverse cultures and backgrounds work together effectively.
","
Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably.
Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions.
Gather and process all types of data including raw, structured, semi-structured, and unstructured data.
Integrate with a variety of data providers ranging from marketing, web analytics, and consumer devices including IoT and Telematics.
Build and maintain dimensional data warehouses in support of business intelligence tools.
Develop data catalogs and data validations to ensure clarity and correctness of key business metrics.
Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result.
Derive an overall strategy of data management, within an established information architecture (including both structured and unstructured data), that supports the development and secure operation of existing and new information and digital services.
Plan effective data storage, security, sharing and publishing within the organization.
Ensure data quality and implement tools and frameworks for automating the identification of data quality issues.
Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Mentor and lead data engineers providing technical guidance and oversight.
Provide ongoing support, monitoring, and maintenance of deployed products.
Drive and maintain a culture of quality, innovation and experimentation.
",None Found,"Passion for data engineering, able to excite and lead by example and mentoring others.
Hungry and eager to learn new systems and technologies.
Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.
Ability to deliver exceptional results through iterative improvement rather than initial perfection.
Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including: business users, technical staff, senior level colleagues, vendors, and partners.
An extensive track record that demonstrates effectiveness in driving business results through data and analytics.
The ability to develop and articulate a compelling vision and generate necessary consensus.
A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.
A proven ability to influence decision making across large organizations.
A proven ability to hire, develop, and effectively lead deeply technical resources.
Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.
Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.
Create an environment where people from diverse cultures and backgrounds work together effectively.
","Location: New York, NY

Position Summary:

The Senior Data Engineer is responsible for building and deploying streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably. As a Senior Data Engineer, you will lead collaboration with product teams, data analysts and data scientists to design and build data-forward solutions. In this highly visible technical lead position, you will be responsible for providing Data Engineering leadership and support to ingest and integrate large volumes of disparate data from a variety of sources. This involves rapid innovation in large scale data pipeline design and development to ensure critical data sets are made available to our users and predictive models in a timely manner. We are looking for someone with strong hands on experience in all layers of the full stack involving data. The Senior Data Engineer plays a significant role in Agile planning, providing advice and guidance, and monitoring emerging technologies. This is not a junior programmer position and requires extensive hands on coding and design experience.

Duties and Responsibilities:

Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably.
Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions.
Gather and process all types of data including raw, structured, semi-structured, and unstructured data.
Integrate with a variety of data providers ranging from marketing, web analytics, and consumer devices including IoT and Telematics.
Build and maintain dimensional data warehouses in support of business intelligence tools.
Develop data catalogs and data validations to ensure clarity and correctness of key business metrics.
Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result.
Derive an overall strategy of data management, within an established information architecture (including both structured and unstructured data), that supports the development and secure operation of existing and new information and digital services.
Plan effective data storage, security, sharing and publishing within the organization.
Ensure data quality and implement tools and frameworks for automating the identification of data quality issues.
Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Mentor and lead data engineers providing technical guidance and oversight.
Provide ongoing support, monitoring, and maintenance of deployed products.
Drive and maintain a culture of quality, innovation and experimentation.
Supervisory Responsibilities:

This is an individual contributor role without direct reports, however as a Senior level role we expect this candidate to coach, mentor, and help develop junior developers and engineers to inspire, motivate, grow, and help structure a high performance team.
Minimum Qualifications:

Advanced degree in relevant field of study strongly desirable, particularly in computer science or engineering level programs.
5+ years professional experience working with data extract/manipulation logic.
5+ years professional experience with object-oriented programming, functional programming, and data design.
7+ years experience with Development, Engineering, R&D or Information Technology.
3+ years working with a public cloud big data ecosystem (certification in AWS a plus).
3+ years working with MPP databases, distributed databases, and/or Hadoop.
Requirements and General Skills:
Passion for data engineering, able to excite and lead by example and mentoring others.
Hungry and eager to learn new systems and technologies.
Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.
Ability to deliver exceptional results through iterative improvement rather than initial perfection.
Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including: business users, technical staff, senior level colleagues, vendors, and partners.
An extensive track record that demonstrates effectiveness in driving business results through data and analytics.
The ability to develop and articulate a compelling vision and generate necessary consensus.
A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.
A proven ability to influence decision making across large organizations.
A proven ability to hire, develop, and effectively lead deeply technical resources.
Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.
Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.
Create an environment where people from diverse cultures and backgrounds work together effectively.
Technical Skills:

Experience deploying and running AWS-based data solutions and familiar with tools such as Cloud Formation, IAM, Athena, and Kinesis.
Experience engineering big-data solutions using technologies like EMR, S3, Spark and an in-depth understanding of data partitioning and sharding techniques.
Experience loading and querying both on premise and cloud-hosted databases such as Teradata and Redshift.
Building streaming data pipelines using Kafka, Spark, or Flink.
Familiarity with binary data serialization formats such as Parquet, Avro, and Thrift.
Experience deploying data notebook and analytic environments such as Jupyter and Databricks.
Knowledge of the Python data ecosystem using pandas and numpy.
Experience building and deploying ML pipelines: training models, feature development, regression testing.
Experience with graph-based data workflows using Apache Airflow.
Expertise writing distributed, high-volume services in Python, Java or Scala.
Expertise with high volume heterogeneous data, preferably with distributed systems.
Knowledge of data modeling, data access, and data storage techniques.
Appreciation of agile software processes, data-driven development, reliability, and responsible experimentation.
Familiar with metadata management, data lineage, and principles of data governance.
professional role in one or more of the following:
Strong and thorough knowledge of the following:

ETL/ELT Tools
BI tools
MDM / Reference Data
RDBMS, NoSQL and NewSQL
MS Office Suite
SiriusXM is an equal opportunity employer that does not discriminate on the basis of sex, race, color, age, national origin, religion, creed, physical or mental disability, medical condition, marital status, sexual orientation, gender identity or expression, citizenship, pregnancy, military or veteran status or any other status protected by applicable law.

The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice."
70,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Latch's data team is continuing to be built from ground up, and we are still actively hiring. We're covering all aspects of the Latch's data strategy including: analysis, engineering, science, & more. We are firm believers of bringing in smart people that can define the roles for themselves, so come join us and start creating greatness.

Smart access isn’t about locking doors, it’s about opening up new possibilities. Latch is the world’s first fully integrated hardware and software system dedicated to bringing seamless access to every door in a modern building. We’re looking for the curious and the creative to join our team and help us change the way we access our most valued spaces.

Responsibilities

Understand the data gathered across the entire Latch organization
Design and implement data pipelines, building scalable and optimized enterprise level data systems
Collaborate with other teams in the company, both engineering and business counterparts
Transform raw data into meaningful sets that are query-able and visualizable.
Work closely with Data Analysts and Data Scientists to implement production ready systems
Be a helping hand with tools used by other teams such as Sales CRMs, Ops Customer Success tools, Marketing automation or Finance ERP. Data from these tools are very important to us.
Requirements

BS in Computer Science, Math, related technical field or equivalent practical experience
3+ years of general software programming experience in Java or similar languages
Excellent grasp of data structures and algorithms
Solid level of understanding in SQL
Knowledge of database technology, schema design, and query optimization techniques
Experience in ETL pipelines and data transformations.
Excellent communication skills
Preferred Qualifications

MS in Computer Science, Mathematics, or related technical field
Experience with Map-Reduce technologies such as Spark or Hadoop.
Understanding of basic data science concepts
Experiencing in productionizing machine learning models.
Acute sense of data analysis: being able to make sense out of many seemingly unrelated data sets.
Founded in 2014, Latch is a venture-backed, high-growth organization that's on a mission to change the way people open, manage, and share their spaces. Today, 1 in 10 new developments in the U.S. depend on our full-building smart access solution to meet the needs of residents and property managers.

We are a team of just over 200 employees, all of whom are passionate self starters with unique backgrounds and unexpected stories. We offer unlimited time off, a competitive health package, and the opportunity to work in a creative, dynamic, and fast-paced office environment. We are located just a quick walk from both Hudson Yards and Penn Station in New York City."
71,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,"
3+ years experience in software development
3+ years experience designing SQL tables, choosing indexes, tuning queries and understanding the intricacies required to optimize a table in different environments
","Dashlane is a password manager and online security app for everyone who lives, works, and plays on the internet. With a simple, intuitive design and patented security technology, Dashlane keeps passwords, personal data, and payment info at users' fingertips, so they can stop guessing passwords and wasting time filling out forms. Dashlane has helped over 11 million users in 180 countries manage and secure their digital identities and has enabled over $17 billion in e-commerce transactions. Our team in New York, Paris and Lisbon is united by our passion for password security and the belief that our success is built on the diverse backgrounds of every member.

You will be based in New York.

Dashlane is looking for a highly talented data engineer to join the Data team. Optimizing our data pipelines and data warehouses will be an essential part to helping our company scale in the coming years. You have several years of experience and a proven track record in building, deploying and keeping such applications up 24x7. You will work on a daily basis with your teammates in New York City.

At Dashlane you will:

Work on the development and maintenance of messaging services, BI tools, data warehouses
Implement custom ETL/ELT processes in distributed computing environments
Work on improving in-house tracking systems for all of our applications
Design and implement data pipelines capable of modeling data from many sources and store it in such a way that users can self-serve
Work on server applications and APIs that are used by our Data Team
Handle the challenges that come with managing terabytes of data
Develop automated reporting for API and system health (process, memory, response time)

Requirements:

3+ years experience in software development
3+ years experience designing SQL tables, choosing indexes, tuning queries and understanding the intricacies required to optimize a table in different environments

We're also looking for:

Having experience in architecting, implementing and testing data processing pipelines (e.g. Spark, Beam, ...) and data mining / data science algorithms either on-premise or on a cloud environment
Having experience in administrating and ingesting data into standard data warehouses (e.g. Amazon Redshift, Microsoft SQL Server, Google BigQuery or Snowflake)
Having strong experience in improving performance of queries, data jobs and scaling systems for exponential growth in data
Being able to communicate and understand complex technical issues in English

Nice-To-Haves:

Have prior knowledge of Python and Node.js
Having experience with data lakes and expertise with designing and maintaining a BI solution
Enjoy writing clean code that is easy to maintain and understand
Have a security background

A true international company, founded in Paris and currently split between Paris, Lisbon and New York, we thrive off diverse perspectives. We recognize that diversity has different aspects: gender, sexual orientation, ability, ethnic origin, social, age, lifestyle, and more. We're committed to finding diverse talent and fostering a culture where everyone is heard and feels a sense of belonging."
72,DATA ENGINEER - competative compensation,"New York, NY 10036",New York,NY,10036,None Found,None Found,None Found,"You will join a small team partnering with product owners and developers at Roivant and Vants to provide end-to-end data solutions for technology tools and products.
You will follow best coding practices to build and optimize tools for data ingestion and storage, including components of client's Data Lake/Warehouse platform.
You will automate and maintain data processing pipelines, implement modern ETL infrastructure, and continuously improve the efficiency of our platform.
You will serve as a subject matter expert on big data analytics projects that provide insights for business and technical stakeholders.
",None Found,"BA/BS degree with strong academic performance, preferably in a quantitative field
4+ years experience with Python, database development, Git, Linux and AWS (S3, EC2, SNS, Lambda, SQS)
Experience with Spark, terraform, Docker, big data and/or healthcare data preferred
Knowledge of Scrum and desire to work in an incredibly fast-moving, agile environment
Team player with strong communication skills and the ability to work with minimal supervision
Quick and scrappy learner who adapts well to a fast-moving environment and gets things done; experience in high-growth or startup environments a plus","One of the global leading innovative biotech companies.
We are seeking a thoughtful, hands-on Data Engineer to join our client's Data Architecture team. Data Architecture is a broad team that develops and operates the data platform used by developers throughout the Roivant family. The team implements all data ingestion, storage, and analytics tooling and provides other ad hoc database needs.


Key Responsibilities


You will join a small team partnering with product owners and developers at Roivant and Vants to provide end-to-end data solutions for technology tools and products.
You will follow best coding practices to build and optimize tools for data ingestion and storage, including components of client's Data Lake/Warehouse platform.
You will automate and maintain data processing pipelines, implement modern ETL infrastructure, and continuously improve the efficiency of our platform.
You will serve as a subject matter expert on big data analytics projects that provide insights for business and technical stakeholders.




Requirements


BA/BS degree with strong academic performance, preferably in a quantitative field
4+ years experience with Python, database development, Git, Linux and AWS (S3, EC2, SNS, Lambda, SQS)
Experience with Spark, terraform, Docker, big data and/or healthcare data preferred
Knowledge of Scrum and desire to work in an incredibly fast-moving, agile environment
Team player with strong communication skills and the ability to work with minimal supervision
Quick and scrappy learner who adapts well to a fast-moving environment and gets things done; experience in high-growth or startup environments a plus"
73,Software Engineer - Big Data,"New York, NY 10001",New York,NY,10001,None Found,None Found,None Found,None Found,None Found,None Found,"Our Consumer & Community Banking Group depends on innovators like you to serve nearly 66 million consumers and over 4 million small businesses, municipalities and non-profits. You’ll support the delivery of award winning tools and services that cover everything from personal and small business banking as well as lending, mortgages, credit cards, payments, auto finance and investment advice. This group is also focused on developing and delivering cutting edged mobile applications, digital experiences and next generation banking technology solutions to better serve our clients and customers.

As an experienced Big Data Engineer, your mission is to help lead our team of innovators and technologists toward creating next-level solutions that improve the way our business is run. Your deep knowledge of design, analytics, development, coding, testing and application programming will help your team raise their game, meeting your standards, as well as satisfying both business and functional requirements. Your expertise in various technology domains will be counted on to set strategic direction and solve complex and mission critical problems, internally and externally. Your quest to embracing leading-edge technologies and methodologies inspires your team to follow suit. And best of all, you’ll be able to harness massive amounts of brainpower through our global network of technologists from around the world.

Bachelor's Degree or better in Engineering, Computer Science or Information Technology
Cloud Computing - AWS
BigData, Hadoop, Hive
ODS - Cassandra
Java 8 / Spark
ETL - Talend
Real time messaging – Kafka / Java on Gaia Application Platform
Exposure to Machine Learning will be a big +
3+ years of experience in middle-tier/backend systems development in Java/Linux
Big data background with experience designing and implementing large scale systems
Working experience with Hadoop, Enterprise Java development, NoSQL data platforms (Cassandra), Pub/sub messaging (Rendezvous, AMPS, Kafka, etc.), Stream processing (Storm, Hbase, Nifi, Spark Streaming, etc.), Batch Processing with tools such as Talend, Informatica, or Hive/SQL and Visualization with Tableau.
Extensive experience with horizontally scalable and highly available system design and implementation, with focus on performance and resiliency
Extensive experience profiling, debugging, and performance tuning complex distributed systems
Willingness to commit extra effort to meet deadlines as required on a high profile and business critical project"
74,Federal - Big Data Engineer,"New York, NY 10011",New York,NY,10011,None Found,"
Bachelor’s Degree in related field and 3 years of experience in JavaScript, SQL and/or Python
U.S. Citizenship
Ability to work in fast paced prototyping environment (Average project length is 6-12 weeks, multiple projects assigned simultaneously)","JavaScript
GitOps (Github, Gitlab)
Pandas
Python
Angular
React
Tensorflow / Keras / Pytorch / Fastai
Flask / Django
Jupyter Notebooks / Anaconda
Spark
Databricks
Docker
Kubernetes
SQL
Docker Compose
Docker Swarm
Vue
Jenkins
Gatsby",None Found,None Found,None Found,"Organization: Accenture Federal Services

Location: New York, NY

Accenture Federal Services, a wholly owned subsidiary of Accenture LLP, is a U.S. company with offices in Arlington, Virginia. Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations. Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.

We believe that great outcomes are everything. It’s what drives us to turn bold ideas into breakthrough solutions. By combining digital technologies with what works across the world’s leading businesses, we use agile approaches to help clients solve their toughest problems fast—the first time. So, you can deliver what matters most.

Count on us to help you embrace new ways of working, building for change and put customers at the core. A wholly owned subsidiary of Accenture, we bring over 30 years of experience serving the federal government, including every cabinet-level department. Our 7,200 dedicated colleagues and change makers work with our clients at the heart of the nation’s priorities in defense, intel, public safety, health and civilian to help you make a difference for the people you employ, serve and protect.

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

Accenture Federal Services is looking for Big Data Engineers to lead the way in tackling the most difficult engineering challenges of data systems. The Big Data Engineer will manage the discovery lab's data environment, data governance, and ETL. This individual uses Accenture Insights Platform and manages the data stores including S3 and NOSQL.

Basic Qualifications:
Bachelor’s Degree in related field and 3 years of experience in JavaScript, SQL and/or Python
U.S. Citizenship
Ability to work in fast paced prototyping environment (Average project length is 6-12 weeks, multiple projects assigned simultaneously)
Preferred Skills:
JavaScript
GitOps (Github, Gitlab)
Pandas
Python
Angular
React
Tensorflow / Keras / Pytorch / Fastai
Flask / Django
Jupyter Notebooks / Anaconda
Spark
Databricks
Docker
Kubernetes
SQL
Docker Compose
Docker Swarm
Vue
Jenkins
Gatsby

An active security clearance or the ability to obtain one may be required for this role.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
75,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Komodo Health is addressing the global burden of disease through the world's most actionable healthcare map. Our solutions drive a more transparent, efficient and productive healthcare ecosystem. We value our culture of encouraging growth, collaboration, and constructive debate as well as delivering innovative solutions that ""wow"" our customers.

The Data Engineering (DE) team is looking for a Senior Data Engineer to help build out our data pipelines and data infrastructure, helping us develop data processing that is scalable, reliable and automated. The ideal candidate will be comfortable going from the whiteboard design of a system all the way to the nuances of code implementation.

This is an opportunity to join a growing company, and be a part of a team of folks accomplished in diverse Engineering disciplines; focused on using the best of what lies at the forefront of technology and skills to address complex, real-world problems in the Healthcare and Life Science space. Some of the tools we use are: Python, Spark, AWS, Kubernetes, Docker, Postgres, Git, Airflow and Flask.

RESPONSIBILITIES
----------------


Design, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources.
Create automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines.
Evaluate new technologies for continuous improvements in Data Engineering.
Collaborate closely with the product team to build out new data features.
Work with the data scientists to implement descriptive, forecasting and predictive algorithms and models using latest technologies.

REQUIREMENTS
------------


BS/MS in Computer Science or related degrees.
5+ years of relevant experience with data engineering or similar experience.
Experience with...
Building and deploying large-scale data processing pipelines.
Python, Scala or Java coding experience, proficiency with at least one of them is required.
Workflow & pipeline systems, like Airflow, Luigi, etc.
Distributed systems for data processing tools such as Spark, Hadoop, Kubernetes, etc.
SQL and Relational Databases, like PostgreSQL.
Continuous integration and automation tools and processes, like Jenkins.
You've been through the planning, launching and refactoring phases of code you wrote.
A serious passion for data.
History of excellence and responsibility in previous engineering positions.
Ability to work as part of a collaborative team in a fast-paced environment.
Sincere interest in working at a startup and scaling with the company as we grow.

NICE TO HAVE


Experience or interest in the life sciences industry.
Basic knowledge (college level or hands on) of machine learning and statistics.
Experience with graph databases.

BENEFITS
--------


Competitive salary and equity compensation
Full Medical, Dental, and Vision benefits
401k plan with employer match
Flexible hours and a ""use as you need"" vacation policy
Opportunities to attend industry conferences and events
Great office location(s) in SF/SOMA and NY/FLAT IRON

"
76,Lead Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Engineering team is developing the backbone of BounceX solutions. To do this, the team is using cutting-edge technologies and frameworks such as BigQuery, Spanner, Bigtable, Dataproc to process and analyze data, to help create the pipelines required to handle millions of events daily, and collaborate with the data science team to enable the innovative work in machine learning and AI to be utilized in our production environment.

Responsibilities


Contribute to the development of BounceX Micro Batch/ Real Time streaming pipelines (lambda architecture)
Work closely with the data science team to build our machine learning platform infrastructure
Work closely with the engineering team on data preprocessing and data structure
Help mentoring the data engineering team and leading the data engineering initiatives
Participate in architecture reviews

Qualifications


BA/BS or MS degree in Computer Science preferred, or equivalent work experience
Previous experience in leadership roles
Understanding of distributed data systems
Previous experience building lambda architecture
Previous experience working with big data tools such as Bigtable, Apache Spark, Apache Spark Streaming, Apache Heron, BigQuery
Preferred:
Google Cloud, Kubernetes
Experience running production Golang systems, Infrastructure background, Low Latency / High Throughput Architectural System Designs
Strong interest in data engineering
Eagerness to learn new technologies on the fly and ship to production
Knowledge in data science is a plus

BounceX is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.

JOIN US ON OUR MISSION

BounceX is a global marketing technology company that has been recognized as a best place to work by Glassdoor and Crain's, and one of America's fastest-growing SaaS companies.

More than 300 companies, including JetBlue, Uniqlo, HelloFresh, and Comcast use BounceX to orchestrate real-time, multichannel marketing programs customized for every individual web visitor.

With offices in New York City and London, BounceX is built on the belief that the success of a company is rooted in the strength of its team, so we've created a collaborative, inclusive environment where people love coming to work.

We provide career coaching, growth and development opportunities, and benefits that are in the 95th percentile of all technology companies. Some highlights include excellent healthcare that starts day one, best-in-class fully paid family leave, 401(k) match, flexible work hours, and more.

What bonds our community together is our commitment to 5 Core Values:


Come Hungry
Carry Each Other
Drive Undeniable Performance
Respect People, Privacy, Ideas
Bounce Back

"
77,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,"
Knowledge of emerging data integration technologies
Spark Streaming, Kafka Streams, Kafka Connect, etc
Exposure to Kubernetes and Helm
Data Science and Modeling pipeline experience
Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools
Strong expertise/background with Linux
Familiarity with MS Sql Server, SSIS, SSAS
","
Determine optimal solutions for integrating data from a variety of sources into a common data warehouse
Implement, maintain and monitor batch & stream data pipelines with best practice quality controls
Evaluate relevant new and mature technologies as needs, gaps, and opportunities arise
Work closely and collaboratively in an Agile environment with our analysts, engineers and product teams to analyze issues and find new insights covering our business and operations
Collaborate with our other data teams, as they are customers
Day to day operational support of data infrastructure, and services
",None Found,"
Solid command Java or Scala, and Python
Experience in stream processing technology (Kafka, Spark, Storm, Samza, Flink, etc)
Knowledge of SQL, data modeling and data warehousing concepts
Distributed and low latency (streaming) application architecture
Familiarity with API design
CI/CD systems experience (Jenkins, Github, etc)
Experience adhering to robust audit standards
BS or MS degree in Computer Science or Engineering related experience
","The Opportunity:
As a Data Engineer on our Data Infrastructure team, you will play a key role in designing and driving our data integration pipelines, ETL/ELT, and analytic strategies across our enterprise. In this role you will be bringing modern data technologies and practices to enable our talented team of researchers, data scientists and analysts to enhance the business with better insights and relevant data products. You will partner with all data teams at Shutterstock to understand requirements and collaborate on providing solutions at scale.

Responsibilities:

Determine optimal solutions for integrating data from a variety of sources into a common data warehouse
Implement, maintain and monitor batch & stream data pipelines with best practice quality controls
Evaluate relevant new and mature technologies as needs, gaps, and opportunities arise
Work closely and collaboratively in an Agile environment with our analysts, engineers and product teams to analyze issues and find new insights covering our business and operations
Collaborate with our other data teams, as they are customers
Day to day operational support of data infrastructure, and services

Requirements:

Solid command Java or Scala, and Python
Experience in stream processing technology (Kafka, Spark, Storm, Samza, Flink, etc)
Knowledge of SQL, data modeling and data warehousing concepts
Distributed and low latency (streaming) application architecture
Familiarity with API design
CI/CD systems experience (Jenkins, Github, etc)
Experience adhering to robust audit standards
BS or MS degree in Computer Science or Engineering related experience

Bonus Skills:

Knowledge of emerging data integration technologies
Spark Streaming, Kafka Streams, Kafka Connect, etc
Exposure to Kubernetes and Helm
Data Science and Modeling pipeline experience
Familiarity with machine learning frameworks such as H2O, scikit-learn or similar tools
Strong expertise/background with Linux
Familiarity with MS Sql Server, SSIS, SSAS

About Shutterstock, Inc.

Shutterstock, Inc. (NYSE: SSTK ( https://studio-5.financialcontent.com/prnews?Page=Quote&Ticker=SSTK )), directly and through its group subsidiaries, is a leading global provider ofhigh-quality licensed photographs ( https://www.shutterstock.com/ ),vectors ( https://www.shutterstock.com/vectors ),illustrations ( https://www.shutterstock.com/category/illustrations-clip-art ),videos ( https://www.shutterstock.com/video/ ) andmusic ( https://www.shutterstock.com/music/ ) to businesses, marketing agencies and media organizations around the world. Working with its growing community of over 750,000 contributors, Shutterstock adds hundreds of thousands of images each week, and currently has more than 260 million images and more than 14 million video clips available.

Headquartered in New York City, Shutterstock has offices around the world and customers in more than 150 countries. The company also ownsBigstock ( https://www.bigstockphoto.com/ ), a value-oriented stock media offering; Shutterstock Custom,a custom content creation platform ( https://www.shutterstock.com/custom ), Offset, ahigh-end image collection ( https://offset.com/ ); PremiumBeat a curatedroyalty-free music ( https://www.premiumbeat.com/ ) library; and Shutterstock Editorial, a premier source ofeditorial images ( https://www.shutterstock.com/editorial ) for the world's media.

For more information, please visitwww.shutterstock.com ( https://www.shutterstock.com/ ) and follow Shutterstock onTwitter ( https://twitter.com/shutterstock ) and onFacebook ( https://facebook.com/shutterstock ).

Equal Opportunity Employer, M/F/D/V"
78,Senior Python Data Engineer,"New York, NY",New York,NY,None Found,None Found,"At least 5 years of software development experience deploying enterprise applications related to Data Management/Data Analytics & reporting.
2+ years being part of Agile teams – Scrum or Kanban
2+ years of programming experience in Python.
5+ years of Database experience – SQL, Teradata, Oracle
2+ years of experience with big data technologies - Hadoop, Hive, Spark (PySpark or Spark Scala)
Experience with Git/SVN or similar configuration management tool
Experience with ETL tools like Informatica is a plus
Experience with Reporting tools like Tableau/Looker is a plus.
Excellent troubleshooting skills
Strong communication skills
Fluent in BDD and TDD development methodologies
Work in an agile CI/CD environment (Jenkins experience a plus)
Prior experience with Health care domains is a plus
",None Found,None Found,None Found,None Found,"It's fun to work in a company where people truly BELIEVE in what they are doing!


We're committed to bringing passion and customer focus to the business.
Cigna, a leading Health Services company, is looking for exceptional data engineers in our Data & Analytics Engineering organization. The Senior Python Data Engineer is responsible for the delivery of a business need end-to-end starting from understanding the requirements to deploying the software into production. This role requires you to be fluent in some of the critical technologies (specifically Python, ETL and AWS) with proficiency in others and have a hunger to learn on the job and add value to the business. Critical attributes of being a Senior Python Data Engineer, among others, is Ownership & Accountability.
In addition to Delivery, the Senior Python Data Engineer should have an automation first and continuous improvement mindset. He/She should drive the adoption of CI/CD tools and support the improvement of the tools sets/processes.
This position is for a team that works on managing data related to capitalized payments to providers and Priced Only claims. In addition, team is responsible for maintaining applications (like Claim Services & Claim Events) that perform analytics on claims data and that is further used in multiple reporting and trending applications as downstream consumers.
Behaviors of a Senior Python Data Engineer :
Able to articulate clear business objectives aligned to technical specifications and work in an iterative, agile pattern daily. They have ownership over their work tasks, and embrace interacting with all levels of the team and raise challenges when necessary. We aim to be cutting-edge engineers – not institutionalized developers.
Key Characteristics:
Minimize ""meetings"" to get requirements and have direct business interactions
Write referenceable & modular code
Design and architect the solution independently
Be fluent in particular areas and have proficiency in many areas
Have a passion to learn
Take ownership and accountability
Understands when to automate and when not to
Have a desire to simplify
Be entrepreneurial / business minded
Have a quality mindset, not just code quality but also to ensure ongoing data quality by monitoring data to identify problems before they have business impact
Take risks and champion new ideas
Qualifications:
At least 5 years of software development experience deploying enterprise applications related to Data Management/Data Analytics & reporting.
2+ years being part of Agile teams – Scrum or Kanban
2+ years of programming experience in Python.
5+ years of Database experience – SQL, Teradata, Oracle
2+ years of experience with big data technologies - Hadoop, Hive, Spark (PySpark or Spark Scala)
Experience with Git/SVN or similar configuration management tool
Experience with ETL tools like Informatica is a plus
Experience with Reporting tools like Tableau/Looker is a plus.
Excellent troubleshooting skills
Strong communication skills
Fluent in BDD and TDD development methodologies
Work in an agile CI/CD environment (Jenkins experience a plus)
Prior experience with Health care domains is a plus
Qualified applicants will be considered without regard to race, color, age, disability, sex (including pregnancy), childbirth or related medical conditions including but not limited to lactation, sexual orientation, gender identity or expression, veteran or military status, religion, national origin, ancestry, marital or familial status, genetic information, status with regard to public assistance, citizenship status or any other characteristic protected by applicable equal employment opportunity laws.
If you require an accommodation based on your physical or mental disability please email: SeeYourself@cigna.com. Do not email SeeYourself@cigna.com for an update on your application or to provide your resume as you will not receive a response."
79,"Senior Data Engineer, Peacock, Direct-to-Consumer","New York, NY",New York,NY,None Found,None Found,"Experience of near Real Time & Batch Data Pipeline development in a similar Big Data Engineering roleProgramming skills in one or more of the following: Java, Scala, R, Python, SQL and experience in writing reusable/efficient code to automate analysis and data processesExperience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from advertising, web analytics, and consumer devicesExperience implementing scalable, distributed, and highly available systems using Google CloudHands on programming experience of the following (or similar) technologies: Apache Beam, Scio, Apache Spark, and Snowflake.Experience in progressive data application development, working in large scale/distributed SQL, NoSQL, and/or Hadoop environment.Build and maintain dimensional data warehouses in support of BI toolsDevelop data catalogs and data cleanliness to ensure clarity and correctness of key business metricsExperience building streaming data pipelines using Kafka, Spark or FlinkData modelling experience (operationalizing data science models/products) a plusBachelors’ degree with a specialization in Computer Science, Engineering, Physics, other quantitative field or equivalent industry experience.",None Found,None Found,None Found,"Experience of near Real Time & Batch Data Pipeline development in a similar Big Data Engineering roleProgramming skills in one or more of the following: Java, Scala, R, Python, SQL and experience in writing reusable/efficient code to automate analysis and data processesExperience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from advertising, web analytics, and consumer devicesExperience implementing scalable, distributed, and highly available systems using Google CloudHands on programming experience of the following (or similar) technologies: Apache Beam, Scio, Apache Spark, and Snowflake.Experience in progressive data application development, working in large scale/distributed SQL, NoSQL, and/or Hadoop environment.Build and maintain dimensional data warehouses in support of BI toolsDevelop data catalogs and data cleanliness to ensure clarity and correctness of key business metricsExperience building streaming data pipelines using Kafka, Spark or FlinkData modelling experience (operationalizing data science models/products) a plusBachelors’ degree with a specialization in Computer Science, Engineering, Physics, other quantitative field or equivalent industry experience.","Position Overview:

NBCUniversal, the global media company that brought you some of the world’s most iconic television and film franchises, including: The Tonight Show, Saturday Night Live, Keeping Up With The Kardashians, The Real Housewives, Mr. Robot, The Voice, This Is Us, The Fast & The Furious, Jurassic Park, Minions, and more - is launching an all-new direct-to-consumer streaming service. It will seamlessly bring together the breadth and depth of NBCU’s broadcast and cable television series, movie titles, premier sporting events, and renowned news reporting... all in one destination… all in one app.

Introducing Peacock, NBCUniversal's new streaming service that combines timeless shows and movies, exclusive originals, kids programming and current hits, with timely news, sports and pop culture. All together. All in one app.

In preparation for our launch in 2020, we are building a world-class team of smart, hungry and fearless professionals who are energized by the possibility of working at the epicenter of content, technology and culture. Join us if you would like to be a part of this exciting initiative.

As part of the Direct-to-Consumer Decision Sciences team, the Senior Data Engineer will be responsible for creating a connected data ecosystem that unleashes the power of our streaming data. We gather data from across all customer/prospect journeys in near real-time, to allow fast feedback loops across territories; combined with our strategic data platform, this data ecosystem is at the core of being able to make intelligent customer and business decisions.

In this role, the Senior Data Engineer will lead and be responsible for the development and maintenance of an optimized and highly available data pipelines that facilitate deeper analysis and reporting by the business, as well as support ongoing operations related to the Direct to Consumer data ecosystem.

Design, build, test, scale and maintain data pipelines from a variety of source systems and streams (Internal, third party, cloud based, etc.), according to business and technical requirements.
Deliver observable, reliable and secure software, embracing “you build it you run it” mentality, and focus on automation and GitOps.
Continually work on improving the codebase and have active participation in all aspects of the team, including agile ceremonies.
Take an active role in story definition, assisting business stakeholders with acceptance criteria.
Work with Principal Engineers and Architects to share and contribute to the broader technical vision.
Develop and champion best practices, striving towards excellence and raising the bar within the department.
Develop solutions combining data blending, profiling, mining, statistical analysis, and machine learning, to better define and curate models, test hypothesis, and deliver key insights
Operationalize data processing systems (dev ops)
Qualifications/Requirements
Experience of near Real Time & Batch Data Pipeline development in a similar Big Data Engineering roleProgramming skills in one or more of the following: Java, Scala, R, Python, SQL and experience in writing reusable/efficient code to automate analysis and data processesExperience in processing structured and unstructured data into a form suitable for analysis and reporting with integration with a variety of data metric providers ranging from advertising, web analytics, and consumer devicesExperience implementing scalable, distributed, and highly available systems using Google CloudHands on programming experience of the following (or similar) technologies: Apache Beam, Scio, Apache Spark, and Snowflake.Experience in progressive data application development, working in large scale/distributed SQL, NoSQL, and/or Hadoop environment.Build and maintain dimensional data warehouses in support of BI toolsDevelop data catalogs and data cleanliness to ensure clarity and correctness of key business metricsExperience building streaming data pipelines using Kafka, Spark or FlinkData modelling experience (operationalizing data science models/products) a plusBachelors’ degree with a specialization in Computer Science, Engineering, Physics, other quantitative field or equivalent industry experience.
Desired Characteristics
Master’s Degree with a specialization in Computer Science, Engineering, Physics or another quantitative field a plus.Experience with graph-based data workflows using Apache AirflowExperience building and deploying ML pipelines: training models, feature development, regression testingStrong Test-Driven Development background, with understanding of levels of testing required to continuously deliver value to production.Experience with large-scale video assetsAbility to work effectively across functions, disciplines, and levelsTeam-oriented and collaborative approach with a demonstrated aptitude, enthusiasm and willingness to learn new methods, tools, practices and skillsExhibit a bias for getting the job doneAbility to recognize discordant views and take part in constructive dialogue to resolve themPride and ownership in your work and confident representation of your team to other parts of NBCUniversal
Sub-BusinessDirect-to-Consumer
Career Level
Experienced
CityNew York
State/Province
New York
CountryUnited States
About Us
At NBCUniversal, we believe in the talent of our people. It’s our passion and commitment to excellence that drives NBCU’s vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It’s what makes us uniquely NBCU. Here you can create the extraordinary. Join us."
80,Data Engineer,"New York, NY 10010",New York,NY,10010,None Found,"Experience using Snowflake and Google Cloud Platform preferred.
Experience with AWS ecosystem. Some preferred services are Redshift, RDS, S3, and SWF.
Proven experience with ETL frameworks (Airflow, Luigi, or our own open sourced garcon ).
Expertise with at least one distributed data stores (Redshift, Cassandra, Snowflake).
Familiarity with noSQL technologies (mongoDB, DynamoDB).
Proficient in scripting language of choice. Python is strongly preferred, PHP a plus.
Highly proficient in writing SQL for a relational datastore (MySQL, PostgreSQL).
Knowledge of technologies that can deal with Big Data is a Big Plus (Kafka, Spark, Hive, Hadoop/MapReduce).
Ability to write automated tests (unit, functional, and integration) to ensure code works as expected.
Desire to collaborate with other engineers through peer code reviews.
Deep understanding of data structures and schema design.
Detail-oriented, proactive problem solving skills.",None Found,"Create and maintain systems to load and transform very large data sets from digital media retailers (iTunes, Spotify, YouTube, etc) as well as social media sources.
Work with a cross-functional team to create data-driven insights and reports for business stakeholders.
Work with other members of the team to create customer-facing analytics tools and visualizations.
Process millions of rows of data daily to provide analytics to our end users.
Take advantage of our continuous integration and deployment.
Participate in technical design and peer review for new projects.",None Found,None Found,"Overview
Sony Music Entertainment is a global recorded music company with a roster of current artists that includes a broad array of both local artists and international superstars, as well as a vast catalog that comprises some of the most important recordings in history. Sony Music Entertainment is a wholly owned subsidiary of Sony Corporation of America.

Sony Music is committed to providing equal employment opportunity for all persons regardless of age, disability, national origin, race, color, religion, sex, sexual orientation, gender, gender identity or expression, pregnancy, veteran or military status, marital and civil partnership/union status, alienage or citizenship status, creed, genetic information or any other status protected by applicable federal, state, or local law.

The Data Engineer will report to the Head of A&R Research. This role will explore and build products using the latest and greatest in Data Analytics, Cognitive Services, Machine Learning and more. This role is based at Sony Music’s offices in New York
Responsibilities
Create and maintain systems to load and transform very large data sets from digital media retailers (iTunes, Spotify, YouTube, etc) as well as social media sources.
Work with a cross-functional team to create data-driven insights and reports for business stakeholders.
Work with other members of the team to create customer-facing analytics tools and visualizations.
Process millions of rows of data daily to provide analytics to our end users.
Take advantage of our continuous integration and deployment.
Participate in technical design and peer review for new projects.
Qualifications
Experience using Snowflake and Google Cloud Platform preferred.
Experience with AWS ecosystem. Some preferred services are Redshift, RDS, S3, and SWF.
Proven experience with ETL frameworks (Airflow, Luigi, or our own open sourced garcon ).
Expertise with at least one distributed data stores (Redshift, Cassandra, Snowflake).
Familiarity with noSQL technologies (mongoDB, DynamoDB).
Proficient in scripting language of choice. Python is strongly preferred, PHP a plus.
Highly proficient in writing SQL for a relational datastore (MySQL, PostgreSQL).
Knowledge of technologies that can deal with Big Data is a Big Plus (Kafka, Spark, Hive, Hadoop/MapReduce).
Ability to write automated tests (unit, functional, and integration) to ensure code works as expected.
Desire to collaborate with other engineers through peer code reviews.
Deep understanding of data structures and schema design.
Detail-oriented, proactive problem solving skills."
81,Google Data Engineer,"New York, NY 10011",New York,NY,10011,None Found,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
",DevOps on an GCP platform. Multi-cloud experience a plus.,None Found,None Found,"Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet today’s high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps – Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
82,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We're working to change the way wealth is created.
YieldStreet is disrupting the trillion-dollar lending and investment industry. We recently raised $113M in financing from world-class venture capitalists to help us scale. Our investor demand is soaring, and our team is expected to double in size this year. Do you have what it takes to join a booming startup that is changing the investment industry?

About YieldStreet

YieldStreet connects investors to opportunities in alternative asset classes like real estate and litigation finance that were historically unavailable to most. YieldStreet was created because our founders were frustrated by the lack of options regular investors had to participate in asset backed investment opportunities that exist outside of the stock market.
We believe top notch technology and data can help us build the worlds most robust alternative investment platform and create financial independence for all. Join us.

About this role

YieldStreet is looking to expand our data science organization by adding an additional Data Engineer dedicated to our platform. In this role you will be responsible for designing, building and scaling our data infrastructure. You will be the bridge between our engineering, data science and business teams - owning the data stack as well as aggregating, analyzing, predicting, and visualizing data and performance. If you are looking to expand your skillset in one of NYC's fastest growing startups and work with an extraordinary team, apply below!

What you'll do


Partner with our engineering team to build and optimize data pipelines from our database, website analytics, marketing analytics, and other financial data sources
Build a data warehouse that is easily accessible and understandable for business teams to run saved reports and ad-hoc analysis
Partner with our data science team to uncover investor and underwriting insights
Partner with our marketing, product and investments team with special projects and ad-hoc analysis
Build processes for ensuring data integrity

Requirements


BS/MS degree or track record of success in a heavily quantitative field
2+ years of experience, including experience designing and scaling data infrastructure, models, and pipelines
Advanced fluency in SQL, NoSQL & Excel
Experience with a major coding language (e.g: Python, R)
Deep knowledge around a variety of data tools and technologies including data warehouse and data visualization

What Makes You Successful

Own and Execute:

Gets things done, with both a short and long-term view in mind
Be able to thrive in a fast-paced, agile environment with exceptional organizational skills and the ability to re-prioritize on a consistent basis
Exemplary planning and organizational skills, along with a high attention to detail

Decide with Data:

Be pragmatic and outcome-oriented, leveraging data to make decisions
Detect new or missed opportunities that will help YieldStreet grow and improve regularly
Be able to package data so it can be easily digested, analyzed and reacted to

Be Passionate:

Bring a focus of YieldStreet’s vision to your day to day tasks
Bet on your strengths; allow these skills to be absorbed by leaders on your team to make an impact
Be known for authenticity, honesty, and integrity

Stay Curious:

Never take no for an answer, be able to dissect an issue and come up with creative solutions
Ask questions. At YieldStreet we believe that progress stops when you stop asking questions
Focus on team collaboration and communication

Benefits

We offer an attractive market salary, stock option plan, health & dental benefits, life insurance, 401(k), paid vacation and holidays and that's before you even step in the office!
This is an opportunity to work with a group of diverse, smart, and friendly people from 8 different countries who speak a total of 17 different languages. Our team is comprised of successful entrepreneurs with combined exits of over $1B, and we get social with each other during happy hours in our work hard/play hard culture. Make sure you get to know Diver the fish, and help yourself to a variety of top tier scotch and whiskies at the company bar.
We're located in a beautiful new office in Midtown, our building is close to most major subway lines.
*** Please, no solicitors or recruiters. We're saving our dollars to pay the right candidate!"
83,Master Data Management Engineer,"New York, NY 10041",New York,NY,10041,None Found,"Excellent attention to detail and accuracy; strong critical self-review skills.
Demonstrates ability to deliver communication which is clear, concise, and relevant to audience through appropriate methods and tools.
Appreciates what constitutes good customer service and displays consistent commitment to delivering.
Exhibits a high degree of professionalism.
Organized methodical application of established data governance standards.
Proactive approach to role and problem solving; solution rather than problem focused.
Able to work collaboratively and communicate effectively with key stakeholders both within and outside of the MDM team to get the job done.
Comfortable pushing back with customers and stakeholders to protect the integrity of the data management process.
Inquisitive and thorough in approach.
Displays a passion for working in master data management.
Self-motivated, flexible, with the ability to deal with high levels of complexity, change and evolving processes, often at short notice.
","Excellent attention to detail and accuracy; strong critical self-review skills.
Demonstrates ability to deliver communication which is clear, concise, and relevant to audience through appropriate methods and tools.
Appreciates what constitutes good customer service and displays consistent commitment to delivering.
Exhibits a high degree of professionalism.
Organized methodical application of established data governance standards.
Proactive approach to role and problem solving; solution rather than problem focused.
Able to work collaboratively and communicate effectively with key stakeholders both within and outside of the MDM team to get the job done.
Comfortable pushing back with customers and stakeholders to protect the integrity of the data management process.
Inquisitive and thorough in approach.
Displays a passion for working in master data management.
Self-motivated, flexible, with the ability to deal with high levels of complexity, change and evolving processes, often at short notice.
",None Found,"2+ years of business experience
2+ years of analytical experience
Bachelor’s Degree in relevant field of study or within one year of completing Bachelor’s Degree and with 3+ years of relevant work experience.
",None Found,"Bruin helps organizations transition their telecom, data and wireless services when business needs change. From online ordering to white glove support, we enable full lifecycle control of your telecom.

We’re not like other teams. The Telecommunication Expense Management (TEM) space is filled with archaic software and boring slogans. Bruin is blazing a new path and building a team of experts with a passion for creating great software products and outstanding customer relationships.

We have high expectations and a career with Bruin means challenging yourself to always be better.

Role
Bruin is looking for a senior level, master data engineer to help build and enhance capabilities on our web app experience, built using JavaScript based libraries and an MSSQL / SOLR backend. Our team’s charter is to revolutionize our current UX, while keeping up with the ever-growing user-base. You will be working in a small, focused scrum team obsessed with creating elegant, yet simple user-interaction for complex workflows that will delight our customers. You should be able to seamlessly integrate into agile teams and work closely with all facets of the organization (products, back-end development, QA, UX, and Design) to achieve high quality results. This position will support the application by monitoring and analyzing master data, key data, and master relationship data within the organization.

This high impact, high visibility role is responsible for ensuring all master data is accurate, complete and consistent across the enterprise. The team drives development of policies and procedures in concert with process owners and governance councils, conducts audits, performs analysis, and recommends and implements improvements. The team supports current implementation projects and performs ongoing guidance after through the production environment.

Ensures master data integrity in key systems as well as maintaining the processes to support the data quality.
Identifies areas for data quality improvements and helps to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design strategies.
Ensures quality of master data in key systems, as well as, development and documentation of processes with other functional data owners to support ongoing maintenance and data integrity.
In collaboration with subject matter experts and data stewards, defines and implements data strategy, policies, controls, and programs to ensure the enterprise data is accurate, complete, secure, and reliable.
Provides assistance in resolving data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design strategies collaborating with subject matter experts (SMEs) and data stewards.
Manages, analyzes, and resolves data initiative issues and manages revisions needed to best meet internal and customer requirements while adhering to published data standards.
Assists in data management, governance, and data quality of master data requirements with other functional data owners to ensure functional master data integrity across the operation of financial systems is consistent and meets stated business rules and requirements.
Work closely with the business/IT to ensure alignment of master data rules and the operations of the application meet all requirements.
Essential Job Functions
Defines, designs, and builds dimensional databases to meet business needs.
Assists in the application and implementation procedures of data standards and guidelines on data ownership, coding structures, and data replication to ensure access to and integrity of data sets.
Conducts data cleaning to rid the system of old, unused data, or duplicate data for better management and quicker access.
Researches, coordinates and installs software releases and database integration to ensure
Develops and implements strategies to translate business requirements and models into feasible and acceptable data warehouse designs to ensure that business needs are met.
Provides data consulting in support of business and information technology initiatives to clients to improve client database systems.
Skills/Qualifications
Essential
Excellent attention to detail and accuracy; strong critical self-review skills.
Demonstrates ability to deliver communication which is clear, concise, and relevant to audience through appropriate methods and tools.
Appreciates what constitutes good customer service and displays consistent commitment to delivering.
Exhibits a high degree of professionalism.
Organized methodical application of established data governance standards.
Proactive approach to role and problem solving; solution rather than problem focused.
Able to work collaboratively and communicate effectively with key stakeholders both within and outside of the MDM team to get the job done.
Comfortable pushing back with customers and stakeholders to protect the integrity of the data management process.
Inquisitive and thorough in approach.
Displays a passion for working in master data management.
Self-motivated, flexible, with the ability to deal with high levels of complexity, change and evolving processes, often at short notice.
Desirable
Experience maintaining master data in MS SQL.
Experience with full stack development, preferably in one or more of OO programming languages, like C#, Java, C++
Hands-on, professional experience with consuming and building RESTful web-services.
Experience in working in an agile development environment
Demonstrated mid-level or above proficiency with MS Office Excel, analyzing and manipulating large datasets through formulas and macros.
Experience participating in technology implementation projects.
Proficiency in JIRA, Atlassian, and Git.
Strong analytical abilities with experience extracting data and developing reports.
Strong interest in systems with demonstrable systems mindset and exposure to a large enterprise systems.
Understanding of SDLC and project management methodology.
Experience defining, writing and implementing business processes and data standards in one or more data domains.
CPA, Six Sigma, PMP, or similar.
Education and Experience
Essential
2+ years of business experience
2+ years of analytical experience
Bachelor’s Degree in relevant field of study or within one year of completing Bachelor’s Degree and with 3+ years of relevant work experience.
Desirable
Exposure to the business functions of a telecommunications enterprise from an operations, finance, supply chain, or business analysis perspective.
Master’s Degree in Computer Science and/or Data Science
Job Type: Full-time/Permanent
Job Location: New York, NY

Bruin participates in the E-Verify Program and visa sponsorship

To apply, send a cover letter and resume/CV to jobs@bruin.com."
84,Data Engineer – Royalties,"New York, NY 10011",New York,NY,10011,None Found,None Found,"Deep knowledge of software engineering best practices
Experience in mentoring and leading junior engineers
Experience in serving as the technical lead for complex software development projects
Experience with large scale distributed data technologies and tools
Strong coding skills for analytics and data engineering (Scala, Java and Python)
Experience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Google’s Cloud Platform
T-Shaped. Your primary area is data engineering but you are comfortable working in a second area such as data presentation, backend engineering or front-end development
Experience working in a large scale, global consumer product company, in an engineering or insights role
Understands how to translate business requirements to technical architectures and designs
Comfortable communicating with stakeholders (customers, product managers, C-level management)",None Found,None Found,None Found,"We are looking for an experienced software engineer to join our Financial Engineering organisation. Our mission is to build the technical platform that underpins our key company decisions, revenue and royalty calculations, financial performance management and our ability to scale out to new markets and products. As a software engineer for our data products, you will join a team of experienced software engineers and data scientists. Some of the challenges we tackle are:

Design and implement complex logic for calculating royalties from many products and hundreds of tailor-made license contracts
Design large scale data pipelines (billions and billions of data points) that can retrospectively correct metrics over a long period of time without reprocessing huge amounts of data
Design, implement and operate our company core datasets that have extreme requirements on scalability, flexibility, and quality
Evolve and scale our fraud platform by developing rules and machine learning models that detect and continuously adapt to ever-shifting behaviors, swiftly mitigating their effects on the company.
Continuously consume and produce massive amounts of data while optimising for speed, accuracy, and quality
Innovate our data products to create a single coherent platform with sources of truth that serve a plethora of stakeholders from Spotify feature teams to our ad sales organisation
Above all, working as a software engineer in Financial Engineering will challenge your design, quality and problem-solving skills to build robust, highly distributed and scalable data processing systems and pipelines.

What you will do
Apply your expertise in software engineering to design and implement data products that meet extreme requirements on quality and scalability
Work closely with cross-functional teams of data and backend engineers, scientists, user researchers, product managers and designers
Research and Innovate what fuels many of Spotify’s critical financial systems and product features such as Daily Mixes and Royalty calculations
Gaining technical expertise in building a data platform at scale to solve business, product and technical use cases
Getting hands-on experience with Google Cloud Platform and technology/languages such as BigQuery, Scala, Scio and Docker
Working hand-in-hand with the data science community to understand various user or content trends that influence product changes and customer acquisition strategies
Cross departmental exposure and flexibility to engage with many teams in the company – both in Stockholm and New York
Communicate insights and recommendations to key stakeholders, engineering and product partners
Who you are
Master’s degree in Computer Science or Electrical Engineering
4+ years of professional software engineering and programming experience (Java, C++, Scala)
3+ years of architecture and design (patterns, reliability, scalability, quality) of complex systems
Advanced coding skills and practices (concurrency, distributed systems, functional principles, performance optimization)
Professional experience working in an agile environment
Strong analytical and problem solving ability
Strong written and verbal communication skills
Experience in operating and maintaining production grade software
Comfortable with tackling very loosely defined problems and thrive when working on a team which has autonomy in their day to day decisions
Preferred Skills
Deep knowledge of software engineering best practices
Experience in mentoring and leading junior engineers
Experience in serving as the technical lead for complex software development projects
Experience with large scale distributed data technologies and tools
Strong coding skills for analytics and data engineering (Scala, Java and Python)
Experience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Google’s Cloud Platform
T-Shaped. Your primary area is data engineering but you are comfortable working in a second area such as data presentation, backend engineering or front-end development
Experience working in a large scale, global consumer product company, in an engineering or insights role
Understands how to translate business requirements to technical architectures and designs
Comfortable communicating with stakeholders (customers, product managers, C-level management)"
85,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"OVERVIEW:
The Data and Analytics organization in GroupM is responsible for managing and analyzing massive amounts of data as well as supporting the whole organization in delivering value from data. Our products are hosted on a variety of infrastructure options including but not limited to local servers, public and private clouds and enable a suit of advanced analytics-based products, specific Data collection modules and customized Data transformation packages.

You are a Data Engineer who can:
Steadily and consistently perform routine activities
Use a systematic approach to organizing work
Present instructions in a methodical, step-by-step manner
Respect established rules and processes
Use a deliberate, methodical approach to solving problems
Develop efficient approaches that improve performance and maintain quality
Develop strict, rigid routines
Closely follow a specific plan for how things will be done

3 Months
Effectively perform the ETL process, the goal being error free data sets. Use a combination of Python, Alteryx, SQL and HIVE to cut, query and QA.
6 Months
Create your own workflows in Alteryx, unsupervised, and publish an app. [eg: automation of metadata]
Tech support for proprietary software which takes the output from media mix models, and generates forecasts with multiple dimensions (budgets, sales).
Code maintenance and development work with the back-end tables in Python/Django, SQL with a goal of improving overall product performance.
12 Months
Enhance the Alteryx published app ecosystem through the pro-active identification and fix of problems.

BEST THINGS ABOUT THE JOB:
Access to infinite data. Opportunities to work creatively and tell stories with highly varied data.
High quality, large international clients.
ABOUT [m]PLATFORM:
[m]Platform is GroupM’s suite of proprietary technology and data that enables truly individualized media planning and activation. Backed by the resources and client roster of the world’s largest media agency group, [m]Platform has the product, organization and scale to transform the way the largest advertisers approach media: from the traditional broadcast model that still dominates our world to an addressable, audience-first model that elevates the power and role of media in marketing.

Take A Virtual Office Tour: https://roundme.com/tour/368213/

GroupM and all its affiliates embrace and celebrate diversity, inclusivity, and equal opportunity. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. We are a worldwide media agency network that represents global clients. The more inclusive we are, the greater work we can create together."
86,Data Engineer / Sr. Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,"
Provide recommendations on data team roles and responsibilities as the company continues to grow
Help manage technical planning for all client implementations to ensure the data team is successfully meeting deadline and project deliverables
Identifying areas of opportunities and providing machine learning solutions to enhance AllazoEngine
Build client reporting from performance monitoring to ad hoc requests
Provide planning and continuous development of the database architecture
",None Found,"
2-4 years of experience in advanced SQL
Experience in SSIS, Talend or any other ETL tools.
Experience in performance tuning and reports development.
Demonstrated success in high growth and early-stage environment
Demonstrated GSD ""Get Stuff Done"" attitude and results
Strong influencer and communicator across all levels of the organization
Detail and metric-oriented
Proficient in Microsoft Office and technology
","Company: AllazoHealth
Location: New York City

AllazoHealth, one of the fastest growing health-tech startups in NYC, combines
behavioral science with data mining and machine learning to help stakeholders across
the healthcare spectrum influence patients to adopt healthier behaviors. Our propriety
AI platform, AllazoEngine TM provides individualized intervention targeting for medication
adherence, disease management, and wellness programs.

Position: Data Engineer / Sr. Data Engineer

We are currently seeking an experienced data engineer with a background of data ware house modeling and ETL development.

Requirements:

2-4 years of experience in advanced SQL
Experience in SSIS, Talend or any other ETL tools.
Experience in performance tuning and reports development.
Demonstrated success in high growth and early-stage environment
Demonstrated GSD ""Get Stuff Done"" attitude and results
Strong influencer and communicator across all levels of the organization
Detail and metric-oriented
Proficient in Microsoft Office and technology

Responsibilities:

Provide recommendations on data team roles and responsibilities as the company continues to grow
Help manage technical planning for all client implementations to ensure the data team is successfully meeting deadline and project deliverables
Identifying areas of opportunities and providing machine learning solutions to enhance AllazoEngine
Build client reporting from performance monitoring to ad hoc requests
Provide planning and continuous development of the database architecture

Benefits:

Very competitive compensation package including cash and equity stock options
Medical/Dental/Vision Benefits
Flat organizational style which empowers everyone in the company to help achieve both company and personal goals
Weekly team outings and high focus on continuously building team rapport and culture as we continue on this incredible growth curve

Company description:
Allazo Health is a high-growth, profitable healthtech start-up founded in 2012 that is leading the development and deployment of revolutionary analytical solutions which is helping patient outcomes and medical costs by influencing patients to adopt healthier behaviors.

If you are passionate about healthcare, are driven to save lives while delivering tremendous value to Fortune 100 customers, and want to make a significant impact within a tremendously fast-growing company, then this is the job for you.

Our customers include some of the largest pharma companies, PBMs, payors, and providers. Backed by extremely sophisticated investors and a strong management team, including a CEO and Founder who built medication adherence programs at one of the industry's leaders, we invite to join us as we revolutionize analytics in the healthcare industry."
87,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,"
Highly proficient in Python
Proficient working with data and distributed systems
Experience with Python scientific libraries such as SciPy, Scikit, Pandas, and NumPy
Experience conducting methods using any of the following: machine learning, predictive modeling, statistical inference, experimental design, data mining, and optimization
Experience in Linux/Unix environment and shell scripting
Experience with ETL
Deep understanding of data structures and schema design
Familiarity with both SQL and NoSQL technologies
Experience with AWS and Azure platforms a plus
Knowledgeable about data modeling, data access, and data storage techniques
Critical thinking: ability to track down complex data and engineering issues, evaluate different algorithmic approaches, and analyze data to solve problems
Creativity: conceive of new data driven products, features, and technologies
","Using neuroscience-based assessments and machine learning algorithms, pymetrics is reinventing the recruiting industry by matching candidates to jobs and companies where they are most likely to succeed. We are leading the charge in an evolving industry, and growing our amazing team to support the mission of using data to unleash one's full potential.

We are looking for a talented Data Engineer to help scale all aspects of our business. You will have the opportunity to work with multiple teams to design and build data driven solutions. Therefore the ideal candidate will be able to wear multiple hats to solve a wide variety of data related challenges while working alongside both technical and non-technical team members.
WHAT YOU’LL DO
Build batch and real-time data pipelines
Collaborate with data scientist and machine learning experts to optimize and scale the predictive modeling process
Design well-structured APIs, services, and architectures for new project objectives
Help improve and expand internal tools for data delivery, access, and analysis
Analyze and visualize data for both internal and client facing use cases
Data quality control
Work with team leads to prioritize business and technology needs
Find and use the right tool for the job and integrate with existing architecture
CURRENT TECHNOLOGIES
Python/Django/Flask
Javascript/Angular/React
Jupyter
AWS/Azure
MySQL/PostgresSQL/Redshift/MongoDB/CosmosDB
RabbitMQ/Celery
Airflow
Docker/Packer/Terraform
Requirements
Highly proficient in Python
Proficient working with data and distributed systems
Experience with Python scientific libraries such as SciPy, Scikit, Pandas, and NumPy
Experience conducting methods using any of the following: machine learning, predictive modeling, statistical inference, experimental design, data mining, and optimization
Experience in Linux/Unix environment and shell scripting
Experience with ETL
Deep understanding of data structures and schema design
Familiarity with both SQL and NoSQL technologies
Experience with AWS and Azure platforms a plus
Knowledgeable about data modeling, data access, and data storage techniques
Critical thinking: ability to track down complex data and engineering issues, evaluate different algorithmic approaches, and analyze data to solve problems
Creativity: conceive of new data driven products, features, and technologies
Benefits
Health Care Plan (Medical, Dental & Vision)
Paid Time Off (Vacation, Sick & Public Holidays)
Family Leave (Maternity, Paternity)
Training & Development
Wellness Resources
Stock Option Plan
Transportation Reimbursement"
88,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"What You'll Be Doing:

Working on problems that affect the lives of real people. Our users depend on us to make positive changes to their health and their lives.
Basing your work on scientifically-proven, peer-reviewed methodologies that are designed by medical professionals
Collaborating with a team both onsite and offsite -- about 50% of our engineering team is fully remote; we worry about results, not time spent in seats.

What We're Looking For:

Experience dealing with data at scale, processing and transforming hundreds of millions of data points per day.
First-rate SQL skills, but are aware of its limits. You know when to use it, and when it's better to find a different solution.
Familiarity with ETL tools and problems -- we use Airflow, Redshift, Glue, and many other systems.
A team player able to work alongside data analysts and data scientists

What Makes This Job Amazing


You'll be helping millions of people lead healthier lives every day
You'll be a part of Noom's rocketship-- revenue has grown 20x in the last 2 years and our team is growing fast.
You'll have wonderful benefits including healthcare/dental, wellness budget, daily gourmet meals prepared by our onsite chefs, and onsite yoga
You'll experience huge learning & professional growth opportunities. Noom believes in supporting your professional and personal growth: we'll cover the cost of books, courses, conferences… you name it!
You'll add to our transparent, high-performing, and close-knit culture

Noom Inc. is a leader in mobile health coaching. We combine the power of technology with the empathy of real human coaches to deliver successful behavior change. Our direct-to-consumer mobile applications have reached more than 45 million users worldwide-- and counting. We've leveraged our behavior change platform to incorporate the CDC's Diabetes Prevention Program (DPP), and have expanded to programs for diabetes management, pre-hypertension, and hypertension. Our Engineering team is at the forefront of this challenge, solving complex technical problems that center around habits, behavior, and lifestyle.

We are looking for a Data Engineer to join our Data team and help us improve and maintain our Data Warehouse. If you like to work with billions of rows of data and be at the center of data-driven decisions, we'll love working with you."
89,Senior Big Data Engineer,"New York, NY 10010",New York,NY,10010,None Found,None Found,None Found,None Found,None Found,None Found,"Named as one of Fortunes’ 100 Fastest Growing Companies for 2019, EPAM is committed to providing our global team of 30,100+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential.

Description

You are curious, persistent, logical and clever. You enjoy living by the code of your craft and developing mind-blowing solutions for complex problems. You are a true techie at heart. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements.

#LI-DNI
What You’ll Do
Provide technical expertise through a hands-on approach to teams and projects developing capabilities for one or more application within Web IT
Assist in translation of business requirements into meaningful requirements or stories
Architectural roadmap(s) – short term/long term
Design and code
Apply expertise and research designs with subject matter experts to devise requirements, design specifications, and usage criteria for Big Data solutions
Maintain involvement in continuous improvement of Big Data solution processes, tools and templates
Responsible for leading the design and maintaining solutions conformant to enterprise standards, architecture, and technologies
Provide oversight, direction and mentoring to ensure adherence to business direction, architectural strategies, and enterprise technology standards
Offer guidance on use of appropriate Big Data tools to meet project needs
Collaborate with developers and business users for overall design oversight on Big Data solutions
Maintain expertise and proficiency with Big Data technologies using business intelligence best practices
Develop a thorough knowledge and understanding of business domains, heath care analytics, and industry trends in order to develop an enterprise view of Big Data Analytic needs
Demonstrates and promotes creativity and innovation. Proactively seeks out alternative solutions to business problems
What You Have
A degree in an associated field and/or other advanced certification along with significant experience
Expertise in Hadoop and related technologies
Excellent Data analysis skills
Knowledge of Apache Hadoop, Apache Spark (including pyspark), Spark streaming, Kafka, Scala, Python, MapReduce, Yarn, Hive, Oozie, SQL, Impala, HBase
Experience with distribution vendor like Cloudera
Good knowledge of Python
Knowledge of Microservice Architecture is a plus
Ability to rapidly prototype and storyboard/wireframe development as part of application design
Comfortable in configuring and using multiple operating systems (Mac/Windows/*nix)
Design expertise with Hadoop, Teradata, Looker, and Tableau along with Cognos tools is a plus
Knowledge of BI tools and statistical packages such as SAS, R or SciPy/NumPy
Experience with disciplined software development lifecycle
Ability to communicate at various levels within large organizations
Knowledge of Big Data tool performance/tuning/measurement and usage criteria to drive appropriate tool selections and use
Documented experience working on advanced Big Data solutions is a plus
Some expertise in designing business intelligence systems, dashboard reporting, and analytical reporting is also a plus
We offer
Medical, Dental and Vision Insurance (Subsidized)
Health Savings Account
Flexible Spending Accounts (Healthcare, Dependent Care, Commuter)
Short-Term and Long-Term Disability (Company Provided)
Life and AD&D Insurance (Company Provided)
Employee Assistance Program
Unlimited access to LinkedIn learning solutions
Matched 401(k) Retirement Savings Plan
Paid Time Off
Legal Plan and Identity Theft Protection
Accident Insurance
Employee Discounts
Pet Insurance"
90,"Cloud Data Engineer, Revenue Science","New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Who We Are:


As data engineers in Revenue Science, our mission is to build real-time and offline solutions to make data accessible and reliable while leveraging the largest-scale data processing technologies in the world - and then apply them to the Revenue’s most critical and fundamental data problems.


Learn more about some of the challenges we tackle on this team:

Building a Petabyte-scale Data Warehouse (Google Cloud Next '18) https://youtu.be/APBF9Z3uBCc
How Twitter Migrated its On-Prem Analytics to Google Cloud (Google Cloud Next '18) https://youtu.be/sitnQxyejUg


What You’ll Do:


As a member of the Data Engineering team, you will build and own mission-critical data pipelines that are ‘source of truth’ for Twitter’s fundamental revenue data, as well as modern data warehouse solutions, while collaborating closely with Ads Data Science team.


You will be a part of an early stage team and have a significant stake in defining its future with a considerable potential to impact all of Twitter’s revenue and hundreds of millions of users.

You will be among the earliest adopters of bleeding-edge data technologies, working directly with Revenue Science and Revenue Platforms teams to integrate your services at scale.

Your efforts will reveal invaluable business and user insights, leveraging vast amounts of Twitter revenue data to fuel numerous Revenue teams including Ads Analytics, Ads Experience, Ads Data Science, Marketplace, Targeting, Prediction, and many others.


Who You Are:


You are passionate about data and driven to take the data organization challenges at the scope of entire Twitter’s Revenue.


What you’ll need:

Strong programming and algorithmic skills
Experience with data processing (such as Hadoop, Spark, Pig, Hive, MapReduce etc).
Proficiency with SQL (Relational, Redshift, Hive, Presto, Vertica)


Nice to have:

Experience writing Big Data pipelines, as well as custom or structured ETL, implementation and maintenance
Experience with large-scale data warehousing architecture and data modeling
Proficiency with Java, Scala, or Python
Experience with GCP (BigQuery, BigTable, DataFlow)
Experience with Druid or Apache Flink
Experience with real-time streaming (Apache Kafka, Apache Beam, Heron, Spark Streaming)
Ability in managing and communicating data warehouse project plans to internal clients"
91,"Data Engineer, BI Engineering Team (Python/SQL)","New York, NY",New York,NY,None Found,None Found,None Found,"
3+ years of engineering experience in a fast-paced environment; 1+ years of experience in scalable data architecture, fault-tolerant ETL, and monitoring of data quality in the cloud
Proficiency in SQL
Proficiency in Python
Proficiency with modern source control systems, especially Git
Experience working with non-technical business stakeholders on technical projects
",None Found,None Found,None Found,"About the Team:
Vimeo is searching for an experienced Python and SQL software engineer for its Business Intelligence team. This is an opportunity to design and maintain a robust, scalable and sustainable enterprise data platform with other members of the Business Intelligence team. The ideal candidate will enjoy working on a variety of projects at once, with lots of different software services and data sources, all orchestrated with Python code.

Our platform has robust analytics tools that can tell video creators where people are from, where they click, and even track where in a video someone stopped watching—giving you insight into creating engaging content in the future.

While working as a peer to both technical and non-technical staff throughout the company, you will drive the improvement process of both our products and our business operations. You will help define data access and discoverability requirements and work to create infrastructure and services that provide access to event streams.

What you'll do:

Work closely with other BI engineers, BI analysts, and business stakeholders to understand and plan technical requirements for BI projects
Provide decision analysis and decision support to business stakeholders using BI and other data
Learn quickly and deeply about Vimeo's software platform, desktop and mobile applications
Contribute software designs, code, tooling, testing, and operational support to a multi-terabyte BI data platform
Collaborative work: iterative development, design and code review sessions
Independent work: author and maintain tools for other developers, plus regular hackweeks to develop for the Vimeo platform

Skills and knowledge you should possess:

3+ years of engineering experience in a fast-paced environment; 1+ years of experience in scalable data architecture, fault-tolerant ETL, and monitoring of data quality in the cloud
Proficiency in SQL
Proficiency in Python
Proficiency with modern source control systems, especially Git
Experience working with non-technical business stakeholders on technical projects

Bonus Points (Nice Skills to Have, but Not Needed):

Airflow, Celery, or other Python-based task processing systems
Cloud-based devops: AWS or Google Cloud Platform
Relational database design
Kafka, Kinesis, PubSub and other durable, scalable messaging systems
Spark
Pandas or R
Redshift, Vertica, Snowflake, CitusDB, or other distributed columnar-store databases
Experience with Mixpanel, Keen.io, Snowplow, or similar event tracking systems
Experience with any distributed map/reduce framework, in Hadoop or otherwise
Basic Linux/Unix system administration skills

About us:
At Vimeo, our mission is to empower video creators to tell exceptional stories and connect with their audiences and communities. Home to more than 90 million members in over 150 countries, Vimeo is the world's largest ad-free open video platform, providing powerful tools to host, share and sell videos in the highest quality possible.

We work hard to enable creators of all kinds to succeed, and to that end, we prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and creativity. We're committed to building a company and a community where people thrive by being themselves and are inspired to do their best work every day.

Vimeo is based in New York City, with additional offices in Europe and India. Vimeo is an operating business of IAC (NASDAQ: IAC). Learn more at www.vimeo.com."
92,Senior Data Engineer - Diner dynamics Team,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About The Opportunity
Here at Grubhub we’ve been dedicated to giving diners the most convenient way to order food from their favorite restaurants (whether that’s a late night Chalupa from Taco Bell or a salad for lunch from a local restaurant the day after they enjoyed said late night Chalupa).

While we are food-obsessed, we are also customer-obsessed. We look to constantly innovate our technology so our diners’ food experience is memorable, restaurant owners get more business and individuals across the country looking for part-time work can deliver the food from the restaurant to the diner flawlessly.We take great pride in knowing that we are a part of 19+ million diners food ordering experience and we are partnered with 115,000 restaurants in 2,200 US cities across our suite of apps (Grubhub, LevelUp, Seamless, Tapingo).

Want to be a part of the biggest movement in the US that is moving eating forward? If so, we want to talk to you - and hear what’s your favorite restaurant for food delivery!

 About The Job:

We are looking for senior data engineers to help us dramatically expand and improve the capabilities of our big data and machine learning pipeline powering the supply growth and market intelligence initiative. We have been focused on assembling a team of smart, humble people who are passionate about creating innovative, robust, automated solutions for our highly complex business problems on three-way marketplace. This is an unbeatable opportunity for developers who hope to work on and deliver world class data product in a friendly and fun environment.

About team:

The mission of the diner dynamics team at Grubhub is to provide various high quality datasets and insights, to monitor, support and boost market growth at both tactical and strategic levels. The team is responsible for designing, implementing, and maintaining data pipelines powering multiple fundamental datasets used across business departments of Grubhub.
Some Challenges You’ll Tackle
Work with data scientists and analysts to productionize and optimize machine learning models, so they can scale and accommodate various business requirements.
Contribute innovative ideas to efficiently transform and mining geospatial and temporal data.
Work with high volumes of data and distributed systems using technologies such as Spark, Hive AWS EMR, AWS S3, Azkaban, Presto and etc.
Build auditing and monitoring tools for data pipelines and data job executions.
Maintain up-to-date technical documentation while continuously deliver technical solutions and business impacts.
Translate from technical to business, and vice versa. You need to be able to speak with the least technically-minded client (internal or external) and make technology make sense to them. Then turn around and do it the other way.
Actively contribute to the adoption of strong software architecture, development best practices, and new technologies.We are always improving the process of building software; we need you to help contribute
You Should Have
5+ years experience with Python or Scala programming languages
4+ years experience developing large data processing pipelines with Apache Spark.
Excellent knowledge on SQL and data modelling
Background in ETL and data processing, familiar with how to transform data to meet business goals
Willingness to understand larger business context
Growth mindset. Be passionate about continuous learning and knowledge sharing
Familiarity with Agile software development methodologies
Enthusiasm for a fast paced, tech and product oriented environment, and the desire to work with a great team!
And Of Course, Perks!
Unlimited PTO. It’s true, no strings attached and all the time you need to recharge.
Better Benefits. Get quality insurance, flex-spending accounts, retirement options and commuter perks.
Free Food. Kitchens are stocked and free Grubhub each week.
Stock Up. All of our employees are owners, in fact, they’re granted Restricted Stock Units, which means we’re all in it to win it.
Casual Culture. Catch rays on the rooftop or get comfy on a couch and get to know your coworkers — because work, should be a place you want to be.
Grubhub is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics. The EEO is the Law poster is available here: DOL Poster. If you are applying for a job in the U.S. and need a reasonable accommodation for any part of the employment process, please send an e-mail to TalentAcquisition@grubhub.com and let us know the nature of your request and contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address."
93,.NET Developer/Data Engineer,"New York, NY 10019",New York,NY,10019,None Found,None Found,None Found,"
Contribute to the overall design, scope, and roadmap of TrueChoice analytics capabilities
Develop and deliver software solutions to meet the evolving requirements of our business users
Work with business users and analysts to gather requirements and produce detailed functional design specifications
Work with the internal IT team & business users to identify requirements, create prototypes, develop and test solutions
Develop, validate, and maintain the core functionality and capabilities associated with Spotfire visualizations and dashboards
Build and maintain documentation, procedures and guidelines around analytical solution development
Help manage execution of all Spotfire version upgrades",None Found,"
Bachelor’s degree in a technical related field (e.g. Computer Science, Engineering, Math)
3+ years of experience in C# and .NET development
Hands-on experience in database/SQL programming and Oracle
Proven success in contributing to a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Ability to work effectively both independently and as part of an integrated team
Excellent communication skills: written and verbal
Strong team work and interpersonal skills","Company Overview
TrueChoice Solutions is a private, Manhattan based, marketing software company which develops visually interactive, web-based applications for brand-name Fortune 500 companies. Our client base is diverse and includes large automotive companies, consumer electronics manufacturers and health care services companies. Our applications typically appear on major, highly trafficked public websites.
Position Overview
We are looking for a .NET Developer with experience working with business intelligence software to help support and expand our client analytics capabilities. This candidate will collaborate with our data analysts, application delivery teams, and software engineers to automate, scale, and enhance our data engineering, visualizations, and business intelligence competencies.
Responsibilities:
Contribute to the overall design, scope, and roadmap of TrueChoice analytics capabilities
Develop and deliver software solutions to meet the evolving requirements of our business users
Work with business users and analysts to gather requirements and produce detailed functional design specifications
Work with the internal IT team & business users to identify requirements, create prototypes, develop and test solutions
Develop, validate, and maintain the core functionality and capabilities associated with Spotfire visualizations and dashboards
Build and maintain documentation, procedures and guidelines around analytical solution development
Help manage execution of all Spotfire version upgrades
Requirements:
Bachelor’s degree in a technical related field (e.g. Computer Science, Engineering, Math)
3+ years of experience in C# and .NET development
Hands-on experience in database/SQL programming and Oracle
Proven success in contributing to a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Ability to work effectively both independently and as part of an integrated team
Excellent communication skills: written and verbal
Strong team work and interpersonal skills
We would like to see:
 2+ years of experience working with TIBCO Spotfire API & TIBCO Spotfire
Prior experience implementing business intelligence solutions
Experience designing complex reports and TIBCO Spotfire visualizations
Experience with additional programming languages, including Python and JavaScript (Web and Node)
Experience with Linux
Experience with AWS and cloud technologies
All applicants MUST have a permanent legal right to work in the United States.
Qualified candidates should live within a reasonable commuting distance to mid-town Manhattan; TrueChoice Solutions will not cover relocation costs.

How to Apply:
Please send resumes to techjobs@truechoicesolutions.com citing "".NET Developer/Data Engineer"" in the subject line."
94,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Bachelors of Science degree in Computer Science or related field.3+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets.Demonstrated strength in data modeling, ETL development, and data warehousing.

Do you love working with data and analytic platforms?

We’re a V1 startup that leverages interactive streaming video to create new shopping experiences for customers. Our group offers a creative and fast-paced work environment where you’ll invent and launch scalable solutions that directly impact Amazon customers. Data is at the center of every product we develop as we create new features and products that serve the needs of our growing base of consumers, brands and advertisers.

The ideal candidate has a strong bias toward data driven decision making, be a self-starter, comfortable with ambiguity, able to think big and be creative (while paying careful attention to detail), and will enjoy working in a fast-paced dynamic environment. If you are excited about using experimentation, data and machine learning to improve customer and brand experience, and want to join a growing team within Amazon - this role is for you.

Advanced degree in Engineering, Computer Science or related fieldUnderstanding of Big Data technologies and solutions (Spark, EMR, Hive, S3, Redshift, etc.)Understanding of Amazon Web Services (AWS) technologiesDemonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment.Excellent verbal and written communication skills with the ability to effectively advocate technical solutions to research scientists, engineering teams and business audiences.
Amazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation."
95,Data Engineer - Astral,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"If you're someone who is passionate about data, seeks out complex data problems, and wants to help build a new and exciting platform, you should consider joining our team as a Data Engineer. IEX is building a big data platform to tackle specialized data problems with high-performance cloud technology to help achieve our mission of creating fairer financial markets. We're currently looking for a strong Data Engineer who can develop creative and scalable solutions to marshal unstructured data. This person will have the opportunity to play a key role on a brand new venture and work directly with the traders at the world's largest hedge funds and asset managers.

If you're up for working in a fast-paced environment and with a small and highly-collaborative FinTech team changing Wall Street for the better – join us!

About you:

Self-starter with the capability of executing on projects independently
Strong analytical and problem-solving skills
Collaborative team player
Strong communication skills to distill clear ideas to traders and non-technical audiences

What you'll do:

Work with complex financial data sets
Design flexible applications to normalize unstructured data
Validate data accuracy and map content to a proprietary database
Work closely with our tech and analytics teams to solve new data problems
Conduct self-directed research to enhance product capabilities and generate new content and ideas for traders

Your background:

Programming and database experience
Demonstrated ability to efficiently manipulate large data sets
Experience as a data engineer
Experience normalizing complex data sets at scale
Nice to haves
PhD in Computer Science, Engineering, or Mathematics
Python, numpy, and pandas programming experience
Understanding of financial products and equities

Here at IEX, we are dedicated to an inclusive workplace and culture. We are an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, alienage or national origin, ancestry, citizenship status, age, disability or handicap, sex, marital status, veteran status, sexual orientation, genetic information or any other characteristic protected by applicable federal, state or local laws. This policy not only complies with all applicable laws and protects workers' rights but is vital to IEX's overall mission and values."
96,Sr Data Engineer - Product,"New York, NY 10176",New York,NY,10176,None Found,None Found,None Found,None Found,None Found,None Found,"Sr Data Engineer - Product
REF#: 34702
CBS BUSINESS UNIT: Showtime
JOB TYPE: Full-Time Staff
JOB SCHEDULE: Full-Time
JOB LOCATION: New York, NY
DESCRIPTION:
The Showtime Product team is looking for an experienced, curious and creative Sr. Data Engineer to help us pursue answers to our increasingly interesting and complex business questions and empower our team to incorporate data-driven features and machine learning into our products, which include our standalone service SHOWTIME and our TV Everywhere service, Showtime Anytime.
In this role, you will be an integral part of our data engineering team and collaborate with our dedicated Product Analytics team, the Showtime Research and Data Strategy teams, the CRM team, and our in-house backend and front-end engineering groups. You will work with the rest of the data team to architect solutions and develop technologies, systems and workflows that enable our analysts and data scientists to focus on algorithms and analyses rather than on the associated engineering.
Ideal candidates will be innovative, self-motivated, a quick study, and willing to develop new skills while constantly improving existing abilities.
Key Technologies: Java, Scala, Groovy, Spark, AWS, AWS/EMR, Spring, Mongo, Git, Redis, Bamboo, JIRA, etc
Responsibilities:
Develop understanding of key business, product and user questions.
Collaborate with other Product Engineering team members to develop, test and support data-related initiatives. Work with other departments to understand their data needs.
Evolve data-driven feature prototypes into production features that scale
Streamline feature engineering, so that the underlying data is efficiently extracted.
Build flexible data pipelines that we can rapidly evolve as our needs change and capabilities grow.
Develop and enhance our data warehouse in AWS S3.
QUALIFICATIONS:
You have at least 3 years of relevant experience in a comparable data engineering role
You have expert-level knowledge of SQL/Spark SQL
You have experience in pursuing and launching data-backed decisions, such as recommendations, to make end-user-facing products better
You like to dive-deep on data questions to come up with effective solutions
You believe in writing code that is easy to understand, test and maintain
You thrive in a workplace that values autonomy, applauds ideas and a enjoys a sense of humor
ABOUT US:
SHOWTIME and its critically-acclaimed, award-winning original series continue to make their mark on the cultural landscape, with one of the most successful programming slates in all of television. With an impressive line-up of new and returning original series, the SHOWTIME hit dramas and comedies include HOMELAND, SHAMELESS, BILLIONS, RAY DONOVAN, THE AFFAIR, SMILF, THE CHI, KIDDING, ESCAPE AT DANNEMORA and BLACK MONDAY. Original series play a key part in the SHOWTIME programming mix, along with box office hits, comedy and music specials, provocative documentaries, and hard-hitting sports programming, including the flagship franchise SHOWTIME CHAMPIONSHIP BOXING® and the Emmy Award-winning veteran series INSIDE THE NFL. SHOWTIME is currently available to subscribers via cable, DBS and telco providers, and as a stand-alone streaming service through Amazon, Apple®, Google, LG Smart TVs, Oculus Go, Roku®, Samsung and Xbox One. Consumers can also subscribe to SHOWTIME via Amazon’s Prime Video Channels, DirecTV Now, FuboTV, Hulu, Sling TV, Sony PlayStation™ Vue, and YouTube TV. The network’s authentication service, SHOWTIME ANYTIME, is available at no additional cost to SHOWTIME customers who subscribe to the network through participating providers. Subscribers can also watch on their computers at [1] www.showtime.com and [2] www.showtimeanytime.com.
References
Visible links
http://www.showtime.com/
http://www.showtimeanytime.com/
EEO STATEMENT:
Equal Opportunity Employer Minorities/Women/Veterans/Disabled"
97,Big Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Big Data Engineer
Saggezza is a proven technology and consulting partner that delivers personalized, high-value solutions to accelerate business growth. We consult, create experiences, analyze, modernize and digitize to help our clients adapt and transform the way they do business. Saggezza consultants work as part of a global team, and throughout their tenure, have the opportunity to work on a variety of different projects across various clients and industries. We are chartered to do one thing, and one thing only – to bring enabling technology to our clients that allows them to move their business forward.
We are currently looking to hire a Big Data Engineer to join our team.
What You'll Definitely Need
B.S. or higher in Computer Science is required
Minimum 3 years of relevant Big Data experience using a combination of the following such as (Hadoop, Spark, Airflow, Flink, Kafka)
Experience with programming languages such as (Java/Scala/Python) is a must have*
What We'd Love to See
We are therefore seeking to hire Big Data Engineers with expertise in both core Java, Scala and Python to work initially onsite on an innovative long-term project for one of our leading investment banking clients based in New York, NY.
Don't tick all the boxes? Don't worry about it: we still want to hear from you if you think
you're the right person for the job.
What Makes Saggezza Unique?
Strong Relationships: We take the time to know our clients well, from the way they do business to their culture and their unique pain points. This level of engagement enables us to develop highly effective solutions.
Clients First: We work to understand our clients then develop a unique point of view to address their pain points. We do whatever it takes to provide the right solutions to address their specific situation.
A Trusted Partner: We collaborate as a true strategic partner and work side-by-side to develop the best solutions to bring our clients' businesses into the future.
Passionate Mindset: We have an unending passion for technology and teamwork. We develop highly effective solutions to help our clients tackle the complex digital landscape.
Creating Value: The right investment in digital change is critical for our clients. That's why we develop personalized, highly effective solutions that deliver value for their businesses.
Delivery Excellence: Our proven processes provide high-quality solutions that are delivered on time.

Why Join Our Team
Our culture is diverse with endless opportunities to work for global corporations in their own backyard.
Our nurturing and supportive environment fosters collaboration across the entire organization.
We are not hierarchical but operate as a flat surface where every opinion matters, ideas are cultivated and innovation is encouraged.
At Saggezza, we are fortunate to have a strong mentorship program that provides each and every one of our consultants the ability to thrive professionally and personally.
We are only as good as our people. Saggezza, Italian for wisdom, is rooted from the perspective that knowledge is power. We create thought-leaders who are constantly exposed and trained in different technologies in the ever-evolving world of software development.
We welcome all types of innovators with an entrepreneurial spirit to grow with our team.
Saggezza is an Equal Employment Opportunity Employer: We believe in treating each employee and applicant for employment fairly and with dignity. We base our employment decisions on merit, experience, and potential, without regard to race, color, national origin, sex, sexual orientation, gender identity, marital status, age, religion, disability, veteran status, or any other characteristic prohibited by federal, state or local law.
zsaww9liSD"
98,Data Engineer,"Edison, NJ 08817",Edison,NJ,08817,None Found,None Found,None Found,None Found,None Found,None Found,"Gathering the business requirements. Analyzing business requirements and defining functional specifications, Analyzing existing applications consisting of multiple batch programs in SAS and developing the code in Python and Spark, Designing rules and workflow processing using python and Spark frame work, Identifying and developing automated batch packages, Participating in the deployment of the applications into existing systems and databases, Documenting modifications and enhancements made to the applications, systems and databases as required by the project.
Educational And Experience Requirements :
Must require at least a bachelor’s degree in Electrical Engineering or related field of study.
Location: Edison, NJ
CV to I5Tech Address: 3 Ethel Rd, Suite#306, Edison, NJ -08817 or
E-Mail: careers@i5techinc.com"
99,Data Engineer Lead,"New York, NY 10016",New York,NY,10016,None Found,None Found,None Found,None Found,None Found,None Found,"Description:
Manages and responsible for successful delivery of large scale data structures and Pipelines and efficient Extract/Load/Transform (ETL) workflows. Acts as the data engineering team lead for large and complex projects involving multiple resources and tasks, providing individual mentoring in support of company objectives.62884

Fundamental Components:
Designs and develops complex and large scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs. Writes complex ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing. Develop frameworks, standards & reference material for architecture and associated products. Designs data marts and data models to support Data Science and other internal customers. Behaves as mentor to junior team members to provide technical advice. Applies knowledge of Aetna systems and products to consult and advise on additional efforts across multiple domains spanning broader enterprise. Collaborates with data science team to transform data and integrate algorithms and models into highly available, production systems. Uses in-depth knowledge on Hadoop architecture, HDFS commands and experience designing & optimizing queries to build scalable, modular, and efficient data pipelines. Uses advanced programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards. Experiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.

Background Experience:
Strong collaboration and communication skills within and across teams.Ability to communicate technical ideas and results to non-technical clients in written and verbal form.Proven ability to create innovative solutions to highly complex technical problems.Ability to leverage multiple tools and programming languages to analyze and manipulate large data sets from disparate data sources.Ability to understand and build complex systems and solve challenging analytical problems.

Advanced knowledge in Java, Hive, Cassandra, Pig, MySQL or NoSQL or similar.

Python and Hadoop experience is a must.

Advanced knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment.

Experience building and implementing data transformation and processing solutions.Has in-depth knowledge of large scale search applications and building high volume data pipelines.Experience with bash shell scripts, UNIX utilities & UNIX Commands.7 or more years of progressively complex related experience. Master’s degree or PhD preferred.Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.

Potential Telework Position:
No

Percent of Travel Required:
0 - 10%

EEO Statement:
Aetna is an Equal Opportunity, Affirmative Action Employer

Benefit Eligibility:
Benefit eligibility may vary by position. Click here to review the benefits associated with this position.

Candidate Privacy Information:
Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately."
100,SR Data Engineer,"New York, NY 10019",New York,NY,10019,None Found,None Found,None Found,"Architect, Design and Maintain Data Pipelines through the lifecycle of the product.Optimize and Monitor existing data pipelines using AWS infrastructure.WritePython/Scala applications for data processing and job scheduling.Understand and Manage massive data-stores.Integrate products from data projects into APIs built in Ruby/RailsExpose large data setsEnjoy being challenged and solve complex problems on a daily basisDesign efficient and robust ETL workflowsManage real time streaming application and data flowInvestigate, procure and ramp up to new technologiesAbility to work in teams and collaborate with others to clarify requirementsBuild analytics tools that utilize the data pipelines to provide meaningful insights into data",None Found,None Found,"Job Description

Publicis Spine is looking for talented Sr. Data Engineer for an exciting opportunity on the data engineering team. The successful candidate will be involved with designing workflows for data and analytics tools that are a big part of the road-map for 2019 while managing data and infrastructure to efficiently query data in the billions. Candidates will be considered based on their ability to design large distributed technical solutions, architect, manage, monitor and optimize data pipeline projects resulting in actionable data and data pipelines which support the larger organization.

Core Responsibilities:

In this role, you will be expected to drive Publicis Spine's mission to grow our clients' businesses through the transformative application of data. Your key priorities will include but are not limited to:
Architect, Design and Maintain Data Pipelines through the lifecycle of the product.Optimize and Monitor existing data pipelines using AWS infrastructure.WritePython/Scala applications for data processing and job scheduling.Understand and Manage massive data-stores.Integrate products from data projects into APIs built in Ruby/RailsExpose large data setsEnjoy being challenged and solve complex problems on a daily basisDesign efficient and robust ETL workflowsManage real time streaming application and data flowInvestigate, procure and ramp up to new technologiesAbility to work in teams and collaborate with others to clarify requirementsBuild analytics tools that utilize the data pipelines to provide meaningful insights into data

Qualifications

null

Additional Information

All your information will be kept confidential according to EEO guidelines."
101,Data Engineer & Quality resource,"New York, NY",New York,NY,None Found,None Found,None Found,"
Familiar with big data processing and concepts like MapReduce, spark RDDs etc.
Knowledge of cloud storage platforms on AWS, Azure etc.
Java, Python and Scala are required skill sets.
Good command over SQL, ETL Best practices and data warehousing concepts.
Familiarity with ELT.
Familiarity with various modes of file storage like Parquet, ORC, Avro etc.
Prior experience in working with Redshift/Snowflake a plus.","
Interact with external / internal data providers to understand the nature of data.
Gather information on general data delivery schedule for all sources.
Build databases and schemas for data warehousing and analysis.
Develop ETLs to move data into the warehouse and analysis layers.
Develop logical checks in ETL process to check for duplicates, obvious data errors etc.
Judge whether an ETL needs to be a batch process or stream process and use appropriate libraries.
Schedule and maintain ETLs for different sources.
Structure / Re-Structure various schema objects and periodically check on query execution plans to maintain optimal performance.
Work with data scientists to ensure the data is formatted in a way that is optimal for their model.
Create and maintain schemas to store MMM results.
Serve as a go to person for ad hoc queries, schemas, views etc.",None Found,None Found,"Responsibilities
Interact with external / internal data providers to understand the nature of data.
Gather information on general data delivery schedule for all sources.
Build databases and schemas for data warehousing and analysis.
Develop ETLs to move data into the warehouse and analysis layers.
Develop logical checks in ETL process to check for duplicates, obvious data errors etc.
Judge whether an ETL needs to be a batch process or stream process and use appropriate libraries.
Schedule and maintain ETLs for different sources.
Structure / Re-Structure various schema objects and periodically check on query execution plans to maintain optimal performance.
Work with data scientists to ensure the data is formatted in a way that is optimal for their model.
Create and maintain schemas to store MMM results.
Serve as a go to person for ad hoc queries, schemas, views etc.

Technical Skills
Familiar with big data processing and concepts like MapReduce, spark RDDs etc.
Knowledge of cloud storage platforms on AWS, Azure etc.
Java, Python and Scala are required skill sets.
Good command over SQL, ETL Best practices and data warehousing concepts.
Familiarity with ELT.
Familiarity with various modes of file storage like Parquet, ORC, Avro etc.
Prior experience in working with Redshift/Snowflake a plus.

BHwwUR7NP8"
102,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Engineering team is developing the backbone of BounceX solutions. To do this, the team is using cutting-edge technologies and frameworks such as BigQuery, Spanner, Bigtable, Dataproc to process and analyze data, to help create the pipelines required to handle millions of events daily, and collaborate with the data science team to enable the innovative work in machine learning and AI to be utilized in our production environment.

Responsibilities


Contribute to the development of BounceX Micro Batch/ Real Time streaming pipelines (lambda architecture)
Work closely with the data science team to build our machine learning platform infrastructure
Work closely with the engineering team on data preprocessing and data structure

Qualifications


BA/BS or MS degree in Computer Science preferred, or equivalent work experience
Understanding of distributed data systems
Previous experience building lambda architecture
Previous experience working with big data tools such as Bigtable, Apache Spark, Apache Spark Streaming, Apache Heron, BigQuery
Preferred:
Google Cloud, Kubernetes
Experience running production Golang systems, Infrastructure background, Low Latency / High Throughput Architectural System Designs
Strong interest in data engineering
Eagerness to learn new technologies on the fly and ship to production
Knowledge in data science is a plus

BounceX is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.

JOIN US ON OUR MISSION

BounceX is a global marketing technology company that has been recognized as a best place to work by Glassdoor and Crain's, and one of America's fastest-growing SaaS companies.

More than 300 companies, including JetBlue, Uniqlo, HelloFresh, and Comcast use BounceX to orchestrate real-time, multichannel marketing programs customized for every individual web visitor.

With offices in New York City and London, BounceX is built on the belief that the success of a company is rooted in the strength of its team, so we've created a collaborative, inclusive environment where people love coming to work.

We provide career coaching, growth and development opportunities, and benefits that are in the 95th percentile of all technology companies. Some highlights include excellent healthcare that starts day one, best-in-class fully paid family leave, 401(k) match, flexible work hours, and more.

What bonds our community together is our commitment to 5 Core Values:


Come Hungry
Carry Each Other
Drive Undeniable Performance
Respect People, Privacy, Ideas
Bounce Back

"
103,Data Engineer,"New York, NY 10018",New York,NY,10018,None Found,None Found,None Found,None Found,None Found,"
A BS/MS degree in Computer Science or relevant field or equivalent professional experience with solid computer science fundamentals in object-oriented design, data structure, algorithms and database systems (SQL and NoSQL)
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets; Strong analytic skills related to working with unstructured datasets
Experience with big data tools: Spark, Kafka; Experience with AWS cloud services: EC2, EMR, RDS, Redshift; Experience with stream-processing systems like Spark-Streaming
Proficient in coding languages such as Python, JavaScript, Scala and Shell scripts","CertiK is a world leading cybersecurity firm that focuses on smart contract analysis and blockchain auditing. CertiK engineers strive to develop the next-generation technologies to improve the reliability, stability, and scalability of large-scale computer systems.
CertiK is looking for talented Data Engineers to join our growing engineering team. Ideal candidates should have experience in designing/implementing big data infrastructures, building ETL pipelines and delivering BI/monitor/alert tools and applications.
Requirements:
A BS/MS degree in Computer Science or relevant field or equivalent professional experience with solid computer science fundamentals in object-oriented design, data structure, algorithms and database systems (SQL and NoSQL)
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets; Strong analytic skills related to working with unstructured datasets
Experience with big data tools: Spark, Kafka; Experience with AWS cloud services: EC2, EMR, RDS, Redshift; Experience with stream-processing systems like Spark-Streaming
Proficient in coding languages such as Python, JavaScript, Scala and Shell scripts
Nice to have:
Passionate about Blockchain and on-chain/off-chain analysis, leverage data science skillsets to solve real-world problems
Experience in deploying, designing, and integrating with the AWS cloud environments to run big data queries
Hands-on experience on building the end-to-end Business Intelligence tools, i.e. able to mine the raw data into valuable datasets and present with data visualizations
Capable of analyzing Blockchain transactional data (Bitcoin and Ethereum), fulfill the business requirements on data traceability and solve the pain points of cryptocurrency AML/Compliance and KYT (Know Your Transaction)"
104,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,"BA/BS Degree (advanced degrees and/or CFAs are great too)
10+ years general business experience, especially in financial services, asset management, and/or management consulting or similar environments
6+ years of data engineering, data architecture, data science, and/or software engineering experience
Excel proficiency
SQL proficiency
Python expertise
Experience with machine learning algorithms and techniques
Experience with modern data pipelines and/or data operating platforms (e.g. Dataiku, Alteryx, Spark)
Able to lead ad-hoc and structured product teams within an agile framework
Excellent interpersonal skills necessary to accomplish goals through others, including employees, peers, and other function/business areas of the company",None Found,"Lead data engineering and/or data science projects to support IM firms
Use domain expertise to understand business requirements and design the right solution
Build or implement data pipelines, databases, visualizations, and other data tools (hands-on)
Contribute to team in a wide range of technical areas by instituting new practices and staying abreast of the latest technical developments
Take initiative to lead/contribute to overall team efforts in software development, data engineering, data science, and technical consulting
Work closely with other data engineers and data scientists to improve processes and enable faster insight-generation from complex datasets
",None Found,None Found,"Summary:
BNY Mellon Investment Management is one of the world's leading investment management organizations and one of the top U.S. wealth managers. Our business encompasses BNY Mellon's affiliated investment management firms, wealth management organization, and global distribution companies. Our goal is to build and deliver investment and wealth management strategies and solutions to meet our clients’ needs.
Drawing on deep expertise, we collaborate with our clients to tailor our best ideas and resources to meet their specific requirements. Through our global network we have developed a significant understanding of local requirements. We pride ourselves on providing dedicated service through our teams.
With extensive experience in anticipating and responding to the investment and financial needs of the world's governments, pension plan sponsors, corporations, foundations, endowments planned giving programs, advisors, intermediaries, individuals and families, and family offices, BNY Mellon Investment Management can help our clients reach their goals.
The Role:
Reporting to the Head of Investment Management (IM) Data Solutions, the Senior Data Engineer will lead, design and architect solutions to complex data integration, process re-engineering, data science, and automation problems. This is a unique opportunity to join the IM Data Solutions team, a small, multi-disciplinary team that is transforming the way information is created, used and communicated within BNY Mellon IM. The group works directly with BNY Mellon’s IM firms on strategic projects, creating valuable tools and insights for decision makers across portfolio management, sales, marketing, and other key areas of the business. The Data Solutions team capitalizes on opportunities across business lines using the best tools and practices, like machine learning, econometrics, agile development (scrum/kanban), and modern collaborative data platforms.
Responsibilities:
Lead data engineering and/or data science projects to support IM firms
Use domain expertise to understand business requirements and design the right solution
Build or implement data pipelines, databases, visualizations, and other data tools (hands-on)
Contribute to team in a wide range of technical areas by instituting new practices and staying abreast of the latest technical developments
Take initiative to lead/contribute to overall team efforts in software development, data engineering, data science, and technical consulting
Work closely with other data engineers and data scientists to improve processes and enable faster insight-generation from complex datasets
Qualifications

BA/BS Degree (advanced degrees and/or CFAs are great too)
10+ years general business experience, especially in financial services, asset management, and/or management consulting or similar environments
6+ years of data engineering, data architecture, data science, and/or software engineering experience
Excel proficiency
SQL proficiency
Python expertise
Experience with machine learning algorithms and techniques
Experience with modern data pipelines and/or data operating platforms (e.g. Dataiku, Alteryx, Spark)
Able to lead ad-hoc and structured product teams within an agile framework
Excellent interpersonal skills necessary to accomplish goals through others, including employees, peers, and other function/business areas of the company


BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer.
Minorities/Females/Individuals With Disabilities/Protected Veterans.

Primary Location: United States-New York-
Internal Jobcode: 70124
Job: Asset Management
Organization: IM Infrastructure-HR14826
Requisition Number: 1914660"
105,Data Engineer - Cloud,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"IEX Cloud is the platform connecting developers and users with financial data. We're looking for a data engineer to own all the architecture and processes that build and support our expansive financial data API. If you're someone who is passionate about data, seeks out complex data problems, and wants to help build a new and exciting platform, this job is for you! We're currently looking for a strong Data Engineer who can develop creative and scalable solutions to maintain an API used by IEX Cloud's rapidly growing user base. This person will have the opportunity to play a key role on a brand new venture and drive huge impact to the business from day 1.

About you:

Self-starter with the capability of executing on projects independently
Strong analytical and problem-solving skills
Collaborative team player
Strong communication skills to distill clear ideas to traders and non-technical audiences
Can optimize a query with the best of them
Know how to make the tradeoffs required to ship without compromising quality

What you'll do:

Work on increasing the efficiency of our ETL processes as the size, variety, and complexity of IEX Cloud's data continues to grow
Investigate how we can enhance our logging and monitoring to discover and resolve issues before they cause problems
Validate data accuracy and map content to a proprietary database
Work closely with our tech and analytics teams to solve new data problems
Conduct self-directed research to enhance product capabilities and generate new content and ideas for traders

Your background:

Programming and database experience
Experience in building data products from ideation to implementation
Demonstrated ability to efficiently manipulate large data sets
Experience normalizing complex data sets at scale
Experience in some of the following: Advanced SQL, ETL, Spark, Hive, Kafka, Elasticsearch, Columnar stores, Event Streaming, Redis, Python
Nice to haves
Understanding of financial products and equities
Experience with NodeJS

Here at IEX, we are dedicated to an inclusive workplace and culture. We are an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, alienage or national origin, ancestry, citizenship status, age, disability or handicap, sex, marital status, veteran status, sexual orientation, genetic information or any other characteristic protected by applicable federal, state or local laws. This policy not only complies with all applicable laws and protects workers' rights but is vital to IEX's overall mission and values."
106,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About the job:
The Data Engineer will join Vigilant's growing engineering team to make major contributions to both technical and business sides of the company. This role is key to our ability to provide increasingly sophisticated data products and expand into new verticals. The person who fills this position will be part of a small data team within engineering and will handle multiple responsibilities.
The ideal candidate has experience with:
Building and maintaining end-to-end ETL pipelines, namely:
Implementing data from a range of sources such as downloadable datasets, API endpoints, massive Excel/CSV files, and non-standardized sources such as PDFs and database dumps
Transforming, normalizing, cleaning, and mapping extracted data both at the extraction stage as well as in the destination data warehouse
Analysis of multivariable datasets to understand the distinctions, whether the datasets encompass each other, and to sort, aggregate or separate the individual datasets as needed
Advanced database administration, including experience maintaining, migrating a cluster, access controls, and implementing subsequent versions
Conducting analysis of data quality, assurance, coverage, and answering specific business queries as needed
About you:
You're comfortable writing generalized data transformation tools to be used by engineers and developers on the team, with thorough documentation
You can write and/or update models based on database-derived datasets
You're able to spin up instances of common databases with Docker and configure production tables, indices, and access controls
You're comfortable getting hands-on with complicated data, and understand that you will not use pre-processed datasets
What you'll do:
Assist with the data warehouse integration into various products and applications
Communicate with the data provider and determine a resolution to the requisite acquisition, integration, and implementation
Administer and optimize our Linux deployments
Implement integrated automated test frameworks, to improve code management and product quality
Provide analysis of performance metrics, and diagnose and troubleshoot potential infrastructure issues as they arise
Work to reduce the development defect cycle time, and conduct statistical and analytical assessments
Establish and develop naming conventions and consistent schemas to ensure clarity and usability in the data aggregated
Vigilant's Values:
Constant learning and insatiable curiosity. We organize information, and we're always keen to know more. We're committed to learning and growth and enjoy working with those who share that commitment.
We're guided by a deep empathy for our customers and their needs. We think carefully about how our users - and our world - may be affected by every decision we make. We're constantly aiming to drive improvements for our users.
We aim high. We're taking on large, difficult problems, and are looking to build meaningful solutions and scale a substantial company that will have lasting impact. Ambition is a compliment in our book.
Relentless tenacity. We do the work. We tend to see obstacles as existing to be overcome. When we decide to do something, we work at it until it gets done.
Real humility. We know that we aren't doing this alone. We hold ourselves accountable for our missteps, and gladly share credit for our successes. We recognize that we don't have all the answers, or even most of them.
We draw the owl (https://i.kym-cdn.com/photos/images/newsfeed/000/572/078/d6d.jpg). We're growing, and there isn't usually a defined set of instructions for any given task. We figure it out. We work, iterate, improve. (In the interest of full disclosure, we shamelessly borrowed this notion from Twilio's corporate values).
We have a bias towards action. If we notice an issue is affecting the business, we work to fix it immediately, even if it's not in our wheelhouse.

We save our paperclips (https://s1.ibtimes.com/sites/www.ibtimes.com/files/styles/embed/public/2014/07/25/ace-greenberg-memo6.JPG). We have an ownership mentality and act in the company's best interest. We're conscious of how our actions and behavior impact the business and the company's bottom line. We celebrate efficient solutions.

We follow through. We do what we say. We get the job done."
107,Senior Data Engineer,"New York, NY 10007",New York,NY,10007,None Found,None Found,None Found,None Found,None Found,None Found,"Knowledge is our product, and data is our platform. We need engineers who look at a data set and want to unlock the answers it holds inside. Engineers who look at a data set and think about how to make sure it is correct. Engineers who look at a data set and want to make infrastructure to help build it better, faster, and stronger.

As a Data Engineer, you will work closely with oncologists and statisticians to build software that will help our customers discover novel insights into their data. You will design our data infrastructure, and use it to develop extensible, robust data and analytics pipelines, tools, visualizations, and services for accessible and flexible data analysis. You will learn more than you ever thought possible about how cancer is treated in the real world, and your work will directly support oncology research and publications.

Who you are:

You hold a BS, MS, or Ph.D. in computer science or related field
You have 2+ years work experience
You have experience with languages like Python, C++, Java, or C#
You are passionate about performance, reliability, and scalability of systems
You are inspired by our mission to improve cancer research through technology
You seek simple approaches to complex problems
You like science and/ or medicine just because it's cool

Bonus points if you have any of the following:

Have a good understanding of relational databases like PostgreSQL, MySQL or MSSQL
Have real passion for data and a strong understanding of statistics
Have developed distributed data processing systems against large, heterogeneous data sets
Have taken a leading role in delivering complex software systems all the way to production
You almost decided to go to med school

Flatiron is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, or Veteran status. If you have a disability or special need that requires accommodation, please let us know."
108,Senior Data Engineer,"New York, NY 10013",New York,NY,10013,None Found,None Found,None Found,None Found,None Found,None Found,"Tracer is a data platform built to maximize marketing efficiency. Our technology was born from the challenges facing today's marketers. By aggregating cross-platform data and adding business context, we provide transparency into the performance of media investments. Tracer has a proven track record of consistently empowering better decisions and solving data challenges for some of the world's biggest brands.

Our Team


We are a small, focused, and collaborative team
We are building for the long term and prioritize a good solution over a quick one
New members quickly take on responsibility for larger parts of the application
We know that engineers love to learn and we encourage you to take on projects outside of your focus

Tracer Values


Winning Together - We believe that individuals don't scale – teams do. It's not about what you can accomplish individually, it's about impacting what we can accomplish together. Share ownership, lend a hand, and make changes that empower others.
Creating Transparency - Transparency is in the DNA of what we build, as well as who we are. Ask questions, share answers, and challenge expectations. Do so in a way that is accessible to everyone.
Reason Not Rank - The best ideas can come from anywhere. When working towards the same goal, the quality of the solution will always outweigh the source. And the highest quality solutions will be collaborative.
Building Community - We are a team that fosters a community to take risks, speak up, and feel heard. Community is derived from mutual respect and humility — in how we act, listen, and lead.
Proactive Leadership - We are a proactive team that believes leadership is an action to be taken, not a title to be given.

What You'll Do:

Architect robust systems for ensuring data integrity. If the data is wrong, everything we build on top of it is useless.
Improve our ETL processes for fetching, normalizing and surfacing data across dozens of APIs, and a huge quantity of complex data sources
Improve memory and time efficiency for thousands of daily jobs across hundreds of brands
Improve our data retrieval engine - taking a wide array of inputs across a large number of tables and delivering an accurate result
Build out an incredible team of data engineers

What We Look For


5+ years of relevant experience
Capable of architecting scalable solutions to complex data problems
Ability to scope, coordinate, and deliver projects with multiple engineers
Someone who can facilitate the growth of a team. See our definition of Winning Together.

Technologies We Use


Python / Pandas
Django REST framework
Heroku / AWS
Ember / Coffeescript / Sass
PostgreSQL

What We've Got


Medical, Dental, and Vision coverage
401K with matching
Unlimited Vacation & Sick Days & Flexible Work Schedule
Summer half days
Flexible Spending Account across Commuter, Health, and Dependent Care
Generous paid parental leave
The week off between Christmas & New Years

"
109,ETL Informatica Engineer,"New York, NY 10011",New York,NY,10011,None Found,"Minimum 4 years of experience developing and implementing Informatica Powercenter or Informatica Data Quality
Bachelor’s Degree or Associate’s Degree with 6 years of work experience or equivalent work experience of 12 years",None Found,None Found,None Found,"
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve clients, our employees must be able to travel when needed. This role requires 100% flexibility to travel and work onsite with clients (typically Monday through Thursday).
Proven success in contributing to a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills","Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements and the way we collaborate, operate and deliver value provides an unparalleled opportunity to grow and advance. Choose Accenture, and make delivering innovative work part of your extraordinary career.

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

 Why Should I Join the Accenture Team?
Drive innovation. People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward.Learn and grow continuously: Follow personalized training to build skills, while expanding your experience defining and implementing solutions on complex client projects with a scope that is unsurpassed in industry.Thrive in our inclusive environment: Bring your whole self to a company that aims to be the most diverse in the world and delivers real-time performance feedback based on individual strengths—not stats.

Job Description
Data and Analytics professionals define strategies, develop and deliver solutions that enable the collection, processing and management of information from one or more sources, and the subsequent delivery of information to audiences in support of key business processes.
Extract, Transform and Load data primarily in Informatica with an emphasis on advocacy toward end-users to produce high quality software designs that are well-documented.
Demonstrate an understanding of technology and digital frameworks in the context of data integration utilizing Informatica.
Ensure code and design quality through the execution of test plans and assist in development of standards, methodology and repeatable processes, working closely with internal and external design, business, and technical counterparts.

Basic Qualifications
Minimum 4 years of experience developing and implementing Informatica Powercenter or Informatica Data Quality
Bachelor’s Degree or Associate’s Degree with 6 years of work experience or equivalent work experience of 12 years
Preferred Qualifications
Experience in ETL Tools in addition to Informatica, including Business Objects Data Services (BODS), DataStage, Ab Initio, Talend, and Pentaho
Experience with a full life-cycle development from functional design to deployment
Database experience (Teradata, Oracle, SQL Server, DB2, Azure SQL)
Strong knowledge and experience of SQL
Understanding of Entity relationship data models and Dimensional Models
Experience with development and production support
Professional Skill Requirements

It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve clients, our employees must be able to travel when needed. This role requires 100% flexibility to travel and work onsite with clients (typically Monday through Thursday).
Proven success in contributing to a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills

All of our consulting professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.


Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or"
110,Data Engineer,"New York, NY",New York,NY,None Found,None Found,"
3+ years’ experience with ETL tool SQL Server Integration Services (SSIS) or similar ETL Tool
3+ years’ experience with SQL Server, strong expertise in T-SQL required
3+ years’ experience with Web Services: RESTful APIs, .NET WCF and SOAP
1+ years’ experience with .NET C# or other OO language required
Experience with Windows Server Operating Systems required
Experience with SSAS and building cubes required
Experience with statistics and machine learning tools preferred: (Python, R, Matlab or Spark)
Experience with public cloud preferred: AWS or Azure
Experience with Big Data or NoSQL technology preferred: Hadoop, Spark, AWS EMR, Hive, etc.
Experience in healthcare a plus","
3+ years’ experience with ETL tool SQL Server Integration Services (SSIS) or similar ETL Tool
3+ years’ experience with SQL Server, strong expertise in T-SQL required
3+ years’ experience with Web Services: RESTful APIs, .NET WCF and SOAP
1+ years’ experience with .NET C# or other OO language required
Experience with Windows Server Operating Systems required
Experience with SSAS and building cubes required
Experience with statistics and machine learning tools preferred: (Python, R, Matlab or Spark)
Experience with public cloud preferred: AWS or Azure
Experience with Big Data or NoSQL technology preferred: Hadoop, Spark, AWS EMR, Hive, etc.
Experience in healthcare a plus",None Found,"
MS/BS in Computer Science or related field",None Found,"Overview
Data Engineer
At Hospital for Special Surgery our clinicians and scientists collaborate to deliver the most innovative care. Our specialized focus on orthopedics and rheumatology enables us to help patients get back to what they need and love to do reliably and efficiently. Our patients are overwhelmingly satisfied with the care they receive at our facilities. When you join us, you will become part of this legacy of commitment to the most cutting-edge research and coordinated care.

The ideal candidate must be passionate about the field of analytics and well versed in all aspects of an enterprise-wide analytics architecture. We are looking for Data Engineers who have a strong understanding of data pipelines, ready to work hard but most importantly curious. They will work collaboratively across the organization with clinical and operational leaders on a diverse range of projects to find insights that support our mission of providing world class musculoskeletal health.
As a Data Engineer You Will
Design, implement, and automate data flows
Assist in the implementation and administration of the analytics data lake
Work with partners and vendors on data integration projects
Design and create data models for new or existing data sources
Design and implement ETLs
Qualifications
Your Technical Skills and Experience
3+ years’ experience with ETL tool SQL Server Integration Services (SSIS) or similar ETL Tool
3+ years’ experience with SQL Server, strong expertise in T-SQL required
3+ years’ experience with Web Services: RESTful APIs, .NET WCF and SOAP
1+ years’ experience with .NET C# or other OO language required
Experience with Windows Server Operating Systems required
Experience with SSAS and building cubes required
Experience with statistics and machine learning tools preferred: (Python, R, Matlab or Spark)
Experience with public cloud preferred: AWS or Azure
Experience with Big Data or NoSQL technology preferred: Hadoop, Spark, AWS EMR, Hive, etc.
Experience in healthcare a plus
About You
You are passionate about data and exploring new technologies
You are a critical thinker who loves solving difficult problems
You are flexible and a natural team player
Education and Certifications
MS/BS in Computer Science or related field"
111,"Lead Data Engineer, Data Engineering","New York, NY 10036",New York,NY,10036,None Found,None Found,None Found,None Found,None Found,None Found,"The New York Times is seeking a Lead Data Engineer to join the Data Engineering group.
Every day, journalists at the New York Times report more than 200 stories from dozens of countries around the world on topics ranging from major international events to the best way to roast a chicken. Those stories are read, listened to and watched by millions of users across our web and mobile products.
As engineers at the New York Times, we take great pride in building products that live up to the quality and reputation of our journalism. We are looking for a lead software engineer to help our data engineering team build the next generation of data products and tools. In the Data Engineering group, you will work with teams who are:
Establishing systems of record for print and digital subscribers and creating a single view of our customers
Evolving the infrastructure that supplies the entire organization with accurate, consistent, secure, and always available data
Creating a foundation for incorporating our data into APIs, machine learning models, and other data products
About the Team
The Data Engineering group at The New York Times is at the intersection of business intelligence, data warehousing, and software engineering. As Maxime Beauchemin wrote in “The Rise of Data Engineering” ( https://medium.freecodecamp.com/the-rise-of-the-data-engineer-91be18f1e603 ), ETL and data modeling have changed, and that change is not just about Big Data. It’s distributed systems, it’s stream processing, and it’s computation at scale. It’s about working with data using the same practices that guide software engineering at large.
A strong data foundation is critical for The New York Times and we’re responsible for it. We use our data platform to ingest and store all the data related to the business of The New York Times. This data powers analytics and data products, delivers relevant experiences to our customers, and fuels data-informed strategic decisions company-wide. This enables us to validate our decisions, make smarter choices, and react to the fast changing world.
Our systems are built with Java, Go, and Python, and Google Cloud services like Dataflow, Pub/Sub, BigQuery, and more. We’re part of the technology organization at The New York Times. Check out the Open blog ( https://open.blogs.nytimes.com ), which is written by engineers and technical team members, and follow @nytdevs on Twitter ( https://twitter.com/nytdevs ) to see what we’re up to.
About You
Along with the desire to join the world’s most important journalism company at a moment in history when the importance of learning from our data is transforming every aspect of the craft and practice of journalism, you have:
6+ years of software engineering experience preferably in environments that utilize one or more of the following programming languages: Go, Java, Python
Experience building applications at scale on any major cloud provider (AWS, GCP, etc)
Experience building and integrating APIs
Experience automating application deployments, testing, monitoring, and maintaining fault tolerant systems as well as using data to diagnose problems
In addition, to thrive in this role, you should:
Be extremely well versed in HTTP and other network related protocols
Know your way around the Linux command line
Be familiar with benchmarking and performance testing and tuning of web based applications/API’s
Identify and proactively tackle technical debt before it grows
Make others better through code reviews, thorough documentation, technical guidance, and mentoring
Have experience working with technologies such as: Google App Engine, BigQuery, SQS, Pub/Sub, DynamoDB, Redis / Memcached and other key-value stores
Be able to collaborate across teams and functional roles
Benefits and Perks:
Support our original, independent and reported journalism.
We provide great health, dental, vision and life insurance for employees and their families
We support responsible retirement planning with a generous 401(k) company match.
We offer a great parental leave.
We are committed to career development and ongoing learning supported by a formal mentoring program as well as $8,000 annually for tuition reimbursement.
We have frequent panel discussions and talks by newsmakers and industry leaders.
Join a community committed to the richness of diversity, experiences and talents in the world we cover, supported by a variety of employee resource groups.
#LI-NC1
The New York Times is committed to a diverse and inclusive workforce, one that reflects the varied global community we serve. Our journalism and the products we build in the service of that journalism greatly benefit from a range of perspectives, which can only come from diversity of all types, across our ranks, at all levels of the organization. Achieving true diversity and inclusion is the right thing to do. It is also the smart thing for our business. So we strongly encourage women, veterans, people with disabilities, people of color and gender nonconforming candidates to apply.
The New York Times Company is an Equal Opportunity Employer and does not discriminate on the basis of an individual's sex, age, race, color, creed, national origin, alienage, religion, marital status, pregnancy, sexual orientation or affectional preference, gender identity and expression, disability, genetic trait or predisposition, carrier status, citizenship, veteran or military status and other personal characteristics protected by law. All applications will receive consideration for employment without regard to legally protected characteristics."
112,Senior Data Engineer,"New York, NY 10022",New York,NY,10022,None Found,None Found,None Found,None Found,None Found,None Found,"About the opportunity

We are seeking a Senior Data Engineer with extensive XBRL experience to join our recently created Data Management team as we help build and launch our data and analytics platform, serving our growing capital markets clientele. You will work in a dynamic group of cross-functional team-members and be responsible for processing XBRL content and the software to manage and interpret related taxonomies, as we build a strong data foundation for our new business.


You will be successful in this role if you:

Have extensive expertise (including 5+ years practical experience) processing XBRL documents, particularly those filed with the SEC via Edgar
Are knowledgeable about US GAAP and/or IFRS accounting standards and their equivalent XBRL Taxonomies
Possess 10+ years’ experience in processing and manipulating data using C#, Java, etc.
Are comfortable with Relational Database systems (especially SQL Server)
Love learning new technologies and ideally have exposure to cloud services (especially Microsoft Azure)
Have experience collaborating with Product Managers and Content Owners in an agile framework
Are highly motivated, self-directed and collaborative


In this role, you will:

Develop Content Management technology processes that are robust, optimized, data-driven and meet the needs of our customers
Extract and process financial data from XBRL regulatory filings
Model complex financial data domains
Integrate data from a diverse set of sources, including XML documents and Web Service APIs
Execute complex engineering projects and propose appropriate technology choices
Work with and/or direct offshore technical teams


About Us

KBRA is a global full-service rating agency on a mission to set a standard of excellence and integrity. Established in 2010 as a challenger brand, KBRA remains dedicated to the restoration of trust in credit ratings. We accomplish this through the creation of new standards for assessing risk and by offering accurate and transparent ratings. KBRA provides market participants with an alternative solution through our timely and in-depth research across various sectors. Since the company was founded, we have published over 32,200 ratings. KBRA has over 350 employees located across the United States (in New York, Pennsylvania and Maryland) and in Dublin, Ireland and London, England."
113,Talent and Operations Associate,"New York, NY 10005",New York,NY,10005,None Found,"
3-5 years of experience working in talent acquisition, talent management, and compliance
Successful track record in building robust diverse talent pipelines
Successful track record in filling technical roles
Quick study, willing to learn new things and learn them quickly by any means necessary
Knowledge of and experience with employment laws
Strong communication and relationship-building skills
Proficient to Advanced across MS Office suite including Excel, Powerpoint, and Word
",None Found,None Found,None Found,None Found,"TALENT AND OPERATIONS ASSOCIATE


About Murmuration

Murmuration transforms how political campaigns, advocates, and organizers identify, engage, and mobilize people and communities. Our focus is on driving change and accelerating progress toward a future where every child in America has the opportunity to benefit from a high-quality public education.

What We Do

We provide sophisticated data and analytics, proprietary technology, strategic guidance, and programmatic support to help our partners build political power and marshal support so necessary changes are made to improve our public schools. Our precise, predictive intelligence and easy-to-use tools are used by practitioners and funders, on their own and working together, to make informed decisions about who they need to reach, what they need to say, and how to achieve and sustain impact.

Background

Over the past four years, Murmuration has established itself as a trusted resource for our partners and demonstrated that our data, analytics, and technology can be used to help deliver improved electoral and organizing outcomes. We have also made early contact with key players across the sector, notably funders and vendors, already working on these and related efforts. Through our work, we have confirmed that to be successful, and to achieve our long-term goals, everyone committed to massively improving public schools and ensuring all children across this country have equal access to quality education must be aligned. Our collective efforts must be focused, disciplined, and the work must be collaborative and wide-ranging. At the same time, we recognize that it will take more than our current offerings – and particularly how and where they are being used, and by whom – to drive sustainable change.

We believe that Murmuration is uniquely positioned to facilitate more coordinated decision-making and collective action among practitioners and funders. We enable and promote collaboration and learning among partners. Our work expands and diversifies the political activities in support of quality public education, and, by sharing our insights so others can learn, we help to advance individual and collective efforts that align with and accelerate progress towards our vision. In short, we believe our perspective on the critical role of politics in transforming public education, the processes to engage in during the journey, and the belief system that underlies the importance of traveling in a particular way, should be broadly understood and embraced by others.

The Role

We’re looking for a smart, collaborative, and strategic Talent and Operations Associate (TOA) to join our Operations team. The TOA will report to the Talent and Operations Manager and play a leading role in proactively identifying and engaging with top talent and developing a robust pipeline of candidates for both current and future hiring needs. The TOA will have the unique opportunity to help build and refine our organizational talent acquisition strategy and collaborate with hiring managers across teams to ensure that we are attracting and hiring the right talent to support Murmuration’s growth.

The types of positions that you will be recruiting for include both technical and non-technical roles, with varying levels of experience. Such positions include: data scientist, data engineer, partnerships manager, director of analytics, director of finance and operations, chief technology officer, etc.

While this role will focus primarily on recruitment, it will also include general operational functions. Over the medium-term, you’ll assist across all operating functions – HR, IT, Finance, and general office management – to ensure that Murmuration is operating as effectively, compliantly, and efficiently as possible.

Sourcing and Recruiting

Manage full-cycle recruitment for a variety of roles within the organization, including sourcing, screening resumes, scheduling and conducting phone interviews, and organizing in-person interviews
Design sourcing strategies and leverage various channels to identify qualified candidates (LinkedIn, BuiltIn, local universities, tech conferences, community networking events, etc.)
Cultivate relationships with active and passive candidates to ensure a diverse and highly qualified talent pool
Identify, adopt, and maintain an applicant tracking system (ATS) to manage the pipeline of qualified candidates for both current and future positions
Analyze data and track metrics to optimize the sourcing and recruitment processes
Build strong market knowledge of key talent roles
Establish strong relationships with hiring managers and various stakeholders to ensure that there is tight alignment throughout the recruiting and hiring processes
Operations Support

Develop a strong familiarity over time with Murmuration systems (across HR, Finance, and IT) and support the organization in achieving compliance and efficiency across all functions
Support in the implementation of various organization-wide initiatives, including but not limited to cultural activities, professional development, teammate relations, etc.
Oversee technology issues including but not limited to procurement, management, and support of staff and office IT needs
Support general office management efforts
Candidate Profile

Murmuration attracts employees with distinctive and diverse backgrounds and accomplishments. Integrity, creativity, flexibility, and drive are key attributes of competitive candidates.

Qualifications

A successful candidate for this role will possess a mix of talent, experience, and curiosity. You should be able to see the big picture while also paying attention to the details. You will have your finger on the pulse of what’s new and possible, and what works (and doesn’t) when it comes to sourcing and recruitment. You have strong interpersonal skills, build long-term relationships, and are able to assess candidates for strong cultural fit, in addition to technical fit.

3-5 years of experience working in talent acquisition, talent management, and compliance
Successful track record in building robust diverse talent pipelines
Successful track record in filling technical roles
Quick study, willing to learn new things and learn them quickly by any means necessary
Knowledge of and experience with employment laws
Strong communication and relationship-building skills
Proficient to Advanced across MS Office suite including Excel, Powerpoint, and Word
Location, Compensation and Benefits

The Talent and Operations Associate is a full-time, salaried position based onsite in New York City, with a comprehensive benefits package. Salary for this position is commensurate with experience.

An Equal-Opportunity Employer with a Commitment to Diversity

Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.

Location

This position will be based in New York, NY."
114,Backend/Data Engineer II,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"The Role

TripleLift is seeking an experienced data engineer to join our team full time. We are a fast growing startup in the advertising technology sector, trying to tackle some of the most challenging problems facing the industry. As a data engineer, you will be responsible for building and improving our data pipeline and reporting infrastructure. This role consists of a variety of responsibilities from building out our large scale data pipelines to building and managing a set of data driven API's with the corresponding front end components.

Core Technologies

We employ a wide variety of technologies here at TripleLift to accomplish our goals. From our early days, we've always believed in using the right tools for the right job, and continue to explore new technology options as we grow. The data engineering team primarily uses the following technologies:


Apache Spark & Scala for our core data processing framework
Java & VoltDB to process to power our sub second real-time data pipeline\
PHP7 & Symfony to power our reporting API's
Apache ZooKeeper & Apache Kafka to power are message bus
Redshift/Snowflake & Druid as our main data stores
Amazon Web Services, Docker, Mesos, & Marathon to power our services

Current Projects / Expected Responsibilities


Build out TripleLift's real time data pipeline to support bringing all the data and metrics in our system up to date with latency measured in milliseconds
Rebuild our spark pipeline to make it more extensible, maintainable, and scalable
Take our monolithic API system and split it out reporting into a microservice based system built from scratch to power all our reporting, scheduling, and alerting needs.

Required Skills


Experience in either of the following:
Large scale distributed data processing & pipelining
Building out Asynchronous API's and services
Experience with RDBMS such as PGSQL
Experience working with queues or other messaging systems such as RabbitMQ or Apache Kafka

Preferred Skills


Experience with a object oriented language such as Java or PHP
Experience with a functional based language such as Scala
Experience with a distributed data processing framework such as Spark
Experience building out RESTful APIs
Experience with distributed data bases such as Druid, VoltDB, Snowflake, or Cassandra
Basic frontend knowledge and competencies

Education Requirement

A Bachelor's degree in computer science,or a related discipline is preferred, although candidates with relevant experience who hold other degrees will be considered.

Experience Requirement

At least 2 years of working in a professional, collaborative environment.

Location

New York, NY

Engineering at TripleLift

The formal engineering department is filled with dedicated, smart, and hardworking individuals who take a great deal of pride in their work. Some of the reasons we all enjoy working here:


You get ownership and accountability for projects and have a voice in how the work happens
An onus on collaboration and learning over pure short-term output
Innovation is baked into how we operate; we're accepting of new ideas and opened to change
Dedicated time set aside each week to pair with others to learn or experiment
Team social events such as cooking classes, D&D, and gaming nights

About TripleLift

TripleLift makes native advertising simple, scalable, and effective. Leveraging pioneering computer vision technology, TripleLift seamlessly transforms visual content like images and video into native ads that match the unique look and feel of a publisher's website. TripleLift's native inventory is accessible through the industry's first real-time, native programmatic exchange, helping marketers reach millions of consumers across any device, at scale. Since 2012, TripleLift has delivered meaningful results for some of the world's biggest brands through what it calls the next evolution of display advertising.

Benefits and Company Perks


Amazing company culture
Competitive salary and performance-based bonuses
Ongoing professional development
Comprehensive Medical, Dental and Vision insurance
Equity options
401(k) program
Snacks on snacks on snacks
Yoga, massages, and meals

TripleLift Awards


Inc. Magazine's list of fastest-growing companies - 2017, 2018, 2019
Crain's Best Places to Work - 2015, 2016, 2017, 2018
Forbes Next Billion Dollar Companies - 2018 Inc.
Crain's Fast 50, Fastest Growing Companies in New York - 2017, 2018
Deloitte Technology Fast 500 - 2017, 2018
CEO & Co Founder Eric Berry won the Ernst & Young Entrepreneur Of The Year 2019 New York Marketing and Advertising Award

Note: The Fair Labor Standards Act (FLSA) is a federal labor law of general and nationwide application, including Overtime, Minimum Wages, Child Labor Protections, and the Equal Pay Act. This role is a FLSA non-exempt role."
115,"Consultancy: T4D Data Engineer, ICTD New York","New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"UNICEF works in some of the world’s toughest places, to reach the world’s most disadvantaged children. To save their lives. To defend their rights. To help them fulfill their potential.

Across 190 countries and territories, we work for every child, everywhere, every day, to build a better world for everyone.

And we never give up.

For every child, a connected world

The overarching strategic goal of UNICEF's ICT Division (ICTD) is to transform and build partnerships with our stakeholders to successfully implement UNICEF programmes globally through the use of innovative technology-enabled solutions.

UNICEF’s Information and Communication Division (ICTD)’s recently-established Field Solutions Unit is a team of experienced software developers, data scientists, and designers within ICTD’s Solutions Centre and Support.

The Unit’s goals are to establish enterprise support to innovative open-source tools used by field-based programmatic experts, planning specialists, partnership managers, and implementing partners around the world. They work with counterparts throughout the organization to tailor open-source software solutions that magnify UNICEF’s impact by driving better decision-making, promoting collaboration, and realizing organizational efficiencies with modern digital tools.

How can you make a difference?

RapidPro is a globally supported mobile engagement platform used by dozens of UNICEF country offices to magnify and monitor programmatic impact for children—and is the technology platform that powers UNICEF’s U-Report programme. With the strong uptake of RapidPro and related apps, the Field Solutions Unit seeks to realize more value from these systems by unlocking insights from data generated and captured by these systems, with a focus on interventions that use the RapidPro platform.

The T4D Data Engineer will liaise with colleagues throughout the organization to identify opportunities and needs for improved data analysis. The T4D Data Engineer may be required to attend meetings, conferences, and other events, as well as attend meetings with colleagues based in the field.

Key Deliverables:

The exact tasks for the consultant will be jointly determined in an agile way and are outlined in summary below. The consultant will directly interface with the ICT Manager of Field Solutions and will work on tasks/areas as prioritized and authorized by the ICT Manager each month.

Phase 1
Data extraction, transformation, and loading from UNICEF RapidPro workspaces and other data sources
Implementation of access management schemes
Documenting API integrations and SLAs
Configuration of charts and dashboards using Superset and PowerBI
Phase 2 (to begin on completion of Phase 1)
Data exploration, computation, enrichment, and aggregation
Generation of metrics, charts and analysis around relevant issues such as social engagement, gender differences, campaign dynamics, etc.
Identify data driven stories and insights that can help better communicate and understand the voices of young people using our platforms.
Required skills and experience:

Minimum 5 years of professional experience as a Data Engineer or Software Developer;
Experience working with relational and distributed databases;
Demonstrable knowledge of SQL scripting to manipulate data across multiple sources;
Experience with Python & Pandas;
Experience creating or contributing to beautiful, clear and compelling graphs;
Attention to detail, research skills and the ability to work under strict deadlines is a must;
Strong understanding of statistics and mathematics;
Bachelor’s degree in Computer Science, Statistics, Mathematics, data science or a related field.
 ADDITIONAL QUALIFICATIONS DESIRED FOR THIS ASSIGNMENT

Education/work experience in engineering/technical fields is a major plus;
Experience with Apache Spark or Databricks;
Experience with data visualization and business intelligence tools, such as Apache Superset and/or Microsoft PowerBI;
Experience with scripting, or workflows using Jupyter and/or Zeppelin notebooks;
Experience with data workflow orchestration tools, such as GNU Make, Apache Nifi, Luigi, or Airflow;
Experience with natural language processing, time series analysis, machine learning or network analysis;
Familiarity with UNICEF organizational structure;
Comfortable coordinating between multiple remote teams and stakeholders;
Superior command of the English language and excellent writing skills;
Experience working with software product development teams;
Experience with processing and synthesizing large amounts of information;
Ability to work in a team and in a diverse work environment.
This consultancy will be on-site (New York Headquarters) for a period of 11.5 months.

Evaluation Method:

Each applicant will be evaluated based on the cumulative analysis methodology (weighted scoring method), where the award of the contract will be made to the individual consultant or vendor whose offer has been evaluated and determined as:

Responsive/compliant/acceptable; and
Having received the highest score out of a pre-determined set of technical skills and financial proposal specific to the solicitation.
Technical Criteria weight: [70%]

Financial Criteria weight: [30%].

Please indicate your ability, availability and daily/monthly rate (in US$) to undertake the terms of reference above (including travel and daily subsistence allowance, if applicable). Applications submitted without a daily/monthly rate will not be considered.

UNICEF is committed to diversity and inclusion within its workforce, and encourages all candidates, irrespective of gender, nationality, religious and ethnic backgrounds, including persons living with disabilities, to apply to become a part of the organization.

UNICEF has a zero-tolerance policy on conduct that is incompatible with the aims and objectives of the United Nations and UNICEF, including sexual exploitation and abuse, sexual harassment, abuse of authority and discrimination. UNICEF also adheres to strict child safeguarding principles. All selected candidates will, therefore, undergo rigorous reference and background checks, and will be expected to adhere to these standards and principles.

Due to the high volume of applicants, only shortlisted candidates will be contacted and advance to the next stage of the selection process.

Opening Date Fri Oct 04 2019 08:00:00 GMT-0500 (Central Daylight Time) Eastern Daylight Time
Closing Date Mon Oct 14 2019 22:55:00 GMT-0500 (Central Daylight Time)"
116,Data Engineer,"New York, NY",New York,NY,None Found,None Found,"
5+ years of experience in using SQL and databases in a business environment
5+ years of experience in custom ETL/ELT design, implementation, and maintenance
3+ years of experience with schema design and data modeling
3+ years of experience applying data architecture or engineering to solve real-world business problems
2+ years of experience with building integration and ingestion frameworks leveraging API based tools and platforms (i.e. Dell Boomi)
Manipulating/mining data from database tables (i.e. SQL Server, Redshift, Oracle)
SQL, ETL/ELT optimization, and analytics tools experience (i.e. R, HiveQL)
Prior implementation experience in building both batch and real-time/near real-time data ingestion frameworks using technologies like Kafka, AWS Kinesis etc.",None Found,"
Collaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions
Conduct and support white-boarding sessions, workshops, design sessions, and project meetings as needed, playing a key role in client relations
Design and build data extraction, transformation, and loading processes by writing custom data pipelines using either cloud native services (AWS/GCP/Azure) or using open source tools (like Airflow and Python)
Design, implement, and support an Enterprise Data platform (Data Lake, Data Warehouse, etc.) that can provide ad-hoc access to large scale structured, semi-structured, and/or unstructured datasets
Experience with both traditional (i.e. SSIS, Informatica, Talend) and modern (i.e. Dell Boomi) data integration iPaaS technologies
Highly self-motivated to deliver both independently and with strong team collaboration
Ability to creatively take on new challenges and work outside comfort zone
Strong written and oral communications along with presentation and interpersonal skills
Deliver transformative solutions to clients that are aligned to industry best practices and provide thought leadership in data architecture and engineering space",None Found,None Found,"Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of what’s possible—together.

Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to nearly 6,000 employees. We were named one of Fortune’s 100 Best Companies to Work For in 2018 and are regularly recognized by our employees as a best place to work. You can find us in 28 cities across the U.S., U.K., and Canada.

Job Title:
Data Engineer

Data is served to hundreds of our clients and their customers. There is a big demand for analytics and data-freshness, so if you are not afraid of complex business problems, and you think traditional tools are slow, and you won’t stop looking for new approaches and technologies, then we have a position for you!

As a Data Engineer, you should have the expertise in the design, creation, management, and business use of large data sets to drive practical insights. You know and love working with analytics tools and can use your technical skills and creative approaches to help clients solve their most critical business challenges. This individual will be an integral part of New York’s Data and Analytics Practice and, in addition to working with clients, collaborate closely with members of the Slalom team who are focused on data visualization, advanced analytics, data science, and data strategy.

Responsibilities:

Collaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions
Conduct and support white-boarding sessions, workshops, design sessions, and project meetings as needed, playing a key role in client relations
Design and build data extraction, transformation, and loading processes by writing custom data pipelines using either cloud native services (AWS/GCP/Azure) or using open source tools (like Airflow and Python)
Design, implement, and support an Enterprise Data platform (Data Lake, Data Warehouse, etc.) that can provide ad-hoc access to large scale structured, semi-structured, and/or unstructured datasets
Experience with both traditional (i.e. SSIS, Informatica, Talend) and modern (i.e. Dell Boomi) data integration iPaaS technologies
Highly self-motivated to deliver both independently and with strong team collaboration
Ability to creatively take on new challenges and work outside comfort zone
Strong written and oral communications along with presentation and interpersonal skills
Deliver transformative solutions to clients that are aligned to industry best practices and provide thought leadership in data architecture and engineering space

Qualifications:

5+ years of experience in using SQL and databases in a business environment
5+ years of experience in custom ETL/ELT design, implementation, and maintenance
3+ years of experience with schema design and data modeling
3+ years of experience applying data architecture or engineering to solve real-world business problems
2+ years of experience with building integration and ingestion frameworks leveraging API based tools and platforms (i.e. Dell Boomi)
Manipulating/mining data from database tables (i.e. SQL Server, Redshift, Oracle)
SQL, ETL/ELT optimization, and analytics tools experience (i.e. R, HiveQL)
Prior implementation experience in building both batch and real-time/near real-time data ingestion frameworks using technologies like Kafka, AWS Kinesis etc.

Nice to have:

Implementation experience with various technologies under Hadoop eco-system (HDFS, Hive, HBase, Sqoop, Pig, Presto etc.) and Spark (PySpark preferred)
Experience with designing digital data platforms leveraging clickstream data from Adobe Analytics (Omniture/SiteCatalyst) or Google Analytics
Experience in languages such as Python and Spark
Experience working in Agile Scrum teams
Linux and Windows proficiency
Management consulting experience
Project management experience
Experience working with various verticals (i.e. insurance, retail, healthcare, financial services, technology)

Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."
117,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,"
Python & Pandas
Spark, MapReduce
AWS
SQL, Postgresql
Past ETL experience
Experience with data governance and ownership of data pipelines",None Found,None Found,None Found,"Join Andrew Yang and the fight to put Humanity First as he runs for president in 2020!

As one of the most exciting and unique candidates in the Democratic presidential race, entrepreneur Andrew Yang sets out to bridge the divide in American society. In 2011, he founded Venture for America, an entrepreneurial non-profit that helped create 2,500 jobs in cities like Cleveland, Detroit, Baltimore, and Pittsburgh.

Once Andrew realized that new technology, such as artificial intelligence, threatened to eliminate one-third of all American jobs, he knew he had to do something. In The War on Normal People (2018), he explains the mounting crisis and makes the case for implementing a freedom dividend of $1,000 a month for every American adult: no strings attached. Most importantly, Andrew Yang understands that the future of America is not left, not right, but forward.

To learn more about the campaign, go to Yang2020.com.
You can also hear more from Andrew on the Joe Rogan Experience!

Every campaign claims to be data driven. After working for us, you'll realize everyone else is just fooling themselves.

To support this, we need data engineering talent to help build the data pipelines the campaign will rely on. We're integrating multiple data sources, many of which are difficult to work with.

We're looking for data engineers who enjoy automating data flows, know how to deal with messy data, and who don't mind doing a few (dozen) manual uploads if that's what the job takes.

We would prefer to hire people in NY or who can relocate, but we're open to remote work for the right candidate.
Skills:
Python & Pandas
Spark, MapReduce
AWS
SQL, Postgresql
Past ETL experience
Experience with data governance and ownership of data pipelines
Data is the lifeblood of the campaign, and as a data engineer you'll be responsible for keeping it moving. This is true in every campaign, but no other organization recognizes that fact or values their engineering staff to the extent that we do at Yang2020.

Yang2020 is committed to diversity among its staff, and recognizes that its continued success requires the highest commitment to obtaining and retaining a diverse staff that provides the best quality services to supporters and constituents. We strongly encourage diverse candidates to apply. Yang2020 is an Equal Opportunity Employer and does not discriminate on the basis of race, color, religion, sex, age, national origin, genetic information, protected veterans, marital or familial status, sexual orientation, gender identity or expression, disability status, criminal record information (except where permitted under applicable law), or any other category prohibited by local, state or federal law. This policy applies to all aspects of employment, including recruitment, placement, promotion, transfer, demotion, compensation, benefits, social and recreational activities and termination. For more information about equal employment opportunity, please click here for “EEO Is the Law.” For information regarding your Right to Work, click here for details in English and Spanish."
118,Data Engineer,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"Help us Build the Future of Money

Gemini Trust Company, LLC (Gemini) is a licensed digital asset exchange and custodian. We built the Gemini platform so customers can buy, sell, and store digital assets (e.g., Bitcoin, Ethereum, and Zcash) in a regulated, secure, and compliant manner.

Digital assets and blockchain technology have the power to transform the world for good. This truth, along with our core values, form the bedrock of our company and culture. At Gemini, no job is too small and no project too big as we endeavor to build the future of money. We are a mission-driven, team-based, inclusive, and determined community of thought leaders who invest in each other and the long game. Join us in our mission!

THE DEPARTMENT: DATA ENGINEERING

THE ROLE: DATA ENGINEER

As a member of our data engineering team, you'll shape the way we approach data at Gemini by using your engineering, analytical and communication skills to work with teams across the business. You know how to ask the right questions and are passionate about using data to support and drive informed business decisions. You are ready to roll up your sleeves and are excited to take on challenging opportunities and projects. You'll help to build data infrastructure at Gemini for cross-functional teams to leverage the data to improve operational efficiency, products and achieve KPIs. Communicating your insights with leaders across the organization is paramount to success.

RESPONSIBILITIES:

Assist in designing and implementation of best-in-class Data Infrastructure and Business Intelligence solutions
Research new tools and technologies to build new and improve existing processes
Participate in design discussions and meetings
Design, automate, build, and launch scalable, efficient and reliable data pipelines in production
Build data integrations to integrate different data sources into the Data Lake
Assist in building real-time data streaming and reporting solutions
Schedule, monitor and maintain data pipelines using a workflow management tool
Design, build and enhance dimensional models for Data Warehouse and Data Marts
Create test plans, test scripts and build data reconciliation processes
Tune SQL queries, reports and ETL/ELT processes
Partner with engineers, project managers, and analysts to deliver insights to the business
Perform root cause analysis and resolve production and data issues

MINIMUM QUALIFICATIONS:

4+ years experience in data engineering and data warehouse technologies
4+ years experience in custom ETL design, implementation and maintenance
2+ years experience working with AWS cloud technologies
2+ experience working with Airflow or a similar workflow management tool
Skilled in programming languages Python and/or Java
Experience building real-time data streaming solutions using Kafka and/or other tools/technologies
Experience with schema design and dimensional data modeling
Experience with one or more MPP databases(Redshift, Bigquery, Snowflake, etc)
Advanced SQL skills is a must
Experienced in working collaboratively across different teams and departments
Strong technical and business communication

PREFERRED QUALIFICATIONS:

Kafka, Spark, HDFS, Cloud computing experience is a plus
Experience building and integrating web analytics solutions
Experience with one or more ETL tools(dbt, Informatica, Pentaho, SSIS, Alooma, etc)
Experience with continuous integration and deployment
Knowledge and experience of financial markets, banking or exchanges

It Pays to Work Here

We take a holistic approach to compensation at Gemini, which includes:


Competitive base salaries across all departments
Ownership in the company via profit sharing units
Amazing benefits, 401k match contribution, and flexible hours
Snacks, Perks, Wellness Outings & Events

Gemini is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you have a disability or special need that requires accommodation, please let us know."
119,"Sr. Data Engineer, Analytics Center of Excellence","New York, NY",New York,NY,None Found,None Found,"10+ years of progressive data application development experience, working in large scale/distributed SQL, NoSQL, and/or Hadoop environments3+ years of experience modeling and implementing ETL/ELT on columnar MPP database technologies such as Snowflake, BigQuery and/or RedshiftExperience with streaming architectures (Kafka, Kinesis, Pub/Sub, etc.)Experience working in hybrid cloud/on-premise environments as well as multi-cloudExperience implementing scalable, distributed, and highly available systems using cloud technologies, such as Microsoft Azure, Amazon Web Services, and/or Google CloudExperience building microservices topologies, including operational concerns such as resiliency, observability, discovery and routing, etc.Undergraduate degree in the field of computer science or engineering, or focus on statistical analysis or equivalent experience highly desired",None Found,None Found,None Found,"10+ years of progressive data application development experience, working in large scale/distributed SQL, NoSQL, and/or Hadoop environments3+ years of experience modeling and implementing ETL/ELT on columnar MPP database technologies such as Snowflake, BigQuery and/or RedshiftExperience with streaming architectures (Kafka, Kinesis, Pub/Sub, etc.)Experience working in hybrid cloud/on-premise environments as well as multi-cloudExperience implementing scalable, distributed, and highly available systems using cloud technologies, such as Microsoft Azure, Amazon Web Services, and/or Google CloudExperience building microservices topologies, including operational concerns such as resiliency, observability, discovery and routing, etc.Undergraduate degree in the field of computer science or engineering, or focus on statistical analysis or equivalent experience highly desired","Work with architects, other data engineers and analysts to identify, engage, and integrate data sources for discovery and profiling and, where necessary, define data services that empower business processes
Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions
Operationalize data processing systems (dev ops)
Develop solutions combining data blending, profiling, mining, statistical analysis, and machine learning, to better define and curate models, test hypothesis, and deliver key insights
Participate in development sprints, demos, and retrospectives, as well as release and deployment
Collaborate in establishing and evolving development, testing, and documentation standards, as well as related code reviews
Partner with business analysts, application engineers, data scientists, etc., leveraging the appropriate tools, solutions, and/or processes as part of their data mining, profiling, blending, and analytical activities
Collaborate with NBCU’s technology domains to lead and ensure development and delivery of analytics solutions
Ensure policies and standards are followed across the organization
Deliver services that meet internal KPIs and customer SLAs
The ability to quickly build rapport and gain the respect and cooperation of both technology and business peers
Able to quickly learn new technologies as they become prevalent and widely implemented, and develop oneself
Driven to utilize both proven and unique solutions to address business problems until issue is resolved
Be able to deal with ambiguity and make quality decisions in a dynamic, fast-paced environment
Customer-focused, action-oriented and driven to achieve results in a positive manner, displaying ethical behavior, integrity, and building trust at all times
Strong teamwork, organizational and interpersonal skills; ability to communicate and persuade peers and thrive in a cross-functional matrix environment
Qualifications/Requirements
10+ years of progressive data application development experience, working in large scale/distributed SQL, NoSQL, and/or Hadoop environments3+ years of experience modeling and implementing ETL/ELT on columnar MPP database technologies such as Snowflake, BigQuery and/or RedshiftExperience with streaming architectures (Kafka, Kinesis, Pub/Sub, etc.)Experience working in hybrid cloud/on-premise environments as well as multi-cloudExperience implementing scalable, distributed, and highly available systems using cloud technologies, such as Microsoft Azure, Amazon Web Services, and/or Google CloudExperience building microservices topologies, including operational concerns such as resiliency, observability, discovery and routing, etc.Undergraduate degree in the field of computer science or engineering, or focus on statistical analysis or equivalent experience highly desired
Desired Characteristics
Analytical – You have experience in delivering self-service analytics solutions that promote data discoveryMedia-focused - Strong working knowledge of media including traditional and Direct to ConsumerCommunicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technicalAction-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trustTechnologist – You have a love for data and exploring new ways to do old things, as well as the outside the box thinking required to build scalable technology solutions with pattern-based thinking.
Sub-BusinessTechnology
Career Level
Experienced
CityNew York
State/Province
New York
CountryUnited States
About Us
The Analytics Center of Excellence (ACOE) at NBCUniversal is looking for a passionate problem solver who’s looking to build the next generation of data and analytics platforms. Reporting to the Chief Architect supporting the enterprise portfolio, this role is right for you if you’re a highly experienced data engineer who likes working in a “hands-on” agile environment and leveraging data to drive innovative solutions. Transcend the static nouns of platforms, data sources, integrations, interfaces, etc., and relish in the verbs of mining, modeling, predicting, innovating, and all-out rockin’ it to unlock the stories yearning to be told.

As part of the global Operations & Technology organization, the ACOE is focused on data and analytics strategies for the future. We support NBCU’s vast portfolio of brands - from broadcast, cable, news, and sports networks to film studios, world-renowned theme parks and a diverse suite of digital properties. We take pride in providing NBCUniversal with data to advise and shape strategic business decisions.
Notices
NBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable."
120,Data Engineer,"New York, NY 10048",New York,NY,10048,None Found,None Found,None Found,None Found,None Found,None Found,"Condé Nast is a premier media company renowned for producing the highest quality content for the world's most influential audiences. Attracting more than 100 million consumers across its industry-leading print, digital and video brands, the company’s portfolio includes some of the most iconic titles in media: Vogue, Vanity Fair, Glamour, Brides, Self, GQ, GQ Style, The New Yorker, Condé Nast Traveler, Allure, Architectural Digest, Bon Appétit, Epicurious, Wired, W, Golf Digest, Golf World, Teen Vogue, Ars Technica, The Scene, Pitchfork and Backchannel. The company’s newest division, Condé Nast Entertainment, was launched in 2011 to develop film, television and premium digital video programming. For more information, please visit condenast.com and follow @CondeNast and @CondeNastCareer for Twitter and @condenastcareers for Instagram
Job Description
The Data Services team is seeking a Data Engineer with a passion for creating data products to help

create more engaging, personalized experiences for users across Conde Nast’s properties. You will have

the opportunity to build both product features and internal data tools, all while working with a diverse

group of datasets - web events, ad streams, content and context models, etc. You will also get to work

with the newest data technologies available. Above all, you will influence how users interact with Conde

Nast’s industry-leading journalism.

Primary Responsibilities

Develop and maintain scalable data pipelines, with a focus on writing clean, fault-tolerant code
Maintain various data stores and distributed systems, such as Hive and Presto
Optimize data structures for efficient querying of those systems
Collaborate with internal and external data sources to ensure integrations are accurate, scalable and
maintainable

Collaborate with data science team on implementing machine learning algorithms to facilitate audience
intelligence and cross-brand personalization initiatives

Collaborate with business intelligence/analytics teams on data mart optimizations, query tuning and
database designs

Execute proof of concepts to assess strategic opportunities and future data extraction and integration
capabilities

Define data models, publish metadata, and best practice querying standards
Required Skills

2+ years data engineering and/or software development experience, preferably with experience using a
scripting language such as Python or Java

Fluency in SQL (any variant)
Experience with Hadoop and related technologies (Hive, Presto, Spark)
Exceptional analytical, quantitative, problem-solving, and critical thinking skills
Have a collaborative work style with strong desire to work in dynamic, fast paced environment that
requires flexibility and ability to manage multiple priorities

Desirable Skills

Experience with workflow / ETL tools and schedulers; e.g. Luigi, Airfow
Experience with AWS tools; especially EMR, S3, Lambda
Experience with GCP tools; e.g. BigQuery, DataFlow, PubSub
Experience with Apache Beam
What happens next?
If you are interested in this opportunity, please apply below and we will review your application as soon as possible. Please note that due to the high level of applications we receive, it is not always possible for us to respond to each applicant in person. Should your profile fit this open position we will contact you within approximately 4 weeks. You can update your resume or upload a cover letter at any time by accessing your candidate profile.
C ondé Nas t is an equal opportunity workplace.

Duties and responsibilities may be adjusted based on years of experience.

Salary is also commensurate with experience."
121,Data Engineer,"New York, NY 10017",New York,NY,10017,None Found,None Found,None Found,None Found,None Found,None Found,"About Caserta:
At Caserta, we work with leading organizations to deliver innovative Data & Analytics solutions. We specialize in Cloud Computing, Big Data, AI/ML, Business Intelligence, Data Warehousing, Modern Data Architecture, and Enterprise Data Management. We are looking for creative, entrepreneurial, and highly-motivated people to carry out our mission of designing, architecting, and implementing the most innovative, forward-looking Data-Driven solutions available to our clients.
DATA ENGINEER
As a Data Engineer at Caserta, you'll work in small teams to deliver innovative solutions using core cloud data warehouse tools and Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics.
Life as a Data Engineer at Caserta:
Work as part of a team to develop modern Data-Pipelines in Python, Spark and PySpark for real-time data streaming
Participate in the development of cloud data warehouses, business intelligence and analytics solutions across multiple industries and technology ecosystems
Exposure to the Latest & Greatest Cloud technologies in the market place, including but not limited to: AWS, EMR, S2, EC2, Glue, Kinesis, Redshift, Snowflake; Google Cloud, GCS, DataPrac, DataFlow, BigQuery, PubSub; Azure, Blob Storage, Data Factory, SQL DW
Work with other highly motived individuals who have a passion for solving complex data & analytics challenges and learning new technologies
Design, Build, Migrate complex ETL pipelines, extracting and combining data from various heterogeneous data sources
How to thrive at Caserta:
Be Agile: Data Analysis and Data Profiling are key, map data across the organization to build robust and efficient pipelines
Be Insightful: Clients come to Caserta to gain Strategic & Actionable Insights from their data, you will guide them along their journey to realizing the value of their data assets
Be Entrepreneurial: Bring ideas and new ways of Innovating in and through data
Be Consultative: Direct interaction with clients requires the ability to explain complex technical issues in non-technical terms, leveraging the business value of data
Be Resourceful: Imagination is a skill at Caserta; you will learn many new technologies, and conceive new ways of efficiently and optimally implementing new tools and products
Be Knowledgable: of Database Structures, Theories, Principles, and Best Practices
bg9ulNoDdW"
122,Data Engineer,"New York, NY 10004",New York,NY,10004,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Engineer will design and develop highly robust, repeatable and scalable workflow patterns to ingest, integrate and publish a wide variety of data from internal and external sources. They will be relentless in their focus on delivery and end ensuring that our data workflows and pipelines are enterprise-grade – reliable, scalable and secure.
Responsibilities include:
Designing, implementing, maintaining highly robust, scalable and flexible ETL jobs and data workflows
Automating workflows leveraging DevOps framework where applicable
Ensuring quality of technical solutions as data moves across Healthfirst environments
Working with solution and data architects to develop, construct, test and maintain architectures, prototypes and design solutions
Helping to maintain the integrity and security of the company data
Providing insight into the changing data environment, data processing, data storage and utilization requirements for the company and offer suggestions for solutions
Articulating both the technical implications as well as data usage implications of proposed solutions or solution options to a wide variety of stakeholders.
Working directly with business users to align solutions with business requirements
Relentless focus on continuous improvement of solutions in to improve data reliability, efficiency, quality and more
Create data monitoring capabilities for each business process and work with data consumers on updates
Collaborating with cross functional teams comprised of technical and business colleagues
Minimum Qualifications:
Bachelor’s Degree in Computer Engineering or related field or equivalent experience
3 + years’ experience in a data engineering
Proficiency in SQL a must - knowledge of SQL and multiple programming languages in order to optimize data processes and retrieval
Strong experience in data programing languages such as Python or Spark
Strong experience working in a Big Data ecosystem processing data; includes file systems, data structures/databases, automation, security, messaging, movement, etc.
Experience working in a production cloud infrastructure
Prior backgrounds with AWS services (e.g. lambda, S3, Aurora, Redshift);
Exposure to CI/CD for software systems
Must be able to develop creative solutions to problems
Demonstrates critical thinking skills with ability to communicate across functional departments to achieve desired outcomes
Ability to work independently and as part of a team
Skilled in Microsoft Office including Project, PowerPoint, Word, Excel and Visio
Preferred Qualifications:
Hands on experience in leading healthcare data transformation initiatives from on-premise to cloud deployment
Demonstrated experience working in an Agile environment as a Data Engineer
Experience with other Cloud Data Warehouses (Big Query, Snowflake);
Knowledge of provider-sponsored health insurance systems/processes and the Healthcare industry
Proficiency with graph and noSQL databases
WE ARE AN EQUAL OPPORTUNITY EMPLOYER. Applicants and employees are considered for positions and are evaluated without regard to mental or physical disability, race, color, religion, gender, national origin, age, genetic information, military or veteran status, sexual orientation, marital status or any other protected Federal, State/Province or Local status unrelated to the performance of the work involved.
If you have a disability under the Americans with Disability Act or a similar law, and want a reasonable accommodation to assist with your job search or application for employment, please contact us by sending an email to careers@Healthfirst.org or calling 212-519-1798 . In your email please include a description of the accommodation you are requesting and a description of the position for which you are applying. Only reasonable accommodation requests related to applying for a position within Healthfirst Management Services will be reviewed at the e-mail address and phone number supplied. Thank you for considering a career with Healthfirst Management Services.


EEO Law Poster and Supplement"
123,Data Engineer (R Shiny),"New York, NY 10010",New York,NY,10010,None Found,None Found,None Found,None Found,None Found,None Found,"dv01 is the world's first end-to-end data management, reporting and analytics platform offering loan level transparency and insight into lending markets, making them more efficient for institutional investors and safer for the world. In a nutshell, we're doing our part to prevent a repeat of 2008.

As the technological hub between lenders and capital markets, dv01 provides all parties with unprecedented data transparency, insight, and analytics. dv01 has integrated data from 16 marketplace lending platforms, including LendingClub, Prosper and SoFi and multiple mortgage servicers. To date, dv01 has provided reporting and analytics on $105 billions of online lending and mortgage loans and $1.4 trillion of securitization coverage.

To get a better idea of what a year at dv01 looks like, check out our 2018 Year in Review page here: https://dv01.co/2018yearinreview/ ( https://dv01.co/2018yearinreview/ ). If that looks like fun to you, get in touch because we'd love to hear from you.

You Will:
---------

Be at the heart of dv01. You will be a critical resource in taking dv01's reporting infrastructure to the next level, contributing to a variety of integral processes that drive dv01 on a daily basis.

Directly impact our largest product offering. dv01 has established itself as a centerpiece in the securitization ecosystem within Marketplace Lending and is quickly developing that reputation within other asset classes. You will become a key stakeholder in expanding and scaling our securitization offerings right off the bat.

Serve as a technical mentor and thought-leader. You will work closely with our team of Reporting and Loan Analysts to transform existing scripts into well-tested, productionalized packages within our R infrastructure. You will be instrumental in developing a stronger engineering culture across our reporting efforts while continuously striving to improve through innovation.

Work with state of the art technology. You will have the opportunity to work with exciting, cutting-edge R technologies, building on-top of an infrastructure that leverages Spark under-the-hood to source all of our data in the cloud.

QUALIFICATIONS:
An experienced developer, with significant R chops. You have at least 3 - 5 years of professional software development experience, with at least 3 years experience working in R. You have worked within productionalized infrastructures, handling datasets that cross into the 10+ GB range and have spent your fair share of time dealing with resource and performance considerations.

An experienced teacher or mentor. You not only have experience helping team members grow as engineers, but love to go out of your way to help people maximize their potential. You make the time to meet with developers for code reviews, knowledge shares, and technical trainings.

Interest and experience in both engineering and finance. You're looking to grow your skills in both disciplines, and are excited about the synergies between finance and technology.

Undergraduate or graduate degree in Computer Science. Note that we're not anti dropouts if you're a superstar.

BENEFITS & PERKS:

Almost 100% paid benefits (medical/dental/vision)
$1k annual professional development stipend
$130 Monthly Commuter Budget
Daily lunch and dinner allowance via Seamless
Free premium Equinox or ClassPass membership
Unlimited PTO and remote days
Free snacks and beverages
Company outings (Happy Hours, Holiday Parties, Team Yoga, Book Club, etc.)
Casual, collaborative culture

dv01 is an equal opportunity employer and all qualified applicants and employees will receive consideration for employment opportunities without regard to race, color, religion, creed, sex, sexual orientation, gender identity or expression, age, national origin or ancestry, citizenship, veteran status, membership in the uniformed services, disability, genetic information or any other basis protected by applicable law."
124,"Senior Data Engineer - Hadoop, VP","New York, NY 10261",New York,NY,10261,None Found,"
6-10 years of relevant experience in Apps Development or systems analysis role
Extensive experience system analysis and in programming of software applications
Experience in managing and implementing successful projects
Subject Matter Expert (SME) in at least one area of Applications Development
Ability to adjust priorities quickly as circumstances dictate
Demonstrated leadership and project management skills
Consistently demonstrates clear and concise written and verbal communication","
Proven ability in working with the development team members and other partners, with minimal supervision
Strong verbal and written communications skills, excellent interpersonal skills with ability to communicate well at all levels
Team Player, self-starter and thorough who is willing to take on any assigned job/responsibilities
Ability to learn new skills quickly with little supervision and ensuring the detail is of high priority
Efficiently and effectively manages work, time, and resources
Ability to work under high-pressure situations and effectively prioritize in a highly dynamic work environment that includes a global focus
Strong problem solving and program execution skills while being process orientated
Ability to understand the big picture – can step back and understand the context of problems before applying analytical skills to address the issues","
Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements
Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards
Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint
Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation
Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals
Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions
Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary
Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.","
Bachelor’s degree/University degree or equivalent experience
Master’s degree preferred",None Found,"The Applications Development Technology Lead Analyst is a senior level position responsible for establishing and implementing new or revised application systems and programs in coordination with the Technology team. The overall objective of this role is to lead applications systems analysis and programming activities.

Responsibilities:
Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements
Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards
Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint
Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation
Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals
Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions
Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary
Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.

Qualifications:
6-10 years of relevant experience in Apps Development or systems analysis role
Extensive experience system analysis and in programming of software applications
Experience in managing and implementing successful projects
Subject Matter Expert (SME) in at least one area of Applications Development
Ability to adjust priorities quickly as circumstances dictate
Demonstrated leadership and project management skills
Consistently demonstrates clear and concise written and verbal communication

Education:
Bachelor’s degree/University degree or equivalent experience
Master’s degree preferred
This job description provides a high-level review of the types of work performed. Other job-related duties may be assigned as required.
Citi’s Treasury and Trade Solutions group is built on the power of our network. Citi Treasury and Trade Solutions (TTS), provides integrated cash management and trade finance services to multinational corporations, financial institutions and public sector organizations across the globe. With a full range of digital and mobile enabled platforms, tools and analytics, TTS continues to lead the way in delivering innovative and tailored solutions to its clients. TTS offers the industry’s most comprehensive suite of treasury and trade solutions including cash management, payments, receivables, liquidity management and investment services, working capital solutions, commercial card programs, trade finance and services.
TTS counts 80% of Global Fortune 500, over 1,100 public sector entities and thousands of financial institutions amongst its diverse client base
Conducts business In 160 countries and jurisdictions, and transacting in over 140 currencies, TTS processes some USD 4.0 trillion of client payments everyday
Collaborates with clients to deliver next-generation financial solutions using TTS Innovation Labs
Application / Team Overview:
The Vanguard TTS Big Data platform is a multi-year critical program within the Data, Innovation and Architecture Technology, part of Treasury & Trade Solutions. The candidate will have great exposure and opportunity to work with the latest technologies such as Hadoop Big Data eco-system, Cloud Technology, Real-time processing and Analytics.
The candidate will work closely with the Big Data development team and interface with other internal teams and learn how a leading organization conducts itself.
Role Description:
Hiring a hands-on Data Engineer who will take the overall responsibility for end to end software development, continuous integration and continuous deployment, meeting a high level of code quality working within established timelines and Engineering Excellence best practices.
Specifically the candidate will work on a cloud-based initiative to enhance our data management capabilities enabling superior analytics, client experience, and regulatory reporting and data science solutions across business lines globally.
The Data Engineer will have a solid understanding of the “big data” infrastructure and can design, build, integrate data from various resources, and manage big data platform. The data engineer may be required to write complex queries and ensure data is easily accessible, works smoothly with an end goal of optimizing performance of Citi's big data ecosystem. The candidate may also be required to run some ETL on top of big datasets and create big data warehouses that can be used for reporting or analysis by data scientists.
The ideal candidate will be a resourceful software professional who can comfortably work in a large development team in a globally distributed, dynamic work environment that fosters diversity, teamwork and collaboration.
Bachelor’s degree (in Science, Computers, Information Technology or Engineering)
At least 3+ years overall IT experience with 3+ years’ experience working as a Data Engineer in Data Warehousing, Data Architecture or Data Management. Minimum 3 years of experience working with Hadoop Spark using Scala/Java.
3 years of Cloudera Hadoop 5.x / 6.x
Experience with real time messaging and ingestion including Kafka
Experience with developing frameworks and utility services including logging/monitoring using ELK or similar
Working experience with Big Data (HBase/Impala/Hive) No-SQL databases
Hands on experience with open source software platforms Linux
Experience with Oracle and/or MYSQL database application development.
Agile build and deploy DevOps experience preferred
Cloud experience with Google Cloud (DataProc or BigQuery) or AWS – EMR/Cloudera on EC2 is a plus
Leadership Skills:
Proven ability in working with the development team members and other partners, with minimal supervision
Strong verbal and written communications skills, excellent interpersonal skills with ability to communicate well at all levels
Team Player, self-starter and thorough who is willing to take on any assigned job/responsibilities
Ability to learn new skills quickly with little supervision and ensuring the detail is of high priority
Efficiently and effectively manages work, time, and resources
Ability to work under high-pressure situations and effectively prioritize in a highly dynamic work environment that includes a global focus
Strong problem solving and program execution skills while being process orientated
Ability to understand the big picture – can step back and understand the context of problems before applying analytical skills to address the issues
-
Grade :All Job Level - All Job FunctionsAll Job Level - All Job Functions - US
-
Time Type :Full time
-
Citi is an equal opportunity and affirmative action employer.
Minority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity.
Citigroup Inc. and its subsidiaries (""Citi”) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity CLICK HERE.
To view the ""EEO is the Law"" poster CLICK HERE. To view the EEO is the Law Supplement CLICK HERE.
To view the EEO Policy Statement CLICK HERE.
To view the Pay Transparency Posting CLICK HERE."
125,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,"
Bachelor’s degree in computer science, software/computer engineering, applied math, statistics or related field required
4+ years of related experience as a data engineer
4+ SQL-based technologies (e.g. PostgreSQL and MySQL)
2+ NoSQL technologies (e.g. Cassandra and MongoDB)
Experience with Hadoop-based technologies (e.g. MapReduce, Hive and Pig)
Good with scripting languages (Python, Perl, etc.)
Data warehousing solutions
Statistical analysis and modeling is a PLUS
Machine learning and data mining is a PLUS

No sponsorship available at this time.",None Found,"
Build real-time data capture and transformation functionality across all products
Work with other engineers to enhance data models and improve data query efficiency
Create complex data queries to facilitate ad hoc and exploratory analytics
Act as in-house data expert and make recommendations regarding standards quality and timeliness
Build out technology stack for Business Intelligence and Data Warehouse
Clean data: review for data inconsistencies and identify opportunities to improve data collection process
Wrangle/Munge data: transform or map data from one raw data form into another format with the intent of making it more appropriate and valuable for analytics
Develop, construct, test and maintain architectures such as databases and large-scale data processing systems
Design, construct, install, test and maintain highly scalable data management systems
Build high-performance algorithms, prototypes, predictive models and proof of concepts
Research opportunities for data acquisition and new uses for existing data
Employ a variety of languages and tools (e.g. scripting languages) to marry systems together
Recommend ways to improve data reliability, efficiency and quality
Collaborate with analysts and team members on project goals and targeted, relevant data sets
Build or recommend data visualization tools and business intelligence tools such as interactive dashboards and automated reports, to enable leaders to make swift, fact-based decisions",None Found,None Found,"Job Description

Honcker is a car leasing app startup that is set out to disrupt the automotive leasing industry. We are looking for a data engineer to join our growing team. You will work on improving our data infrastructure, while building out our new data warehouse and business intelligence tools. You will get to make decisions, architect solutions and elevate the organization by solving real problems that will allow the business to make key decisions. Excellent growth opportunity to grow with a startup that is moving fast!

Responsibilities:
Build real-time data capture and transformation functionality across all products
Work with other engineers to enhance data models and improve data query efficiency
Create complex data queries to facilitate ad hoc and exploratory analytics
Act as in-house data expert and make recommendations regarding standards quality and timeliness
Build out technology stack for Business Intelligence and Data Warehouse
Clean data: review for data inconsistencies and identify opportunities to improve data collection process
Wrangle/Munge data: transform or map data from one raw data form into another format with the intent of making it more appropriate and valuable for analytics
Develop, construct, test and maintain architectures such as databases and large-scale data processing systems
Design, construct, install, test and maintain highly scalable data management systems
Build high-performance algorithms, prototypes, predictive models and proof of concepts
Research opportunities for data acquisition and new uses for existing data
Employ a variety of languages and tools (e.g. scripting languages) to marry systems together
Recommend ways to improve data reliability, efficiency and quality
Collaborate with analysts and team members on project goals and targeted, relevant data sets
Build or recommend data visualization tools and business intelligence tools such as interactive dashboards and automated reports, to enable leaders to make swift, fact-based decisions

Qualifications:
Bachelor’s degree in computer science, software/computer engineering, applied math, statistics or related field required
4+ years of related experience as a data engineer
4+ SQL-based technologies (e.g. PostgreSQL and MySQL)
2+ NoSQL technologies (e.g. Cassandra and MongoDB)
Experience with Hadoop-based technologies (e.g. MapReduce, Hive and Pig)
Good with scripting languages (Python, Perl, etc.)
Data warehousing solutions
Statistical analysis and modeling is a PLUS
Machine learning and data mining is a PLUS

No sponsorship available at this time."
126,Data Engineer,"New York, NY 10022",New York,NY,10022,None Found,None Found,None Found,"
Build efficient codes to extract data and documents from various sources
Build OCR and NLP pipeline to retrieve data from unstructured data
Build standard reports from extracted data per business requirement
Conduct unit testing and document the finalized code set
Assist production implementation in set up infrastructure and automated processes
",None Found,None Found,"Description
Spruce is looking for a Data Engineer to join the Data Science team in NYC. The Data Engineer will develop new digital capability for new title underwriting automation and data science tools in the real estate industry. You will have initiative, innovation and good judgment in a dynamic startup environment.

Responsibilities:

Build efficient codes to extract data and documents from various sources
Build OCR and NLP pipeline to retrieve data from unstructured data
Build standard reports from extracted data per business requirement
Conduct unit testing and document the finalized code set
Assist production implementation in set up infrastructure and automated processes

You will possess strong social and interpersonal skills and be able to seamlessly collaborate with other members of the team and across functions. You will demonstrate attention to detail, organization, and work, including driving projects to completion in accordance with established timelines.
This position will work with the Director, Data Science and will locate in midtown Manhattan.

Qualifications

Bachelor’s degree in quantitative fields is required, computer science degree preferred
Have two years of Java, Python, and SQL database programming experience. The Python experience should include web scraper development
Have one year of natural language processing work or research experience
Unix, OCR, MongoDB, GCP experience are a plus
Knowledge of insurance industry and statistics is a plus
Excellent written and verbal presentation skills
Highly motivated and creative problem solver; able to self-initiate and thrive in a results-oriented environment; positive attitude
Ability to convey sense of urgency and to work productively and cooperatively with team members

"
127,Data Engineer,"New York, NY 10017",New York,NY,10017,None Found,None Found,None Found,None Found,None Found,None Found,"What is Slice?

Slice is the leading technology and marketing platform made exclusively for local pizzerias, making it super easy to order delicious, authentic local pizza anywhere, anytime. We serve the $45 billion U.S. pizzeria market in two ways: by providing a pizza-centric mobile and web ordering experience for consumers, and by empowering local restaurants with the technology, tools, and marketing to grow their business, while helping them compete with Big Pizza. Can you imagine what a small mom and pop pizza shop could achieve with the resources of Domino's?

The Role

We are looking for a business savvy Data Engineer to join our growing team of analytics experts. You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. You will support our software developers, business-intelligence and data scientists on data initiatives and will ensure optimal data delivery. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even building a new component of our company's data architecture to support our next generation of products and data initiatives. We believe in tested automation of pipelines. The member of our team should not only be agile in producing the data pipelines, but able to test those pipelines to ensure a robust and stable platform, without data leakage and with understanding of the expected outcome.

What you'll do:

Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build and support the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and DevOps teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure
Create data tools for analytics and data scientist team members which assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.

What we're looking for:

Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 2+ years of experience in a Data Engineer role.
They should also have experience with the types of technology represented below, but do not need to be the exact named-software:
Experience with big data tools: Hadoop, Spark, Kafka, etc.

History of Slice:
Slice was born in 2010 and has quietly bootstrapped its way to building a network of more than 11,000 pizzerias nationwide. Ilir Sela ( https://www.forbes.com/sites/amyfeldman/2018/04/10/pizza-unchained-tech-startup-slice-helps-local-pizzerias-get-online-and-fight-back-against-dominos/#56b483651263 ), our founder and CEO, started the company as a passion project to help his friends and family in the pizza business, but he quickly saw a massive opportunity to champion these small businesses by bringing their craft to the masses. Ilir has since built an amazing team of operators, marketers, technologists, and investors — all dedicated to making it easier for people to enjoy their favorite local pizza while helping local shops succeed.

Our Pizza Philosophy

Slice is on a journey to be the most valuable pizza brand on the planet. We connect makers and eaters to enrich their lives through the power of specialization and pizza expertise. Backed by generations of knowledge and cutting-edge technology, we give makers the platform and voice to take charge of their industry, expand their coverage, and fill the world with authentic cuisine. When passionate makers turn their craft into their livelihood, we all live happier, fuller lives. People stop accepting the homogenous, mass-produced pies of big chains and robotic trucks because the quality, variety, and authenticity of Slice is both hyper-convenient and known worldwide. We believe pizza isn't just food. It's a slice of community, a slice of culture, a slice of life.

We're growing our family every day — so, if you've got a passion for local, authentic pizza and the drive to help share it with the world, we'd love to have you on the team! Check out a few awards we've recently won for our workplace and culture: Inc. ( https://www.inc.com/profile/Slice ), Crain's ( https://www.crainsnewyork.com/html-page/672566 ), BuiltinNYC ( https://www.builtinnyc.com/companies/best-places-to-work-nyc-2019 )

Slice is an Equal Opportunity Employer and is committed to building an inclusive environment for people of all backgrounds and everyone is encouraged to apply. We do not discriminate on the basis of race, color, gender, sexual orientation, gender identity or expression, religion, disability, national origin, protected veteran status, age, or any other status protected by applicable national, federal, state, or local law."
128,"Training Solutions Advisor, Google Cloud","New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,"
Understand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.
Connect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.
Partner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.
Support escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.
Partner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.
",None Found,None Found,"Note: By applying to this position your application is automatically submitted to the following locations: Sunnyvale, CA, USA; Austin, TX, USA; New York, NY, USA; Reston, VA, USA
Minimum qualifications:

Bachelor's degree in Computer Science or a related technical field, or equivalent practical experience.
5 years of experience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.
Experience working in a customer facing environment in a technology company and helping customers identify solutions that best fit their unique needs.
Ability to travel to support Customer Engagements up to 30% of the time.

Preferred qualifications:

Google Cloud Certified e.g. Cloud Architect, Data Engineer or Associate Cloud Engineer or other comparable Cloud Certification.
Experience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.
Ability to take customers’ technical requirements and architect a proposal that maps to technical skills required.
Ability to quickly learn and understand new training offerings.
Ability to work well cross functionally and understand when to pull in specialist knowledge into Customer conversations.
Excellent communication and presentation skills.
About the job
The Google Cloud team helps customers transform and evolve their business through the use of Google’s global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you'll help shape the future of businesses of all sizes and enable them to better use technology to drive innovation.This role will enable you to make a huge impact across Google Cloud’s most strategic accounts and ensure they have Learning Plans that effectively help them develop the knowledge and skills they need to adopt Google Cloud. The role is also an exciting mix of elements as you will be working in Technical Customer Facing activities (usually in partnership with the CE or PSO/TAM Account owner), working in partnership with Cloud Learning GTM leads on Learning Plan development, and working with the Curriculum Tech leads to provide curriculum feedback and validate proposals as required.

Google Cloud helps millions of employees and organizations empower their employees, serve their customers, and build what’s next for their business — all with technology built in the cloud. Our products are engineered for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. And our teams are dedicated to helping our customers and developers see the benefits of our technology come to life.
Responsibilities
Understand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.
Connect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.
Partner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.
Support escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.
Partner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.
At Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form."
129,Sr Data Engineer,"New York, NY 10002",New York,NY,10002,None Found,None Found,None Found,None Found,None Found,None Found,"The Senior Data Engineer will help drive the design and development of highly scalable enterprise data mart deployed on Amazon for our core analytics and data science integration platform. You will create, own, manage, share support of, and drive best practices for our enterprise data mart which is used across multiple data intensive applications including reporting and data science for McGraw-Hill's Digital Platform Group.

Our Digital Platform Group is a world-class Education Technology team utilizing state-of-the-art applications to deliver intuitive learning experiences. DPG builds data-driven digital products that enhance teaching and improve learning outcomes. The team also conducts research and development targeted at new market opportunities in the quickly evolving education technology space. Join our DPG team and you’ll assist in innovating learning technology and impacting the next generation of learners!

As a Sr Data Engineer, you will contribute to the team by Working effectively with Technical Product Management and SCRUM masters to meaningfully contribute to our agile team. High/Low level designing, hands on software development, create frameworks, use third party frameworks, code reviews. Identifying gaps and proactively improve system service level agreements. Providing technical mentoring and coaching to engineers on the development team.

What you’ll need to be successful: 8+ years of experience with ETL data processing concepts with full implementation cycle experience in enterprise data marts, including advanced SQL development skills 6+ years Architecture and Optimization experience for Database systems technologies with a focus on data marts and data warehouses, including column stores and NoSQL implementations 2+ years of experience with Apache Spark and with a modern development language (Scala, Python, Java) Strong understanding of data modeling concepts, including schema development, validation, and evolution (normalized and denormalized) Experience with performance tuning and scaling production databases


Placed on hold due to lack of communication from hiring manager and manager repeatedly missing bi-weekly 1on1s.

When you join our team, you become part of a company that impacts millions of students and teachers every day. As a leader in the EdTech space, McGraw-Hill offers flexibility and collaboration while creating innovative products that positively impact learning. Our mission is to unlock the potential of every learner and every employee.

Join us for a career where you’ll grow both personally and professionally in a welcoming, diverse, and inclusive environment.
United States-Massachusetts-Boston"
130,Senior Data Engineer – Data Infrastructure,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"Spotify is looking for a Data Engineer to join us. You will build infrastructure to help scale Spotify’s internal data quality framework, ensuring that hundreds of engineers can feel confident in the data that they produce, so that they can continue to build data-driven solutions to bring music and digital media experiences to our 100 million active users and millions of artists. Above all, your work will influence the way people experience music.

What you’ll do
Build large-scale and real-time systems to provide the Spotify data processing environment with the ability to guarantee delivery of high quality data.
Use best practices in continuous integration and delivery.
Help drive optimization, testing and tooling to improve data quality.
Collaborate with other software engineers, Data Engineers and stakeholders, taking learning and leadership opportunities that will arise every single day.
Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.
Who you are
You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
You know how to write distributed, high-volume services in Java or Scala.
You are knowledgeable about data modeling, data access, and data storage techniques.
You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
You understand the value of partnership within teams."
131,SQL Developer/Data Engineer,"Elmwood Park, NJ 07407",Elmwood Park,NJ,07407,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description

Essential Duties and Responsibilities:Extract data from Client’s machines and restoring databases.Identify data elements from various database as requested by client.Working with Several Different Database Management Systems.Be very proficient with SQL Server.Strong SQL skills (SQL Server, Oracle, MySQL, Postgres etc.).Experience in database design and structure, with an emphasis on scalability.A strong desire to develop new and innovative ways to improve our data storage and processing.Prepare and perform data analysis and transformations to align data to business rules.Work with our clients Subject Matter Experts to obtain a greater understating of the business needs and goals.Contribute to knowledge management activities and promote best practices for project execution.Programming experience required in any object oriented programming languages like C#, Java etc.Excellent SQL skills, with experience in building and interpreting complex queries.

Qualifications

QualificationsA minimum 3 years of professional experience as SQL Developer or Data EngineerA minimum 5 years of professional experience in information technology.A minimum of bachelor's or higher in Computer Science, Information Systems, or equivalent degree or strong industry experience.

Nice-to-Haves:Healthcare Domain knowledge preferred.Experience in HL7,CCDA preferred.
Additional Information

This is a full-time, onsite position at our HQ currently located in Elmwood Park, NJ. Company HQ will be relocating to Elmwood Park, NJ office shortly. Remote work is not available.
For more information on our company, visit www.ELLKAY.com.
Interested applicants should submit a letter of interest with salary requirements and resume.
ELLKAY LLC is a Smoke-Free Workplace.
AA/EOE."
132,Data Engineer,"New York, NY",New York,NY,None Found,None Found,"
PhD in Computer Science, Mathematics, or related technical field
Familiarity with open source cloud and application platforms, AWS development experience
Experience with big data technologies such as Spark, Hive, Presto and Impala.
Experience working with MPP databases such as Redshift, Snowflake, Vertica and Netezza
Hands-on experience working in SOA and high throughput environments
",None Found,"
Code in a variety of languages, primarily Python, Java and/or C++
Design and implement data pipelines, building scalable and optimized enterprise level data systems
Work cross functionally with Product, Ops and Engineering counterparts
Participation and collaboration from inception to deployment
",None Found,"
MS in Computer Science, Math, related technical field or equivalent practical experience
4+ years of general software programming experience in Java, C/C++, Python and SQL
Large systems software design and development experience, with knowledge of Unix/Linux
Knowledge of database technology, schema design, and query optimization techniques
Solid foundation in data structures, algorithms and software design with strong analytical and debugging skills
","Beeswax is looking for a Data Engineer to join our growing team. We were recently recognized on the Inc. 5000 list as #46 in the fastest growing companies and #5 in the top software companies. In 2018, we were also named by Business Insider as the ""fastest growing company in AdTech""

Beeswax is a high scale, high availability digital advertising platform founded by executives from Google and funded by leading VCs including RRE and Foundry Group. We aim to offer the most extensible and transparent advertising system, servicing technology enabled clients and executing and processing billions of events every day.

The Beeswax engineering team is a top-notch group with backgrounds from Google, Amazon, Oracle and other premier technical teams. Because digital advertising operates at an extremely high scale (millions of transactions per second) and low latency, the opportunity to learn about web-scale distributed systems and hard scaling problems is a great advantage of our engineering culture and work.

We are looking for a Data Engineer to build clean pipelines and maintain data products that our customers rely on.

Our products are built on a variety of technologies including C++, Java, Python and LEMP stack (PHP, MySQL, nginx), and while our engineers typically specialize in one area, they need to coordinate and integrate with a wide variety of systems, including homegrown and AWS-native services.

Responsibilities:

Code in a variety of languages, primarily Python, Java and/or C++
Design and implement data pipelines, building scalable and optimized enterprise level data systems
Work cross functionally with Product, Ops and Engineering counterparts
Participation and collaboration from inception to deployment

Requirements:

MS in Computer Science, Math, related technical field or equivalent practical experience
4+ years of general software programming experience in Java, C/C++, Python and SQL
Large systems software design and development experience, with knowledge of Unix/Linux
Knowledge of database technology, schema design, and query optimization techniques
Solid foundation in data structures, algorithms and software design with strong analytical and debugging skills

Preferred Qualifications:

PhD in Computer Science, Mathematics, or related technical field
Familiarity with open source cloud and application platforms, AWS development experience
Experience with big data technologies such as Spark, Hive, Presto and Impala.
Experience working with MPP databases such as Redshift, Snowflake, Vertica and Netezza
Hands-on experience working in SOA and high throughput environments

About You:

You are passionate about learning, mentoring and building a world class team and culture, while constantly empowering others around you
You take a second to step back and look at the big picture before diving in head first
You care about the quality of the data flowing through your code as much as about the quality of the code
Not afraid to take risks, voice opinions or ideas that help build the next generation of data platforms all within a massive distributed system
You're the type to peel back the layers and use non-conventional means to solve the task at hand.

"
133,Finance Data Engineer - Finance Platforms & Data,"Jersey City, NJ 07302",Jersey City,NJ,07302,None Found,"Strong programming experience in at least one compiled language (e.g. C, C++, Java)In-depth knowledge of relational and columnar SQL databases, including database designExperience with continuous delivery and deploymentProficient at working with large and complex code basesComfortable working in highly dynamic and rapid development environment (Agile development experience)Technologies: Web/RESTful service development: HTML 5, JavaScript/AngularJS, JSONTechnologies: Linux and shell scripting, TDD (JUnit), build tools (Maven/Gradle/Ant), Scala, Spark, Tableau",None Found,None Found,None Found,None Found,"MORE ABOUT THIS JOB
In Finance Engineering, you’ll find an exciting confluence of computer science, finance and mathematics being used to solve for what our shareholders would like from us – a high return for the right risk taken.
Our Data, Platform, Infrastructure and Risk engineers work with multiple Finance businesses to drive consistency, efficiency and reuse across Finance & Risk Engineering solutions via the delivery of common services, tools, frameworks, and practices.
With increasing complexity and volumes in Finance, we continuously need to scale our data. Cutting across all areas of Finance, our Data Engineering team is designing our common datastores in the Data Lake. We are a dynamic team of talented junior and senior developers, technical architects, and functional analysts who work in concert to deliver high profile projects using OO technologies.
RESPONSIBILITIES AND QUALIFICATIONS
HOW YOU WILL FULFILL YOUR POTENTIAL
Work in a dynamic, fast-paced environment that provides exposure to all areas of FinanceBuild strong relationships with business partnersUnderstand business needs, facilitating and developing process workflow, data requirements, and specifications required to support implementationDevelop technical specifications, high level/detailed design, testing strategies, and implementation plans from business requirementsManage end-to-end systems development cycle from requirements analysis, coding, testing, UAT and maintenance

SKILLS AND EXPERIENCE WE ARE LOOKING FOR
Bachelors degree in Computer Science, Mathematics, Electrical Engineering or related technical disciplineExperience in software development, including a clear understanding of data structures, algorithms, software design and core programming conceptsComfortable multi-tasking, managing multiple stakeholders and working as part of a teamExcellent communication skills including experience speaking to technical and business audiences and working globallyExpertise in Java development & Relational DatabasesCan apply an entrepreneurial approach and passion to problem solving and product developmentStrong problem solving and analytical skills

Preferred Qualifications
Strong programming experience in at least one compiled language (e.g. C, C++, Java)In-depth knowledge of relational and columnar SQL databases, including database designExperience with continuous delivery and deploymentProficient at working with large and complex code basesComfortable working in highly dynamic and rapid development environment (Agile development experience)Technologies: Web/RESTful service development: HTML 5, JavaScript/AngularJS, JSONTechnologies: Linux and shell scripting, TDD (JUnit), build tools (Maven/Gradle/Ant), Scala, Spark, Tableau
ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

Â© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet."
134,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,"
AWS Certified DevOps Engineer; AWS Certified Solutions Architect; AWS Certified Big Data
Azure or Google Certified Professional Data Engineer
Master's or Ph.D degree
","
Enable machine learning and data analysis
Model data and metadata to support dashboards, ad-hoc, and pre-built reporting
Adopt best practices in reporting and analysis: data integrity, analysis, validation, and documentation
Ability to engage clients and lead relevant data discussions","
Provide technical design leadership with the responsibility to ensure the efficient use of resources, the selection of appropriate technology, and use of appropriate design methodologies
Work closely with business/product stakeholders in understanding requirements and translating them to engineering requirements
Support and enhance data architecture, data instrumentation, define database schema, create ETL pipelining, generate reports/insights, a guide algorithm design
Define and evangelize data warehouse fundamentals and best practices
Work across the organization in optimizing data capture (parameters, metadata, etc.)
Work in DevOps to bring up new data systems and supporting existing data services
Evaluate SaaS solutions (BI, pipelining, etc.) and make build/buy recommendations",None Found,"
Bachelor’s degree in Computer Science, Engineering, Math, Physics, or equivalent work experience
5+ years of software development experience
Proficient in SQL, NoSQL databases, and GNU Linux
Experience building secure, concurrent, distributed server applications
Experience in data science, analytics, or big data solutions [Hadoop, Spark, AWS, Python, etc.]
Experience with Scala, MongoDB, Cassandra, PostgreSQL, Docker, Kubernetes
Ability to work collaboratively with a distributed team or remotely with clients","Team: Engineering
From collecting and processing terabytes of data a day to improving and building new data assets, tools, and pipelines, we build data driven teams and foster a data-informed culture. Our ideal candidate thrives in a fast-paced consulting environment, relishes working with large transactional volumes and big data, enjoys the challenge of highly complex business problems and, above all else, is passionate about data and analytics. A successful candidate knows and loves working with technical tools, is comfortable accessing and working with data from multiple sources, and partners with the client to identify strategic opportunities and deliver results.
As a Senior Data Engineer at SFL Scientific you will enable data-driven decision making by collecting, transforming, and visualizing data. You will design, build, maintain, and troubleshoot data processing systems with a particular emphasis on the security, reliability, fault- tolerance, scalability, fidelity, and efficiency of such systems. The Senior Data Engineer also analyzes data to gain insight into business outcomes, builds statistical models to support decision-making, and creates machine learning models to automate and simplify key business processes. You will be responsible for designing and implementing solutions using third-party technologies (e.g., different cloud providers) and SFL solutions.

Responsibilities:
Provide technical design leadership with the responsibility to ensure the efficient use of resources, the selection of appropriate technology, and use of appropriate design methodologies
Work closely with business/product stakeholders in understanding requirements and translating them to engineering requirements
Support and enhance data architecture, data instrumentation, define database schema, create ETL pipelining, generate reports/insights, a guide algorithm design
Define and evangelize data warehouse fundamentals and best practices
Work across the organization in optimizing data capture (parameters, metadata, etc.)
Work in DevOps to bring up new data systems and supporting existing data services
Evaluate SaaS solutions (BI, pipelining, etc.) and make build/buy recommendations
General Requirements:
Bachelor’s degree in Computer Science, Engineering, Math, Physics, or equivalent work experience
5+ years of software development experience
Proficient in SQL, NoSQL databases, and GNU Linux
Experience building secure, concurrent, distributed server applications
Experience in data science, analytics, or big data solutions [Hadoop, Spark, AWS, Python, etc.]
Experience with Scala, MongoDB, Cassandra, PostgreSQL, Docker, Kubernetes
Ability to work collaboratively with a distributed team or remotely with clients
Other Skills/Experience:
Enable machine learning and data analysis
Model data and metadata to support dashboards, ad-hoc, and pre-built reporting
Adopt best practices in reporting and analysis: data integrity, analysis, validation, and documentation
Ability to engage clients and lead relevant data discussions
Preferred Qualifications:
AWS Certified DevOps Engineer; AWS Certified Solutions Architect; AWS Certified Big Data
Azure or Google Certified Professional Data Engineer
Master's or Ph.D degree

You must be authorized to work in the US. We support flexible work hours and paid professional development.
Please send cover letter, CV or resume to careers@sflscientific.com or apply below."
135,Staff Data Engineer - Application Engineering,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Slack is looking for an Staff level Data Engineer to help design and scale our batch and streaming data pipelines to help our engineers, operations team, product managers, and analysts make better decisions with data.Our backend data infrastructure is built on Kafka, Hadoop, Hive, Presto, Spark, and MySQL. We are looking for engineers that understand that simplicity and reliability are aspects of a system that can’t be tacked on but are carefully calculated with every decision made. This position is based in New York and supports our Enterprise team.

Slack has a positive, diverse, and supportive culture—we look for people who are curious, inventive, and work to be a little better every single day. In our work together we aim to be smart, humble, hardworking and, above all, collaborative. If this sounds like a good fit for you, why not say hello?

What you will be doing

Work with the backend engineering team to design systems that reliably transport and process billions of events per day
Help our analysts and engineers build and optimize data pipelines to understand our large and ever-growing user base
Whiteboard a fix to a scaling problem - and then make it happen
Help our skilled support team triage bugs
Provide support to the operations staff in troubleshooting and solving production issues
Build tools that help us make better decisions
What you should have

You have a curiosity about how things work.
You’ve built large-scale data pipelines professionally and can craft clean and beautiful code in Java, Scala, and/or SQL.
You have hands-on experience running, upgrading, and debugging mission-critical streaming pipelines using Kafka and have strong opinions about the lambda and kappa architectures.
You’ve built batch data pipelines with Hadoop/Spark as well as with relational database engines, and you understand their respective strengths and weaknesses.
You can jump into situations with few guardrails and make things better.
You possess strong computer science fundamentals: data structures, algorithms, programming languages, distributed systems, and information retrieval.
You are a strong communicator. Explaining complex technical concepts to product managers, support, and other engineers is no problem for you.
You know how to build reliable and safe distributed systems and understand the tradeoffs made when engineering a feature.
When things break — and they will — you are eager and able to help fix things.
You are someone that others enjoy working with due to your technical competence and positive attitude.
Here are a few extra things that would lift you up a couple of notches in our eyes:
Academic background in computer science or mathematics (BSc or MSc)
Experience with SOLR and Redis
Experience building simple scripts and web applications using Python, Ruby, or PHP.
A solid grasp of basic statistics (regression, hypothesis testing)
Experience in small start-up environments

Slack is a layer of the business technology stack that brings together people, data, and applications – a single place where people can effectively work together, find important information, and access hundreds of thousands of critical applications and services to do their best work. From global Fortune 100 companies to corner markets, businesses and teams of all kinds use Slack to bring the right people together with all the right information. Slack is headquartered in San Francisco, CA and has ten offices around the world. For more information on how Slack makes teams better connected, visit slack.com.
Ensuring a diverse and inclusive workplace where we learn from each other is core to Slack’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and a pleasant and supportive place to work.
Come do the best work of your life here at Slack."
136,Senior Data Scientist,"New York, NY 10016",New York,NY,10016,None Found,None Found,None Found,None Found,None Found,None Found,"Description:
Use statistical principals to evaluate scenarios and make predictions on future outcomes in order to solve highly complex business problems and support decision making. Analyze large data sets in real time databases. Develop and implement mathematical approaches and solve complex business problems. Participate in the development, validation and delivery of algorithms, statistical models and reporting tools. Convey the solution of complex numerical problem to technical and non-technical business partners. Develop and / or use algorithms and statistical predictive models and determine analytical approaches and modeling techniques to evaluate scenarios and potential future outcomes. Perform analyses of structured and unstructured data to solve multiple and / or complex business problems utilizing advanced statistical techniques and mathematical analyses and broad knowledge of the organization and / or industry. Collaborate with business partners to understand the nature of their business and recommend an appropriate statistical approach to deal with business problems and measurements. Develop and participate in presentation and consultations to existing and prospective constituents on analysis results and solutions. Interact with internal and external peers and managers to exchange complex information related to areas of specialization. Use strong knowledge in algorithms and predictive models to investigate problems, detect patterns and recommend proactive solutions. Use strong programming skills to explore, examine and interpret large volume of data in various forms.
64920

Background Experience:
Master’s degree in Statistics, Mathematics or related

Minimum 1 year of experience with data analytics/engineering in the healthcare industry, either as a Data Engineer or Data Analyst. Must have experience using advanced analytics tools and languages to extract, transform and analyze large data sets from multiple data sources; extracting/manipulating large datasets; and experience handling large datasets for data processing, analytics and reporting purposes. Must also have experience with the following tools/technologies: Hadoop; Hive; Spark; Python; and SQL.

Potential Telework Position:
No

Percent of Travel Required:
N/A

EEO Statement:
Aetna is an Equal Opportunity, Affirmative Action Employer

Benefit Eligibility:
Benefit eligibility may vary by position. Click here to review the benefits associated with this position.

Candidate Privacy Information:
Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately."
137,Data Engineer,"New York, NY",New York,NY,None Found,None Found," BS or MS in management information systems, computer science, or a related field.",None Found," Support the design, build and execution of post source system extraction and data lake ingestion and business transformation, CARR creation framework and development processes in production.",None Found,None Found,"Data Engineer - (19001847)
Description

Position Summary
Our Data Engineer will be responsible for assisting with the implementation and maintenance of the enterprise wide data management solution for certified analytics and reporting rectangles (CARRs) and data assets. This includes the analysis, design, development, testing, implementation, and initial maintenance of the solution and acting as interface with SME/Product owners and enterprise data analytics team, by taking a holistic approach for delivering reusable CARRs and data assets enabling data solutions.
Your Responsibilities
 Support the design, build and execution of post source system extraction and data lake ingestion and business transformation, CARR creation framework and development processes in production.
 Enhance analytic environments and platform required for structured, semi-structured and unstructured data.
 Develop data quality metrics that identify gaps and ensure compliance with enterprise wide standards.
 Provide technical support for projects and team members, along with SMEs and product owners.
 Build data pipelines to feed descriptive and predictive analytics use cases, KPI and enterprise wide reporting.
 Interface with architects, product managers/SMEs and product analysts to understand data needs and support the implementation of the business rules into transformation.
 Document the data blending process along with the specifications and workflow/data lineage.
 Maintain/add to existing data dictionaries and work with UI team for creating the profiling platform and data profiling for already created CARRs.
 Execute project-based data governance processes.
 Work with our team in India
Reporting Relationships
As our Data Engineer, you will report to our Data Scientist who reports to our Vice President, Predictive Analytics.


Qualifications

Your Qualifications
 BS or MS in management information systems, computer science, or a related field.
 4-6 years of experience in data management.
 Programming capability in Python, SQL/NoSQL and Unix/shell scripting, as well as in data engineering, databases (e.g., SQL, MongoDB), platform architecture and ETL concepts.
 Amazon Web Services experience.
 Excellent communication and collaboration skills to work across multiple groups within the organization.
 Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operation.
 Experience working with offshore colleagues
Location
 This position can be located in our newest offices in Hudson Yards in Manhattan, in our Holmdel NJ offices (the historic former Bell Labs facility) or in our Gold LEED facility in Bethlehem PA with work from home flexibility.
Travel
 Less than 10% travel between NYC, Holmdel NJ, Bethlehem PA.
Benefits
 Medical, dental, vision, and prescription plans
 Competitive compensation package
 401k with company match
 Attractive Paid Time Off (PTO)
 Life and disability insurance
 Tuition assistance program
 Wellness discounts
Philanthropic Opportunities
 Social responsibility is part of our mission. It stems from our corporate values of putting people first, doing the right thing, and holding ourselves to high standards.
 As a company and as employees, we are engaged in a variety of initiatives such as volunteering within our local communities, educational alliances with colleges, focusing on sustainability, and promoting diversity and inclusion.
Guardian has been helping people protect their futures and secure their lives for more than 150 years.
Every day, we serve approximately 27 million people through a range of insurance and financial products. We help people and their families pursue financial security and well-being in life, health, and wealth. We help companies take care of their employees. And we help people recover and thrive after unexpected loss.
From our founding in 1860, when a community of immigrants joined together to insure and protect their businesses and families, doing the right thing for our policyholders and customers has guided everything we do. Our dedication to customers has helped us remain one of the most highly rated in client satisfaction and financial strength.
And as one of the largest mutual insurance companies, we know what matters most: putting the needs of our customers first.
Because everyone deserves a Guardian.
Learn more about Guardian at www.GuardianLife.com.
© Copyright 2019 The Guardian Life Insurance Company of America, New York, NY Guardian2019

Primary Location: United States-New York-New York
Other Locations: United States-New Jersey-Holmdel, United States-New York, United States-Pennsylvania, United States-Pennsylvania-Bethlehem, United States-New Jersey
Job: Information Technology
Schedule: Full-time
Shift: Day Job
Job Type: Standard
Travel: Yes, 10 % of the Time
Job Posting: Sep 23, 2019, 9:44:21 AM"
138,Data Engineer Optimus Health Analytics,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description
Bachelor’s Degree in Computer Science or Engineering, with a focus in data science and analytics.

Job duties include: designing, modeling, and implementing data warehousing systems according to customer requirements, as well as programming and configuring warehouses of database information and providing support to warehouse users, among others. Specific duties will consist of:
Developing and building data (ETL) pipelines, analysis tools, and core data processing components in a cloud-based data processing environment.
Creating and maintaining pipelines for data which function according to the dataflow architecture on a regular basis.
Working with the data and analytics partners to create and build enterprise level dataflow architecture.
Creating technical guidance for data warehouses in support of the project management team when defining the scope of data integration projects.
Developing and implementing data extraction procedures from other systems, working with project managers and data partners to regularly ingest Electronic Medical Records (EMR) and then storing them in the required data warehouse and formatting for the analytics partners.
Remaining current on emerging open source data processing projects and tools.
Job Type: Full-time
You have requested that Indeed ask candidates the following questions:
Have you completed the following level of education: Bachelor's?
Are you in New York, NY?
Are you authorized to work in the following country: United States?
Are you willing to undergo a background check, in accordance with local law/regulations?


Send Your Updated CV To info@optimusha.com"
139,Big Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,"Implementation including loading from disparate data sets, preprocessing using Hive and Pig.
Manage the technical communication between the team and client
Work with big data team to deliver cutting edge solutions
",None Found,None Found,"Responsibilities:
Implementation including loading from disparate data sets, preprocessing using Hive and Pig.
Manage the technical communication between the team and client
Work with big data team to deliver cutting edge solutions
Qualification:
2-5 years of demonstrable experience designing technological solutions to complex data problems, developing & testing modular, reusable, efficient and scalable code to implement those solutions.
Ideally, this would include work on the following technologies:
Expert-level proficiency in at-least one of R, C++ or Python (preferred). Scala knowledge a strong advantage.
Strong understanding and experience in distributed computing frameworks, particularly Apache Hadoop 2.0 (YARN; MR & HDFS) and associated technologies - one or more of Hive, Sqoop, Avro, Flume, Oozie, Zookeeper, etc..
Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage.
Operating knowledge of cloud computing platforms (AWS, especially EMR, EC2, S3, SWF services and the AWS CLI)
Experience working within a Linux computing environment, and use of command line tools including knowledge of shell/Python scripting for automating common tasks
Ability to work in a team in an agile setting, familiarity with JIRA and clear understanding of how Git works
In addition, the ideal candidate would have great problem-solving skills, and the ability & confidence to hack their way out of tight corners.
Education:
B.E/B.Tech in Computer Science or related technical degree

xNc0S6Y38a"
140,AWS Redshift & Matillion,"New York, NY 10017",New York,NY,10017,None Found,None Found,None Found,"
Work as part of a team to develop ETL and ELT code in Matillion OR Python, Spark and PySpark for real-time data streaming
Participate in the development of cloud data warehouses and business intelligence solutions
Data wrangling of heterogeneous data and explore and discover new insights
Hands-on experience with new data platforms and programming languages (e.g. Python, Matillion, Hive, Spark)
",None Found,None Found,"AWS Redshift & Matillion - NYC
Position: AWS Redshift & Matillion - NYC
Location: New York, United States
Remuneration: $ 95.00 per hour
Who is hiring?
As a Data Engineer at this company, you’ll work in small teams to deliver innovative solutions using core cloud data warehouse tools and Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you’ll be working with some of the most forward-thinking organizations in data and analytics.

What will you be doing?

Responsibilities:

Work as part of a team to develop ETL and ELT code in Matillion OR Python, Spark and PySpark for real-time data streaming
Participate in the development of cloud data warehouses and business intelligence solutions
Data wrangling of heterogeneous data and explore and discover new insights
Hands-on experience with new data platforms and programming languages (e.g. Python, Matillion, Hive, Spark)

Qualifications:
Related work experience in Data Engineering or Data Warehousing
Proven experience with data warehousing, data ingestion, and data profiling
Experience within an AWS Redshift environment and the ability to integrate data into Redshift or Snowflake.
Proficient in Matillion OR Python and Spark
Strong aptitude for learning new technologies and analytics techniques
Highly self-motivated and able to work independently as well as in a team environment
Understanding of agile project approaches and methodologies
Optimizing the performance of business-critical queries and dealing with ETL job-related issues
Building and migrating the complex ETL pipelines from Talend to redshift
Extracting and combining data from various heterogeneous data sources
Experience using Apache Spark
Familiarity with streaming data ingestion
Consulting experience
Bachelor’s degree in Computer Science or a closely related field required

Why you shouldn’t miss this opportunity?
Consulting Opportunities all over the world, Onsite and Remote flexibility
Data Science(Data Engineer), Python, Spark, Amazon Redshift"
141,Data Engineer,"New York, NY",New York,NY,None Found,None Found,"
Expertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
",None Found,None Found,None Found,None Found,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Expertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
142,Data Analytics Director,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The AlphaWise Primary Research team collaborates closely with Morgan Stanley fundamental analysts, strategists, and economists on key investment debates. Building on a decade of experience, the AlphaWise Primary Research team gathers alternative data to generate unique analytics and insights. The group is seeking a seasoned Data Analytics Director to provide technical expertise and strategic leadership to the team.• Own delivery of web data analytics for investment research, driving the strategy, tools and workflows for crawl/scrape analysis, data management and insights generation• Partner with research analysts and larger AlphaWise team to deliver projects• Develop relationship with analysts, proactively identifying where data may add value to the investment thesis, brainstorming creative approaches for tactical explorations and showcasing new data sources and uses cases• Acquire high quality data by identifying potential data vendors, conducting pilot evaluations and presenting the investment business case to management• Design and implement with the team automated data quality checks and processes• Collaborate with IT and Research data management team to build infrastructure and processes for scalable and efficient data analytics support• Potential to grow into a regional or functional head, overseeing the strategy, operations, staff, resourcing and metricsTechnical Qualifications• Experience in running large scale web scrapes and domain knowledge of best practices• Experience in modern programming languages (Python, Perl, JavaScript) and statistical packages such as R or SAS• Experience with using data profiling tools to query the data, identify anomalies, gaps and issues• Experience configuring technical architecture to support data analysis activities• Experience in working with data in various forms (data warehouses/SQL, unstructured data environments/PIG,HIVE, Impala)• Experience applying data science techniques and statistics to develop data driven solutions in a business context is a plus• Strong understanding of how data is utilized in an equity research investment processes to generate insights and ideas• Understanding of data management practices, emerging trends, and issues

Bachelor’s Degree from an accredited higher learning institution required or equivalent combination of experience and education (preferred area of study: Economics, Quantitative Analysis, Business, Mathematics, Computer Science or other similar fields)• 8 years experience as a data analyst, data scientist, data engineer or related quantitative role with experience managing junior and/or off-shore personnel• 3 years experience in financial markets• Proven track record of answering business questions using a quantitative, data-driven, analytical approach• Experience in effectively leading project teams with a mix of data analytics professionals, alongside providing technical expertise• Comfortable working in a large organization with a broad set of stakeholders across various functions• Excellent communication of technical concepts to expert & non-expert audiences• Excellent analytical and reasoning skills; able to decompose complex problems and projects into manageable pieces; comfortable suggesting and presenting solution

"
143,"Data Engineer, Analytics, Intern","New York, NY 10017",New York,NY,10017,None Found,None Found,None Found,None Found,None Found,None Found,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.
Would you like to work with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. You will work with some of the brightest minds in the industry, and you'll have the opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match. This is an internship position based in Menlo Park, Seattle, or New York.
RESPONSIBILITIES
Architect, implement and deploy new data models and data processes in production.
Perform data analysis to generate business insights.
Interface with Engineers, Product Managers and Product Analysts to understand product goals and data needs.
Build data expertise and own data quality for allocated areas of ownership.
Manage data warehouse plans for a product or a group of products.
Support critical data processes running in production.
MINIMUM QUALIFICATIONS
Currently has, or is in the process of obtaining, a Bachelors, Masters or PhD degree in Computer Science, Mathematics, or related technical field
Programming expertise in a language of your choice
Knowledge of SQL
Knowledge of database systems
Must obtain work authorization in country of employment at the time of hire, and maintain ongoing work authorization during employment
PREFERRED QUALIFICATIONS
Intent to return to degree-program after the completion of the internship
Curious, self-driven, analytical and excited to play with data
Ability to thrive in a fast paced work environment
Facebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com."
144,Distinguished Data Engineer,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"114 5th Ave (22114), United States of America, New York, New York

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Distinguished Data Engineer

At Capital One, we believe in the values of Excellence and Doing the Right Thing. We are a technology-oriented company delivering financial products to market through modern technology and constant innovation at a massive scale.

Distinguished Engineers are...

Deep technical experts, senior-level individual contributors and thought leaders that help accelerate adoption of the very best engineering practices, while maintaining knowledge on industry innovations, trends and practices

Visionaries, helping solve Capital One’s toughest technology challenges, to deliver on business needs that directly impact the lives of millions of our customers and associates

Role models and mentors, helping to coach and strengthen the technical expertise and know-how of our engineering and product community

Supporters, both internally and externally, helping to elevate the Distinguished Engineering community and establish themselves as a go-to resource on given technologies and technology-enabled capabilities

Leaders who gain the trust and confidence of those around them, from hands on engineers to executives

Whether a member of our engineering or architecture teams, Distinguished Engineers are individual contributors expected to solve problems in a fast-paced, collaborative, and iterative delivery environment. In order to meet these demands, candidates should be influential engineering leaders with deep technology expertise, and a collaborative style that brings others into the decision-making process. S/He will significantly impact the Tech agenda within their organization and devise clear roadmaps to deliver next generation technology solutions across organizational boundaries.

Responsibilities:
Leverage sound judgment and problem solving to tackle some of Capital One’s most critical problems and connect the dots to broader implications of the work

Build awareness, increase knowledge and drive adoption of modern technologies and architecture patterns, sharing customer and engineering benefits to gain buy-in

Strike the right balance between lending expertise and providing an inclusive environment where others’ ideas can be heard and championed; leverage expertise to grow skills in the broader Capital One team

Promote a culture of engineering excellence and being well-managed, using opportunities to reuse and innersource solutions where possible

Effectively communicate with and influence key stakeholders across the enterprise, at all levels of the organization

Operate as a trusted advisor for a specific technology, platform or capability domain, helping to shape use cases and implementation in an integrated manner

Lead the way in creating next-generation talent for Tech, mentoring internal talent and actively recruiting external talent to bolster Capital One’s Tech talent

Integrate the modern core with key financial systems

Build a powerful set of APIs and event streams that create highly distributed, massively scalable “virtual core” allowing vital financial systems to easily interact with modern and legacy ledgers

Operate in a cutting edge cloud-native, microservices-driven architecture

Basic Qualifications:
Bachelor’s Degree

At least 10 years’ of enterprise software engineering experience

At least 5 years' of data engineering experience

Preferred Qualifications:
Masters’ Degree or higher in computer science or a related field

7+ years of building large scale data systems

5+ years of building web APIs for high-throughput systems

5+ years of writing back-end systems in open source languages such as Java and Go

5+ years of using AWS ecosystem of tools

3+ years building large data sets in Cassandra

2+ years of experience with Kafka

2+ years of building microservices deployed as containers, preferably on Kubernetes

Capital One will consider sponsoring a new qualified applicant for employment authorization for this position."
145,Sr. Data Engineer - Power BI,"New York, NY",New York,NY,None Found,None Found,"Bachelor’s Degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience
5+ years of sound experience with data modelling, data intake and data-curation procedures as well as outstanding SQL skills, incl. advanced concepts such as distributed queries and spatial queries.
Strong hands on experience with Power BI dashboard design, development and WS modeling.
Knowledge of modern web-based procedures for data visualization
Ability to think and work in a Group-wide network, strongly service orientated, team player and highly motivated.
Strong interpersonal skills.
Proficient oral and written communication skills.
Ability to schedule and prioritize workload demands, including multi-tasking.
Experience in results monitoring and rate/product development.
4+ years of hands-on HQL, SQL, Power BI, SAS (a plus).
Insurance/reinsurance/actuarial experience highly preferred.
Experience in the commercial insurance industry (especially property and casualty) is preferred but not necessary.
2+ years of Data architecture experience with regards to duplicative or redundant metadata, data structures or processes.
Proven experience in software development projects with Java Spring, Docker, Git, Maven/Gradle and Jenkins, proven agile project management and requirements engineering skills (SCRUM, Design Thinking).
Proven experience in Data Visualization tools ( such as Power BI, SAS VA, etc.)
Experience leading projects preferred.",None Found,None Found,None Found,None Found,"We’re adding to our diverse team of experts and are looking to hire those who are committed to building a culture that enables the creation of innovative solutions for our business units and clients.

The Insurance Operations
Munich Re US is launching new insurance operations described as Munich Re Specialty Insurance that will unite the strengths and expertise of Munich Re’s specialty commercial teams in North America under a new management structure. This will allow for the development of an overarching sales strategy through which the specialty risk appetite and product offerings can be profitably optimized and expanded.

As a member of Munich Re's US operations, we offer the financial strength and stability that comes with being part of the world's preeminent insurance and reinsurance brand. Our risk experts work together to assemble the right mix of products and services to help our clients stay competitive – from traditional reinsurance coverages, to niche and specialty reinsurance and insurance products.

The Opportunity
Future focused and always one step ahead

The chosen candidate will independently provide data and business analytics support by working closely with Analytics, Underwriting, Actuarial, IT, Operations and Claims teams. This support includes expert business knowledge on the extraction, interpretation, architecting, modeling, transforming, and preparation of both quantitative and qualitative data as well as the creation and maintenance of actuarial and analytics databases, process automation and developing end-user tools to support data-driven decision making.

Major accountabilities include:
Supervise the maintenance and enhancement work of key strategic dashboards
Perform extraction, cleaning, transforming, manipulation and loading of various data files formats, databases and cloud/web sources.
Develop complex Power BI design and DAX code
Work with databases containing data from multiple sources, at various granular levels, to deliver data in good quality for use in analytics and visualization projects.
Make data architecture design decision for data dashboard/report and assist to develop data dashboard with visualization capabilities.
Lead complex data requirement gathering phases of dashboard and analytics projects.
Work with business and IT to define demands on the infrastructure of the MR Data Lakes on the basis of Hadoop, Hive, Power BI Workspace and SAS on an ongoing basis.
Work with IT and other stakeholders to prepare, maintain and quality assure internal/external data sources.
Qualifications
Bachelor’s Degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience
5+ years of sound experience with data modelling, data intake and data-curation procedures as well as outstanding SQL skills, incl. advanced concepts such as distributed queries and spatial queries.
Strong hands on experience with Power BI dashboard design, development and WS modeling.
Knowledge of modern web-based procedures for data visualization
Ability to think and work in a Group-wide network, strongly service orientated, team player and highly motivated.
Strong interpersonal skills.
Proficient oral and written communication skills.
Ability to schedule and prioritize workload demands, including multi-tasking.
Experience in results monitoring and rate/product development.
4+ years of hands-on HQL, SQL, Power BI, SAS (a plus).
Insurance/reinsurance/actuarial experience highly preferred.
Experience in the commercial insurance industry (especially property and casualty) is preferred but not necessary.
2+ years of Data architecture experience with regards to duplicative or redundant metadata, data structures or processes.
Proven experience in software development projects with Java Spring, Docker, Git, Maven/Gradle and Jenkins, proven agile project management and requirements engineering skills (SCRUM, Design Thinking).
Proven experience in Data Visualization tools ( such as Power BI, SAS VA, etc.)
Experience leading projects preferred.
Company NameMunich Re America
Requisition Number
3203BR
CountryUnited States of America
Employment Type
Full Time"
146,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Description
How will this role have an impact?
As a member of one of the infrastructure teams, our new Data Engineer will be given the freedom to make meaningful and measurable improvements impacting millions of people. This role will report to our VP, Technology Security & Infrastructure.

What will you do?
Query data to include aggregations, calculations, and producing metrics from data.
Create data pipelines to include ETL and streaming data such as log data or tool/sensor data to indexes. Experience with Splunk / Sumo forwarders, ELK (Elasticsearch, Kafka, Logstash), or ES/Splunk python libraries preferred.
Provide novice level expertise with hands on configuration, tuning and operating of distributed data storage stacks. Open source tools including Kafka, Logstash, Beats, Elasticsearch, Kibana, or Splunk / Sumo preferred.
Script languages. Python preferred
Design and implement data visualizations. Experience with Kibana or Sumo preferred.
We are looking for someone with:
Computer science, engineering, information science or a related technical discipline plus 1+ years of relevant experience or Master’s degree
Experience working on an Agile development team
Experience with code repositories, esp Git/GitHub
Experience with ALM tools, esp Jira
Experience with Elastic Common Schema or Splunk / Sumo Common Information Model
Experience with tools in both Linux and Windows environments
Understand current cyber exploits, attack methodology, and detection techniques using a wide variety of security products.
Cloud workload experience (Amazon Web Services, Azure)
Understanding of machine learning, and it’s use in anomaly detection
Experience evaluating new methodologies and technologies to meet requirements and deliver capabilities
Our benefits include but are not limited to:
Paid Time Off
401(k) Savings Plan with match
Medical, Vision & Dental
Pre-Tax Commuter Benefits
Parental Leave
Gym Reimbursement
Tuition Reimbursement"
147,Manager of Data Engineering,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"114 5th Ave (22114), United States of America, New York, New York

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Manager of Data Engineering

Being Capital One Tech:
At Capital One, we consider ourselves the bank a technology company would build. We’re delivering best-in-class innovation so that our 65 million customers - and counting - can manage their finances with ease. Our reality and vision empower our engineers to use artificial intelligence and machine learning to transform real-time data, software, and algorithms into financial clarity.

We’re all-in on the cloud and a leader in the adoption of open source, RESTful APIs, microservices, and containers. We build our own products and release them with a speed and agility that allows us to get new customer experiences to market quickly. We’re going boldly where no bank has gone before. And, as a founder-led company, we’re inspired and empowered to make, break, do, and do good . So, let’s do something great together.

Your #LifeatCapitalOne

Looking to work somewhere with the flexibility of a start-up but the financial muscle of a Top-10 bank? You’re in the right place! And here’s what that means for you…

You'll have a flexible work schedule—we want to understand where and when you're at your best so you have a healthy work-life balance. Diversity and Inclusion are cultural norms here—you’ll have access to active local chapters of Women in Tech, Blacks in Tech, and Hispanics in Tech and more. Plus, you’ll be given time to support the next generation of technologists by volunteering with youth programs like Capital One Coders - our engineer-led experience that teaches middle school students in underserved communities how to code. Want to learn more? See what our associates are up to at #LifeatCapitalOne !

Calling All Engineering Managers:
A hub for innovation, our New York presence is expanding, and we need Engineering Managers who know their stuff to join our team. As a Capital One Data Engineer , you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One. You’ll work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems. You’ll collaborate with digital product managers, and deliver robust cloud-based solutions to drive powerful experiences that help millions of Americans achieve financial empowerment. Want to learn more? Check out the low-down on our high-tech

Who You Are:
You are fun to work with – you’re excited by a team environment

You are curious. You like to learn new technologies , and you adapt well to change

You are passionate about current state-of-the-art software technologies and tools, with experience implementing them effectively

You are excited about working with cloud-native stack, building on AWS using technologies like Kubernetes and Serverless

You possess a sense of intellectual curiosity and a burning desire to learn

You are motivated and actively looking for ways to contribute

You are passionately focused on the customer and the details that make their experience exceptional

You value data and truth over ego

You possess a strong sense of engineering craftsmanship, take pride in your code

You’re pragmatic - you make the best use of time and resources to find the simplest workable solution

You think and act like an owner, taking personal responsibility for both team and product success

You possess great communication and reasoning skills, including the ability to influence and make a strong case for technology choices

You thrive in collaborative agile teams and are ready to take on new and unexpected challenges while building the next wave of engineering solutions

What You’ll Own:
Collaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies

Leading the craftsmanship, security, availability, resilience, and scalability of your solutions

Bringing a passion to stay on top of current trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community

Encouraging innovation, implementation of cutting-edge technologies, outside-of-the-box thinking, teamwork, and self-organization

Assisting in the hiring of top engineering talent and maintaining our commitment to diversity and inclusion

Basic Qualifications:
Bachelor’s Degree
3+ years in coding i n coding in data management, data warehousing or unstructured data environments
At least 3 years experience with big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)

Preferred Qualifications:
Master's Degree
3+ years with Agile engineering practices
3+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)
3+ years experience with NoSQL implementation (Mongo, Cassandra)
3+ years experience developing Java based software solutions
3+ years experience in at least one scripting language (Python, Perl, JavaScript, Shell)
3+ years experience developing software solutions to solve complex business problems
3+ years experience with Relational Database Systems and SQL
3+ years experience designing, developing, and implementing ETL
3+ years experience with UNIX/Linux including basic commands and shell scripting
Familiarity with data science tools and concepts

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
148,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,"
8+ years of software development experience focused on web technologies including significant production work with Python
3+ years of experience designing, building and maintaining enterprise data pipelines and/or warehouses
High level knowledge of machine learning algorithms and how ML models are built and deployed
Demonstrable knowledge of big data databases such as columnar data stores (e.g. Cassandra or BigTable) or Hadoop as well as SQL (MySQL, MSSQL or PostgreSQL) and ability to select the right tool for the job
Experience with queued work management and message processing (e.g., Kafka, RabbitMQ)
Experience working closely with product and account support personnel to help prioritize the best solutions to the largest problems.
Reliable organization and communication skills and follow through on verbal and written commitments.
Persistent approach to problem-solving and ability to see solutions through to completion even in the face of complexities or unknowns. A proactive mindset that drives you to pursue solutions rather than waiting for the answers to come to you.
Attention to detail in work and ability to identify ambiguities in specifications.
Exceptional written and verbal communication skills, especially when communicating trade-offs between technical decisions to non-technical colleagues.
Flexibility to work and maintain focus in an evolving environment. Ability to let go of previous projects and move on to new ones or to dig deeper into existing projects and grow them depending on the business needs.
","At Urbint, our mission is to make communities more resilient. We do this by pairing external data with artificial intelligence to identify areas of high risk and prevent catastrophic loss for utilities and infrastructure operators across the country. We are a team of close-knit engineers, entrepreneurs, and data geeks who obsess over problem-solving, new technologies, and making a positive impact in our communities.
At Urbint, you will collaborate within a cross functional team of other software engineers, product designers, machine learning engineers and product managers to architect, create and maintain custom data pipelines, data lake(s) and a data warehouse to feed our customer facing applications and internal experimentation and modeling teams. Our data sources range from large, high security enterprise databases and storage services hosted by our customers to disparate open data from government organizations to proprietary third party aggregators and auditability and security are very important to their results. You will be working to solve direct customer needs and finding common abstractions to apply and guide the future of information at data driven organization.
Requirements
8+ years of software development experience focused on web technologies including significant production work with Python
3+ years of experience designing, building and maintaining enterprise data pipelines and/or warehouses
High level knowledge of machine learning algorithms and how ML models are built and deployed
Demonstrable knowledge of big data databases such as columnar data stores (e.g. Cassandra or BigTable) or Hadoop as well as SQL (MySQL, MSSQL or PostgreSQL) and ability to select the right tool for the job
Experience with queued work management and message processing (e.g., Kafka, RabbitMQ)
Experience working closely with product and account support personnel to help prioritize the best solutions to the largest problems.
Reliable organization and communication skills and follow through on verbal and written commitments.
Persistent approach to problem-solving and ability to see solutions through to completion even in the face of complexities or unknowns. A proactive mindset that drives you to pursue solutions rather than waiting for the answers to come to you.
Attention to detail in work and ability to identify ambiguities in specifications.
Exceptional written and verbal communication skills, especially when communicating trade-offs between technical decisions to non-technical colleagues.
Flexibility to work and maintain focus in an evolving environment. Ability to let go of previous projects and move on to new ones or to dig deeper into existing projects and grow them depending on the business needs.
Benefits
What We Offer:
Mission Driven - Some companies use AI to serve better digital ads and trade stocks, we seek to make our communities more resilient.
Top Compensation - Competitive compensation package.
Best in Class Medical Coverage - 100% benefits and premiums paid.
Prime NoHo Location - Our office sits in the heart of NYC’s historic NoHo district and is just minutes away from the BDFM, 6, and RW subway lines.
Health Perks - Gym reimbursement and Citibike membership.
Strong Culture - collaborative office focused on teamwork, humility, and hustle.
Catered lunch on Thursdays, plus a kitchen filled with snacks and drinks.
We're an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status."
149,Data Engineer,"New York, NY 10038",New York,NY,10038,None Found,None Found,None Found,None Found,None Found,None Found,"About Birchbox:
---------------

We started Birchbox in 2010 to redefine the way consumers discover and shop for beauty and grooming. The company quickly grew from an exciting idea to a business that has materially shaped the beauty industry: we've activated an enormous group of underserved, untapped consumers, awakening their relationship with beauty by making the experience relevant, easy and fun. Our innovation isn't the simple concept of delivering a box of samples - it's understanding that although not everyone is passionate about beauty and grooming, everyone still deserves to have a great experience finding, trying and buying it.

Birchbox operates in six countries, reaching more than 2.5 million active customers with a portfolio of 500 best-in-class prestige brand partners. We have retail locations within select Walgreens stores across the US as well as a flagship Birchbox store in Paris.

About the role:
---------------

Birchbox is seeking an ambitious and experienced Data Engineer to help evolve our data systems as we continue to grow. In addition to fueling our recommender system and beauty box optimization toolkit, our data services enable and support decision-making company-wide. This software engineering role will be very impactful as we build out the next generation of our data services, and will have considerable leeway in leading architectural direction.

You'll be joining a lean Agile team supporting Data Infrastructure and Machine Learning. Primary upcoming initiatives include refreshing our algorithmically-driven subscription box optimization system, iterating on our e-commerce recommender services, retooling our event data collection systems, and supporting the next generation of our A/B testing machinery. Each of these projects will involve some DevOps work and some amount of data extraction and loading, but will also involve substantial backend software development.

Responsibilities
----------------

As a Birchbox Engineer, you will:

Build fault-tolerant, scalable batch and real-time distributed data processing systems to drive our personalization and BI systems.
Design and configure hosted and cloud-based data and machine learning infrastructure (Hadoop, Spark, AWS EMR).
Implement fault tolerant data integrations between internal systems and with third-party APIs as needed, supporting product and marketing needs.
Build and support internal A/B testing tooling for the engineering and product teams.
Support the Business Intelligence team as needed, advising them on data provenance and reliability.
Design and build components of internal tooling used by our subscription operations team (Python, MILP, CP).
Tool our systems for observability, including logging, metrics monitoring, and dashboarding (Datadog).
Contribute to maintaining an open, empowering, responsible, and proactive engineering culture.

About You
---------

Ideally, we're seeking somebody with:

3+ years of professional software experience (or equivalent).
Bachelor's degree in Computer Science or related field (or equivalent experience).
Expert in Python, and comfortable with at least two other languages (e.g., Java, Ruby, PHP).
Strong command line skills for working within virtualized machines (bash, tmux / screen, vim / emacs).
Experience orchestrating data infrastructure (e.g., Pig/Hive, HBase, Spark, S3, Kafka/Kinesis, Redshift).
Experience with modern workflow management systems (e.g., Airflow, Luigi, Azkaban).
Advanced SQL skills (MySQL and/or Postgres), familiarity with data warehousing and dimensional modeling.
Familiarity with non-relational data stores and/or indexes (e.g., MongoDB, HDFS, Cassandra, Solr, ElasticSearch).
Experience working on teams using distributed version control (e.g., Git, Mercurial).
Insatiable curiosity, a commitment to precision and excellence, and a desire to work collaboratively within and between teams.

Bonus Points
------------

We'll definitely want to have a conversation if:

You're committed to enabling DevOps culture with development, deployment, and observability / monitoring tooling.
You've read more IETF RFCs than you'd admit in polite company.
You're opinionated about when and how to leverage different algorithms, data manipulation techniques, and frameworks.
You're passionate about idiomatic code style, readability, and when to use which software design patterns.

At Birchbox you will...

-----------------------


Take your ideas to the next level right away. We experiment, iterate, learn, and repeat.
Build things people love. Our goal is to surprise and delight our customers as much as possible. Great design, simple user experience, and access to data to make smart decision help us to achieve it.
Collaborate with purpose. You'll work in small groups with other talented thinkers and figure out how to make Birchbox's software even better.
Work with people who care. We're a group of talented professionals who pride ourselves on what we do. We're smart, innovative, energetic, and lots of fun.

Tech @ Birchbox
---------------

The technology team at Birchbox spans software engineering, technical operations, product, and business intelligence. We are responsible for developing the company's customer-facing sites in six countries, managing hosted and cloud infrastructure, and closely supporting other teams (logistics, marketing, et al.) in our 230+-person global company. Our service-oriented platform is built on a wide variety of open-source technologies: AWS, Apache Mesos, Marathon, and Docker; Salt, Jenkins, and Shippable for automation and CI/CD; Apache Spark and Hadoop ecosystem tools; Ruby on Rails, Java, Python, and PHP backing our React-based customer sites; Datadog, Kibana, and PagerDuty for observability and alerting. We rely on engineers to be self-motivated and quickly follow through on tasks without requiring close supervision. In return, engineers are given substantial freedom to use their own initiative and make their own decisions.

Our challenges include:

Evolving our software and systems architecture to support a rapidly growing customer base across multiple countries and languages.
Designing and implementing the best user experience for our customers. We strive to revolutionize online retail.
Using data, complex algorithms, and statistics, to personalize the Birchbox experience for our customers, both offline and online.

As an engineer at Birchbox you:

Are self-sufficient: you've learned how to navigate projects you're unfamiliar with.
Exercise wisdom: you're able to identify opportunities to apply lessons learned, and do so with confidence and care.
Take humble initiative: you're always on the lookout to improve technology and processes, but understand how and when to best present this to your peers.
Cultivate passion: not only do you continue to improve your craft, you share your discoveries with your colleagues.
Are given empowering responsibility: you're trusted with tools and processes to implement your vision, and you create this chance for others.
Document kindly: your code and processes have documentation that will help those who come after you, regardless of skill level.
Collaborate eagerly: you understand that helping others isn't a distraction, but a vital part of what we do.

"
150,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"CompStak, a prominent NYC based data startup, focuses on gathering unique datasets via crowdsourcing and providing unprecedented insights to Commercial Real Estate industry via state of the art data-intensive applications. We are expanding our tech team and looking for talented Senior Data Engineers to help us bring our unique data products to the market. What you will work on will challenge the status quo and change our industry forever.

You will be working in the following environment:

Scala/Haskell based public and private APIs
Machine learning systems for processing and analyzing crowdsourced and public data
Sophisticated search tech, based on Elasticsearch
Automated DevOps via SaltStack, Terraform, Nomad, Consul and Vault
Polyglot development environment
Data-driven product offerings
And much more

What can you do?
We are looking for senior data engineers with 5+ years of experience who have a proven track record of delivering high-quality, maintainable data-driven software. Your experience should include:

Designing and implementing distributed data systems
Kafka/Kafka Streams/Kafka Connectors
Apache Spark + Scala
Cloud computing with AWS (S3, Lambda, DynamoDB, EMR ...)
Relational and NoSQL data stores
Domain Driven Design
Microservice architecture
Elasticsearch is a big plus
Spark MLib is a big plus
Event sourcing is a plus

Who are you?
At CompStak we provide a fantastic environment for your continuous personal and professional growth, and in return, we hope that:

You love data-driven products
Your professional mission is to develop rock-solid systems and write clean and maintainable code
You are focused on solving problems via well designed composable systems
You are interested in working in a lean startup environment
You are always eager to learn no matter the depth of your experience
You are a kind person who has an easy time collaborating with other people
You are happy to mentor less experienced gifted engineers
You have a BS or MS degree in Computer Science
You have experience with Scala, and you count Scalaz, Cats, Shapeless among your favorite libraries
You have experience in other functional languages like Haskell or OCaml
You are interested in Machine Learning and NLP

Who are the people of CompStak?
CompStak was born out of our dreams to revolutionize Commercial Real Estate and deliver information and insight out of fragmented and sparse data. We hail from 500 Startups, one of the most prestigious Silicon Valley incubators. Our goals and potential are lofty, and that is why we are backed by one of the most acclaimed VC firms – Canaan Partners. When you join our family, you will be joining a smart and collaborative team that is currently disrupting an antiquated industry.

What will you be paid?

Highly competitive compensation: salary + bonus + equity
Karma points for doing great things in life.

An Equal Opportunity Employer ----- M/F/D/V"
151,Java/Big Data Engineer,"New York, NY 10022",New York,NY,10022,None Found,None Found,None Found,None Found,None Found,None Found,"Java/Big Data Engineer
United States-NY-New York | Full-time (FT) | Technology [IT] | Job ID 137514
Credit Suisse is a leading global wealth manager with strong investment banking capabilities. Headquartered in Zurich, Switzerland, we have a global reach with operations in about 50 countries and employ more than 45,000 people from over 150 different nations. Embodying entrepreneurial spirit, Credit Suisse delivers holistic financial solutions to our clients, including innovative products and specially tailored advice. Striving for quality and excellence in our work, we recognize and reward extraordinary performance among our employees, provide wide-ranging training and development opportunities, and benefit from a diverse range of perspectives to create value for our clients, shareholders and communities. We are Credit Suisse.
We Offer

This is a great opportunity to join the Big Data and Analytics team, which is building out a new platform within Group Operations. We are global team spread across New York, Raleigh, London, Singapore, and Pune. This role requires hands on development, technical leadership, and creative problem solving across the various projects related to the Big Data environment. This role also involves collaborating with the L1 / L2 support organization to maintain production stability when needed.

You will be responsible for development, technical thought leadership, and delivery of features and capabilities for the Data & Analytics Platform. You will be expected to:
Develop against and support Hadoop Infrastructure using shell scripting, Python, Control-M
Implement processes including loading data sets and preprocessing using Hive / Impala
Write queries and scripts to query against and transform data in a Hadoop environment
Have experience with streaming technology such as Kafka
Develop applications with using technical stack / tools such as Java, J2EE, Spring Boot, Junit, Javascript, Angular
Have experience with analytics tools and languages such as Alteryx, Tableau, R
Have Experience with non-relational & relational databases (SQL, NoSQL, Hadoop, MongoDB, etc.)
Know architectural patterns in big data and streaming data environments
Dive into existing application code base to resolve functionality
Work in an agile environment using JIRA
Have experience using DevOps tooling including GIT and Transporter
Have familiarity with cloud based platforms and tools including Docker
Have the ability to design solutions independently based on high-level architecture
Effectively communicate with project managers, business analysts and users, negotiate and lead technical meetings as well as influence outcomes
Be up to date in the ever changing world of big data and analytics

Credit Suisse maintains a Working Flexibility Policy, subject to the terms as set forth in the Credit Suisse United States Employment Handbook.
You Offer

Bachelor’s Degree in Computer Science or Engineering with equivalent experience
You have minimum 7-9 years of IT experience. Investment Banking Operations IT background ideal
You have 5 to 7 years of Financial Industry/banking experience
You have extensive expertise in big data tools and technologies including Hadoop, HDFS, Hive, Impala, SQL, noSQL databases
Have extensive experience with Unix/Linux scripting, Python, Control-M
Have experience with distributed technologies: JAVA, J2EE, AngularJS, Spring, JavaScript and other related Web technologies
You have sound knowledge of version control & debugging tools
You have experience in methodologies like Agile/Scrum, Test Driven Development (Junit) and real passion for automating Integration Testing
You have deep knowledge of Continuous Integration and Continuous Testing toolsets such as Jira/Confluence/Stash/Jenkins or TeamCity
Experience with cloud based platforms and tools such as Docker
You possess analytical and problem solving skills and are hardworking & committed.
You have hands on development experience and an ability to deliver high quality programs with a strong focus on delivery
You are able to lead and drive delivery with best practices and standards.
You have excellent organizational, problem solving, leadership, written and verbal communication skills"
152,Software Engineer/Data Engineer,"New York, NY 10017",New York,NY,10017,None Found,"
Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.
Familiarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured (Postgres, MySQL) and unstructured data sources (Elastic, Kafka, and the Hadoop ecosystem) as implemented at Internet-scale.
Experience writing and optimizing streaming and batch analytics.
Experience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.
BS/BA in Computer Science, Engineering, or relevant field experience.",None Found,None Found,None Found,None Found,"Ideally, the successful candidate will be located near our NYC or College Park, MD office. However, there is the opportunity to work remotely based on role and level.
Software Engineer/Data Engineer
BlueVoyant is seeking a Software Engineer/Data Engineer to help us build a data analytics platform powerful enough to protect some of the world's biggest networks, and nimble enough to adapt to a quickly evolving product vision. We are solving interesting, exciting, and important problems with smart people.
Qualifications for the Software Engineer/Data Engineer:
Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.
Familiarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured (Postgres, MySQL) and unstructured data sources (Elastic, Kafka, and the Hadoop ecosystem) as implemented at Internet-scale.
Experience writing and optimizing streaming and batch analytics.
Experience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.
BS/BA in Computer Science, Engineering, or relevant field experience.
What you will do as a Software Engineer/Data Engineer:
Work closely with analysts to transform threat analytics into production-level code.
Actively contribute to application architecture and product vision.
Participate in requirements gathering and transformation from prototype to product design.
Participate in daily development stand-up meetings and regular sprint planning and product demo meetings.
Help us stay current on the latest data processing tools and trends.
Ideal candidates will:
Thrive in our small, fast-paced, product-driven environment
Collaborate with teams from across the organization
Deliver features and fixes on tight schedules and under pressure
Present ideas in business-friendly and user-friendly language
Create systems that are maintainable, flexible and scalable
Define and follow a disciplined development and engineering workflow
Demonstrate ownership of tasks with escalation as needed
Be a subject matter expert in one or more of the technologies employed
Relentlessly push for successful customer outcomes
Possess a strong interest or background in cyber security
General responsibilities include:
Participate in all stages of an agile software development lifecycle, including product ideation, requirements gathering, architecture, design, implementation, testing, documentation, and support
Refine our software development methodology based on agile/lean practices with continuous feedback and well-defined metrics to drive improvement
Maintain up-to-date knowledge of technology standards, industry trends, emerging technologies, and software development best practices
Ensure technical issues are quickly resolved and help implement strategies and solutions to reduce the likelihood of reoccurrence
Identify competitive offerings and opportunities for innovation including assessments of risk/reward to the company.
About BlueVoyant
BlueVoyant is a global cybersecurity firm that provides Advanced Threat Intelligence, for large companies and a comprehensive Managed Security Service and Professional Services for small businesses, powered by one of the largest commercially available cyber threat databases in the world.
By working with BlueVoyant, companies can gain unique and far-reaching visibility into malicious activity on their networks, in the dark web and across the internet, as well as real-time, automatable remediation services. Through our unique real-time external threat monitoring, predictive human and machine-sourced intelligence, and proactive managed security and incident response, BlueVoyant offers the private sector exceptional cyber defense capabilities.
Co-founded by CEO Jim Rosenthal, former Chief Operating Officer at Morgan Stanley, and Executive Chairman Tom Glocer, former Chief Executive Officer at Thomson Reuters, BlueVoyant has attracted a management team that comes from the world's preeminent intelligence, law enforcement, and private sector organizations. Other leaders include:
Jim Penrose, COO, former EVP at Darktrace with 17 years at the NSA in key leadership roles.
Gad Goldstein, Head of BlueVoyant Europe and Chairman of Israel, former division head (Major General equivalent) in the Israel Security Agency, Shin Bet.
Robert Hannigan, Chairman of BlueVoyant International, former Director of GCHQ.
Austin Berglas, Global Head of Professional Services, former head of the FBI's New York Cyber Branch.
David Etue, Global Head of MSS, former VP of Managed Services at Rapid7.
Milan Patel, Chief Client Officer, former CTO of the FBI Cyber Division.
Ron Feler, Global Head of Threat Intelligence and Operations, former Deputy Commander of Unit 8200, the cybersecurity division of the Israel Defense Forces.
Eldad Chai, CPO, former SVP of Products at Imperva.
Jim Bieda, Senior Advisor, former NSA Deputy CTO.
Bill Crumm, Senior Advisor, former NSA SIGINT Director and former Cybersecurity Head, Morgan Stanley.
Dan Ennis, Senior Advisor, former Head of Threat Intelligence at the NSA
All employees must be authorized to work in the United States or Israel. BlueVoyant provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, BlueVoyant complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.
Come work with us!
BlueVoyant is hiring software developers, infrastructure engineers, data science experts, and technologists of all types to build next generation predictive threat intelligence and advanced security monitoring solutions.
Projects currently in development include:
An Internet-scale (multi-PB, > 500k TPS) repository made up of unstructured, structured, and semi-structured data sources used for real time alerting, threat analysis, and research and development internally.
A powerful, enterprise-scale suite of products used to provide managed security services and Security Operations Center (SOC) functionality to small and medium sized enterprises around the globe.
A unique internal platform to support the data research needs of our analysts and SOC so they can quickly and effectively identify new threat actors and techniques.
fZzYqhYXzr"
153,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,"
Bachelor's degree (CS, EE or Math preferred) or equivalent work experience as well as interest in a fast paced, complex environment.
5+ years of experience Scala preferred in a commercial environment
Expert in Spark, experience with the Hadoop ecosystem and similar frameworks
Expert in SQL
Familiarity with various tools such as AWS and Docker and an instinct for automation
Strong understanding of Software Architecture principles and patterns.
Experience working with 3rd party software and libraries, including open source
Experience with Postgres
",None Found,None Found,None Found,"About Us

The SecurityScorecard ratings platform helps enterprises across the globe manage the cyber security posture of their vendors. Our SaaS products have created a new category of enterprise software and our culture has helped us be recognized as one of the 10 hottest SaaS startups in NY for two years in a row. Our investors include both Sequoia and Google Ventures. We are scaling quickly but are ever mindful of our people and products as we grow.

Position Summary

The Senior Data Analytics Engineer will build meaningful analytics that inform companies of security risk. You will be working closely with our Data Science team, implementing algorithms and managing the analytic pipeline. We have over 1 PB of data, so the ideal candidate will have experience processing and querying large amounts of data.

This role requires senior level experience with Spark, SQL and Scala. Our interview process will include live coding using these technologies.

Responsibilities


Manage the analytic pipeline using Spark, Hadoop, etc
Leverage cutting-edge technologies to support new and existing and services and processes.
Quickly and efficiently design and implement in an agile environment
Work with other team members to implement consistent architecture
Drive projects through all stages of development
Actively share knowledge and responsibility with other team members and teams
Improve the effective output of the engineering team by managing quality, and identifying inconsistencies.

Skills and Experience:

Bachelor's degree (CS, EE or Math preferred) or equivalent work experience as well as interest in a fast paced, complex environment.
5+ years of experience Scala preferred in a commercial environment
Expert in Spark, experience with the Hadoop ecosystem and similar frameworks
Expert in SQL
Familiarity with various tools such as AWS and Docker and an instinct for automation
Strong understanding of Software Architecture principles and patterns.
Experience working with 3rd party software and libraries, including open source
Experience with Postgres

Traits:

Quick-thinker who takes ownership and pride in their work
A commitment and drive for excellence and continual improvement
A strong sense of adventure, excitement and enthusiasm.
Excellent systems analytical, problem solving and interpersonal skills

Interview Process:

Initial Conversation with a SecurityScorecard Talent team to learn more about your experience and career objectives
Technical Interview with 1- 2 data engineers. This will include live coding in SQL, Spark, Scala.
Coding Exercise - take home exercise
Final Interview: Meet 1-2 engineering leaders

SecurityScorecard embraces diversity. We believe that our team is strengthened through hiring and retaining employees with diverse backgrounds, skillsets, ideas, and perspectives. We make hiring decisions based upon merit and do not discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status."
154,Principal Data Engineer,"New York, NY 10003",New York,NY,10003,None Found,None Found,"
Eat, sleep, breathe data/databases (SQL (MySQL, etc.), NoSQL/BigData(Cassandra, etc.)
Better delivery than Dominos (architectural expertise, CI/CD experience, PjM skills)
More renown architecturally than Frank Lloyd Wright
Knows clouds (AWS) better than the sky
Plays well with others","
Understand and align data architecture strategy to the business and technology strategy
Partner with architecture and development teams to evolve software products to exceed the needs and expectations of the consumer
Deliver technology products that yield immediate business value
Collaborate as part of a high-performing team and strengthen the NML tech community as a whole
Understand development costs and resourcing, compliance, security, and risk",None Found,None Found,"At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference.
Synopsis:
The ideal candidate is an experienced data pipeline builder and data wrangler who is experienced at optimizing data platforms and building them from the ground up. The Principal Data Engineer will support developers, architects, analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout projects.

The candidate must be self-directed and comfortable supporting the needs of multiple teams, systems, and products simultaneously. The right candidate will be excited by the prospect of optimizing and re-designing NM’s data architecture to support our next generation of products and data initiatives.

Responsibilities:
Understand and align data architecture strategy to the business and technology strategy
Partner with architecture and development teams to evolve software products to exceed the needs and expectations of the consumer
Deliver technology products that yield immediate business value
Collaborate as part of a high-performing team and strengthen the NML tech community as a whole
Understand development costs and resourcing, compliance, security, and risk

Skills:
Eat, sleep, breathe data/databases (SQL (MySQL, etc.), NoSQL/BigData(Cassandra, etc.)
Better delivery than Dominos (architectural expertise, CI/CD experience, PjM skills)
More renown architecturally than Frank Lloyd Wright
Knows clouds (AWS) better than the sky
Plays well with others

What you’ll do every day:
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, NoSQL, and AWS technologies.
Create and maintain optimal data pipeline architectures
Assemble large, complex data sets that meet functional / non-functional business requirements.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Assure NML’s data compliance policies are strictly enforced throughout NML IPS data platforms.
Create data pipelines for the analytics and data scientist team members’ and assist them in building and optimizing our products into an innovative industry leader.
Be a thought leader on strategies and best practices with respect to data architecture, data integration, and data quality
Keep abreast of industry trends & standards
Communicate effectively with technical & non-technical resources
Train and mentor teammates and other (x-dept) Data Engineers

What kind of person should you be?
You pride yourself on your demonstrated secure data architecture and engineering skills
You have incredible and proven problem-solving, and analytical skills
You have the ability to quickly learn unfamiliar platforms and frameworks
You can handle the most stressful situations calmly
You often suggest ideas that can improve operations
You're self-motivated
Your ability to debug & triage complex problems is outstanding
You have the ability to assess maintainability & long-term impacts of design choices
Work well independently or as part of a team
Yourself. Integrity is paramount

What skills do you need?
Bachelor’s degree in Computer Science or Engineering, or equivalent experience
8+ years of work experience with data architecture/engineering experience
8+ years of demonstrated expertise with relational and non-relational database technology
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Experience building and optimizing data pipelines, architectures, and data sets. “Big Data” experience preferred
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Familiar with stream-processing systems: Storm, Spark-Streaming, etc.
Familiar with data pipeline and workflow management tools
Professional CSP Experience (AWS preferred) with services such as RDS, Glue, etc
OSWAP-minded, experienced with static and dynamic code analytical tools (Blackduck, etc)
Excellent written and verbal communications skills

Grow your career with a best-in-class company that puts our client’s interests at the center of all we do. Get started now!
We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.

Req ID: 25650
Position Type: Regular Full Time
Education Experience: Bachelor's Required
Employment Experience: 9+ years
Licenses/Certifications:
FLSA Status: Exempt
Posting Date: 07/15/2019"
155,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"HelloFresh delivers ""cook from scratch"" meal plans straight to your door, with easy to follow recipe cards and high quality, pre-portioned fresh ingredients—saving time and money that you can instead spend with your loved ones!

HelloFresh is the world's leading meal kit company and is expanding rapidly. Founded in November 2011, HelloFresh now operates in the U.S., the United Kingdom, Germany, the Netherlands, Belgium, Luxembourg, Australia, Austria, Switzerland, and Canada. HelloFresh delivers millions of meals to millions of customers across the globe. and listed in November 2017 at the German Stock Exchange in Frankfurt.

Data Science Department:
Embedded in the NYC Tech Hub, we are building a cross-functional team of data scientists, analysts, and engineers with the mission to bring the modeling capabilities of our marketing organization to the next level. At HelloFresh we believe that automation and Machine Learning can take our marketing efforts to the next level by marrying the right customer with the proper advertisement. We will accomplish this using technology on a large scale instead of using manual processes. We are actively investing in the right people, data, and infrastructure to support this vision.

JOB DESCRIPTION:
We are hiring a Data Engineer to work alongside marketing. Our vision is to build an automated platform directing our marketing initiatives on delivering the right food box at the desired price point to the front door of all our customers.

We are currently developing 2 major projects:
1.Optimization of audiences and targeting across our foodtech platforms

2. A data-driven and ROI based, campaign testing machine

Responsibilities


Design and build a world-class self-service data platform
Taking end-to-end responsibility for design, build and maintenance of batch and real-time data pipelines.
Understand and bring solutions for complex business problems, enabling insights that can empower better decision-making
Work closely with data scientist in bringing their models in production

Basic Requirements


Degree in Computer Science, Software Engineering, or equivalent experience
3 + years' related experience
Advanced Python skills
Experience with Apache Spark
Experience with Amazon AWS, DevOps and Automation
The ability to design, implement and deliver maintainable and high-quality code
Experience with data modeling, design patterns, building highly scalable and secured solutions
Experience with RDBMS such as PostgreSQL or MySQL
Experience with job orchestration tools like Airflow, Luigi or similar
Experience with end-to-end testing of data pipelines
Knowledge of visualization tools (Tableau, Grafana)
Experience with Data Quality checks and tools

You'll get


Competitive Salary & 401k company match that vests immediately upon participation
Generous parental leave of 16 weeks & PTO policy
$0 monthly premium and other flexible health plans
75% discount on your subscription to HelloFresh (as well as other product initiatives)
Snacks, cold brew on tap & monthly catered lunches
Company sponsored outings & Employee Resource Groups
Collaborative, dynamic work environment within a fast-paced, mission-driven company

It is the policy of HelloFresh not to discriminate against any employee or applicant for employment because of race, color, religion, sex, sexual orientation, gender identity, national origin, age, marital status, genetic information, disability or because he or she is a protected veteran."
156,Data Engineer (Python),"New York, NY 10003",New York,NY,10003,None Found,None Found,None Found,None Found,None Found,None Found,"What we're looking for

We are looking for a motivated data engineer who is passionate about building reliable and scalable data pipelines to make data sets available for analysis. The ideal candidate is entrepreneurial, motivated to grow, and has a passion for Python development.

Our Engineering Values
----------------------


Collaboration: We believe that engineers do their best work when working together in cohesive teams.
Excellence: We believe in doing things the ""right way"" rather than the ""fast way"", and holding ourselves to a high standard of excellence.
Growth: We believe engineers do their best work when they are constantly growing, learning, and changing.
Communication: We believe in combining empathy with openness and honesty to set clear expectations and hold each other accountable.
Impact: We believe we're making the world a better place by empowering marketers to really help their customers rather than just sell stuff.

What you'll be doing
--------------------


Developing and enhancing multiple ETL pipelines
Designing and implementing data storage structures and ETL pipelines, keeping long-term impacts in mind
Assemble large, complex data sets that meet functional / non-functional business requirements.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Identifying and improving upon current internal processes through automation and optimization

------

Who you are:
------------


Bachelor's degree in Computer Science, Mathematics, or related field/equivalent experience
3+ years of experience with Python, Java, Scala, R, Go or similar language
3+ years of experience in working with cloud computing technology (AWS, Google Cloud Platform, etc.)
3+ years experience working on data warehouse systems such as Snowflake
Have developed systems based on key principles (consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms)
Comfortable using version control and working in a collaborative environment
Experienced with CI/CD tools for testing and deployment
Self-starter with the ability to work independently or in a team
Able to manage one's schedule and prioritize tasks independently

Why You Should Join
-------------------

At Conductor, we are looking for engaged and passionate engineers that can raise the bar. There is a tremendous opportunity here to have immediate impact in the day-to-day and affect the company's success and growth. We all share in the same values and push each other to meet these standards.

We are made up of a diverse group of people from all backgrounds and include a team of exceptional engineers in Kyiv as well. Wherever you are from, you will find a common ground here for continuing to push forward your career and make a difference in this industry.

Conductor, Inc. is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.

About Conductor
---------------

Conductor's search and content intelligence platform helps marketers create and optimize content to improve visibility online.

The technology generates customer intent insights that lead to compelling content, increased traffic, and higher organic marketing ROI. Customizable dashboards and workflows guide marketers through the content creation process, empowering them to measure, refine, and demonstrate the effectiveness of their SEO and content marketing efforts.

In addition to its SaaS platform, Conductor offers a suite of services and support including site audits, site migrations, and managed services that empower in-house marketing teams and digital marketing agencies to drive results and put their customers' needs first.

Conductor's forward-thinking customers include global and emerging enterprise brands like Citibank, Salesforce, ClassPass, and WeWork."
157,GS Data Scientist,"New York, NY 10282",New York,NY,10282,None Found,"
Bachelor’s Degree in a field like computer science, statistics, economics or applied math.
Strong knowledge of statistical analysis and exploratory data analysis techniques for prescriptive analytics.
Professional experience in an advance analytics-based data science/ machine learning engineer or stats role.
Professional experience working in cloud environment like AWS.
Strong knowledge of machine learning concepts like regression, classification, clustering, heuristics, boosting, feature engineering, etc.
Strong Proficiency in SQL, notebooks and Python.
Understanding of open source libraries for solving orchestration, model deployment, model explainability and management.
Basic understanding of new deep learning techniques like GANs, RNNs, CNNs and platforms like keras/tensorflow used to implement these.",None Found,"Build machine learning and NLP models to build new product features and improve business metrics.
Perform exploratory data analysis and causation/attribution/correlation identifying key insights to help decisions and business strategy.
Document model explainability and decay and deploy models with proper governance.
Work with product managers, engineers, marketers and designers, and senior executives to optimize key translate business insights into decisions and action
Build and improve our machine learning deployment and data pipeline platforms.",None Found,None Found,"MORE ABOUT THIS JOB
What We Do
At Goldman Sachs, our Engineers don’t just make things – we make things possible. Change the world by connecting people and capital with ideas. Solve the most challenging and pressing engineering problems for our clients. Join our engineering teams that build massively scalable software and systems, architect low latency infrastructure solutions, proactively guard against cyber threats, and leverage machine learning alongside financial engineering to continuously turn data into action. Create new businesses, transform finance, and explore a world of opportunity at the speed of markets.

Engineering, which is comprised of our Technology Division and global strategists groups, is at the critical center of our business, and our dynamic environment requires innovative strategic thinking and immediate, real solutions. Want to push the limit of digital possibilities? Start here.

Who We Look For
Motivated Data Engineer with proven industry experience in machine learning, data engineering and building robust data pipelines. Strong computer science fundamentals are key to success in this role. The ideal candidate should be an individual contributor, a positive team player and willing to get things done.
RESPONSIBILITIES AND QUALIFICATIONS
Responsibilities:
Build machine learning and NLP models to build new product features and improve business metrics.
Perform exploratory data analysis and causation/attribution/correlation identifying key insights to help decisions and business strategy.
Document model explainability and decay and deploy models with proper governance.
Work with product managers, engineers, marketers and designers, and senior executives to optimize key translate business insights into decisions and action
Build and improve our machine learning deployment and data pipeline platforms.

Qualifications
Bachelor’s Degree in a field like computer science, statistics, economics or applied math.
Strong knowledge of statistical analysis and exploratory data analysis techniques for prescriptive analytics.
Professional experience in an advance analytics-based data science/ machine learning engineer or stats role.
Professional experience working in cloud environment like AWS.
Strong knowledge of machine learning concepts like regression, classification, clustering, heuristics, boosting, feature engineering, etc.
Strong Proficiency in SQL, notebooks and Python.
Understanding of open source libraries for solving orchestration, model deployment, model explainability and management.
Basic understanding of new deep learning techniques like GANs, RNNs, CNNs and platforms like keras/tensorflow used to implement these.

Why Goldman Sachs
Unparalleled responsibility and career opportunity to make a highly visible global impact
You will be part of a large community of like-minded technologists in a flat organization with a culture that promotes collaboration, “can do” mindsets and teamwork
Responsibility for requirements gathering, analysis, design and development of funding systems
Forge strong relationships with our clients across all of our businesses, as well as other technology teams to develop and enhance our systems and processes
Develop solutions that directly impact the bottom line by enabling new investment opportunities for our business partners
Opportunity to work on unprecedented projects both at Goldman Sachs & more broadly in the financial services industry
ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet."
158,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,"
Design and development of ETL and data pipeline solutions for complex business problems to load Data Warehouse
Data Stewardship - own or support the data definitions and lineage across our organization.
Create a data integration plan and build data integrations between systems.
Figure out the best way to share information and build the tech needed to execute.
Mentoring - help teach other team members about data architecture and also be a consultant for developers who need help with data.
",None Found,"
At least 3 years of relevant experience.
Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments like RDS, MySQL, Python, Pyspark, Airtable, Talend.
Experience developing, deploying, and testing in AWS
","When was the last time you were planning a business trip and really tried to save your company money? If your company allowed you to stay in a fancy hotel, would you ever volunteer to stay at an Airbnb or at a friend's house? How about flying coach instead of business class? The vast majority of employees optimize for comfort and convenience, spending at the high end of their company policy limits, because, well, why not? So how can a company get its employees to care about expenses without implementing draconian policies, creating friction and frustrating employees? How can a company motivate its employees to save?

The answer is Rocketrip. We're a NYC-based startup that rewards business travelers for cost-sensitive behavior. It's a win-win: companies save, while employees cash in with real rewards.

The Role:
---------

We are seeking a Data Engineer who can operate within our engineering organization and help take our data strategy to the next level. The Data Engineer will be able to design, code and provide architecture solutions for the team, including but not limited to ETL, data warehousing, and data integration. The right candidate for this role is someone who is passionate about technology and interacting with product owners, thrives in ambiguity, and is focused on delivering exceptional results with great teamwork skills in a scrappy, startup environment. The candidate will have the opportunity to influence and interact with fellow engineers beyond their team.

Responsibilities:
-----------------


Design and development of ETL and data pipeline solutions for complex business problems to load Data Warehouse
Data Stewardship - own or support the data definitions and lineage across our organization.
Create a data integration plan and build data integrations between systems.
Figure out the best way to share information and build the tech needed to execute.
Mentoring - help teach other team members about data architecture and also be a consultant for developers who need help with data.

Requirements:
-------------


At least 3 years of relevant experience.
Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments like RDS, MySQL, Python, Pyspark, Airtable, Talend.
Experience developing, deploying, and testing in AWS

Here at Rocketrip, we...
------------------------


Are in growth mode where all work has impact.
Offer great benefits, including medical, dental and optical.
Give all employees free membership to One Medical.
Provide access to a 401k plan and offer matching.
Believe it's important to rejuvenate and offer a ""take what you need"" vacation policy.
Encourage employees to spend the holidays exploring, relaxing, or with loved ones by closing our offices during the last week of December
Regularly huddle up as a company to share goals, learnings and celebrate!
Have a dog-friendly office.
Provide access to gym membership and Citibike discounts.

Founded in 2013 and headquartered in New York City, Rocketrip is aiming to revolutionize business travel by introducing the motivation to save. We're a group of tech innovators who looked at the current state of business travel, became frustrated by the antiquated employee and employer experiences, and decided to do something big about it. Our team is focused on utilizing technology, design and data to align employee and company interests.

Rocketrip is backed by a renowned set of investors and advisors, including Google Ventures, Bessemer Venture Partners, Canaan Partners, Genacast Ventures, and Y Combinator."
159,"Senior Data Engineer, Refinitiv Labs - NYC","New York, NY",New York,NY,None Found,None Found,"
Bachelor degree required in Computer Science, Technology, or similar field.
3 - 5 years of work experience.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience building and optimizing data pipelines, architectures and data sets.
Experience creating data profiling, cleansing and data management services
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Working knowledge of message queuing, stream processing, and highly scalable big data stores.
Strong project management and organizational skills.
Interest in Machine Learning & Analytics Operations",None Found,None Found,None Found,"
Experience with relational SQL, NoSQL, and graph databases, including DynamoDB, Redis, Postgres, Neo4J and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Athena
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, Gremlin etc. and experience building MVPs.
Experience with big data tools: Spark, Kafka, Hadoop.
Experience/working with ETL patterns and tools.
Experience with deployment and containerization, e.g., Docker and Kubernetes.","Refinitiv Labs provides a unique opportunity to work in a global company with a huge amount of high quality data. You will be:
Work collaboratively with team members and customers to identify opportunities and build next generation solutions for our customers.
Building agile data pipelines to support prototypes and MVPs
Working with large, diverse data sets using big data and public cloud technologies
Defining and implementing data engineering best practice across all Lab locations
Creating modern data infrastructure to support research, data-science and machine-learning projects.
Researching new data engineering approaches to handle alternative data
Developing and broadening your skills through mentoring and collaboration with experienced professionals

As a member of our NYC team you will be part of our global network of data and research scientists, engineers, UX/UI designers and other like-minded colleagues in our sister labs in San Francisco, London, and Singapore.
Data is the infrastructure that our Lab runs on. Refinitiv has the worlds largest repository of curated data. From petabytes of tick-by-tick market data to the worlds largest collection of financial documents, our content fuels the financial markets allowing investors to make decisions and financial institutions to reduce risk and even fight financial crime.
Requirements
Collaborate with the Data Science & Engineering teams to share best practice for building data pipelines across all locations of Refinitiv Labs
Provide expertise on development of data models and data wrangling
Create and maintain optimal data pipeline architecture
Manage internal stakeholders to obtain access and meet requirements for data security.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS, GCP, Azure and open source technologies.
Build analytics tools that monitor the data pipelines to ensure resiliency and cost efficiency.
Create data tools for analytics and data scientist team members to easily access, experiment and productionise our innovation and research projects.
Collaborate within the labs, with other teams within the company, and with our customers and partners.

Qualifications for Data Engineer
Bachelor degree required in Computer Science, Technology, or similar field.
3 - 5 years of work experience.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience building and optimizing data pipelines, architectures and data sets.
Experience creating data profiling, cleansing and data management services
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Working knowledge of message queuing, stream processing, and highly scalable big data stores.
Strong project management and organizational skills.
Interest in Machine Learning & Analytics Operations

Desirable Technical Requirements
Experience with relational SQL, NoSQL, and graph databases, including DynamoDB, Redis, Postgres, Neo4J and Cassandra.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Athena
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, Gremlin etc. and experience building MVPs.
Experience with big data tools: Spark, Kafka, Hadoop.
Experience/working with ETL patterns and tools.
Experience with deployment and containerization, e.g., Docker and Kubernetes.
The Financial and Risk Business of Thomson Reuters is now Refinitiv. Refinitiv equips the financial community with access to an open platform that uncovers opportunity and catalyzes change. With a dynamic combination of data, insights, technology, and news from Reuters, our customers can access solutions for every challenge, including a breadth of applications, tools, and content—all supported by human expertise. At Refinitiv, we facilitate the connections that propel people and organizations to find new possibilities to move forward.
As a global business, we rely on diversity of culture and thought to deliver on our goals. Therefore we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under country or local law. Refinitiv is proud to be an Equal Employment Opportunity/Affirmative Action Employer providing a drug-free workplace.

Intrigued by a challenge as large and fascinating as the world itself? Come join us.

Locations
New York-New York-United States of America"
160,"Engineer, Data","New York, NY 10001",New York,NY,10001,None Found,"
Advanced degree in relevant field of study strongly desirable, particularly in computer science or engineering level programs.
Minimum 2 years professional experience working with data extract/manipulation logic.
Minimum 2 years professional experience with object-oriented programming, functional programming, and data design.
1-3 years working with a public cloud big data ecosystem (certification in AWS a plus).
1-3 years working with MPP databases, distributed databases, and/or Hadoop.
","
Passion for data engineering, able to excite and lead by example.
Hungry and eager to learn new systems and technologies.
Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.
Ability to deliver exceptional results through iterative improvement rather than initial perfection.
Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including: business users, technical staff, senior level colleagues, vendors, and partners.
An extensive track record that demonstrates effectiveness in driving business results through data and analytics.
The ability to develop and articulate a compelling vision and generate necessary consensus.
A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.
A proven ability to influence decision making across large organizations.
A proven ability to hire, develop, and effectively lead deeply technical resources.
Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.
Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.
Create an environment where people from diverse cultures and backgrounds work together effectively.
","
Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably.
Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions.
Gather and process all types of data including raw, structured, semi-structured, and unstructured data.
Integrate with a variety of data providers ranging from marketing, web analytics, and consumer devices including IoT and Telematics.
Build and maintain dimensional data warehouses in support of business intelligence tools.
Develop data catalogs and data validations to ensure clarity and correctness of key business metrics.
Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result.
Derive an overall strategy of data management, within an established information architecture (including both structured and unstructured data), that supports the development and secure operation of existing and new information and digital services.
Plan effective data storage, security, sharing and publishing within the organization.
Ensure data quality and implement tools and frameworks for automating the identification of data quality issues.
Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Provide ongoing support, monitoring, and maintenance of deployed products.
Drive and maintain a culture of quality, innovation and experimentation.
",None Found,"
Passion for data engineering, able to excite and lead by example.
Hungry and eager to learn new systems and technologies.
Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.
Ability to deliver exceptional results through iterative improvement rather than initial perfection.
Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including: business users, technical staff, senior level colleagues, vendors, and partners.
An extensive track record that demonstrates effectiveness in driving business results through data and analytics.
The ability to develop and articulate a compelling vision and generate necessary consensus.
A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.
A proven ability to influence decision making across large organizations.
A proven ability to hire, develop, and effectively lead deeply technical resources.
Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.
Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.
Create an environment where people from diverse cultures and backgrounds work together effectively.
","Location: New York, NY

Position Summary:

The Data Engineer is responsible for building and deploying streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably. You will be responsible for ingesting and integrating large volumes of disparate data from a variety of sources, including but not limited to subscriber & listener data, customer journey data, vehicle data, video and 2nd/3rd party data. This will involve rapid innovation in large scale data pipeline design and development to ensure critical data sets are made available to our users and predictive models in a timely manner. We are looking for someone with hands on experience in all layers of the full stack involving data. The Data Engineer plays a significant role as both an enabler and practitioner of the data and analytics driven culture at SiriusXM.

Duties and Responsibilities:

Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably.
Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions.
Gather and process all types of data including raw, structured, semi-structured, and unstructured data.
Integrate with a variety of data providers ranging from marketing, web analytics, and consumer devices including IoT and Telematics.
Build and maintain dimensional data warehouses in support of business intelligence tools.
Develop data catalogs and data validations to ensure clarity and correctness of key business metrics.
Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result.
Derive an overall strategy of data management, within an established information architecture (including both structured and unstructured data), that supports the development and secure operation of existing and new information and digital services.
Plan effective data storage, security, sharing and publishing within the organization.
Ensure data quality and implement tools and frameworks for automating the identification of data quality issues.
Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.
Provide ongoing support, monitoring, and maintenance of deployed products.
Drive and maintain a culture of quality, innovation and experimentation.
Supervisory Responsibilities:

None
Minimum Qualifications:

Advanced degree in relevant field of study strongly desirable, particularly in computer science or engineering level programs.
Minimum 2 years professional experience working with data extract/manipulation logic.
Minimum 2 years professional experience with object-oriented programming, functional programming, and data design.
1-3 years working with a public cloud big data ecosystem (certification in AWS a plus).
1-3 years working with MPP databases, distributed databases, and/or Hadoop.
Requirements and General Skills:

Passion for data engineering, able to excite and lead by example.
Hungry and eager to learn new systems and technologies.
Self-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.
Ability to deliver exceptional results through iterative improvement rather than initial perfection.
Excellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including: business users, technical staff, senior level colleagues, vendors, and partners.
An extensive track record that demonstrates effectiveness in driving business results through data and analytics.
The ability to develop and articulate a compelling vision and generate necessary consensus.
A successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.
A proven ability to influence decision making across large organizations.
A proven ability to hire, develop, and effectively lead deeply technical resources.
Demonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.
Articulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.
Create an environment where people from diverse cultures and backgrounds work together effectively.
Technical Skills:

Experience deploying and running AWS-based data solutions and familiar with tools such as Cloud Formation, IAM, Athena, and Kinesis.
Experience engineering big-data solutions using technologies like EMR, S3, Spark and an in-depth understanding of data partitioning and sharding techniques.
Experience loading and querying both on premise and cloud-hosted databases such as Teradata and Redshift.
Building streaming data pipelines using Kafka, Spark, or Flink.
Familiarity with binary data serialization formats such as Parquet, Avro, and Thrift.
Experience deploying data notebook and analytic environments such as Jupyter and Databricks.
Knowledge of the Python data ecosystem using pandas and numpy.
Experience building and deploying ML pipelines: training models, feature development, regression testing.
Experience with graph-based data workflows using Apache Airflow a plus.
Knowledge of data profiling, data modeling, and data pipeline development.
Strong knowledge with high volume heterogeneous data, preferably with distributed systems.
Strong knowledge writing distributed, high-volume services in Python, Java or Scala.
Familiar with metadata management, data lineage, and principles of data governance.
Knowledge of data modeling, data access, and data storage techniques.
Appreciation of agile software processes, data-driven development, reliability, and responsible experimentation.
Minimum 2 years' experience with the following:

professional role in one or more of the following: Development, Engineering, R&D or Information Technology
Strong and thorough knowledge of the following:

ETL/ELT Tools
BI tools
MDM / Reference Data
RDBMS, NoSQL and NewSQL
MS Office Suite
SiriusXM is an equal opportunity employer that does not discriminate on the basis of sex, race, color, age, national origin, religion, creed, physical or mental disability, medical condition, marital status, sexual orientation, gender identity or expression, citizenship, pregnancy, military or veteran status or any other status protected by applicable law.

The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice."
161,Data Engineer- Free Product,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"Spotify is looking for Data Engineers to join our Free Product team. You will build infrastructure to help scale Spotify’s Free Product and Ads ranking/targeting capabilities. This is an opportunity to work with massive scale real time data and build solutions to run predictive algorithms on production.
You will build data-driven solutions to bring music and digital media experiences to our 100 million active users and millions of artists either by working directly on product features, publishing and insight tools for artists, or by improving the quality of our data tools and large scale data infrastructure. You will take on complex data-related problems using some of the most diverse datasets available — user behaviors, acoustical analysis, revenue streams, cultural and contextual data, and other signals across our broad range of mobile and connected platforms. Above all, your work will impact the way the world experiences music.

*Please note: this posting represents multiple roles across various teams, including a range of responsibilities and experience level*

What You’ll Do
Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.
Use best practices in continuous integration and delivery.
Help drive optimization, testing and tooling to improve data quality.
Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.
Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.
Who You Are
You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
You know how to write distributed, high-volume services in Java or Scala.
You are knowledgeable about data modeling, data access, and data storage techniques.
You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
You understand the value of partnership within teams.
You are willing to work from our awesome office in New York. We offer relocation packages if you do not currently live in New York."
162,Data Engineer,"New York, NY 10003",New York,NY,10003,None Found,None Found,"
You hold at least a bachelor’s degree in Computer Science or a related discipline
You have at least 3 years’ experience working with large and complex data sets
You have at least 6 years experience working with clinical data (e.g. clinical trial data, lab data, EMR data, etc.)
You are a proficient Java developer
You are a proficient SQL developer
You have experience writing ETL/ELT code
You have experience with data profiling tools and concepts",None Found,None Found,None Found,"Medidata: Conquering Diseases Together

WHAT WE'RE LOOKING FOR
Medidata’s Data and Analytics Group is seeking an engineer that is passionate about working with clinical data. We’re looking for someone who is excited by data and the value that can be found in it, and who has the skills and drive to deliver value from it. You will be a key contributor in growing and expanding our clinical data platform by making our data assets usable and accessible to platform consumers.
We work with a variety of cutting edge as well as industry standard techniques and tools. We are looking for candidates that can both support existing technologies and apply new techniques to solve the next generation of data issues that we will encounter as we extend and enlarge data platform.
WHAT YOU’LL DO
In this role, you will be responsible for data acquisition from source systems, transformation, standardization, and delivery to enterprise repositories and systems. You will work with product teams to understand business requirements. Using your understanding of clinical data, you will define the acquisition, transformation and delivery needs, will lead efforts to understand and design for production data workloads and shape, and will work with our architecture team to deliver solutions that are aligned with enterprise architecture plans.
You will deliver solutions that work within a DevOps delivery pipeline using infrastructure designs that are scalable, resilient and highly performant. You will develop mapping and testing artifacts that enable data movement solutions to be verified as meeting functional, non-functional and business requirements. Your mission will be to deliver efficient and error free data movement systems and processes.

WHO YOU ARE
Required Skills:
You hold at least a bachelor’s degree in Computer Science or a related discipline
You have at least 3 years’ experience working with large and complex data sets
You have at least 6 years experience working with clinical data (e.g. clinical trial data, lab data, EMR data, etc.)
You are a proficient Java developer
You are a proficient SQL developer
You have experience writing ETL/ELT code
You have experience with data profiling tools and concepts
Nice to Have:
Experience with build frameworks such as Maven, Gradle or Ant
Agile experience
Experience working with Message Bus technologies
Any combination of these AWS Technologies:
EC2 w/ EBS
Step Functions
Aurora
Redshift
Kinesis
EMR
ECS/ECR
Prior experience working with ETL systems (e.g. Pentaho, Talend, Informatica, etc.)
Oracle PL/SQL experience
Medidata Solutions, Inc. is an Equal Opportunity Employer. Medidata Solutions provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or status as a veteran. Medidata Solutions complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.

#LI-AS1"
163,Data Scientist - Data Engineer,"New York, NY 10176",New York,NY,10176,None Found,None Found,None Found,None Found,None Found,None Found,"Data Scientist - Data Engineer
REF#: 32836
CBS BUSINESS UNIT: Simon & Schuster
JOB TYPE: Temporary / Per Diem / Freelance
JOB SCHEDULE: Full-Time
JOB LOCATION: New York, NY
ABOUT US:
Simon & Schuster, a part of CBS Corporation, is a global leader in the field of general interest publishing, dedicated to providing the best in fiction and nonfiction for consumers of all ages, across all printed, electronic, and audio formats. Its divisions include Simon & Schuster Adult Publishing, Simon & Schuster Children’s Publishing, Simon & Schuster Audio, Simon & Schuster Digital, and international companies in Australia, Canada, India and the United Kingdom.
DESCRIPTION:
Simon & Schuster has an exciting role for a data engineer to join a fast-paced, leading-edge team working to help advance it’s publishing business. In this Freelance or Independent Contractor role, you will be working with a small team of data scientists and analysts to rapidly prototype data applications and analytics tools to serve our fast-growing imprint.
The full-time, contract position will be responsible for developing, testing and maintaining data flows and information architecture. This includes aggregating data from multiple databases in our data warehouse as well as external sources, establishing data pipelines, designing data models, deploying machine learning models to production and building robust data visualizations and reporting tools that align with the business needs of our team.
This is an exciting opportunity for the right candidate to build a robust data science and analytics environment from the ground up. You’ll be working with a small team of professionals based remotely. Hours are flexible. Two scheduled check-ins/code reviews are required weekly.
Responsibilities
Collaborate with team to align architecture with business requirements
Stand up, test and maintain databases and information architecture
Build tools to automate data acquisition and aggregation
Develop data models and table schemas
Identify ways to improve data reliability, efficiency and quality
Deploy sophisticated analytics programs, machine learning and statistical methods
Prepare data for predictive and prescriptive modeling
Use data to discover tasks that can be automated
Develop data visualizations, dashboards, reports and notification tools that respond to business needs
Design analytical models that support analysis and inference.
QUALIFICATIONS:
Skills and experience:
Computer science degree or equivalent with 2+ years data engineering and/or full stack developer experience
Background in data science or equivalent continuing education
Strong knowledge of Python 3
Experience in developing and maintaining SQL databases and writing SQL queries; NoSQL (MongoDB) experience preferred but not required
Experience establishing and maintaining AWS, Google Cloud, Heroku, Tableau or similar cloud-based environments
Knowledge of HTML, CSS and JS and lightweight web frameworks such as Django, Flask
Familiarity with database administration tools and best practices
Background in extracting and storing data from APIs preferred but not required
Background in deploying machine learning models to production preferred but not required
Knowledge of Git/GitHub and software development workflows
Strong written and verbal communication skills.
Ability to summarize key findings into clear and actionable recommendations, and to interact with individuals at all levels.
Strong project management skills and ability to plan and prioritize work in a fast-paced environment.Please send a link to your work samples on Github
EEO STATEMENT:
Equal Opportunity Employer Minorities/Women/Veterans/Disabled"
164,"Data Engineer, Marketing Science","New York, NY 10018",New York,NY,10018,None Found,None Found,None Found,None Found,None Found,None Found,"ABOUT THE ROLE:
The Data Engineer is responsible for managing the development and maintenance of databases and business intelligence tools. You must also be comfortable with acting as the subject matter expert in the tools and processes they manage, along with being able to gather knowledge from other SMEs and requirements from key stakeholders. The Data Engineer is expected to demonstrate: hands-on skills in data analysis and management, problem solving, research skills, excellent oral and written communication skills, client focus, and the ability to support and enable teamwork. You must be self-motivated and able to operate independently with limited guidance and direction. You will report to the Associate Director or Director and will provide overviews of project status, progression and roadblocks. This role requires a strong familiarity with all available analytics and technology tools, both syndicated and proprietary.

YOUR IMPACT:

Business Analysis/Project Management
Analyze data and consult with subject matter experts to design and develop business requirements for various tools and processes; Create, gather and write Business and Technical Requirements Documents
Act as the subject matter expert for questions/projects relating to data quality, BI tools, business rules; Ensure that best practices are being followed amongst all projects
Must familiarize themselves with the full campaign implementation workflow across all functional teams, including campaign set up, launch, testing, reporting and reconciliation
ETL Processes/Business Intelligence Dashboards
Manage/Build Data pipelines and automation.
Manage the investigation and resolution of potential data issues existing within business intelligence tools
Track the progress and resolution of data issues, elevating any critical issues to upper management
Supervise Senior Associates and Associates to perform data quality assurance and data processing tasks
Overall
Build positive relationships with internal teams; effectively communicate roles, responsibilities and expectations
Build positive relationships with vendors and partners

YOUR SYSTEMS & TOOLS EXPERIENCE:
Strong familiarity with relational database management systems; Experience with SQL, Microsoft Azure a plus
Familiarity with business intelligence tools such as Tableau, PowerBI, Datorama, DOMO, Looker
Experience with advertising campaign management/buying platforms such as Sizmek, DCM, Moat, DV, IAS, Mediaocean, Media Tools, Nielsen, ComScore, Facebook, Twitter, Pinterest, Snapchat, 4C, Brand Networks, YouTube, Google Ads, Bing, Kenshoo, SA360, DV360, Google Analytics, Adobe Analytics
Advanced excel data manipulation skills required; VBA/macros is a plus
Familiarity with programming languages/development tools a plus: Python, R, JavaScript, GitHub, etc.

YOUR QUALIFICATIONS:
Bachelors or advanced degree in Statistics, Economics, Business, Math, Computer Science, or Sciences is preferred
Minimum 3 years’ experience in data governance processes in a data warehousing, ETL and business intelligence environment
3 years’ experience in media; digital Investment, analytics, or ad operations is preferred
1-2 years’ experience in business analysis
Experience and understanding of processes across multiple media channels (digital, search, social, offline, etc.); Experience in handling diverse datasets (paid media, owned, etc.)
Demonstrates strong eye/attention to detail
Excellent verbal and written skills
Exceptional organizational skills, multi-tasking capabilities and detail oriented
Ability to foster collaborative relationships with other cross-functional teams
Ability to manage and prioritize competing projects and deliverables
Comfortable working with budgets and numbers

ABOUT MINDSHARE:
At Mindshare, we believe in speed, teamwork and provocation. Mindshare USA empowers people to challenge the status quo, through what we call Provocation with Purpose. We’ll encourage you to break the rules, knock things down, rebuild them better, and push the boundaries of what’s possible. It takes a team of people who invest in other’s success, and together we make Mindshare the place where we do the best work of our career. We’re a team that is radically honest – we call this our Authenticity Quotient. We are curious and competitive – that’s our Passion Quotient. We embrace teamwork and adaptability – that’s our Emotional Quotient. We’re proud to be unmistakably #TeamMindshare.

Take A Virtual Office Tour: https://roundme.com/tour/335529.

GroupM and all its affiliates embrace and celebrate diversity, inclusivity, and equal opportunity. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. We are a worldwide media agency network that represents global clients. The more inclusive we are, the greater work we can create together."
165,AWS Data Engineer,"New York, NY 10011",New York,NY,10011,None Found,"At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.","DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud",None Found,None Found," Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
§ Certified AWS Developer - Associate
§ Certified AWS DevOps – Professional (Nice to have)
§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
166,Data Engineer Northeast,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"We are:

Applied Intelligence, the people who love using data to tell a story. We’re also the world’s largest team of data scientists, data engineers, and experts in machine learning and AI. A great day for us? Solving big problems using the latest tech, serious brain power, and deep knowledge of just about every industry. We believe a mix of data, analytics, automation, and responsible AI can do almost anything—spark digital metamorphoses, widen the range of what humans can do, and breathe life into smart products and services. Want to join our crew of sharp analytical minds? Visit us here to find out more about Applied Intelligence.


You are:

An expert engineer with an eye for AI. You want to change how the world works and lives by taking AI out of the lab and into everyday life.


The work:

You’ll be part of a team with incredible end-to-end digital transformation capabilities that shares your passion for digital technology and takes pride in making a tangible difference. If you want to contribute on an incredible array of the biggest and most complex projects in the digital space, consider a career with Accenture Digital.

Here’s what you need:
Minimum 2+ years of experience in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments using Spark, pySpark, SparkSQL, with Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)
Minimum 1 year of designing and building performant data tiers (or refactoring existing ones), that supports scaled AI and Analytics, using different Cloud native data stores on AWS (Kinesis, S3. GLUE, DynamoDB etc.) or Azure (HDInsights, AzureData Factory) or GCP (DataProc, PubSub, BigQuery) as well as using NoSQL and Graph Stores.
Minimum 1 year of designing and building streaming data ingestion, analysis and processing pipelines using Kafka, Kafka Streams, Spark Streaming and similar cloud native technologies
Minimum 1-year performance engineering, profiling, debugging very large big data and ML production solutions on Spark and native Cloud technologies
Bonus points if:

Minimum 6 months of experience in implementation with Databricks.
Minimum 1 year of designing and building secured and governed Big Data ETL pipelines, using Talend or Informatica technologies; for data curation and analysis of large le production deployed solutions.
Experience implementing smart data preparation tools such as Palate, Trifacta, Tamr for enhancing analytics solutions.
Minimum 1 year of building Business Data Catalogs or Data Marketplaces for powering business analytics using technologies such as Alation, Collibra, Informatica or custom solutions


Important information

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.

Accenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration."
167,Data Engineer,"New York, NY 10017",New York,NY,10017,None Found,None Found,None Found,"
Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer
Creating data systems that ensure quality and consistency on our data platform
Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data
Playing a major role in the future architecture of our rapidly expanding backend platform
Writing high quality code, participating actively in code reviews, and consistently helping to ship software",None Found,None Found,"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By creating a single source of truth, Reonomy products empower individuals, teams and companies to share information, unlock insights and discover new opportunities.

Headquartered in New York, Reonomy has raised ~$70 million from top investors, including Sapphire Ventures, Bain Capital, Softbank and Primary Ventures. Our clients represent the biggest names in CRE, including Newmark Knight Frank, Cushman & Wakefield, Tishman Speyer and WeWork.

If you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning!

ABOUT THE ROLE:

As a Data Engineer at Reonomy, you will tackle hard challenges everyday! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE SaaS solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, ElasticSearch and Docker.
Responsibilities include:
Collaborating with the Engineering team to design, build and improve Reonomy’s complex data layer
Creating data systems that ensure quality and consistency on our data platform
Solving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data
Playing a major role in the future architecture of our rapidly expanding backend platform
Writing high quality code, participating actively in code reviews, and consistently helping to ship software
ABOUT YOU:
6+ years of experience in a Data Engineering capacity
Expertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets
Proven ability leveraging database technologies to solve non-trivial, large-scale problems
Advanced/Expert knowledge in SQL and data analysis
Experience programming in both typed (Scala, Java, etc..) and non-type (Python, Ruby, etc..) languages on production projects
Experience building modern, data-driven, web applications with emphasis on strong software design methodologies
A serious passion for data
History of excellence and responsibility in previous engineering positions
BENEFITS:
Competitive salary
Company stock options
100% coverage on medical, vision and dental health plans
Unlimited Vacation
401k plan and commuter benefits
Office perks: catered lunches 3x/week, catered breakfast 2x/week, unlimited snacks, team happy hours, Free Citi Bike membership, fitness discounts, Free Spotify membership, dedicated wellness rooms in the office!
We do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means."
168,Lead Big Data Engineer,"New York, NY 10018",New York,NY,10018,None Found,None Found,None Found,None Found,None Found,None Found,"FreeWheel, A Comcast Company, comprised of FreeWheel Publishers, FreeWheel Markets, and FreeWheel Advertisers – empowers all segments of The New TV Ecosystem. We are structured to provide the full breadth of solutions the advertising industry needs to achieve their goals. We power the technology, data enablement, and convergent marketplaces required to ensure buyers and sellers can transact across all screens, across all data types, and all sales channels, in order to ensure the ultimate goal – results for marketers. With offices in New York, San Francisco, Chicago, London, Paris, Beijing, and across the globe, FreeWheel, A Comcast Company, stands to advocate for the entire industry through the FreeWheel Council for Premium Video.


FreeWheel, A Comcast Company, comprised of FreeWheel Publishers, FreeWheel Markets, and FreeWheel Advertisers empowers all segments of The New TV Ecosystem. We are structured to provide the full breadth of solutions the advertising industry needs to achieve their goals. We power the technology, data enablement, and convergent marketplaces required to ensure buyers and sellers can transact across all screens, across all data types, and all sales channels, in order to ensure the ultimate goal results for marketers. With offices in New York, San Francisco, Chicago, London, Paris, Beijing, and across the globe, FreeWheel, A Comcast Company, stands to advocate for the entire industry through the FreeWheel Council for Premium Video.

FreeWheel is hiring a New York based Lead Big Data Developer reporting to Director, Engineering.

FreeWheel helps the television industry generate revenue from their premium content through a robust technology platform, which enables consumers to watch entertainment on any devices from PC, mobile, set-top box to traditional TV.
New York Data Engineering Team at FreeWheel is part of a global engineering group that is building advanced and high quality solutions to process terabytes of raw set top box data on state of the art big data platform, and make it accessible in a secure and cost effective manner for other teams.

ResponsibilitiesDevelop software to run on cloud native big data infrastructure built on AWS using Spark, Lambda, S3, and other cloud native services.Design and implement high-performance, scalable and optimized data solutions.Write applications to derive insights from terabytes of highly valuable data sets.Develop quick proof of concepts for marketing teams and clients with big data needs.Manage and improve existing code base and add new features to support business needsConduct design and code reviewsWork in a collaborative manner with a smart, diverse and fast-growing New York based Data Engineering scaled agile (SAFe) teams to rapidly deliver high quality solutions.Interface with global engineering, product and operation teams in US/EU/China to incorporate their innovations and vice versaAbility to mentor and provide direction in architecture and design to developersMonitors industry trends and directions; develops and presents substantive technical recommendations to senior managementBe in tune with emerging trends Big data and cloud technologies and participate in evaluation of new technologies to improve our data processing architecture to improve our data processing architecture to improve our data processing architecture
About You
Bachelor's or master's degree in Computer Science or similar field of study5+ years of experience building large scale big data applicationsProficient with SQL and database general conceptsHands-on experiences with data processing are strongly preferred.Expertise with at least one of the following languages Golang, Scala, Python, and or Java. You are flexible with languages and tools and are willing to learn whatever is necessary to get the job done.Experience with CI/CD tools like Jenkins.AWS certification is preferable: AWS Developer/Architect/DevOps/Big DataExperience in AWS technologies such as Lambda, Kinesis, EC2, IAM, CloudFormation, Route 53, ELB, VPC, Cloudwatch, Lambda, EMR, S3 and Big data related technologies like HIVE, Presto, Kafka is strongly preferred.Experience with bash, Linux utilities and environments.Understanding of various columnar file formats like Parquet, ORC etc.Strong problem-solver and analysis skills, and a team player. Have experience successfully introducing new patterns and frameworks.Must be able to quickly understand technical and business requirements and can translate them into technical implementationsSolid understanding of secure application development methodologiesExcellent written and verbal communication skills. Ability to communicate well in a team.


Comcast is an EOE/Veterans/Disabled/LGBT employer"
169,Data Analyst / Data Engineer,"New York, NY 10018",New York,NY,10018,None Found,None Found,None Found,None Found,None Found,None Found,"About Us

MediaMath's mission is to help innovative marketers delight their customers and drive real business outcomes. Over 9,500 marketers in 42 countries use our enterprise solution to reach the right user with the right message for the right price, wherever they are. These are the leading brands and agencies in the world who rely on MediaMath to provide technology, services and leadership to help them continue to transform their marketing practices and compete in a quickly changing industry. Founded in 2007 as a pioneer in ""programmatic"" advertising, MediaMath has been recognized as a leader in the space in four independent evaluations over the last two years. MediaMath has offices in 16 cities worldwide and is headquartered in New York City.

Key Responsibilities

MediaMath's Product team is currently seeking a Data Engineer with the knowledge, passion, and capability to build and work with complex datasets that are used by product and account management teams to discover and deliver audience insights for our clients. The key remit of the role is to support manual processes addressing immediate client demand for audience targeting and measurement for CTV campaigns. As the Data Engineer, you will support these initiatives through building, maintaining, and optimizing data infrastructure

You will:

Become an expert in MediaMath data flows and the Analytics data infrastructure.
Build, maintain, and own scalable data pipelines to support client data integration.
Become a team SME in data munging and automated ETL processes.
Work with Analysts to understand and leverage big data to solve client problems and needs.
Ensure that data pipelines/systems adhere to team and company standards, and raise the bar on the standards when possible.
Be a team player, and bring the team and company forward by solving team and company priorities.

You are:

Experienced in writing readable, re-usable code SQL and Python
Experienced with distributed system technologies, Hadoop, HiveQL, and Spark SQL/PySpark
Experienced in implementing data pipeline health monitoring, alerting
Experienced with data infrastructure troubleshooting and working with system logs
Experienced developing data flow schematics/blueprints
Advocate for automation and building efficient, scalable solutions
Self-driven, with a hunger to learn and spread knowledge by teaching others
Excellent communication skills – ability to synthesize and communicate technical concepts, limitations, and requirements to client-facing teams and stakeholders

You have:

Bachelor's Degree or higher, preferably with a concentration in a computational field such as Computer Science, Mathematics, Statistics, Physics, Engineering;
1 - 3 years of experience in building, troubleshooting, and optimizing production ETL pipelines - ideally held a Data Engineer position previously
Experience with data modelling, data integration, and working with disparate data sources, including APIs and relational databases
Experience partnering with client-facing teams to understand client needs and translate them to technical requirements
Experience with cloud computing technology, preferably AWS (EC2, S3, RDS, Lambda)

Why We Work at MediaMath

We are restless innovators, smart, passionate and kind. At the heart of our culture are six values that provide a framework for how we approach our work and the world: Teams Win, Scale + Innovation, Obsess Over Learning & Growth, Align then Execute, Do Good Better and Embrace the Journey. These values inform how we energize one another and engage with our clients. They get us amped to come to work. And, let's face it, so do the free snacks, great benefits, and unlimited vacation.

MediaMath is committed to equal employment opportunity. It is a fundamental principle at MediaMath not to discriminate against employees or applicants for employment on any legally-recognized basis including, but not limited to: age, race, creed, color, religion, national origin, sexual orientation, sex, disability, predisposing genetic characteristics, genetic information, military or veteran status, marital status, gender identity/transgender status, pregnancy, childbirth or related medical condition, and other protected characteristics as established by law."
170,Machine Learning Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Machine Learning Engineering at Bowery means building and deploying the production-facing data products which automate the agricultural and operational functions in our network of indoor vertical farms. The Data & AI function at Bowery is central to the vision of the company as well as its day to day operation. You’ll work with data scientists and analysts who build everything from computer vision models of plant growth to optimization solvers for farm utilization.

What you’ll do at Bowery

You’ll implement innovative quantitative solutions and advance the state of indoor farming alongside our team of data scientists and software engineers.
You’ll use the latest machine learning and data stack technologies to deploy AI services which ensure our farms operate smoothly and our crops flourish.
You’ll be a bridge between our team and our partners in Software engineering as we design and build co-dependent services and features.
You’ll help shape the way our data & AI team does work - researching and making key decisions about what we build, how we build it, and which tools are best for solving our problems.
You’ll help us scale and shape our cloud data infrastructure as our needs evolve and new cloud services come into the market.
You’ll add an engineer’s perspective to the code and design choices considered by our data scientists and analysts - mentoring and building skills together.

Who you are

BA/BS in field which shaped your thinking around engineering problems. 4+ years industry experience preferred.
You have strong experience and opinions about our favorite tools including python, redshift, keras, tensorflow, nomad, airflow, flask, kinesis.
You are skilled at and enjoy being a champion of software engineering best practices: testing, code review, documentation, continuous deployment, monitoring.
You are confident as a data engineer -- from our streaming data sources to ingestion and storage and integration with downstream dependencies.
You are confident taking ownership of projects from start to finish and enjoy the process of turning nebulous ideas into reality.
You believe that teams succeed and fail together and take responsibility for ensuring the success and safety of your teammates.

About Bowery
Bowery is growing food for a better future by revolutionizing agriculture. Our modern farming company combines the benefits of the best local farms with advances made possible by technology to grow produce you can feel good about eating. BoweryOS, our proprietary software system, uses vision systems, automation technology, and machine learning to monitor plants and all the variables that drive their growth 24/7. Because we control the entire process from seed to store, Bowery farms use zero pesticides, 95% less water, and are 100+ times more productive on the same footprint of land than traditional agriculture. Bowery produce is currently available at select Whole Foods and Foragers stores in the Tristate area, and featured on the menus of Tom Colicchio’s New York restaurants Craft and Temple Court. Based in New York City, the company has raised over $120 million from leading investors including GV, General Catalyst, GGV Capital, First Round Capital, Temasek and Almanac.

We are an equal opportunity employer. We welcome people of different backgrounds, experiences, abilities and perspectives. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status."
171,Senior Data Engineer,"New York, NY 10016",New York,NY,10016,None Found,None Found,None Found,None Found,None Found,None Found,"As a Senior Data Engineer, here's what we'll be looking for you to bring:
Hands-on Engineering Leadership
Proven track record of Innovation and expertise in Data Engineering
Tenure in coding, architecting and delivering complex projects
Deep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others
Deep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies
Deep understanding of relational database technologies and database development techniques
Understanding of how to architect solutions for data science and analytics
Data management for reporting and BI experience is a plus
Understanding of “Agility”, including core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Passion for software craftmanship
A rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..
Strong stakeholder management and interaction experience at different levels
Any experience building and leading an offshore/outsourcing function would be highly beneficial.
There's no typical day or engagement for our Senior Engineers. Here’s what you’ll do:

Be the SME. Develop Big Data architectural approach to meet key business objectives and provide end to end development solution
You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that Big Data has to solve their most pressing problems.
On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.
It could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.
Whatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.
You have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.
You recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.
Regardless of what you do at ThoughtWorks, you’ll always have the opportunity to:

Think through hard problems, and work with a team to make them reality.
Learn something new every day.
Work in a dynamic, collaborative, transparent, non-hierarchal, and ego-free culture where your talent is valued over a role title
Travel the world.
Speak at conferences.
Write blogs and books.
Develop your career outside of the confinements of a traditional career path by focusing on what you’re passionate about rather than a predetermined one-size-fits-all plan
Be part of a company with Social and Economic Justice at the heart of its mission.
A few important things to know:
Projects are almost exclusively on customer site, so candidates should be flexible and open to travel.

Candidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.

Not quite ready to apply? Or maybe this isn’t the right role for you? That’s OK, you can stay in touch with AccessThoughtWorks, our learning community (click ""contact me about recruitment opportunities"" to hear about jobs in the future).

It is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment."
172,Data Engineer,"New York, NY 10003",New York,NY,10003,None Found,None Found,None Found,None Found,None Found,None Found,"At Bitly, our mission is to empower businesses to create trusted, powerful, recognizable links that maximize the impact of every digital initiative. We are also proud of our community and our culture, and we are actively looking for team members who align with our vision, mission, and values.

The Role

As a Data Engineer you will work closely with the Project manager and Analysts to architect, build, and maintain a data and analytics infrastructure. This will include a data warehouse, in support of our business analytics and data reporting. The infrastructure created by this team will be critical in the team’s mission to support and provide insight for operations across the entire business.

Our Platform & Infrastructure team works on challenging product and infrastructure-focused problems. Our systems are built to process and provide real-time and near-time analytics for the world’s best brands. We support over 10 billion clicks and 5 Billion uniques per month from every country in the world and on every platform imaginable. To support our scale, our hybrid cloud consists of hundreds of servers, dozens or micro-services and numerous real-time data streams.
What You'll Do
Work with people throughout the company to understand the goals, needs, and priorities of systems and turn that understanding into a specific plan of attack
Use a variety of tools and languages including Go, Python, Bash, Hadoop, SQL, BigQuery and more
Build and consume APIs to ensure that relevant data is available across all systems
Who You Are
You have 2-5 years of engineering experience.
You have a solid background in writing understandable, testable, and efficient code
You have familiarity with relevant tools and technologies and stay up to date with ongoing changes and advancements
You can clearly communicate technical concepts and ideas to other engineers and to other parts of the organization
You have knowledge of how web protocols, clients, and servers work.
You have the ability to take initiative in addressing issues or seeking support from fellow team members
You have experience using a flavor of version control—we use git and Github to collaborate and manage pull requests
Bonus Points
Experience with Go and/or Python
Prior experience with GCP and specifically BigQuery (or AWS/Redshift) is a huge plus
A love of Scrum and an interest in helping the team develop agreements and processes around that methodology
Startup experience
Our Values
Nobody wins unless everybody wins.
>(!=) We strive for excellence in all we do. We care.
Knowledge is power. We are curious. We strive to discover new things and share them with the world.
Do or do not; there is no try. We will do great things. We will take risks. We will make it happen.
This land is our land. We are in this together, and we are all owners.
RESPECT. We treat our teammates, customers and partners with integrity and value the relationships that we build.
In keeping with our beliefs and goals, no employee or applicant will face discrimination/harassment based on: race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status.

Above and beyond discrimination/harassment based on “protected categories,” Bitly also strives to prevent other, subtler forms of inappropriate behavior (e.g., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place at Bitly."
173,Cyber Data Engineer,"Brooklyn, NY",Brooklyn,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About New York City Cyber Command
NYC Cyber Command was created in 2017 by Executive Order to lead the City’s cyber defense efforts, working across more than 100 agencies and offices to prevent, detect, respond, and recover from cyber threats. NYC Cyber Command is committed to protecting NYC infrastructure and critical systems from malicious attacks through the use of the latest technologies, public-private partnerships, and regular training and exercises for City employees.

Job Description
Support the Cybersecurity Data Science team, led by the Cybersecurity Lead Data Scientist;Analyze data in order to defend against cyber threats to the City;Integral to the defense of the City’s information environment, and works directly with the NYC3 Architecture, Engineering, and Threat Management teams;Design, build, implement, and operate systems that ingest, normalize, correlate, analyze, and present cybersecurity relevant data from across the City.


Minimum Qual Requirements

1. A baccalaureate degree, from an accredited college including or supplemented by twenty-four (24) semester credits in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or

2. A four-year high school diploma or its equivalent approved by a State’s department of education or a recognized accrediting organization and three years of satisfactory experience in any of the areas described in “1” above; or

3. Education and/or experience equivalent to “1” or “2”, above. College education may be substituted for up to two years of the required experience in “2” above on the basis that sixty (60) semester credits from an accredited college is equated to one year of experience. In addition, twenty-four (24) credits from an accredited college or graduate school in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or a certificate of at least 625 hours in computer programming from an accredited technical school (post high school), may be substituted for one year of experience.


Preferred Skills

The preferred candidate should possess the following:
At least 1 year experience (including internships) analyzing large, high velocity, heterogeneous datasets;Proficient in using cloud-based tools for ingesting, normalizing, analyzing, and presenting data for future engineering solutions and executive decision makers;A bachelor’s degree in computer science or information systems and have specializations in mathematics, number theory, applied cryptography, or statistics or relevant experience;Strong practical knowledge of data structures, Java, relational database design, and familiarity with system level and distributed programming;Familiarity with Unix scripting, Web development, and automated testing would be highly desirable, but not necessary.


To Apply

Special Note: Taking and passing civil service exams are necessary to maintain employment with the City of New York. Please check the Department of Citywide Administrative Services (DCAS) website (http://www.nyc.gov/html/dcas/html/work/exam_monthly.shtml) for important exam filing information. Please ensure that you are either a permanent employee in the civil service title listed on this posting, or, that you file for the examination when there is an open filing period. For more information regarding the civil service process, please visit the DCAS website at: http://www.nyc.gov/html/dcas/html/work/work.shtml

Interested applicants with other civil service titles who meet the preferred requirements should also submit a resume for consideration
For City employees, please go to Employee Self Service (ESS), click on Recruiting Activities > Careers, and search for Job ID #398932
For all other applicants, please go to www.nyc.gov/jobs/search and search for Job ID #398932

SUBMISSION OF A RESUME IS NOT A GUARANTEE THAT YOU WILL RECEIVE AN INTERVIEW
APPOINTMENTS ARE SUBJECT TO OVERSIGHT APPROVAL

Department of Information Technology & Telecommunications and the City of New York are equal opportunity employers.

DoITT participates in E-Verify


Hours/Shift

Day - Due to the necessary technical support duties of this position in a 24/7 operation, candidate may be required to work various shifts such as weekends and/or nights/evenings.


Work Location

New York, NY


Residency Requirement

New York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview."
174,Data Engineer - Hux,"New York, NY 10112",New York,NY,10112,None Found,"
4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
1+ years of experience on distributed, high throughput and low latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.",None Found,None Found,None Found,None Found,"Hux Data Engineer
Locations: New York, NY – Greensboro, NC - Chicago, IL – Raleigh, Durham, Chapel-Hill, NC - Denver, CO
What is Hux? Hux is the Human Experience Platform by Deloitte Digital.
In today’s world, customers expect companies to know who they are and what they want. Customers want to have products, services or experiences that best suit their needs delivered to them seamlessly across physical and digital channels.
Customers are human first: driven by dynamic wants, needs, and desires. The ability for brands to make personal, meaningful connections on a human level has never been greater and Hux by Deloitte Digital delivers on those experiences in a way that allows companies to own the customer journey end to end. We help companies connect key data sources to understand what matters most to people; connect to advanced technologies like AI and machine learning to sense and respond to those needs at scale; and connect their systems to unlock insights, create collaboration and drive acquisition, engagement and loyalty. Most importantly, we empower companies to connect with customers in personal, meaningful ways that respect them as people, not just customers.
Hux by Deloitte Digital gives companies the ability to build and leverage the connections – between people, systems, data and technologies – so they can deliver personalized, contextual experiences to customers at scale.

Work you’ll do
As a Hux Data Engineer, you’ll design, implement, and maintain a full suite of real-time and batch jobs that fuels our cutting edge AI to provide real-time marketing intelligence to our existing clients.
You’ll develop, test and deliver production grade code to help our clients solve their marketing challenges using cutting-edge big-data tools. You’ll also ensure data integrity, resolve production issues, and assist in the support and maintenance of our overall Platform.
As you grow your capabilities and learn how to build a platform that can ingest, load and process billions of data points, you’ll enjoy new challenges and opportunities to showcase your development skills by joining project teams to build innovative new-client platforms and execute high-value strategic development projects with high visibility.
Your responsibilities will include:
Design, construct, install, test and maintain highly scalable data pipelines with state-of-the-art monitoring and logging practices.
Bring together large, complex and sparse data sets to meet functional and non-functional business requirements.
Design and implements data tools for analytics and data scientist team members to help them in building, optimizing and tuning our product.
Integrate new data management technologies and software engineering tools into existing structures.
Help build high-performance algorithms, prototypes, predictive models and proof of concepts.
Use a variety of languages, tools and frameworks to marry data and systems together.
Recommend ways to improve data reliability, efficiency and quality.
Collaborate with Data Scientists, DevOps and Project Managers on meeting project goals.
Tackle challenges and solve complex problems on a daily basis.
Qualifications
Required:
4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
1+ years of experience on distributed, high throughput and low latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.
Preferred:
Producing high-quality code in Python.
Passionate about testing, and with extensive experience in Agile teams using SCRUM you consider automated build and test to be the norm.
Proven ability to communicate in both verbal and writing in a high performance, collaborative environment.
Follows data development best practices, and enjoy helping others learn to do the same.
An independent thinker who considers the operating context of what he/she is developing.
Believes that the best data pipelines run unattended for weeks and months on end.
Familiar with version control, you believe that code reviews help to catch bugs, improves code base and spread knowledge.
Helpful, but not required:
Knowledge in:
Experience with large consumer data sets used in performance marketing is a major advantage.
Familiarity with machine learning libraries is a plus.
Well-versed in (or contributes to) data-centric open source projects.
Reads Hacker News, blogs, or stays on top of emerging tools in some other way
Data visualization
Industry-specific marketing data
Technologies of Interest:
Languages/Libraries – Python, Java, Scala, Spark, Kafka, Hadoop, HDFS, Parquet.
Cloud – AWS, Azure, Google
The team
Advertising, Marketing & Commerce
Our Advertising, Marketing & Commerce team focuses on delivering marketing and growth objectives aligned with our clients’ brand values for measurable business growth. We do this by creating content, communications, and experiences that engage and inspire their customers to act. We implement and operate the technology platforms that enable personalized content, commerce and marketing user-centric experiences. In doing so, we transform our clients’ marketing and engagement operations into modern, data-driven, creatively focused organizations. Our team brings deep experience in creative and digital marketing capabilities, many from our Digital Studios.

We serve our clients through the following types of work:Cross-channel customer engagement strategy, design and development(web, mobile, social, physical)eCommerce strategy, implementation and operationsMarketing Content and digital asset management solutionsMarketing Technology and Advertising Technology solutionsMarketing analytics implementation and operationsAdvertising campaign ideation, development and executionAcquisition and engagement campaign ideation, development and executionAgile based, design-thinking, user-centric, empirical projects that accelerate results

How you’ll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.
Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitte’s culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.
Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.
Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.
kwhux"
175,"Data Engineer, Newsroom","New York, NY 10176",New York,NY,10176,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description:
The Wall Street Journal seeks a data engineer who will be responsible for developing tools to help the newsroom in its data science work. The Journal is expanding its use of data in both editorial and audience-related projects, and this engineer will be an important force in bolstering the newsroom’s data capacities.
The Journal is seeking a full-stack data engineer who will be responsible for (1) acquiring new datasets, (2) creating and maintaining data pipelines, (3) deploying data and insights to editors in the newsroom, and (4) building prototypes of tools for editors and newsroom staffers.
This role is responsible for making and maintaining a data pipeline for all the data sets we want, have and need. This is a function tied to the newsroom’s top-level strategy, working in collaboration with the Audience group, the R&D Lab and the broader newsroom. The engineer will collaborate with data scientists and work directly with a number of highly sophisticated audience and content data sets. The engineer will also help with rapid prototyping and testing of newsroom data tools as well as help maintain ones that are successful.
We are looking for someone with deep knowledge of audience behavior around common journalism types, like breaking news and enterprise journalism, as well as experience in newsroom tools and data dashboards. The data engineer should have strong background in A/B testing as well as managing data processes that inform content optimization. This role is suited for a talented engineer with a strong understanding of newsroom workflow and a passion for helping journalists connect with their audiences.
Skills:
Experience running and supporting production of enterprise data platforms
Experience creating internal tools that combine content and audience data
Experience in building infrastructure required for optimal extraction, transformation and loading of data from various resources
Knowledge of JavaScript, Python, Bash and SQL
Build data pipelines with tools and cloud-based data services like Google’s BigQuery, AWS, Dataproc and Pub/Sub
2+ years of data engineering experience
Strong statistics skills
This position reports to The Wall Street Journal’s Head of Data Solutions.
LI-JA1-WSJ
Dow Jones , Making Careers Newsworthy
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, or disability status. EEO/AA/M/F/Disabled/Vets .
Dow Jones is committed to providing reasonable accommodation for qualified individuals with disabilities, in our job application and/or interview process. If you need assistance or accommodation in completing your application, due to a disability, please reach out to us at TalentResourceTeam@dowjones.com . Please put “Reasonable Accommodation"" in the subject line.
Business Area: NEWS/WSJ
Job Category: IT Development Group
About Us
The Wall Street Journal is a global news organization that provides leading news, information, commentary and analysis. The Wall Street Journal engages readers across print, digital, mobile, social, and video. Building on its heritage as the preeminent source of global business and financial news, the Journal includes coverage of U.S. and world news, politics, arts, culture, lifestyle, sports, and health. It holds 38 Pulitzer Prizes for outstanding journalism. The Wall Street Journal is published by Dow Jones, a division of News Corp (NASDAQ: NWS, NWSA; ASX: NWS, NWSLV).
If you are a current employee at Dow Jones, do not apply here. Please go to the Career section on your Workday homepage and view ""Find Jobs - Dow Jones."" Thank you.
Req ID: 14570"
176,"Data Engineer, Computational Oncology","New York, NY 10017",New York,NY,10017,None Found,None Found,None Found,None Found,None Found,None Found,"Company Overview
At Memorial Sloan Kettering (MSK), we’re not only changing the way we treat cancer, but also the way the world thinks about it. By working together and pushing forward with innovation and discovery, we’re driving excellence and improving outcomes.

For the 28th year, MSK has been named a top hospital for cancer by U.S. News & World Report. We are proud to be on Becker’s Healthcare list as one of the 150 Great Places to Work in Healthcare in 2018, as well as one of Glassdoor’s Employees’ Choice Best Place to Work for 2018. We’re treating cancer, one patient at a time. Join us and make a difference every day.

Job Description
Are you passionate about contributing meaningfully to battling cancer? Then join us here at MSK, where we can provide you with the opportunity to make a difference with your career. We believe this is a very exciting opportunity for someone who has the right skillset and drive to make an impact to support our mission.

The Computational Oncology Program in the Department of Epidemiology and Biostatistics is seeking a talented, highly skilled Data Engineer to join their team. We are motivated by contributing meaningfully to contemporary progress in cancer research driven by advances in computing and data. The right person will work in close collaboration with researchers and software engineers, and be responsible for managing data from leading edge, large scale research efforts in computational biology including genomics, imaging and clinical data analysis and interpretation. The Data Engineer will have experience managing data utilizing robust, enterprise level contemporary software systems.
You Will:
Manage data from high-throughput next-generation sequencing and imaging
Contribute to the design of databases as part of bioinformatics data processing and analysis systems
Maintain and monitor streaming and batch ETLs operating on structured and unstructured sources
Maintain a data lake with hundreds of terabytes of data
Develop workflows and integrate systems with REST APIs
Compile datasets and verify data consistency
Communicate with stakeholders of the data and upon request, conduct data query tracking and resolution
Identify inefficiencies and work with software engineers to simplify processes, debug systems and automate routine tasks
You Need:
Bachelor’s Degree in Computer Science, Information Systems, or Database Management (or equivalent experience), Master’s degree is preferred
3+ years of experience, preferably with bioinformatics lab information management systems
Experience designing databases and defining system requirements for data collection
Strong software engineering skills in Python, and working with SQL and NoSQL data
Solid experience in Linux systems, and shell scripting

#LI-POST
Closing
MSK is an equal opportunity and affirmative action employer committed to diversity and inclusion in all aspects of recruiting and employment. All qualified individuals are encouraged to apply and will receive consideration without regard to race, color, gender, gender identity or expression, sexual orientation, national origin, age, religion, creed, disability, veteran status or any other factor which cannot lawfully be used as a basis for an employment decision.
Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment."
177,"Data Engineer, Data Acquisition","Hoboken, NJ 07030",Hoboken,NJ,07030,None Found,"Play a pivotal design and hands on implementation role in improving the Data infrastructure in a project-oriented work environment.Influence cross functional architecture in sprint planningGather and process raw data at scale from internal and external data sources and expose mechanisms for large scale parallel processingDesign, implement and manage a near real-time ingestion pipeline into a data warehouse and Hadoop data lake.Process unstructured data into a form suitable for analysis and then empower state-of-the-art analysis for analysts, scientists, and APIsSolve complex SQL and Big Data Performance challenges.Mitigate Risks in our data infrastructure by developing the best in class tools and processes.Implement controls, policies, processes and best practices in the Data Engineering space.Evangelize an extremely high standard of code quality, system reliability, and performance.Help us improve our database deployment and change management process.Provide reliable and efficient Data services as part of the global data team.Work closely with the team on development best practices and standards.Be a mentor.",None Found,None Found,None Found,None Found,"Position Description
As part of the newly created Data Strategy and Enablement Team (DS&E), this role will be an enabler of our journey to be the world’s leading data-driven retailer. As part of this transformation, we are seeking an individual who will be responsible for establish robust data pipelines and services – including both in house developed data enabling services and systems integrations across the DS&E team to ensure we our technical deliverables meet and exceed the quality expectations.

We are looking for a highly motivated, resourceful, team-oriented individual to drive the data engineering process. You are exceptionally talented Data engineer with an outstanding track record of working with very large data sets and building robust ETL pipelines for data acquisition for internal systems and external data sources. You will be modernizing and improving the data acquisition infrastructure from the ground up. You will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.

The Data Engineer role will report up to the Lead Data Engineer/Senior Manager Data Engineering
Minimum Qualifications
Play a pivotal design and hands on implementation role in improving the Data infrastructure in a project-oriented work environment.Influence cross functional architecture in sprint planningGather and process raw data at scale from internal and external data sources and expose mechanisms for large scale parallel processingDesign, implement and manage a near real-time ingestion pipeline into a data warehouse and Hadoop data lake.Process unstructured data into a form suitable for analysis and then empower state-of-the-art analysis for analysts, scientists, and APIsSolve complex SQL and Big Data Performance challenges.Mitigate Risks in our data infrastructure by developing the best in class tools and processes.Implement controls, policies, processes and best practices in the Data Engineering space.Evangelize an extremely high standard of code quality, system reliability, and performance.Help us improve our database deployment and change management process.Provide reliable and efficient Data services as part of the global data team.Work closely with the team on development best practices and standards.Be a mentor.
Who you are:
You have prior experience with leading data engineering efforts across a variety of data systemsYou have deep understanding of commercial data sources and understand database concepts and terminologyYou have a demonstrated track record of handling multiple complex sourcing projects and delivering results in the data engineering areaYou have strong SQL experience and the ability to work on multiple aspects of a data projects including ETL, tools integrations, data results and APIs.You are a team player, with the courage to drive change through disruption while maintaining a respect for the team
Requirements:
Very Strong engineering skills. Should have an analytical approach and have good programming skills.Provide business insights, while leveraging internal tools and systems, databases and industry dataMinimum of 5+ years’ experience. Experience in retail business will be a plus.Excellent written and verbal communication skills for varied audiences on engineering subject matterAbility to document requirements, data lineage, subject matter in both business and technical terminology.Guide and learn from other team members.Demonstrated ability to transform business requirements to code, specific analytical reports and toolsThis role will involve coding, analytical modeling, root cause analysis, investigation, debugging, testing and collaboration with the business partners, product managers other engineering teamExperience working with large data sets, experience working with distributed computing (MapReduce, Hadoop, Hive, Pig, Apache Spark, etc.) and platforms such as HDP, Cloudera etc.Strong Hadoop scripting skills to process petabytes of dataExperience in Unix/Linux shell scripting or similar programming/scripting knowledgeReal time data ingestion (Kafka)Experience in ETL/ processes with exposure to one or more tools such as Nifi, Talend, Informatica, SSIS etc.
Additional Preferred Qualifications

Company Summary
The Walmart eCommerce team is rapidly innovating to evolve and define the future state of shopping. As the world’s largest retailer, we are on a mission to help people save money and live better. With the help of some of the brightest minds in technology, merchandising, marketing, supply chain, talent and more, we are reimagining the intersection of digital and physical shopping to help achieve that mission.
Position Summary
As part of the newly created Data Strategy and Enablement Team (DS&E), this role will be an enabler of our journey to be the world’s leading data-driven retailer. As part of this transformation, we are seeking an individual who will be responsible for establish robust data pipelines and services – including both in house developed data enabling services and systems integrations across the DS&E team to ensure we our technical deliverables meet and exceed the quality expectations.

We are looking for a highly motivated, resourceful, team-oriented individual to drive the data engineering process. You are exceptionally talented Data engineer with an outstanding track record of working with very large data sets and building robust ETL pipelines for data acquisition for internal systems and external data sources. You will be modernizing and improving the data acquisition infrastructure from the ground up. You will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.

The Data Engineer role will report up to the Lead Data Engineer/Senior Manager Data Engineering."
178,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Blink
-----------

Blink Health is fixing how broken, opaque, and unfair healthcare is. We are a New York based, mission driven, well-funded healthcare technology company. We're changing healthcare through technology and transparency. With our proprietary technology, everyone now has access to one, low negotiated price on over 15,000 medications. But there is more work to do.

About The Team
--------------

Blink Engineering strives to build trusted, highly observable, data-driven products to bring affordable, accessible healthcare to all Americans. We understand healthcare is the most complex system most of us will ever fix. We believe in solving this complexity through the use of simple, well-known technologies. We are a highly collaborative team that believes in owning outcomes over owning code and putting patients at the center of everything we do.

The Blink Health Data Engineering and Analytics team is a small team responsible for building infrastructure, frameworks and tooling to enable data-driven decisions; building and maintaining our data warehouse for security and scale. This role is central to building and executing on a robust and forward-looking data strategy for the company, and the successful candidate blends top-tier software engineering expertise with the ability to look ahead at what we need to build for the future.

About the Role
--------------

As the senior software engineer for data, you will be a thought leader within the data engineering team that is designing and building our next generation of data tools and frameworks, in addition to developing and maintaining data products and infrastructure. You will proactively assess production DW support trends to determine and implement short- and long-term solutions, and be able to design for data integrity, reliability, and performance. You will set a high bar for clean and correct code, setting code standards, and performing peer code and architecture reviews.

Required Experience


You have 6+ years hands-on experience and demonstrated strength with:
Python, building data pipelines, and managing data at multiple companies.
Writing complex, highly-optimized SQL queries across large data sets.
Building and maintaining robust and scalable data integration (ETL) pipelines using SQL, EMR, Python and Spark.
Designing and maintaining columnar databases (e.g., Redshift, Snowflake)
Distributed data processing (Hadoop, Spark, Hive)
ETL with batch (Data Pipeline) and streaming (Kinesis, Airflow)
Integration and design for Business Intelligence tools (e.g., Looker, QuickSight)
Creating scalable data models for analytics.
You have experience designing and refactoring large enterprise data warehouses and associated ETLs, with continuous improvement examples for automation and simplification across all aspects of the DW environment, inclusive of both engineering and business reporting.
Experience owning features from design through delivery along with ongoing support.
Proven success with communicating effectively across diverse disciplines (including product engineering, infrastructure, analytics, data science, finance, marketing, customer support, etc.) to collect requirements and describe data engineering strategy and decisions.
Experience providing clear data engineering technical leadership, mentoring, and best practices for data management and quality within and across teams.
Undergraduate or graduate degree in Computer Science

Desired Experience


Healthcare-relevant company experience as part of the required experience above, with demonstrated industry knowledge of handling sensitive information.

"
179,Data Engineer - CIMD - Marcus by Goldman Sachs Engineering,"New York, NY 10282",New York,NY,10282,None Found,None Found,None Found,None Found,None Found,None Found,"MORE ABOUT THIS JOB
Consumer and Investment Management (CIMD)
The Consumer and Investment Management Division includes Goldman Sachs Asset Management (GSAM), Private Wealth Management (PWM) and our Consumer business (Marcus by Goldman Sachs). We provide asset management, wealth management and banking expertise to consumers and institutions around the world. CIMD partners with various teams across the firm to help individuals and institutions navigate changing markets and take control of their financial lives.

Consumer
Consumer, externally known as Marcus by Goldman Sachs, is comprised of the firm’s digitally-led consumer businesses, which include our deposits and lending businesses. It also includes our personal financial management app, Clarity Money. Consumer combines the strength and heritage of a 150-year-old financial institution with the agility and entrepreneurial spirit of a tech start-up. Through the use of insights and intuitive design, we provide customers with powerful tools that are grounded in value, transparency and simplicity to help them make smarter decisions about their money.
RESPONSIBILITIES AND QUALIFICATIONS
HOW YOU WILL FULFILL YOUR POTENTIAL
Design and develop data ingest and transform processes
Develop data visualizations using BI tools and web-based technologies
Work as part of a global team using Agile software methodologies
Partner with Marcus risk, product, acquisition and servicing teams
Use Marcus data to drive change throughout the Marcus business

SKILLS AND EXPERIENCE WE ARE LOOKING FOR
Bachelor’s degree or equivalent required
Minimum 3 years of relevant professional experience
Proficient at Python, Spark and the Hadoop ecosystem
Experience with SQL and relational databases
Self-starter, motivated, and good communication skills Strong sense of ownership and driven to manage tasks to completion
ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet."
180,Data Platform Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Knotch:
-------------

Knotch is the Content Intelligence Platform that enables communications and marketing teams to measure, understand, and optimize content ROI across their digital efforts. We work closely with the following verticals: financial (including JP Morgan Chase & Co., Capital One, Citi, TD, Discover, Ally, Bank of America and Synchrony), automotive (including Ford), telecommunications (including AT&T and Sprint), retail (including Walmart), technology (including Salesforce, HP Inc, HP Enterprise, Citrix, and AWS), and consulting (including Deloitte and PwC.) We are currently expanding quickly into the insurance, fashion and health verticals.

At Knotch, we:

Enable brands to become better content creators through real-time and independent measurement & optimization across on and off property content investments.
Activate content to move customers to high value actions.
Turn content into a first party audience data source that flows into the rest of the downstream marketing platforms (CRM, DMP, CDP etc).

We're based in SoHo, NYC and we're proud to have been named to both Inc.'s Best Places to Work ( https://www.inc.com/best-workplaces-2019.html ) and Built In NYC's Best Places to Work ( https://www.builtinnyc.com/companies/best-places-to-work-nyc-2019 ) lists two years in a row!

Engineering at Knotch:
----------------------

Engineering is the cornerstone of our organization and we work hard everyday to build the most impactful products as possible. We love to experiment, find a deep joy in product iteration, achieve stability with thoughtful architecture and testing all while monitoring our performance and progress at every step.

Knotch's founding mission has always been to improve the advertising and marketing industries in a lasting and meaningful way. Transparency through data is our ethos and something every member of our company takes seriously. We are looking for highly motivated engineers who are passionate about data and who are eager to transform an industry to join us on our journey.

Data Platform Engineering
-------------------------

At Knotch, our data platform is vital to providing key intelligence and insights to our clients. As a result, our data platform is the most important element when it comes to successfully scaling our products and technologies. As our platform evolves, agility with stability are critical to avoid bottlenecks in our products and to ultimately increase our value. We're looking for an experienced data engineer who deeply values clever platform architecture and who emphasizes speed and stability to join our growing team.

What You'll Do at Knotch
------------------------


Design and implement resilient backend architectures that process gigabytes and beyond
Write software for backend services using Python, Scala, and other languages
Work directly with our Data Science team facilitate new understandings and insights from our data
Work directly with our full-stack engineering team to expose value from our data

What We Want From You
---------------------


3+ years of data platform engineering experience
Working experience with AWS services including Kinesis, Lambda, S3, and RedShift
Experience with various AWS offerings such as EC2, ECS, RDS, and ElastiCache
Experience with EMR, Scala, Ruby, Postgres, and Redis
Experience utilizing Python to work with Data Science teams to productize models and applications

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
181,"Data Engineer Full-time New York, NY","New York, NY 10001",New York,NY,10001,None Found,None Found,None Found,None Found,None Found,None Found,"We are in search of a Data Engineer who loves to take huge chunks of data, transform it to facilitate analytics and customer understanding. In this role you will work with business teams to translate operational requirements into secure tech solutions.

Our stack currently includes AWS, Spark, Redshift, Postgres, Pentaho, Airflow and Tableau.We constantly evaluate the latest technologies and we have the freedom to choose the right ones to solve the task.
This role will become an integral part of TIDAL and will develop secure BI solutions. Those solutions will help bring to light actionable insights to improve business decisions and provide better experiences for our customers.
As a Data Engineer at TIDAL you will:

Develop ETL jobs using Spark, Alteryx, or other ETL tools
Serve as a liaison between the Analytics team and Integration teams to serve data in a way that fulfills analytics requirements
Work closely with other teams such as Analytics, Finance, Product, Retention, and Growth to gather specifications and provide them with relevant prepared data
Automate report distribution to partners in the music industry, tech integrations, and carriers
Build tech integrations between 3rd party tracking systems and internal databases
Respond to ad hoc data requests and troubleshoot data issues
Maintain and enforce role based security to ensure that data is secure and only accessed by approved employees
Work closely with stakeholders on the marketing team to enhance and maintain data flows, reporting, and event / user tracking on Adjust, Braze, and other platforms in the marketing tech stack


What you need to succeed in this role:

4 Year Bachelor of Science with a concentration in Computer Science, Engineering, or Data Science preferred
Experience with Python or Scala
Experience using BI and ETL tools
Knowledge of SQL programming and development of data warehouses
Ability to understand complex business logic
Knowledge of Spark is an advantage


Who you are:
You know how to work with high volume heterogeneous data, preferably with distributed systems such as AWS Redshift Spectrum and RDS.
You are knowledgeable about data modeling, data access, and data storage techniques.
You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
You understand the value of partnership within teams
You are prepared to have frequent business trips to Oslo


Does this sound like you?

Apply today and become a new Data Engineer at TIDAL.

What we offer:

Technology: You will work with the latest technology and collaborate with skilled teams
Our office: We are located in the middle of Times Square, NYC.
Visit Scandinavia: Since you will be part of our BI team in Oslo, you will have the opportunity to travel to Norway on frequent business trips.

Social: We host intimate concerts in the office and we have a Christmas party."
182,Data Engineer,"Manhattan, NY",Manhattan,NY,None Found,None Found,"Three or more years of experience working with *nix-based, open source data processing tools
Fluent in Python, SQL, Spark, Hadoop, AirFlow and/or similar technologies/toolsets
Two or more years of experience developing production ETL applications
Four or more years of experience in software development
Three or more years of experience with SQL
Curious, informed and opinionated about data processing technologies
Experience with structured and unstructured data storage and modeling
Deep understanding of database and filesystem storage/access
Experience with various data engineering architecture patterns
Interest in Data Science and Data Analysis
",None Found,"Building python-based ETL jobs
Light web application development of purpose-built internal tools
Technical guidance in support of our project management team when defining the scope of data integration projects
Contribute to designs of new components in modeling and data pipelines
Remain current on emerging open source data processing projects and tools
Must have experience with AWS services, Redshift data bases and HIPAA compliant architecture models.
","BS or MS in Computer Science
Experience writing production Python
Implementation experience with Airflow, Python, Spark etc.
Experience with healthcare data formats (x12 EDI, HL7, etc)
Experience implementing stream processing pipelines (Spark, etc)
MapReduce/Hadoop ecosystem experience (Hive, HDFS/S3, Presto)
Experience with source control technology like Git
Experience with testing frameworks (unit and end-to-end)
Familiar with the usage of Continuous Integration/Continuous Deployment frameworks in AWS like Jenkins, CircleCI, Code deploy and code commit
Understanding of Docker implementation in AWS
Understanding of AWS serverless services like Lambda/API gateway
Good to have: AWS elastic beanstalk, AWS cloud formation
",None Found,"VillageCare – Redefining Wellness
112 Charles St., New York, NY
VillageCareMax
Job Title: Data Engineer

Roles & Responsibilities:
The VillageCare Data Engineer will be responsible for helping to build ETL processes for Village Care’s Data Warehouse in an AWS cloud environment. The Data Engineer will build ETL pipelines, get analysis tools working properly, and stand up core data processing components in Village Care’s cloud-based data processing environment.
Building python-based ETL jobs
Light web application development of purpose-built internal tools
Technical guidance in support of our project management team when defining the scope of data integration projects
Contribute to designs of new components in modeling and data pipelines
Remain current on emerging open source data processing projects and tools
Must have experience with AWS services, Redshift data bases and HIPAA compliant architecture models.
Qualifications:
Three or more years of experience working with *nix-based, open source data processing tools
Fluent in Python, SQL, Spark, Hadoop, AirFlow and/or similar technologies/toolsets
Two or more years of experience developing production ETL applications
Four or more years of experience in software development
Three or more years of experience with SQL
Curious, informed and opinionated about data processing technologies
Experience with structured and unstructured data storage and modeling
Deep understanding of database and filesystem storage/access
Experience with various data engineering architecture patterns
Interest in Data Science and Data Analysis
Preferred Education and Experience
BS or MS in Computer Science
Experience writing production Python
Implementation experience with Airflow, Python, Spark etc.
Experience with healthcare data formats (x12 EDI, HL7, etc)
Experience implementing stream processing pipelines (Spark, etc)
MapReduce/Hadoop ecosystem experience (Hive, HDFS/S3, Presto)
Experience with source control technology like Git
Experience with testing frameworks (unit and end-to-end)
Familiar with the usage of Continuous Integration/Continuous Deployment frameworks in AWS like Jenkins, CircleCI, Code deploy and code commit
Understanding of Docker implementation in AWS
Understanding of AWS serverless services like Lambda/API gateway
Good to have: AWS elastic beanstalk, AWS cloud formation

Integrity
You are a team member who serves as a positive example and reflection of why others trust the intentions of VillageCare by:
Being honest and trustworthy
Meeting your commitments and obligations
Acknowledging your role in actions or events with unsatisfactory outcomes
Customer Focus/Cultural Awareness
You are a team member who understands the importance of strong customer service internally and externally and you demonstrate this by identifying customer needs and expectations, and responding to them in a timely and effective manner. You are consistently customer focused by:
Demonstrating an awareness of the needs of individuals through recognizing multiple levels of connections
Anticipates and prevents delays or other things that can adversely affect the customer.
Keeping customers informed about the status of pending actions and inquires
Flexibility/Agility
You are a team member who adjusts quickly and effectively to changing conditions and demands. You understand that change is a necessary and an inevitable aspect of organizational life as well as an opportunity to learn new things. As such, you are flexible and agile by:
Maintaining a positive view of potentially stressful situations
Accepting and adapting to organizational or departmental changes
Viewing change as opportunities for VillageCare to grow in a direction that better serves our clients and our employees
Result Oriented/Innovative Thinking
You are a team member who consistently looks for new and innovative approaches that will improve efficiency in your role. You champion new ideas and build upon existing processes by:
Using data/fact-based information to make decisions relevant your role
Understands that obstacles will occur and refuses to use them as an excuse for not achieving results
BEVital
You are a team member that consistently supports VillageCare’s larger organizational culture by displaying a commitment to the three cultural drivers that make VillageCare and our employees vital to the healthcare space by:
Exceeding expectations in both internal and external customer service areas
Using data and key information to inform decisions pertinent to your role (where applicable)
Utilizing relationships, tools and positivity to enhance organizational performance through communication and collaborative team work
VillageCare is committed to superior outcomes in quality health care. Do you share a common commitment to* patient care, customer service and passion *for individuals’ well-being ?
Apply now!
VillageCare:
With over 25,000 people served in 2017, VillageCare’s mission is to promote healing, better health and well-being to the fullest extent possible.
VillageCare began in 1977 as a project by community volunteers to rescue and reorganize a for-profit nursing home slated for closure. It has become a much larger organization that provides post-acute care, community-based services and managed long-term care. As a result of this history, VillageCare has become a valued resource for the people we serve, their caregivers and other provider organizations with which we partner.
VillageCare is committed to the tenets of diversity and workforce that are strengthened by the inclusion of and respect for our differences. We offer our employees a highly competitive compensation and benefits package, a 403(b) retirement plan, and much more.
VillageCare is an equal opportunity employer. We promote recognition and respect for individual and cultural differences, and we work to make our employees feel valued and appreciated, whatever their race, gender, background, or sexual orientation.
* EOE Minorities/Women/Disabled/Veterans*"
183,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,"
We’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.
The desire to learn and code in Scala
Experience in working in an Agile environment
Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.
A strong coding background in either Java, Python or Scala
","Founded in 2016 with only a handful of individuals, Quantexa purpose was built that through a greater understanding of context, better decisions can be made. 3 years, 6 locations and 180+ employees later we still believe that today. Working within industries such as Finance, Insurance, Energy and Government, we connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.

Our success is driven by the talent of our staff and our commitment to quality. We are looking for Data Engineers to join us in tackling some of the industry’s most challenging problems.
What does a Data Engineer role at Quantexa look like?
In order to be a successful data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.
Being Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service.
We want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.
Requirements
What do I need to have?
We’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.
The desire to learn and code in Scala
Experience in working in an Agile environment
Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.
A strong coding background in either Java, Python or Scala
Experience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.
Passion and drive to grow within one of the UK’s fastest growing Start-ups
Benefits
Why join Quantexa?

We know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying “thank you” for all your hard work!
Competitive Salary
Company Bonus
Excellent private healthcare, Dental and Optic coverage, Life assurance, LTD and STD coverage
401k where we’ll match up to 5%
Online training customized to your personal preferences
Generous annual leave
Amazing working environment - Ranging from regular social events, free beverages"
184,Data Engineer,"New York, NY 10013",New York,NY,10013,None Found,None Found,None Found,None Found,None Found,None Found,"Lab49 has an exciting opportunity for a Data Engineer to work with our clients to create next generation data and analytics solutions using the latest techniques and technologies. This role is responsible for understanding and solving the technical problems of each engagement, including identifying sources of relevant data, transforming and scrubbing to create data pipelines, doing feature engineering in collaboration with data scientists, and creating scalable solutions that can work in production.

The role will require a combination of strong technical skills and problem-solving capabilities, with an understanding of the underlying business concepts and meaning of the data. The Data Engineer is central to the success of every engagement, making sense of the data so that it can be analyzed and modeled to create insights.

Candidate Profile

The successful candidate will be a professional with prior experience working on data and analytics initiatives. This person will be passionate about getting to the bottom of the problems underlying data and finding creative and unique solutions.

The successful candidate will have the ability to use programmatic tools and techniques to quickly understand different data sources, transform and combine them into reliable pipelines of clean data, help data scientists engineer features from the data, and scale the resulting solutions into production-quality systems.

Required Attributes
3+ years of experience working in a hands-on capacity on data and analytics initiatives in the financial services industry
Ability to quickly learn financial services concepts to successfully understand the meaning of data and analytics related to the business problem being addressed
Strong software development experience with languages such as Python, R, Scala, including strong proficiency with data manipulation libraries like pandas
Previous experience and clear proficiency on the command-line using tools like Bash and integrated scripting
Previous experience using a variety of tools for data exploration, analysis, and presentation, including advanced use of Microsoft Excel
Significant experience with relational databases and advanced use of SQL
Sufficient mathematical and statistical literacy to work with business stakeholders, quantitative analysts, and data scientists
Good interpersonal skills, with an ability to interact with teammates and clients to successfully understand and solve problems
Effective communication skills, with an ability to clearly present ideas and data, including making use of visualization techniques
Preferred, but not required, attributes
Experience using non-RDBMS technologies, e.g. graph databases, columnar stores, document databases, OLAP cubes, etc.
Experience working in a variety of cloud and PaaS environments, e.g. Azure, AWS, Databricks, Domino
Experience with a variety of big data frameworks, e.g. Spark, Hadoop, Cassandra
Experience doing basic modeling and analytics using different frameworks, e.g. TensorFlow, scikit-learn, Keras, Torch
About Lab49

Lab49 is a global strategy, design and technology consulting company specializing in capital markets and finance. We help our clients realize rapid transformational change at an industrial scale assisting those companies with their most key strategic business programs and technology investments.

Lab49’s Data Practice partners with clients to help understand the potential of their data assets and create and implement compelling products to realize that potential. We take a consultative and domain-led approach to understanding our clients’ businesses, and apply advanced data science and machine learning techniques to create next generation solutions using cutting edge technologies."
185,Data Engineer,"New York, NY",New York,NY,None Found,None Found,"3+ years of Software Engineering experience
2+ years experience work with real-time/streaming data
Experience with a RDBMS (e.g. MySQL, PostgreSQL)
Experience with real time data streaming tools (e.g. Apache Kafka, AWS Kinesis)
Experience working with an OLAP or Time Series Databases (e.g. Druid)
Experience with a data processing solution. (e.g. AWS Athena, Apache Spark)
",None Found,None Found,None Found,None Found,"Who We Are
Fairygodboss is an early-stage start-up based in New York. Our mission is to improve the workplace for women by creating transparency. We do this by creating a safe, anonymous and supportive place for women to leave job reviews and insight into compensation, benefits and culture. Our platform reaches over 3 million users every month and is sponsored by over 80 top employers. We are looking to double our technical team in 2019.
What You Will Be Doing

We are currently rebuilding our platform and looking to bring on an experienced data engineer to:
Develop personalization and recommendation engines used to tailor unique experiences and results on a user-by-user basis for each of our various projects.
Build out and support our Real-time and Batch processing pipelines.
Configure and monitor our event logs and analytical data stores.
Configure and maintain our BI tools and reporting libraries.

Qualifications
The ideal candidate should have experience with real-time user data processing and storage. The candidate will also have experience working with stakeholders to identify important metrics that can then be incorporated into our APIs for both internal and external use.
Required:

3+ years of Software Engineering experience
2+ years experience work with real-time/streaming data
Experience with a RDBMS (e.g. MySQL, PostgreSQL)
Experience with real time data streaming tools (e.g. Apache Kafka, AWS Kinesis)
Experience working with an OLAP or Time Series Databases (e.g. Druid)
Experience with a data processing solution. (e.g. AWS Athena, Apache Spark)

Nice To Have:
Knowledge of JavaScript & Node.js
Experience building APIs
Experience building personalization or recommendation engines
Experience with Machine Learning/Data Science

Other Tools We Use:
AWS
Docker & Container Orchestration Tools
Druid & Kinesis

Compensation:
Highly competitive and commensurate with experience.

Company Description:
Who We Are:

Fairygodboss is an early-stage start-up based in New York. Our mission is to improve the workplace for women by creating transparency. We do this by creating a safe, anonymous and supportive place for women to leave job reviews and tips about employer pay, benefits and culture. We’re growing rapidly and expanding our team.
If you're interested in learning the ins and outs of running a digital startup and improving the world for women at the same time, this job is for you."
186,"Principal Data Engineer, AdSmart","New York, NY",New York,NY,None Found,None Found,None Found,None Found,"Serve as a senior data engineer for audience studio data products.Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and SRAT leadershipDevelop and code the data management services that is core to Audience Studio, under the leadership of the VP ArchitectureSupport product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership",None Found,None Found,"As a member of the Product Engineering Team, the Principal, Data Engineer, AdSmart will be directly responsible for data design, management, and development as part of building out the necessary platform and products for NBCUniversal’s AdSmart . NBCUniversal’s audience management products will enable NBCUniversal to better understand its brand’s audiences such as NBC News, Bravo, The Tonight Show, Saturday Night Live, and USA Network as well as audiences that cross brands. The goal is to ensure we know who is watching what, where and when. In turn enabling NBCUniversal’s sales teams to properly align our audiences with the market advertisements that can benefit them the most.

You’re a big thinker who can analyze and evangelize a long range opportunity, architect a ground breaking solution, and roll-up your sleeves to get code out the door when needed. You are data driven and analytical. You understand the concept of a value proposition and evaluation criteria, and you know how to align them with low level milestones to get the work done. You can apply domain knowledge from one technical subject, in order to quickly ramp and deliver on a new one. You know how to learn from failure until you succeed, and you are able to articulate and quantify the reasons for your decisions.

You will be part of the AdSmart's Data Engineering team, participating in the data architecture that will drive both current and future data management initiatives within NBCUniversal’s Audience Studio group.

Responsibilities
Serve as a senior data engineer for audience studio data products.Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and SRAT leadershipDevelop and code the data management services that is core to Audience Studio, under the leadership of the VP ArchitectureSupport product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership
Qualifications/Requirements
Bachelor’s degree in Computer Science or related field
5+ years of software development experience, as a developer or manager
Fluency in Scala and/or Java programming languages
Strong OO & FP design patterns, data structure, and algorithm design skills
Extensive experience developing Apache Spark applications
2+ years of experience with both relational database design (SQL), non-relational (NoSQL) databases, big data, real-time technologies
Familiar with various cloud data sources and architectures such as AWS/S3, HDFS, Kafka
Experience with software containerization, such as Docker
Experience developing and / or consuming web interfaces (REST API) and associated skills (HTTP, web services)
Self-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment

Additional Job Requirements:
Interested candidate must submit a resume/CV through www.nbcunicareers.com to be considered
Must be willing to work in New York, NY
Desired Characteristics
Experience as a development manager (with direct authority over development staff)
Experience with Cluster Management and Container Orchestration technologies such as Mesos, Kubernetes, Hadoop/Yarn
Experience with Apache Kafka or similar streaming technologies
Experience with digital advertising technologies.
Able and eager to learn new technologies
Able to easily transition between high-level strategy and day-to-day implementation
Excellent teamwork and collaboration skills
Results oriented, high energy, self-motivated
Sub-BusinessTechnology
Career Level
Experienced
CityNew York
State/Province
New York
CountryUnited States
About Us
At NBCUniversal, we believe in the talent of our people. It’s our passion and commitment to excellence that drives NBCU’s vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It’s what makes us uniquely NBCU. Here you can create the extraordinary. Join us.
Notices
NBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable."
187,Informatica Engineer,"New York, NY 10011",New York,NY,10011,None Found,"Minimum of 1 year of Experience developing and implementing Informatica Powercenter or Informatica Data Quality
Bachelor’s Degree or Associate’s Degree with 6 years of work experience or equivalent work experience of 12 years",None Found,None Found,None Found,"
It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve clients, our employees must be able to travel when needed. This role requires 100% flexibility to travel and work onsite with clients (typically Monday through Thursday).
Proven success in contributing to a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills","Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements and the way we collaborate, operate and deliver value provides an unparalleled opportunity to grow and advance. Choose Accenture, and make delivering innovative work part of your extraordinary career.


As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!


 Why Should I Join the Accenture Team?
Drive innovation. People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward.Learn and grow continuously: Follow personalized training to build skills, while expanding your experience defining and implementing solutions on complex client projects with a scope that is unsurpassed in industry.Thrive in our inclusive environment: Bring your whole self to a company that aims to be the most diverse in the world and delivers real-time performance feedback based on individual strengths—not stats.


Job Description
Informatica Data and Analytics professionals define strategies, develop and deliver solutions that enable the collection, processing and management of information from one or more sources, and the subsequent delivery of information to audiences in support of key business processes.

Produce clean, standards based, modern data mapping with Informatica an emphasis on advocacy toward end-users to produce high quality software designs that are well-documented.
Demonstrate an understanding of technology and digital frameworks in the context of data integration with Informatica and other technologies.
Ensure SQL code and design quality through the execution of test plans and assist in development of standards, methodology and repeatable processes, working closely with internal and external design, business, and technical counterparts.

Basic Qualifications
Minimum of 1 year of Experience developing and implementing Informatica Powercenter or Informatica Data Quality
Bachelor’s Degree or Associate’s Degree with 6 years of work experience or equivalent work experience of 12 years
Preferred Qualifications
Experience with one or more in addition to Informatica ETL tools, including Business Objects Data Services (BODS), DataStage, Ab Initio, Talend, and Pentaho
Experience with a full life-cycle development from functional design to deployment
Database experience (Teradata, Oracle, SQL Server, DB2, Azure SQL)
Strong knowledge and experience of SQL
Understanding of Entity relationship data models and Dimensional Models
Experience with development and production support
Professional Skill Requirements

It is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve clients, our employees must be able to travel when needed. This role requires 100% flexibility to travel and work onsite with clients (typically Monday through Thursday).
Proven success in contributing to a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills

All of our consulting professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.


Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or"
188,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"This Data Engineer (DE) will work as an individual contributor and technical lead as a member of the CDO (Chief Data Officer) team. Core responsibilities will be creating data pipeline and big data warehousing solution for TickPick. the data engineer will be responsible to develop, construct, tests and maintains solutions, such as databases and large-scale data processing systems. Specific responsibilities include:
 Ensure architecture will support business requirements

 Discover opportunities for data acquisition and implement solutions
 Develop production processes and solutions to model, mine, and surface data
 Integrate production systems, employing a variety of languages and tools
 Improve and ensure data reliability, quality, and efficiency
 Work with other members of the data team and beyond, including data analysts and data scientists
 Challenge the ethos of TickPick with out of the box thinking to further enhance the capabilities and solutions
 Grow and maintain a deep knowledge of the business along with stakeholder needs and constraints
 Work with senior team to review and improve agile delivery and engineering practices
 Drive innovative ideas, solutions and products through thought leadership and decisive action
 Advance the engineering culture of TickPick
 Maintain strong relationships with business and engineering teams to ensure product goals are met
Requirements
TECHNICAL SKILLS:
 Experience with latest NoSQL and Big Data Technologies as well as RDBMS
 Experience with Microservices architecture and solutions
 Experience with latest Cloud technologies with focus on container-based solutions
 Extensive knowledge of programming and scripting languages, such as Java, Bash, C++, Phyton, R, Spark, Hive
 Experience with object-oriented design, coding and testing patterns
 Experience in engineering software platforms and large-scale data solutions
 Capability to architect highly scalable distributed systems, using different open source tools.
 Experience implementing high-performance algorithms.

POSSIBLE INITIATIVES:
 Create primary cloud-based data platform for TickPick
 Centralize data ingestion and data pipelining
 Automate analytical data reporting capabilities
 Publish data services to support data-driven marketplace operation and marketing efforts
 Deepen data integration for large marketing partnership opportunities (FanDuel, WeFest, RiotFest etc.).
 CRM integration and automation

KEY SELECTION CRITERIA
5+ years of experience in software and data engineering with roles in a comparable company at a comparable scale. Prior experience in a larger company is not required, but an environment where one is schooled in proper systems, processes and procedures is preferred. Proof of points transitioning into and successfully working in a smaller, faster-paced, more hands-on, and entrepreneurial environment is a plus. Hands-on experience technology delivery is required. Well-developed skills in working with peers in a similar-sized environment are essential. The DE needs to be an adept communicator with both technical and non-technical stakeholders, including external parties as required. The successful candidate will be hands-on; intellectually curious; versatile; creative and resourceful; and will demonstrate a bias for action and a commitment to continuous process improvement.

CRITICAL COMPETENCIES
 Leadership & Influencing Skills — Demonstrates stellar leadership skills working with engineering team and
stakeholders; effectively leads technology delivery functions; acts as a key member of the company’s engineering
team in all technical matters.
 Strong Analytical Skills — Curious with strong quantitative and qualitative analytic skills; demonstrates a balance
between theoretical and practical solutions and is fact-based in decision-making; synthesizes lots of data derived
from a wide range of sources and is creative in determining appropriate course of action.
 Initiative & Urgency — Knows how to work independently; ensures responsive and timely follow-up; sets and
meets aggressive deadlines; demonstrates a sense of urgency; prioritizes exceptionally well while utilizing excellent
organizational skills.
 Strong Communication Skills — Crisp and effective oral and written communication skills; ability to communicate
at all levels of the organization; builds and maintains strong relationships with peers and prioritizes interaction with
departments and staff across the org.
 Creative Thinker —Enjoys thinking of new ways to solve traditional problems; a visionary at heart who is truly
satisfied when satisfying customers’ needs through creative, thoughtful solutions.

PERSONAL ATTRIBUTES
 Low ego and hands on leader who is proactive in helping advance the organization.
 Dynamic, curious, and flexible as business conditions change.
 Strategic capabilities to think like an owner of the business in terms of technical excellent, especially with a date-driven approach towards the delivery of products and capabilities.
 Adaptable, nimble, resilient and passionate, with a real sense of urgency.
 Sets a high bar for performance in the data organization (and elsewhere), including analytic capability, work ethic, integrity, loyalty and commitment to company values.
 Results-oriented and proactive effectively working with external partners, including customers and partners.
 Superior verbal and written communication skills and strong EQ in building relationships with peers and others.
 Professional style that includes analytic rigor, strategic thinking, and intellectual agility.
Benefits
 Competitive compensation
 Daily lunch stipend
 Medical, dental, and vision plans
 PTO policy
 401K plan
 Fully stocked pantry and keg on site
 Monthly team happy hours
 Summer Fridays
 Work with a fun and fast moving team"
189,Data Engineer I,"Newark, NJ 07102",Newark,NJ,07102,None Found,None Found,None Found,None Found,None Found,None Found,"A Bachelor’s degree or higher in Computer Science, Engineering, Mathematics, Physics, or a related fieldProficiency in SQL and programming languages such as python2+ years of hands on experience in working with data including but not limited to Data Warehouse (DWH) environment with data integration/ETL of large and complex data sets, Data modeling skills such as Star/Snowflake schema design for DWH, or building large scale data-processing systems with experience in Big Data technologies such as MapReduce, Hadoop, Spark, Kafka or AWS equivalents.Familiarity with Database technologies such as AWS Redshift, Oracle, Teradata, or othersFamiliarity with Business Intelligence (BI) and Visualization platforms such as MicroStrategy and AWS QuicksightAbility to communicate effectively and work independently with little supervision to deliver on time quality productsWillingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision

This opportunity is within Audible’s Data Engineering group. The Data Engineering group owns technology platforms and datasets that enable systems and people to uncover new insights and fine-tune operations to meet business goals. We need your help designing and building these.

KEY RESPONSIBILITIES
Apply broad knowledge of technology options, technology platforms, design techniques and approaches across the Data Engineering ecosystem to build systems that meet business needsBuild systems and datasets using software engineering best practices, data management fundamentals, data storage principles, recent advances in distributed systems, and operational excellence best practicesAnalyze source systems, define underlying data sources and transformation requirements, design suitable data models and document the design/specificationsDemonstrate passion for quality and productivity by use of efficient development techniques, standards and guidelinesEffectively communicate with various teams and stakeholders, escalate technical and managerial issues at the right time and resolve conflictsPeer review work. Actively mentor other members of the team, improving their skills, their knowledge of our systems and their ability to get things done
HOW DOES AMAZON FIT IN?
We're a part of Amazon, they are our parent company and it's a great partnership. You'll get to play with all of Amazon's technologies like EC2, SQS and S3 but it doesn't stop there. Audible's built on Amazon technology and you'll have insight into the inner workings of the world's leading ecommerce experience. There's a LOT to learn!

If you want to own and solve problems, work with a creative dynamic team, fail fast in a supportive environment whilst growing your career and working on a platform that powers web applications used by millions of customers worldwide we want to hear from you.

Hands on experience with BI and Visualization platforms such as MicroStrategy and AWS QuicksightExperience with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena
Audible is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age"
190,Data Engineer (AWS),"New York, NY",New York,NY,None Found,None Found,"
Bachelor’s Degree Required: Computer Science or Engineering discipline preferred.
3+ years technology experience working in Software Engineering capacity.
3+ years working in Python (other modern languages considered).
1+ years working within Analytic/Data Warehouse/Data Lake environment.
1+ years working with AWS public cloud (certification a plus).
Expertise in SQL: 10 out of 10, SQL Ninja analytic capabilities.
Strong working knowledge of with Linux Shell.
Understanding of Data Warehouse principles, including Dimensional Modeling.
Creative, flexible, and quick to learn.
Experience with Redshift a plus.",None Found,"
Design, develop, deploy and manage a reliable and scalable data analysis pipeline, using technologies including Python, S3, and Redshift.
Participate in cross-functional initiatives to develop new capabilities, including hands-on development responsibilities.
Ability to integrate data from a variety of sources, assuring they adhere to data quality and accessibility standards.
Document processes and standard operating procedures.
Evaluate and conduct POC’s with new technologies.

",None Found,None Found,"Founded in 2011 by executives at Equinox, Blink Fitness is a premium quality, value-based fitness brand with more than 90 locations open or in development throughout New York, New Jersey, Pennsylvania and California. Blink puts Mood Above Muscle™, which celebrates the positive feeling you get from exercise, not just the physical benefits.
Blink recently launched a franchising system to complement its fast growing company-owned business model and has secured franchise development agreements in Georgia, Illinois, Massachusetts, Michigan, Virginia and upstate NY. This includes an agreement with Golden State Warriors forward Draymond Green, two-time NBA Champion, All Star, Olympic Gold Medalist and Defensive Player of the Year, who announced a franchise development deal to bring 20 gyms to his home state of Michigan and a portion of Illinois.
Blink is an exciting and dynamic business that is still in the start-up mode. We are a passionate team with a great entrepreneurial spirit and a willingness to roll up our sleeves to get the work done.
While Blink has grown rapidly and has already achieved significant profitability, the business is just getting warmed-up. Its leadership has a lofty vision of opening more than 300 locations in the next five years through a combination of company-owned and franchise development.
For more information visit Blink’s consumer website - blinkfitness.com – and its franchise website – blinkfranchising.com.
Job Description:
Blink is seeking a Data Engineering professional to join our technology team. This role is a hands-on engineering position responsible for the build and maintenance of our cutting edge cloud based data platform.
Responsibilities:
Design, develop, deploy and manage a reliable and scalable data analysis pipeline, using technologies including Python, S3, and Redshift.
Participate in cross-functional initiatives to develop new capabilities, including hands-on development responsibilities.
Ability to integrate data from a variety of sources, assuring they adhere to data quality and accessibility standards.
Document processes and standard operating procedures.
Evaluate and conduct POC’s with new technologies.

Qualifications:
Bachelor’s Degree Required: Computer Science or Engineering discipline preferred.
3+ years technology experience working in Software Engineering capacity.
3+ years working in Python (other modern languages considered).
1+ years working within Analytic/Data Warehouse/Data Lake environment.
1+ years working with AWS public cloud (certification a plus).
Expertise in SQL: 10 out of 10, SQL Ninja analytic capabilities.
Strong working knowledge of with Linux Shell.
Understanding of Data Warehouse principles, including Dimensional Modeling.
Creative, flexible, and quick to learn.
Experience with Redshift a plus.
Compensation and Benefits:
Competitive Base Salary
Complimentary Blink membership
Comprehensive benefits package
And more"
191,Azure Data Engineer,"New York, NY 10011",New York,NY,10011,None Found,"At least 5 years of consulting or client service delivery experience on Azure
",DevOps on an Azure platform,None Found,None Found," Proven ability to build, manage and foster a team-oriented environment
","Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
192,Sr. Data Engineer (Consultant),"New York, NY 10017",New York,NY,10017,None Found,None Found,None Found,None Found,None Found,None Found,"Sr. Data Engineer (Consultant) - 170k+ Total compensation - New York

Who is hiring?

We are currently working with a leading Data Consulting client in the US thatspecializes in all things data and analytics including big data, modern data architecture, cloud migration, enterprise data management, business intelligence, data visualization, advanced analytics, and machine learning.

They seek to hire a Senior Data Engineer to assist them in building an AWS Cloud-based solution with Python, PySpark Data pipelines for a recommendation engine that they are building for one of their Fitness clients in Manhattan.

This recommendation engine will be taking info from user devices and pushing them into their proprietary platform in order to make fitness suggestions for their clients. This will be a brand new product offering, that will need to be user-friendly and drive more users to the application.

What will you be doing?

This will require a background in:

Knowledge of Data Warehousing and SQL
Experience working with Python, Spark & PySpark
Some Experience working with Software Development
Experience with AWS Redshift and RDS or similar
Experience with ETL and Business Intelligence tools
Knowledge of data modeling, data access, and data storage techniques

You will be working with the following technologies to successfully build this platform:

Python
AWS
API Development
ETL Integration with Python
Code intensive environment

Why you shouldn’t miss this opportunity?
As a Data Engineer at this client, you’ll work in small teams to deliver innovative solutions using core cloud data warehouse tools and Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you’ll be working with some of the most forward-thinking organizations in data and analytics.

Contract to hire - Salary Range - 130 - 170k+"
193,Google Certified Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Mediaagility

MediaAgility is a Digital Consulting company and a Google Cloud Premier partner. The company is headquartered in Princeton, NJ with offices in US, UK, Mexico, India and Singapore. Our mission is to create customer-centric intelligence to help innovators succeed. We do this through a unique BrainTrust engagement model, Agile development and deep expertise on latest technologies.

We offer full spectrum digital consulting and bring our customers from idea to impact with ‘Strategic Thought Partnership’ to bring the best thinking from across our company, partners like Google and industry experts. We build ‘Intelligence Solutions’ with our years of consulting experience packaged into productized solution patterns to address business challenges. Our focus is on Analytics/ML, Location Intelligence, Conversational Intelligence and Modernising applications / data warehouses. We have helped over a thousand customers across nine countries.


Job Description

Design and Develop Analytics and Machine Learning implementations using Python, numpy, scipy, scikit-learn, pandas, Tensorflow, Keras, PyTorch etc.

Lead the efforts in building end to end streaming and batch data analytics pipelines. From data ingestion, processing, storage, analysis, machine-learning to visualization. Understand big-data principles and best practices. Deliver projects in data analytics, machine learning and AI.

Lead the efforts in building Industry specific Machine Learning and Deep Learning models on structured and unstructured data. Work towards creating IP.

Design architectures, publish reference code and establish data structure design based on business requirements. Should be pretty hands on.

Perform code reviews, ensure code quality and encourage a culture of excellence.

Be a front face of the company in front of customers and prospects.


Required Skills

Mandatory Certification: Google Cloud Certified Professional Data Engineer

Strong analytical skills.

3+ years of strong technology experience in the field of Big Data, Data Analytics, Data Science or Machine Learning.

Machine Learning Experience- Should have experience with training custom models with Python, numpy, scipy, scikit-learn, pandas, Tensorflow, Keras, PyTorch etc.

Big Data experience - should have experience in building end to end data pipelines with products like Apache Beam, Kafka, Spark etc.

Cloud Experience- Should have experience with GCP data products like BigQuery, DataFlow, Firestore, CloudSQL etc.

Should understand and be able to command architecture design for Data Analytics and Machine Learning systems.

Experience and solid knowledge in Agile (Scrum) Methodologies

Working Knowledge of BI & visualization tools like Google Data Studio, Power-BI, Qlik Sense, Micro-strategy etc

Familiarity with standard source repositories (GIT, BitBucket)

Excellent communication skills"
194,Software Engineer - Big Data,"New York, NY 10001",New York,NY,10001,None Found,None Found,None Found,None Found,None Found,None Found,"Our Consumer & Community Banking Group depends on innovators like you to serve nearly 66 million consumers and over 4 million small businesses, municipalities and non-profits. You’ll support the delivery of award winning tools and services that cover everything from personal and small business banking as well as lending, mortgages, credit cards, payments, auto finance and investment advice. This group is also focused on developing and delivering cutting edged mobile applications, digital experiences and next generation banking technology solutions to better serve our clients and customers.

As an experienced Big Data Engineer, your mission is to help lead our team of innovators and technologists toward creating next-level solutions that improve the way our business is run. Your deep knowledge of design, analytics, development, coding, testing and application programming will help your team raise their game, meeting your standards, as well as satisfying both business and functional requirements. Your expertise in various technology domains will be counted on to set strategic direction and solve complex and mission critical problems, internally and externally. Your quest to embracing leading-edge technologies and methodologies inspires your team to follow suit. And best of all, you’ll be able to harness massive amounts of brainpower through our global network of technologists from around the world.

Bachelor's Degree or better in Engineering, Computer Science or Information Technology
Cloud Computing - AWS
BigData, Hadoop, Hive
ODS - Cassandra
Java 8 / Spark
ETL - Talend
Real time messaging – Kafka / Java on Gaia Application Platform
Exposure to Machine Learning will be a big +
3+ years of experience in middle-tier/backend systems development in Java/Linux
Big data background with experience designing and implementing large scale systems
Working experience with Hadoop, Enterprise Java development, NoSQL data platforms (Cassandra), Pub/sub messaging (Rendezvous, AMPS, Kafka, etc.), Stream processing (Storm, Hbase, Nifi, Spark Streaming, etc.), Batch Processing with tools such as Talend, Informatica, or Hive/SQL and Visualization with Tableau.
Extensive experience with horizontally scalable and highly available system design and implementation, with focus on performance and resiliency
Extensive experience profiling, debugging, and performance tuning complex distributed systems
Willingness to commit extra effort to meet deadlines as required on a high profile and business critical project"
195,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Requisition no: 503035
Work type: Full Time
Location: Medical Center
School/Department: Biomedical Informatics
Grade: Grade 105
Categories: Information Technology, Research (Lab and Non-Lab)
Position Summary

The Department of Biomedical Informatics (DBMI) at Columbia University is revolutionizing the clinical research enterprise with the help of information technology. At DBMI, we are building the infrastructure of the future to support and enable better research and dissemination. We have an immediate opening for a talented and self-motivated data engineer developer who can succeed in a collaborative work environment. The ideal candidate will have experience with data pipelines and cloud environments. The candidate will be responsible for data processing, data exchange/transfer/load (ETL), data visualization, DevOps, and software architecture. The ideal candidate will have professional experience in a number of programming languages, databases, and development environments. The candidate should be able to contribute to improving the reliability and quality of data. Experience in clinical medicine, clinical vocabulary, and cloud development are not required but preferred. The successful candidate will contribute to the development of open source solutions together with a community of international researchers.

Current available position is grant-funded.

Columbia University's Department of Biomedical Informatics is internationally recognized as one of the best programs of its kind. Our mission is to improve health for society by focusing on discovery and impact: we develop new informatics methods, enrich the biomedical knowledge base, and enhance the health of the population.

Responsibilities

1. Software and system design, implementation, and testing (75%)

2. Application deployment and configuration (10%)

3. Communicate with technical individuals at various grant sites (10%)

4. Software requirements specification (5%)

Minimum Qualifications

Bachelor's degree or equivalent in education and experience (computer science, biomedical informatics, information science), plus four years of related experience.

Other Requirements

Great communication skills; Experience with one or more compiled programming languages (e.g. Java, Scala, C#, C++, etc.) and one or more interpreted programming languages (Python, JavaScript, Perl, bash, etc.)

Working knowledge of SQL; Experience with big data, NoSQL databases, and health care data a plus.

Equal Opportunity Employer / Disability / Veteran

Columbia University is committed to the hiring of qualified local residents.

Applications open: Sep 12 2019 Eastern Daylight Time
Applications close:"
196,Data Engineer – Customer Support Experience,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for a Data Engineer to join Spotify’s Customer Support Systems & Technology R&D department. Our mission is to amplify the voice of the customer to Spotify, while creating effortless and proactive digital help experiences – when and where users need it. We believe by achieving this mission we are enabling support to be a differentiator for Spotify and increasing our competitive advantage.
You will collaborate with highly skilled researchers, engineers, data scientists, product managers, and others in a fast-paced, multi-disciplinary environment. Above all, you will have a hand in impacting brand loyalty and improving the overall Spotify experience.

As a part of our data engineering team, you will be creating data sets that can be easily consumed and analyzed to understand customer pain points, and thereby improving the Spotify product family in turn. You will be instrumental in our journey to transition our systems from traditional databases to distributed data sets that can be easily used to gather insights about our products by data scientists and analysts across the company. This includes, but is not limited to:

Build large-scale batch and real-time data pipelines with data processing frameworks like Scio, Beam, DataFlow on the Google Cloud Platform.
Use best practices in continuous integration and delivery.
Help drive optimization, testing and tooling to improve data quality.
Collaborate with other software engineers, ML experts, and stakeholders, taking learning and leadership opportunities that will arise every single day.
Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.
Who You Are
You have 3+ years experience in a hands-on role
You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, Apache Beam, Spark.
You know how to write distributed, high-volume data processing systems in Java or Scala.
You have experience working with Python, the more the better.
You are knowledgeable about data modeling, data access, and data storage techniques.
You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
You understand the value of partnership within teams.
You understand that product value is a function of usefulness and not technical implementation details.
You are willing to work from our awesome office in New York. We offer relocation packages if you do not currently live in New York."
197,"Data Engineer, Measurement Program, YouTube","New York, NY",New York,NY,None Found,None Found,None Found,None Found,"
Manage partner technical integration projects and ensure the prompt and proper resolution of technical challenges.
Develop and maintain third-party data validation methodologies, including building and maintaining automated and scalable technical infrastructure.
Guarantee the technical aspects of a partner’s integration (both new and ongoing) by providing technical guidance and documentation.
Identify, drive, and optimize new third-party reporting opportunities by leveraging YouTube technologies.
Write and maintain lines of code (Python, C++, etc.) to support your own small to medium scale Extract, Transform, Load (ETL) pipelines.
",None Found,None Found,"Minimum qualifications:

Bachelor's degree in Computer Science, or a related technical field, or equivalent practical experience.
3 years of experience with ETL and SQL.
2 years of experience working with one or more of the following: C/C++, Java, Go, Python, Unix/Linux systems.
Scripting experience in Shell, Perl or Python.

Preferred qualifications:

MBA, Master's degree or PhD.
2 years of work experience in a client-facing role.
Experience in driving highly cross-functional initiatives that range from structured project management to ambiguous thought leadership.
Experience in successfully navigating large organizations in order to complete both individual and collaborative projects.
Excellent data management, quantitative, and qualitative skills.
About the job
YouTube's Technology Solutions Organization is a global organization dedicated to developing and managing the company's largest and most strategic partnerships. We work closely with the YouTube product, engineering, and content teams to address our partners' most pressing and complex technology challenges. As a Partner Technology Manager, you'll lead deployments, optimize implementations, and handle integrations to build strong, successful, long-term partnerships.
The YouTube Measurement Program (YTMP) is responsible for ensuring that YouTube is represented fairly in third-party reporting tools, and that internal data feeding those tools is accurate and consistent. We provide partner technical support, analytical services, and thought leadership to several initiatives shaping the long-term direction of YouTube.
As a Data Engineer for the YouTube Measurement Program, you will be the integration manager responsible for the success of some of our most important third-party measurement partnerships. You will optimize and scale our validation programs, drive cross functional buy-in on roadmaps, and bring thought leadership to an important and highly visible ecosystem.
At YouTube, we believe that everyone deserves to have a voice, and that the world is a better place when we listen, share, and build community through our stories. We work together to give everyone the power to share their story, explore what they love, and connect with one another in the process. Working at the intersection of cutting-edge technology and boundless creativity, we move at the speed of culture with a shared goal to show people the world. We explore new ideas, solve real problems, and have fun — and we do it all together.
Responsibilities
Manage partner technical integration projects and ensure the prompt and proper resolution of technical challenges.
Develop and maintain third-party data validation methodologies, including building and maintaining automated and scalable technical infrastructure.
Guarantee the technical aspects of a partner’s integration (both new and ongoing) by providing technical guidance and documentation.
Identify, drive, and optimize new third-party reporting opportunities by leveraging YouTube technologies.
Write and maintain lines of code (Python, C++, etc.) to support your own small to medium scale Extract, Transform, Load (ETL) pipelines.
At Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form."
198,Data Engineer,"New York, NY 10010",New York,NY,10010,None Found,None Found,None Found,None Found,None Found,None Found,"dv01 is the world's first end-to-end data management, reporting and analytics platform offering loan level transparency and insight into lending markets, making them more efficient for institutional investors and safer for the world. In a nutshell, we're doing our part to prevent a repeat of 2008.

As the technological hub between lenders and capital markets, dv01 provides all parties with unprecedented data transparency, insight, and analytics. dv01 has integrated data from 16 marketplace lending platforms, including LendingClub, Prosper and SoFi and multiple mortgage servicers. To date, dv01 has provided reporting and analytics on $105 billion of online lending and mortgage loans and $35 billion of securitization coverage.

YOU WILL:
---------

Be at the heart of data processing at dv01. You will own the suite of transformation logic that processes and standardizes our varied input file packages into the dv01 internal data model. You will operate as the bridge between the engineering and finance teams, contributing to a variety of integral processes that drive dv01 on a daily basis. Every new dataset that gets integrated within dv01 will have your fingerprints all over it.

Be an owner of dv01's most valuable asset. You'll own the business logic in our data pipeline, encapsulating all the knowledge we've accumulated across hundreds of datasets. The output from the pipeline powers all of dv01's customer offerings and is critical to the success of our business, so you will be a resource to both our clients and all the other teams within dv01 who harness our internal data warehouse.

Be customer-facing. You have the opportunity to get direct exposure to high-level contacts at hedge funds, banks, and asset originators, providing valuable insights to help them answer complex questions.

Work with state-of-the-art technology. You'll work with popular, modern, and exciting open source technologies like Apache Spark. The skills you develop here will serve you well beyond dv01.

QUALIFICATIONS:
---------------

A well-rounded engineer. You have 3-5+ years of professional programming experience with Apache Spark, Scala, Java, R, or Python. You are able to write thought-out code while accounting for resource and performance constraints and are also capable of performing ad-hoc data investigations with SQL.

Excited about big data. You should have 3-5+ years of professional engineering experience working with large datasets in a distributed data processing framework, with exposure to large datasets related to loan products an added plus. You enjoy working with data, from expressing complex business logic as scalable data processing logic to configuring and debugging intricate big data pipelines. You love the intricate details of a thorough investigation, but also stay aware of the bigger picture while operating across multiple threads of work.

Interest and experience in both engineering and finance. You're looking to grow your skills in both disciplines and are excited about the synergies between finance and technology.

Knowledgeable about consumer credit. You understand how investors evaluate loan portfolios and the complexities of amortization, prepay, and default. You strive to further your knowledge in the credit market.

Undergraduate or graduate degree in Finance, Math, or Engineering. Note that we're not anti dropouts if you're a superstar.

Perks and Benefits:
Almost 100% Paid Benefits (medical/dental/vision)
Monthly Commuter Budget
Daily Lunch and Dinner Allowance
Free Premium Equinox or ClassPass Membership
Unlimited PTO and Remote days
Casual, collaborative culture
Company Outings (Happy Hours, Team Yoga, Book Club, etc.)

To get a better idea of what a year at dv01 looks like, check out our 2018 Year in Review page here: https://dv01.co/2018yearinreview/ ( https://dv01.co/2018yearinreview/ ). If that looks like fun to you, get in touch because we'd love to hear from you.

dv01 is an equal opportunity employer and all qualified applicants and employees will receive consideration for employment opportunities without regard to race, color, religion, creed, sex, sexual orientation, gender identity or expression, age, national origin or ancestry, citizenship, veteran status, membership in the uniformed services, disability, genetic information or any other basis protected by applicable law."
199,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,"
Mastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role
",None Found,None Found,None Found,None Found,"Join SADA as a Sr. Data Engineer!

Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Mastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Hihg
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
200,Senior Data Engineer,"New York, NY 10011",New York,NY,10011,None Found,None Found,None Found,None Found,None Found,None Found,"Help us Build the Future of Money

Gemini Trust Company, LLC (Gemini) is a licensed digital asset exchange and custodian. We built the Gemini platform so customers can buy, sell, and store digital assets (e.g., Bitcoin, Ethereum, and Zcash) in a regulated, secure, and compliant manner.

Digital assets and blockchain technology have the power to transform the world for good. This truth, along with our core values, form the bedrock of our company and culture. At Gemini, no job is too small and no project too big as we endeavor to build the future of money. We are a mission-driven, team-based, inclusive, and determined community of thought leaders who invest in each other and the long game. Join us in our mission!

THE DEPARTMENT: DATA ENGINEERING

THE ROLE: SENIOR DATA ENGINEER

As a member of our data engineering team, you'll shape the way we approach data at Gemini by using your engineering, analytical and communication skills to work with teams across the business. You know how to ask the right questions and are passionate about using data to support and drive informed business decisions. You are ready to roll up your sleeves and are excited to take on challenging opportunities and projects. You'll mentor data engineers and analysts and guide our internal teams to use data to improve the product and achieve KPIs. Communicating your insights with leaders across the organization is paramount to success.

RESPONSIBILITIES:

Design, architect and implement best-in-class Data Warehousing and reporting solutions
Lead and participate in design discussions and meetings
Mentor data engineers and analysts
Design, automate, build, and launch scalable, efficient and reliable data pipelines into production
Build real-time data and reporting solutions
Design, build and enhance dimensional models for Data Warehouse and BI solutions
Research new tools and technologies to improve existing processes
Develop new systems and tools to enable the teams to consume and understand data more intuitively
Partner with engineers, project managers, and analysts to deliver insights to the business
Perform root cause analysis and resolve production and data issues
Create test plans, test scripts and perform data validation
Tune SQL queries, reports and ETL pipelines
Build and maintain data dictionary and process documentation

MINIMUM QUALIFICATIONS:

7+ years experience in data engineering with data warehouse technologies
7+ years experience in custom ETL design, implementation and maintenance
7+ years experience with schema design and dimensional data modeling
Experience building real-time data solutions and processes
Experience building and integrating web analytics solutions
Advanced SQL skills is a must
Skilled in programming languages Python and/or Java
Experience with one or more MPP databases(Redshift, Bigquery, Snowflake, etc)
Experience with one or more ETL tools(Informatica, Pentaho, SSIS, Alooma, etc)
Strong computer science fundamentals including data structures and algorithms
Strong software engineering skills in any server side language, preferable Python
Experienced in working collaboratively across different teams and departments
Strong technical and business communication

PREFERRED QUALIFICATIONS:

Kafka, HDFS, Hive, Cloud computing, machine learning, text analysis, NLP & Web development experience is a plus
NoSQL experience a plus
Experience with Continuous integration and deployment
Knowledge and experience of financial markets, banking or exchanges

It Pays to Work Here

We take a holistic approach to compensation at Gemini, which includes:


Competitive base salaries across all departments
Ownership in the company via profit sharing units
Amazing benefits, 401k match contribution, and flexible hours
Snacks, Perks, Wellness Outings & Events

Gemini is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you have a disability or special need that requires accommodation, please let us know."
201,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"114 5th Ave (22114), United States of America, New York, New York

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Senior Data Engineer

Being Capital One Tech:
At Capital One, we consider ourselves the bank a technology company would build. We’re delivering best-in-class innovation so that our 65 million customers - and counting - can manage their finances with ease. Our reality and vision empower our engineers to use artificial intelligence and machine learning to transform real-time data, software, and algorithms into financial clarity.

We’re all-in on the cloud and a leader in the adoption of open source, RESTful APIs, microservices, and containers. We build our own products and release them with a speed and agility that allows us to get new customer experiences to market quickly. We’re going boldly where no bank has gone before. And, as a founder-led company, we’re inspired and empowered to make, break, do, and do good . So, let’s do something great together.

Your #LifeatCapitalOne

Looking to work somewhere with the flexibility of a start-up but the financial muscle of a Top-10 bank? You’re in the right place! And here’s what that means for you…

You'll have a flexible work schedule—we want to understand where and when you're at your best so you have a healthy work-life balance. Diversity and Inclusion are cultural norms here—you’ll have access to active local chapters of Women in Tech, Blacks in Tech, and Hispanics in Tech and more. Plus, you’ll be given time to support the next generation of technologists by volunteering with youth programs like Capital One Coders - our engineer-led experience that teaches middle school students in underserved communities how to code. Want to learn more? See what our associates are up to at #LifeatCapitalOne !

Calling All Senior Data Engineers:
A hub for innovation, our New York presence is expanding, and we need Senior Data Engineers who know their stuff to join our team. As a Capital One Data Engineer , you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One. You’ll work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems. You’ll collaborate with digital product managers, and deliver robust cloud-based solutions to drive powerful experiences that help millions of Americans achieve financial empowerment. Want to learn more? Check out the low-down on our high-tech .

Who You Are:
You are fun to work with – you’re excited by a team environment

You are curious. You like to learn new technologies , and you adapt well to change

You are passionate about current state-of-the-art software technologies and tools, with experience implementing them effectively

You are excited about working with cloud-native stack, building on AWS using technologies like Kubernetes and Serverless

You possess a sense of intellectual curiosity and a burning desire to learn

You are motivated and actively looking for ways to contribute

You are passionately focused on the customer and the details that make their experience exceptional

You value data and truth over ego

You possess a strong sense of engineering craftsmanship, take pride in your code

You’re pragmatic - you make the best use of time and resources to find the simplest workable solution

You think and act like an owner, taking personal responsibility for both team and product success

You possess great communication and reasoning skills, including the ability to influence and make a strong case for technology choices

You thrive in collaborative agile teams and are ready to take on new and unexpected challenges while building the next wave of engineering solutions

What You’ll Own:
Collaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies

Leading the craftsmanship, security, availability, resilience, and scalability of your solutions

Bringing a passion to stay on top of current trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community

Encouraging innovation, implementation of cutting-edge technologies, outside-of-the-box thinking, teamwork, and self-organization

Assisting in the hiring of top engineering talent and maintaining our commitment to diversity and inclusion

Basic Qualifications:
Bachelor’s Degree

At least 4 years of experience in application development

At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)

Preferred Qualifications:
Master's Degree

5+ years of experience in application development

2+ years of experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink

1+ years of experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service

1+ years of experience with Ansible / Terraform

2+ years of experience with Agile engineering practices

1+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)

1+ years of experience with NoSQL implementation (Mongo, Cassandra)

2+ years of experience developing Java based software solutions

2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)

2+ years of experience developing software solutions to solve complex business problems

2+ years of experience with UNIX/Linux including basic commands and shell scripting

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
202,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Latch's data team is continuing to be built from ground up, and we are still actively hiring. We're covering all aspects of the Latch's data strategy including: analysis, engineering, science, & more. We are firm believers of bringing in smart people that can define the roles for themselves, so come join us and start creating greatness.

Smart access isn't about locking doors, it's about opening up new possibilities. Latch is the world's first fully integrated hardware and software system dedicated to bringing seamless access to every door in a modern building. We're looking for the curious and the creative to join our team and help us change the way we access our most valued spaces.

Responsibilities


Understand the data gathered across the entire Latch organization
Design and implement data pipelines, building scalable and optimized enterprise level data systems
Collaborate with other teams in the company, both engineering and business counterparts
Transform raw data into meaningful sets that are query-able and visualizable.
Work closely with Data Analysts and Data Scientists to implement production ready systems
Be a helping hand with tools used by other teams such as Sales CRMs, Ops Customer Success tools, Marketing automation or Finance ERP. Data from these tools are very important to us.

Requirements


BS in Computer Science, Math, related technical field or equivalent practical experience
3+ years of general software programming experience in Java or similar languages
Excellent grasp of data structures and algorithms
Solid level of understanding in SQL
Knowledge of database technology, schema design, and query optimization techniques
Experience in ETL pipelines and data transformations.
Excellent communication skills

Preferred Qualifications


MS in Computer Science, Mathematics, or related technical field
Experience with Map-Reduce technologies such as Spark or Hadoop.
Understanding of basic data science concepts
Experiencing in productionizing machine learning models.
Acute sense of data analysis: being able to make sense out of many seemingly unrelated data sets.

Founded in 2014, Latch is a venture-backed, high-growth organization that's on a mission to change the way people open, manage, and share their spaces. Today, 1 in 10 new developments in the U.S. depend on our full-building smart access solution to meet the needs of residents and property managers.

We are a team of just over 200 employees, all of whom are passionate self starters with unique backgrounds and unexpected stories. We offer unlimited time off, a competitive health package, and the opportunity to work in a creative, dynamic, and fast-paced office environment. We are located just a quick walk from both Hudson Yards and Penn Station in New York City."
203,Cyber Data Engineer,"Manhattan, NY 10007",Manhattan,NY,10007,None Found,None Found,None Found,None Found,None Found,None Found,"About New York City Cyber Command
NYC Cyber Command was created in 2017 by Executive Order to lead the City’s cyber defense efforts, working across more than 100 agencies and offices to prevent, detect, respond, and recover from cyber threats. NYC Cyber Command is committed to protecting NYC infrastructure and critical systems from malicious attacks through the use of the latest technologies, public-private partnerships, and regular training and exercises for City employees.

Job Description
Data Engineers support the Cybersecurity Data Science team, led by the Cybersecurity Lead Data Scientist;Data Science team analyzes data in order to defend against cyber threats to the City. This team is integral to the defense of the City’s information environment, and works directly with the NYC3 Architecture, Engineering, and Threat Management teams;Data Engineers design, build, implement, and operate systems that ingest, normalize, correlate, analyze, and present cybersecurity relevant data from across the City.


Minimum Qual Requirements

1. A baccalaureate degree, from an accredited college including or supplemented by twenty-four (24) semester credits in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or

2. A four-year high school diploma or its equivalent approved by a State’s department of education or a recognized accrediting organization and three years of satisfactory experience in any of the areas described in “1” above; or

3. Education and/or experience equivalent to “1” or “2”, above. College education may be substituted for up to two years of the required experience in “2” above on the basis that sixty (60) semester credits from an accredited college is equated to one year of experience. In addition, twenty-four (24) credits from an accredited college or graduate school in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or a certificate of at least 625 hours in computer programming from an accredited technical school (post high school), may be substituted for one year of experience.


Preferred Skills

The preferred candidate should possess the following:
Data Engineers have at least 1 year experience (including internships) analyzing large, high velocity, heterogeneous datasets. They are proficient in using cloud-based tools for ingesting, normalizing, analyzing, and presenting data for future engineering solutions and executive decision makers;Possess at least bachelor’s degrees in computer science or information systems and have specializations in mathematics, number theory, applied cryptography, or statistics or relevant experience;Should have strong practical knowledge of data structures, Java, relational database design, and familiarity with system level and distributed programming;Familiarity with Unix scripting, Web development, and automated testing would be highly desirable, but not necessary.


To Apply

Interested applicants with other civil service titles who meet the preferred requirements should also submit a resume for consideration
For City employees, please go to Employee Self Service (ESS), click on Recruiting Activities > Careers, and search for Job ID #407411
For all other applicants, please go to www.nyc.gov/jobs/search and search for Job ID #407411

SUBMISSION OF A RESUME IS NOT A GUARANTEE THAT YOU WILL RECEIVE AN INTERVIEW
APPOINTMENTS ARE SUBJECT TO OVERSIGHT APPROVAL

The Department of Information Technology & Telecommunications and the City of New York are equal opportunity employers.

DoITT participates in E-Verify


Hours/Shift

Day - Due to the necessary technical support duties of this position in a 24/7 operation, candidate may be required to work various shifts such as weekends and/or nights/evenings.


Residency Requirement

New York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview."
204,Data Analyst,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,"
Transform raw data into actionable and meaningful analyses.
Work with a cross-functional team to define and clarify analytic goals, develop analysis plans, and ensure data quality standards.
Identify key metrics and develop dashboards/reports to better the understanding of member marketing solutions.
Work closely with our data engineer(s) to seamlessly integrate data across audience management platforms and integrate learnings with member profiles for actionable next steps.
Help measure and compare the success of marketing metrics across campaigns.
Document data requirements and think through complex problems to make actionable recommendations.
Perform ad-hoc reporting and analyses as needed.",None Found,"
Bachelor or Master's degree in Economics, Statistics, Finance, or Mathematics preferred.
3-5 years of analytics or quantitative experience in which you extracted meaningful insights from big datasets.
Experience with test design, KPI development and post campaign analyses.
Experience working in a company with a subscription based business.
Strong experience with using BI tools to build dynamic/scalable dashboards. Looker experience a plus.
Strong Knowledge of SQL, your queries are optimized for readability and performance. Google BigQuery a plus.
Strong conceptual/analytical/creative thinker – ability to push beyond specific requests and identify the business need in advance of execution.
Ability to handle multiple tasks, good time management and organizational skills.
Excellent communication and organizational skills.","Data Analyst
Company Overview:

WW is looking for candidates to help change people’s lives. We are a global wellness technology company inspiring millions of people to adopt healthy habits for real life. We do this through engaging digital experiences, face-to-face workshops and sustainable programs that encourage people to move more, shift their mindset and eat healthier while enjoying the foods they love. By drawing on over five decades of experience and expertise in behavioral science, we build communities in order to deliver wellness for all.

To learn more about WW and jobs with a purpose, visit ww.com.

Role Overview:
We are now looking for a talented Data Analyst to join our growing Analytics group to help drive a data-first culture across WW.
The data analyst will work closely with both business and technical teams to proactively identify gaps in our knowledge base across member marketing. The candidate will be charged with providing analytics reporting and data-driven strategic insights, trends, and perspective.
You will work closely with a cross functional team through all development stages, from identifying opportunities, to analyzing the impact of efforts, and discovering areas for improvement.
Key Responsibilities:
Transform raw data into actionable and meaningful analyses.
Work with a cross-functional team to define and clarify analytic goals, develop analysis plans, and ensure data quality standards.
Identify key metrics and develop dashboards/reports to better the understanding of member marketing solutions.
Work closely with our data engineer(s) to seamlessly integrate data across audience management platforms and integrate learnings with member profiles for actionable next steps.
Help measure and compare the success of marketing metrics across campaigns.
Document data requirements and think through complex problems to make actionable recommendations.
Perform ad-hoc reporting and analyses as needed.


Goals / Deliverables:

Build segments in audience management platforms for member marketing teams.
Develop test plans and identify KPIs/targeted audiences for testing across member marketing communications.
Provide post test analyses with actionable recommendations and next steps.


Requirements:
Bachelor or Master's degree in Economics, Statistics, Finance, or Mathematics preferred.
3-5 years of analytics or quantitative experience in which you extracted meaningful insights from big datasets.
Experience with test design, KPI development and post campaign analyses.
Experience working in a company with a subscription based business.
Strong experience with using BI tools to build dynamic/scalable dashboards. Looker experience a plus.
Strong Knowledge of SQL, your queries are optimized for readability and performance. Google BigQuery a plus.
Strong conceptual/analytical/creative thinker – ability to push beyond specific requests and identify the business need in advance of execution.
Ability to handle multiple tasks, good time management and organizational skills.
Excellent communication and organizational skills.


As a company, our purpose is to inspire healthy habits for real life. And as an employer, we inspire the greatest people to do their best work. We provide benefits for real life to help protect your health, finances and overall well-being, including:
Competitive compensation and profit-sharing plan
A 401K plan to help you plan for your future, plus company match
Health care coverage starting on your first day
Tuition reimbursement and online courses to help you reach your career aspirations
Commuter benefits
Yearly well-being allowance for your physical, financial, social and emotional well-being
Free WW membership for you plus 3 free WW memberships for your friends and 3 for your family
Free fruit, snacks and coffee to get you through your day
Summer Fridays, happy hours, and company outings
Robust employee referral bonuses
Developmental opportunities and assignments to grow your career
WW is an equal opportunity employer. WW does not discriminate on the basis of sex, race, color, creed, national origin, marital status, age, religion, sexual orientation, gender identity, gender expression, veteran status, or disability.
Any offer of employment is contingent upon the satisfactory results of reference and background checks."
205,Lead Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"X-Mode provides real-time location data and technologies that power location intelligence for advertising and business decisions in financial services, healthcare, high-tech, real-estate, retail, and the public sector. X-Mode's flagship product is a fast-growing big data location platform, which maps daily the precise routes of 10% of the U.S. Population and maps monthly 1 in 3 adult U.S. smartphone users. X-Mode strives to produce and monetize the world's largest location platform and ultimately create a global ""living map"" of 1 billion people with the highest quality location data in order to fuel the best location intelligence business solutions.

X-Mode Social, Inc. is looking for a full-time lead data engineer to work on X-Mode's data platform and join our rapidly growing team. For this position, you can work in either our Reston, VA headquarters or remotely. Our technical staff is scattered across the U.S, so you'll need to be comfortable working remotely. We often use videoconferencing tools (like Slack, Google Meet) to coordinate, as well as Jira for tasking, and Bitbucket for source control. We work in short sprints, and we'll count on you to provide estimates for tasks to be completed and delivered. We're looking to hire someone to start right away! Think you've got what it takes? Apply below!

WHAT YOU'LL DO:
---------------


Use big data technologies, processing frameworks, and platforms to solve complex problems related to location
Build, improve, and maintain data pipelines that ingest billions of data points on a daily basis
Efficiently query data and provide data sets to help Sales and Client Success teams' with any data evaluation requests
Ensure high data quality through analysis, testing, and usage of machine learning algorithms

WHO YOU ARE:
------------


3-5+ years of Spark and Scala experience
Experience working with very large databases and batch processing datasets with hundreds of millions of records
Experience with Hadoop ecosystem, e.g. Spark, Hive, or Presto/Athena
Real-time streaming with Kinesis, Kafka or similar libraries
4+ years working with SQL and relational databases
3+ years working in Amazon Web Services (AWS)
A self-motivated learner who is willing to self-teach
Willing to mentor junior developers
Self-starter who can maintain a team-centered outlook
BONUS: Experience with Python, Machine Learning
BONUS: GIS/Geospatial tools/analysis and any past experience with geolocation data

WHAT WE OFFER:
--------------


Cool people, solving cool problems.
Competitive Salary
Medical, Dental and Vision
15 Days of PTO (Paid Time Off)
We value your input. This is a chance to get in on the ""ground floor"" of a growing company

"
206,Senior Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Hello, World! Codecademy has helped over 45 million people from around the world upgrade their careers with engaging, accessible, and flexible education on programming and data skills. We provide over 200 hands-on interactive lessons ranging from Python to R to Javascript and everything in between. Our learners have gone on to start companies, new jobs, and new lives thanks to what they’ve learned with Codecademy, and we’re thrilled to be working to take that impact to the next level.

Codecademy was started in 2011 by two college students in a dorm room at Columbia that were frustrated by the huge gap between education and employment. A few years later, we are a rapidly growing, diverse team of 75+ in SoHo, NYC. We’ve raised over $40m in venture capital funding from top investors including Union Square Ventures, Kleiner Perkins, Naspers, Y Combinator, and more.

If you want to help build a business that impacts tens of millions of people each year and helps them lead better lives, join us!

Codecademy's Infrastructure & Services team is responsible for the infrastructure and operations to deliver our service to millions of users learning to code. We are looking for a Senior Data Engineer to join this team; in this role you will also work closely with our Data Science team. In this role you will take ownership of our event based architecture and data warehousing. You will also need to assess and devise an effective end to end data infrastructure. The ideal candidate will be comfortable with Amazon Redshift, ETLs, and MongoDB.

WHAT YOU'LL DO
Build scalable data infrastructure solutions.
Design and optimize new and existing data pipelines.
Integrate new data sources into our existing data architecture.
Collaborate with a cross-functional team of software engineers and data scientists.
WHAT YOU'LL NEED
Hands-on experience building and maintaining large scale ETL systems.
Deep understanding of database design and data structures.
Fluency in one of the following languages: Python, Java, Scala.
Experience working with cloud-based data platforms (we use AWS).
SQL and data warehousing skills - able to write clean and efficient queries.
Ability to make pragmatic engineering decisions in a short amount of time
Strong project management skills; a proven ability to gather and translate requirements from stakeholders across functions and teams into tangible results.
WHAT WILL MAKE YOU STAND OUT
Experience with tools in our current warehousing stack: Apache Airflow, Redshift, Segment, Kinesis, S3, Looker.
Familiarity with the database technologies we use in production: MongoDB, PostgreSQL
Comfort with containerization technologies: Docker, Kubernetes, etc.
Experience (or interest in learning to) productionizing machine learning models.
At Codecademy, we are committed to teaching people the skills they need to upgrade their careers. Codecademy aims to educate a richly diverse demographic of users with our product and in order to accomplish this, we believe our team should reflect that rich diversity. Our company celebrates diversity in all of its forms- race, gender, color, national origin, marital status, sexuality, religion, veteran status, age, ability, disability status- and works to create an inclusive workplace where people of all backgrounds and beliefs are empowered to better their futures."
207,Data Engineer,"New York, NY",New York,NY,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We're looking for a developer experienced with data engineering and interested in expanding into new technologies to join our team in New York. You’ll work with our clients to build great products that delight users while using regular investment time to improve yourself, the company, and our community.

About thoughtbot
thoughtbot works with companies in every step of the process to help identify and solve problems. We lead and participate in product design sprints, build high-quality apps, and then deploy them. We use emerging and effective technologies and methods on both internal and client projects. We believe there is always a better way to do our work, and we want to find it and share it with as many people as possible.
Additionally, we maintain an inclusive work environment where everyone can thrive professionally, as well as have full lives outside of work. thoughtbot does not discriminate on the basis of race, sex, color, religion, age, national origin, marital status, disability, veteran status, genetic information, sexual orientation, gender identity, or any other reason prohibited by law in provision of employment opportunities and benefits. We welcome you to apply and let us know if you need any reasonable accommodations during the interview process.
Want to dig deeper? Read more about our Purpose and Values, how we work in our Playbook, or check out this video to hear from our team.
Requirements
thoughtbot data engineers are able to build high-quality, high-throughput data pipelines. Well-qualified candidates have an excellent knowledge of data engineering, including streaming data, distributed data processing, and big data. Experience with Scala and common data stores like Kafka, Cassandra, and ElasticSearch are a plus. Data pipelines will power user interfaces written in Ruby on Rails, Django, and React. Interest and familiarity with those languages is a plus, as is knowledge of building APIs using GraphQL. Being able to contribute to both sides of the API is a huge plus.
Benefits
Our team works in a relaxed and educational environment to develop excellent products for our clients. We work at a sustainable pace of 40 hours per week, consulting for clients four days each week. We dedicate our non-client time to improving ourselves, our communities, and thoughtbot. Everything we do is predicated on having a great team and a culture of growing. We use the latest technologies and are willing to try new methods on both internal and client projects.
INVESTMENT DAYS
We have an investment day each Friday where we learn new tools and techniques, work on open source, create new products, write blog posts, and try to make ourselves, each other, and the community better. If you’ve used our open source libraries, read our blog, attended the local events we host, or seen us speak at conferences then you’ve seen the fruits of investment time.
ALWAYS LEARNING
We have a culture of continuous improvement. Investment days are a critical component in this, but we also offer training and conference benefits. We will cover 100% of all expenses incurred when you speak at a conference and a minimum of 50% of your expenses for any conference or training you attend.
You’ll also enjoy working with and learning from a diverse set of teammates across your client projects and during investment time. We dedicate time each week towards working collaboratively on exciting investment projects, wrapping the year up with a two-day event which our teammates often use to build brand new products in exciting new technologies.
STAY FRESH
We offer 25 paid vacation days and 11 paid holidays per year in addition to 10 paid sick days. New parents receive at least 6 weeks paid parental leave, as well as the ability to take up to 6 months off.
HEALTHCARE + FINANCIAL
We offer a competitive salary and excellent benefits. We pay 100% of medical, dental, vision, and life insurance premiums for full time employees and 90% of medical premiums for dependents. We also offer a comprehensive 401k plan."
