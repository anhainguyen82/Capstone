,Title,Location,City,State,Zip,Country,Qualifications,Skills,Responsibilities,Education,Requirement,FullDescriptions
0,Senior Microsoft Power BI Developer,"Dallas, TX 75202",Dallas,TX,75202,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description:
Applies specialized knowledge in Microsoft Power BI, Flow and Azure Analysis Services to conceptualize, design, develop, unit-test, configure, and implement portions of new or enhanced (upgrades or conversions) business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies.
Responsibilities:
Core team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives
Develop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams
Document requirements and translate into proper system requirements specifications using high-maturity methods, processes and tools.
Develop visualization, user experience and configuration elements of solution design.
Execute and coordinate requirements management and change management processes. Participates as a member of and leads development teams.
Designs units for others.
Completes development to implement complex components.
Designs solutions for others to develop.
Participates in cross-functional teams.
Leads design activities and provides mentoring and guidance to developers.
Designs, prepares and executes unit tests.
Represents team to clients.
Demonstrates technical leadership and exerts influence outside of immediate team.
Develops innovative team solutions to complex problems.
Contributes to strategic direction for teams.
Applies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g. Power BI and Power App development).
Integrates technical expertise and business understanding to create superior solutions for clients.
Consults with team members and other organizations, clients and vendors on complex issues.
Education and Experience Required:
Typically, a technical bachelor’s degree or equivalent experience and a minimum of 8 years of related experience or a master’s degree and a minimum of 6 years of experience.
Knowledge and Skills:
8 or more years’ experience writing code using languages such as (and not limited to) Power BI, Tableau, QlikView, Java, C, C++, C#, VB.Net.
Significant hands on experience in creating and deploying KPI, analytic, and dashboard solutions including design, development, deployment, data engineering, data warehousing, and data management projects and practices.
Advanced understanding of RDBMS databases such SQL Server and Oracle.
Advanced understanding of modern software design and development methodologies.
Experience on multiple full release project life cycles.
Advanced understanding of modern SCM (software configuration management).
Advanced understanding of testing tools and unit test and integration test scripting, and testing methodologies
Advanced experience using an Integrated Development Environment (e.g., Eclipse, Visual Studio) and development of tool add-ins.
Strong understanding of basic Database Administration. Able to define quality and security standards.
Good verbal and written communication and negotiation skills.
General project management/team leader skills.
Ability to work effectively in a globally dispersed team and with clients and vendors.
Demonstrated technical leadership skills.
Business skills - 6+ years of technology services industry and business operations experience in a technology services organization
Data engineering, analytics and systems subject matter expert on financial, workforce and operational systems for technology services business desired.
Experience in multiple solution development methodologies and participation in a fast paced, Dev/Ops environment
Drives the construction of highly innovative statistical and financial models to analyze new aspects of business performance.
Establishes the metrics required to measure business performance and recommends the go-forward strategy to address performance gaps."
1,"Data Strategy Specialist - Business & Data Analysis, Cloud, AWS, Azure, Big Data","Dallas, TX",Dallas,TX,None Found,None Found,None Found, 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:,None Found,None Found,None Found,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The North America Data Strategy & Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.

 Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model

Required Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:
o Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling
o Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.
o Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel
 Able to travel up to 100% (Mon-Thu)

Optional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model
 Experience in compiling business cases and roadmaps for data, analytics and technology investments

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
2,Data Engineer,"Allen, TX",Allen,TX,None Found,None Found,None Found,"5 years of working with following technology stack: Core languages are Java and C#; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, and AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, and SQS.
Growing track record of success or the groundwork to be an impactful member of the team. We’re looking for candidates that exhibit many of the following skills/attributes:
Strong Educational Background
Hands-on Engineering experience in
Problem solving and debugging skillsWriting and deploying code the Linux, Windows, or cloud environmentsFamiliarity with algorithms and performance analysisWillingness to contribute to the operational responsibility of the team’s applications
Some experience with one or more of the following:
Relational Databases & SQL NoSQL databases (Cassandra, Redis, DynamoDB, MongoDB)Big Data tools such as Hadoop, Hive, EMR, Storm, Spark, DynamoDB, HBase",None Found,None Found,None Found,"The engineering team consists of talented, team-oriented individuals who are empowered to take advantage of the latest cloud and distributed technologies to deliver reliable, high-throughput applications.

As a Data Engineer, you’ll employ your skills on a daily basis to design and build data processing and storage applications to handle millions of transactions per day. You will analyze business requirements and consult with the broader team to ensure successful processing, storage and reporting of our Big Data. You’ll have a wide variety of languages and technologies at your disposal that you can use to solve problems. Your work will directly shape and create our data architecture to ultimately deliver systems that stand up to unpredictable environments at massive scale.

Technical Skills Needed:
5 years of working with following technology stack: Core languages are Java and C#; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, and AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, and SQS.
Growing track record of success or the groundwork to be an impactful member of the team. We’re looking for candidates that exhibit many of the following skills/attributes:
Strong Educational Background
Hands-on Engineering experience in
Problem solving and debugging skillsWriting and deploying code the Linux, Windows, or cloud environmentsFamiliarity with algorithms and performance analysisWillingness to contribute to the operational responsibility of the team’s applications
Some experience with one or more of the following:
Relational Databases & SQL NoSQL databases (Cassandra, Redis, DynamoDB, MongoDB)Big Data tools such as Hadoop, Hive, EMR, Storm, Spark, DynamoDB, HBase"
3,Data Engineer-Senior Advisor,"Richardson, TX",Richardson,TX,None Found,None Found,7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.7+ years of SQL development skills within a multi-tier environment are required.,None Found,None Found,None Found,None Found,"The Data Analyst, Senior Advisor will work with data stewards, data owners, master data management analysts, operations teams and IT partners across the organization to drive the enterprise data conformance program. Data research and analysis, cross-functional requirements gathering and documentation and solution development are key aspects of the role. The Data Analyst, Senior Advisor supports reactive data quality by researching and fixing known data issues and proactive data quality by defining the requirements for data controls and enhancements to data capture, monitoring and maintenance processes, procedures and standards.

Primary responsibilities include performing data operations such as conversion, address hygiene, and postal presort, as well as variable document composition and file creation.

Essential Functions:

Data Analysis
Idependently design, develop and implement data integration solutions that support our platforms resiliency, stability, and supportability using a variety of ETL and database technologies.Rapidly develop and refine data integration solutions using Infosphere Datastage, SQL, FastTrack, or other technologies.Experience integrating large structured and unstructured data in multiple format, character sets and delivery methodsWorks with business sponsors, SMES and application teams; to understand the business requirements; analyze and assess availability, quality, and lineage of source system data.Design, map data from source to target and develop data integration solutions that meet business needs.Develop and socialize data integration standards.Partners with other engineers through design reviews, providing feedback on feasibility, scalability, performance and adherence to standards.Partners with business, analysts, BI team, application teams and other stakeholders to design, map data from source to target, develop, test, and implement production data integration solutions that are fully integrated into the Enterprise Data Warehouse.Ensuring model design solves the end users need.Contribute information to the data governance software to improve knowledge downstream.
Data Conformance
Performs analysis on known data quality issues and develops and/or recommends operational or technical solutions for remediation, including development and implementation of automated data quality controls that proactively trigger notifications to process owners when data is out of range. Keeps stakeholders informed of progress and solutions in a timely manner.Autonomously, and proactively performs data profiling to explore data, identify issues and summarize findings.Defines data quality metrics to assess completeness, accuracy, consistency, and conformance to business rules. Designs dashboards to support continuous monitoring and measurement of data quality.Partners with IT to cleanse data to achieve the desired level of data qualityPartners with data stakeholders, process and product owners across the organization to define data standards and communicate changes to data capture procedures, processes, standards, and controls.Cleanses and prepares datasets to be consumed by data scientists and other analystsCollaborates with external teams working on data integration and engineeringAdvocates data governance and hygiene best practicesAssists with scoping and integrating orphan datasets
You will gain hands on experience implementing through embedding standardized data elements in the database or system, employing standardized data elements in an exchange mechanism (usually XML schema), or mapping the application data elements to the standardized elements for purposes of exchange.

Big Data tools:
 Big Data: Hadoop, PIG, Sqoop, Hive and Hcatalog & NoSQL (HBase, Cassandra) , SQL.
 Programming: Scala, Java, Python, Spark

Required Qualifications
7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.7+ years of SQL development skills within a multi-tier environment are required.

Preferred Qualifications
In depth understanding of data integration best practices, leading industry applications and features such as master data management, entity resolution, data quality assessment, metadata management, etc.Expertise in flat file formats, XML within PL/SQL and file format conversion.Exposure to application security technologies and approaches is preferred.Experience processing and parsing CSV, JSON and XML file formatsDatastage, SQL, FastTrack, or other technologiesStrong analytical, debugging and testing skillsSoftware development experience using scripting languages such as JavaScript, Python or RubyProficient using Infosphere/DataStage or equivalent ETL software.Proficient with relational databases and using SQL to query, create tables, views, indexes, joins.Proficient using Unix and applicable scripting/scheduling tools.Experience with Python for data analysis
• Knowledge of clinical and financial Healthcare data • • Knowledge of all data formats (HL7, EDI, CSV, XML, etc)

Education
Bachelor's Degree in Computer Science, Computer Engineering, Computer Information Systems, Information Systems, Management Information Systems or related engineering discipline.

Equivalent work experience will be considered in lieu of degree.

Business Overview
It’s a new day in health care.

Combining CVS Health and Aetna was a transformative moment for our company and our industry, establishing CVS Health as the nation’s premier health innovation company. Through our health services, insurance plans and community pharmacists, we’re pioneering a bold new approach to total health. As a CVS Health colleague, you’ll be at the center of it all.

We offer a diverse work experience that empowers colleagues for career success. In addition to skill and experience, we also seek to attract and retain colleagues whose beliefs and behaviors are in alignment with our core values of collaboration, innovation, caring, integrity and accountability.

CVS Health is an equal opportunity/affirmative action employer. Gender/Ethnicity/Disability/Protected Veteran – we highly value and are committed to all forms of diversity in the workplace. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities. We comply with the laws and regulations set forth in the following EEO is the Law Poster: EEO IS THE LAW and EEO IS THE LAW SUPPLEMENT. We provide reasonable accommodations to qualified individuals with disabilities. If you require assistance to apply for this job, please contact our Advice and Counsel Reasonable Accommodations team. Please note that we only accept applications for employment via this site.

If technical issues are preventing you from applying to a position, contact Kenexa Helpdesk at 1-855-338-5609 or cvshealthsupport@us.ibm.com. For technical issues with the Virtual Job Tryout assessment, contact the Shaker Help Desk at 1-877-987-5352."
4,Senior Microsoft SQL Database Developer,"Dallas, TX 75202",Dallas,TX,75202,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description:
Applies specialized knowledge in Microsoft SQL and Azure technologies to conceptualize, design, develop, unit-test, configure, and implement business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies. Analyze customer information requirements and product specifications to define technical content strategy and plan. Designs and develops written and/or visual product-related information – hard copy, web - (e.g., user/configuration/troubleshooting guides), and online information (interactive demos, help systems) integrated into product, for a variety of audiences (end user, system administrators, internal support engineers, product developers, training developers). Codes, builds, compiles, and tests online information and/or sets-up, loads and tests systems hardware to create information deliverables and provide feedback on ease of use and user interfaces to product development. As customer advocate, helps define/refine product requirements. Develops standards and style documents and templates, scripts, style sheets, and script and graphic libraries to ensure common look and feel. Interfaces with cross-functional areas as a member of the product development team, such as marketing, test, support, and manufacturing.
Core team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives. Design, develop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams. Works with peers outside immediate organization to define and characterize complex technology or process problems and/or develops new solutions, yet works independently to drive technical problems to a solution. Provides advanced technical consulting and advice to proposal efforts, solution design.
Produces strategies which assist company in becoming No. 1 in the market place. Actively participates in company professions program and Practice Improvement activities. Role models knowledge sharing and re-use within practice or profession. Proactively encourages Leads technically significant work on enterprise scale projects. Is recognized by peers as an expert in a particular area of technology. Sustain Architect custom solutions of project and program or operational scope. Architect reusable solutions of project or operational scope. Build custom reusable solutions of project and program or operational scope. Capture and share architectural IP at the project and program level. Assess business impact of specific technologies/strategies Identify and address technical or operational risks. Provide review/input on project activities for medium to large business unit level projects Collaborates with the project manager to develop detailed project plans and work breakdown structures for medium to large business unit level projects.
Education and Experience Required:
Typically, a technical bachelor’s degree in information systems, computer science, or related field and a minimum of 8 years of related experience or a master’s degree and a minimum of 6 years of experience.
Knowledge and Skills:
Expert in Microsoft SQL BI solutions, architecture, design, development, data engineering, data warehousing, and data management. Extensive experience in Kimball method of data warehousing, data models, dimensional maintenance, and TSQL development in high-performance environments required. Experience in SSIS – Particularly using C# scripting object, C#, .Net with MVC, ClickOnce, and TFS or SVN experience
Expert in Microsoft SQL reporting solutions and development experience in T-SQL, stored procedures, complex reporting query development, query optimization, and DevOps
6+ years of technology services and business operations experience in a technology services organization
Data engineering, analytics and systems subject matter expert on financial, workforce and operational systems for technology services business models
Experience in multiple solution development methodologies and participation in a fast paced, Dev/Ops environment
Experience in scorecard, dashboard and business analytics solutions for executive information reporting, analysis and decision support
Excellent analytical thinking, technical analysis, and data manipulation skills.
Ability to leverage new analytical techniques to develop creative approaches and insights.
Can validate/evaluate if an information systems or operational architecture meets business needs.
Be able to evaluate the effect of external factors on designed solution.
Data, analytics and systems subject matter expert on leading financial, workforce and services solutions
Advanced understanding of modern software design and development methodologies. Experience on multiple major full release project life cycles. Advanced understanding of modern SCM (software configuration management). Advanced understanding of testing tools and unit test and integration test scripting, and testing methodologies. Advanced experience using an Integrated Development Environment (e.g., Eclipse, Visual Studio) and development of tool add-ins.
Expert in defining quality and security standards.
Ability to work effectively in a globally dispersed team. High level of technical leadership skills.
Strong verbal and written communication and negotiation skills. Strong project management/team leader skills. Excellent oral and written communication skills for interacting with stakeholders. Strong presentation skills.
Expert analytical skills and the ability to synthesize change quickly using advanced subject and process knowledge. Excellent time management skills and ability to prioritize
Ability to generate original ideas and to bring about their implementation.
Knowledge of and ability to utilize a variety of tools and technologies to support multiple technologies."
5,Azure Data Architect,"Dallas, TX",Dallas,TX,None Found,None Found,"At least 5 years of consulting or client service delivery experience on Azure
",DevOps on an Azure platform,None Found,None Found," Proven ability to build, manage and foster a team-oriented environment
","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Azure Technical Architect is a highly performant Azure Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data solutions on cloud. Using Azure public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today's corporate and emerging digital applications.

Role & Responsibilities:Work with Sales and Bus Dev teams in providing Azure Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Qualifications
Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 10 years of experience in big data, database and data warehouse architecture and delivery
Minimum of 5 years of professional experience in 2 of the following areas:
§ Solution/technical architecture in the cloud
§ Big Data/analytics/information analysis/database management in the cloud
§ IoT/event-driven/microservices in the cloud
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
 - Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
Strong in Power BI, Java, C##, Spark, PySpark, Unix shell/Perl scripting
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
6,Sr. Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,"
5-8 years of Python or Java/J2EE development experience
3+ years of demonstrated technical proficiency with Hadoop and big data projects
5-8 years of demonstrated experience and success in data modeling
Fluent in writing shell scripts [bash, korn]
Writing high-performance, reliable and maintainable code
Ability to write MapReduce jobs
Knowledge of database structures, theories, principles, and practices
Understand how to develop code in an environment secured using a local KDC and OpenLDAP
Familiarity with and implementation knowledge of loading data using Sqoop
Knowledge and ability to implement workflow/schedulers within Oozie
Experience working with AWS components [EC2, S3, SNS, SQS]
Analytical and problem-solving skills, applied to Big Data domain
Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark
Aptitude in multi-threading and concurrency concepts
M.S. in Computer Science or Engineering",None Found,"
Translate complex functional and technical requirements into detailed design
Hadoop technical development and implementation
Loading from disparate data sets by leveraging various big data technology e.g. Kafka
Pre-processing using Hive, Impala, Spark, and Pig
Design and implement data modeling
Maintain security and data privacy in an environment secured using Kerberos and LDAP
High-speed querying using in-memory technologies such as Spark
Following and contributing best engineering practice for source control, release management, deployment etc
Production support, job scheduling/monitoring, ETL data quality, data freshness reporting",None Found,None Found,"Beyondsoft Consulting, Inc., is a leading, technical solutions and consulting partner. We combine emerging technologies and proven methodologies to tailor elegant solutions that solve complex challenges and empower our customers to accelerate their business goals. Our services include end-to-end support for cloud, digital, data analytics, multi-language translation, and testing.

Our client is growing their Data Engineering team within a demanding and well recognized enterprise and information technology company. This role will be the core solution of the Strategic Analytics organization, ensuring both the reliability and applicability of the team’s data products to the organization. This individual will have extensive experience with ETL design, coding, and testing patterns as well as engineering software platforms and large-scale data infrastructures. The Data Engineers will have the capability to architect highly scalable end-to-end pipeline using different open source tools, including building and operationalizing high-performance algorithms. Proven experience with technologies to solve big data problems with expert knowledge in programming languages like Java, Python, Linux, PHP, Hive, Impala, and Spark.
Responsibilities
Responsibilities:

Translate complex functional and technical requirements into detailed design
Hadoop technical development and implementation
Loading from disparate data sets by leveraging various big data technology e.g. Kafka
Pre-processing using Hive, Impala, Spark, and Pig
Design and implement data modeling
Maintain security and data privacy in an environment secured using Kerberos and LDAP
High-speed querying using in-memory technologies such as Spark
Following and contributing best engineering practice for source control, release management, deployment etc
Production support, job scheduling/monitoring, ETL data quality, data freshness reporting
Qualifications
Qualifications:

5-8 years of Python or Java/J2EE development experience
3+ years of demonstrated technical proficiency with Hadoop and big data projects
5-8 years of demonstrated experience and success in data modeling
Fluent in writing shell scripts [bash, korn]
Writing high-performance, reliable and maintainable code
Ability to write MapReduce jobs
Knowledge of database structures, theories, principles, and practices
Understand how to develop code in an environment secured using a local KDC and OpenLDAP
Familiarity with and implementation knowledge of loading data using Sqoop
Knowledge and ability to implement workflow/schedulers within Oozie
Experience working with AWS components [EC2, S3, SNS, SQS]
Analytical and problem-solving skills, applied to Big Data domain
Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark
Aptitude in multi-threading and concurrency concepts
M.S. in Computer Science or Engineering"
7,Big Data Engineer (Kafka),"Allen, TX 75013",Allen,TX,75013,None Found,None Found,None Found,None Found,None Found,None Found,"Experian is seeking a Big Data Engineer to join our CIS Product Delivery team. This is a great opportunity for someone who specializes in Kafka Architecture to enable cloud-based financial services platform to access timely, accurate and relevant data. An ideal candidate will have built real time software services platforms where large volume messaging is core to the solution set.

About Experian

Experian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. For five years in a row, we have been named in the Top 100 “World’s Most Innovative Companies” by Forbes Magazine. With a focus on our employees, we were rated the #1 Top Workplace by the Orange County Register. Experian Consumer Information Services is redefining the way our clients do business within all aspects of the customer credit lifecycle. Fueled by best-in-class data and innovative technology we help businesses make smarter decisions, identify consumers, make decisions on loans, market to prospects and collect.

About this role

As a Big Data Engineer, you must be a messaging expert with extensive, well-rounded background in a diverse set of messaging middleware solutions (commercial, open source, in-house) with in-depth understanding of architectures of such solutions such Kafka. You’ll need an established track record with Kafka technology (administration, configuration, and troubleshooting), with hands-on production experience and a deep understanding of the Kafka architecture and internals of how it works, along with interplay of architectural components such as Kafka Connect, Kafka Streams. Additional responsibilities for this position include:
Design, develop and implement the Kafka ecosystem by creating a framework for leveraging technologies such as Kafka Connect, KStreams/KSQL, Attunity, Schema Registry, and other streaming-oriented technology
Assist in building out the DevOps strategy for hosting and managing our SDP microservice and connector infrastructure in AWS cloud
Strong track record of design/implementing big data technologies around Apache Hadoop, Kafka streaming, No SQL, Java/J2EE and distributed computing platforms in large enterprises where scale and complexity have been tackled.
Proven experience participating in agile development projects for enterprise-level systems component design and implementation
Deep understanding and application of enterprise software design for implementation of data services and middleware.

What your background looks like

5+ years of experience in relevant Streaming/Queueing implementation roles
Bachelor degree in Technical discipline; Masters preferred
Experience in monitoring the health of Kafka cluster (data loss and data lagging) and strategy for short TTD (time to detect) of broker failure and fast TTR (time to recover)
Strong coder who can implement Kafka producers and consumers in various programming languages following the common patterns and best practices
Experience in various integration with Kakfa such as Elastic Search, Databases (RDBMS or NoSQL)
Experience in Spark stream processing is a plus
Experience in RDBMS change log streaming is a plus
Systems integration experience, including design and development of APIs, Adapters, and Connectors and Integration with Hadoop/HDFS, Real-Time Systems, Data Warehouses, and Analytics solutions.
Financial Industry experience preferred

Perks

You’ll be working in a big data analytic incubator with talented, passionate individuals who are focused on architecting world class cutting edge analytics solutions
Four weeks of vacation to start, five sick days and two volunteer days (plus eleven paid holidays)
This is a bonus eligible position with a 15% bonus target
Employee stock purchase program and 401K matching
Wellness initiatives, online discounts, employee discounts, pet insurance and more

EOE including disability/veterans"
8,Tax Services Senior – National Tax – Tax Technology and Transformation (TTT) – Data Scientist – Advanced Technologies - Dallas,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,"Strong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.
Experience with common data science programming languages, such as Python R
Strong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential
Experience with data visualization tools, such as PowerBI, D3.js, etc.
Experience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB
Strong scripting and programming skills
Ownership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.
Ability to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies
Ability to consolidate tax data to make analysis and planning more efficient
Focus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities
Willingness to support project team members in any way needed to help ensure timely completion of deliverables",None Found,None Found,None Found,"Tax Technology and Transformation offers services to companies in response to the impact of existing and emerging technology, including the growing data burden that many businesses face, driving efficiencies to create a cost-effective tax function and the need to understand how to make data an asset. The underlying objective of the combined offerings is to help businesses navigate the digital age of tax transparency alongside new trends in tax compliance and tax audit methods, as well as helping to solve the most pressing challenges that businesses face. Tax Technology and Transformation is composed of the following services:
Digital tax transformation
Tax applications-as-a-service
Tax data and improvement
Tax analytics and reporting enhancement
Emerging tax technology, including robotic process automation (RPA), artificial intelligence (AI), blockchain, cloud solutions, data lake development and business intelligence innovation
Tax technology program mobilization
Custom tax technology application development and deployment
Tax technology strategy and road mapping
Direct and indirect tax systems implementation and configuration
Post-transaction (M&A) tax function operational services
Tax operating model transformation, including process improvement, risk and controls
Tax function assessments
The opportunity

Tax Technology and Transformation is an area that has seen significant growth and investment recently, and you will see that reflected in your experience. It is no exaggeration to say that you will be working on highly publicized projects. The field of taxation is constantly changing as new laws, regulations, and technologies are created, and this is your opportunity to be part of that development.

Key responsibilities

We are looking for an ambitious, self-motivated data scientist or data engineer who will help us discover the information hidden in vast amounts of data, and help us deliver even better products to our clients. Your primary focus will be in applying data mining techniques, doing statistical analysis and building high quantity prediction systems integrated with our products. You will be expected to team on a national and even global scale, so strong communication skills, attention to detail, and ability to effectively drive results are essential.
Selecting features and, building and optimizing classifiers using machine learning techniques
Data mining using state-of-the-art methods
Enhancing data collection procedures to include information that is relevant for building analytic systems
Processing, cleansing and verifying the integrity of data used for analysis
Doing ad-hoc analysis and presenting results in a clear manner

Depending on your unique skills and ambitions, you could be supporting various client projects, ranging from assisting in the production of leading edge machine models, to designing and implementing robust data pipelines that can handle data at a multinational, enterprise scale. Whatever you find yourself doing, you will contribute and help toward developing a highly trained team, all the while handling activities with a focus on quality and commercial value. This is a highly regulated industry, so it is all about maintaining our reputation as trusted advisors by taking on bold initiatives and owning new challenges.

Skills and attributes for success
Strong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.
Experience with common data science programming languages, such as Python R
Strong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential
Experience with data visualization tools, such as PowerBI, D3.js, etc.
Experience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB
Strong scripting and programming skills
Ownership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.
Ability to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies
Ability to consolidate tax data to make analysis and planning more efficient
Focus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities
Willingness to support project team members in any way needed to help ensure timely completion of deliverables
To qualify for the role, you must have
A bachelor’s degree in information system, tax technology, management information systems or computer science or related field and a minimum of two years of related work experience
A passionate interest in data science and its role in the organization
Excellent communication and business writing skills
A natural flair for problem solving and an entrepreneurial approach to work
Strong organizational and time management skills, with exceptional client-serving consulting skills
Demonstrated ability to capture and synthesize business requirements
Desire and demonstrated ability to provide leadership within a team
Ideally, you’ll also have
Experience with Apache Spark
Experience with Hadoop and/or distributed database systems
Experience working in the Microsoft Azure Cloud environment
Experience developing ETL solutions using SSIS or other tools
ERP experience, including SAP and/or Oracle-preferred but not required
Practical experience or strong theoretical understanding of neural networks
What we look for

We are looking for knowledgeable data science professionals with a passion for turning data into actionable insight. You will need strong business acumen and a firm strategic vision, so if you are ready to use those skills to develop your team, this role is for you.

What working at EY offers

We offer a competitive compensation package where you will be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, a minimum of three weeks of vacation plus ten observed holidays and three paid personal days; and a range of programs and benefits designed to support your physical, financial and social well-being. We also offer:
Support and coaching from some of the most engaging colleagues in the industry
Opportunities to develop new skills and progress your career
The freedom and flexibility to handle your role in a way that is right for you
About EY

As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.
EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

If you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible.

Make your mark. Apply today.

."
9,Data Reliability Engineer,"Dallas, TX 75240",Dallas,TX,75240,None Found,None Found,None Found,None Found,None Found,None Found,"Hotels.com
Job Description
Are you a data engineer who wants to work for a strong and creative online travel technology company? If you enjoy working in a flexible, Agile environment where your efforts to ensure data reliability and availability affect all the teams in the data technology pillar, then… Expedia is looking for YOU!
The team at Expedia Group are looking for data and software development enthusiasts who are highly motivated and focused, to join their awesome Hotels.com Marketing Technology team. This role will collaborate with teams across our Dallas and London offices.
What you'll do...
Writing queries – lots of them. Much of our code is in Spark SQL and Python – scheduled using Apache Airflow - but we also use whatever other tools or languages are needed. We welcome programmers of all backgrounds as long as you are keen to work with data and deliver good-quality code!
Making sure code deploys smoothly and correctly into our cloud environment (AWS).
Getting a good understanding of our data sets and the business processes and teams that they support.
Working in an Agile team and looking for ways to continuously improve through asking for and providing feedback.
Sharing your technical knowledge with the team, reviewing work and providing guidance where you have seniority or experience with a specific technology.
Acting as a point of contact for technical issues for specific workstreams within a project.
Implementing product features, working with Product and Data Engineering teams. Using good practice like of checking requirements, unit testing, and code reviews to ensure these are delivered correctly.
Feeding into and helping implement our Roadmap, which includes investigating new data sets, innovating with new technologies and approaches and helping to build up and broaden the technical skills of yourself and the rest of your team.
Who you are...
We don’t like skill matching against a list of buzzwords. We look for clever people with good general programming and operational skills because we believe that extraordinary people can learn new technologies quickly and well. However, it wouldn't hurt to have experience with some of the following (or a passion to learn them):
Coding in Python (including use of Airflow), Java or another object-oriented language
“Big Data”/ distributed systems - Hadoop, Hive, and Amazon EMR
Experience of using cloud services (e.g. Amazon Web Services), particularly with an “infrastructure-as-code” approach (we use Terraform and Ansible)
Knowledge of visualisation technologies such as Tableau, Power BI, Kibana
Scalable relational database systems such as Redshift, Teradata
Passionate about learning, be that about languages, tools, data sets or how they are used by the business
The tech stack: a widely varying mix of: SQL/HQL , Griffin, Quibble, Hadoop stack (including mainly Hive and HDFS), Python, AWS stack ( including EMR, EC2, S3), Airflow, Azkaban, Spark, Tableau, Power BI, Kibana, Graphite and Grafana, Bash, Maven, Jenkins, Qubole, GitHub, Stash, GCP, JIRA, Confluence, Agile
Why join us:
Expedia Group recognizes our success is dependent on the success of our people. We are the world's travel platform, made up of the most knowledgeable, passionate, and creative people in our business. Our brands recognize the power of travel to break down barriers and make people's lives better – that responsibility inspires us to be the place where exceptional people want to do their best work, and to provide them the tools to do so.
Whether you're applying to work in engineering or customer support, marketing or lodging supply, at Expedia Group we act as one team, working towards a common goal; to bring the world within reach. We relentlessly strive for better, but not at the cost of the customer. We act with humility and optimism, respecting ideas big and small. We value diversity and voices of all volumes. We are a global organization but keep our feet on the ground so we can act fast and stay simple. Our teams also have the chance to give back on a local level and make a difference through our corporate social responsibility program, Expedia Cares.
If you have a hunger to make a difference with one of the most loved consumer brands in the world and to work in the dynamic travel industry, this is the job for you.
Our family of travel brands includes: Brand Expedia®, Hotels.com®, Expedia® Partner Solutions, Egencia®, trivago®, HomeAway®, Orbitz®, Travelocity®, Wotif®, lastminute.com.au®, ebookers®, CheapTickets®, Hotwire®, Classic Vacations®, Expedia® Media Solutions, CarRentals.com™, Expedia Local Expert®, Expedia® CruiseShipCenters®, SilverRail Technologies, Inc., ALICE and Traveldoo®.
About Hotels.com®
Hotels.com is the most rewarding way to book a place to stay. We really love travel and we know you do too. That’s why we make it really easy to book with us. With hundreds of thousands of places to stay around the world and 90 local websites in 41 languages, Hotels.com has it all. So, whether you’re looking for value in Vegas, treehouses in Thailand or villas with views, it’s all just a click away. And with our “Reward-winning” loyalty program you earn free* nights while you sleep…what could be better? Booking just got smarter too. With over 25 million real guest reviews and an app so easy to use that it’s been downloaded 70 million times, you can be sure to find the perfect place for you.
Expedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. This employer participates in E-Verify. The employer will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization."
10,Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,"
Experience on client-facing projects, including working in close-knit teams
Experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)
Experience or familiarity with real-time ingestion and streaming frameworks is a plus
Experience and desire to work with open source and branded open source frameworks
Experience working on projects within the cloud ideally AWS or Azure
Experience with NLP, Machine Learning, etc. is a plus
Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time
Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R
Data Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models
Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.
A deep personal motivation to always produce outstanding work for your clients and colleagues
Excel in team collaboration and working with others from diverse skill-sets and backgrounds",None Found,None Found,None Found,None Found,"Summary:

You have experience with client projects and in handling vast amounts of data – working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Analytics team. Play a key role on projects from a data engineering perspective, working with our Architects and clients to model the data landscape, obtain data extracts and define secure data exchange approaches.
Plan and execute secure, good practice data integration strategies and approaches
Acquire, ingest, and process data from multiple sources and systems into Big Data platforms
Create and manage data environments in the Cloud
Collaborate with our business analysts and data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models
Have a strong understanding of Information Security principles to ensure compliant handling and management of client data
This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science
Qualifications:
Experience on client-facing projects, including working in close-knit teams
Experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)
Experience or familiarity with real-time ingestion and streaming frameworks is a plus
Experience and desire to work with open source and branded open source frameworks
Experience working on projects within the cloud ideally AWS or Azure
Experience with NLP, Machine Learning, etc. is a plus
Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time
Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R
Data Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models
Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.
A deep personal motivation to always produce outstanding work for your clients and colleagues
Excel in team collaboration and working with others from diverse skill-sets and backgrounds
Cervello is a dynamic technology company that is focused on business analytics and planning. We take an innovative approach to making complex solutions simple so our clients can focus on running their businesses. Our services and applications enable our clients to gain the benefits of a world-class analytics and planning capability without the headaches.

hG1oHsQXvK"
11,Microsoft Power BI Developer,"Dallas, TX 75202",Dallas,TX,75202,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description:
Job Description
Applies specialized knowledge of Microsoft Power BI and Azure Analysis services to conceptualize, design, develop, unit-test, configure, and implement portions of new or enhanced (upgrades or conversions) business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate techniques and tools to analyze, identify, and deliver high quality reporting tools. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies.
Responsibilities:
Core team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives
Develop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams
Document requirements and translate into proper system requirements specifications using high-maturity methods, processes and tools.
Develop visualization, user experience and configuration elements of solution design.
Execute and coordinate requirements management and change management processes. Participates as a member of and leads development teams.
Designs, prepares and executes unit tests.
Completes development to implement complex components.
Designs solutions for others to develop.
Participates in cross-functional teams.
Leads design activities and provides mentoring and guidance to developers.
Designs, prepares and executes unit tests.
Represents team to clients.
Demonstrates technical leadership and exerts influence outside of immediate team.
Develops innovative team solutions to complex problems.
Contributes to strategic direction for teams.
Applies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g. Power BI and Power App development).
Integrates technical expertise and business understanding to create superior solutions for clients.
Consults with team members and other organizations, clients and vendors on complex issues.
Education and Experience Required:
Typically, a technical bachelor's degree or equivalent experience and a minimum of 6 years of related experience or a master's degree and a minimum of 4 years of experience.
Knowledge and Skills:
4 or more years’ experience writing code using languages such as (and not limited to) Power BI, Tableau, QlikView, Java, C, C++, C#, VB.Net.
Significant hands on experience in creating and deploying KPI, analytic, and dashboard solutions including design, development, deployment, data engineering, data warehousing, and data management projects and practices.
Advanced understanding of RDBMS databases such SQL Server and Oracle.
Advanced understanding of modern software design and development methodologies.
Experience on multiple full release project life cycles.
Advanced understanding of modern SCM (software configuration management).
Advanced understanding of testing tools and unit test and integration test scripting, and testing methodologies
Advanced experience using an Integrated Development Environment (e.g., Eclipse, Visual Studio) and development of tool add-ins.
Strong understanding of basic Database Administration. Able to define quality and security standards.
Good verbal and written communication and negotiation skills.
General project management/team leader skills.
Ability to work effectively in a globally dispersed team and with clients and vendors.
Demonstrated technical leadership skills.
Business skills - 4+ years of technology services industry and business operations experience in a technology services organization."
12,Network Data Engineer -- Data Modelling,"Richardson, TX 75081",Richardson,TX,75081,None Found,None Found,None Found,None Found,None Found,None Found,"What you’ll be doing...
SDN Planning group under Verizon Network Technology and Planning organization is looking for self-motivated and innovative engineer in support of Verizon SDN architecture and technology development.As a senior level member of technical staff, you will be interacting with software engineers, network/system operations staff, network/system architects and vendor partners to provide SDN technology evolution strategies and solutions to keep our service relevance in the market place in the fast moving and quickly evolving networking industry. Proactive technology research, industry trend analysis, and developing next generation network architecture using modern networking technology (e.g., Software Defined Networking) and providing production deployable solutions are key functions of the team.
Responsibilities include:
New technology validation and prototyping.
Software Defined Networking (SDN) ecosystem development and technical vendor management.
SDN/NFV platform architecture and development.
Design and develop machine learning and deep learning models for real world, large scale problems in computer networks.
Develop data collections, labeling pipelines, and evaluation pipelines.
Optimize models for on-device and multi-modal intelligence.
Design and implement software applications using Machine Learning and Artificial Intelligence for data verification, transformation, and analytics.
Open API development and verification in support of Verizon SDN platform infrastructure.
Linux application deployment and linux networking lab infrastructure support.
Linux scripting, networking and administration knowledge.
Participate in open-source and open standard industry collaboration activities.
What we’re looking for...
You’ll need to have:
Bachelor’s degree or four or more years of work experience.
Six or more years of relevant work experience.
Experience developing applications using Machine Learning algorithms.
Even better if you have:
Master’s degree or PhD degree in Computer Science or Electrical Engineering with modern data communication technology discipline.
Experience on machine learning algorithms, from supervised and unsupervised to reinforcement learning
Java, C/C++, and/or Python programming skills.
Experience with one or more of the following: artificial neural networks, classification, pattern recognition, recommendation systems, targeting systems, ranking systems or similar.
Experience in analyzing sophisticated and dynamic patterns.
Five or more years of data communication industry experience.
Network (WAN/LAN) Engineering (both Layer 2 and 3) experience in Service Provider Network or Enterprise Network environment.
SDN knowledge and development experience.
Experience on with automation, Machine Learning, and Deep Learning tools and applications (TensorFlow, RPA, etc.).
Experience in understanding the complexity of the algorithms as well as in optimizing algorithms.
Knowledge of Packet, Optical, and Wireless network technologies.
Industry technology leadership skill.
Multi-vendor system integration and technical vendor management experience.
Open source tools development, implementation, and collaboration experience (Robot Framework, Jenkins, Phabricator, Kafka, Screwdriver, etc.).
Open networking, open API, and open platform development experience.
When you join Verizon...
You’ll be doing work that matters alongside other talented people, transforming the way people, businesses and things connect with each other. Beyond powering America’s fastest and most reliable network, we’re leading the way in broadband, cloud and security solutions, Internet of Things and innovating in areas such as, video entertainment. Of course, we will offer you great pay and benefits, but we’re about more than that. Verizon is a place where you can craft your own path to greatness. Whether you think in code, words, pictures or numbers, find your future at Verizon.
Equal Employment Opportunity
We're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better."
13,AI Model Development Lead for Virtual Channels (Analytic Manager 5),"Irving, TX",Irving,TX,None Found,None Found,"
8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques",None Found,None Found,None Found,None Found,"Overview:
Wells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.

SUCCESS PROFILE
Check out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.

Analytical
Detail-oriented
Insightful
Inventive
Problem Solver
Curious
Benefits
Wells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:

Medical, Dental and Vision
Employer Matching 401(k)
Tuition Reimbursment
Maternity and Paternity Leave
Paid Time Off
Responsibilties
Job Description
At Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.
Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.
Enterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.
As part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, Virtual Channels, and virtual channels is looking for an experienced AI leader to manage the development of AI models for Virtual Channels.

This leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on Virtual Channels’ AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and Virtual Channels executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.

KEY RESPONSIBILITIES INCLUDE:
Build and grow a team of data scientists responsible for AI model development in support of Virtual Channels
Design, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions
Partner with Virtual Channels executives to frame the problem, manage the model development process, and business relationship
Manage a portfolio of the data science projects including the following responsibilities:
Help finalize project scope working with business partners
On-going touch-base with business partners and governance stakeholders
Define priorities in partnership with the business partners during on-going development
Work with AI technology and production teams to operationalize models
Will be called upon to review vendor models and solutions and/or models developed outside of EADS
Divisional Information:
Data Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.
As a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:
Lead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.
Accomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.

Required Qualifications

8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques


Desired Qualifications

Strong analytical skills with high attention to detail and accuracy
Ability to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members
Meeting facilitation experience in leading discussions that result in consensus and commitment


Other Desired Qualifications
4+ years managing or directing data scientist/ statistician/ data engineer teams
Hands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet
Detail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable
Experience with agile project management methodologies for data science
Experience with Big Data or Hadoop tools such as Spark, Hive, Kafka and Map

Street Address
NC-Charlotte: 401 S Tryon St - Charlotte, NC
MN-Minneapolis: 600 S 4th St - Minneapolis, MN
NC-Charlotte: 11625 N Community House Road - Charlotte, NC
SC-Fort Mill: 3480 State View Blvd - Fort Mill, SC
TX-Addison: 5080 Spectrum Dr - Addison, TX
TX-DAL-Downtown Dallas: 1445 Ross Ave - Dallas, TX
TX-Irving: 5000 Riverside Drive - Irving, TX
AZ-Tempe: 1150 W Washington St - Tempe, AZ
IA-Des Moines: 6200 Park Ave - Des Moines, IA
IA-Des Moines: 800 Walnut St - Des Moines, IA
GA-Atlanta: 3579 Atlanta Ave - Atlanta, GA


Disclaimer

All offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.

Relevant military experience is considered for veterans and transitioning service men and women.

Wells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.

ENT FINANCE"
14,Tax Staff – National Tax – Tax Technology and Transformation (TTT) – Data Scientist – Advanced Technologies - Dallas,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,"
Strong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.
Experience with common data science programming languages, such as Python R
Strong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential
Experience with data visualization tools, such as PowerBI, D3.js, etc.
Experience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB
Strong scripting and programming skills
Ownership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.
Ability to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies
Ability to consolidate tax data to make analysis and planning more efficient
Focus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities
Willingness to support project team members in any way needed to help ensure timely completion of deliverables",None Found,None Found,None Found,"Tax Technology and Transformation offers services to companies in response to the impact of existing and emerging technology, including the growing data burden that many businesses face, driving efficiencies to create a cost-effective tax function and the need to understand how to make data an asset. The underlying objective of the combined offerings is to help businesses navigate the digital age of tax transparency alongside new trends in tax compliance and tax audit methods, as well as helping to solve the most pressing challenges that businesses face. Tax Technology and Transformation is composed of the following services:
Digital tax transformation
Tax applications-as-a-service
Tax data and improvement
Tax analytics and reporting enhancement
Emerging tax technology, including robotic process automation (RPA), artificial intelligence (AI), blockchain, cloud solutions, data lake development and business intelligence innovation
Tax technology program mobilization
Custom tax technology application development and deployment
Tax technology strategy and road mapping
Direct and indirect tax systems implementation and configuration
Post-transaction (M&A) tax function operational services
Tax operating model transformation, including process improvement, risk and controls
Tax function assessments
The opportunity
Tax Technology and Transformation is an area that has seen significant growth and investment, and you will see that reflected in your experience. It is no exaggeration to say that you will be working on highly publicized projects. The field of taxation is constantly changing as new laws, regulations, and technologies are created, and this is your opportunity to be part of that development.

Key responsibilities
We are looking for an ambitious, self-motivated data scientist or data engineer who will help us discover the information hidden in vast amounts of data, and help us deliver even better products to our clients. Your primary focus will be in applying data mining techniques, doing statistical analysis and building high quantity prediction systems integrated with our products. You will be expected to team on a national and even global scale, so strong communication skills, attention to detail, and ability to effectively drive results are essential.
Selecting features and, building and optimizing classifiers using machine learning techniques
Data mining using state-of-the-art methods
Enhancing data collection procedures to include information that is relevant for building analytic systems
Processing, cleansing and verifying the integrity of data used for analysis
Doing ad-hoc analysis and presenting results in a clear manner
Depending on your unique skills and ambitions, you could be supporting various client projects, ranging from assisting in the production of leading edge machine models, to designing and implementing robust data pipelines that can handle data at a multinational, enterprise scale. Whatever you find yourself doing, you will contribute and help toward developing a highly trained team, all the while handling activities with a focus on quality and commercial value. This is a highly regulated industry, so it is all about maintaining our reputation as trusted advisors by taking on bold initiatives and owning new challenges.

Skills and attributes for success
Strong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.
Experience with common data science programming languages, such as Python R
Strong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential
Experience with data visualization tools, such as PowerBI, D3.js, etc.
Experience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB
Strong scripting and programming skills
Ownership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.
Ability to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies
Ability to consolidate tax data to make analysis and planning more efficient
Focus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities
Willingness to support project team members in any way needed to help ensure timely completion of deliverables
To qualify for the role, you must have
A bachelor’s degree in information system, tax technology, management information systems or computer science or related field and a minimum of one year of related work experience
A passionate interest in data science and its role in the organization
Excellent communication and business writing skills
A natural flair for problem solving and an entrepreneurial approach to work
Strong organizational and time management skills, with exceptional client-serving consulting skills
Demonstrated ability to capture and synthesize business requirements
Desire and demonstrated ability to provide leadership within a team
Ideally, you’ll also have
Experience with Apache Spark
Experience with Hadoop and/or distributed database systems
Experience working in the Microsoft Azure Cloud environment
Experience developing ETL solutions using SSIS or other tools
ERP experience, including SAP and/or Oracle-preferred but not required
Practical experience or strong theoretical understanding of neural networks
What we look for
We are looking for knowledgeable data science professionals with a passion for turning data into actionable insight. You will need strong business acumen and a firm strategic vision, so if you are ready to use those skills to develop your team, this role is for you.

What working at EY offers
We offer a competitive compensation package where you will be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, a minimum of three weeks of vacation plus ten observed holidays and three paid personal days; and a range of programs and benefits designed to support your physical, financial and social well-being. We also offer:
Support and coaching from some of the most engaging colleagues in the industry
Opportunities to develop new skills and progress your career
The freedom and flexibility to handle your role in a way that is right for you
About EY
As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.

EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

If you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible.

Make your mark. Apply today.

."
15,Data Engineer - ETL Developer,"Irving, TX",Irving,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Advanced career position responsible for leading development of Pioneer's enterprise data solutions. Major Responsibilities: Works independently designing, developing, documenting, and supporting full lifecycle data, reporting, and analytic solutions. These solutions should include large data ingestion, persistence, transformation, and retrieval. Will develop, maintain, and deploy code to include data acquisition, aggregation, and data warehousing. Addresses problems of systems integration and compatibility. Works on complex, less clearly-defined projects. Requires advanced knowledge of Pioneer's enterprise solutions. Applies a full understanding of established systems software development principles and best practices to analyze complex issues and update existing solutions. Responsible for reviewing work product of other developers. Acts as team leader on projects and provides guidance and training to less-experienced developers. Work closely with enterprise architects to ensure that solutions are developed using best practice and following standard software development lifecycle (SDLC) methodologies. You will work with independently and with team members on projects that include incident analysis, break-fix enhancements, ETL/ELT framework enhancements, data quality enhancements, usage reporting and general design, coding and release/build activities. Minimum Qualifications: Bachelor’s degree in Computer Science, Information Systems, Engineering, Science, or related field is required; consideration given for related technical experience. Minimum of 6 years related experience is required; consideration given for related technical aptitude. Experience in agile development methodologies 4 or more years of ETL development in tools like ODI, SSIS, DataStage or Informatica 4 or more years of experience in technologies like Oracle Database (v11+), Microsoft SQL Server, SQL & PL/SQL, T-SQL development, stored procedures, triggers, and ANSI SQL standards. 4 or more years of Data modeling, Datawarehouse and Data mart design in tools like Oracle Business Analytics Warehouse, and/or Microsoft SQL Server. Preferred Qualifications: Advanced knowledge of agile development methodologies. Java or Python programming experience. Experience building streaming data pipelines Experience working with Azure cloud technologies like Azure Datawarehouse, ADF and ADF2 Experience working with cloud-based data platforms Experience working with data virtualization technologies like Tibco Data Virtualization Advanced with Linux or Unix systems / scripting Experience predictive analytics implementations Experience with NoSQL data platforms Must have aptitude for problem-solving. Must have a “can do” attitude. Must demonstrate desire to learn. Exceptionally detail-oriented Must be able to perform in a collaborative manner by coordinating changes while maintaining a positive relationship with team members, external consultants and vendors, business partners, interfacing application system contacts and numerous technology services groups. Must be able to communicate effectively at all levels in the organization and with personnel located at headquarters and in field locations. Curious and excited by new ideas Energized by a fast-paced environment Able to understand and translate business needs into leading-edge technology Comfortable working as part of a connected team, but self-motivated Salary competitive and commensurate with qualifications and experience Pioneer Natural Resources is an EEO/M/F/disability/veteran Employer"
16,Hadoop Developer,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Title: Hadoop Developer

Location: Dallas, TX

Type: Contract

Duration: Long term

JD:

· We need Big Data (Hadoop) HortonworksData Platform (HDP ) Data Engineer who is well versed with Hortonworks Data Flow ( HDF) and apache Nifi /miNifi and Kafka tools .

· The candidate should have strong experience on Scala & Spart.

· The resource should be 6+ yrs exp with good in depth knowledge & experience in Hadoop around all the Hadoop ecosystem- HDP, HDF, Nifi , M/R, Hive, pig, Spark/scala, kafka, Hbase and having ETL Background.

· The resource should be able Develop the framework of Data Ingestion into Data lake with Utilities around this Platform.

· 6+ years total experience in development mainly around Java and all related technologies in the Java stack (e.g. Spring)

· 6+ year in depth knowledge & experience in Hadoop around all the Hadoop ecosystem (HDP, HDF, M/R, Hive, pig, Spark/scala, kafka, Hbase, Elastic search and log stash a plus)

· 4+ years of experience working in Linux/Unix

· Good understanding & experience with Performance and Performance tuning for complex S/W projects mainly around large scale and low latency.

· Experience with leading Design & Architecture

· Hadoop/Java certifications is a plus

· Excellent communication skills.

· Ability to work in a fast-paced, team oriented environment."
17,Big Data Developer- Senior Advisor,"Richardson, TX",Richardson,TX,None Found,None Found,"7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.4+ years of SQL development skills within a multi-tier environment are required.experience in Big Data tools: Hadoop, PIG, Sqoop, Hive and Hcatalog & NoSQL (HBase, Cassandra) , SQL.",None Found,None Found,None Found,None Found,"The Big Data Developer will work with data stewards, data owners, master data management analysts, operations teams and IT partners across the organization to drive the enterprise data conformance program. Data research and analysis, cross-functional requirements gathering and documentation and solution development are key aspects of the role. The Data Engineer supports reactive data quality by researching and fixing known data issues and proactive data quality by defining the requirements for data controls and enhancements to data capture, monitoring and maintenance processes, procedures and standards.

Primary responsibilities include performing data operations such as conversion, address hygiene, and postal presort, as well as variable document composition and file creation.

Essential Functions:

Data Analysis
Idependently design, develop and implement data integration solutions that support our platforms resiliency, stability, and supportability using a variety of ETL and database technologies.
Rapidly develop and refine data integration solutions using Infosphere Datastage, SQL, FastTrack, or other technologies.
Experience integrating large structured and unstructured data in multiple format, character sets and delivery methods
Works with business sponsors, SMES and application teams; to understand the business requirements; analyze and assess availability, quality, and lineage of source system data.
Design, map data from source to target and develop data integration solutions that meet business needs.
Develop and socialize data integration standards.
Partners with other engineers through design reviews, providing feedback on feasibility, scalability, performance and adherence to standards.
Partners with business, analysts, BI team, application teams and other stakeholders to design, map data from source to target, develop, test, and implement production data integration solutions that are fully integrated into the Enterprise Data Warehouse.
Ensuring model design solves the end users need.
Contribute information to the data governance software to improve knowledge downstream.

Data Conformance
Performs analysis on known data quality issues and develops and/or recommends operational or technical solutions for remediation, including development and implementation of automated data quality controls that proactively trigger notifications to process owners when data is out of range. Keeps stakeholders informed of progress and solutions in a timely manner.
Autonomously, and proactively performs data profiling to explore data, identify issues and summarize findings.
Defines data quality metrics to assess completeness, accuracy, consistency, and conformance to business rules. Designs dashboards to support continuous monitoring and measurement of data quality.
Partners with IT to cleanse data to achieve the desired level of data quality
Partners with data stakeholders, process and product owners across the organization to define data standards and communicate changes to data capture procedures, processes, standards, and controls.
Cleanses and prepares datasets to be consumed by data scientists and other analysts
Collaborates with external teams working on data integration and engineering
Advocates data governance and hygiene best practices
Assists with scoping and integrating orphan datasets

You will gain hands on experience implementing through embedding standardized data elements in the database or system, employing standardized data elements in an exchange mechanism (usually XML schema), or mapping the application data elements to the standardized elements for purposes of exchange.

Required Qualifications
7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.4+ years of SQL development skills within a multi-tier environment are required.experience in Big Data tools: Hadoop, PIG, Sqoop, Hive and Hcatalog & NoSQL (HBase, Cassandra) , SQL.
Programming: Scala, Java, Python, Spark

Preferred Qualifications
Experience in programming languages like Scala, Java, Python, Spark
In depth understanding of data integration best practices, leading industry applications and features such as master data management, entity resolution, data quality assessment, metadata management, etc.Expertise in flat file formats, XML within PL/SQL and file format conversion.Exposure to application security technologies and approaches is preferred.Experience processing and parsing CSV, JSON and XML file formatsDatastage, SQL, FastTrack, or other technologies
Strong analytical, debugging and testing skills
Software development experience using scripting languages such as JavaScript, Python or RubyProficient using Infosphere/DataStage or equivalent ETL software.Proficient with relational databases and using SQL to query, create tables, views, indexes, joins.Proficient using Unix and applicable scripting/scheduling tools.
-Knowledge of clinical and financial Healthcare data -Knowledge of all data formats (HL7, EDI, CSV, XML, etc)

Education
Bachelors Degree in Computer Science or related field required

Business Overview
It’s a new day in health care.

Combining CVS Health and Aetna was a transformative moment for our company and our industry, establishing CVS Health as the nation’s premier health innovation company. Through our health services, insurance plans and community pharmacists, we’re pioneering a bold new approach to total health. As a CVS Health colleague, you’ll be at the center of it all.

We offer a diverse work experience that empowers colleagues for career success. In addition to skill and experience, we also seek to attract and retain colleagues whose beliefs and behaviors are in alignment with our core values of collaboration, innovation, caring, integrity and accountability.

CVS Health is an equal opportunity/affirmative action employer. Gender/Ethnicity/Disability/Protected Veteran – we highly value and are committed to all forms of diversity in the workplace. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities. We comply with the laws and regulations set forth in the following EEO is the Law Poster: EEO IS THE LAW and EEO IS THE LAW SUPPLEMENT. We provide reasonable accommodations to qualified individuals with disabilities. If you require assistance to apply for this job, please contact our Advice and Counsel Reasonable Accommodations team. Please note that we only accept applications for employment via this site.

If technical issues are preventing you from applying to a position, contact Kenexa Helpdesk at 1-855-338-5609 or cvshealthsupport@us.ibm.com. For technical issues with the Virtual Job Tryout assessment, contact the Shaker Help Desk at 1-877-987-5352."
18,Technical Advisor - Data Engineer,"Plano, TX",Plano,TX,None Found,None Found,"Bachelor’s degree; STEM background preferred5+ years’ experience developing scalable big data solutions in cloud and on premise environmentsExperience with big data technologies (Spark, Hadoop); data lakesExperience developing data processing/ETL/ELT solutions using e.g. Scala, Python, Java, SQLExperience deploying statistical/machine learning models to productionExperience building cloud-based data solutions; Azure experience preferred",None Found,"Develop, construct, test and maintain architectures for scalable, timely and efficient big data processingBuild data pipelines in lambda architectures for structured/unstructured, streaming/batch dataDevelop solutions for data modeling, mining and quality monitoringDevelop solutions for deploying models to productionIdentify and make recommendations on ways to improve data quality, reliability and efficiencyDiscover opportunities for data acquisitionMaintain awareness of ongoing developments in big data and data engineering technologies both in the industry in general and within the enterprise",None Found,None Found,"Company: FedEx Services
Job Title: Technical Advisor - Data Engineer
Job Requisition Number: RC196455
Category: Information Technology
Locations:
Plano, Texas 75024
United States

Job Summary:
The Technical Advisor - Data Engineer will play a critical role in modernizing the architectures and processes enabling operationalization of big data acquisition, processing and quality monitoring for our retail domain (Omni-channel, manufacturing network, back office). You will work closely with technical solution architects, data scientists and business stakeholders to develop capabilities for enabling timely connections to high-quality data sourced from many disparate systems. While this role will primarily be technical in nature, we are looking for an individual who can effectively collaborate with our business partners and other IT members to identify and define scalable architectures and capabilities. This is an exciting opportunity to work with a team of passionate practitioners using current data technologies, and with significant potential for growth (machine learning, etc.).

Key Responsibilities
Develop, construct, test and maintain architectures for scalable, timely and efficient big data processingBuild data pipelines in lambda architectures for structured/unstructured, streaming/batch dataDevelop solutions for data modeling, mining and quality monitoringDevelop solutions for deploying models to productionIdentify and make recommendations on ways to improve data quality, reliability and efficiencyDiscover opportunities for data acquisitionMaintain awareness of ongoing developments in big data and data engineering technologies both in the industry in general and within the enterprise

Basic Qualifications
Bachelor’s degree; STEM background preferred5+ years’ experience developing scalable big data solutions in cloud and on premise environmentsExperience with big data technologies (Spark, Hadoop); data lakesExperience developing data processing/ETL/ELT solutions using e.g. Scala, Python, Java, SQLExperience deploying statistical/machine learning models to productionExperience building cloud-based data solutions; Azure experience preferred

Preferred Qualifications
Experience with notebook-based development (Databricks, Jupyter)Experience with data governance/master data managementExperience developing statistical and/or machine learning modelsExperience developing serverless solutionsExperience with graph databasesIoT knowledge; edge computingRetail domain knowledgeSAFe/agile experience
Domicile: Plano, TX
Relocation assistance may be available for this position, but is based on business decision.

Minimum Qualifications:
Bachelor's Degree, in computer science, engineering, information system or related field and/or equivalent formal training or work experience. Requires five (5) or more years qualifying work experience in information technology or engineering environment. A related advanced degree may offset the related experience requirements.

To apply for this position, please upload a current copy of your resume and complete the required screening questionnaire by close of business (5:00 PM CST) on _10/14/19, to be considered.


Want a career where you are empowered to make a difference? Want to work for a company that is environmentally responsible? Want to grow and develop on the job? If so, FedEx is the place for you! Every day FedEx delivers for its customers with transportation and business solutions. FedEx serves more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx employees. FedEx has over 400,000 talented employees who are tasked with making every FedEx experience outstanding. FedEx has been recognized on many different lists both for business success and for being a great employer.
Here are some of the recognitions FedEx has received from the past couple of years:
Fortune ""World’s Most Admired Companies"" – 2019
Forbes ""Best Employers for Diversity"" - 2019
Reputation Institute ""World\’s Most Reputable Companies"" – 2019
National Business Inclusion Consortium ""Best-of-the-Best Corporations for Inclusion"" - 2019
Women\’s Business Enterprise National Council ""America’s Top Corporations for Women’s Business Enterprises"" - 2018
Corporate Responsibility Magazine ""100 Best Corporate Citizens"" – 2018
Black Enterprise ""50 Best Companies For Diversity"" – 2018
When 400,000 employees around the globe are all working together it is amazing what we can achieve! FedEx connects people and ideas. If you would like to make a difference on a global scale while receiving top notch benefits, competitive pay, and plenty of opportunities to develop, click ‘Apply’ and tell us more about yourself.
EEO Statement - FedEx is an equal opportunity/affirmative action employer (minorities/females/disability/veterans) that is committed to diversifying its workforce."
19,Senior Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management, and product design, combined with digital engineering excellence.

As a Senior Data Engineer, here's what we'll be looking for you to bring:


Hands-on Engineering Leadership
Proven track record of Innovation and expertise in Data Engineering
Tenure in coding, architecting and delivering complex projects
Deep understanding and application of modern data processing technology stacks. For example Spark, Kafka, Hadoop, ecosystem technologies, and others
Deep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies
Deep understanding of relational database technologies and database development techniques
Understanding of how to architect solutions for data science and analytics
Data management for reporting and BI experience is a plus
Understanding of ""Agility"", including core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Passion for software craftmanship
A rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..
Strong stakeholder management and interaction experience at different levels
Any experience building and leading an offshore/outsourcing function would be highly beneficial.

There's no typical day or engagement for our Senior Engineers. Here's what you'll do:


Be the SME. Develop Big Data architectural approach to meet key business objectives and provide end to end development solution
You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that Big Data has to solve their most pressing problems.
On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.
It could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.
Whatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.
You have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.
You recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.

Regardless of what you do at ThoughtWorks, you'll always have the opportunity to:


Think through hard problems, and work with a team to make them reality.
Learn something new every day.
Work in a dynamic, collaborative, transparent, non-hierarchal, and ego-free culture where your talent is valued over a role title
Travel the world.
Speak at conferences.
Write blogs and books.
Develop your career outside of the confinements of a traditional career path by focusing on what you're passionate about rather than a predetermined one-size-fits-all plan
Be part of a company with Social and Economic Justice at the heart of its mission.

A few important things to know:
-------------------------------

Projects are almost exclusively on customer site, so candidates should be flexible and open to travel.

Candidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.

Not quite ready to apply? Or maybe this isn't the right role for you? That's OK, you can stay in touch with AccessThoughtWorks ( https://www.thoughtworks.com/careers/access?utm_source=apply-jobs&utm_medium=jd&utm_campaign=access-thoughtworks ), our learning community (click ""contact me about recruitment opportunities"" to hear about jobs in the future).

It is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.

#LI-NA"
20,Data Engineer Cloud,"Dallas, TX",Dallas,TX,None Found,None Found,"4 year College degree in Computer Science, Information Technology or equivalent demonstrated experience. Masters degree preferred.
Strong SQL development skills using databases like oracle and Vertica.
Experience in cloud databases like Snowflake or Redshift is a plus
Experience with AWS technologies such as EC2, S3 and other basic AWS technologies
Certification –preferably AWS Certified Big Data or any other cloud data platforms, big data platforms
4+ years of experience in the data and analytics space
Solid Programing experience in Python - needs to be an expert in this 4/5 level.
Experience with workload automation tools such as Airflow, Autosys.
4+ years experience developing and implementing enterprise-level data solutions utilizing Python , Java, Spark, and Scala, Airflow , Hive
3+ years in key aspects of software engineering such as parallel data processing, data flows, REST APIs, JSON, XML, and micro service architectures.
6+ years of RDBMS concepts with Strong Data analysis and SQL experience
3+ years of Linux OS command line tools and bash scripting proficiency
Cloud data warehouse experience - Snowflake is a plus
Must be an EXPERT with ETL Development on Unix Servers.
Must have demonstratable working knowledge of modern information and delivery practices for on-premises and cloud environments.
Must have demonstratable experience delivering robust information delivery and management solutions as part of a fast paced data platform program.
MUST BE AN EXPERT applying business rules/logic using SQL scripts.
Must have working knowledge of various data modeling techniques (3NF, denormalized, STAR Schema).
Position requires a self-starter, capable of quickly turning around vaguely defined projects with minimal supervision or assistance.
Ability to conduct analysis of source data sets to achieve target data set objectives.
STRONG VERBAL/WRITTEN COMMUNICATION is a MUST. Interacting with business community/users is a core requirement of the role.
Must be able to provide specialized support for our Legacy platforms, as well as the new Cloud Based Data Platform.",None Found,None Found,None Found,None Found,"COMPANY OVERVIEW
For over a century, Neiman Marcus Group has served the unique needs of our discerning customers by staying true to the principles of our founders: to be the premier omni-channel retailer of luxury and fashion merchandise dedicated to providing superior service and a distinctive shopping experience in our stores and on our websites. Neiman Marcus Group is comprised of the Specialty Retail Stores division, which includes Neiman Marcus and Bergdorf Goodman, and our international brand, mytheresa.com. Our portfolio of brands offers the finest luxury and fashion apparel, accessories, jewelry, beauty, and home décor. The Company operates more than 40 Neiman Marcus full-line stores in the most affluent markets across the United States, including U.S. gateway cities that draw an international clientele. In addition, we operate 2 Bergdorf Goodman stores in landmark locations on Fifth Avenue in New York City. We also operate more than 40 Last Call by Neiman Marcus off-price stores that cater to a value oriented, yet fashion minded customer. Our upscale eCommerce and direct-to-consumer division includes NeimanMarcus.com, BergdorfGoodman.com Horchow.com, LastCall.com, and CUSP.com. Every day each of our 15,000 NMG associates works towards the goal of enabling our customer to shop any of our brands ""anytime, anywhere, and on any device."" Whether the merchandise we sell, the customer service we offer, or our investments in technology, everything we do is to enhance the customer experience across all channels and brands.
DESCRIPTION
Neiman Marcus Group has an immediate opening for a Lead Data Platform Engineer.
The Senior Data Platform Engineer will have the unique combination of business acumen needed to interface directly with key stakeholders to understand business challenges, along with the skills and vision required to translate the need into a world-class technical solution using the latest technologies.
This person will be in a hands-on role as part of a development team responsible for building data engineering solutions for the NMG Enterprise using cloud based data platforms. They will work closely with solution architects and support teams and take a lead on day-to-day development and support for data engineering workloads. In this role, you need to be equally skilled with the whiteboard and the keyboard.
SUMMARY
Primary focus of this position is to provide design and development support to the Data Platform team. Day to day activities will include, but not be limited to…ingesting various data sources into the data platform, designing and deploying consolidation and summary tables, design and deployment of data marts, data/process modeling, systematic auditing of load processes, creating batch process automation, and analyzing source data from various applications for purposes of reporting, business intelligence and data science initiatives.
This position is a hands-on, intense role of working side by side with our end user partners to drive better analytics, reporting and most importantly…competitive advantage. This person will be an integral part of the Customer Data platform team, working with architects, developers and data platform engineers to support the overall Neiman Marcus enterprise.

ESSENTIAL DUTIES AND RESPONSIBILITIES (include the following, other duties not listed may be assigned)…
Work primarily with architects and at times with business partners and data science teams to understand business context and craft best-in-class solutions to their toughest problems
Provide data modeling, process modeling, and data mart design support.
Create Python Scripts/SQL scripts in support of data platform load and batch processes.
Design and deploy consolidation and summary tables as required within the data warehousing environment.
Perform periodic performance assessments of the automated load processes.
Be proactive in identifying and working with issues.
Provide specialized support for our Legacy platforms, as well as the new EDW.
Documentation of deliverables.
Standardization of deliverables.
Create robust and automated pipelines to ingest and process structured and unstructured data from source systems into analytical platforms using batch and streaming mechanisms leveraging a cloud native toolset
Implements automation to optimize data platform compute and storage resources
Develops and enhances end to end monitoring capability of cloud Data platforms
Responsible for implementing custom data applications as required for delivering actionable insights
Provides regular status updates to all relevant stakeholders
Participates in daily scrum calls and provides clear visibility to work products
Participates in developing projects plan, timelines and providing estimates
Provide hands-on technical assistance in all aspects of data engineering design and implementations including data ingestion, data models, data structures, data storage, data processing, and data monitoring at scale
Develop data engineering best practices with considerations for high data availability, fault tolerance, computational efficiency, cost, and quality

Qualifications

REQUIREMENTS
4 year College degree in Computer Science, Information Technology or equivalent demonstrated experience. Masters degree preferred.
Strong SQL development skills using databases like oracle and Vertica.
Experience in cloud databases like Snowflake or Redshift is a plus
Experience with AWS technologies such as EC2, S3 and other basic AWS technologies
Certification –preferably AWS Certified Big Data or any other cloud data platforms, big data platforms
4+ years of experience in the data and analytics space
Solid Programing experience in Python - needs to be an expert in this 4/5 level.
Experience with workload automation tools such as Airflow, Autosys.
4+ years experience developing and implementing enterprise-level data solutions utilizing Python , Java, Spark, and Scala, Airflow , Hive
3+ years in key aspects of software engineering such as parallel data processing, data flows, REST APIs, JSON, XML, and micro service architectures.
6+ years of RDBMS concepts with Strong Data analysis and SQL experience
3+ years of Linux OS command line tools and bash scripting proficiency
Cloud data warehouse experience - Snowflake is a plus
Must be an EXPERT with ETL Development on Unix Servers.
Must have demonstratable working knowledge of modern information and delivery practices for on-premises and cloud environments.
Must have demonstratable experience delivering robust information delivery and management solutions as part of a fast paced data platform program.
MUST BE AN EXPERT applying business rules/logic using SQL scripts.
Must have working knowledge of various data modeling techniques (3NF, denormalized, STAR Schema).
Position requires a self-starter, capable of quickly turning around vaguely defined projects with minimal supervision or assistance.
Ability to conduct analysis of source data sets to achieve target data set objectives.
STRONG VERBAL/WRITTEN COMMUNICATION is a MUST. Interacting with business community/users is a core requirement of the role.
Must be able to provide specialized support for our Legacy platforms, as well as the new Cloud Based Data Platform.
ADDITIONAL REQUIREMENTS
Candidate MUST possess a STRONG INITIATIVE.
Candidate MUST be able to run with a project on their own, with as little as a few sentences to begin the project with. Candidates requiring design specs will not be successful.
Retail experience is a plus.
Experience in the migration of data from an on premise database to a cloud based data warehouse platform is a strong plus.
Experience with Qlik, Business Objects, or Tableau is a plus.
A candidate with experience working with terabyte sized data warehouses and complex ETL mappings that process 50+ million records per day is strongly preferred.
NICE TO HAVE
Kubernetes and Docker experience a plus
Prior working experience on data science work bench
Cloud data warehouse experience - Snowflake is a plus
Data Modeling experience a plus
LANGUAGE/WRITING SKILLS
Strong, concise and grammatical oral and written communication skills.

MATHEMATICAL SKILLS

Basic math skills
REASONING/ ANALYTICAL ABILITY

Complex problem solving skills
Extensive data analytical skills
Initiative to develop efficiencies and process improvements
Documentation

Primary Location: United States of America-Texas-DALLAS-Irving-Information Services
Work Locations: Information Services Neiman Marcus 111 Customer Way Irving 75039
Job: Information Technology
Organization: Corporate
Schedule: Full-time
Shift: Day
Employee Status: Regular
Job Type: Standard
Job Level: Individual Contributor
Travel: No
Job Posting: Jun 3, 2019, 1:35:50 PM"
21,Sr. Data Engineer,"Plano, TX",Plano,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Locations: TX - Plano, United States of America, Plano, Texas

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Sr. Data Engineer

Candidates should possess strong knowledge and interest across big data technologies and have a background in data engineering. In this role you will be part of a collaborative, agile team you will:

Build data pipeline frameworks to automate high-volume and real-time data delivery for our Spark and streaming data hub

Transform complex analytical models in scalable, production-ready solutions

Provide support and enhancements for an advanced anomaly detection machine learning platform

Continuously integrate and ship code into our cloud production environments

Develop cloud based applications from the ground up using a modern technology stack

Work directly with Product Owners and customers to deliver data products in a collaborative and agile environment

Your responsibilities will include:
Developing sustainable data driven solutions with current new generation data technologies to drive our business and technology strategies

Building data APIs and data delivery services to support critical operational and analytical applications

Contributing to the design of robust systems with an eye on the long-term maintenance and support of the application

Leveraging reusable code modules to solve problems across the team and organization

Handling multiple functions and roles for the projects and Agile teams

Defining, executing and continuously improving our internal software architecture processes

Being a technology thought leader and strategist

Basic Qualifications:
Bachelor’s Degree

At least 2 years of experience on designing and developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python

At least 2 years experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC), Resource Management, Distributed Processing and RDBMS

At least 2 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps

At least 2 years of experience with SQL and Shell Scripting experience

Preferred Qualifications:
Master's Degree

1+ years’ experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service

2+ years of experience working with Streaming using Spark or Flink or Kafka or NoSQL

2+ years of experience working with Dimensional Data Model and pipelines in relation with the same

Intermediate level experience/knowledge in at least one scripting language (Python, Perl, JavaScript)

Hands on design experience with data pipelines, joining data between structured and unstructured data

Basic understanding of ETL standards and best practices

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
22,Data Engineer,"Plano, TX",Plano,TX,None Found,None Found,None Found,None Found,None Found,None Found,"
Based on understanding of system upstream & downstream, provide feedback and inputs on gaps in requirements and technical feasibility of requirements.
","Technical Lead

Qualification: Bachelors in science , engineering or equivalent

Responsibility:Project Planning and Setup:

Understand the project scope, identify activities/ tasks, task level estimates, schedule, dependencies, risks and provide inputs to Module Lead for review.
Provide inputs to testing strategy, configuration, deployment, hardware/software requirement etc.
Review plan and provide feedback on gaps, timeline and execution feasibility etc as required in the project.
Participate in KT sessions conducted by customer/ other business teams and provide feedback on requirements.

Requirement Understanding and Analysis: • Analyze functional/non functional requirements and seek clarifications for better understanding of requirements.

Based on understanding of system upstream & downstream, provide feedback and inputs on gaps in requirements and technical feasibility of requirements.

Design: • Prepare the LLD/ detailed design documents based on HLD and briefing from Module Lead.

Seek inputs from the developers on specific modules as applicable.
Consolidate all modules and provide to Module Lead/ Architects/ Designers for review.
Suggest changes in design on technical grounds.
Develop components inventory for the code to be developed tying it to the nonfunctional requirements.
Perform sampling of data to understand the character/ quality of the data (project dependent in the absence of data analyst or designer).
Identify tools and technologies to be used in the project as well as reusable objects that could be customized for the project.

Coding: • Follow coding standards and best practices to develop code and check code quality.

Share developed code with supervisor for review.
Rework on the code based on inputs if required.
Perform complex integration.
Guide the developers in identifying, preparing and conducting unit test cases and fixing defects based on results.
Consolidate the test results and share with supervisor.
Provide periodic status update to supervisor and highlight / recommend any changes in design based on challenges faced.
Anticipate unreported defects and raise the same to supervisor.
Conduct technical troubleshooting.
Conduct reviews for codes created by team.

Testing Management: • Develop unit test case for each module.

Conduct/ guide conducting of unit and integration testing and fix defects.
Review/ approve code to be moved to testing environment.
Provide support to the QA team and coordinate for various phases of testing.
Address queries raised by QA within defined timelines.
Investigate critical defects and establish need for fixing.
Raise issues to leads/QA.
Report defect status as per project standard process within agreed timelines.
Share revised code with supervisor for review.
Assist team lead and project manager on estimates around defect fixes.

Configuration Management: • Maintain versions of the code or consolidate version maintained by the Developers.

Provide support as required to the Administrators during configuration, code backups, deployment etc.

Deployment: • Assess and create deployment/ roll back plan.

Validate if all the components have been migrated and the right version is checked in.
Maintain deployment tracker.
Perform sanity checks post deployment to ensure smooth production.
Share activity status with supervisor and highlight concerns if any.

Project Execution Monitoring & closure (Support to Project Management activities): • Monitor work of the developers and share work achieved with them.

Provide guidance through SDLC.
Provide status of progress to leads.
In case of change requests, provide inputs on the plan.

Service Support and Maintenance:
Specific to production and maintenance support: • Provide support for 1 week and hand over to production team.

Identify if the incoming request is a service request/ defect during the warranty or an incident.
If it is code defect in the warranty, highlight to Lead and initiate defect fix process.
Post warranty, support in transition to maintenance team.

Knowledge Management: • Post release participate in project review call and discuss points on what went well and what didn't.

Create and update knowledge articles (case studies, lessons learnt) in the knowledge management repository.
Guide developers in creating such documents.
Publish white papers/ blogs/ articles (if required).

People Management: • Conduct training through academy or internally within the team.

Conduct technical, face to face interviews for internal transfer or external hiring.
Provide feedback on Developers form technical /domain standpoint to the module lead.

Technical Skills SNo Primary Skill Proficiency Level * Rqrd./Dsrd. 1 Snowflake PL1 Desired 2 Apache Spark PL3 Required 3 Amazon Web Services PL1 Required 4 Python PL1 Required 5 Core Java PL1 Desired


Proficiency Legends Proficiency Level Generic Reference PL1 The associate has basic awareness and comprehension of the skill and is in the process of acquiring this skill through various channels. PL2 The associate possesses working knowledge of the skill, and can actively and independently apply this skill in engagements and projects. PL3 The associate has comprehensive, in-depth and specialized knowledge of the skill. She / he has extensively demonstrated successful application of the skill in engagements or projects. PL4 The associate can function as a subject matter expert for this skill. The associate is capable of analyzing, evaluating and synthesizing solutions using the skill.

Employee Status : Full Time Employee
Shift : Day Job
Travel : No
Job Posting : Aug 28 2019

About Cognizant Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 193 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @Cognizant.
Cognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service."
23,Senior Big Data Engineer / ML Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,"
Participate in the design and development of a big data analytics application
Design, support and continuously enhance the project code base, continuous integration pipeline, etc.
Write complex ETL processes and frameworks for analytics and data management
Developing new processes and models
Implement large-scale near real-time streaming data processing pipelines
Work with a team of industry experts on cutting-edge big data technologies to develop solutions for deployment at massive scale",None Found,"
5+ years of experience in Hadoop ecosystem
3+ years of hands-on experience in architecting, designing, and implementing data ingestion pipes for batch, real-time, and streams.
3+ years of hands-on experience with a proven track record in building data lakes on Public Clouds (preferably GCP and Azure (HDInsight))
3+ years of hands-on experience in Bigdata tools such as Sqoop, Hive, Spark, Scala, HBase, MapReduce etc.
2+ years of experience in Python and Scala.
Ready to work onsite I the USA up to 3 months.
Experience in data wrangling, advanced analytic modeling, and AI/ML capabilities is preferred.
BA/BS required; preferably in Computer Science, Data Analytics, Data Science or Operations Research.
Highly analytical, motivated, decisive thought leader with solid critical thinking able to quickly connect technical and business ‘dots’.
Has strong communication and organizational skills and has the ability to deal with ambiguity while juggling multiple priorities and projects at the same time.
Able to understand statistical solutions and execute similar activities.","Responsibilities:
Participate in the design and development of a big data analytics application
Design, support and continuously enhance the project code base, continuous integration pipeline, etc.
Write complex ETL processes and frameworks for analytics and data management
Developing new processes and models
Implement large-scale near real-time streaming data processing pipelines
Work with a team of industry experts on cutting-edge big data technologies to develop solutions for deployment at massive scale
Requirements:
5+ years of experience in Hadoop ecosystem
3+ years of hands-on experience in architecting, designing, and implementing data ingestion pipes for batch, real-time, and streams.
3+ years of hands-on experience with a proven track record in building data lakes on Public Clouds (preferably GCP and Azure (HDInsight))
3+ years of hands-on experience in Bigdata tools such as Sqoop, Hive, Spark, Scala, HBase, MapReduce etc.
2+ years of experience in Python and Scala.
Ready to work onsite I the USA up to 3 months.
Experience in data wrangling, advanced analytic modeling, and AI/ML capabilities is preferred.
BA/BS required; preferably in Computer Science, Data Analytics, Data Science or Operations Research.
Highly analytical, motivated, decisive thought leader with solid critical thinking able to quickly connect technical and business ‘dots’.
Has strong communication and organizational skills and has the ability to deal with ambiguity while juggling multiple priorities and projects at the same time.
Able to understand statistical solutions and execute similar activities.
Will be a plus:
Having hands-on experience in using Infoworks / Nifi will be an added advantage.
Having experience in leading, guiding, and coaching data engineers is a plus.
Having exposure to R and ML technologies is a plus.
We offer:
Opportunity to work on bleeding-edge projects
Work with a highly motivated and dedicated team
Competitive salary
Flexible schedule
Benefits program
Social package - medical insurance, sports
Corporate social events
Professional development opportunities
About us:
Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, scalable omnichannel services, DevOps, and cloud enablement."
24,Big Data Engineer II,"Irving, TX",Irving,TX,None Found,None Found,"Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming
Experience in Big Data technologies (Hadoop, Spark, NiFi, Kafka)
Ability to demonstrate experience in distributed UNIX environments.
Experience in writing shell scripts
Ability to demonstrate proficiency in Microsoft Access, Excel, Word, PowerPoint and Visio.
Ability to multi-task and work under pressure.
Ability to be careful and thorough with detail.
Ability to work both independently and in a collaborative environment.
Ability to analyze information and use logic to address work related issues and problems.
Experience in the Healthcare Industry is a plus.","Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming
Experience in Big Data technologies (Hadoop, Spark, NiFi, Kafka)
Ability to demonstrate experience in distributed UNIX environments.
Experience in writing shell scripts
Ability to demonstrate proficiency in Microsoft Access, Excel, Word, PowerPoint and Visio.
Ability to multi-task and work under pressure.
Ability to be careful and thorough with detail.
Ability to work both independently and in a collaborative environment.
Ability to analyze information and use logic to address work related issues and problems.
Experience in the Healthcare Industry is a plus.","Design, implement, and test major subsystems of AWS or AZURE cloud platform and core service offerings using the Scrum agile framework
Develop and follow best practices relative to design, implementation, and testing
Prototype new ideas or technologies to prove efficacy and usefulness in production
Build a service structure on AWS or AZURE capable of being deployed and scaled to run a variety of platform components dynamically
Build a next-generation tools platform for creating, managing and deploying multi-channel outreach campaigns in the AWS or AZURE cloud
Construct a state-of-the-art data lake using Hadoop or Cassandra, Apache Spark, NiFi, and Kafka
Design & develop data pipelines for batch & streaming data sets using ETL and Data Integration tools, open source, AWS & Azure tech stack
Mentor junior team members","The knowledge typically acquired during the course of attaining a Bachelor’s degree in Computer Science, Mathematics, or related discipline is required. A combination of education and experience may be used in lieu of a diploma.",None Found,"We are seeking a passionate and intellectually curious Big Data Engineer II for our Data Engineering team. Data Engineering team is responsible for creating data pipelines in big data space including data lake and data warehouse in AWS (Amazon Web Services), Azure cloud and on premise environments.

Essential Responsibilities:
Design, implement, and test major subsystems of AWS or AZURE cloud platform and core service offerings using the Scrum agile framework
Develop and follow best practices relative to design, implementation, and testing
Prototype new ideas or technologies to prove efficacy and usefulness in production
Build a service structure on AWS or AZURE capable of being deployed and scaled to run a variety of platform components dynamically
Build a next-generation tools platform for creating, managing and deploying multi-channel outreach campaigns in the AWS or AZURE cloud
Construct a state-of-the-art data lake using Hadoop or Cassandra, Apache Spark, NiFi, and Kafka
Design & develop data pipelines for batch & streaming data sets using ETL and Data Integration tools, open source, AWS & Azure tech stack
Mentor junior team members
Non-Essential Responsibilities:
Other duties as assigned
Qualifications : Knowledge, Skills and Abilities:
Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming
Experience in Big Data technologies (Hadoop, Spark, NiFi, Kafka)
Ability to demonstrate experience in distributed UNIX environments.
Experience in writing shell scripts
Ability to demonstrate proficiency in Microsoft Access, Excel, Word, PowerPoint and Visio.
Ability to multi-task and work under pressure.
Ability to be careful and thorough with detail.
Ability to work both independently and in a collaborative environment.
Ability to analyze information and use logic to address work related issues and problems.
Experience in the Healthcare Industry is a plus.
Work Conditions and Physical Demands:
Primarily sedentary work in a general office environment
Ability to communicate and exchange information
Ability to comprehend and interpret documents and data
Requires occasional standing, walking, lifting, and moving objects (up to 10 lbs.)
Requires manual dexterity to use computer, telephone and peripherals
May be required to work extended hours for special business needs
May be required to travel at least 10% of time based on business needs
Minimum Education:
The knowledge typically acquired during the course of attaining a Bachelor’s degree in Computer Science, Mathematics, or related discipline is required. A combination of education and experience may be used in lieu of a diploma.
Minimum Related Work Experience:
2-4 years’ experience designing and delivering production software
2 years’ experience designing and implementing big data high performance operational systems
Proven experience using the Microsoft development tools and stack, e.g., TFS, Github, Eclipse, JVM, etc
Nothing in this job description restricts management’s right to assign or reassign duties and responsibilities to this job at any time.

EOE including disability/veteran.
Job Posting : Aug 13, 2019, 4:09:21 PM
Work Locations : USA-Texas-Irving"
25,AI Model Development Lead for Virtual Channels (Analytic Manager 5),"Dallas, TX",Dallas,TX,None Found,None Found,"
8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques",None Found,None Found,None Found,None Found,"Overview:
Wells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.

SUCCESS PROFILE
Check out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.

Analytical
Detail-oriented
Insightful
Inventive
Problem Solver
Curious
Benefits
Wells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:

Medical, Dental and Vision
Employer Matching 401(k)
Tuition Reimbursment
Maternity and Paternity Leave
Paid Time Off
Responsibilties
Job Description
At Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.
Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.
Enterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.
As part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, Virtual Channels, and virtual channels is looking for an experienced AI leader to manage the development of AI models for Virtual Channels.

This leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on Virtual Channels’ AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and Virtual Channels executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.

KEY RESPONSIBILITIES INCLUDE:
Build and grow a team of data scientists responsible for AI model development in support of Virtual Channels
Design, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions
Partner with Virtual Channels executives to frame the problem, manage the model development process, and business relationship
Manage a portfolio of the data science projects including the following responsibilities:
Help finalize project scope working with business partners
On-going touch-base with business partners and governance stakeholders
Define priorities in partnership with the business partners during on-going development
Work with AI technology and production teams to operationalize models
Will be called upon to review vendor models and solutions and/or models developed outside of EADS
Divisional Information:
Data Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.
As a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:
Lead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.
Accomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.

Required Qualifications

8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques


Desired Qualifications

Strong analytical skills with high attention to detail and accuracy
Ability to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members
Meeting facilitation experience in leading discussions that result in consensus and commitment


Other Desired Qualifications
4+ years managing or directing data scientist/ statistician/ data engineer teams
Hands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet
Detail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable
Experience with agile project management methodologies for data science
Experience with Big Data or Hadoop tools such as Spark, Hive, Kafka and Map

Street Address
NC-Charlotte: 401 S Tryon St - Charlotte, NC
MN-Minneapolis: 600 S 4th St - Minneapolis, MN
NC-Charlotte: 11625 N Community House Road - Charlotte, NC
SC-Fort Mill: 3480 State View Blvd - Fort Mill, SC
TX-Addison: 5080 Spectrum Dr - Addison, TX
TX-DAL-Downtown Dallas: 1445 Ross Ave - Dallas, TX
TX-Irving: 5000 Riverside Drive - Irving, TX
AZ-Tempe: 1150 W Washington St - Tempe, AZ
IA-Des Moines: 6200 Park Ave - Des Moines, IA
IA-Des Moines: 800 Walnut St - Des Moines, IA
GA-Atlanta: 3579 Atlanta Ave - Atlanta, GA


Disclaimer

All offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.

Relevant military experience is considered for veterans and transitioning service men and women.

Wells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.

ENT FINANCE"
26,AI Model Development Lead For Marketing (Analytics Manager 5),"Addison, TX 75001",Addison,TX,75001,None Found,"
8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques
4+ years of experience working with digital data science including analyzing cookie-level data",None Found,None Found,None Found,None Found,"Overview:
Wells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.

SUCCESS PROFILE
Check out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.

Analytical
Detail-oriented
Insightful
Inventive
Problem Solver
Curious
Benefits
Wells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:

Medical, Dental and Vision
Employer Matching 401(k)
Tuition Reimbursment
Maternity and Paternity Leave
Paid Time Off
Responsibilties
Job Description
At Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.
Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.
Enterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.
As part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, marketing, and virtual channels is looking for an experienced AI leader to manage the development of AI models for marketing.

This leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on marketing’s AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and marketing executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.
KEY RESPONSIBILITIES INCLUDE:
Build and grow a team of data scientists responsible for AI model development in support of marketing
Design, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions
Partner with marketing executives to frame the problem, manage the model development process, and business relationship
Manage a portfolio of the data science projects including the following responsibilities:
Help finalize project scope working with business partners
On-going touch-base with business partners and governance stakeholders
Define priorities in partnership with the business partners during on-going development
Work with AI technology and production teams to operationalize models
May be called upon to review vendor models and solutions and/or models developed outside of EADS
Divisional Information:
Data Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.
As a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:
Lead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.
Accomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.

Required Qualifications

8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques
4+ years of experience working with digital data science including analyzing cookie-level data


Desired Qualifications

Strong analytical skills with high attention to detail and accuracy
Ability to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members
Meeting facilitation experience in leading discussions that result in consensus and commitment


Other Desired Qualifications
Data science experience in the ad tech space (DMPs, DSPs, Google and/or Adobe Cloud, digital attribution)
4+ years managing or directing data scientist/ statistician/ data engineer teams
Hands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet
Detail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable
Experience with agile project management methodologies for data science

Disclaimer

All offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.

Relevant military experience is considered for veterans and transitioning service men and women.

Wells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.

ENT FINANCE"
27,Sr Security Data Engineer - Security Incident Response Team,"Dallas, TX 75201",Dallas,TX,75201,None Found,"Financial Services industry experiencePrevious work experience in Cyber Security field is a plus.Experience with the Hadoop eco-system (HDFS, Spark)Experience with cloud based big data platforms such as AWS or Google a plus.",None Found,None Found,None Found,None Found,"MORE ABOUT THIS JOB
Goldman Sachs Technology Risk is leading threat, risk analysis and data science initiatives
that are helping to protect the firm and our clients from information and cyber security risks. Our team equips the firm with the knowledge and tools to measure risk, identify and mitigate threats and protect against unauthorized disclosure of confidential information for our clients, internal business functions, and our extended supply chain.
SECURITY INCIDENT RESPONSE TEAM (SIRT) supports and enables a comprehensive technical Cyber Defense program for the firm while increasing awareness of current and potential Cyber Threats. Works across the organization to operate efficiently, provide technical
investigative support and mitigate threats to the firm.
Do you enjoy solving challenging puzzles? Protecting critical networks from cyber-attacks? Designing and integrating state-of-the-art technical solutions? A position as a Security Data Engineer on Goldman Sachs’ Threat Management Center lets you do all this and more:
RESPONSIBILITIES AND QUALIFICATIONS
HOW YOU WILL FULFILL YOUR POTENTIAL
Design and develop data ingest and transform processesEngineer streaming data processing pipelinesDrive adoption of Cloud technology for data processing and warehousingEngage with data consumers and producers in order to design appropriate models to suit all needsApply latest technologies in machine learning, data mining, and predictive analytics to correlate the big datasets and events, and derive dynamic cybersecurity rules.Collaborate with a global team to continually operate and improve a world-class cyber program by driving the uplift of sensory tools, detection tuning, and access to data sources to increase detection effectiveness by applying data analytics.Participate in a 24x7 coverage model to prevent and remediate security threats against Goldman Sachs’ global business network.

SKILLS AND EXPERIENCE WE ARE LOOKING FOR
5+ years of relevant work experience in a team-focused environmentBachelor’s degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)Working knowledge one or more programming languages (Python, Java, C++, C#, etc.)Extensive knowledge and proven experience applying domain driven design to build complex business applicationsDeep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processesIn-depth knowledge of relational and columnar SQL databases, including database designExcellent communications skills and the ability to work with subject matter experts to extract critical conceptsAbility to multi-task and prioritize work effectivelyIndependent thinker, willing to engage, challenge or learnHighly motivated self-starter who can provide thought leadership in big data analyticsStrong work ethic, a sense of ownership and urgencyStrong analytical and problem solving skillsResponsive to challenging tasking.Ability to document and explain technical details in a concise and understandable manner.Strong sense of ownership and driven to manage tasks to completion.

Preferred Qualifications
Financial Services industry experiencePrevious work experience in Cyber Security field is a plus.Experience with the Hadoop eco-system (HDFS, Spark)Experience with cloud based big data platforms such as AWS or Google a plus.
ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

Â© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet."
28,Senior Big Data Engineer,"Irving, TX",Irving,TX,None Found,None Found,"
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.",None Found,"
Responsible for design, prototyping and delivery of software solutions within the big data eco-system
Leading projects and/or serving as analytics SME to provide new or enhanced data to the business
Improving data governance and quality increasing the reliability of our data
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
",None Found,None Found,"Where good people build rewarding careers.
Think that working in the insurance field can’t be exciting, rewarding and challenging? Think again. You’ll help us reinvent protection and retirement to improve customers’ lives. We’ll help you make an impact with our training and mentoring offerings. Here, you’ll have the opportunity to expand and apply your skills in ways you never thought possible. And you’ll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.
About our team
360 Finance Advanced Analytics data engineering team works with multiple internal and external data sources to deliver data that is readily available, easily accessible, accurate and complete. They are responsible for building a centralized data lake/hub using the Hadoop ecosystem that will be used by Reporting & Operational Analytics teams and the Machine learning teams.
Job Description
This Lead Consultant is an experienced professional who is responsible for leveraging data and analytics to help automate and optimize Claims Analytics Data processes enabling our Claims employees to focus on serving our customers and delivering the most advanced claims experience on the planet. They will be responsible for the strategy around how we bring together complex data into clean and useful data structures making our valuable data more approachable.
Key Responsibilities
Responsible for design, prototyping and delivery of software solutions within the big data eco-system
Leading projects and/or serving as analytics SME to provide new or enhanced data to the business
Improving data governance and quality increasing the reliability of our data
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
Key Responsibilities (Cont'd)
Responsible for designing and building new Big Data systems for turning data into actionable insights
Train and mentor junior team members on Big Data/Hadoop tools and technologies
Identifies opportunities for improvement and presents recommendations to management
Seeks out and evaluates emerging big data technologies and open-source packages
Participate in strategic planning discussions with technical and non-technical partners
Uses, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).
Uses, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e. Java, Python, SQL, R)
Job Qualifications
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.
Job Qualifications (Cont'd)
Some understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.
Experience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required
4-5+ years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required
Experience with Agile development methodologies and tools to iterate quickly on product changes, developing user stories and working through backlog (Continuous Integration and JIRA a plus)
Experience with Airflow a plus
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work. Good Life. Good Hands®.
As a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.
Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.
Allstate generally does not sponsor individuals for employment-based visas for this position.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click “here” for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click “here” for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.
It is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment."
29,Google Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
",DevOps on an GCP platform. Multi-cloud experience a plus.,None Found,None Found,"Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet today’s high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps – Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
30,Senior Big Data Engineer*,"Irving, TX",Irving,TX,None Found,None Found,"Bachelors/4 Year Degree.
7-10 years of experience.
Expert knowledge of Big Data technologies including but not limited to Python and/or Databricks.
Strong Analytical and problem-solving skills.
Knowledgeable in cloud platforms (preferable AWS: both traditional EC2 and serverless ambda), micro-services architecture, CI/CD solutions (including Docker), DevOps principles, message queue system.
Proficiency in API security frameworks, token management and user access control including OAuth, JWT, etc.
Solid foundation and understanding of relational and NoSQL database principles.",None Found,"
Development of custom APIs for inbound/outbound data integrations, Spark jobs for ETL development and data integrations to internal and external systems.
Participating in designing new data applications.
Code deployments, Dev Ops, and Reviews.",None Found,None Found,"Overview
Who we are
Imagine working in a place where continuous improvement and innovation is celebrated and rewarded; where fast-paced, high-impact teams come together to positively drive results for one of the largest & most iconic brands in the world.

As the only rapidly growing retailer, you may know us as your friendly neighborhood store. You probably know our familiar name, have seen our pervasive logo, and have tried our highly sought-after products, such as Slurpee® and Big Bite®. “Brain Freeze” is a 7-Eleven registered trademark for our 53-year old Slurpee® and with over 67,000 stores globally (more than any other retailer or food service provider), we sell over 14 million a month.

But there’s a lot more to our story and much more left to be written. We are transforming our business, ensuring we are customer obsessed and digitally enabled to seamlessly link our brick and mortar stores with digital products and services.
At 7-Eleven the entrepreneurial spirit is in our DNA and has been ever since our inception 90+ years ago. It’s what drove us to invent the convenience industry in 1927 by envisioning how a simple ice dock could provide household staples such as milk and eggs to better serve the needs of our customers.
Today we are redefining convenience and the customer experience in big ways...we are fundamentally changing our culture and we want talented, innovative, customer obsessed, and entrepreneurial people like you to come make history with us.
How we lead
At 7-Eleven we are guided by our Leadership Principles.
Be Customer Obsessed
Be Courageous with Your Point of View
Challenge the Status Quo
Act Like an Entrepreneur
Have an “It Can Be Done” Attitude
Do the Right Thing
Be Accountable
Each principle has a defined set of behaviors which help guide the 7-Eleven team to Serve Customers and Support Stores.
About This Opportunity
Responsibilities
Work in the digital delivery organization to partner with vendors to support delivery of data integration solutions utilizing tools such as Databricks, Python custom development, Azure containers, and other Azure tool kits. Must have a strong and continuously evolving technical mastery of RESTful API development focused on Python.
Development of custom APIs for inbound/outbound data integrations, Spark jobs for ETL development and data integrations to internal and external systems.
Participating in designing new data applications.
Code deployments, Dev Ops, and Reviews.
Qualifications
Bachelors/4 Year Degree.
7-10 years of experience.
Expert knowledge of Big Data technologies including but not limited to Python and/or Databricks.
Strong Analytical and problem-solving skills.
Knowledgeable in cloud platforms (preferable AWS: both traditional EC2 and serverless ambda), micro-services architecture, CI/CD solutions (including Docker), DevOps principles, message queue system.
Proficiency in API security frameworks, token management and user access control including OAuth, JWT, etc.
Solid foundation and understanding of relational and NoSQL database principles."
31,Data Engineering/ETL/BI - Senior/Mid-Level Roles,"Dallas, TX 75207",Dallas,TX,75207,None Found,None Found,None Found,"Design and develop data models, ETL mappings and associated database objects for analytical solutions using ERWin, ETL tools like Informatica, Oracle DB.
Create/review technical documentation for all new and modified data model.
Review solution design and ensure that the defined EDW standards and framework are followed.
Review and validate logical and physical design to ensure alignment with the defined solution architecture.
Ensure quality assurance plan and cases are comprehensive to validate the solution thoroughly.
Experience developing Packages, Procedures and Functions with PL/SQL to support ETL processes.
Experience in performance analysis and optimization of reports, dashboards, ETL and other components.
Tableau or equivalent reporting platform’s Infrastructure experience, including tuning performance issues, monitoring applications, analyzing logs and performing system operations
",None Found,None Found,"Job Category
Products and Technology
Job Details
Senior Data Engineer - Dallas
Software Engineer - ETL - Informatica
Tableau Developer/Admin

Locations: Dallas


Location Guidance - While we are always looking for great talent for this role in all locations , we are currently hiring for opportunities in Dallas . Please don't let that stop you from applying. While this/these are the current openings, your information will be saved for new opportunities as they open in all locations.


In school, or graduated within the last 12 months? Please visit FutureForce for opportunities.
Senior Data Engineer - Dallas

The IT Enterprise Data Warehouse (EDW) function is responsible for the delivery of operational reporting and performance metrics to various business domains including Sales and Sales Operations, Marketing, Finance, and Employee Success. Team also manages all aspects of rolling out Salesforce’s analytics tool Wave (Einstein Analytics) to internal business partners. The Salesforce IT EDW team is looking for an experienced BI developer who will work on designing, developing and implementing new functionality and increasing test coverage of systems that support various internal business processes at Salesforce.com . This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel.


Responsibilities
Design and develop data models, ETL mappings and associated database objects for analytical solutions using ERWin, ETL tools like Informatica, Oracle DB.
Create/review technical documentation for all new and modified data model.
Review solution design and ensure that the defined EDW standards and framework are followed.
Review and validate logical and physical design to ensure alignment with the defined solution architecture.
Ensure quality assurance plan and cases are comprehensive to validate the solution thoroughly.
Experience developing Packages, Procedures and Functions with PL/SQL to support ETL processes.
Experience in performance analysis and optimization of reports, dashboards, ETL and other components.
Tableau or equivalent reporting platform’s Infrastructure experience, including tuning performance issues, monitoring applications, analyzing logs and performing system operations
Required Skills
Bachelor's Degree in Computer Science, MIS, or a related discipline, with 7+ years of related Information systems experience in Data Engineering, Data warehousing, Business Intelligence and delivery of BI solutions.
Deep understanding of Data extraction mechanisms, data warehousing concepts, relational star-schema database designs, big data platforms and associated tools.
4+ years of experience in implementing solutions using Informatica and Oracle database or equivalent BI/reporting platforms.
Deep and strong knowledge of SQL and relational database models.
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Experience working in an agile environment is required, must have a clear understanding of the role and responsibility of a Scrum Master.
Excellent team player able to work with virtual and global across functional teams at all levels.
Self-starter, highly motivated, able to shift directions quickly when priorities change, think through problems to come up with innovative solutions and deliver against tight deadlines.
Excellent spoken and written communication as well as receptive listening skills, with the ability to present complex ideas in a clear, concise fashion to technical and nontechnical audiences.
Excellent interpersonal skills will be needed in order to build strong relationships that will be critical for the success of this role.
Nice to Have
Deep understanding and hands-on experience in unix or Python scripting
Understanding of Cloud Infrastructure and ecosystem: AWS or GCP
Hands on experience in Tableau tool implementations.
Knowledge around salesforce products
Software Engineer - ETL - Informatica - Dallas, TX


The IT Enterprise Data Services (EDS) function has historically been responsible for the delivery of operational reporting and performance metrics to various business domains including Sales and Sales Operations, Marketing, Finance, and Employee Success. This platform also includes management of a Big Data Hadoop platform to store unstructured application log data that is generated by the Salesforce product offerings, this data is predominately used by the data science teams. Team also manages all aspects of rolling out Salesforce’s analytics tool Wave to internal business partners.


The Salesforce IT EDS team is looking for an experienced ETL Engineer who will work on designing, developing and implementing new functionality and increasing test coverage of systems that support various internal business processes at Salesforce.com. This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel.


Responsibilities:
Design and development of analytical solutions including Informatica,Oracle, Cloud, Hadoop technologies.
Identify opportunities to refine the code release and deployment processes
Actively Participate in performance testing of various BI tiers and tools, monitor resource utilization and provide improvement recommendations.
Maintain the tools and framework defined for automating unit and system testing of ETL processes.
Develop and implement ETL frameworks using Shell and Python languages.
Design and Develop Pig/hive as well support existing jobs in Hadoop
Evaluate, determine root cause and resolve production issues.
Evaluate and actively participate in migration of BI services to cloud.
Required Skills:
Bachelor's Degree in Computer Science, MIS, or related discipline, with 5+ years related ETL experience.
Deep understanding of Informatica 9.x, and Oracle 11g system components, Big data/ Hadoop internal processes and architecture
Excellent knowledge with Python scripting and hand-on experience with scheduler tools like Control-M, Tidal, DAC etc.
Hands on knowledge in Git repository branching and code versioning
Good knowledge of SQL and relational database models.
Deep hands-on experience with Linux is a must
Hands-on experience in process automation, test automation, technology efficiency, and best practices
Good understanding of data warehousing concepts and relational star-schema database designs
Experience working with IT systems in a global business environment
Excellent team player able to work with virtual and global crosses functional teams.
Ability to working an Agile/Scrum environment and manage multiple projects/tasks in a fast paced environment.

Desired Skills:
Salesforce, Data Warehousing, ERP solutions and Data Analysis
Data visualization tool experience like Tableau, Qlikview or Spotfire is a plus.
Strong understanding of Hadoop ecosystem (HDFS, YARN, Zookeeper, HCatalog, HBase, Pig, Hive)
Java, HTML,CSS knowledge is preferable
Tableau Developer/Admin
Enterprise Data Services - Member of Technical Staff


Salesforce, the Customer Success Platform and world's #1 CRM, empowers companies to connect with their customers in a whole new way. We are the fastest growing of the top 10 enterprise software companies, the World’s Most Innovative Company according to Forbes, and one of Fortune’s 100 Best Companies to Work For six years running. The growth, innovation, and Aloha spirit of Salesforce are driven by our incredible employees who thrive on delivering success for our customers while also finding time to give back through our 1/1/1 model, which leverages 1% of our time, equity, and product to improve communities around the world. Salesforce is a team sport, and we play to win. Join us!The IT Enterprise Data Warehouse (EDW) function is responsible for the delivery of operational reporting and performance metrics to various business domains including Sales and Sales Operations, Marketing, Finance, and Employee Success.Team also manages all aspects of rolling out Salesforce’s analytics tool Wave to internal business partners.The Salesforce IT EDW team is looking for an experienced BI developer who will work on designing, developing and implementing new functionality and increasing test coverage of systems that support various internal business processes at Salesforce.com . This requires the candidate to be able to learn quickly, work in a fast paced environment, and have the ability to communicate well with technical and non-technical personnel.


Responsibilities
Install, Configure and manage Tableau platform or equivalent reporting platforms, Configuring the metadata repository (RPD) at the physical, logical and presentation layers to meet business requirements
Install , Configure and Maintain our Informatica, Business objects & Tidal scheduler .
Perform performance analysis and optimization of BI (Tableau or Business objects) reports, dashboards, ETL and other components.
Tableau or equivalent reporting platform’s Infrastructure experience, including tuning performance issues, monitoring applications, analyzing logs and performing system operations
Reporting layer security configuration experience, including Single Sign-On with EBS Financials R12, definitions of groups and roles, restricting dashboard and Analysis access to specific groups, performing data filtering based on user group, and configuring LDAP integration
Build Frameworks and automation across our stack for efficiency.
Required Skills
Bachelor's Degree in Computer Science, MIS, or a related discipline, with 5+ years of related Information systems experience in Data Engineering, Datawarehousing, Business Intelligence and delivery of BI solutions.
Basic understanding of Data extraction mechanisms, data warehousing concepts, relational star-schema database designs, big data platforms and associated tools.
Deep understanding and hands-on experience in unix shell scripting
System Administration experience with Tableau, Informatica , Enterprise schedulers or equivalent BI / reporting platforms
Cloud infrastructure knowledge of Aws or GCP is preferred.
Deep and strong knowledge of SQL and relational database models.
Experience working in an agile environment is required, must have a clear understanding of the role and responsibility of a Scrum Master.
Excellent team player able to work with virtual and global across functional teams at all levels.
Self-starter, highly motivated, able to shift directions quickly when priorities change, think through problems to come up with innovative solutions and deliver against tight deadlines.
Excellent spoken and written communication as well as receptive listening skills, with the ability to present complex ideas in a clear, concise fashion to technical and nontechnical audiences.
Excellent interpersonal skills will be needed in order to build strong relationships that will be critical for the success of this role.
Excellent analytical, problem solving and debugging skills, with strong ability to quickly learn and solve problems in order to effectively develop technical solutions to their requirements.
Posting Statement
Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay fees to any third-party agency or company that does not have a signed agreement with Salesforce.com or Salesforce.org."
32,Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"7+ years of experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.4+ years of experience data modeling concepts3+ years of Python and/or Java development experience3+ years experience in Big Data stack environments (EMR, Hadoop, MapReduce, Hive)

AWS BTS OPS group team is looking for a Sr. Big Data Engineer to play a key role in building their industry leading Customer Information Analytics Platform. Are you passionate about Big Data and highly scalable data platforms? Do you enjoy building end to end Analytics solutions to help drive business decisions? And if you have experience in building and maintaining highly scalable data warehouses and data pipelines with high transaction volumes then we need you!!!

The full stack Data Engineer will design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for analytics and deep learning. Implement data ingestion routines both real time and batch using best practices in data modeling, ETL/ELT processes leveraging AWS technologies and Big data tools. Provide on-line reporting and analysis using business intelligence tools and a logical abstraction layer against large, multi-dimensional datasets and multiple sources. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture. Produce comprehensive, usable dataset documentation and metadata. Provides input and recommendations on technical issues to the project manager.

. Meets/exceeds Amazon’s leadership principles requirements for this role
. Meets/exceeds Amazon’s functional/technical depth and complexity for this role
Experience in gathering requirements and formulating business metrics for reporting.
Experience with Kafka, Flume and AWS tool stack such as Redshift and Kinesis are preferred.Experience building on AWS using S3, EC2, Redshift, DynamoDB, Lambda, QuickSight, etc.Experience using software version control tools (Git, Jenkins, Apache Subversion)AWS certifications or other related professional technical certificationsExperience with cloud or on-premise middleware and other enterprise integration technologiesExperience in writing MapReduce and/or Spark jobsDemonstrated strength in architecting data warehouse solutions and integrating technical componentsGood analytical skills with excellent knowledge of SQL.4+ years of work experience with very large data warehousing environmentExcellent communication skills, both written and verbal
Amazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation."
33,Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,None Found,None Found,"
3+ years of data engineering experience
Robust experience with Python, Spark, and Jenkins
Experience writing and executing complex SQL queries
Experience building data pipelines and ETL design (implementation and maintenance)
Experience with AWS or other cloud provider
Scrum/Agile software development process.","We’re looking for an experienced Data Engineer to help deliver critical business intelligence through our data warehouse. Our Data Engineering team handles all aspects of managing our data lake, data warehouse, and real-time data pipelines from four ecommerce products. This position is responsible for understanding stakeholders requirements and building out application database to data warehouse ETL.
Requirements
3+ years of data engineering experience
Robust experience with Python, Spark, and Jenkins
Experience writing and executing complex SQL queries
Experience building data pipelines and ETL design (implementation and maintenance)
Experience with AWS or other cloud provider
Scrum/Agile software development process.
Nice to have
Experience with data technologies, Hadoop, Hive, Postgres, Kafka, Solr
Deep understanding of AWS services: EMR, S3, Redshift
Operational experience with Jenkins or Airflow
Proficiency in using BI dashboard tools
Analytical experience debugging slow queries and scripts
About Us

Scalable Press is using technology to reinvent the mass customization industry.

Scalable Press was founded with an initial investment of just $2,000. Today, we’re on track to produce more than 12 million shipments across 185 countries. We’re bootstrapped, profitable, and rapidly expanding.

We operate four B2C and B2B websites that are accessed by over 100 million visitors per year. Whether it’s embroidered dress shirts, all-over printed socks, or digitally printed t-shirts, we make it easier, cheaper, and faster to order customized goods than ever before.

Scalable Press is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements."
34,Data Warehouse Engineer,"Richardson, TX 75082",Richardson,TX,75082,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Steward Software Development is looking for a driven, pragmatic Data Engineer with real-world experience creating and maintaining enterprise-grade data solutions in the cloud. Utilizing industry-leading technologies and modern architectural concepts, the Data Engineer will provide expertise in multiple areas including data systems design, web-based workflows (ETL), advanced analytics, BI, Big Data, cloud storage and cloud processing.
Key Responsibilities
ò Design, build and implement enterprise-grade data solutions for large-scale, multi-tenant web applications
ò Produce high-quality, well architected solutions with an upfront focus on testing and long-term support
ò Ensure best practices and industry standard patterns are utilized whenever possible
ò Actively review existing code and other developers' work and provide feedback
ò Collaborate with developers and analysts on technical and functional designs
ò Work with the Product Owner and key stakeholders to fully understand functional requirements for enhancements and new products
ò Assist with project scoping, sprint and release planning
ò Work with QA and DevOps to perform unit/integrated testing and assist with development of automation strategies and tools
ò Collaborate with DevOps to help identify areas for performance improvement
ò Advocate Agile Software Development principles and practices
ò Participate and assist in conducting daily scrum meetings
ò Contribute to team body of knowledge by educating colleagues on industry developments and new client-side technologies
Qualifications
Education
ò BA/BS/MS in Computer Science or related 4+-year technical degree
Experience
ò 3+ years of experience in data engineering, or equivalent DBA + software development
Skills
ò Solid background in computer science and mathematics
ò Proficient with relational database management systems, preferably Microsoft SQL Server
ò Proficient with SQL and preferably with T-SQL
ò Demonstrated experience developing and supporting high performance batch extract, transform, cleanse and load processes using industry standard tools such as SQL Server Integration Services
ò Experience with SQL Server Analysis Services and Data Analysis Expressions (DAX)
ò Experience with BI tools such as SQL Server Reporting Services, Power BI, Tableau
ò Experience with real-time data processing and integration in the cloud (IoT) with technology like Microsoft Azure Stream Analytics
ò Knowledge and some experience with Big Data technologies such as Hadoop, HDInsight, Azure Data Lake
ò Proficient in object-oriented programming concepts and development techniques
ò Proficient with at least one OO language, preferably C#
ò Experience with server-side frameworks and environments such as ASP.NET MVC, Python, PHP, Node
ò Knowledge of ORM tools like Entity Framework, NHibernate, Dapper
ò Knowledge of service-oriented architecture (SOA) and related architectural styles using REST, SOAP, RPC, XML,
ò Experience with modern distributed version control systems like Git
ò Experience with test, build, deploy (CI) software such as Microsoft Team Services, Bamboo, Jenkins, TeamCity, etc.
ò Experience with an integrated development environment (IDE), preferably Visual Studio
ò Experience working in an Agile-based development environment and knowledge of different methods such as Scrum, Feature-driven development (FDD) and Kanban
General
ò Strong analytical and decision-making skills
ò Excellent oral and written communication skills
ò Strong desire to seek out and learn new technologies and development practices
ò Ability and willingness to adapt quickly to changing priorities and requests
ò Ability to thrive under pressure and take control of urgent and demanding issues
About Steward Software Development
Steward Health Care is working on something profoundly different. We have an unwavering commitment to enhancing care delivery with the use of cutting edge technology and we are looking for highly skilled engineers with the desire to apply their talents to something genuinely valuable. It's time to truly advance the way we provide care.
The Software Engineering team at Steward Health Care is focused on advanced web-based application development utilizing industry leading predictive modeling and cloud computing to provide elegant, innovative solutions in a myriad of care settings. We believe in a culture of trust, open communication and regular collaboration that allows the cultivation of team harmony. Our environment values hard work, creativity and a devotion to learning. We love to be challenged, have an innate need to solve problems and enjoy unparalleled satisfaction from delivering beautifully crafted solutions.

Location: Corporate · 1301.72317 IS Acquisitions
Schedule: Full Time, Flexible hours, 9:00am-5:30pm"
35,Big Data Engineer,"Plano, TX",Plano,TX,None Found,None Found,None Found,"
Hands-on experience with the Hadoop eco-system - HDFS, MapReduce, HBase, Hive, Impala, Spark, Kafka
Experience in implementing Hadoop Data Lakes - Data storage, partitioning, splitting, file types (Parquet, Avro, ORC) for specific use cases etc.
Experience with Query languages – SQL, Hive, Impala, Drill etc.
Experience with NoSQL databases – MapR-DB, HBase, MongoDB, Cassandra etc.
Experience in agile(scrum) development methodology
Exposure to Data ingestion frameworks such as Kafka, Sqoop, Storm, Nifi, Spring Cloud, etc.
Experience with building stream-processing systems using solutions such as Kafka, MapR-Streams, Spark-Streaming etc.
Experience with development/automation skills. Must be very comfortable with reading and writing Scala, Python or Java code
Experience with one of the Hadoop open source distributions - MapR and Cloudera
Bachelor or master’s Degree in engineering in Computer Science or Information Technology","
Partner with data analyst, product owners and data scientists, to better understand requirements, solution designs, finding bottlenecks, resolutions, etc.
Support/Enhance data pipelines and ETL using heterogeneous sources
Transform data using data mapping and data processing capabilities like Spark, Spark SQL, HiveQL etc.
Expands and grows data platform capabilities to solve new data problems and challenges
Ability to dynamically adapt to conventional big-data frameworks and tools with the use-cases required by the project",None Found,"10+ years Enterprise Development
5+ years of experience with the Hadoop and Big Data technologies
2+ Years Design with Big Data Strategies.","Req ID: 58054

At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees are key factors in our company’s growth, market presence and our ability to help our clients stay a step ahead of the competition. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA Services and for the people who work here.

NTT DATA Services currently seeks a Big Data Engineer to join our team in Plano, Texas (US-TX), United States (US).

Responsibilities
Partner with data analyst, product owners and data scientists, to better understand requirements, solution designs, finding bottlenecks, resolutions, etc.
Support/Enhance data pipelines and ETL using heterogeneous sources
Transform data using data mapping and data processing capabilities like Spark, Spark SQL, HiveQL etc.
Expands and grows data platform capabilities to solve new data problems and challenges
Ability to dynamically adapt to conventional big-data frameworks and tools with the use-cases required by the project

Basic Skill Requirements
10+ years Enterprise Development
5+ years of experience with the Hadoop and Big Data technologies
2+ Years Design with Big Data Strategies.
Other Skills
Hands-on experience with the Hadoop eco-system - HDFS, MapReduce, HBase, Hive, Impala, Spark, Kafka
Experience in implementing Hadoop Data Lakes - Data storage, partitioning, splitting, file types (Parquet, Avro, ORC) for specific use cases etc.
Experience with Query languages – SQL, Hive, Impala, Drill etc.
Experience with NoSQL databases – MapR-DB, HBase, MongoDB, Cassandra etc.
Experience in agile(scrum) development methodology
Exposure to Data ingestion frameworks such as Kafka, Sqoop, Storm, Nifi, Spring Cloud, etc.
Experience with building stream-processing systems using solutions such as Kafka, MapR-Streams, Spark-Streaming etc.
Experience with development/automation skills. Must be very comfortable with reading and writing Scala, Python or Java code
Experience with one of the Hadoop open source distributions - MapR and Cloudera
Bachelor or master’s Degree in engineering in Computer Science or Information Technology

This position is only available to those interested in direct staff employment opportunities with NTT DATA, Inc. or its subsidiaries. Please note, 1099 or corp-2-corp contractors or the equivalent will NOT be considered. We offer a full comprehensive benefits package that starts from your first day of employment.
About NTT DATA Services

NTT DATA Services partners with clients to navigate and simplify the modern complexities of business and technology, delivering the insights, solutions and outcomes that matter most. We deliver tangible business results by combining deep industry expertise with applied innovations in digital, cloud and automation across a comprehensive portfolio of consulting, applications, infrastructure and business process services.

NTT DATA Services, headquartered in Plano, Texas, is a division of NTT DATA Corporation, a top 10 global business and IT services provider with 118,000+ professionals in more than 50 countries, and NTT Group, a partner to 88 percent of the Fortune 100. Visit nttdataservices.com to learn more.

NTT DATA, Inc. (the “Company”) is an equal opportunity employer and makes employment decisions on the basis of merit and business needs. The Company will consider all qualified applicants for employment without regard to race, color, religious creed, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other class protected by law. To comply with applicable laws ensuring equal employment opportunities to qualified individuals with a disability, the Company will make reasonable accommodations for the known physical or mental limitations of an otherwise qualified individual with a disability who is an applicant or an employee unless undue hardship to the Company would result.

INDAPPS"
36,Big Data Engineer with HL7 Healthcare/Python,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,"Extensive experience in all phases of Software development life cycle (SDLC) is a must.Excellent skills in analyzing system architecture usage, defining and implementing procedures.Hands on experience in programming and implementation of Java, Scala and Python codes with strong knowledge in Object Oriented Concepts.",None Found,None Found,None Found,"Job Details
Job Code
JPSC-6455
Posted Date
10/25/17
Experience
8 Years
Primary Skills
Automation,integration,Performance,KAFKA,Sqoop,Flume,1) Big Data Solutions in Cloudera Environment (Act as a technical expert addressing problems related to system and application design,etc.) 2) Enterprise Data Lake App. Development(Hive
Required Documents
Resume
Overview
Role: Big Data Engineer with HL7 Healthcare/Python
Location: Dallas, TX
Duration: 9 Months
Top Three Skills:
1) Big Data Solutions in Cloudera Environment (Act as a technical expert addressing problems related to system and application design, performance, integration, Automation, etc.)
2) Enterprise Data Lake App. Development(Hive, Kafka, Flume, Sqoop, oozie, Spark, Shell Scripting, Python, SQL.all a must)
3) Data Warehousing (experience with relational database management systems like Oracle, Microsoft SQL Server, etc. )
4) Healthcare Domain Knowledge (HL 7, Health care exchange data, population health, claims data etc)
Job Description:
The Big Data Engineer is responsible for designing and building big data platforms, tools, and solutions that help manage, secure, and generate value from data. The person in this role creates scalable and reusable solutions for gathering, collecting, storing, processing, and serving data at scales.

As a member of the Big Data team at Ascension, this person will specialize in Big Data technologies and solutions, helping to support teams in response to deliverables, ownership and pro-activeness to drive positive customer experience.

Ascension Health (Ascension Information Systems) is in the process of constructing a large enterprise data warehouse (data lake, etc. ). Construction of structured data segments within the enterprise data warehouse is imperative to segmenting the provider healthcare data being pulled from the different Ministries (hospital networks) within the Ascension Health Network. The developers within this task will have expert understanding of design patterns and have strong analytical skills. The organization craves forward thinking, resourceful developers with experience in Big Data solutions for enterprise businesses. Proficiency in gathering requirements, analysis and validation of business requirements, specifications; along with functional specifications for schema creations and table creations.

Extensive experience in all phases of Software development life cycle (SDLC) is a must.Excellent skills in analyzing system architecture usage, defining and implementing procedures.Hands on experience in programming and implementation of Java, Scala and Python codes with strong knowledge in Object Oriented Concepts."
37,Senior Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,"
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and CDC processes
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytical skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
5+ years of experience in a Data Engineer role
Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Experience using the following software/tools:
Spark, Kafka, Kinesis, etc.
Relational SQL and NoSQL databases, including Postgres, MSSQL, Redis
Data pipeline and workflow management tools: Airflow, AWS Glue, Step functions etc.
AWS cloud services: EC2, EMR, RDS, Redshift, Athena, Lambda
Stream-processing systems: Storm, Spark-Streaming, etc.
Languages: Python, Java, Golang",None Found,"
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of structured and unstructured data sources using ‘big data’ technologies preferably using AWS services
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics
Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Keep our data separated and secure through multiple AWS regions
Create data tools for analytics and data science team members and assist them in building and optimizing our product into an innovative industry leader
Assemble large, complex data sets that meet functional / non-functional business requirements
Create and maintain optimal data pipeline architecture
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with data and analytics experts to strive for greater functionality in our data systems
",None Found,None Found,"Lucid is a market research platform that provides access to authentic, first-party data in over 90 countries. Our products and services enable anyone, in any industry, to ask questions of targeted audiences and find the answers they need – fast. These answers can be used to uncover consumer motivations, increase revenue, and measure the impact of digital advertising. Founded in 2010, Lucid is headquartered in New Orleans, LA with offices in Dallas, New York, London, Sydney, Singapore, Gurgaon, Prague, and Hamburg.


The Opportunity

We are looking for a talented Data Engineer to join our expanding team of engineers. This candidate will be responsible for expanding and optimizing our growing data needs for cross-functional teams. The ideal candidate is experienced in traditional and cloud databases and one who enjoys optimizing data systems and building them from the ground up. The Data Engineer will collaborate with cross-functional teams on data initiatives and will ensure optimal data architecture. The ideal candidate must be self-motivated and comfortable supporting the data needs of multiple product and company initiatives.
Responsibilities
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of structured and unstructured data sources using ‘big data’ technologies preferably using AWS services
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics
Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Keep our data separated and secure through multiple AWS regions
Create data tools for analytics and data science team members and assist them in building and optimizing our product into an innovative industry leader
Assemble large, complex data sets that meet functional / non-functional business requirements
Create and maintain optimal data pipeline architecture
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with data and analytics experts to strive for greater functionality in our data systems

Qualifications
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and CDC processes
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytical skills related to working with unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
5+ years of experience in a Data Engineer role
Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
Experience using the following software/tools:
Spark, Kafka, Kinesis, etc.
Relational SQL and NoSQL databases, including Postgres, MSSQL, Redis
Data pipeline and workflow management tools: Airflow, AWS Glue, Step functions etc.
AWS cloud services: EC2, EMR, RDS, Redshift, Athena, Lambda
Stream-processing systems: Storm, Spark-Streaming, etc.
Languages: Python, Java, Golang


At Lucid we foster a collaborative and inspiring workplace. We pride ourselves in doing this by recruiting, hiring and retaining diverse, passionate, and forward-thinking talent. Lucid is committed to and encourages an inclusive environment and we are dedicated to providing equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please let us know."
38,Data Engineer,"Dallas, TX 75244",Dallas,TX,75244,None Found,"
3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions.
At least 2 years of experience with Big Data technologies.
At least 2 years of coding experience in data environments.
3+ years design & implementation experience with distributed applications.
2+ years of experience in database architectures and data pipeline development.
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience in manipulating multiple, complex and large data sources.
Experienced in Data Science, statistical models as a plus.
Experience in Designing, implementing and maintaining SQL Server databases.
Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.
Experience in SQL query tuning and optimization.
Experience working in SaaS, IaaS, and PaaS.
",None Found,"Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organizationResponsible for design, development and implementation of optimal solutions to integrate, store, process and analyze huge data setsRecommend and implement strategies for bi-directional synchronization between sourcing data repositories and the central normalized data repositoryBuild data pipeline frameworks to automate high-volume and real-time data deliveryBuild data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partnersWork on multiple projects/tasks simultaneously to meet project deadlines for self and others as required.Establish and maintain positive working relationships with other employeesAll other duties as assigned.",None Found,None Found,"Job Description:
Basic Function
Komen is seeking a Data Engineer who lives and breathes data, sweats the details, deeply cares about data quality, data flows, integration, ETL, storage & performance. The Data Engineer will be creating data pipelines and process data sets that are available within Komen covering constituents, campaigns, research and patient domains.
Primary Responsibilities
Building and maintaining data processing workflows feeding our analytics, CRM and various other internal applications

Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organizationResponsible for design, development and implementation of optimal solutions to integrate, store, process and analyze huge data setsRecommend and implement strategies for bi-directional synchronization between sourcing data repositories and the central normalized data repositoryBuild data pipeline frameworks to automate high-volume and real-time data deliveryBuild data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partnersWork on multiple projects/tasks simultaneously to meet project deadlines for self and others as required.Establish and maintain positive working relationships with other employeesAll other duties as assigned.
Job Qualifications:
Minimum Experience Required
The ideal candidate will have a Bachelor's Degree in Computer Science or Math and 7-10 years of directly related experience that includes:

3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions.
At least 2 years of experience with Big Data technologies.
At least 2 years of coding experience in data environments.
3+ years design & implementation experience with distributed applications.
2+ years of experience in database architectures and data pipeline development.
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience in manipulating multiple, complex and large data sources.
Experienced in Data Science, statistical models as a plus.
Experience in Designing, implementing and maintaining SQL Server databases.
Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.
Experience in SQL query tuning and optimization.
Experience working in SaaS, IaaS, and PaaS.
In addition to the minimum qualifications above, the successful candidate should have:

Strong working and conceptual knowledge of building and maintaining physical and logical data models.
Strong project management, business writing, communication and presentation skills.
Ability to work cross-functionally within the organization.
Familiarity with or experience working in Agile Scrum software development teams.
Ability to multi-task and maintain flexibility and creativity in a variety of situations.
Ability to analyze and resolve problems.
Ability to set and meet goals and consistently meet deadlines.
Preferred Experience

Specialized Knowledge Requirements

Department:
Information Technology/Gift Management System"
39,Data Engineer - Senior Level,"Plano, TX",Plano,TX,None Found,None Found,None Found,None Found,None Found,None Found,"
Utilizing technical expertise; Creates system requirements, performs design and analysis, and coding and unit testing of complex to highly complex system functionality and/or defect correction across multiple platforms.
Identifies ideas to improve system performance and impact availability.
Resolves complex technical design issues.
Provides functional and/or technical guidance in evaluating applications systems or evaluating requests for proposals.
Coordinates changes and influences and prioritizes tasks with business or technical departments.
Analyzes and influences technical, system, and/or user requirements.
Identifies and creates solutions to improve system performance and availability.
Facilitates root cause analysis of system issues to minimize impact and future occurrences.
Creates system documentation/play book(s) and serves as a lead technical reviewer and contributor in requirements, design and code reviews.
Typically serves as a resource to the business.
Develops accurate estimates on work packages and ensures the accuracy of estimates developed by less experienced internal and third-party team members.
Analyzes and designs specifications for less experienced internal and third-party team members to execute.
Acts as a technical resource throughout the development life cycle.
May also actively contribute to the technical and soft skills development of team members.
Assists team leads and management with delegation of technical work packages to cross functional and third-party team members for execution through the full development life cycle.
Keeps management appropriately informed of progress and issues.
Coordinates system application transition from development teams to maintenance and production teams, and/or constructs and implements necessary controls to assure system/application traceability.","Purpose of Job
We are currently seeking a talented Data Engineer - Senior Level for our Plano facility. Data Engineers are engaged in all phases of the software development lifecycle which include; gathering and analyzing user/business system requirements, responding to outages and creating application system models. Data Engineers primary functions are to design, develop, document, test and debug new and existing software systems and/or applications for internal use, perform defect corrections (analysis, design, code). In addition, Data Engineers participate in design meetings and consult with business clients to refine, test, and debug programs to meet business needs, and interact and sometimes direct third-party partners in the achievement of business and technology initiatives. This is an advanced-level role occupied by those demonstrating in-depth technical and/or business knowledge within their respective areas of specialization. Incumbents work independently on complex work assignments and may determine methods and procedural approaches on new assignments. In addition, responsibilities include serving as resources to internal and third-party team members on escalated technical issues and analyzing and designing specifications for less experienced team members to execute.
Job Requirements
ABOUT USAA
USAA knows what it means to serve. We facilitate the financial security of millions of U.S. military members and their families. This singular mission requires a dedication to innovative thinking at every level.
In each of the past five years, we've been a top-40 Fortune 100 Best Companies to Work For®, and we've ranked among Victory Media's Top 10 Military Friendly® Employers primar13 years straight. We embrace a robust veteran workforce and encourage veterans and veteran spouses to apply.
ABOUT USAA IT
Our most important qualification isn't technical, it's human. Here, we don't just sit in front of a screen. We stand behind our 11 million members who rely on us every day.

We are over 3,000 employees strong, a passionately supportive and collaborative team built on Agile principles. We've been a top-two Computerworld 100 Best Places to Work in IT five years in a row and were recently named a Top 50 Employer for Minority Engineers & IT by Workforce Diversity Magazine.

See what it's like to work for a company where your passion meets our purpose:

USAA Information Technology: A Realistic Preview

Utilizing technical expertise; Creates system requirements, performs design and analysis, and coding and unit testing of complex to highly complex system functionality and/or defect correction across multiple platforms.
Identifies ideas to improve system performance and impact availability.
Resolves complex technical design issues.
Provides functional and/or technical guidance in evaluating applications systems or evaluating requests for proposals.
Coordinates changes and influences and prioritizes tasks with business or technical departments.
Analyzes and influences technical, system, and/or user requirements.
Identifies and creates solutions to improve system performance and availability.
Facilitates root cause analysis of system issues to minimize impact and future occurrences.
Creates system documentation/play book(s) and serves as a lead technical reviewer and contributor in requirements, design and code reviews.
Typically serves as a resource to the business.
Develops accurate estimates on work packages and ensures the accuracy of estimates developed by less experienced internal and third-party team members.
Analyzes and designs specifications for less experienced internal and third-party team members to execute.
Acts as a technical resource throughout the development life cycle.
May also actively contribute to the technical and soft skills development of team members.
Assists team leads and management with delegation of technical work packages to cross functional and third-party team members for execution through the full development life cycle.
Keeps management appropriately informed of progress and issues.
Coordinates system application transition from development teams to maintenance and production teams, and/or constructs and implements necessary controls to assure system/application traceability.
MINIMUM REQUIREMENTS
Bachelor's degree or 4 additional years of related experience beyond the minimum required may be substituted in lieu of a degree
6 or more years of software development experience demonstrating depth of technical understanding within Business Intelligence, ETL, and/or Data Engineering
PREFERRED REQUIREMENTS
2 or more years of ETL development in tools like DataStage or Informatica
2 or more years of Java or Python programming experience.
1 or more years of experience building streaming data pipelines (Kafka, NiFi, Spark, and Hadoop)
Advanced in SQL
Advanced with Linux or Unix systems / scripting
Experience predictive analytics implementations
Experience with NoSQL data stores such as Couchbase, Cassandra, HBase
Experience with agile development methodologies
DESIRED CHARACTERISTICS
USAA Data Engineers create innovative solutions that impact our members. Collectively, we are:
Curious and excited by new ideas
Energized by a fast-paced environment
Able to understand and translate business needs into leading-edge technology
Comfortable working as part of a connected team, but self-motivated
Community-focused, dependable and committee
Exceptionally detail-oriented
The above description reflects the details considered necessary to describe the principal functions of the job and should not be construed as a detailed description of all the work requirements that may be performed in the job.
At USAA our employees enjoy one of the best benefits package in the business, including a flexible business casual or casual dress environment, comprehensive medical, dental and vision plans, along with wellness and wealth building programs. Additionally, our career path planning and continuing education will assist you with your professional goals.
Relocation assistance is available for this position."
40,AI Model Development Lead For Marketing (Analytics Manager 5),"Irving, TX",Irving,TX,None Found,None Found,"
8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques
4+ years of experience working with digital data science including analyzing cookie-level data",None Found,None Found,None Found,None Found,"Overview:
Wells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.

SUCCESS PROFILE
Check out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.

Analytical
Detail-oriented
Insightful
Inventive
Problem Solver
Curious
Benefits
Wells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:

Medical, Dental and Vision
Employer Matching 401(k)
Tuition Reimbursment
Maternity and Paternity Leave
Paid Time Off
Responsibilties
Job Description
At Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.
Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.
Enterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.
As part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, marketing, and virtual channels is looking for an experienced AI leader to manage the development of AI models for marketing.

This leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on marketing’s AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and marketing executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.
KEY RESPONSIBILITIES INCLUDE:
Build and grow a team of data scientists responsible for AI model development in support of marketing
Design, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions
Partner with marketing executives to frame the problem, manage the model development process, and business relationship
Manage a portfolio of the data science projects including the following responsibilities:
Help finalize project scope working with business partners
On-going touch-base with business partners and governance stakeholders
Define priorities in partnership with the business partners during on-going development
Work with AI technology and production teams to operationalize models
May be called upon to review vendor models and solutions and/or models developed outside of EADS
Divisional Information:
Data Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.
As a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:
Lead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.
Accomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.

Required Qualifications

8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques
4+ years of experience working with digital data science including analyzing cookie-level data


Desired Qualifications

Strong analytical skills with high attention to detail and accuracy
Ability to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members
Meeting facilitation experience in leading discussions that result in consensus and commitment


Other Desired Qualifications
Data science experience in the ad tech space (DMPs, DSPs, Google and/or Adobe Cloud, digital attribution)
4+ years managing or directing data scientist/ statistician/ data engineer teams
Hands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet
Detail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable
Experience with agile project management methodologies for data science

Disclaimer

All offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.

Relevant military experience is considered for veterans and transitioning service men and women.

Wells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.

ENT FINANCE"
41,Senior Data Engineer,"Dallas, TX 75219",Dallas,TX,75219,None Found,None Found,None Found,None Found,None Found,None Found,"Title : Senior Data Engineer Location : Dallas, Texas (In Uptown Area) Your Role :

Join the rewardStyle team! We are looking for you, a talented and passionate developer looking to tackle challenging features and interesting problems at large scale. You’ll work with a team of highly motivated and fast moving professionals focused on continuing to grow ourselves, our innovative apps and our products.
What you’ll do…
Be an integral part of rewardstyle’s data engineering team.
Build data delivery services that support critical operational and analytical applications for our business operations, customers and partners.
Collaborate with Product, Business Intelligence and Client Success teams to understand the requirements and implement them into scalable solutions.
Work and succeed in agile development environment.
Evaluate new technologies for reliability, performance, scale and cost.
Be a technical expert and helping raise up the teams you are on.
Continuously integrate and deploy your own code in production setting.
What we’re looking for…
5+ years of coding in production big data ecosystem.
Passion towards building data pipelines, data lakes and ETL solutions at large scale.
Working knowledge of Big Data tools (Spark, Hive).
Working knowledge of NoSQL and SQL Databases (Cassandra, Postgres).
Programming expertise in Scala, Python or Java.
Self driven to propose and build solutions to reduce infrastructure cost and improve scalability.
Experience in AWS Big Data and serverless solutions is a plus (AWS Glue, EMR, Redshift, Athena).
Retail domain knowledge is a plus.
What's in it for you?

Fully stocked bars and kitchen (free snacks all day!)
Catered meals weekly
Interesting problems to solve
Chair massages
Competitive comp and benefits including 401K
Laid back office environment
Flexible work schedule
Disrupting the retail industry!
Want to see what it is like inside rewardStyle HQ?
https://youtu.be/U39GQ0f7UMI

Who we are...

Since 2011, rewardStyle has fueled the arrival of a new influence on the retail industry: professional content creators. By providing the innovative technologies, strategic consultancy and partnerships necessary to empower a global army of 40,000 premium content creators, rewardStyle is making a tangible impact on global e-commerce sales. In 2017, rewardStyle Influencers are driving over $1 billion in retail sales.

 LIKEtoKNOW.it, rewardStyle's consumer-facing, ready-to-shop content platform which makes beautiful and original influencer content actionable for millions of consumers worldwide, was launched in 2014. In March 2017, the game-changing LIKEtoKNOW.it app was launched, featuring a proprietary technology that allows consumers to instantly shop their screenshots of influencer-created imagery anywhere they discover it across social media and the mobile web.

Honored as one of the 50 Most Innovative Companies in the World by Fast Company, rewardStyle has redirected the style publishing industry and contributed to the professionalization and financial independence of thousands of influencers worldwide, enabling them to earn meaningful revenue on their digital content, ultimately empowering them to create and grow small businesses into international brands.

Today, more than 200 team members work from rewardStyle offices in Dallas, London, New York, Shanghai and São Paulo.

____

We are not offering sponsorships opportunities at this time for persons requiring employment visas, such as an H-1B; authorization to work in the U.S. is a precondition of employment.


Any unsolicited resumes/candidate profiles submitted through our website or to personal email accounts of employees of rewardStyle are considered property of rewardStyle and are not subject to payment of agency fees"
42,Big Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,None Found,None Found,"
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale
Experience working in Cloud-based Big Data Infrastructure eg: Azure
Good working experience on Cloud, Delta Lake, ETL processing.
Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc.
Working knowledge on Python and PySpark Programming.
Working with a wide range of data sources like (DB2, SAP HANA etc) and intermediate expertise in SQL and PL/SQL(optional)
Ability to work with a global team, playing a key role in communicating problem context to the remote teams, stake holders and product owners.
Work in a highly agile environment
Excellent communication and teamwork skills.
Knowledge on Data Governance & Security Principles
","Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.
This role will be responsible for Architecture, Designing and implementing Advanced Analytics capabilities . These capabilities include Batch and Streaming Analytics, Machine learning models, Natural Language processing and Natural language generation and other emerging technologies in the field of Advanced Analytics.

Requirements
Experience in developing, deploying and operating on large scale distributed systems on a commercial scale
Experience working in Cloud-based Big Data Infrastructure eg: Azure
Good working experience on Cloud, Delta Lake, ETL processing.
Experience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc.
Working knowledge on Python and PySpark Programming.
Working with a wide range of data sources like (DB2, SAP HANA etc) and intermediate expertise in SQL and PL/SQL(optional)
Ability to work with a global team, playing a key role in communicating problem context to the remote teams, stake holders and product owners.
Work in a highly agile environment
Excellent communication and teamwork skills.
Knowledge on Data Governance & Security Principles
Benefits
Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility."
43,AWS Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,"At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.","DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud",None Found,None Found," Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
§ Certified AWS Developer - Associate
§ Certified AWS DevOps – Professional (Nice to have)
§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
44,Senior Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,"
Commercial experience leading on client-facing projects, including working in close-knit teams
5+ years of experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)
5+ years of experience working on projects within the cloud ideally AWS or Azure
5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent
Experience with open source tools like Apache Airflow and Griffin
Experience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform
Data Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift
Experience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models
Experience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma
Experience building automated data quality and testing into data pipelines
Experience with AI, NLP, Machine Learning, etc. is a plus
Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R
Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time
Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.
A deep personal motivation to always produce outstanding work for your clients and colleagues
Excel in team collaboration and working with others from diverse skill-sets and backgrounds",None Found,None Found,None Found,None Found,"Summary:

You have experience with client projects and in handling vast amounts of data – working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Data Science team. Lead on projects from a data engineering perspective, working with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches
Plan and execute secure, good practice data integration strategies and approaches
Acquire, ingest, and process data from multiple sources and systems into Big Data platforms
Create and manage data environments in the Cloud
Collaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models
Have a strong understanding of Information Security principles to ensure compliant handling and management of client data
This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science
Qualifications:
Commercial experience leading on client-facing projects, including working in close-knit teams
5+ years of experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)
5+ years of experience working on projects within the cloud ideally AWS or Azure
5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent
Experience with open source tools like Apache Airflow and Griffin
Experience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform
Data Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift
Experience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models
Experience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma
Experience building automated data quality and testing into data pipelines
Experience with AI, NLP, Machine Learning, etc. is a plus
Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R
Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time
Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.
A deep personal motivation to always produce outstanding work for your clients and colleagues
Excel in team collaboration and working with others from diverse skill-sets and backgrounds
Cervello is a dynamic technology company that is focused on business analytics and planning. We take an innovative approach to making complex solutions simple so our clients can focus on running their businesses. Our services and applications enable our clients to gain the benefits of a world-class analytics and planning capability without the headaches.

7E9OZvBK9m"
45,Big Data Engineer,"Irving, TX 75038",Irving,TX,75038,None Found,None Found,None Found,None Found,None Found,None Found,"What you’ll be doing...
Verizon’s Data Analytics, Insights and Enablement team works with some of the largest data sets collected from the largest, more reliable network. We are responsible for providing fast, clean, and relevant data for Verizon’s network and field organizations as well as key measurements and insights about performance to front line employees serving our customers. You will work side by side with Verizon team members and other partners to develop next generation technologies to ensure our networks are available when our customers need them through complex, scalable data platforms, with ever growing and interesting challenges.
Designing, architecting and developing data analytics systems for various use cases including but not limited to network performance and field operations.
Participating and contributing in engineering lifecycle including writing production code, building end-to-end machine learning solutions, conduct code reviews and working closely with infrastructure teams
Building scalable systems to process petabytes of wireline and wireless data to provide real-time insights into the health of Verizon networks.
Building batch and real-time data pipelines to ingest and transform data for model training/ testing; and building and deploy model inference pipeline.
What we’re looking for...
You will need to have:
Bachelor's degree or four or more years of work experience.
Six or more years of relevant work experience.
Even better if you have:
A degree in Computer Science, Engineering or related discipline.
Six or more years of architectural design experience, preferably focused on Hadoop, Open Source and Big Data.
Solid Software Development skills with proficiency in Java and Python.
Computer science fundamentals in object-oriented design, data structures and algorithms and complexity analysis.
Experience working in distributed computing with Apache Hadoop, Apache Spark, Apache Druid and data ingestion frameworks like Apache Gobblin, Logstash, open source data connectors, and real-time data streaming services like Apache Kafka, Apache Flink and or Apache Storm/Pulsar.
Experience with Machine learning tools like TensorFlow, scikit-learn, Pandas, Kera’s, etc.
Strong experience with Big data processing tools like Hadoop/Hive/HBase/Oozie/HDFS/Yarn.
Experience implementing and monitoring big data pipelines and working in a large, complex devops and CICD environment.
Experience in production scale software development with ML/AI use cases.
Strong background in basic machine learning techniques including Anomaly Detection, Time-series, Deep Learning, supervised and unsupervised learning.
Experience in building scalable solutions using advanced analytics and machine learning techniques.
Working in an Agile-team environment, with other data engineers, data scientist, ML engineers, etc.
Experience in feature engineering for both ML/DL models, A/B testing, data management and model governance.
Experience in productionizing ML/DL models with containers using Docker and Kubernetes.
Network domain knowledge.
Willingness to travel.
When you join Verizon...
You’ll have the power to go beyond – doing the work that’s transforming how people, businesses and things connect with each other. Not only do we provide the fastest and most reliable network for our customers, but we were first to 5G - a quantum leap in connectivity. Our connected solutions are making communities stronger and enabling energy efficiency. Here, you’ll have the ability to make an impact and create positive change. Whether you think in code, words, pictures or numbers, join our team of the best and brightest. We offer great pay, amazing benefits and opportunity to learn and grow in every role. Together we’ll go far.
Equal Employment Opportunity
We're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better."
46,Senior Microsoft Azure Analysis Services Developer,"Dallas, TX 75202",Dallas,TX,75202,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description:
Applies specialized knowledge to conceptualize, design, develop, unit-test, configure, and implement portions of new or enhanced (upgrades or conversions) business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies.
Responsibilities:
Core team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives
Develop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams
Document requirements and translate into proper system requirements specifications using high-maturity methods, processes and tools.
Develop visualization, user experience and configuration elements of solution design.
Execute and coordinate requirements management and change management processes.
Participates as a member of and leads development teams.
Designs, prepares and executes unit tests.
Completes development to implement complex components.
Designs solutions for others to develop.
Participates in cross-functional teams.
Leads design activities and provides mentoring and guidance to developers.
Designs, prepares and executes unit tests.
Represents team to clients.
Demonstrates technical leadership and exerts influence outside of immediate team.
Develops innovative team solutions to complex problems.
Contributes to strategic direction for teams.
Applies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g. Azure Analysis Services, SQL Server, Power BI, and Power App development).
Integrates technical expertise and business understanding to create superior solutions for clients.
Consults with team members and other organizations, clients and vendors on complex issues.
Education and Experience Required:
Typically, a technical bachelor’s degree or equivalent experience and a minimum of 8 years of related experience or a master’s degree and a minimum of 6 years of experience.
Knowledge and Skills:
Building business intelligence solutions in a SQL Server environment
Tabular and Multidimensional SSAS design and development
Senior to Expert level DAX/MDX, SQL
Building load specifications for cubes
Power BI on-Prem and Cloud experience
Building and modifying stored procedures, functions, creating tables and views
SQL Server Analysis Services (SSAS) and SQL Server Reporting Services (SSRS)
Experience in data movement analysis, validation and cube performance tuning
Passion for technology specifically related to data, dashboards, reports and analytics
Working knowledge of star and snowflake schema(s)
Strong analytical skills and the ability to learn new technologies
Ability to meet tight deadlines
Must be a team player who loves to code and can thrive on a team that is mostly remote
Predictive analytics experience is a plus strong understanding of basic Database Administration.
Able to define quality and security standards.
Good verbal and written communication and negotiation skills.
General project management/team leader skills.
Ability to work effectively in a globally dispersed team and with clients and vendors.
Demonstrated technical leadership skills."
47,Data Engineer 1,"Dallas, TX 75201",Dallas,TX,75201,None Found,None Found,"
Strong Database Development skills across multiple platforms
Superior Interpersonal Skills: Ability to interface with a wide range of personalities and levels within Cvent and client organizations; Professional communication style
Data Collection and Analysis: Proactive listening; resourceful in collecting sufficient data; Analysis of data to develop and implement best solution
Initiative: Self-starter with strong sense of ownership; Tenacity in problem solving with positive outcomes; Motivated to increase capacity and responsibility
Detailed Oriented: Detailed administrative skills for tracking and reporting","
Design and implement database structures for OLAP and OLTP systems
Design and implement ETL and ELT process to consolidate data across multiple systems
Design and implement APIs, services, data transfers to internal and external systems
Identify and resolve performance and security issues relating to data access and maintenance
Define and enforce data design, security and performance standards
Understand and contribute to a corporate data model and overall data governance
Communicate with application, back-office and external customer teams regarding data requirements, standards, performance and access
Define and follow best practices for a full software development lifecycle involving data and database code.
Define and perform unit testing of database code
Contribute to the analysis and remediation of system behavior using tools like New Relic to understand application and process bottlenecks
Perform code reviews and audits of application team’s database code to ensure compliance with established best practices.
Contribute to new technology evaluations and recommended usage
Provide on call support for database related issues affecting system or process availability.
",None Found,"
Bachelors degree in Computer Science, CIS or related field
4+ years’ experience with multiple databases (SQL Server, Oracle, Postgres, NoSQL)
Experience with ETL, ELT, Replication (SSIS, Informatica, GoldenGate)
Experience with Data Marts, Data Warehouses, Data Lakes
Experience with one or more reporting tools (Birst, Cognos, Business Objects)
Experience with Amazon, RedShift, Cloud
","Cvent is an exciting, fast-growing tech company that provides industry-leading software to event professionals around the world. Our suite of services – online event registration, venue selection, mobile apps, email marketing, web surveys, and targeted hotel advertising opportunities – have positioned us a major player in the estimated $565 billion global meetings and events industry.

Essential Duties and Responsibilities

Design and implement database structures for OLAP and OLTP systems
Design and implement ETL and ELT process to consolidate data across multiple systems
Design and implement APIs, services, data transfers to internal and external systems
Identify and resolve performance and security issues relating to data access and maintenance
Define and enforce data design, security and performance standards
Understand and contribute to a corporate data model and overall data governance
Communicate with application, back-office and external customer teams regarding data requirements, standards, performance and access
Define and follow best practices for a full software development lifecycle involving data and database code.
Define and perform unit testing of database code
Contribute to the analysis and remediation of system behavior using tools like New Relic to understand application and process bottlenecks
Perform code reviews and audits of application team’s database code to ensure compliance with established best practices.
Contribute to new technology evaluations and recommended usage
Provide on call support for database related issues affecting system or process availability.


Job Requirements

Bachelors degree in Computer Science, CIS or related field
4+ years’ experience with multiple databases (SQL Server, Oracle, Postgres, NoSQL)
Experience with ETL, ELT, Replication (SSIS, Informatica, GoldenGate)
Experience with Data Marts, Data Warehouses, Data Lakes
Experience with one or more reporting tools (Birst, Cognos, Business Objects)
Experience with Amazon, RedShift, Cloud


Key Skills & Competencies

Strong Database Development skills across multiple platforms
Superior Interpersonal Skills: Ability to interface with a wide range of personalities and levels within Cvent and client organizations; Professional communication style
Data Collection and Analysis: Proactive listening; resourceful in collecting sufficient data; Analysis of data to develop and implement best solution
Initiative: Self-starter with strong sense of ownership; Tenacity in problem solving with positive outcomes; Motivated to increase capacity and responsibility
Detailed Oriented: Detailed administrative skills for tracking and reporting"
48,Data Engineer - Senior Consultant (Spark) - National,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.

We are...

 • The Industry-recognized data and analytics leaders
 • Passionate problem solvers across a broad spectrum of technologies and industries
 • Value seekers for measurable business outcomes
 • Continuous learners through training and education
 • Focused on a work-life balance with an unlimited paid time off policy

Data engineers are challenged with building the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally. Our engagements typically target a variety of use cases across data engineering, data science, data governance, and visualization.
Data engineers deliver value through...
Hands-on, self-directed design and development of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools
 Demonstration of technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven insights
Collaboration with team members, business stakeholders and data SMEs to elicit requirements and to develop business metrics and analytical insights
Internal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO
A data engineer’s skills include, but are not limited to...
Bachelors Degree and 5+ years of work experience
5+ years of professional IT work experience
SQL, SQL, SQL!
2+ years of Spark
Programming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)
Linux / Windows (Command line)
Big Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop)
Cloud Platforms (AWS, Azure, Google Cloud Platform)
Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)
Data Integration Tools (Talend, DataStage, Informatica, SSIS)
Databases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)
Markup Languages (JSON, XML, YAML)
Code Management Tools (Git/GitHub, SVN, TFS)
DevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)
Testing / Data Quality (TDD, unit, regression, automation)
Solving complex data and technology problems
Leading technical teams of 2+ consultants
Ability to design components of a larger implementation
Excellent communication to narrate data driven insights and technical approach

If this sounds like you, let’s talk!

Candidates must be comfortable with a national travel model to client locations weekly (M-TH is typical).

Clarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.

GLDR"
49,Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,None Found,"
Bachelor in Computer Science, Data Science, Informatics, Engineering, or a related field.
",None Found,"Pieces Tech is currently in need of a Data Engineer to be responsible for building and scaling the next generation of the Pieces data engineering solutions to support our fast-growing artificial intelligent (AI) and analytics business in healthcare.

More specifically, this role will leverage the state-of-the-art data engineering technologies to develop and evolve the Pieces AI engine that delivers machine learning, natural language processing (NLP), and other AI and analytics based solutions in a highly scalable fashion. It involves the development of the following infrastructure and systems:

AI data engineering infrastructure to efficiently support predictive modeling and analytics tasks on healthcare big data
Analytics and Clinician-in-the-loop systems for AI-assisted chart review, rapid model development, explanatory model development, and model quality assurance (QA)
This role is also responsible for exploratory work, improving and innovating, while focusing on delivering the required applications of relevance to Pieces Tech’s goals.

Additional roles and responsibilities include the following:

Develop and maintain data solution architecture (e.g. data ingestion modules, AI pipelines, ETL, data models, etc) in and around the Pieces AI engine (including NLP engine)
Develop and maintain distributed computing infrastructures within and around Pieces AI engine that are horizontally scalable (e.g. Docker and Kubernetes)
Develop and maintain relational databases (e.g. MySQL), distributed databases (e.g. Elasticsearch), and their data models that support both analytics and production pipelines
Develop and maintain an efficient and reliable deployment process across all clients for both analytical model deployment and functional module deployment
Implement monitoring and QA layers for all existing and new components within and around Pieces AI engine
Collaborate with front-end developers, back-end developers, AI architect, and data scientists to build end-to-end AI systems
Support general data analytics to answer business questions for product ideation, product development, and client engagement
 Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.


Some of the benefits that the successful candidate can expect are below:


Opportunity to work remotely
A knowledgeable, high-achieving, experienced and fun team
The chance to be part of a rapidly growing startup and next success story
A competitive base salary
Structured career development



Required Skills
Ability to self-manage, work independently and meet deadlines
Critical thinking skills
Willingness to learn
Teamwork mentality and skills
Strong communication, interpersonal and analytical skills
Team-based communication and project management tools (Google Doc, SmartSheet, JIRA, Confluence, Bitbuckets, etc)
Proficient with Linux and MacOS operating system
Proficient with programming languages including Python, Java, and SQL


Required Experience
Required Education

Bachelor in Computer Science, Data Science, Informatics, Engineering, or a related field.


Preferred Education

Master, or PhD degree in Computer Science, Data Science, Informatics, Engineering or equivalent.


Required Experiences


At least 3-year experience in full-stack software engineering
Proven experience in data modeling in relational databases (e.g. MySQL, PostgreSQL, Snowflake, etc) and/or NoSQL databases (e.g. Elasticsearch, MongoDB, Columnar databases, etc)
Proven experience with web service development using Spring (Spring Boot, Spring Data, Spring security, etc), REST, and JSON.
Proven experience in developing container-based system (e.g. Docker and Kubernetes)


Preferred Experiences

Experience in building distributed systems (e.g. MapReduce, Hadoop, or Spark)
Experience in front-end development (e.g. AngularJS, JavaScript, HTML5, etc)
Experience in developing systems on healthcare data (e.g. Epic, Meditech, Cerner, Allscripts, etc)
Experience in machine learning, deep learning, active learning, and/or transfer learning to solve practical AI problems in a primary or supporting role
Experience in developing NLP system"
50,Sr. Data Engineer (Houston),"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Darby Consulting is looking for an experienced Sr. Data Engineer with Azure experience to support our client in the oil and gas industry. The Sr. Data Engineer will have a minimum of four years working with big data processing utilizing Azure Dev Ops, Azure Data Factory and Azure Data Warehouse in an Agile development environment. This position is offered to candidates based in the Houston-area on a 1099 Independent Contractor or W2-Hourly Employee basis. Excellent pay, relocation assistance, top-tier client, excellent work environment, long-term assignment and flexible work schedule are just a few of the many perks for this opportunity.

WORK HOURS AND LOCATION
Services to be provided in Houston, TX during normal business hours (typically Monday through Friday from 8:00AM to 5:00PM, excluding holidays when the client office is closed).

ABOUT DARBY CONSULTING

If you've worked for a few consulting firms by now, you know there's a lot of ""great"" places to work. We've worked with many of them too. But, let's be honest: if everyone claims to be great, then great really is just the new average.

Our goal is to build an amazing place to work. That means designing and building a company that goes beyond average. We want amazing. For us, Amazing is the opportunity to join a growing company of highly talented and experienced professionals. It's about learning and working alongside great people who care as much about you and your success as they do about their clients. Amazing is about flexibility, work-life balance and opportunities to deliver solutions for respected companies who value your knowledge, skills and experience. If you want great, there's a lot of companies out there. But if you want amazing, welcome to Darby Consulting!

WHAT WE DO

Darby Consulting is a full-service IT consulting firm specializing in IT project management, systems design and deployment of software and hardware-related projects. Supporting clients in the energy, government and education sectors, Darby helps growing organizations to maximize the value from their IT projects by integrating experienced and specialized IT project professionals, success-based methodology and cloud-based project management tools at affordable rates for growing organizations."
51,Data Integration Engineer,"Arlington, TX 76016",Arlington,TX,76016,None Found,"
You can consume and utilize new languages, design patterns, APIs and toolsets.
You can work in a fast-paced and collaborative environment.
Effective communication skills and a willingness to learn are a must.
Experience with Test-Driven Development and writing unit and integration tests.
Experience using a Behavior-Driven Development suite like Cucumber.
Competent writing software with JavaScript ecosystems like React.
Comfortable working in a cloud environment like AWS.
Must have experience basic Linux/Unix CLI and using Git and GitHub for source code control.
Knowledge of Continuous Integration/Continuous Delivery systems like Jenkins.
Knowledge of Docker and Kubernetes is a plus.
Exhibits enthusiasm and well-rounded knowledge of backend systems and software architecture.
Applies best practices including design patterns and linting to all software development.
Approaches engineering requests from a user's vantage point to form architectural and technical requirements.
Stays well-informed of emerging technologies and software trends.
Capable of debugging problems related to HTTP, XHR, JSON, CORS, SSL, S3, etc.
Able to investigate performance and memory issues.
Able to reduce complex requirements and user interaction flows into long-term API designs.
Good understanding of architectural messaging patterns and pitfalls using Kafka, Rabbit MQ, etc.
Endeavors to establish positive relationships, both inter-departmentally, and cross-functionally.","
Proficient with interviewing and gathering business requirements, definition and design of data source and data flow, data quality analysis, and working with the data architect on the development of logical data models. Proficient using Infosphere/DataStage or equivalent ETL software.
Proficient with relational databases and using SQL to query, create tables, views, indexes, joins
Proficient using Unix and applicable scripting/scheduling tools
Experience with the SDLC, ITSM and privacy and security concepts.
",None Found,"
Bachelor's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology required upon hire
Master's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology preferred","
3 Years Working as a Data Integration Engineer or Data Integration Production Support Engineer with a Bachelor's degree is required.
Experience working in a healthcare or related field
Teradata Database Experience
Experience upgrading IBM Infosphere Tools","Currently seeking Data Integration Engineers located in or near Arlington, TX. As a data engineer, you'll be handling the design and construction of scalable systems, ensure that all data systems meet company requirements, and also research new uses for data acquisition. You should also know the ins and outs of the industry such as data mining practices, algorithms, and how data can be used.

Required Skills
Proficient with interviewing and gathering business requirements, definition and design of data source and data flow, data quality analysis, and working with the data architect on the development of logical data models. Proficient using Infosphere/DataStage or equivalent ETL software.
Proficient with relational databases and using SQL to query, create tables, views, indexes, joins
Proficient using Unix and applicable scripting/scheduling tools
Experience with the SDLC, ITSM and privacy and security concepts.

Qualifications
You can consume and utilize new languages, design patterns, APIs and toolsets.
You can work in a fast-paced and collaborative environment.
Effective communication skills and a willingness to learn are a must.
Experience with Test-Driven Development and writing unit and integration tests.
Experience using a Behavior-Driven Development suite like Cucumber.
Competent writing software with JavaScript ecosystems like React.
Comfortable working in a cloud environment like AWS.
Must have experience basic Linux/Unix CLI and using Git and GitHub for source code control.
Knowledge of Continuous Integration/Continuous Delivery systems like Jenkins.
Knowledge of Docker and Kubernetes is a plus.
Exhibits enthusiasm and well-rounded knowledge of backend systems and software architecture.
Applies best practices including design patterns and linting to all software development.
Approaches engineering requests from a user's vantage point to form architectural and technical requirements.
Stays well-informed of emerging technologies and software trends.
Capable of debugging problems related to HTTP, XHR, JSON, CORS, SSL, S3, etc.
Able to investigate performance and memory issues.
Able to reduce complex requirements and user interaction flows into long-term API designs.
Good understanding of architectural messaging patterns and pitfalls using Kafka, Rabbit MQ, etc.
Endeavors to establish positive relationships, both inter-departmentally, and cross-functionally.

Requirements
3 Years Working as a Data Integration Engineer or Data Integration Production Support Engineer with a Bachelor's degree is required.
Experience working in a healthcare or related field
Teradata Database Experience
Experience upgrading IBM Infosphere Tools

Education
Bachelor's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology required upon hire
Master's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology preferred
3xW99DwBQZ"
52,Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Datalitical Inc is one largest consulting firms in U.S and with lots of engagements. We are a diverse community of people, all working together to bring high-end solution to any business problem and provide the best service to our clients. We’re looking for talented individuals who want to work in an energetic, respectful, collaborative environment. With a wide array of jobs, internships, training and more, there are countless opportunities for you to grow your career with us.

Responsibilities:
Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth at Datalitical Consistently evolve data model & data schema based on business and engineering needs Implement systems tracking data quality and consistency Develop tools supporting self-service data pipeline management (ETL) SQL and MapReduce job tuning to improve data processing performance
Experience & Skills:
Extensive experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet) Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle) Good understanding of SQL Engine and able to conduct advanced performance tuning Strong skills in scripting language (Python, Ruby, Perl, Bash) Experience with workflow management tools (Airflow, Oozie, Azkaban, UC4) Comfortable working directly with data analytics to bridge business requirements with data engineering
Bonus:
MPP database experience (Redshift, Vertica, Teradata) Experience with building tools to support self-service pipeline Experience with one of the messaging system (Kafka, SQS, Kinesis) and different data serialization (json, protobuf, avro)

Our corporate office supports and offers a competitive benefits package including medical/dental/vision, term life insurance, paid vacation/holidays, 401(k) savings plan with company match.

Apply: hr@datalitical.com"
53,"Senior Data Engineer, Products","Farmers Branch, TX 75244",Farmers Branch,TX,75244,None Found,None Found,None Found,"Lead a technical team to rapidly architect, design, prototype, and implement and optimize architectures to tackle the Big Data and Data Science problems.Design, maintain and oversee the operational process to develop modular code base to solve “real” world problems.Conduct regular peer code reviews to ensure code quality and compliance following best practices in the industry.Work in cross-disciplinary teams to understand client needs and ingest rich data sources.Research, experiment, and utilize leading Big Data methodologies (Hadoop, Spark, Kafka, Netezza, Snowflake, and AWS) with cloud/on premise/hybrid hosting solutions.Oversee a technical team to provide proficient documentation and operating guidance for users of all levels.Lead a technical team to architect, implement, and test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings (AWS, Azure, client technology stacks, and on-prem clusters)Translate advanced business analytics problems into technical approaches that yield actionable recommendations across multiple, diverse domains; Communicate results and educate others through design and build of insightful visualizations, reports, and presentations.Develop skills in business requirement capture and translation, hypothesis-driven consulting, work stream and project management and client relationship developmentHelp drive the process for pursuing innovations, target solutions, and extendable platforms for Merkle’s products.Participate in developing and presenting thought leadership, and assist in ensuring that Merkle’s “data source” technology stack incorporates and is optimized for using specific technologies.Promote the Merkle brand in the broader “data source” community.",None Found,None Found,"Job Description

We are seeking a highly motivated, and technically proficient Data Engineer to work on architecting, designing, building and managing data products on ""big data"" infrastructure.
You will expand upon your current skill set through cross-disciplinary collaboration with some of the smartest and nicest people in the industry, while learning the inner workings of a fast-paced global performance marketing agency. You will work closely with a team of engineers in an agile development environment to expand our proprietary marketing products.
Additionally, as a Data Engineer, you’ll play an integral role in the growth of our team. You will assist in reviewing complicated and mission-critical designs, code, and tests; and documenting ways to improve our current codebase and system processes using the latest technologies.
Key ResponsibilitiesLead a technical team to rapidly architect, design, prototype, and implement and optimize architectures to tackle the Big Data and Data Science problems.Design, maintain and oversee the operational process to develop modular code base to solve “real” world problems.Conduct regular peer code reviews to ensure code quality and compliance following best practices in the industry.Work in cross-disciplinary teams to understand client needs and ingest rich data sources.Research, experiment, and utilize leading Big Data methodologies (Hadoop, Spark, Kafka, Netezza, Snowflake, and AWS) with cloud/on premise/hybrid hosting solutions.Oversee a technical team to provide proficient documentation and operating guidance for users of all levels.Lead a technical team to architect, implement, and test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings (AWS, Azure, client technology stacks, and on-prem clusters)Translate advanced business analytics problems into technical approaches that yield actionable recommendations across multiple, diverse domains; Communicate results and educate others through design and build of insightful visualizations, reports, and presentations.Develop skills in business requirement capture and translation, hypothesis-driven consulting, work stream and project management and client relationship developmentHelp drive the process for pursuing innovations, target solutions, and extendable platforms for Merkle’s products.Participate in developing and presenting thought leadership, and assist in ensuring that Merkle’s “data source” technology stack incorporates and is optimized for using specific technologies.Promote the Merkle brand in the broader “data source” community.

Qualifications

Qualified individuals possess the Merkle attributes of being smart, curious, committed to vision, passionate, fun/pleasant, an achiever and having a sense of urgencyMinimum of ten years of big data experience with multiple programming languages and technologies, three years as a lead and three years at a management level with minimum five years of big data experience.Bachelor's degree or Master's degree from an accredited college/university in Computer Science, Computer Engineering, or related field(i.e. math and physics);Ability to manage complex engagements and interface with senior level management internally as well as with clients.Ability to communicate complex technical concepts succinctly to non-technical colleagues, understand & manage interdependencies between all facets of a project.Ability to lead client presentations; Must have demonstrated advanced proficiency in complex, mature and sophisticated Design & Analysis technologies and solutions.Ability to mentor others and publish whitepapers or articles on complex D&A technologies or solutions.Market-leading proficiency with multiple large scale and/or distributed processing methodologies (Hadoop, Storm, Spark).Skilled ability to rapidly ingest, transform, engineer, and visualize data, both for ad hoc and product-level (e.g., automated) data & analytics solutions.Market-leading fluency in several programming languages (Python, Scala, or Java), with the ability to pick up new languages and technologies quickly.Understanding of cloud and distributed systems principles (such as load balancing, networks, scaling, in-memory vs. disk).Experience with large-scale, big data methods (MapReduce, Hadoop, Spark, Hive, Impala, or Storm, SnowFlake) and AWS solutions (EC2, S3, RDS, EMR, Kinesis, DynamoDB, Redshift).Experience storing, managing, and processing massive data sets using tools such as Hadoop, Apache Spark, AWS EMR, Hive, SnowFlake or the like.Ability to work efficiently under Unix/Linux environment and .NET, having experience with source code management systems like GIT and SVN.Strong knowledge with programming methodologies (version control, testing, QA) and development methodologies (Waterfall and Agile).Experience with object-oriented design, coding, and testing patterns as well as experience in engineering (commercial or open source) software platforms and large-scale data infrastructures.Familiarity with different architecture patterns of development such as Event Driven, SOA, micro services, functional programming, Lambda.Capability to architect highly scalable distributed systems, using different open source tools.Knowledge of traditional and digital data-driven marketing.
Additional Information

All your information will be kept confidential according to EEO guidelines. At Merkle, we believe that a diverse environment improves us as a community and as a business. We want to foster an environment of growth, where all ideas and contributions are encouraged. We need this culture of courage to continue to thrive in our fast-paced industry. We embrace differences of opinion. We value diversity of experience and thought, which help us to challenge and define industry-leading solutions, and support our goal of being a great place to work."
54,Big Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Multiple Bigdata Job Positions across Dallas, TX and Tampa, FL
Position 1: Big Data Engineer
Location: Dallas TX
Experience Required - 6+ years
Job Description
Hadoop/HDFS.
Spark is a must. Scala preferred but Java is ok too.
Experience Spark core, Spark SQL, Spark Streaming ( these 3 are a must) and Spark ML is good to have.
Position 2: Sr. Big Data Engineer
Experience Required - 8-15 years
Location: Tampa, FL
Job Description
The client is looking for a senior engineer who can drive things rather than being managed by the client.
Hadoop/Hive and Kafka.
Spark streaming using Java is a must.
Position 3: Lead Big Data Engineer
Location: Dallas TX
Experience Required - 8-15 years
Job Description
Primary requirement - Spark, Scala developer with knowledge of Kafka.
Good to have exposure to other NoSQL databases like Casandra.
AWS experience will be a definite plus."
55,Data Engineer,"Dallas, TX 75244",Dallas,TX,75244,None Found,"3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions.
At least 2 years of experience with Big Data technologies.
At least 2 years of coding experience in data environments.
3+ years design & implementation experience with distributed applications.
2+ years of experience in database architectures and data pipeline development.
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience in manipulating multiple, complex and large data sources.
Experienced in Data Science, statistical models as a plus.
Experience in Designing, implementing and maintaining SQL Server databases.
Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.
Experience in SQL query tuning and optimization.
Experience working in SaaS, IaaS, and PaaS.

",None Found,"3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions.
At least 2 years of experience with Big Data technologies.
At least 2 years of coding experience in data environments.
3+ years design & implementation experience with distributed applications.
2+ years of experience in database architectures and data pipeline development.
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience in manipulating multiple, complex and large data sources.
Experienced in Data Science, statistical models as a plus.
Experience in Designing, implementing and maintaining SQL Server databases.
Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.
Experience in SQL query tuning and optimization.
Experience working in SaaS, IaaS, and PaaS.

",None Found,None Found,"Basic Function:

Komen is seeking a Data Engineer who lives and breathes data, sweats the details, deeply cares about data quality, data flows, integration, ETL, storage & performance. The Data Engineer will be creating data pipelines and process data sets that are available within Komen covering constituents, campaigns, research and patient domains.

Primary Responsibilities:

Building and maintaining data processing workflows feeding our analytics, CRM and various other internal applications

â€¢ Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organization
â€¢ Responsible for design, development and implementation of optimal solutions to integrate, store, process and analyze huge data sets
â€¢ Recommend and implement strategies for bi-directional synchronization between sourcing data repositories and the central normalized data repository
â€¢ Build data pipeline frameworks to automate high-volume and real-time data delivery
â€¢ Build data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners
â€¢ Work on multiple projects/tasks simultaneously to meet project deadlines for self and others as required.
â€¢ Establish and maintain positive working relationships with other employees
â€¢ All other duties as assigned.

Position Qualifications:

The ideal candidate will have a Bachelor's Degree in Computer Science or Math and 7-10 years of directly related experience that includes:
3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions.
At least 2 years of experience with Big Data technologies.
At least 2 years of coding experience in data environments.
3+ years design & implementation experience with distributed applications.
2+ years of experience in database architectures and data pipeline development.
Strong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.
Experience in manipulating multiple, complex and large data sources.
Experienced in Data Science, statistical models as a plus.
Experience in Designing, implementing and maintaining SQL Server databases.
Experience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.
Experience in SQL query tuning and optimization.
Experience working in SaaS, IaaS, and PaaS.

In addition to the minimum qualifications above, the successful candidate should have:
Strong working and conceptual knowledge of building and maintaining physical and logical data models.
Strong project management, business writing, communication and presentation skills.
Ability to work cross-functionally within the organization.
Familiarity with or experience working in Agile Scrum software development teams.
Ability to multi-task and maintain flexibility and creativity in a variety of situations.
Ability to analyze and resolve problems.
Ability to set and meet goals and consistently meet deadlines."
56,Lead Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"As a Lead Data Engineer, here's what we'll be looking for you to bring:

Hands-on Engineering Leadership
Proven track record of Innovation and expertise in Data Engineering
Tenure in coding, architecting and delivering complex projects
Deep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others
Deep understanding of streaming data architectures and technologies for real-time and low-latency data processing
Deep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies
Understanding of how to architect solutions for data science and analytics such as productionizing machine learning models and collaborating with data scientists
Understanding of agile development methods including: core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Passion for software craftsmanship
A rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..
Strong stakeholder management and interaction experience at different levels


There's no typical day or engagement for our Senior Data Engineers. Here’s what you’ll do:

Be the SME. Develop modern data architectural approaches to meet key business objectives and provide end to end data solutions
You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems.
On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.
It could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.
Whatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.
You have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.
You recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.
A few important things to know:
Projects are almost exclusively on customer site, so candidates should be flexible and open to extensive travel.

Candidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.

Not quite ready to apply? Or maybe this isn’t the right role for you? That’s OK, you can stay in touch with AccessThoughtWorks, our learning community (click ""contact me about recruitment opportunities"" to hear about jobs in the future).

It is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment."
57,Azure Data Engineer,"Dallas, TX",Dallas,TX,None Found,None Found,"At least 5 years of consulting or client service delivery experience on Azure
",DevOps on an Azure platform,None Found,None Found," Proven ability to build, manage and foster a team-oriented environment
","Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
58,Sr. Data Engineer,"Dallas-Fort Worth, TX",Fort Worth,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"SUMMARY:
The Senior Data Engineer is responsible for designing, developing, testing, tuning, and deploying Extract, Transformation, and Load (ETL) processes and API's for a moderate number of projects related to below:

Datasets within the Oncology reporting environment, and/or Data sources within the core data model of the Enterprise Data Warehouse (EDW), and Subject area data marts within a defined CMS defined Clinical Program.

RESPONSIBILITIES:
Acquire and interpret business requirements, create technical artifacts and determine most efficient design solution.
Experience working in large data warehouse. Working knowledge of Master Data Management, Metadata management, Data Architecture, Data Governance and Data Quality.
Combined experience of advanced SQL and PL/SQL (in Oracle, SQL Server Database Platforms) with hands-on experience of designing solutions.
Prepares clear and complete complex specifications, designs, test plans, and operations documentation from which ETL processes are developed.
Participates in teams to define design and development standards and processes
Monitors production processes using enterprise scheduler tools and troubleshoots incidents surrounding supported solutions, including after-hours escalations of major incidents.
Configuration of Staging, Base Objects Tables, Look-ups, Cleanse Lists, Cleanse functions, Mappings, Audit trail/ Delta detection and Develop ETL Mappings to move the data loaded from stage to downstream applications.
Participates in work group sessions with users and other project stakeholders and actively contributes to the definition of requirements, data analysis to define reporting objectives and/or metric benchmarks, and review of in progress solution development.
Understands and applies HIPAA standards and regulations to all areas of work.
Develops expertise with current ETL technologies, trends, and best practices.
Adheres to standards, methodologies, and processes. Identifies possible improvements and champions revisions with appropriate standards owner.
Experience in designing, implementing and operationalizing data solutions in cloud production environments Azure using one or more relevant technologies such as Azure Data Lake is big plus
Experience working in SAFE Agile or Scrum based environment and Lead development efforts using Agile/Scrum based methodologies
QUALIFICATIONS:
Bachelor's degree.
Demonstrated experience in creating medium to large-scale logical data models
Experience building data marts and business intelligence solutions
Worked on one or more of the following databases: Oracle, SQL Server, MySQL
Strong experience using at least one of the top data modeling tools, such as ERwin, ER/Studio or Oracle data modeler
Familiarity with open source data pipeline and transformation technologies
Strong leadership and ability to work and influence cross-functional teams
Experience authoring technical documents and working with teams to drive reference implementation to production scale
For Internal Use: Career Level P3
McKesson is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, national origin, disability, or protected Veteran status.Qualified applicants will not be disqualified from consideration for employment based upon criminal history.McKesson is committed to being an Equal Employment Opportunity Employer and offers opportunities to all job seekers including job seekers with disabilities. If you need a reasonable accommodation to assist with your job search or application for employment, please contact us by sending an email to Disability_Accommodation@McKesson.com. Resumes or CVs submitted to this email box will not be accepted.Current employees must apply through internal career site.Join us at McKesson!"
59,Senior Big Data Data Engineer,"Irving, TX 75038",Irving,TX,75038,None Found,None Found,None Found,None Found,None Found,None Found,"What you’ll be doing...
Looking for a Senior Data Engineer to work with a small team responsible for building, deploying, and supporting a Big Data solution that will enable operations for a large enterprise environment. Must be able to design, build and maintain Enterprise Level Data Pipe-Lines utilizing the tools available within Big Data Eco-System. As a Senior Big Data Engineer - You will work on Advanced Analytics using Big Data technologies such as Hadoop and Data Warehousing.
Build analytical solutions to enable Data Scientist by manipulating large data sets and integrating diverse data sources.
Perform ad-hoc analysis and develop reproducible analytical approaches to meet business requirements.
Perform exploratory and targeted data analyses using descriptive statistics and other methods.
Use complex algorithms to develop systems & applications that deliver business functions or architectural components.
Typical duties will include the following:
Work closely with the data scientists, and database and systems administrators to create data solutions.
Follow best practices on design and implementation to aid in company-wide data governance.
Improve existing data pipelines by simplifying and increasing performance.
Design, build, and deploy new data pipelines within Big Data Eco-Systems.
Documents new/existing pipelines, Data Sets and Data Sets.
Abides by department development standards and SOP's.
Attends all department meetings.
All other duties as assigned.
Keeps updated on latest technologies relevant to position’s duties.
Has great knowledge of commonly used software concepts and design.
Great knowledge of the development lifecycle.
Keeps management updated on projects and assigned work.
What we’re looking for...
You will need to have:
Bachelor’s degree or four or more years of work experience.
Six or more years of relevant work experience.
Even Better If You Have:
A Degree.
Experience of designing, building, and deploying production-level data pipelines using tools from Hadoop stack (HDFS, Hive, Spark, HBase, Kafka, NiFi, Oozie, Splunk etc).
Experience with various noSQL databases (Hive, MongoDB, Couchbase, Cassandra, and Neo4j).
Experience with analytic or feature engineer programming (python or scala or java).
Experience implementing open source frameworks & exposure to various open source & package software architectures (AngularJS, ReactJS, Node, Elasticsearch, Spark, Scala, Splunk, Apigee, and Jenkins etc.).
Experience troubleshooting JVM-related issues.
Experience with SQL databases and Change Data Capture.
Experience and strategies to deal with mutable data in Hadoop.
Experience with Stream sets.
Experience of Agile and DevOps methodologies.
Experiencewithjudgment to plan and accomplish goals.
Works under general supervision.
Experience in full development life cycle and significant experience in delivering applications and architecture services.
Experience in data visualization tools like Kibana, Grafana, Tableau and associated architectures.
Experience evaluating and implementing cutting-edge digital technologies.
Experience with Cloud technologies (AWS, GCP, PCF, Docker, Kubernetes and application migration.
When you join Verizon...
You’ll have the power to go beyond – doing the work that’s transforming how people, businesses and things connect with each other. Not only do we provide the fastest and most reliable network for our customers, but we were first to 5G - a quantum leap in connectivity. Our connected solutions are making communities stronger and enabling energy efficiency. Here, you’ll have the ability to make an impact and create positive change. Whether you think in code, words, pictures or numbers, join our team of the best and brightest. We offer great pay, amazing benefits and opportunity to learn and grow in every role. Together we’ll go far.
Equal Employment Opportunity
We're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better."
60,Product Designer/Data Architect,"Plano, TX 75023",Plano,TX,75023,None Found,None Found,None Found,None Found,None Found,None Found,"Who we are

Collaborative.
Respectful. A place to dream and do. These are just a few words that describe
what life is like at Toyota. As one of the world’s most admired brands, Toyota
is growing and leading the future of mobility through innovative, high-quality
solutions designed to enhance lives and delight those we serve. We’re looking
for diverse, talented team members who want to Dream. Do. Grow. with us.
An important part of the Toyota family is Toyota Financial
Services (TFS), the finance and insurance brand for Toyota and Lexus in North
America. While TFS is a separate business entity, it is an essential part of
this world-changing company – delivering on Toyota’s vision to move people
beyond what’s possible. At TFS, you will help create best-in-class customer
experiences in an innovative, collaborative environment.

Who we’re looking for

The
TFS IDS Department is looking for a passionate and highly-motivated Product Designer. The role
is
primarily responsible for the TFS enterprise data warehouse architecture and
development lifecycle process that translates business requirements into data
solutions aligned to a Factory or and DIO’s Application Portfolio or specific
portion of the application portfolio.




What you’ll be doing
Create and manage conceptual, logical and physical data models using iDERA
Embarcadero ER/Studio suite
 Implement physical data models in Snowflake, SQL Server, Postgres, AWS
Redshift, and NoSQL platforms (MongoDB)
Participate as an active member of the team to shape Data Architecture vision,
standards and guidelines
 Design and Implement data devops processes for code deployment and
synchronization across environments
Conduct code reviews and provide constructive feedback on database changes
from product developers and engineers
Provide technical leadership and mentoring on database technologies and
issues to product designers and engineers
What you bring

Multiple years of experience working with Financial Services organization with emphasis on Captive Auto Finance where the potential candidate has working knowledge of
Need to have substantial experience BigData/ cloud ecosystem (e.g., Hadoop, Data Lake)
Skills with iDERA - Embarcadero ER/Studio suite, Erwin, or other data modeling tool for OLTP systems
Expert in data modeling for relational and document-based data stores, including conceptual, logical, and physical data models (ability to build from scratch)
Experience with various data integration patterns across a variety of sources
Experience working with terabyte data sets, billion+ row tables, and high transaction volumes in an OLTP
Experience with distributed systems and cloud data platforms
As a Big Data Engineer You have experience designing, developing and maintaining systems at scaleAs a Big Data Engineer you will be responsible for the continued development of the data pipeline

What we’ll bring


During your interview process, our team can fill you in on all the details of our industry-leading benefits and career development opportunities. A few highlights include:


A work environment built on teamwork, flexibility and respect
Professional growth and development programs to help advance your career, as well as tuition reimbursement
Vehicle purchase & lease programs
Comprehensive health care and wellness plans for your entire family
Flexible work options based on business needs
Toyota 401(k) Savings Plan featuring a company match, as well as an annual retirement contribution from Toyota regardless of whether you contribute
Paid holidays and paid time off
Referral services related to prenatal services, adoption, child care, schools and more
Flexible spending accounts
Relocation assistance
Onsite amenities such as fitness center, restaurants, etc.
What you should know

Our success begins and ends with our people. We embrace diverse perspectives and value unique human experiences. We are proud to be an equal opportunity employer that celebrates the diversity of the communities where we live and do business. Applicants for our positions are considered without regard to race, ethnicity, national origin, sex, sexual orientation, gender identity or expression, age, disability, religion, military or veteran status, or any other characteristics protected by law.

Have a question or need assistance
with your application? Check out the How to Apply section of our
careers page on Toyota.com"
61,Cloud Engineer,"Irving, TX",Irving,TX,None Found,None Found,None Found,"
Bachelor’s Degree in Computer Science or related area, or equivalent work experience.
Building out a scalable multi-tenant AWS private and public cloud environment that meets the architectural requirements of the cloud platform
Familiar with software development to drive data & analytic efforts using Java and either Scala or Python as the programming languages.
Developer experience building applications/solutions using public/private cloud infrastructure
AWS Knowledge and experience - Lambda, API Gateway, EC2, S3 etc.
Database knowledge – Oracle or Any No SQL DB like Couchbase, MongoDB, DocumentDB
Experience with CICD (continuous integration & delivery) pipelines
Automation, Configuration Management (e.g. Ansible, Puppet), Dev-ops practices
Familiarity with Linux Containers / Docker / Kubernetes.
","Design and implement a centralized unstructured data management ecosystem to collect, analyze, store, protect and govern unstructured data for the enterprise.
Work closely with the team to design, plan, build and maintain a scalable Cloud platform for the evolving needs to support Allstate’s Unstructured Data Strategy.
Provide recommendations on implementation of organization wide processes and procedures for the management of data risk especially if the solution is based on a public cloud services.
Coordinate with Network and Infrastructure teams to design systems that can scale to the operational needs of handling large volumes of Unstructured data
Partners with other internal teams and peers in the department to ensure holistic solutions meet the needs of various stakeholders.
Leverages and uses Big Data best practices to develop technical solutions used for handling of Unstructured data.
Supports Innovation; regularly provides new ideas to help people, process, and technology that interact with data ecosystem.
Develop and builds frameworks/prototypes that integrate Unstructured data and advanced analytics to make better business decisions.
Supports a clear communication strategy that keeps all relevant stakeholders informed and provides an opportunity to influence the direction of the work
Trains and develops other engineers.",None Found,None Found,"Where good people build rewarding careers.
Think that working in the insurance field can’t be exciting, rewarding and challenging? Think again. You’ll help us reinvent protection and retirement to improve customers’ lives. We’ll help you make an impact with our training and mentoring offerings. Here, you’ll have the opportunity to expand and apply your skills in ways you never thought possible. And you’ll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.
Job Summary
This role is responsible for a DevOps approach to development of new systems for analyzing/processing/enriching Unstructured content; the coding & development of advanced analytics solutions to make/optimize business decisions and processes; integrating new tools to improve analytics; and address new technical challenges using existing and emerging technology solutions.
Responsibilities
Design and implement a centralized unstructured data management ecosystem to collect, analyze, store, protect and govern unstructured data for the enterprise.
Work closely with the team to design, plan, build and maintain a scalable Cloud platform for the evolving needs to support Allstate’s Unstructured Data Strategy.
Provide recommendations on implementation of organization wide processes and procedures for the management of data risk especially if the solution is based on a public cloud services.
Coordinate with Network and Infrastructure teams to design systems that can scale to the operational needs of handling large volumes of Unstructured data
Partners with other internal teams and peers in the department to ensure holistic solutions meet the needs of various stakeholders.
Leverages and uses Big Data best practices to develop technical solutions used for handling of Unstructured data.
Supports Innovation; regularly provides new ideas to help people, process, and technology that interact with data ecosystem.
Develop and builds frameworks/prototypes that integrate Unstructured data and advanced analytics to make better business decisions.
Supports a clear communication strategy that keeps all relevant stakeholders informed and provides an opportunity to influence the direction of the work
Trains and develops other engineers.
Knowledge/Skills/Abilities/Experience
Bachelor’s Degree in Computer Science or related area, or equivalent work experience.
Building out a scalable multi-tenant AWS private and public cloud environment that meets the architectural requirements of the cloud platform
Familiar with software development to drive data & analytic efforts using Java and either Scala or Python as the programming languages.
Developer experience building applications/solutions using public/private cloud infrastructure
AWS Knowledge and experience - Lambda, API Gateway, EC2, S3 etc.
Database knowledge – Oracle or Any No SQL DB like Couchbase, MongoDB, DocumentDB
Experience with CICD (continuous integration & delivery) pipelines
Automation, Configuration Management (e.g. Ansible, Puppet), Dev-ops practices
Familiarity with Linux Containers / Docker / Kubernetes.
Desired Experience
2+ Experience as a Big Data Engineer or System Engineer
Good understanding of data flow, data governance principles
Familiar with Development frameworks: Hibernate, Micro Service, Spring boot, Spring Cloud, Spring Integration
Familiar with Messaging/Streaming: JMS, Kafka/Kinesis
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.


Good Work. Good Life. Good Hands®.

As a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.

Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.

Allstate generally does not sponsor individuals for employment-based visas for this position.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.

For jobs in San Francisco, please click ""here"" for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click ""here"" for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.

It is the policy of Allstate to employ the best qualified individuals available for all jobs without regard to race, color, religion, sex, age, national origin, sexual orientation, gender identity/gender expression, disability, and citizenship status as a veteran with a disability or veteran of the Vietnam Era."
62,Google Technical Architect,"Dallas, TX",Dallas,TX,None Found,None Found,"Minimum 5 years of Consulting or client service delivery experience on Google GCP
",DevOps on an GCP platform. Multi-cloud experience a plus.,None Found,None Found,"Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google Cloud Platform (GCP) Technical Architect Delivery is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would also be responsible for developing and delivering Google GCP cloud solutions to meet todays high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Google GCP Technical Architect is a highly performant GCP Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data soltuions on cloud. Using Google GCP public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications.

Role & Responsibilities:Work with Sales and Bus Dev teams in providing Data and GCP Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
Minimum 5 years of Consulting or client service delivery experience on Google GCP
Minimum 10 years of experience in big data, database and data warehouse architecture and delivery
Bachelors degree or 12 years previous professional experience
Able to travel 100% (M-TH)
Minimum of 5 years of professional experience in 2 of the following areas:
Solution/technical architecture in the cloud
Big Data/analytics/information analysis/database management in the cloud
IoT/event-driven/microservices in the cloud
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc.:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Tensorflow & Sheets

Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Certified GCP Solutions Architect - Associate
Certified GCP Solutions Architect – Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)
Certified GCP AI/ML Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP
Strong in Java, C##, Spark, PySpark, Unix shell/Perl scripting
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
- Multi-cloud experience beyond GCP a plus - AWS and Azure

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
63,"Sr Professional, Data Engineer","Irving, TX",Irving,TX,None Found,None Found,"BS Degree or equivalent work experience
Formal training and /or certifications in programming languages a plus
Typically have 4-8 years of directly related experience.
Experience with data used in CoreLogic products and processes (public record, tax, census, appraisal, etc.).
Fluent in multiple program languages at an expert level
Strong experience with various computer platforms and application environments
Expertise with developing multiple tiers of multi-tiered software applications
Expertise designing programs and data systems
Constantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team
Project management skills
Experience with ETL methodologies
Strong understanding of data structures and design
Experience with data flow, data enrichment, transformations
Ability to optimize database queries and performance
Demonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation
Experience in developing data service processes and system infrastructure to be used Enterprise Wide
Strong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.
","BS Degree or equivalent work experience
Formal training and /or certifications in programming languages a plus
Typically have 4-8 years of directly related experience.
Experience with data used in CoreLogic products and processes (public record, tax, census, appraisal, etc.).
Fluent in multiple program languages at an expert level
Strong experience with various computer platforms and application environments
Expertise with developing multiple tiers of multi-tiered software applications
Expertise designing programs and data systems
Constantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team
Project management skills
Experience with ETL methodologies
Strong understanding of data structures and design
Experience with data flow, data enrichment, transformations
Ability to optimize database queries and performance
Demonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation
Experience in developing data service processes and system infrastructure to be used Enterprise Wide
Strong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.
",None Found,"BS Degree or equivalent work experience
Formal training and /or certifications in programming languages a plus
Typically have 4-8 years of directly related experience.
Experience with data used in CoreLogic products and processes (public record, tax, census, appraisal, etc.).
Fluent in multiple program languages at an expert level
Strong experience with various computer platforms and application environments
Expertise with developing multiple tiers of multi-tiered software applications
Expertise designing programs and data systems
Constantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team
Project management skills
Experience with ETL methodologies
Strong understanding of data structures and design
Experience with data flow, data enrichment, transformations
Ability to optimize database queries and performance
Demonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation
Experience in developing data service processes and system infrastructure to be used Enterprise Wide
Strong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.
",None Found,"Job Summary
Join the team that powers the global real estate economy - CoreLogic is an innovative, future focused company whose vision is to deliver unique property-level insights that power the global real estate economy.
We are a $1.95 billion in sales company with more than 6,000 employees globally serving the financial services and insurance industries. We are evolving at a rapid pace and the clients we serve are challenged from every direction, which means we are growing and innovating to help drive their success. Working together, and differentiated by our superior data, analytics and data-enabled solutions, we empower our clients to make smarter business decisions through data-driven insights. We take initiative, are fully accountable, build respect and trust, make transparency a must—and engage, include and collaborate at every turn.
We take pride in our work and believe in cultivating a work environment that supports and values our greatest asset: our talented employees.
Job Description:
Job Duties
Devises/modifies procedures to solve problems considering computer equipment capacity and limitations, operating time, and desired results for a specific data process. Designs, codes, tests, debugs, and documents those programs.
Prepare detailed specifications from which programs will be written, designed, coded, tested and debugged.
Consult with users and develop business relationships and integrate activities with other departments to ensure successful implementation.
May lead small projects or regularly coach other team members to ensure data processes and systems are developed in a way that complies with data architecture, standards and internally established methodologies and practices applicable to Data Operations.
Monitor and report to management on the status of project efforts, anticipating/identifying issues that inhibit the attainment of project goals and implementing corrective actions.
Develop business relationships and integrate activities with other departments to ensure successful implementation and support project efforts. Foster and maintain good relationships with customers and colleagues to meet expected customer service levels.
Job Qualifications:
Education, Experience, Knowledge and Skills
BS Degree or equivalent work experience
Formal training and /or certifications in programming languages a plus
Typically have 4-8 years of directly related experience.
Experience with data used in CoreLogic products and processes (public record, tax, census, appraisal, etc.).
Fluent in multiple program languages at an expert level
Strong experience with various computer platforms and application environments
Expertise with developing multiple tiers of multi-tiered software applications
Expertise designing programs and data systems
Constantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team
Project management skills
Experience with ETL methodologies
Strong understanding of data structures and design
Experience with data flow, data enrichment, transformations
Ability to optimize database queries and performance
Demonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation
Experience in developing data service processes and system infrastructure to be used Enterprise Wide
Strong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.
CoreLogic offers an empowered work environment that encourages creativity, initiative and professional growth and provides a competitive salary and benefits package. CoreLogic is an Equal Opportunity/Affirmative Action employer committed to attracting and retaining the best-qualified people available, without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, disability or status as a veteran of the Armed Forces, or any other basis protected by federal, state or local law. CoreLogic maintains a Drug-Free Workplace. We are fully committed to employing a diverse workforce and creating an inclusive work environment that embraces everyone’s unique contributions, experiences and values. Please apply on our website for consideration."
64,Data Engineer - Wealth Management,"Plano, TX 75024",Plano,TX,75024,None Found,"Bachelor’s degree in Computer Science or any other relevant field.
Strong Database/SQL skills
5+ years hands on ETL skills – Informatica, Pentaho
Unix/Perl scripting
Strong knowledge of Data modeling/Data Warehousing concepts
Experience of a scheduling tool - CA/Autosys
Experience working within an Agile environment
Demonstrated analytical and problem solving skills.
","Bachelor’s degree in Computer Science or any other relevant field.
Strong Database/SQL skills
5+ years hands on ETL skills – Informatica, Pentaho
Unix/Perl scripting
Strong knowledge of Data modeling/Data Warehousing concepts
Experience of a scheduling tool - CA/Autosys
Experience working within an Agile environment
Demonstrated analytical and problem solving skills.
",None Found,None Found,None Found,"As a member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation & engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You’ll work in a collaborative, trusting, thought-provoking environment—one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.

This role requires a wide variety of strengths and capabilities, including:
Strong Data Engineer with strong command over PL/SQL, data integration/transformation tools like Pentaho, Informatica and exposure to Big Data technology stack (Hadoop, Spark, Hive, Impala, etc.).
Utilize Agile methodology and adhere to coding standards, procedures and techniques while contributing to the technical code documentation.
Provide high quality technology solutions that address business needs developing applications within mature technology environments.
Converting data to stories using advanced analytical and visualization techniques to help with data-driven decision making and management reporting.
Devops model - Design, develop, code, test, debug, document and support.
Collaborate with team and come up with solutions for any identified problem by team.
Deployment of newly build modules in QA and Production environment.
Manage code quality for total build effort.
Coordinate with end users during User Acceptance Testing.
BS/BA degree or equivalent experience
Advanced knowledge of application, data and infrastructure architecture disciplines
Qualifications/Skills Required:
Bachelor’s degree in Computer Science or any other relevant field.
Strong Database/SQL skills
5+ years hands on ETL skills – Informatica, Pentaho
Unix/Perl scripting
Strong knowledge of Data modeling/Data Warehousing concepts
Experience of a scheduling tool - CA/Autosys
Experience working within an Agile environment
Demonstrated analytical and problem solving skills.
Desirable Skills
Hadoop / Spark / Hive, big data exposure is a big plus
Experience with any Reporting/Business Intelligence tools (Qlikview/Qliksense/Tableau/OBIEE)
Coding skills in Java/JavaScript/Python
Financial industry experience
Scrum/Agile knowledge
Strong verbal communication skills
Our Asset and Wealth Management division is driven by innovators like you who are driven to create technology solutions that make us work more efficiently and help our businesses grow. It’s our mission to efficiently take care of our clients’ wealth, helping them get, and remain properly invested. Across 27 cities, our team of 4,600 agile technologists thrive in a cloud-native environment that values continuous learning using a data-centric approach in developing innovative technology solutions.

When you work at JPMorgan Chase & Co., you’re not just working at a global financial institution. You’re an integral part of one of the world’s biggest tech companies. In 20 technology centers worldwide, our team of 50,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $10B+ annual investment in technology enables us to hire people to create innovative solutions that will are transforming the financial services industry.


At JPMorgan Chase & Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you’re looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you."
65,Data Engineer 3 (TX),"Plano, TX",Plano,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Description:

Responsible for completing our transition into fully automated operational reports across different functions within Care (including repair operations, contact center, digital support, product quality and finance) and for bringing our Care Big Data capabilities to the next level by designing and implementing a new analytics governance model, with emphasis on architecting consistent root cause analysis procedures resulting in enhanced operational and customer engagement results.

Summary:
The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization�s data assets.

Job Responsibilities:
� Design, construct, install, test and maintain highly scalable data management systems.
� Ensure systems meet business requirements and industry practices.
� Design, implement, automate and maintain large scale enterprise data ETL processes.
� Build high-performance algorithms, prototypes, predictive models and proof of concepts.

Skills:
� Ability to work as part of a team, as well as work independently or with minimal direction.
� Excellent written, presentation, and verbal communication skills.
� Collaborate with data architects, modelers and IT team members on project goals.
� Strong PC skills including knowledge of Microsoft SharePoint.

Education/Experience:
� Bachelor's degree in a technical field such as computer science, computer engineering or related field required.
� 5-7 years of experience required.
� Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI.
***Please submit candidates with the following must haves***

Analytical and problem solving skills, applied to Big Data domain
Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark
5-8 years of Python or Java/J2EE development experience
3+ years of demonstrated technical proficiency with Hadoop and big data projects

Day to Day duties:
Data collection – gather information and required data fields.
Data manipulation – Join data from multiple data sources and build ETLs to be sent to Tableau for reporting purpose
- Measure & Improve - Implement success indicators to continuously measure and improve, while providing relevant insight and reporting to leadership and teams.

Interview process:
4 interviewers – Ashok Padmaraju and Jyotsna Pagadrai to evaluate from the technical perspective. Kayla and Guilherme Koga to evaluate from the business perspective.

Analytical and problem solving skills, applied to Big Data domain
Proven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark
5-8 years of Python or Java/J2EE development experience
3+ years of demonstrated technical proficiency with Hadoop and big data projects"
66,Enterprise Data Architect Consultant,"Plano, TX 75023",Plano,TX,75023,None Found,None Found,None Found,None Found,None Found,None Found,"Who we are

Collaborative.
Respectful. A place to dream and do. These are just a few words that
describe what life is like at Toyota. As one of the world’s most admired
brands, Toyota
is growing and leading the future of mobility through innovative, high-quality
solutions designed to enhance lives and delight those we serve. We’re looking
for diverse, talented team members who want to grow and challenge what's possible.
An important part of the Toyota family is Toyota Financial
Services (TFS), the finance and insurance brand for Toyota and Lexus in North America.
While TFS is a separate business entity, it is an essential part of this
world-changing company – delivering on Toyota’s vision to move people beyond
what’s possible. At TFS, you will help create best-in-class customer
experiences in an innovative, collaborative environment.

Who we’re looking for

The TFS Information
and Digital Systems (IDS) Department is looking for a passionate and highly-motivated
Data Architect Consultant responsible for the entire TFS enterprise data
architecture and development lifecycle process that translates business
requirements into functioning software systems, services and solutions aligned
to a Factory or and DIO’s Application Portfolio or specific portion of the
application portfolio. This role will
require a strong knowledge of data life cycle and application development,
database design and systems implementation as this role is a ""enterprise
data architect"" crafting technical solutions to complex business needs.
The
Enterprise Data Architect must synthesize requirements information from the
business and develop comprehensive technical architecture design that
constitutes the blueprint for a the solution.
Data architect play a significant role in ensuring the solution aligns
and fits with enterprise systems and configurations. This roll (often
times working with vendor resources) will develop technical products that will
be validated, reviewed and tested. The
best candidates will be highly technical professionals experienced in data
architecture/database programming and web application development.
 Solid knowledge and implementation experience
of Cloud Computing
 Domain Knowledge per area
 Multiple years of experience working with
Financial organization with emphasis on Captive Auto Finance where the
potential candidate has working knowledge of
 Need to have strong experience with programming
using Java or Scala or Python
Need to have working experience in MongoDB
 Need to have substantial experience BigData
ecosystem (e.g., Hadoop, Data Lake)
 Skills with iDERA - Embarcadero ER/Studio
suite, Erwin, or other data modeling tool for OLTP systems
 Expert in data modeling for relational and
document-based data stores, including conceptual, logical, and physical data
models
 Guides teams through the complete software
development life cycle-from concept to delivery
 Experience working with terabyte data sets,
billion+ row tables, and high transaction volumes in an OLTP
 Demonstrated leadership in root cause analysis
and risk mitigation
 Expertise in system performance analysis and
tuning
 Experience with service-oriented architectures
and microservices
 Experience with distributed systems and cloud
computing
 Experience with Jenkins or similar continuous
integration systems
Demonstrated leadership in agile software
development environments
 Self-starter with exceptional communication
skills
5 + years of experience in a leadership role of
establishing, driving and implementing a multi-year technology strategy(s) and
Technology Roadmap
 At least 7 years of experience in technology
consulting, enterprise and solutions architecture and architectural frameworks
 At least 7+ years of experience in project
execution
Solid knowledge and implementation experience
with Integration techniques and enabling technology working knowledge including
streaming services/processes (e.g., Kafka, Kinesis,
Spark etc.)
Provide a critical role in a highly agile scrum environment working
against tight requirements
 Drive standards around and propose system
solutions to meet business requirements.
Integrate
the data and technical system changes across the entire application portfolio
 Oversee
development of data and technical changes and validate and implement the data
and technical changes provided by the vendor.
Act
as liaison with the vendors, factories and product owners and end users to
create and finalize end-to-end technical solutions.
 Supervise
and provide guidance to factories product designers and other domain Architects
on the team
Review/approve
systems documentation as changes/enhancements that are planned to deploy to
ensure accuracy need to communicate and coordinate with key business
stakeholders on business process changes, requirements definition (e.g., BRD,
FRD), testing, end user acceptance criteria, operational readiness, and change
management are crucial responsibilities of this position.
 Success
in this role requires in-depth knowledge and hands-on technical skills, with
strong leadership, oral, written communication skills; an ability to present an
discuss technical, functional and managerial information clearly and concisely
to executive management. The ability to
work effectively within IDS, various vendors (service integrators as well as
technology providers) and business teams to develop solutions that meet
technology standards and business needs is required.
 Solid
knowledge and implementation experience of Cloud Computing
Solid
knowledge and implementation experience with Integration techniques and
enabling technology working knowledge including streaming services/processes
(e.g., Kafka, Kinesis, Spark etc.)
 Provide a critical role in a highly agile scrum environment
working against tight requirements
What you bring
8 – 12 years’ industry experience with having auto finance experience will be a plus
4 years’ experience as a Solution Architect
4 years’ experience as a Software Engineer
3 years’ experience using the .Net framework
3 years’ experience using J2EE framework
1 years’ experience developing software for mobile devices using emerging technologies
4 years’ experience developing software for internet web applications and portals
1 years’ experience with performance architecture, tuning and debugging distributed systems.
Mastery of multi-tenancy, multi-branding, multi-device and multi country and language architecture and enterprise services implementation
Mastery of database design for applications and familiar with Information Management / Data Management
Possesses excellent verbal and written communication and interpersonal skills and the ability to interface with leadership and all levels of associates.
Experience with Open Source Development
Demonstrated success as Solution Architect on both large and small projects
As a Big Data Engineer You will be accountable for data storage and ETL development and improvements
Demonstrated continued knowledge acquisition of emerging technologies
Ability to work independently, with strong organizational and flexibility skills in a team-oriented environment

What we’ll bring


During your interview process, our team can fill you in on all the details of our industry-leading benefits and career development opportunities. A few highlights include:


A work environment built on teamwork, flexibility and respect
Professional growth and development programs to help advance your career, as well as tuition reimbursement
Vehicle purchase & lease programs
Comprehensive health care and wellness plans for your entire family
Flexible work options based on business needs
Toyota 401(k) Savings Plan featuring a company match, as well as an annual retirement contribution from Toyota regardless of whether you contribute
Paid holidays and paid time off
Referral services related to prenatal services, adoption, child care, schools and more
Flexible spending accounts
Relocation assistance
Onsite amenities such as fitness center, restaurants, etc
What you should know

Our success begins and ends with our people. We embrace diverse perspectives and value unique human experiences. We are proud to be an equal opportunity employer that celebrates the diversity of the communities where we live and do business. Applicants for our positions are considered without regard to race, ethnicity, national origin, sex, sexual orientation, gender identity or expression, age, disability, religion, military or veteran status, or any other characteristics protected by law.

Have a question or need assistance with your application? Check out the How to Apply section of our careers page on Toyota.com!"
67,AI Model Development Lead For Marketing (Analytics Manager 5),"Dallas, TX",Dallas,TX,None Found,None Found,"
8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques
4+ years of experience working with digital data science including analyzing cookie-level data",None Found,None Found,None Found,None Found,"Overview:
Wells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.

SUCCESS PROFILE
Check out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.

Analytical
Detail-oriented
Insightful
Inventive
Problem Solver
Curious
Benefits
Wells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:

Medical, Dental and Vision
Employer Matching 401(k)
Tuition Reimbursment
Maternity and Paternity Leave
Paid Time Off
Responsibilties
Job Description
At Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.
Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.
Enterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.
As part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, marketing, and virtual channels is looking for an experienced AI leader to manage the development of AI models for marketing.

This leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on marketing’s AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and marketing executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.
KEY RESPONSIBILITIES INCLUDE:
Build and grow a team of data scientists responsible for AI model development in support of marketing
Design, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions
Partner with marketing executives to frame the problem, manage the model development process, and business relationship
Manage a portfolio of the data science projects including the following responsibilities:
Help finalize project scope working with business partners
On-going touch-base with business partners and governance stakeholders
Define priorities in partnership with the business partners during on-going development
Work with AI technology and production teams to operationalize models
May be called upon to review vendor models and solutions and/or models developed outside of EADS
Divisional Information:
Data Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.
As a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:
Lead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.
Accomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.

Required Qualifications

8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques
4+ years of experience working with digital data science including analyzing cookie-level data


Desired Qualifications

Strong analytical skills with high attention to detail and accuracy
Ability to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members
Meeting facilitation experience in leading discussions that result in consensus and commitment


Other Desired Qualifications
Data science experience in the ad tech space (DMPs, DSPs, Google and/or Adobe Cloud, digital attribution)
4+ years managing or directing data scientist/ statistician/ data engineer teams
Hands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet
Detail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable
Experience with agile project management methodologies for data science

Disclaimer

All offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.

Relevant military experience is considered for veterans and transitioning service men and women.

Wells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.

ENT FINANCE"
68,Lean Big Data Engineer,"Irving, TX",Irving,TX,None Found,None Found,"
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets",None Found,"
Responsible for design, prototyping and delivery of software solutions within the big data eco-system
Leading projects and/or serving as analytics SME to provide new or enhanced data to the business
Improving data governance and quality increasing the reliability of our data
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
",None Found,None Found,"Where good people build rewarding careers.
Think that working in the insurance field can’t be exciting, rewarding and challenging? Think again. You’ll help us reinvent protection and retirement to improve customers’ lives. We’ll help you make an impact with our training and mentoring offerings. Here, you’ll have the opportunity to expand and apply your skills in ways you never thought possible. And you’ll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.
About our team
360 Finance Advanced Analytics data engineering team works with multiple internal and external data sources to deliver data that is readily available, easily accessible, accurate and complete. They are responsible for building a centralized data lake/hub using the Hadoop ecosystem that will be used by Reporting & Operational Analytics teams and the Machine learning teams.
Job Description
This Lead Consultant is an experienced professional who is responsible for leveraging data and analytics to help automate and optimize Claims Analytics Data processes enabling our Claims employees to focus on serving our customers and delivering the most advanced claims experience on the planet. They will be responsible for the strategy around how we bring together complex data into clean and useful data structures making our valuable data more approachable.
Key Responsibilities
Responsible for design, prototyping and delivery of software solutions within the big data eco-system
Leading projects and/or serving as analytics SME to provide new or enhanced data to the business
Improving data governance and quality increasing the reliability of our data
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
Key Responsibilities (Cont'd)
Responsible for designing and building new Big Data systems for turning data into actionable insights
Train and mentor junior team members on Big Data/Hadoop tools and technologies
Identifies opportunities for improvement and presents recommendations to management
Develop solutions and iterates quickly to continuously improve
Seeks out and evaluates emerging big data technologies and open-source packages
Participate in strategic planning discussions with technical and non-technical partners
Uses, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).
Uses, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e. Java, Python, SQL, R)
Job Qualifications
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Roles and Responsibilities
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.
Some understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.
Experience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required
5-6+ years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required
Experience with Agile development methodologies and tools to iterate quickly on product changes,
Roles and Responsibilities (Cont'd)
developing user stories and working through backlog (Continuous Integration and JIRA a plus)
Experience with Airflow a plus
Working knowledge of Tableau – a plus
Advanced oral and written communication skills
Strong quantitative and analytical abilities
Good organizational and time management skills
Ability to manage and coach others
Decision making capabilities including problem solving approaches, decision frameworks; ability to design and lead complex analysis
Strong interpersonal skills
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work. Good Life. Good Hands®.
As a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.
Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.
Allstate generally does not sponsor individuals for employment-based visas for this position.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click “here” for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click “here” for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.
It is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment."
69,Data Engineer,"Irving, TX",Irving,TX,None Found,None Found,"
Skilled in Python and preferably in one or more programming languages like C++, Java, Go, etc
Experience with Docker and Kubernetes
Experience working with SQL and NoSQL based database solutions
Public cloud technology experience in production (Azure, AWS, or Equivalent)
3+ years of collective experience in data engineering and data analysis
2+ years of experience architecting, building and administering large-scale distributed applications
",None Found,"
Skilled in Python and preferably in one or more programming languages like C++, Java, Go, etc
Experience with Docker and Kubernetes
Experience working with SQL and NoSQL based database solutions
Public cloud technology experience in production (Azure, AWS, or Equivalent)
3+ years of collective experience in data engineering and data analysis
2+ years of experience architecting, building and administering large-scale distributed applications
",None Found,None Found,"Overview:
Our client is looking for a Data Engineer to join the R&D Team. You’re joining a multidisciplinary team of product directors, product managers, and digital analysts to create product listings across the top ecommerce marketplaces.

We are preferred supplier to 7-Eleven. This position may be available for conversion to full time employment upon successful evaluation of performance. You’ll be working in a fast-paced, eclectic environment of talented professionals who are leading the industry in digital capabilities.

Responsibilities:
The ideal candidate will be driven and passionate in creating the next generation of data products and capabilities. You will work directly with Product Owners and customers to deliver data products in a collaborative and agile environment. The ideal candidate will build data pipeline frameworks to automate high-volume and real-time data delivery and streaming data hub.

Basic Qualifications:

Skilled in Python and preferably in one or more programming languages like C++, Java, Go, etc
Experience with Docker and Kubernetes
Experience working with SQL and NoSQL based database solutions
Public cloud technology experience in production (Azure, AWS, or Equivalent)
3+ years of collective experience in data engineering and data analysis
2+ years of experience architecting, building and administering large-scale distributed applications

Preferred Qualifications:

Experience in engineering data pipelines using Big Data technologies (Hadoop, Spark, Storm, Kafka, etc) on large scale unstructured data sets is a plus
Familiarity with distributed data stores like Elasticsearch is a plus
Familiarity with Machine Learning concepts is a plus

GOIN Technology is a technology consulting and managed services business headquartered in Dallas/Ft. Worth. The company is focused on CIO/CTO Advisory Consulting, Information Security, Digital Transformation, Software/Data Engineering, Innovation as a Service and Managed Services."
70,Data Engineer - Wealth Management,"Plano, TX 75024",Plano,TX,75024,None Found,"Bachelor’s degree in Computer Science or any other relevant field.
Strong Database/SQL skills
5+ years hands on ETL skills – Informatica, Pentaho
Unix/Perl scripting
Strong knowledge of Data modeling/Data Warehousing concepts
Experience of a scheduling tool - CA/Autosys
Experience working within an Agile environment
Demonstrated analytical and problem solving skills.
","Bachelor’s degree in Computer Science or any other relevant field.
Strong Database/SQL skills
5+ years hands on ETL skills – Informatica, Pentaho
Unix/Perl scripting
Strong knowledge of Data modeling/Data Warehousing concepts
Experience of a scheduling tool - CA/Autosys
Experience working within an Agile environment
Demonstrated analytical and problem solving skills.
",None Found,None Found,None Found,"As a member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation & engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You’ll work in a collaborative, trusting, thought-provoking environment—one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.

This role requires a wide variety of strengths and capabilities, including:
Strong Data Engineer with strong command over PL/SQL, data integration/transformation tools like Pentaho, Informatica and exposure to Big Data technology stack (Hadoop, Spark, Hive, Impala, etc.).
Utilize Agile methodology and adhere to coding standards, procedures and techniques while contributing to the technical code documentation.
Provide high quality technology solutions that address business needs developing applications within mature technology environments.
Converting data to stories using advanced analytical and visualization techniques to help with data-driven decision making and management reporting.
Devops model - Design, develop, code, test, debug, document and support.
Collaborate with team and come up with solutions for any identified problem by team.
Deployment of newly build modules in QA and Production environment.
Manage code quality for total build effort.
Coordinate with end users during User Acceptance Testing.
BS/BA degree or equivalent experience
Advanced knowledge of application, data and infrastructure architecture disciplines
Qualifications/Skills Required:
Bachelor’s degree in Computer Science or any other relevant field.
Strong Database/SQL skills
5+ years hands on ETL skills – Informatica, Pentaho
Unix/Perl scripting
Strong knowledge of Data modeling/Data Warehousing concepts
Experience of a scheduling tool - CA/Autosys
Experience working within an Agile environment
Demonstrated analytical and problem solving skills.
Desirable Skills
Hadoop / Spark / Hive, big data exposure is a big plus
Experience with any Reporting/Business Intelligence tools (Qlikview/Qliksense/Tableau/OBIEE)
Coding skills in Java/JavaScript/Python
Financial industry experience
Scrum/Agile knowledge
Strong verbal communication skills
Our Asset and Wealth Management division is driven by innovators like you who are driven to create technology solutions that make us work more efficiently and help our businesses grow. It’s our mission to efficiently take care of our clients’ wealth, helping them get, and remain properly invested. Across 27 cities, our team of 4,600 agile technologists thrive in a cloud-native environment that values continuous learning using a data-centric approach in developing innovative technology solutions.

When you work at JPMorgan Chase & Co., you’re not just working at a global financial institution. You’re an integral part of one of the world’s biggest tech companies. In 20 technology centers worldwide, our team of 50,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $10B+ annual investment in technology enables us to hire people to create innovative solutions that will are transforming the financial services industry.


At JPMorgan Chase & Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you’re looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you."
71,AI Model Development Lead for Virtual Channels (Analytic Manager 5),"Addison, TX 75001",Addison,TX,75001,None Found,"
8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques",None Found,None Found,None Found,None Found,"Overview:
Wells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.

SUCCESS PROFILE
Check out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.

Analytical
Detail-oriented
Insightful
Inventive
Problem Solver
Curious
Benefits
Wells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:

Medical, Dental and Vision
Employer Matching 401(k)
Tuition Reimbursment
Maternity and Paternity Leave
Paid Time Off
Responsibilties
Job Description
At Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.
Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.
Enterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.
As part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, Virtual Channels, and virtual channels is looking for an experienced AI leader to manage the development of AI models for Virtual Channels.

This leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on Virtual Channels’ AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and Virtual Channels executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.

KEY RESPONSIBILITIES INCLUDE:
Build and grow a team of data scientists responsible for AI model development in support of Virtual Channels
Design, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions
Partner with Virtual Channels executives to frame the problem, manage the model development process, and business relationship
Manage a portfolio of the data science projects including the following responsibilities:
Help finalize project scope working with business partners
On-going touch-base with business partners and governance stakeholders
Define priorities in partnership with the business partners during on-going development
Work with AI technology and production teams to operationalize models
Will be called upon to review vendor models and solutions and/or models developed outside of EADS
Divisional Information:
Data Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.
As a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:
Lead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.
Accomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.

Required Qualifications

8+ years of experience in analytics, modeling, or a combination of both
6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function
A Master's degree or higher
5 + years of experience using quantitative machine learning techniques


Desired Qualifications

Strong analytical skills with high attention to detail and accuracy
Ability to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members
Meeting facilitation experience in leading discussions that result in consensus and commitment


Other Desired Qualifications
4+ years managing or directing data scientist/ statistician/ data engineer teams
Hands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet
Detail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable
Experience with agile project management methodologies for data science
Experience with Big Data or Hadoop tools such as Spark, Hive, Kafka and Map

Street Address
NC-Charlotte: 401 S Tryon St - Charlotte, NC
MN-Minneapolis: 600 S 4th St - Minneapolis, MN
NC-Charlotte: 11625 N Community House Road - Charlotte, NC
SC-Fort Mill: 3480 State View Blvd - Fort Mill, SC
TX-Addison: 5080 Spectrum Dr - Addison, TX
TX-DAL-Downtown Dallas: 1445 Ross Ave - Dallas, TX
TX-Irving: 5000 Riverside Drive - Irving, TX
AZ-Tempe: 1150 W Washington St - Tempe, AZ
IA-Des Moines: 6200 Park Ave - Des Moines, IA
IA-Des Moines: 800 Walnut St - Des Moines, IA
GA-Atlanta: 3579 Atlanta Ave - Atlanta, GA


Disclaimer

All offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.

Relevant military experience is considered for veterans and transitioning service men and women.

Wells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.

ENT FINANCE"
