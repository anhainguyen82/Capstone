,Title,Location,City,State,Zip,Country,Qualifications,Skills,Responsibilities,Education,Requirement,FullDescriptions
0,"Data Engineer, Music Data Experience","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Bachelor’s degree in Data Science, Applied Science, Computer Science, Computer Engineering or related technical discipline3+ years of experience as a data/software developer/scientist or related technical jobExperience with SQL and Spark based data pipelinesExperience with traditional and cloud data modeling techniquesA passion for improving customer experienceExcellent verbal and written communication skills and technical writing skills

Amazon Music is awash in data! To help make sense of it all, the Music Data Experience team enables repeatable, easy, in depth analysis of music customer behaviors. We reduce the cost in time and effort of analysis, data set building, model building, and user segmentation. Our goal is to empower all teams at Amazon Music to make data driven decisions and effectively measure their results by providing high quality, high availability data, and democratized data access through self-service tools.

If you love the challenges that come with big data then this role is for you. We collect billions of events a day, manage petabyte scale data on Redshift and S3, and develop data pipelines using Spark/Scala EMR, SQL based ETL, and Java services.

You are a talented, seasoned, and detail-oriented Data Engineer, BI Engineer, or Data Scientist who wants to take on big data challenges in an agile way. Duties include designing events and signals, building big data pipelines, creating efficient data models, performing analysis, and statistical/ML modeling. We manage Amazon Music's most important data pipelines and data sets, and are expanding our self-service data knowledge and capabilities through an Amazon Music data university.

This role requires you to focus on the data end to end, from producers to consumers. You will develop an understanding of our data, analytical techniques, and how to connect insights to the business, and you will gain practical experience in insisting on highest standards on operations in ETL and big data pipelines. With our Amazon Music Unlimited and Prime Music services, and our top music provider spot on the Alexa platform, providing high quality, high availability data to our internal customers is critical to our customer experiences.

Music Data Experience team develops data specifically for a set of key business domains like personalization and marketing and provides and protects a robust self-service core data experience for all internal customers. We deal in AWS technologies like Redshift, S3, EMR, EC2, DynamoDB, Kinesis Firehose, and Lambda. In 2019 this team will migrate Amazon Music's information model and data pipelines to a data lake storage and EMR/Spark processing layer. You'll build our data university and partner with Product, Marketing, BI, and ML teams to build new behavioral events, pipelines, datasets, models, and reporting to support their initiatives. You'll also continue to develop our offline analytics capabilities in Tableau and build out our real time dashboarding capabilities.

Amazon Music

Imagine being a part of an agile team where your ideas have the potential to reach millions. Picture working on cutting-edge consumer-facing products, where every single team member is a critical voice in the decision-making process. Envision being able to leverage the resources of a Fortune-500 company within the atmosphere of a start-up. Welcome to Amazon Music, where ideas are born and come to life as Amazon Music Unlimited, Prime Music, and so much more.

Everyone on our team has a meaningful impact on product features, new directions in music streaming, and customer engagement. We are looking for new team members across a variety of job functions including software engineering/development, marketing, design, ops and more. Come join us as we make history by launching exciting new projects in the coming year.

Our team is focused on building a personalized, curated, and seamless music experience. We want to help our customers discover up-and-coming artists, while also having access to their favorite established musicians. We build systems that are distributed on a large scale, spanning our music apps, web player, and voice-forward audio engagement on mobile and Amazon Echo devices, powered by Alexa to support our customer base. Amazon Music offerings are available in countries around the world, and our applications support our mission of delivering music to customers in new and exciting ways that enhance their day-to-day lives.

Come innovate with the Amazon Music team!

Graduate degree in a related technical field5+ years of experience as a data/software developer/scientist or related technical jobExperience with Agile DevelopmentExperience with statistical modeling or machine learning (Classification, Collaborative Filtering)Experience with AWS servicesA love of music!
Amazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

#MusicJobs"
1,"Data Engineer - Sunnyvale, California","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Working at the clients site in Sunnyvale
Flexible work hours, full benefits, 401K
A day off for your birthday
Competitive total compensation
So, are you ready for a challenge, work in a multi disciplinary team and have an immediate impact on our projects? Well, this role is for you.
Don't forget to follow Averna's journey to becoming the best in its field, on YouTube and LinkedIn."
2,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
Minimum of 3 years of relevant experience in building and architecting data solutions
Deep understanding of distributed systems
Expert in SQL and high-level languages such as Python, Java, or Scala
Built and maintained data warehouses and ETL pipelines
Experience with Data modeling
You have worked with big data solutions like Redshift, Snowflake, Hadoop or Hive
Experience with realtime data streaming infrastructure like AWS Kinesis, Spark or Kafka
Worked with Cloud-based architecture such as AWS or Google Cloud",None Found,"
Build and maintain our data warehouse and data pipelines
Scaling up our data infrastructure to meet business needs
Deploy sophisticated analytics programs, machine learning and statistical methods
Work cross-functionally with our product, business, finance and engineering teams",None Found,None Found,"About Spin

Spin operates electric scooters in cities and campuses nationwide, bringing sustainable last-mile mobility solutions to diverse communities. Recognized for its consistent cooperation and collaboration with cities, Spin partners closely with transportation planners, elected officials, community groups, and university administrators to bring stationless mobility options to streets in a responsible and carefully orchestrated manner.

Based in San Francisco, Spin is a diverse team of engineers, designers, urban planners, policymakers, lawyers and operators with experience from Y Combinator, Lyft, Uber, local and federal government, and the transportation advocacy world. Spin was known for launching the first stationless mobility program in Seattle, and has since expanded to become the exclusive electric scooter partner in mid-sized cities like Coral Gables, Florida and Lexington, Kentucky, and one of a few permitted scooter operators in large cities like Denver, Detroit, and Washington, D.C. The team embeds in cities and neighborhoods to understand their specific transportation needs, and hires locally from the community.

Spin is expanding quickly and looking for top-tier talent to help us bring affordable and accessible transportation options to cities and define what future safe streets will look like.

About the Role

Being a data informed company, data helps us create exceptional experience for our customers and provide insights into the effectiveness of our product.

We are looking for Data Engineers that will build and maintain our data warehouse and data pipelines, collect data from multiple sources, and expose services that make data a first class citizen at Spin. You will be building, architecting and launching highly reliable and scalable data pipelines to support data processing and analytics needs. Your efforts will allow access to business and user behavior insights.

The Team

Our engineering team consists engineers that are passionate about creating finely polished and intuitive experiences and, at the same time, obsess over performance and reliability of what we build. We challenge the status quo and strive towards finding the best way to solve problems.

We promote being a more well rounded engineer by working on different parts of the engineering stack. We also work in very small groups to keep processes and overhead low, so we have a lot of trust and accountability to perform the work required to build the best product.
Responsibilities
Build and maintain our data warehouse and data pipelines
Scaling up our data infrastructure to meet business needs
Deploy sophisticated analytics programs, machine learning and statistical methods
Work cross-functionally with our product, business, finance and engineering teams
Qualifications
Minimum of 3 years of relevant experience in building and architecting data solutions
Deep understanding of distributed systems
Expert in SQL and high-level languages such as Python, Java, or Scala
Built and maintained data warehouses and ETL pipelines
Experience with Data modeling
You have worked with big data solutions like Redshift, Snowflake, Hadoop or Hive
Experience with realtime data streaming infrastructure like AWS Kinesis, Spark or Kafka
Worked with Cloud-based architecture such as AWS or Google Cloud
Benefits & Perks
Opportunity to join a fast-growing startup and help shape and establish the company’s industry leadershipCompetitive health benefitsDaily catered lunch in our SF officeUnlimited PTO for salaried rolesCommuter stipend plus pre-tax benefitsMonthly cell phone bill stipendWellness perk for salaried roles

Spin is an equal opportunity employer and will not discriminate against any employee or applicant for employment in an unlawful matter. We celebrate diversity and are committed to creating an inclusive environment for all individuals. Spin treats all employees and job applicants on the basis of merit, qualifications, and competence without regard to any qualified individuals' sex, race, color, religion, national origin, ancestry, gender (including pregnancy, breastfeeding, or related medical condition), sexual orientation, gender identity, gender expression, age, physical or mental disability, medical condition, genetic characteristic or information, marital status, military and veteran status, or any other characteristic protected by state or federal law. Spin also considers qualified applicants with criminal histories, consistent with applicable local, state, and federal law.

Spin is committed to providing reasonable accommodations for qualified individuals with disabilities in its job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at job_accommodations@spin.pm."
3,Principal Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Varo is on a mission to redefine banking so it's easy for everyone to make smart choices with their money. Our app offers bank accounts and high-yield savings accounts that don’t cost a thing, tools to help you manage your money and save automatically, and invitation-only personal loans at competitive rates. On the contrary, traditional banks charge fees, offer next-to-nothing savings rates, and don’t work with their customer’s best interests in mind.

Varo is distinct from other fintechs: With preliminary approval for a bank charter from the Office of the Comptroller of the Currency (OCC), we're on our way to becoming the first mobile-centric national bank in the country. Our unique team combines the best people in tech and banking, and we’re wildly passionate about keeping our customers happy by helping them manage and grow their money. Based in San Francisco and privately held, Varo has raised $178M to date, led by Warburg Pincus and The Rise Fund / TPG Growth.

DATA ENGINEERING AT VARO

As a Principal Big Data Engineer, you will play a senior role in implementing a variety of solutions to ingest data into, process data within, and expose data from, a Data Lake that enables our data warehouse, data mart, reporting and data analysts and scientists to use and explore data in an automated or self-service fashion.

As a technical leader, you will take ownership of the data architecture for processing and analyzing data across the Platform.
WHAT YOU'LL DO
Develop and maintain data strategy for Varo in terms of capabilities, architecture, and control mechanisms that support company intentions to be a bank
Design, build and maintain Big Data workflows/pipelines to process records into and out of Varo’s lake
Provide technical leadership in the area of data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics
Collaborate to actively gain buy-in from stakeholders at all levels on technology direction
Work with business partners on requirements clarification and results from rollout efforts
Participate in developing and enforcing data security & access control policies
Architect effective controls for a resilient data ingestion process
Support application data integration design and build efforts, including real-time capabilities
Proficiency in Amazon AWS big data technologies including S3, RDS, RedShift, Elasticsearch, Lambda, AWS Glue
Conduct code reviews in accordance with team processes and standards
PREVIOUS EXPERIENCES THAT'LL HELP YOU BE GREAT
Bachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience
10+ years of ETL, data modeling, warehouse and data pipelines experience.
5+ years’ experience working within the AWS Big Data/Hadoop Ecosystem (EMR is preferred)
Experience developing extract load transform tooling
Experience with downstream consumption patterns is a plus (reports, dashboards, API)AWS Glue, Redshift, RDS is a plus
Experience in Hadoop, HDFS, Hive, Python, REST API/ SOAP API, Spark2, Oozie WFs is a plus"
4,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Hinge Health’s mission is to improve the lives of people suffering from chronic conditions by digitizing the delivery of care - starting with musculoskeletal health. Our vision is to be the world’s most patient-centered Digital Hospital. We’re already achieving remarkable outcomes - helping people overcome chronic pain, avoid surgeries, return to work, and get back to doing the things they love. We've raised close to $37M and our growth shows no sign of slowing.

We are looking for an experienced Data Engineer to manage and scale our growing data assets while maintaining a high level of integrity and precision.

Our ideal candidate has an extensive understanding of how data is used for both business analysis and data science. You will work with a wide variety of data sources and storage systems to enable R&D, commercial, and clinical teams to solve problems.

Our tech stack: AWS, Aptible, Postgres, Redis, Rails, Python, Airflow, Mode Analytics, Android, React, and React Native.
RESPONSIBILITIES
Maintain our current ETL-lite while scaling it for the future
Create and maintain views and expand use of rollup tables
Identify opportunities to improve the integrity of our datasets and implement the fixes
Assist in building out our payments platform for managing medical claims
Help explore options for delivering data to clients, including possible API access
Inform our 2020 objectives and key results around scaling and data needs
REQUIREMENTS
Bachelor’s degree in C.S. or comparable degree preferred
Minimum of 3 years relevant experience in data engineering
Ability to collaborate and problem solve across teams
Excellent communication skills, both written and verbal
Python: using community-standards, linting, and testing at all appropriate levels.
SQL: comfort with joins, unions, views, rollups, windowing functions, testing
JSON parsing and fluency with RESTful APIs
Operational competency with cloud-hosted systems such as AWS, Aptible, or Heroku
Ability to correlate data across multiple sources: RDBs, csv, json
Understands how to write efficient code and can optimize existing software and queries
BONUS POINTS
Prior experience with healthcare data (PHI/PII/HIPAA requirements)
Experience developing software in Ruby on Rails
Understanding of user experience principles
History of technical writing
WHAT YOU'LL LOVE ABOUT US
Competitive compensation with meaningful stock options
Medical, dental, vision
401K match
3 months paid parental leave
Daily lunch
Professional Development budget
Monthly wellness benefit
Noise-cancelling headphones
Work from home policy
Opportunity to join a fantastically talented, diverse, and passionate team at a pivotal time in the company’s lifecycle
No recruiters, please.

Hinge Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
We celebrate diversity and are committed to creating an inclusive environment for all employees."
5,Sr. Data Engineer,"San Francisco, CA 94143",San Francisco,CA,94143,None Found,None Found,None Found,None Found,None Found,"Experience with building scalable and reliable data pipelines using Data engine technologies like Matillion (Informatica), Python, and SQL based programming.8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.4+ years' experience designing and developing complex ETL/ELT programs with the following Matillion (Informatica), Python etc.8+ years' experience developing complex SQLExperience using Cloud database technologies such as RedShift, Snowflake3+ years' experience programming in Python, and/or Java2+ years' experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)Experience with integration of data from multiple data sourcesExperience developing and implementing streaming data ingestion solutionsExperience in Agile methodology (2+ years)
","Role : Sr. Data Engineer 6+ month contract
San Francisco location preferred but San Jose location is okay also.
Local candidates only they must interview onsite in San Francisco with the team for the final round.
Top 3 skill sets: SQL, Python, ETL development, AWS Redshift
Requirements:Experience with building scalable and reliable data pipelines using Data engine technologies like Matillion (Informatica), Python, and SQL based programming.8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.4+ years' experience designing and developing complex ETL/ELT programs with the following Matillion (Informatica), Python etc.8+ years' experience developing complex SQLExperience using Cloud database technologies such as RedShift, Snowflake3+ years' experience programming in Python, and/or Java2+ years' experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)Experience with integration of data from multiple data sourcesExperience developing and implementing streaming data ingestion solutionsExperience in Agile methodology (2+ years)

4v1tQw9tmw"
6,Genomics Data Engineer,"Oakland, CA",Oakland,CA,None Found,None Found,"
This position requires a B.S. (M.S. or Ph.D. preferred) with 3 + years of experience in computer science, specializing in high-performance/distributed computing, data mining, machine learning, or bioinformatics.
3+ years of software engineering experience with proficiency in Python, Scala, Java or C/C++, programming languages.
Knowledge of distributed compute technologies, such as Spark, Hadoop, map-reduce, MPI, or other parallel computing frameworks is essential.
Strong foundation in data engineering, data science, and/or machine learning, with demonstrated experience applying these technologies at scale on real-world data sets.
Knowledge of database technologies, indexing/partitioning, and SQL.
Experience with cloud computing (AWS preferred).
Experience engineering high volume data and scientific dataflows.
Background of bioinformatics, life sciences, genomics or biology is highly preferred.
Team player with excellent communication skills to effectively collaborate with multiple cross-functional teams of scientists, clinicians, and engineers.",None Found,"
Build out a big data distributed architecture capable of efficiently processing large-scale genomics data
Develop and deploy bioinformatics/AI analysis algorithms at scale
Build automated and production-quality data processing systems
Interact and collaborate with scientists to clearly define and iterate on requirements
Keep abreast of new state-of-the-art software data engineering and data science technologies
Aggregate and analyze genomic and other types of clinical data to find novel insights.
Develop code to implement analysis workflows in a robust and reproducible fashion.
Follow processes to improve transparency and reliability of applications, reducing project risks for on-time milestones.
Educate other scientists, engineers and management on the methods developed and how they apply to the subject domain and customer needs.",None Found,None Found,"You will work at the interface of genomics, big data engineering, and advanced analytics. The candidate will contribute to the expansion of our Apache Spark-based distributed analytics platform, building production-quality data processing infrastructure and developing scalable algorithms to analyze genomic and health data for diagnostics of rare genetic disease. This role encompasses engineering of end-to-end solutions that unify and structure diverse data sets efficiently and implement AI algorithms at scale to derive genomic insights in support of clinical applications.

The ideal candidate will have a strong background in computer science, data mining, machine learning, or a related field, with demonstrated experience in engineering scalable and performant data processing software in Spark or another distributed compute environment. In addition, previous experience in a life sciences domain or biotech is essential.
Responsibilities
Build out a big data distributed architecture capable of efficiently processing large-scale genomics data
Develop and deploy bioinformatics/AI analysis algorithms at scale
Build automated and production-quality data processing systems
Interact and collaborate with scientists to clearly define and iterate on requirements
Keep abreast of new state-of-the-art software data engineering and data science technologies
Aggregate and analyze genomic and other types of clinical data to find novel insights.
Develop code to implement analysis workflows in a robust and reproducible fashion.
Follow processes to improve transparency and reliability of applications, reducing project risks for on-time milestones.
Educate other scientists, engineers and management on the methods developed and how they apply to the subject domain and customer needs.
Qualifications
This position requires a B.S. (M.S. or Ph.D. preferred) with 3 + years of experience in computer science, specializing in high-performance/distributed computing, data mining, machine learning, or bioinformatics.
3+ years of software engineering experience with proficiency in Python, Scala, Java or C/C++, programming languages.
Knowledge of distributed compute technologies, such as Spark, Hadoop, map-reduce, MPI, or other parallel computing frameworks is essential.
Strong foundation in data engineering, data science, and/or machine learning, with demonstrated experience applying these technologies at scale on real-world data sets.
Knowledge of database technologies, indexing/partitioning, and SQL.
Experience with cloud computing (AWS preferred).
Experience engineering high volume data and scientific dataflows.
Background of bioinformatics, life sciences, genomics or biology is highly preferred.
Team player with excellent communication skills to effectively collaborate with multiple cross-functional teams of scientists, clinicians, and engineers.
Candidates must have pre-existing US work authorization."
7,Senior Data Engineer,"Novato, CA",Novato,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Who We Are:

2K publishes some of the most popular video game franchises on the planet including Mafia, Borderlands, BioShock, NBA 2K, WWE 2K, Evolve, XCOM, and Sid Meier’s Civilization. The analytics group is responsible for collecting, processing and utilizing the data in the right way to identify problems and opportunities across the company and to build solutions for them. As part of this, the analytics team works with the studios and other partners to build reliable, scalable, and high-performance data pipelines to help inform all aspects of our business. From data designed to improve our development processes to data designed to drive critical business decisions, our group is constantly facing fun and challenging problems in the big data space.


What We Need:

We are looking for a Data Engineer to be part of our growing data engineering team within our analytics group. Collaborate with cross-functional teams, studios, external data providers to architect, design and develop a metadata driven data pipeline framework focusing on reusability, scalability, and productivity. S/he will work with data analysts and data scientists to understand data requirements, design and develop data pipelines to ingest data from multiple disparate sources. It’s a once in a lifetime opportunity to be part of a great team chartered to define the future of data and analytics platform for 2k. This position will be reporting directly to the Director of Data Engineering, located at our Novato office.


What You Will Do:
Actively contribute to the architecture and design of data platform and data engineering practice
Design and develop a data pipeline framework for ingesting structured and unstructured data
Work with the game, marketing, and analytics teams to gather requirements, build, test, and deploy new data pipelines based on business requirements
Translate data and BI requirements into technical design documents and data mapping documents
Mentor junior engineers and be part of their career growth


Who We Think Will Be a Great Fit:
If you have experience building large scale data solutions, are interested in taking up another challenge to shape the future of data and platform capabilities in a fast-paced environment, we think you will be a great fit and we’d love to hear from you!

5+ Years of experience in developing near real-time data pipelines
Strong hands-on experience with object-oriented/object function scripting languages: Python[Preferred], Java, Scala, etc.
Expert knowledge in Data warehouse concepts and implementation of Dimensional and star models
Experience with data pipeline and workflow management tools: Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, Kinesis, RDS
Prior Implementation Experience with stream-processing systems: Storm, Kafka, Spark-Streaming etc.
Proficient using Source Control, build and deploy tools like perforce/Git and Jenkins
Strong project management, organizational and interpersonal skills"
8,Big Data Engineer,"San Francisco Bay Area, CA",San Francisco Bay Area,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Frontend Arts brings together the brightest minds to create breakthrough technology solutions, helping our customers gain competitive advantage. At Frontend Arts, we are continuously evolving how we work and how we look at the business challenges, so we can continue to deliver measurable, sustainable solutions to our clients.
We are looking for self-motivated ""Big Data Engineer"" with excellent communication and customer service skills
Data Engineer.
Python.
Hadoop Stack
Data Pipeline Using ETL or other tool
Handling High Volume Data
AWS ( S3 or lambdas)
Minimum Experience: 7 Yrs
Education:
Bachelor's Degree in Engineering"
9,Senior Data Engineer,"San Francisco, CA 94104",San Francisco,CA,94104,None Found,None Found,None Found,"
Work with analysts and data scientists to define processes and standards to inform system design, transform their needs into streaming or batch processing
Design infrastructure and systems to scale easily as data ingest grows
Implement new data pipeline features with verified high quality from unit test coverage and production monitoring
Focus on data quality! Detect data/analytics quality issues and implement bug fixes and data validation for prevention
Help understand our day to day operations for continuous improvement of production systems",None Found,None Found,"As a Senior Data Engineer on the Analytics Systems team, you’ll work with a talented team of engineers to improve Credit Karma’s data pipeline that powers our recommendation systems, enterprise tools, and data warehousing. You’ll help to build a general, secure, scalable, fast, and high throughput data pipeline to process many terabytes of data a day.

We are very passionate about performance, correctness, and data quality. The team spends their time day to day developing new pipelines or features that make it seamless for data to be moved throughout our infrastructure.This includes working with cutting edge tools such as Scala, Kafka, Spark, Akka and Google Cloud. If you enjoy working in a collaborative environment where everyone can have their say while still being able to set and hit deadlines, then this is the role for you.
Responsibilities
Work with analysts and data scientists to define processes and standards to inform system design, transform their needs into streaming or batch processing
Design infrastructure and systems to scale easily as data ingest grows
Implement new data pipeline features with verified high quality from unit test coverage and production monitoring
Focus on data quality! Detect data/analytics quality issues and implement bug fixes and data validation for prevention
Help understand our day to day operations for continuous improvement of production systems
Our Ideal Candidate
5+ years experience with Big Data technologies
Experience with Scala/Java, Spark, Kafka, or demonstrated ability to pick up new technology quickly
Fundamental knowledge about databases and strong SQL skills
Familiar with Google Cloud ecosystem such as BigQuery, GCS, DataProc, etc
Enjoys working collaboratively; CK’s values include empathy and helpfulness
Able to estimate and meet deadlines
Excellent verbal and written communication skills
Nice to have
Experience working with cloud technologies
Experience scaling data throughput or building low latency streaming pipelines
Experience solving for data quality
Credit Karma is committed to a diverse and inclusive work environment. We believe that such an environment advances long-term professional growth, creates a robust business, and supports our mission of championing financial progress for everyone. We offer generous benefits and perks with an eye single to fostering an inclusive environment that recognizes the contributions of all. We’ve worked hard to build an intensely collaborative and creative environment, a diverse and inclusive employee culture, and the opportunity for professional growth. As part of the Credit Karma team, your voice will be heard, your contributions will matter, and your unique background and experiences will be celebrated.

Credit Karma is also proud to be an Equal Opportunity Employer. We welcome all candidates without regard to race, color, religion, age, sex (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity or expression, marital status, national origin, disability, genetic information, status as a protected veteran, or any other protected characteristic. We prohibit discrimination of any kind and will also consider qualified applicants with arrest and conviction records in a manner consistent with applicable federal, state, and local law.

Our people are everything, our core values are real, and our guiding mission is strong. Join us!"
10,BIG DATA ENGINEER,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for amazing people diverse in thought, perspective, and culture to join our team. We check our egos at the door, roll up our sleeves, work hard, move fast, and support each other. If that sounds like fun to you, please apply!

DATA ENGINEER, BIG DATA

We are looking for engineers who are excited about building distributed data pipelines. We want you to help us shape our internal and external brand-new data warehouses, leveraging the latest advances of big data processing: a combination of Kafka Streams, Hadoop, and traditional RDBMS.

YOU WILL
Be a part of our local San Francisco (Financial District) big data team and be a part of an overall Data Organization spanning multiple offices.
Maintain and incrementally improve existing solutions.
Get to build brand new pipelines with the technology stack including Spark, Spark Structured Streaming, Kafka, Hadoop, MySql, Python.
Work with senior engineers who can help you learn and grow with the focus on Big Data technologies in a small dynamic team
WHAT WE ARE LOOKING FOR
Understanding of distributed system fundamentals.
Experience in developing data pipelines that are used in production environments.
Demonstrated professional experience working with various components of Big Data ecosystem: Spark/Spark Streaming, Hive, Kafka/KSQL, Hadoop (or similar NoSQL ecosystem), et. al, in a production system.
Strong software engineering skills with Python.
Knowledge of some flavor of SQL (MySQL, Oracle, Hive, Impala), including the fundamentals of data modeling and performance.
EVEN BETTER IF YOU HAVE
Skills in real-time streaming applications.
Knowledge of Scala.
A development workflow using Docker containers.
Compulsion for automating your day-to-day processes.
WHAT WE OFFER
Ability to see your direct impact on a high visibility project.
An opportunity to both create new projects and help improve the existing big data pipelines.
Work in a mature, private, nationally-known company with a CEO approval of 85% on Glassdoor and a positive atmosphere in our San Francisco tech team.
401K plan with employer matching.
Commuter pre-tax contributions.
Flexible working hours and work-from-home days.
Health plan.
In-office snacks.
Organized team events.
Centro is an Equal Opportunity Employer. We respect and support an inclusive workplace diverse in thought, perspective and culture. We celebrate all team members regardless of gender/identity, sexual orientation, race or cultural background, religion, physical disability and age. We are better together."
11,Data Science Engineer,"San Francisco, CA 94102",San Francisco,CA,94102,None Found,None Found,None Found,None Found,None Found,None Found,"Join our team @ Eskalera! A super{set} venture studio company
DATA SCIENCE ENGINEER
LOCATION: San Francisco or New York City
At Eskalera, our ultimate vision is an environment where every worker, regardless of background, pedigree, race, geography etc, gets a fair shot, and where AI and data-driven methods measurably improve results for progressive businesses that recognize people and talent as their most critical asset. The key to our and our clients’ success is rooted in constructing inclusive cultures.
About the Role
So, you can query large amounts of data using your favorite big-data toolkits, analyze it using established statistical and machine learning techniques, and communicate the derived insights through clear and concise charts and reports. You are a technologist at heart always seeking to push the current boundaries to process more data and run more sophisticated machine learning algorithms. In your past, you may have doubled as a data scientist, a data engineer, or perhaps a machine learning engineer, but deep inside, all you really care about is building best in class products, applications, and systems that extract knowledge from data at any scale and deliver value to the business.
Great, we are excited to talk to you! We're looking for people who get things done by using their smarts and whatever tools get the job done. Are you at the beginning of your career and this is where you see yourself in the future? Let us know; we love to work with bright people looking to grow.
Your Role:
R&D of cutting edge algorithmic solutions to real-world problems producing a shippable product as well as intellectual property (papers and patents).
Using statistical and machine learning principles to discover hidden patterns, perform predictive analysis and build models that drive insights.
Clean, transform and validate data for uniformity and accuracy.
Devise and utilize algorithms and statistical approaches to mine data stores, perform data analysis and improve model performance.
Communicate findings internally and externally, generating reports and dashboards, building narratives that resonate with clients and stakeholders.
Scale efforts to democratize data internally and externally, be an ambassador for data-driven culture.
Become and stay an expert in current and emerging technologies, techniques, and tools.
Your Skills and Qualities:*
3+ years of professional data science related work
Understanding of key machine learning and data mining approaches.
A bayesian at heart but can report significance if asked.
Understand the fundamentals of computer science including programming principles, design patterns, database fundamentals, and distributed systems.
Make things work and get things done using the programming language of your choice (Python/Scala among others).
Are a great communicator, able to articulate complex concepts in easy to understand language.
Love to learn new things and can do so quickly.
Like working in, and being part of, interdisciplinary teams.
About Eskalera
Eskalera enables large and medium-sized companies to transform their HR operations by improving employee engagement, productivity, and growth. Our end-to-end platform arms HR professionals with the most modern applications of AI, data science, and evidence-based findings on implicit bias and D&I. By capturing, processing, and analyzing data from easy-to-use experiences and integrating other available employee data, companies gain a real-time view of the zeitgeist of their employee base to drive measurable business results.
Eskalera is proud to be an equal opportunity workplace. Individuals seeking employment at Eskalera are considered without regard to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, or sexual orientation.
We do not accept resumes from headhunters, placement agencies, or other suppliers that have not signed a formal agreement with us.
This is Eskalera: https://eskalera.com/about-us/
How we are different?: https://eskalera.com/solutions/
Job Type: Full-time"
12,Senior Data Engineer,"San Francisco, CA 94103",San Francisco,CA,94103,None Found,None Found,None Found,"
Build & maintain low-latency, high-scalability data pipelines in service of our human-in-the-loop machine learning workflows platform.
Build & maintain adapter services for ingesting data from a wide variety of streaming and batch-based sources.
Build & maintain services for throttling, backpressure, schema management, and normalization.
Implement QA and testing strategies. Promote best practices for writing maintainable code.
Participate in selecting tools and setting development standards at Figure Eight.
Ability/readiness to develop excellent working relationships with a diverse team of peers across organizations (Engineering, QA, DevOps, Product, Design, et al).
",None Found,None Found,"About Us

Figure Eight is the essential Human-in-the-Loop Machine Learning platform for data science and machine learning teams. The Figure Eight platform transforms unstructured text, image, audio, and video data into customized high-quality training data to make AI work in the real world. Figure Eight's technology and expertise supports a wide range of use cases including autonomous vehicles, intelligent personal assistants, medical image labeling, consumer product identification, content categorization, customer support ticket classification, social data insight, CRM data enrichment, product categorization, and search relevance.

Headquartered in San Francisco and backed by Canvas Ventures, Trinity Ventures, Industry Ventures, Microsoft Ventures, and Salesforce Ventures, Figure Eight serves Fortune 500 and fast-growing data-driven organizations across a wide variety of industries. For more information, visit www.figure-eight.com.

About the Role

Figure Eight users range from engineers and data scientists to subject matter experts creating training data for machine learning. You will be working on the most important problems in technology today: how can humans and AI collaborate to solve important and sometimes complicated tasks?

As a member of our core team, you will design, build, and improve on tools used by many of the most widely-known tech companies with large-scale machine learning initiatives active today. This may include collecting and managing training data for AI models, evaluating the performance of the machine learning models used by that data, or building infrastructure and managing data pipelines. Specifically, you will work on a generalized annotation API that consists of both automated and human-driven annotation tools for 2D and 3D images, video, text, and audio data. The platform will combine human input (eg: bounding boxes on objects) and Machine Learning input (eg: automatic object tracking in videos) for maximum efficiency and effectiveness. You will be on a cross-functional team collaborating with members of the Product, Machine Learning, Dev Ops, and Backend Engineering teams.

Your work will consist of implementing new features and services, maintaining infrastructure, and migrating existing services to a SOA/microservice-based architecture. You’ll mentor less experienced developers and constantly work on improving your own skills and the quality of our code-base. For more about what we build, please visit www.figure-eight.com/overview

The Ideal Candidate:
You enjoy thinking about and working on enterprise-level data management systems. You are looking for a company at the epicenter of a rapidly-developing machine learning industry and are driven by a hunger to learn and develop your skills. You are passionate about working on a project that contributes meaningfully to the further development of technology and to humanity as a whole. You care about best practices and you choose the tools you work with judiciously and deliberately. You have strong analytical skills, an unwavering commitment to quality, an open-minded and collaborative work ethic, and cutting-edge coding skills.

Responsibilities / Opportunities:

Build & maintain low-latency, high-scalability data pipelines in service of our human-in-the-loop machine learning workflows platform.
Build & maintain adapter services for ingesting data from a wide variety of streaming and batch-based sources.
Build & maintain services for throttling, backpressure, schema management, and normalization.
Implement QA and testing strategies. Promote best practices for writing maintainable code.
Participate in selecting tools and setting development standards at Figure Eight.
Ability/readiness to develop excellent working relationships with a diverse team of peers across organizations (Engineering, QA, DevOps, Product, Design, et al).

Competencies:

5+ years of software development experience in cloud-based, multi-tiered, enterprise application systems.
5+ years managing data platforms/engineering using enterprise service bus or message-based architectures, such as Kafka, Redis, RabbitMQ, or similar.
3+ years production environment-level experience with Ruby on Rails application development.
Hands-on experience with developing microservices and successfully building products using SOA.
Hands-on experience with event-sourcing and functional programming patterns.
Hands-on experience with AWS, Git, Docker, Gradle, Jenkins, Jira, and Confluence.

Nice-to-have Competencies:

Familiarity with batch processing and workflow tools such as Airflow, Luigi, Celery, or others
Prior production experience with Python, Java, and/or Scala.
Familiarity with basic machine learning concepts.

Figure Eight offers an attractive total compensation package including outstanding benefits and stock options. Learn more about our culture at https://www.figure-eight.com/company/careers/."
13,Business Intelligence Engineer Manager,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Womply is leveling the playing field for American small businesses with proprietary data and next-generation SaaS that helps SMBs get more customers, keep them coming back, and save time every day. We’re one of the fastest growing software companies in the country, serving over 100,000 small business merchants across 400+ business verticals in every corner of America.

The Data Team at Womply advances the state of our data, and empowers the company to make better decisions from our data. We’re seeking a talented and motivated Data Engineer to lead our team. As the Business Intelligence Engineer Manager, you will hold the keys to the infrastructure that powers our current and future Data Products. Your team of data engineers are helping us build and leverage the latest technologies to tap into our firehose of data to open up new paths for analysis and discovery. You will have the opportunity to make a big impact, and work with extremely talented peers on a fast paced, high energy team.

You will lead your team to deliver large-scale projects, set and drive roadmap execution through resource planning and allocation. Your focus is to help us evolve our data-driven philosophy and become a world-class data organization. Your team owns the design, execution, and ongoing support of critical data warehousing projects enabling accurate reporting and advanced analytics for all of Womply’s internal business units.

You will have to be self-sufficient - we are a startup, so everyone might do a bit of everything to get things done. We look for people who take pride in their work, execute on it, and deliver phenomenal results.

In order to be successful in this role, you will be responsible for:

Vision - Your and your team will leverage our data foundation to design and implement innovative solutions to our hardest data problems
Execution - Building and maintaining the data pipelines from various data sources, while maintaining high accuracy, consistency, and reliability
Feedback & Development - You see people’s strengths and weaknesses and are both gifted and courageous in talking with them about it; you motivate and train your team to be the best version of themselves
Partnership - You establish strong relationships with business and technical leaders across the organization
You must have:

Must have ""Hands-On"" coding experience as a Lead or Manager
3-8 years in software engineering
Experience with Data Warehousing, Architecting Pipelines, and Data Modeling
Team-oriented, self-motivated, success-driven, roll-up-your-sleeves attitude
Strong intellectual curiosity and demonstrated ability to understand and question the data
Healthy Skepticism to challenge the status quo so we can improve
Technically proficient in:
Languages - Python / Scala / SQL / Bash
Technologies - Snowflake, AWS, Airflow
Nice to Have:

Spark
Come build something amazing at Womply

We’re a fanatically values-based company with $50 million raised to accelerate our growth. We work hard and push each other to be the best, but we also have fun and don’t take ourselves too seriously. If you want to win and make a big impact, let’s talk. We’re hiring in San Francisco and Lehi, Utah for engineering, DevOps, design, data science, sales, marketing, business development, account management, and more.

PLEASE NOTE - Direct applicants ONLY. Any recruiter/3rd party submissions we receive will be considered a gift."
14,Data Engineer - Data Modeling Platform,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,None Found,None Found,None Found,None Found,None Found,"At Uber, we ignite opportunity by setting the world in motion. We take on big problems to help drivers, riders, delivery partners, and eaters get moving in more than 600 cities around the world.

We welcome people from all backgrounds who seek the opportunity to help build a future where everyone and everything can move independently. If you have the curiosity, passion, and collaborative spirit, work with us, and let’s move the world forward, together.

The Global Data Warehouse Team (GDW) powers analytics for many of Uber’s businesses. Want to know how many users joined Uber as Riders and subsequently decided to become Drivers on our platform? The Global Data Warehouse team maintains the data objects which answer this question. Need to analyze how the wait times shown in the Rider app correlate with Rider and Driver ratings? We have the data at the ready. We model tables and build data pipelines for the core of our business including Driver, Rider and Trip analytics. We collaborate with teams including Eats, Fraud, Ops, Finance, and Marketing to support domain specific needs. We ingest truly massive volumes of data generated from our globally distributed users and structure this data in an analytics-friendly way while guaranteeing highest fidelity of historical data and low latency - questions at Uber don’t wait for an answer for a very long time.

As a Senior Software Engineer in Data at Uber you will play a leading role in scaling the global data warehouse to power analytics for teams across Uber. You are a self-starter with extensive industrial experience in SQL, Data Modeling, and ETL pipeline design. You have deep experience implementing ETL pipelines in Hive or another MPP database architecture. You are comfortable with Spark and Presto having used one or both frequently to process very large volumes of data. You possess at least a working knowledge on a platform for streaming analytics. You are comfortable coding in Python, Java, or Scala. You have demonstrated strong competency in reliably operating 100s of ETL pipelines with adherence to strict SLAs and quickly root-causing and correcting complex data problems. Peers describe you as the go-to person for the most challenging data ingestion and modeling problems. You actively mentor junior team members and attract others inside and outside your company to join your team. Detail-orientation, thoroughly tested code, and great documentation are the hallmarks of your work but you excel equally well at explaining concepts in “big picture” terms to a less technical audience. If this describes you and you tick off the boxes below, we would love to hear from you.
Required skills:
5+ years expertise creating and evolving dimensional data models & schema designs to structure data for business-relevant analytics.
5+ years hands-on experience using SQL to build and deploy production-quality ETL pipelines.
3+ years experience ingesting and transforming structured and unstructured data from internal and third party sources into dimensional models.
3+ years experience writing and deploying Python, Scala, or Java code.
3+ years hands-on experience using Hadoop, Hive, Vertica or another MPP database system like AWS Redshift or Teradata.
Track record of successful partnerships with product and engineering teams resulting in on-time delivery of impactful data products.
Demonstrated ability to think strategically about business, product, and technical challenges and implement data solutions which scale to meet future needs.
Experience developing scripts and tools to enable faster data consumption.

Preferred skills:
2+ years experience building and operating realtime streaming data pipelines using Spark Streaming, or Flink
In-depth understanding of with Kimball’s data warehouse lifecycle.
Extensive experience with real-time data ingestion and stream processing.
Demonstrated familiarity with industry-leading Big Data ETL practices."
15,Data Engineer,"San Mateo, CA",San Mateo,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Brief Description
Job Advertisement
Data Engineer

Ten-X Commercial is the CRE marketplace that is a force multiplier for sellers, buyers and brokers. Ten-X precision-matches assets, accelerates close rates, and streamlines the entire transaction process with more than $55 billion in sales and increasing daily. Leveraging desktop and mobile technology, Ten-X allows people to safely and easily complete real estate transactions entirely online. We bring quality assets to the market and attract prospective investors from around the world. By virtue of our best-in-class marketing and scalable technology platform, buyers and seller are able to conduct transactions in an efficient manner.
Ten-X empowers consumers, investors and real estate professionals with unprecedented levels of flexibility, control and simplicity – and the convenience of transacting properties whenever and wherever they want. As real estate continues to move online, Ten-X is uniquely positioned at the forefront of this dramatic industry evolution.

https://www.ten-x.com/

The Role:
Data and our ability to leverage it is seen and championed as a key competitive advantage from our CEO on down. We are looking for a top tier data engineer to work with our data science team on building out proprietary tools and models around our customer and asset data (both internal and external sets). You will be working on key projects that have board level visibility.

Responsibilities

Play a leading role in designing, developing and implementing Big Data databases (Hadoop, Graph, MySQL, NoSQL, MongoDB) that contains multiple data sets from both internal and external sources
Lead the setup of data pipelines of new internal and external data sets into the database
Work with Data Scientists to help dedupe and fuzzy match data
Work with software engineers on developing APIs

Experience

Undergraduate degree (ideally a Masters) in a relevant quantitative subject (Math, Statistics, Computer Science, Engineering, Economics, etc.)
5+ Years’ Experience in data engineering, including: 2+ years in a modern data stack environment, specifically the Hadoop stack, 3+ Years' Python experience relating to data engineering
Experience with iterative Agile methodologies and use of supporting tools like JIRA, Confluence and Git
Experience in the following will be a plus:
Spark
Kafka
Clickstream data
Machine Learning
Streaming Data
Elastic Search
Containers (Docker)
Fuzzy Matching / NLP
Ability to understand business problems and translate them into data science requirements
Understanding and Familiarity with:
Hadoop and all the related stack (Pig, Hive, HBase, etc.)
SQL skills and SQL Databases
Strong oral and written communication skills and be able to communicate complex technical knowledge in meaning terms
Ability to work in a fast-paced environment and fluidly adapt to changing priorities
Must be passionate about getting to the root cause of issues and driving to whys
Proven ability to obtain buy-in/ partner with the data science team, including demonstrated ability to partner with functional leaders toward common goals
Well-developed analytical and interpersonal skills with ability to draw conclusions and communicate/present them confidently and effectively to broad audiences, including senior leadership
High energy and passion about solving business needs through data
Organized, structured thinker with ability to handle multiple assignments, remain calm under pressure, and digest information from multiple, disparate parts
Continuous improvement mindset
Not afraid to challenge conventional thinking or analyses

"
16,Senior Data Engineer,"San Francisco, CA 94107",San Francisco,CA,94107,None Found,None Found,None Found,None Found,None Found,None Found,"ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.

As a Senior Data Engineer, here's what we'll be looking for you to bring:
Hands-on Engineering Leadership
Proven track record of Innovation and expertise in Data Engineering
Tenure in coding, architecting and delivering complex projects
Deep understanding and application of modern data processing technology stacks. For example Spark, Kafka, Hadoop, ecosystem technologies, and others
Deep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies
Deep understanding of relational database technologies and database development techniques
Understanding of how to architect solutions for data science and analytics
Data management for reporting and BI experience is a plus
Understanding of “Agility”, including core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Passion for software craftmanship
A rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..
Strong stakeholder management and interaction experience at different levels
Any experience building and leading an offshore/outsourcing function would be highly beneficial.
There's no typical day or engagement for our Senior Engineers. Here’s what you’ll do:

Be the SME. Develop Big Data architectural approach to meet key business objectives and provide end to end development solution
You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that Big Data has to solve their most pressing problems.
On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.
It could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.
Whatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.
You have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.
You recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.
Regardless of what you do at ThoughtWorks, you’ll always have the opportunity to:

Think through hard problems, and work with a team to make them reality.
Learn something new every day.
Work in a dynamic, collaborative, transparent, non-hierarchal, and ego-free culture where your talent is valued over a role title
Travel the world.
Speak at conferences.
Write blogs and books.
Develop your career outside of the confinements of a traditional career path by focusing on what you’re passionate about rather than a predetermined one-size-fits-all plan
Be part of a company with Social and Economic Justice at the heart of its mission.
A few important things to know:
Projects are almost exclusively on customer site, so candidates should be flexible and open to travel.

Candidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.

Not quite ready to apply? Or maybe this isn’t the right role for you? That’s OK, you can stay in touch with AccessThoughtWorks, our learning community (click ""contact me about recruitment opportunities"" to hear about jobs in the future).

It is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment."
17,Big Data Engineer,"San Francisco, CA 94107",San Francisco,CA,94107,None Found,None Found,None Found,None Found,None Found,None Found,"As a member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation & engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You’ll work in a collaborative, trusting, thought-provoking environment—one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.
This role requires a wide variety of strengths and capabilities, including:

BS/BA degree or equivalent experience
Advanced knowledge of application, data and infrastructure architecture disciplines
Understanding of architecture and design across all systems
Working proficiency in developmental toolsets
Knowledge of industry wide technology trends and best practices
Ability to work in large, collaborative teams to achieve organizational goals, and passionate about building an innovative culture
Understanding of software skills such as business analysis, development, maintenance and software improvement
Technical proficiency in Java, Spring, and Spring Boot
Hands on experience with web technologies (e.g. HTTP, XML, REST, HTML, etc.)
Experience with NoSQL databases (Cassandra, MongoDB, etc.).
Experience with distributed streaming platform (Kafka), Data protection, replication, reconciliation, and distribution
Experience with Spark/Spark Streaming (Big Data)
Experience with web-based version control tools (GIT, Bitbucket)
Experience in DevOps - build, deployment, integration, code management and similar tools like Jenkins, Maven, automated deployment etc.
Understanding of design patterns and their application
Good to have Hands on experience developing and deploying applications to cloud platforms namely AWS & Cloud Foundry
Ability to work collaboratively in teams and develop meaningful relationships to achieve common goals
Extensive experience with horizontally scalable and highly available system design and implementation, with focus on performance and resiliency and extensive experience profiling, debugging, and performance tuning complex distributed systems
Excellent logical reasoning and analytical skills
Demonstrated professional writing/communication skills
Strong organization, interpersonal and management skills
Passionate about the digital landscape with desire for continuous learning and development
Team player that is able to work with diverse groups across an organization
Our Consumer & Community Banking Group depends on innovators like you to serve nearly 66 million consumers and over 4 million small businesses, municipalities and non-profits. You’ll support the delivery of award winning tools and services that cover everything from personal and small business banking as well as lending, mortgages, credit cards, payments, auto finance and investment advice. This group is also focused on developing and delivering cutting edged mobile applications, digital experiences and next generation banking technology solutions to better serve our clients and customers.

When you work at JPMorgan Chase & Co., you’re not just working at a global financial institution. You’re an integral part of one of the world’s biggest tech companies. In 15 technology centers worldwide, our team of 50,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $11B annual investment in technology enables us to hire people to create innovative solutions that are transforming the financial services industry.

At JPMorgan Chase & Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you’re looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you."
18,Senior Cloud Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,"
Design and develop framework to automate data ingestion and integration of structured data from a wide variety of enterprise data sources, at scale.
Design and develop data pipeline components and integrate them with the Splunk and other ETL Platforms.
Design data quality monitoring and automated data cleaning.
Assist the business liaison and ETL function with data related issues such as assessing data quality, data consolidation, evaluating existing data sources, etc.
Experience with handling large data infrastructure platform and driving stability through automated monitoring, alerting, and actions.
Experience developing for, configuring, and supporting Cloud computing solutions","
Bachelor degree in Computer Science or related field","
Experience with building scalable and reliable data pipelines using Data engine technologies like APIs, AWS Redshift, Snowflake, Talend.
8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.
4+ years experience designing and developing complex ETL/ELT programs with Python, Visual ETL Tools etc
3+ years experience developing complex SQL
Experience using Cloud Storage and computing technologies such as RedShift, Snowflake
3+ years experience programming in Python
2+ years experience with Bitbucket
2+ years experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues
2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)
Experience with API based integration from multiple SaaS data sources
Experience developing and implementing streaming data ingestion solutions
Experience in Agile methodology (2+ years)","Job Description: That’s a cool job - I want it!
Ready to shake things up? Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and strive to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun, and most significantly to each other’s success. We continue to be on a tear while enjoying incredible growth year over year.
As a Cloud Data Engineer, you should be an expert with data warehousing technical components (e.g., ETL, ELT, Cloud Databases and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data lake solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions.
Responsibilities: I want to and can do that!
Design and develop framework to automate data ingestion and integration of structured data from a wide variety of enterprise data sources, at scale.
Design and develop data pipeline components and integrate them with the Splunk and other ETL Platforms.
Design data quality monitoring and automated data cleaning.
Assist the business liaison and ETL function with data related issues such as assessing data quality, data consolidation, evaluating existing data sources, etc.
Experience with handling large data infrastructure platform and driving stability through automated monitoring, alerting, and actions.
Experience developing for, configuring, and supporting Cloud computing solutions
Requirements: I’ve already done that or have that!
Experience with building scalable and reliable data pipelines using Data engine technologies like APIs, AWS Redshift, Snowflake, Talend.
8+ years of experience with and detailed knowledge of AWS based data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.
4+ years experience designing and developing complex ETL/ELT programs with Python, Visual ETL Tools etc
3+ years experience developing complex SQL
Experience using Cloud Storage and computing technologies such as RedShift, Snowflake
3+ years experience programming in Python
2+ years experience with Bitbucket
2+ years experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues
2+ years implementing and programming data ingestion and ETL programs with large datasets (Terabytes sized analytical environment)
Experience with API based integration from multiple SaaS data sources
Experience developing and implementing streaming data ingestion solutions
Experience in Agile methodology (2+ years)
Education: Got it!
Bachelor degree in Computer Science or related field
We value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which you are applying.
For job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records."
19,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Womply helps small businesses thrive in a digital world. Our software makes it easy for small businesses to boost their online reputations, engage their customers, and monitor the health of their businesses with data and technology they can't get anywhere else. We're one of the fastest growing software companies in the country, serving more than 100,000 small businesses across 400+ business verticals in every corner of America.

In your new role at Womply you will be part of a team of engineers working to manage our ever­-growing collection of payment and other merchant data from across the USA. You'll work primarily with Java and Apache Spark and a variety of data stores including Cassandra, PostgreSQL, and Aurora. You'll work with the rest of the engineering and product teams to design and optimize the schemas needed to support the products. Going forward, you'll support our evaluations of new tools and technologies to scale and analyze our data.

You Must Have:

5+ years of experience in software engineering with experience as a senior contributor or team lead.
MUST HAVE - Good programming skills in Java or Scala.
MUST HAVE - Experience delivering Spark-based data consumption to consumer facing products / systems.
MUST HAVE - Data Platform and Pipeline experience
Experience with Cassandra, Mongo, or similar data stores.
Strong background in SQL, Data Modeling, and Performance Tuning in both relational and noSQL databases.
Experience with distributed and federated systems and data processing pipelines
Familiarity with monitoring, backup, and disaster recovery of data systems
Experience building POCs, architecting new systems and improving existing systems to solve business problems and support scaling

Nice to have:

AWS experience
Database Administration Experience.
Experience with Python / Pyspark
Experience generating and evaluating data quality metrics
Experience mentoring engineers in best practices and methods.
Experience with PCI data practices

Come build something amazing at Womply

We're a fanatically values-based company with $50 million raised to accelerate our growth. We work hard and push each other to be the best, but we also have fun and don't take ourselves too seriously. If you want to win and make a big impact, let's talk. We're hiring in San Francisco and Lehi, Utah for engineering, DevOps, design, data science, sales, marketing, business development, account management, and more.

PLEASE NOTE - Direct applicants ONLY. Any recruiter/3rd party submissions we receive will be considered a gift.

More:
Work at Womply ( https://womply.com/jobs/ )

Life at Womply ( https://womply.com/life-at-womply/ )

How we work ( https://womply.com/how-we-work )

Our values ( https://womply.com/values/ )

Benefits ( https://womply.com/benefits )

Diversity ( http://womply.com/diversity/ )"
20,"Senior Data Engineer, Batch Recommendation Systems","San Francisco, CA 94104",San Francisco,CA,94104,None Found,None Found,None Found,None Found,None Found,None Found,"Credit Karma is a mission-driven company, focused on championing financial progress for our more than 100 million members in the U.S., Canada and U.K. While we're best known for pioneering free credit scores, our members turn to us for everything related to their financial goals, including helping them with improving their credit, identity monitoring, applying for credit cards, and shopping for loans (car, home and personal) - all for free. Credit Karma has grown significantly through the years: we've added more than 70 million members in the last five years alone and now have more than 1,000 employees across our offices in San Francisco, Charlotte, Los Angeles, Leeds, London and soon Oakland.

Our mission is to show the most appropriate notification at the most appropriate time for all of our 100MM+ members. This includes powering the financial products marketplace and data driven personalized content. We do this by using several different recommendation strategies, historical member behavior and understanding customer financial goals. Apart from the machine learning challenges on rich data sets, we also deal with core engineering problems of delivering high throughput data pipeline to process all Credit Karma members everyday, to provide a personalized experience.
What will you do?
You will work closely with marketing and data science team to provide solutions for various product requirements and excellent personalization experience for our Credit Karma members
You will also engage heavily in design and discovery to ensure excellent stakeholder experience.
Design infrastructure and systems to scale easily as data ingest grows
Take a metrics first approach when creating new systems
Implement new data pipeline features with verified high quality from unit test coverage and production monitoring
Help understand our day to day operations for continuous improvement of production systems
Do you have the skills needed for success?
5+ years experience with Big Data technologies
Experience with Scala/Java, Spark, Apache Beam, Kafka, or demonstrated ability to pick up new technology quickly
Fundamental knowledge about databases and strong SQL skills
Familiar with Google Cloud ecosystem such as BigQuery, GCS, DataFlow, etc
Enjoys working collaboratively; CK’s values include empathy and helpfulness
Able to estimate and meet deadlines
Excellent verbal and written communication skills

If you are ready to make an impact on a product that is used by millions of people around the world, including your own friends and family, join us.
Equal Employment Opportunity

Credit Karma is committed to a diverse and inclusive work environment. We believe that such an environment advances long-term professional growth, creates a robust business, and supports our mission of championing financial progress for everyone. We offer generous benefits and perks with a single eye to nourishing an inclusive environment that recognizes the contributions of all and fosters diversity by supporting our internal Employee Resource Groups. We’ve worked hard to build an intensely collaborative and creative environment, a diverse and inclusive employee culture, and the opportunity for professional growth. As part of the Credit Karma team, your voice will be heard, your contributions will matter, and your unique background and experiences will be celebrated.

Credit Karma is also proud to be an Equal Opportunity Employer. We welcome all candidates without regard to race, color, religion, age, marital status, sex (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity or gender expression, national origin, veteran or military status, disability (physical or mental), genetic information, or any other protected characteristic. We prohibit discrimination of any kind and operate in compliance with the San Francisco Fair Chance Ordinance.

Learn more about Credit Karma at creditkarma.com/careers
#LI-AG1"
21,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"goPuff is a high-growth startup that is revolutionizing hyper-local e-commerce. We warehouse and deliver thousands of products to your door super fast. We're currently operating in over 70 markets within the US and expanding quickly.

Love data? Love to build stuff? As a Data Engineer at goPuff, you’ll provide hands-on expertise in building the foundation for data-driven decision-making across the company. This role sits within our full-stack data & analytics team that is designed to move fast. You’ll have the opportunity to make a huge impact as the future success of goPuff will largely depend on the business and customer insights we derive from our data.

Responsibilities


Work on data processing and data warehouse projects for the entire organization ( Marketing, Operations, Merchandising, Supply Chain, Finance, and Business Strategy)
Design/code event, data and ML pipeline features, build data tables and data tools that enable robust business/customer analytics, KPI monitoring, operational insights, predictive analytics and personalization
Collaborate with data analysts and other functions to gain a deep understanding of how data impacts decision-making
Be the expert in how data flows throughout the business
Identify, design, and implement internal data process improvements including automating manual processes, optimizing data delivery and re-designing infrastructure for greater scalability
Own data availability and integrity

Requirements


Bachelors in Computer Science, Engineering or other quantitative field
3+ years of relevant work experience in data engineering
Experience with object-oriented/object function scripting languages: Python (pandas/numpy), node JS, Java/Scala, etc.
Advanced working SQL knowledge and experience working with a variety of databases (e.g. Postgres, Redshift/Vertica, Hive, Presto)
Extensive experience with ETL pipelines and stream processing architectures, particularly implementing them in a fast-paced startup environment
Experience with data pipeline and workflow management tools: Airflow, Luigi, etc.
Experience with big data and stream-processing technologies: Spark, Kafka, SQS, Kinesis, Spark-Streaming, Flink etc.
Experience with AWS and/or Azure cloud services: EC2, Lambda, SageMaker, EMR, RDS, Redshift, Azure Function apps, HDInsight is a plus
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Experience with BI platforms like Looker and Tableau
Excellent communication and presentation skills
Top notch organizational skills and ability to manage multiple projects in a fast-paced environment
Move fast, be a team player, always be learning and give back

For the people who have better things to do than go out of their way to stop at the store (again), goPuff is the largest digital convenience retailer delivering thousands of products ranging from snacks, drinks, and ice cream to alcohol, home essentials, and personal care items directly from centrally located warehouses to our customers’ doors.

We’re currently in 80+ markets and growing fast, so we're looking for the most motivated and passionate talent to be a part of our team, grow with us, and join in our mission of delivering the moments that matter most. Note: must love snacks to work at goPuff.

The goPuff Fam is committed to an inclusive workplace that does not discriminate against race, nationality, religion, age, marital status, physical or mental disability, sexual orientation, gender, or gender identity. We believe in diversity and encourage any qualified individual to apply. We are an EEOC Employer."
22,Python Engineer,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,None Found,None Found,None Found,None Found,None Found,"We are:

Applied Intelligence, the people who love using data to tell a story. We’re also the world’s largest team of data scientists, data engineers, and experts in machine learning and AI. A great day for us? Solving big problems using the latest tech, serious brain power, and deep knowledge of just about every industry. We believe a mix of data, analytics, automation, and responsible AI can do almost anything—spark digital metamorphoses, widen the range of what humans can do, and breathe life into smart products and services. Want to join our crew of sharp analytical minds? Visit us here to find out more about Applied Intelligence.

You are:

A data expert with serious analytical and statistical chops. You know how to take massive amounts of data and find the insights our clients need to help their companies do more.

The work:

Solve complex analytic challenges using analytic algorithms and AI
Design, build and deploy predictive and prescriptive models using statistical modeling, machine learning, and optimization
Find ways to turn goals into products that use specific analytic tools to help clients
Use structured decision-making to complete projects.
Work out the best way to complete Enterprise Data & Analytics projects

Here’s what you need:
Strong quantitative and analytical skills with minimum of 2 years of experience in data science tools, including Python, R, Scala, Julia, or SAS
At least 2 years of implementing and delivering projects using CI/CD rigor and tools such as git, Jenkins, docker, Kubernetes, Kubeflow Pipelines, etc.
At least 2 years of experience in data science and use of statistical methodologies
At least 2 years of experience in machine learning methods, including familiarity with techniques in clustering, regression, optimization, recommendation, neural networks, and other
Bonus points if:

At least 1 year of designing and implementing data engineering, ingestion and curation functions on GCP cloud using GCP native or custom programming
Google Cloud Platform data engineer certification a big plus
At least 1 year of experience in architecting large-scale data solutions, performing architectural assessments, crafting architectural options and analysis, finalizing preferred solution alternative working with IT and business stakeholders
Bachelor's degree in data science or related disciplines such as mathematics, statistics, computer science, physics, or related fields. Master's degree preferred


Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).


Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.


Accenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.


Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Accenture is committed to providing veteran employment opportunities to our service men and women."
23,Business Intelligence - Data Analyst,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,"
BA/BS in a quantitative field
1-2 years of work experience as a data analyst, data engineer, or in a highly analytical role
Proficiency in writing SQL queries and using a BI tool
Experience using the command line and git
Strong grasp of statistics and experience conducting rigorous data analyses
Experience with a scripting language (preferably Python) for data processing and analysis a plus
Experience developing models and visualizations in Looker a plus
Experience at an e-commerce or fintech company a plus
Proficiency in Excel and and a strong familiarity with advanced functions
",None Found,"Business Intelligence-Data Analyst

Who is Credible?

We believe life's changes create financial needs for people and that the traditional financial system often puts up unnecessary obstacles. People celebrate major milestones like going to college, getting married, and buying a home. And most of the time, these milestones come with financial implications.

At Credible, we have built a company with the mission of bringing transparency, choice, simple processes and savings to accessing credit for life's important moments. What you see is what you get. We are committed to being upfront, honest, and clear about your options. There are no mysteries, no hidden fees, and no secret clauses.

Credible is a fast-growing Australian Securities Exchange (ASX) listed Fintech company that has world class management, has raised multiple rounds of funding, is generating significant revenue and is disrupting the lending market and helping people save money and get out of debt faster.

About the role

Our Business Intelligence team is looking for a Business Intelligence Analyst who is passionate about data, analytics, and business strategy. You will help the team learn more about our business, teach others in the company about analytics, and improve the use of our data. You'll be an integral part of providing data-driven insights that inform significant company decisions.

You Will:

Partner with teams across the organization to understand their analytics needs and create dashboards and reporting that allow them to execute more effectively
Work with business leaders to define key metrics and build reporting to monitor and understand performance along those metrics
Conduct in-depth data analyses that lead to actionable insights, owning the entire process from ideation to execution to presentation of findings to stakeholders
Develop data models in our data warehouse that enable performant, intuitive analysis
Build data pipelines and python-based ETL tools for getting, processing, and delivering data
Become an expert on all aspects of Credible's data and analytics infrastructure
Be the driving force behind the adoption and effective use of our BI tool within every team at Credible

Education and Experience:

BA/BS in a quantitative field
1-2 years of work experience as a data analyst, data engineer, or in a highly analytical role
Proficiency in writing SQL queries and using a BI tool
Experience using the command line and git
Strong grasp of statistics and experience conducting rigorous data analyses
Experience with a scripting language (preferably Python) for data processing and analysis a plus
Experience developing models and visualizations in Looker a plus
Experience at an e-commerce or fintech company a plus
Proficiency in Excel and and a strong familiarity with advanced functions

Education and Experience:

The capacity to juggle multiple priorities effectively within a fast-paced environment is critical
You're a highly motivated self-starter with the ability to work efficiently with minimal supervision.
Anticipate business needs and think with a business owner mindset – think critically about analyses, don't just complete them
Passion for spreading the value of data throughout the company and communicating insights to a broad audience with varying levels of technical expertise

Why work at Credible:
We are a fast moving, fun-loving, seriously smart group of people who really care about impacting the lives of our customers. We empower our employees to make decisions, take risks, drive our business and make changes when we don't get it right. These are our values:


Exceed Customer Expectations: We provide an exceptional experience to each and every customer that compels them to share it with others.
Take Ownership: We are trusted to make decisions that are in the best interests of our customers and our business. We think and act like owners. We care – and that makes all the difference.
Be Curious: We are curious, ask questions, seek to understand and try new things.
Do the Right Thing: We earn trust by being transparent, respectful and honest with each person with whom we interact.
Get Results: Results fuel our excitement and we know how our personal accomplishments tie to the success of the company
Be Bold: We are courageous and take risks that scare us. Our enthusiasm for experimenting is how we will find the next breakthrough.

Our benefits: We offer competitive compensation, generous benefits, free food and a flexible vacation policy.

But mainly, you want to work at Credible because you believe in our mission and want to have a major role in delivering on it! We look forward to getting to know you.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
24,Big Data Developer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Big Data Developer. Scroll down to learn more about the position’s responsibilities and requirements.

We are looking for a Big Data Engineer that will work on collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company.
#WestReferralCampaign
What You’ll Do
Select and integrate any big data tools and frameworks required to provide requested capabilities
Implement ETL process
Monitor performance and advise on any necessary infrastructure changes
Define data retention policies
What You Have
A degree in an associated field and/or other advanced certification along with significant experience
Proficient understanding of distributed computing principles
Management of Hadoop cluster, with all included services
Ability to solve any ongoing issues with operating the cluster
Proficiency with Hadoop v2, MapReduce, HDFS
Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming
Good knowledge of big data querying tools, such as Pig, Hive, and Impala
Experience with Spark
Experience with integration of data from multiple data sources
Experience with NoSQL databases, such as HBase, Cassandra, MongoDB
Knowledge of various ETL techniques and frameworks, such as Flume
Experience with various messaging systems, such as Kafka or RabbitMQ
Experience with big data ML toolkits, such as Mahout, Spark ML, or H2O (if you are going to integrate machine learning in your big data infrastructure)
Good understanding of Lambda Architecture, along with its advantages and drawbacks
Experience with Cloudera/MapR/Hortonworks
We offer
Medical, Dental and Vision Insurance (Subsidized)
Health Savings Account
Flexible Spending Accounts (Healthcare, Dependent Care, Commuter)
Short-Term and Long-Term Disability (Company Provided)
Life and AD&D Insurance (Company Provided)
Employee Assistance Program
Unlimited access to LinkedIn learning solutions
Matched 401(k) Retirement Savings Plan
Paid Time Off
Legal Plan and Identity Theft Protection
Accident Insurance
Employee Discounts
Pet Insurance
EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring"
25,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Womply helps small businesses thrive in a digital world. Our software makes it easy for small businesses to boost their online reputations, engage their customers, and monitor the health of their businesses with data and technology they can’t get anywhere else. We’re one of the fastest growing software companies in the country, serving more than 100,000 small businesses across 400+ business verticals in every corner of America.

In your new role at Womply you will be part of a team of engineers working to manage our ever-growing collection of payment and other merchant data from across the USA. You’ll work primarily with Java and Apache Spark and a variety of data stores including Cassandra, PostgreSQL, and Aurora. You’ll work with the rest of the engineering and product teams to design and optimize the schemas needed to support the products. Going forward, you'll support our evaluations of new tools and technologies to scale and analyze our data.

You Must Have:

5+ years of experience in software engineering with experience as a senior contributor or team lead.
MUST HAVE - Good programming skills in Java or Scala.
MUST HAVE - Experience delivering Spark-based data consumption to consumer facing products / systems.
MUST HAVE - Data Platform and Pipeline experience
Experience with Cassandra, Mongo, or similar data stores.
Strong background in SQL, Data Modeling, and Performance Tuning in both relational and noSQL databases.
Experience with distributed and federated systems and data processing pipelines
Familiarity with monitoring, backup, and disaster recovery of data systems
Experience building POCs, architecting new systems and improving existing systems to solve business problems and support scaling
Nice to have:

AWS experience
Database Administration Experience.
Experience with Python / Pyspark
Experience generating and evaluating data quality metrics
Experience mentoring engineers in best practices and methods.
Experience with PCI data practices
Come build something amazing at Womply

We’re a fanatically values-based company with $50 million raised to accelerate our growth. We work hard and push each other to be the best, but we also have fun and don’t take ourselves too seriously. If you want to win and make a big impact, let’s talk. We’re hiring in San Francisco and Lehi, Utah for engineering, DevOps, design, data science, sales, marketing, business development, account management, and more.

PLEASE NOTE - Direct applicants ONLY. Any recruiter/3rd party submissions we receive will be considered a gift.


More:

Work at Womply

Life at Womply

How we work

Our values

Benefits

Diversity"
26,Senior Data Engineer - Walmart Media Group,"San Bruno, CA",San Bruno,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Walmart Media Group

At Walmart, we enable the connection between supplier brands and retail shoppers at unprecedented scale. As primary stewards of our brand promise, “Save Money. Live Better,” we work alongside some of the most talented people in the world to engage with the more than 150M households who shop with us. This is a unique opportunity to join a small, high-visibility team within the largest company in the world. We believe all digital advertising can be targeted and accountable – and we have Walmart’s sales data to prove it. Walmart Media Group wins when suppliers invest in digital media to drive growth; Walmart and our supplier partners win when your digital expertise helps sell more goods online and offline. Growth in our digital advertising business is key to Walmart’s overall growth strategy.

Walmart Media Group (the digital ad sales arm for Walmart.com, Jet, Hayneedle, Online Grocery, Vudu, etc.) is dedicated to driving measurable outcomes for our suppliers, merchants, stores, GMs, brand advertisers, and agencies. Our full funnel ad solutions leverage Walmart’s in-store and online data, extensive reach, and to provide measurable results for our clientele. With a range of flexible pricing and buying models, including self-service; these solutions help businesses build brand awareness, engage with Walmart consumers, and convert Walmart consumers to shoppers.

RESPONSIBILITIES

Join a local team of analytical, technical, and operational individual contributors that support our internal tools, systems, and infrastructure.
Develop and maintain a data environment leveraging industry standard technologies at Walmart scale.
Manage data-set structure and schemas for hundreds of tables and terabytes of data.
Work together with infrastructure engineers to build an extensible data streaming platform.


MINIMUM QUALIFICATIONS

A. or B.S. degree in Computer Science or other technical field, or equivalent experience
5+ years of experience in a software development / data engineering role
Strong familiarity with cloud and cloud design patterns.
Experience working with data at scale, counting in TBs of data.
Experience working and coalescing data from a variety of data sources in a single environment.
Strong SQL skills and expert knowledge of at least one scripting language (Python, Ruby, etc.). NoSQL experience also considered."
27,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,"
5+ years working as a data engineer
Experience working with data ETL pipelines
Mastery of at least one programming language
Mastery of SQL, performance tuning, and general database skills
Experience leading large data projects through design, implementation, and long term maintenance
","Common Networks was founded on the idea that everyone should have a choice for fast, affordable access to broadband internet. Right now, most homes in the U.S. don't. In fact, 62% of homes live in a monopoly broadband market. High-speed access unlocks all the superpowers on the internet. When it works, it can be a great leveling force across the world, giving everyone access to educational tools, entertainment, immediate translations, or even medical care that they wouldn’t otherwise have.

Common Networks provides suburban neighborhoods with internet using wireless technology. We interconnect homes in a neighborhood, creating a mesh network between homes and our fiber internet sources. A whole community can then have fast and reliable internet service with only a few locations needing fiber access.

Role

As a Senior Data Engineer, you’ll be responsible for designing and building our data platform. You’ll work closely with Product, Business, Data Science, and the rest of the engineering team to build new data-enabled internet features; ensure robust and timely collection of critical metrics; flow data into performant, user-friendly dashboards; and reinforce a culture that prizes decision-making justified by excellent data.
You will:
Design and implement high performance batch and real-time data pipelines
Design and build visualization and analytics tools to empower internal and external customers with actionable insights
Instrument metrics across Common’s wireless mesh network and suite of products
Translate product and business requirements into data models that are maintainable and extensible
Champion data quality, retention, security and privacy within the company. Document and promote best practices for working with Common data
Who you are
You love to code, and you’re excited to work at a place where you spend 90% of your day heads down coding.
You’re not afraid to get your hands dirty. You’re happy to instrument new metrics in an app or debug broken graphs in a dashboard - whatever it takes to make sure we have the data we need to make the best decisions possible.
You’re relentlessly curious. When something breaks, you’re not satisfied with surface-level explanations and proximate causes, you need to know what the underlying issue was and you’re not afraid to dig in and find out for yourself.
You enjoy working closely with others on a cross-functional team, whether it’s teaching engineers ETL best practices or working with business operators to define critical metrics. You thrive in a highly collaborative environment and embrace diversity of thought and experience when thinking through your designs.
You’ve mastered your craft over years of professional data engineering. You know how to tune, tweak and optimize every kind of query, you know how to fully explore a solution space, and you know the value of good monitoring, alerting, and automated tests.
Requirements
5+ years working as a data engineer
Experience working with data ETL pipelines
Mastery of at least one programming language
Mastery of SQL, performance tuning, and general database skills
Experience leading large data projects through design, implementation, and long term maintenance
Nice to have
Experience with Google’s suite of data services (BigQuery, Datalab, Dataproc, Cloud Pub/Sub, etc)
Experience with golang
Experience with linux / the linux networking stack
Experience with visualization platforms (e.g. Tableau) and libraries (e.g. D3.js)
Equal Employment Opportunity

Common Networks is committed to being an equal opportunity employer – we evaluate all employees and job applicants equally, based on merit, competence, and qualifications. We do not discriminate on the basis of race, religion, color, national origin, gender identity, gender expression, sexual orientation, age, marital status, veteran status, disability status, or any other characteristic protected by law."
28,"Senior / Lead / Principal Data Engineer, Machine Learning / Deep Learning (Einstein, Mulesoft)","San Francisco, CA 94105",San Francisco,CA,94105,None Found,None Found,"We run on AWS. We dockerize applications. You should have some notion of how to build, test, and deploy code to run on cloud infrastructure.
Experience with open source tools for information retrieval (e.g. Solr)
Search, Data Scoring/ Ranking expertise
Data visualization
Experience with Deep Learning for NLP
Experience developing in open-source machine-learning libraries such as Apache Mahout or MLLib
Strong understanding of security, including threat propagation and malware analysis
",None Found,None Found,None Found,"Job Category
Products and Technology
Job Details
Data Engineer / Machine Learning / Deep Learning
Teams include: Sales, Service, Marketing, Security, Analytics, Einstein, IoT, Commerce, Mulesoft
Location: US & Canada (Relocation candidates)
The role:

Salesforce is looking for both Senior, Lead and Principal Data / Deep Learning / Machine Learning Engineers with Java, Python, Scala and/or Spark experience, to help us take on one of the world’s most extensive data sets and transform it into amazing products that feel like magic. You will work on cutting-edge AI applications and products. Brainstorming data product ideas with data scientists and engineers to build data products used by hundreds of millions people every day.


A typical day for you might include the following:
Developing data infrastructure that ingest and transforms data from different sources and customers at scale.
Creating machine/deep learning infrastructure that generalizes across hundreds of thousands of Salesforce customers, but is expressive enough to generate high lift.
Partner end-to-end with Product Managers and Data Scientists to understand customer requirements and design prototypes and bring ideas to production
Working with internal product teams to ingest their data and sprinkle machine/deep learning fairy dust on their products.
Participating in meal conversations with your team members about really important topics, such as: Should the cuteness of panda bears be a factor in their survivability? Is love a decision tree or a regression model? How far ahead would society be today if we had 12 fingers instead of 10?

What we care about:
We develop real products. You need to be an expert in coding, including Java and Object-Oriented Programming. We also use Scala and Functional Programming principles.
We prioritize professional industry experience; advanced degrees alone do not replace real world experience.
We have massive scale. You need to have experience in distributed, scalable systems. Consistency / availability tradeoffs are made here. You’ve tinkered with modern data storage, messaging, and processing tools (Kafka, Spark, Hadoop, Cassandra, etc.) and demonstrated experience designing and coding in big-data components such as HBase, DynamoDB, or similar.
We’re a growing, diverse team and we work together on projects. We love to collaborate and help each other, and we want someone to share that ideology.
You have to be a very quick learner - we face new challenges every day, anything that ranges between the operating model of a financial services companies, conversation model for chatbots, tinkering with convolutional and recurrent networks, to how to make Spark work with the S3 file system. No school could prepare you for all of these, so you need to be very quick on your feet.
Self-starter who can see the big picture, and prioritize their work to make the largest impact on the business’ and customer’s vision and requirements
Excellent communication, leadership, and collaboration skills

Preferred Skills: (different teams will care about some of the following over others)
We run on AWS. We dockerize applications. You should have some notion of how to build, test, and deploy code to run on cloud infrastructure.
Experience with open source tools for information retrieval (e.g. Solr)
Search, Data Scoring/ Ranking expertise
Data visualization
Experience with Deep Learning for NLP
Experience developing in open-source machine-learning libraries such as Apache Mahout or MLLib
Strong understanding of security, including threat propagation and malware analysis

Salesforce, the Customer Success Platform and world's #1 CRM, empowers companies to connect with their customers in a whole new way. The company was founded on three disruptive ideas: a new technology model in cloud computing, a pay-as-you-go business model, and a new integrated corporate philanthropy model. These founding principles have taken our company to great heights, including being named one of Forbes’s “World’s Most Innovative Company” ten years in a row and one of Fortune’s “100 Best Companies to Work For” nine years in a row. We are the fastest growing of the top 10 enterprise software companies, and this level of growth equals incredible opportunities to grow a career at Salesforce. Together, with our whole Ohana (Hawaiian for ""family"") made up of our employees, customers, partners and communities, we are working to improve the state of the world.
LI-Y
Posting Statement
Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay fees to any third-party agency or company that does not have a signed agreement with Salesforce.com or Salesforce.org.
Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, Salesforce will consider for employment qualified applicants with arrest and conviction records."
29,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"----------
Who we are
----------

Our Company was founded on the idea that there are patterns in people's behavior that, with the right logic, can be used to predict future outcomes. We are a small but rapidly growing organization that works in partnership with our customers to create solutions that are simply not found anywhere else. We work in groups rather than in structured corporate hierarchies; our culture is creative and entrepreneurial where everyone contributes to company goals in very real way. We are a hardworking group, but we have a lot of fun with what we do and are looking for new people with a similar mindset to join the organization.

----------
What we do
----------

Our proprietary software-as-a-service helps automotive dealerships and sales teams better understand and predict exactly which customers are ready to buy, the reasons why, and the key offers and incentives most likely to close the sale. Its micro-marketing engine then delivers the right message at the right time to those customers, ensuring higher conversion rates and a stronger ROI.

We are looking for talented individuals to join our team in building the core components of our software that at heart of it uses machine learning and data intelligence


You will build large-scale batch and real-time data pipelines with data processing frameworks such as Beam, Google DataFlow, Spark and the Google Cloud Platform.
You will help drive testing and tooling to improve data quality
You will closely work with software engineers, architects, data scientists and stakeholders
Use best practices in continuous integration and delivery.
You will work in agile teams to iterate and deliver on new product features.

-----------
Who you are
-----------


You know how to work with data with distributed systems such as Spark, Beam, Dataflow and Cassandra.
You have experience with one or more higher-level JVM-based data processing frameworks such as Beam, Dataflow, Storm, and Spark
Experience using SQL like abstractions such as BigQuery, Presto or Hive
You are knowledgeable about data modeling, data access, and data storage techniques.
You understand agile software development processes, data-driven development, and reliability
Strong communication skills required to be able effectively communicate with both technical and non-technical teammates and stakeholders
Ability to collaborate in multi-functional agile teams

----------------------
Expected Hours of Work
----------------------

This is a full-time position. Generally, work is performed Monday through Friday, though holidays and weekends may be required.

Travel: This role may require travel to our New York office several times per year.

--------------------------------------------
We believe in equal employment opportunities
--------------------------------------------

The company provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, the company complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

The company expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of the company's employees to perform their job duties may result in disciplinary actions up to and including discharge."
30,Senior Data Engineer,"San Francisco, CA 94107",San Francisco,CA,94107,None Found,"
5+ years experience as a software or data engineer at an enterprise software / data analytics company
5+ years experience with relational databases (PostgreSQL preferred) and SQL, including a strong understanding of database concepts
BS / MS in Computer Science, Mathematics, or an Engineering discipline from a top university
Strong understanding of algorithms, and 3+ years experience in a programming language such as Java or Python
Strong communication skills - both written and verbal
Knowledge of geospatial data is a plus
Knowledge of statistics and/or data science is a plus
Quick learner, and a strong team player","
5+ years experience as a software or data engineer at an enterprise software / data analytics company
5+ years experience with relational databases (PostgreSQL preferred) and SQL, including a strong understanding of database concepts
BS / MS in Computer Science, Mathematics, or an Engineering discipline from a top university
Strong understanding of algorithms, and 3+ years experience in a programming language such as Java or Python
Strong communication skills - both written and verbal
Knowledge of geospatial data is a plus
Knowledge of statistics and/or data science is a plus
Quick learner, and a strong team player","
Develop analytics algorithms using a deep understanding of our raw data, and productize them in conjunction with the InSight application team
Implement customized analytics for clients and collaborate with the Solutions Engineering team
Design and implement components within the StreetLight InSight® data processing pipeline (from raw input files to optimized tables used for analyses)
Contribute to additional projects within the Data Science and Data Engineering team as appropriate",None Found,None Found,"StreetLight Data, the pioneer in Big Data for mobility, is revolutionizing transportation and urban planning to help the world better deploy infrastructure and adapt to new forms of mobility. From legacy systems to ride sharing and bike sharing to autonomous vehicles, our platform powers 1,500+ mobility projects every month for government and private clients, and we're just getting started.

StreetLight Data is seeking a strong data engineer to be part of our growing Engineering team. This team member will work on developing new analytics algorithms for our customers, and productizing them into the StreetLight InSight® platform. This position initially reports to the VP of Engineering.
Key Responsibilities:
Develop analytics algorithms using a deep understanding of our raw data, and productize them in conjunction with the InSight application team
Implement customized analytics for clients and collaborate with the Solutions Engineering team
Design and implement components within the StreetLight InSight® data processing pipeline (from raw input files to optimized tables used for analyses)
Contribute to additional projects within the Data Science and Data Engineering team as appropriate
Skills & Qualifications:
5+ years experience as a software or data engineer at an enterprise software / data analytics company
5+ years experience with relational databases (PostgreSQL preferred) and SQL, including a strong understanding of database concepts
BS / MS in Computer Science, Mathematics, or an Engineering discipline from a top university
Strong understanding of algorithms, and 3+ years experience in a programming language such as Java or Python
Strong communication skills - both written and verbal
Knowledge of geospatial data is a plus
Knowledge of statistics and/or data science is a plus
Quick learner, and a strong team player"
31,Data Engineer,"Redwood City, CA 94065",Redwood City,CA,94065,None Found,"2+ years of industry experience in a Data Engineer role.
A Bachelor degree in a quantitative field, such as Computer Science, Applied Mathematics, or Statistics, or equivalent professional experience.
Working experience with SQL, Python (3.x) and Scala is a plus.
Working experience on an ETL system.
Strong communication skills, both written and oral, and an ability to convey complex results in a clear manner.
",None Found,"Work with the team to manage the data warehouse and ETL for all of Perfect World Entertainment products.
Design, build and launch new data models in production.
Interface with engineers from other products to ensure proper data collection.
Implement new requests from product managers and data analysts to fulfill their data needs.
Ensure data quality by implementing data detection mechanisms.
Support existing processes running in production and optimize it when possible.
",None Found,None Found,"Join our Data Engineering team and help build a scalable real-time analytics platform that processes streaming data to make our product even more intelligent! Own and extend our data pipeline, perform data modeling, and improve data reliability and quality. Become part of a team focused on creating innovative real-time analytics and machine learning feedback loops.

Responsibilities
Work with the team to manage the data warehouse and ETL for all of Perfect World Entertainment products.
Design, build and launch new data models in production.
Interface with engineers from other products to ensure proper data collection.
Implement new requests from product managers and data analysts to fulfill their data needs.
Ensure data quality by implementing data detection mechanisms.
Support existing processes running in production and optimize it when possible.



Required Qualifications
2+ years of industry experience in a Data Engineer role.
A Bachelor degree in a quantitative field, such as Computer Science, Applied Mathematics, or Statistics, or equivalent professional experience.
Working experience with SQL, Python (3.x) and Scala is a plus.
Working experience on an ETL system.
Strong communication skills, both written and oral, and an ability to convey complex results in a clear manner.

Desired Qualifications
Working experience with Machine Learning and predictive analytics.
Familiarity with Hadoop framework.
Experience with Spark is highly desirable.
Familiar with data visualization through Tableau, or other tools."
32,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About us:
Want to infuse a $25B sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? States Title is intelligently transforming closings by applying machine intelligence to the age-old processes and procedures in the $25B Title and Settlement industry. Our streamlined, efficient algorithms have revolutionized the title and escrow process and allowed us to scale rapidly. We are poised to transform this industry, repurposing the billions wasted in rote, manual tasks to make homeownership easier and less risky, helping people invest time and money into more meaningful parts of their lives.

You're fired up to:

Design, build, and maintain data pipelines for extraction, transformation, and loading of data from a wide variety of data sources to various data services.
Build a next-generation enterprise data lake with raw production data as source of truth and always-on, versioned data pipelines.
Document data sources, data pipelines, and data infrastructure to share knowledge and understanding of the solutions being implemented.
Work with teams across the organization to assist with data-related technical issues and support their data infrastructure needs.

You definitely have:

Experience with scripting languages, particularly Python.
Experience with cloud services, particularly Azure: Blob Storage, VMs, Data Factory, SQL Data Warehouse, HDInsight, etc.
Ability to write complex SQL joins in your sleep and experience with stored procedures, triggers, etc.
Experience with the command line for Linux systems (also preferably Windows).
Knowledge of database modeling and data warehousing concepts.

You might even have:

Experience with big data platforms and tools like Spark, Hive, Presto, Kafka, etc.
Experience with compiled languages like Java, Scala, C#, etc.
Familiarity with ML concepts (supervised learning, feature engineering, etc) and experience with ML frameworks (Scikit-learn, TensorFlow, SparkML, etc).
Familiarity with data lake architecture.
Knowledge of DevOps practices and experience with related tooling (containers, infrastructure-as-code, observability, etc).

We want the work you do here to be the best work of your life.

We believe the most valuable investment we can make - and the greatest boost we can give to your career - is to build an outstanding team of colleagues who are passionate about our mission.

We currently offer the following benefits and will continually evolve them with the goal of efficiently attracting, retaining, and leveraging the very highest quality talent.


Our passionate, capable team will always be our #1 benefit
Learn something new every day
Get more done than you would anywhere else
Highly competitive salaries and stock option grants
Health, dental, and vision benefits for you and your family
Flexible work hours
Unlimited vacation policy
A modern, helpful 401(k) plan
Wellness and commuter benefits

"
33,Senior Data Engineer,"Menlo Park, CA 94025",Menlo Park,CA,94025,None Found,None Found,"MS in Computer Sciences
5+ years of experience in a relevant software engineering position
5+ years hands-on experience with Java/J2EE, Relational databases and NoSQL data stores
In-depth experience building scalable, distributed Analytical Systems
Proficient with Predictive Analytics, AI and Machine Learning
Experience using AWS or GCP
Comfortable with Linux environments
","
Design and develop a highly performant and highly scalable analytical system to collect, store, process and analyze genomic and food sample metadata, utilizing modern technologies.
Use GCP's features and apply AI/ML functionality to enhance our analytical capabilities.
Utilize DDD, Event Sourcing and CQRS to build Microservices and API's for the analytical system.
Suggest, assess and translate system requirements into implementation designs and data models.
Ensure excellent quality and test coverage, as well as effective performance.
Work on timely resolution of issues and other tasks relevant to the position.
",None Found,"
Design and develop a highly performant and highly scalable analytical system to collect, store, process and analyze genomic and food sample metadata, utilizing modern technologies.
Use GCP's features and apply AI/ML functionality to enhance our analytical capabilities.
Utilize DDD, Event Sourcing and CQRS to build Microservices and API's for the analytical system.
Suggest, assess and translate system requirements into implementation designs and data models.
Ensure excellent quality and test coverage, as well as effective performance.
Work on timely resolution of issues and other tasks relevant to the position.
","The Senior Data Engineer, focused on Software and Analytics, will report to the VP of Engineering and be part of a team responsible for the development of our analytical platform. You will help drive the architecture and implementation of a robust, highly performant, and scalable analytical system. The role will require close collaboration with our Bioinformatics team, and involves building data pipelines and relevant backend services. This is a hands-on position, providing the opportunity to build an intelligent, next generation genetic processing and analytical platform that will reimagine how Food Safety is done across the industry!

We offer great opportunities to grow your career! Be part of an exiting, fast growing startup in the biotech sector! You can learn more about us at https://www.clearlabs.com/
Requirements
Responsibilities
Design and develop a highly performant and highly scalable analytical system to collect, store, process and analyze genomic and food sample metadata, utilizing modern technologies.
Use GCP's features and apply AI/ML functionality to enhance our analytical capabilities.
Utilize DDD, Event Sourcing and CQRS to build Microservices and API's for the analytical system.
Suggest, assess and translate system requirements into implementation designs and data models.
Ensure excellent quality and test coverage, as well as effective performance.
Work on timely resolution of issues and other tasks relevant to the position.
Desired Skills and Experience Requirements:
MS in Computer Sciences
5+ years of experience in a relevant software engineering position
5+ years hands-on experience with Java/J2EE, Relational databases and NoSQL data stores
In-depth experience building scalable, distributed Analytical Systems
Proficient with Predictive Analytics, AI and Machine Learning
Experience using AWS or GCP
Comfortable with Linux environments
Preferred:
Experience building Microservices and API's
Experience with LIMS or related software systems
Understanding of Bioinfomatic Pipelines
Exposure to Genomics and NGS
Benefits
Clear Labs offers solid Health Care, a 401k Plan, Food Catering, Massages, and an easy-going, open work environment."
34,"Data Engineer - REQ 19075, 19083","South San Francisco, CA",South San Francisco,CA,None Found,None Found,None Found,"An interest in biological sciences; experience with LIMS/ELN systems and with NGS, flow cytometry, and imaging data is a bonus, but not at all required
Excellent oral and written communication skills
Ability to work independently and collaboratively among cross-functional teams, with a particular understanding for and drive to solve data issues faced by research and clinical scientists
Ability to work efficiently, prioritize workflow, meet deadlines and balance competing priorities
Excellent analytical skills and scientific/technical expertise
",None Found,"
Degree (B.S., M.S. or PhD) in Computer Science, Math, Physics, or biological sciences is preferred, but not required
","
Degree (B.S., M.S. or PhD) in Computer Science, Math, Physics, or biological sciences is preferred, but not required
","At Lyell, our vision is to develop curative cell-based immunotherapies for solid tumor cancers. We have innovative science originating from our founder’s world class labs and a unique and disruptive approach to research and development. Our company is first and foremost focused on understanding the science. We are a learning organization, dependent on deep collaborative relationships between all of our colleagues, partners and founders. Our culture is based on Respect, Science, Courage and Intensity, and it reflects who we are and the environment we are creating.
Position Summary:
Lyell is looking for several Data Engineers, of all seniority levels, to join the Information Sciences team in South San Francisco or Seattle. Data Engineers at Lyell will be responsible for building a cloud data platform to enable Lyell scientists and collaborators with real-time data access, real-time collaboration, and making meaningful and actionable scientific insights from large data volumes of many different data modalities. We are looking for people who are passionate about data and are interested in making a difference in the world!
Essential Functions:
Design and build Lyell’s cloud-based Data Platform to manage petabyte scale data volumes, include NGS, flow cytometry, imaging, etc. data
Implement robust, distributed data pipelines that that stream data from instruments in the lab up to the cloud, and make this data easily searchable and accessible
Integrate open source, commercial, and in-house implemented analytics tools with our Data Platform
Make data available and optimized for large scale analytics, including machine learning
Work collaboratively in cross-functional groups, such as software engineering, biological sciences, data sciences, regulatory, and manufacturing
Requirements
Preferred Education/Experience:

Degree (B.S., M.S. or PhD) in Computer Science, Math, Physics, or biological sciences is preferred, but not required
Python, Java, or Scala
AWS or Google Cloud
Managing large-scale data on a S3 data lake, SQL data warehouse, or NoSQL system
Distributed real-time and batch computing, include big data technologies such as Spark, Hadoop, Hive, Presto, AWS Batch
Building distributed data pipelines
Implementing microservices with Docker
Bonus: Infrastructure as Code tools such as Terraform, CloudFormation
Bonus: Managing streaming data with technologies such as Kafka or Kinesis
Bonus: Deploying and/or implementing machine learning models
Knowledge, Skills and Abilities:
An interest in biological sciences; experience with LIMS/ELN systems and with NGS, flow cytometry, and imaging data is a bonus, but not at all required
Excellent oral and written communication skills
Ability to work independently and collaboratively among cross-functional teams, with a particular understanding for and drive to solve data issues faced by research and clinical scientists
Ability to work efficiently, prioritize workflow, meet deadlines and balance competing priorities
Excellent analytical skills and scientific/technical expertise
Benefits
At Lyell, we believe that highest performing teams include people from a wide variety of backgrounds and experiences who respectfully challenge each other. We are committed to building an open, diverse and inclusive culture for all employees.
We’ve learned from experience that some of the best people don’t always match our requirements perfectly - if you’re interested and think you could fit, please don’t hesitate to apply.
Lyell is proud to be an equal opportunity employer and does not discriminate on the basis of race, color, citizenship status, national origin, ancestry, sex, sexual orientation, age, religion, creed, physical or mental disability, medical condition, marital status, veteran status or any other characteristics protected under applicable federal, state and local laws.
If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process or are limited or unable to access or use this online application process and need an alternative method for applying, please contact us via our company website."
35,Sr. Data Engineer,"San Mateo, CA",San Mateo,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Brief Description
Job Advertisement
Senior Data Engineer

Ten-X Commercial is the CRE marketplace that is a force multiplier for sellers, buyers and brokers. Ten-X precision-matches assets, accelerates close rates, and streamlines the entire transaction process with more than $55 billion in sales and increasing daily. Leveraging desktop and mobile technology, Ten-X allows people to safely and easily complete real estate transactions entirely online. We bring quality assets to the market and attract prospective investors from around the world. By virtue of our best-in-class marketing and scalable technology platform, buyers and seller are able to conduct transactions in an efficient manner.

Ten-X empowers consumers, investors and real estate professionals with unprecedented levels of flexibility, control and simplicity – and the convenience of transacting properties whenever and wherever they want. As real estate continues to move online, Ten-X is uniquely positioned at the forefront of this dramatic industry evolution.

The Role:
Data, and our ability to leverage it, is seen and championed as a key competitive advantage from our CEO on down. We are looking for a top tier data engineer to work with our data engineering team on high impact projects that improve data availability and quality, and provide reliable access to data for the rest of the business. You will be working on key projects that have board level visibility.

Responsibilities

Design, architect and support new and existing data and ETL pipelines and recommend improvements and modifications.
Create optimal data pipeline architecture and systems.
Be responsible for ingesting data into our data lake and providing frameworks and services for operating on that data.
Work with Data Scientists to help dedupe and fuzzy match data
Analyze, debug and correct issues with data pipelines
Identify, design, and implement internal process improvements: automating manual processes and optimizing data delivery.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Spark and AWS technologies.

Experience

Undergraduate degree (ideally a Masters) in a relevant quantitative subject (Math, Statistics, Computer Science, Engineering, Economics, etc.)
8+ Years’ Experience in data engineering, data warehousing, business intelligence including: 5+ years in a modern data stack environment, specifically the Hadoop stack, 3+ Years' Python experience relating to data engineering
Experience with iterative Agile methodologies and use of supporting tools like JIRA, Confluence and Git
Experience in the following will be a plus:
Spark
Kafka
Clickstream data
Machine Learning
Streaming Data
Elastic Search
Containers (Docker)
Fuzzy Matching / NLP
Ability to understand business problems and translate them into data engineering requirements
Understanding and Familiarity with:
Hadoop and all the related stack (Pig, Hive, HBase, etc.)
SQL skills and SQL Databases
Strong oral and written communication skills and be able to communicate complex technical knowledge in meaning terms
Ability to work in a fast-paced environment and fluidly adapt to changing priorities
Must be passionate about getting to the root cause of issues and driving to whys
Proven ability to obtain buy-in/ partner with the data science team, including demonstrated ability to partner with functional leaders toward common goals
Well-developed analytical and interpersonal skills with ability to draw conclusions and communicate/present them confidently and effectively to broad audiences, including senior leadership
High energy and passion about solving business needs through data
Organized, structured thinker with ability to handle multiple assignments, remain calm under pressure, and digest information from multiple, disparate parts
Continuous improvement mindset
Not afraid to challenge conventional thinking or analyses

Bonus:

Cloud migration experience from On-Prem to cloud preferably AWS.
Machine Learning experience

"
36,Freelance Consultant,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,None Found,None Found,None Found,None Found,None Found,"DesignMind ( http://www.designmind.com/ ) is a premier consulting and services company specializing in Data Engineering and Analytics Consulting. Our team delivers Business Intelligence, Big Data, Data Science, Custom Software / Web Development, and Mobile Solutions, as well as Technical Staffing.

Using both mainstream and emerging technologies, we help our clients maximize the value of their data and make fact-based decisions. For more than a decade, we have helped our clients plan, develop and deploy robust, high impact solutions.

Join the DesignMind Extended Team!

We often have opportunities for people who want to work on a 1099 contract basis. If you are interested in being considered for work as a 1099 Independent Contractor, and want to be on our short list of high quality extended teammates, please apply.

If you meet our qualifications, we will contact you as opportunities come up. If you are available at the time, and the project is right for you, we'll get you started working with our team.

Desired Skills


Ability to multi-task and meet strict deadlines
Consulting Experience Preferred
Flexible schedule
Expert skills in data-related technologies
A team player who is passionate about excellent service delivery
Travel requirements: Must be able to travel to client sites in San Francisco Bay Area

Some of the Roles and Technologies


Big Data Engineer – Hadoop and related, Cassandra
Business Intelligence Consultant - MSBI
Report and Dashboard Developer - Power BI, Tableau
Database Engineer – SQL Server, Oracle, MySQL, etc.
Application Developer - .NET, Java, etc.
Mobile Developer – Visual Designer, iOS, Android, C# with Zamarin
Front-End UI/UX Developer
Technical Project Manager

Compensation:
DesignMind provides a competitive hourly rate for qualified independent contractors. The rate depends on the project and service required. Contractors are compensated for time on-site, mainly in the Bay Area, Sacramento, or Southern CA (Irvine) area."
37,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
Bachelor Degree in Computer Science, Engineering, or other STEM related area
4 years of experience in Data Warehouse implementation, 2 years with a Masters
Development experience in ETL. Experience with SSIS a plus.
Experience with programming or scripting languages like PowerShell, Java, VB Script, PHP, Ruby, or R
Extensive experience writing SQL queries, with experience specifically on Teradata and SQL Server a plus
Familiarity with Hadoop, Map Reduce, and/or HDFS
Experience in working with AWS or other cloud computing platforms
Experience writing scripts to prepare, integrate, clean, run and analyze data using Hive, Pig, etc. a plus
Communicate to multiple levels of management within the organization as well as business partners",None Found,"
Develop and Manage ETL processes to quickly organize data for analysis
Leverage SQL for data querying across enterprise platforms, including the construction of automated processes to facilitate ongoing business
Build out the necessary data structures optimized for eCommerce analytics, leveraging the on-premise data warehouse and other structured and unstructured data warehousing/storage platforms
Provide technical design/development guidance for data warehouse tools and initiatives
Partner with infrastructure and support teams to ensure the integrity of the environment
Provide business analytics guidance as needed to ensure data quality in business deliverables",None Found,"
Bachelor Degree in Computer Science, Engineering, or other STEM related area
4 years of experience in Data Warehouse implementation, 2 years with a Masters
Development experience in ETL. Experience with SSIS a plus.
Experience with programming or scripting languages like PowerShell, Java, VB Script, PHP, Ruby, or R
Extensive experience writing SQL queries, with experience specifically on Teradata and SQL Server a plus
Familiarity with Hadoop, Map Reduce, and/or HDFS
Experience in working with AWS or other cloud computing platforms
Experience writing scripts to prepare, integrate, clean, run and analyze data using Hive, Pig, etc. a plus
Communicate to multiple levels of management within the organization as well as business partners","The next great shift in consumer behavior driven by technological disruption is underway, and it’s happening in the food & beverage industry.

As other sectors have shifted to eCommerce-first business models in recent years, food & beverage has continued to rely predominantly on traditional brick & mortar models, but this is changing rapidly. New technologies are transforming every aspect of reaching consumers, from the rise of digital marketing and online grocery platforms, to the creation of supply chain tools that enable speedy at-home delivery. According to the Food Marketing Institute, 70% of U.S. consumers will shop online for groceries by 2024, with an estimated annual spend of $100 billion.

To deepen our efforts to seize this opportunity and lead the food & beverage industry into its remarkable next chapter, PepsiCo – a global company with powerhouse brands including Frito-Lay, Gatorade, Pepsi-Cola, Quaker, and Tropicana – is expanding its Global eCommerce Team. We’re looking for the greatest minds in data & analytics, software development, machine learning optimization, and next-generation supply chain. Given PepsiCo’s incredible reach – our foods and beverages are enjoyed more than one billion times a day in more than 200 countries and territories, and our value chain involves diverse partners ranging from farmers and food scientists to retailers and logistics specialists – the challenges we’re addressing are complex and the solutions will be deeply impactful.

Although PepsiCo is a large multinational, the PepsiCo Global eCommerce Team prides itself on having the entrepreneurial, action-oriented culture of an exciting startup business. Our group includes startup founders, Silicon Valley veterans, food & beverage experts, and seasoned executives from digital transformation leaders such as Amazon and Walmart. Our goal is to build the technological products and capabilities that will reinvent our industry and make us the #1 food & beverage business in eCommerce for decades to come.

As a Data Engineer you will design and build technical solutions which leverage both Enterprise and Big Data as it relates to B2B and B2C eCommerce. You will work to enhance the collection, storage, processing, and analysis of disparate data sets. You will drive initiatives with business partners and cross-functional technical teams to promote long term supportability and lead efforts to integrate data into the company’s larger Enterprise Architecture.

Responsibilities:
Develop and Manage ETL processes to quickly organize data for analysis
Leverage SQL for data querying across enterprise platforms, including the construction of automated processes to facilitate ongoing business
Build out the necessary data structures optimized for eCommerce analytics, leveraging the on-premise data warehouse and other structured and unstructured data warehousing/storage platforms
Provide technical design/development guidance for data warehouse tools and initiatives
Partner with infrastructure and support teams to ensure the integrity of the environment
Provide business analytics guidance as needed to ensure data quality in business deliverables
Qualifications/Requirements
Bachelor Degree in Computer Science, Engineering, or other STEM related area
4 years of experience in Data Warehouse implementation, 2 years with a Masters
Development experience in ETL. Experience with SSIS a plus.
Experience with programming or scripting languages like PowerShell, Java, VB Script, PHP, Ruby, or R
Extensive experience writing SQL queries, with experience specifically on Teradata and SQL Server a plus
Familiarity with Hadoop, Map Reduce, and/or HDFS
Experience in working with AWS or other cloud computing platforms
Experience writing scripts to prepare, integrate, clean, run and analyze data using Hive, Pig, etc. a plus
Communicate to multiple levels of management within the organization as well as business partners
Relocation Eligible: Not Applicable
Job Type: Regular

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity

Our Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901 - 4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance.

If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy

Please view our Pay Transparency Statement"
38,Data Engineer Team Lead,"Oakland, CA",Oakland,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for an experienced Data Engineering leader to found a new team at LaunchDarkly. We are growing rapidly, and need reliable and efficient answers for questions key to our future. You'll have ownership of tools and process, hiring for a team and developing our practice for the next stage of growth.

We are looking for someone with a track record of strong and trusted relationships with executives, an ability to communicate the results from analyses and influence roadmaps, and are passionate about leading, mentoring, and developing a team in a dynamic environment. The ideal fit is someone who can dig into the business challenge and ask the core questions, ensure data integrity and statistical significance, and build a convincing case to take action.
What you'll do:
Map out the plan to create
Hire and manage a small team of engineers
Create a data pipeline that is trustworthy and reliable
Choose and deploy a toolset to support our data needs
Help answer the business' most pressing questions
Build tools and teach people how to find answers to their own questions
On day one, you should have:
3+ years experience in building and operating a data practice for a growing company
Understanding of the fundamentals of data models and ETL
Expertise in SQL and proficiency in another data programming language (Python, R, etc.)
Deep experience in data instrumentation and history of strong partnership with engineering teams
Experience leading people: hiring, motivating, growing, empowering, and performance managing
You may also know (bonus skills):
Experience in both enterprise (or B2B) as well as consumer (or B2C) environments
MS, MBA or PhD degree in a relevant topic
About LaunchDarkly:
LaunchDarkly is a Feature Management Platform that serves over 100+ billion feature flags daily to help software teams build better software, faster. Feature flagging is an industry standard methodology of wrapping a new or risky section of code or infrastructure change with a flag. Each flag can easily be turned off independent of code deployment (aka ""dark launching""). LaunchDarkly has SDKs for all major web and mobile platforms. We are building a diverse team so that we can offer robust products and services. Our team culture is dynamic, friendly, and supportive. Our headquarters are in Oakland.

At LaunchDarkly, we believe in the power of teams. We're building a team that is humble, open, collaborative, respectful and kind. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status.

We've partnered with KeyValues to help demonstrate the amazing culture we've built here at LaunchDarkly, find more info at https://www.keyvalues.com/launchdarkly"
39,"Data Engineer - Principal (Big Data Architect) - San Francisco, CA","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.

We are...
The Industry-recognized data and analytics leadersPassionate problem solvers across a broad spectrum of technologies and industriesValue seekers for measurable business outcomesContinuous learners through training and education

Focused on a work-life balance with an unlimited paid time off policy
Big data architects are challenged with leading the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally, targeting a variety of use cases across data engineering, data science, data governance, and visualization.
Big data architects deliver value through...
Design of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools
Demonstration of client and solution leadership through strong communication skills to recommend actionable, data-driven insights
Collaboration with team members, business stakeholders and data SMEs to elicit, translate, and prescribe requirements
Management of technical, project, and organizational complexity, with a proven ability to simplify tough problems, track many moving parts across functional teams and business domains, and influence strategic direction
Internal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO
A big data architect's skills include but are not limited to...
Bachelors Degree and 6+years of experience
6+ years of professional IT work experience
3+ years consulting experience
SQL, SQL, SQL!
Programming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)
Linux / Windows (Command line)
Big Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop, Spark)
Cloud Platforms (AWS, Azure, Google Cloud Platform)
Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)
Data Integration Tools (Ab Initio, DataStage, Informatica, SSIS, Talend)
Databases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)
Markup Languages (JSON, XML, YAML)
Code Management Tools (Git/GitHub, SVN, TFS)
DevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)
Testing / Data Quality (TDD, unit, regression, automation)
Solving complex data and technology problems
Excellent communication to narrate data driven insights and technical approach
B.S. in computer science, engineering or related degree
If this sounds like you, let’s talk!

Clarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.


GLDR
#LI-NT1"
40,"SR DATA ENGINEER, BIG DATA","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for amazing people diverse in thought, perspective, and culture to join our team. We check our egos at the door, roll up our sleeves, work hard, move fast, and support each other. If that sounds like fun to you, please apply!

SENIOR DATA ENGINEER, BIG DATA

We are looking for engineers who are excited about building distributed data pipelines. We want you to help us shape our internal and external brand-new data warehouses, leveraging the latest advances of big data processing: a combination of Kafka Streams, Hadoop, and traditional RDBMS.
YOU WILL
Be a part of our local San Francisco (Financial District) big data team and be a part of an overall Data Organization spanning multiple offices.
Participate in architecture discussions and bring your experience in scalable data pipelines using Kafka Streams and/or other Big Data tools.
Take ownership of design and implementation of scalable and fault tolerant projects.
Maintain and incrementally improve existing solutions.
Get to build brand new pipelines with the technology stack including Spark, Spark Structured Streaming, Kafka, Hadoop, MySql, Python.
WHAT WE ARE LOOKING FOR
Solid understanding of distributed system fundamentals.
Experience in developing, troubleshooting, diagnosing, and performance tuning of distributed data pipelines at scale.
Demonstrated professional experience working with various components of Big Data ecosystem: Spark/Spark Streaming, Hive, Kafka/KSQL, Hadoop (or similar NoSQL ecosystem), et. al, in a production system.
Strong software engineering skills with Python.
Knowledge of some flavor of SQL (MySQL, Oracle, Hive, Impala), including the fundamentals of data modeling and performance.
EVEN BETTER IF YOU HAVE
Skills in real-time streaming applications.
Knowledge of Scala.
A development workflow using Docker containers.
Compulsion for automating your day-to-day processes.
WHAT WE OFFER
Ability to see your direct impact on a high visibility project.
An opportunity to both create new projects and help improve the existing big data pipelines.
Work in a mature, private, nationally-known company with a CEO approval of 85% on Glassdoor and a positive atmosphere in our San Francisco tech team.
401K plan with employer matching.
Commuter pre-tax contributions.
Flexible working hours and work-from-home days.
Health plan.
In-office snacks.
Organized team events.
Centro is an Equal Opportunity Employer. We respect and support an inclusive workplace diverse in thought, perspective and culture. We celebrate all team members regardless of gender/identity, sexual orientation, race or cultural background, religion, physical disability and age. We are better together."
41,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About HomeLight

We are on a mission to empower people to make smarter decisions during one of life's most important moments: buying or selling their home. By analyzing 30 million+ real estate transactions from 2 million agents across the country, we are able to take an unbiased, data-driven approach to match our clients with top performing real estate agents to meet each of their specific needs. Our algorithm and marketplace solutions guide our clients through every step of the home buying or selling experience. Join us and build our first Data Engineering team and the # destination for home sellers! Learn more about our engineering team! ( https://www.homelight.com/engineering )

What You'll Do Here:
We are building our Data Engineering team to tackle HomeLight's diverse, data challenges. This position is an excellent opportunity for an engineer that wants to own the development, optimization, and operation of our data pipeline, which collects, processes, and distributes data to a suite of HomeLight products and teams. You will provide mission-critical data to both our algorithms and internal users, refining our product and identifying new markets. Some projects you will work on:


Optimize and execute on requests to pull, analyze, interpret and visualize data
Partner with team leaders across the organization to build out and iterate on team, and individual performance metrics
Optimize our data release processes, and partner with team leads to iterate on and improve existing data pipelines.
Design and develop systems that ingest and transform our data streams using the latest tools.
Design, build, and integrate new cutting edge databases and data warehouses, develop new data schemas and figure out new innovative ways of storing and representing our data.
Research, architect, build, and test robust, highly available and massively scalable systems, software, and services.

You Have


5+ years of data engineering experience
Robust experience with Python, Spark, and Airflow
Experience writing and executing complex SQL queries
Experience building data pipelines and ETL design (implementation and maintenance)
Experience with AWS or other IAAS or PAAS provider
Scrum/Agile software development process.

Bonus points for:

Real estate experience
Experience with Periscope, Looker, Tableau and other BI tools
Experience with building data pipelines
Experience with machine learning

The Perks


Medical, vision, dental, and paternity/maternity benefits.
401(k)
Commuter benefits
Flexible time off policy
Catered lunches and snacks
Corporate gym membership
Company events: happy hours, bowling, bocce league, etc.

To make sure we hire the most qualified people, we have a multi-step interview process which may include interviews, a homework assignment, and a reference check. We're excited to get to know you and hope you're ready to give this opportunity everything you've got!"
42,Data Engineer - Google Cloud Platform,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,"
Candidates must have a minimum of 3 years of hands on experience working with data and Google Cloud Platform (GCP).
Candidate should be able to demonstrate the following through previous work experience:
Candidate must demonstrate a good understanding of GCP architecture & GCP marketing module.
Minimum of 3 years experience building the pipeline from on prem data store to GCP.
Minimum of 3 years experience working with different forms of data, such as: rational, flat file, xml, jason.
Minimum of 3 years experience and demonstrated proficiency with sql/plsql, python and shell scripting.
Candidates must be able to travel Monday-Thursday on a weekly basis for client/project needs. This may also be referred to as 100% travel.
Bachelor's degree or equivalent (minimum 12 years) work experience. (If Associate’s Degree, must have minimum 6 years work experience)
",None Found,None Found,None Found,None Found,"Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions – underpinned by the world’s largest delivery network – Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With approximately 469,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward.
Analytics professionals create new insights from predictive statistical modeling activities that target and deliver value to our clients.
Job Description
A professional at this position level within Accenture has the following responsibilities:
Adapts existing methods and procedures to create possible alternative solutions to moderately complex problems.
Understands the strategic direction set by senior management as it relates to team goals.
Uses considerable judgment to determine solution and seeks guidance on complex problems.
Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture.
Determines methods and procedures on new assignments with guidance.
Decisions often impact the team in which they reside.
Manages small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture.
This role will require someone that has past experience working with data & advanced analytics in the cloud, specifically Google Cloud Platform. We will also refer to the Google Cloud Platform as GCP throughout the job description. Someone in this role will have the following responsibilities.
Design and develop data pipeline in GCP
Prepare various data sets for blg query for advance analytics
Prepare data sets for marketing campaign
Create new Data APIs for downstream consumption

Basic Qualifications
Candidates must have a minimum of 3 years of hands on experience working with data and Google Cloud Platform (GCP).
Candidate should be able to demonstrate the following through previous work experience:
Candidate must demonstrate a good understanding of GCP architecture & GCP marketing module.
Minimum of 3 years experience building the pipeline from on prem data store to GCP.
Minimum of 3 years experience working with different forms of data, such as: rational, flat file, xml, jason.
Minimum of 3 years experience and demonstrated proficiency with sql/plsql, python and shell scripting.
Candidates must be able to travel Monday-Thursday on a weekly basis for client/project needs. This may also be referred to as 100% travel.
Bachelor's degree or equivalent (minimum 12 years) work experience. (If Associate’s Degree, must have minimum 6 years work experience)
Professional Skill Requirements
Google Cloud Certification is highly desired Proven success in contributing to a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent leadership, communication (written and oral) and interpersonal skills
All of our consulting professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a federal contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
43,Data Engineer II,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,None Found,None Found,None Found,None Found,None Found,"At Ravel we develop the legal profession’s most innovative products for legal analytics and research, using the latest in visualization and text mining. Lawyers use our products to forecast how judges will rule, find critical cases, and make data-driven decisions. We're working with great technology and the challenges that get us fired up involve large-scale heterogeneous text and data mining, beautiful UI, and machine learning. Industry leader LexisNexis acquired Ravel in 2017.
 In this role you will work on new product development in a small team environment. You will perform mid-level research, design, and software development. Qualified applicants will have the ability to do individual pieces of work and solve problems including the design of the program flow of individual pieces of code, effective coding, and unit testing. You will get the experience of working in a start-up culture with the benefits of an established company and large data sets.
 This position is located in San Francisco, California.
 RESPONSIBILITIES
Interface with other technical personnel or team members to finalize requirements.Write and review portions of detailed specifications for the development of system components of moderate complexity.Work closely with other development team members to understand moderately complex product requirements and translate them into software designs.Successfully implement development processes, coding best practices, and/or code reviews.Operate in an agile development environment while collaborating with key stakeholders.Resolve technical issues as necessary.Keep abreast of new technology developments.
MINIMUM QUALIFICATIONS
1+ years of Software Engineering experienceBS Engineering/Computer Science or equivalent experience required

TECHNICAL SKILLS
Proficiency with data manipulation languages.Ability to work with complex data models.Proficiency in one or more development languages such as Java and/or ScalaKnowledge of AWS services or other similar platforms.Knowledge of Spark, Hadoop, or other distributed computing systems.Knowledge or interest in Machine Learning.Knowledge of relational and NoSQL databases.Knowledge of test-driven development.Familiarity of industry best practices.Good oral and written communication skills.

LexisNexis Legal & Professional (www.lexisnexis.com) is a leading global provider of content and technology solutions that enable professionals in legal, corporate, tax, government, academic and non-profit organizations to make informed decisions and achieve better business outcomes. As a digital pioneer, the company was the first to bring legal and business information online with its LexisNexis services. Today, LexisNexis Legal & Professional harnesses leading-edge technology and world-class content, to help professionals work in faster, easier and more effective ways. Through close collaboration with its customers, the company ensures organizations can leverage its solutions to reduce risk, improve productivity, increase profitability and grow their business.
Part of RELX Group plc, LexisNexis Legal & Professional serves customers in more than 100 countries with 10,000 employees worldwide. LexisNexis, a division of RELX Group, is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. If a qualified individual with a disability or disabled veteran needs a reasonable accommodation to use or access our online system, that individual should please contact 1.877.734.1938 or accommodations@relx.com.

RSRLNLP"
44,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"It's fun to work in a company where people truly BELIEVE in what they're doing!
We're committed to bringing passion and customer focus to the business.
Cloudmark as part of Proofpoint is looking for a Senior Data Engineer to join our engineering team in San Francisco. The Cloudmark team builds software to provide high performance messaging security services to customers around the world, including AT&T, Time Warner Cable, Orange and Swisscom. We are looking for someone who is passionate about building high performance software and joining the fight against messaging abuse. The ideal person will work out of our convenient South of Market office in San Francisco, California location.
Your day-to-day
Participate in a creative, enthusiastic, and geographically-distributed team of world-class security engineers that are responsible for identifying and responding quickly to attacks levied against some of the world's largest ISPs.
Leads and participates in the design, implementation, and management of data warehousing, data pipelines, and (extract, transform, load) ETL workflows.
Assemble large, complex datasets that meet functional requirements for various projects
Apply your software engineering skills to automate manual processes, optimize data delivery, re-architecture data platform(s) as needed for scalability
Collaborate with data science team to transform to build internal data tools and products
What you bring to the team
3+ years of experience with designing data platforms
Advanced SQL knowledge across various DBMS including MySQL, BigQuery, RedShift, and Athena
Advanced working knowledge modern database types including relational, time-series, columnar, key-value, document, and full text.
Experience building and optimizing data pipelines architectures leveraging both on-premise and/or cloud (Amazon/Google)
Working knowledge of messaging queuing, and stream processing.
Strong scripting skills using minimally Python, and shell.
Fundamental understanding software design and data structures.
Comfortable working in a predominately Linux environment.
Bachelor’s degree or higher in a STEM program or equivalent combination of education and experience.
Preferred
Background in the information security or telecommunications space
Familiarity with GoLang
Familiarity with AWS ecosystem beyond database, streaming, and queuing technologies.
Working knowledge of Splunk, ELK, CouchDB, MongoDB
#LI-VW1
If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!"
45,Sr. Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,"
Proactively drive the execution of our data engineering, business intelligence, and data warehouse roadmap
Understand and translate business needs into data models to support long-term, scalable, and reliable solutions
Create logical and physical data models using best practices to ensure high data quality and reduced redundancy
Drive data quality across the organization; develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking
Define and manage SLA's for data sets and processes running in production
Continuously improve our data infrastructure and stay ahead of technology
Design a system for data backup in case of system failure
Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs
",None Found,"
4+ years of experience in Data Engineering and Data Warehousing
Bachelor's degree in a quantitative field, e.g. Computer Science, Math, Physics
Experience scaling data environments with distributed/RT systems and self-serve visualization environments
Advanced proficiency with SQL, Perl, Python, Postgres, REST/GraphQL
Experience designing and implementing cloud based and SaaS data warehouse (e.g. WS, Hadoop, NoSQL) and developing ETL/ELT pipelines
Experience integrating and building data platform in support of BI, Analytics, Data Science, and real-time applications
Strong communication skills, with the ability to initiate and drive projects proactively and accurately
Work full-time in our San Francisco office
Eligible to work in the United States legally
","Who we are:
MasterClass is transforming online education by enabling anyone in the world to learn from the very best. We are deconstructing what makes an actor able to cry on demand, how an athlete defies gravity, and what it takes to write a bestseller. Our online learning content is available to students anywhere anytime, which supports our mission to ignite the greatness in others.

We are a quickly growing VC-funded startup based in San Francisco, with additional offices in Los Angeles. We have created online classes taught by famous masters—Gordon Ramsay, Martin Scorsese, Serena Williams, Helen Mirren, Margaret Atwood, Annie Leibovitz, Steph Curry, Carlos Santana, Timbaland and many more to come.

Since launching in 2015, we have been growing our team. Apply now to find out more about what we are doing behind the scenes.

What we're looking for:
Data, and how they are used, play a central role at MasterClass and are at the heart of how we make business, product, content, and operational decisions. Our growing Analytics, Data Science, and Data Engineering teams sit at the core of the company and collaborate with a variety of departments to drive decisions and provide direction for future growth at MasterClass. The team tackles challenging problems across many technical disciplines, including time series forecasting, causal inference, optimization, and machine learning. We are looking for an exceptional Senior Data Engineer to help build our data platform to scale the business and enable our Analytics and Data organization in solving those challenges.

Responsibilities of the role:

Proactively drive the execution of our data engineering, business intelligence, and data warehouse roadmap
Understand and translate business needs into data models to support long-term, scalable, and reliable solutions
Create logical and physical data models using best practices to ensure high data quality and reduced redundancy
Drive data quality across the organization; develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking
Define and manage SLA's for data sets and processes running in production
Continuously improve our data infrastructure and stay ahead of technology
Design a system for data backup in case of system failure
Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs

Requirements:

4+ years of experience in Data Engineering and Data Warehousing
Bachelor's degree in a quantitative field, e.g. Computer Science, Math, Physics
Experience scaling data environments with distributed/RT systems and self-serve visualization environments
Advanced proficiency with SQL, Perl, Python, Postgres, REST/GraphQL
Experience designing and implementing cloud based and SaaS data warehouse (e.g. WS, Hadoop, NoSQL) and developing ETL/ELT pipelines
Experience integrating and building data platform in support of BI, Analytics, Data Science, and real-time applications
Strong communication skills, with the ability to initiate and drive projects proactively and accurately
Work full-time in our San Francisco office
Eligible to work in the United States legally

At MasterClass, we believe we put our best work forward when our employees bring together ideas that are diverse in thought. We are proud to be an equal opportunity workplace and are committed to equal employment opportunity regardless of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or any other characteristic protected by applicable federal, state or local law. In addition, MasterClass will provide reasonable accommodations for qualified individuals with disabilities. If you have a disability or special need, we would like to know how we can better accommodate you."
46,Specialist,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,"
Experienced. 6+ years’ experience selling business solutions to large/global enterprise customers with a focus on data platform technologies (Big Data, Analytics, ML, Relational and NoSQL databases, etc.) preferred
Account Management. Effective territory/account management: planning, opportunity qualification and creation, stakeholder and executive communication, needs analysis, value engineering, services/partner engagement, opportunity management, pipeline management, large dollar licensing and deal negotiation required
Executive Presence. Experience and expertise selling to senior business decision makers by aligning & reinforcing the value of the solution to the customer’s overall business pain and/or strategic opportunities and decision criteria.
Problem Solver. Ability to solve customer problems through cloud technologies required
Collaborative. Orchestrate and influence virtual teams to pursue sales opportunities and lead v-teams through influence
","
Experienced. 6+ years’ experience selling business solutions to large/global enterprise customers with a focus on data platform technologies (Big Data, Analytics, ML, Relational and NoSQL databases, etc.) preferred
Account Management. Effective territory/account management: planning, opportunity qualification and creation, stakeholder and executive communication, needs analysis, value engineering, services/partner engagement, opportunity management, pipeline management, large dollar licensing and deal negotiation required
Executive Presence. Experience and expertise selling to senior business decision makers by aligning & reinforcing the value of the solution to the customer’s overall business pain and/or strategic opportunities and decision criteria.
Problem Solver. Ability to solve customer problems through cloud technologies required
Collaborative. Orchestrate and influence virtual teams to pursue sales opportunities and lead v-teams through influence
","40% of your time will be spent with customers identifying and surfacing new engagements that align with the customer’s business strategy. You will work with partners and others at Microsoft, as well as use our core tools, targeted account lists to identify and engage prioritized customers. You will be required to be disciplined in business-management, adaptable to a culture of accountability and build a strong and active business network.","
Experienced. 6+ years’ experience selling business solutions to large/global enterprise customers with a focus on data platform technologies (Big Data, Analytics, ML, Relational and NoSQL databases, etc.) preferred
Account Management. Effective territory/account management: planning, opportunity qualification and creation, stakeholder and executive communication, needs analysis, value engineering, services/partner engagement, opportunity management, pipeline management, large dollar licensing and deal negotiation required
Executive Presence. Experience and expertise selling to senior business decision makers by aligning & reinforcing the value of the solution to the customer’s overall business pain and/or strategic opportunities and decision criteria.
Problem Solver. Ability to solve customer problems through cloud technologies required
Collaborative. Orchestrate and influence virtual teams to pursue sales opportunities and lead v-teams through influence
",None Found,"Microsoft is on a mission to empower every person and every organization on the planet to achieve more. Our culture is centered on embracing
a growth mindset, a theme of inspiring excellence, and encouraging teams and leaders to bring their best each day. Growth mindset encourages
each of us to lean in and learn what matters most to our customers, to create the foundational knowledge that enables us to make customer-
first decisions in everything we do. In doing so, we create life-changing innovations that impact billions of lives around the world. You can help
us achieve our mission.
Are you insatiably curious? Do you embrace uncertainty, take risks, and learn quickly from your mistakes? Do you collaborate well with others, knowing that better solutions come from working together? Do you stand in awe of what humans dare to achieve, and are you motivated every day to empower others to achieve more through technology and innovation? Are you ready to join the team that is at the leading edge of Innovation at Microsoft?
To learn more about Microsoft’s mission, please visit: https://careers.microsoft.com/mission-culture
Check out all our products at: http://www.microsoft.com/en-us

Responsibilities
As an Azure Data & AI Specialist you will be a senior solution sales leader within our enterprise sales organization working with our most important customers. You will lead a virtual team of technical, partner and consulting resources to advance the sales process and achieve/exceed quarterly Azure Data Services (including our Data Platform, AI and IoT services) revenue and usage/consumption as well as SQL Server revenue targets in your assigned accounts. You will be a trusted advisor and a Data & AI subject matter expert. You will help customers evaluate their applications, recommend solutions that meet their requirements and demonstrate these solutions to win the technical decision. You will need to support customers to remove roadblocks to deployment and drive customer satisfaction. You will help our customers take advantage of our unique hybrid data platform to realize the value of digital transformation.

Primary accountabilities for this role include:40% of your time will be spent with customers identifying and surfacing new engagements that align with the customer’s business strategy. You will work with partners and others at Microsoft, as well as use our core tools, targeted account lists to identify and engage prioritized customers. You will be required to be disciplined in business-management, adaptable to a culture of accountability and build a strong and active business network.
40% of your time will be spent on being the key technical leader, trusted advisor and influencer in shaping customer decisions to buy and adopt Microsoft Data & AI solutions. You will own winning the technical decision at customers for sales opportunities and usage scenarios, through tailoring your message, bringing ideas to customers, engaging with them to show our technology differentiation, and guiding them in decision making. You will lead presentations and solution demonstrations to explain and prove to our largest customers the capabilities of Microsoft's Data & AI solutions, and how we can make their businesses more successful.
20% of your time will be spent on influencing the Microsoft Data & AI go to market strategies by providing feedback to sales, marketing, and engineering on current and future product requirements and sales blockers. You will be recognized for sharing, learning and driving work that results in business impact for customers, partners and Microsoft. We encourage thought leadership and we encourage all our employees to continuously maintain and enhance their technical, sales, professional skills and competitive readiness. You will therefore be required to attain and maintain required certifications

Qualifications
Experiences Required: Education, Key Experiences, Skills and Knowledge:
Professional
Experienced. 6+ years’ experience selling business solutions to large/global enterprise customers with a focus on data platform technologies (Big Data, Analytics, ML, Relational and NoSQL databases, etc.) preferred
Account Management. Effective territory/account management: planning, opportunity qualification and creation, stakeholder and executive communication, needs analysis, value engineering, services/partner engagement, opportunity management, pipeline management, large dollar licensing and deal negotiation required
Executive Presence. Experience and expertise selling to senior business decision makers by aligning & reinforcing the value of the solution to the customer’s overall business pain and/or strategic opportunities and decision criteria.
Problem Solver. Ability to solve customer problems through cloud technologies required
Collaborative. Orchestrate and influence virtual teams to pursue sales opportunities and lead v-teams through influence
Technical
Azure Platform. Understanding of Microsoft server products and/or complementing solutions. The position requires the ability to articulate and demonstrate the business value of Microsoft's solutions and have a firm understanding of Microsoft's strategies and products relative to major Microsoft competitors required
Leadership. Experience leading large cloud deals especially those involving Data Platform modernization and migration, AI and related required
Competitive Landscape. Knowledge of enterprise software solutions and platform competitor landscape
Partners. Understanding of partner ecosystems and the ability to leverage partner solutions to solve customer needs.
Certifications. Azure Data Engineer or Azure AI Engineer required
Education
Bachelor’s Degree required, MBA preferred, or equivalent experience.

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work."
47,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Arnaout Lab at UCSF seeks an experienced Data Engineer to whom wants to participate in cutting-edge research with transformational impact to clinical and research medicine across a wide array of diseases, working with decades of high-quality medical data alongside clinical domain experts. The successful candidate will use local, hybrid, and / or cloud computing to develop strategies and software / hardware pipelines for modular, secure automation, scaling, and crowdsourcing of data mining, preprocessing, storage, labeling and computing. The position also provides opportunities to publish, present at research conferences, and for professional advancement.

Develops and optimizes a variety of computational, data science, and CI research tools and components. Performs research on current and future HPC, data, and CI technologies, hardware and software projects. Works on algorithm development, optimization, programming, performance analysis and / or benchmarking assignments of moderate scope where the tasks involve knowledge of either domain / computer science research requirements and / or CI design / implementation requirements.

LI-JD2
MEDICINE / CARDIOLOGY / ARNAOUT LABORATORY
The Arnaout laboratory studies deep and machine learning for biomedical imaging and related clinical data, with the goals of decreasing diagnostic error and developing and scaling novel phenotypes to drive precision medicine research.

UCSF is a top-10 medical center and a leader in cross-campus efforts to mine, harmonize, and analyze multi-modal clinical data for the University of California’s 15 million patients.

The Arnaout laboratory is part of both the Bakar Computational Health Sciences Institute, where the abovementioned efforts are based, and the nationally ranked Department of Medicine (DOM). Projects focus on deep learning for medical imaging, and through collaborative work with intra- and inter-institutional partners, touch the electronic health record, genetics, and other sources of data.

ABOUT UCSF

The University of California, San Francisco (UCSF) is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It is the only campus in the 10-campus UC system dedicated exclusively to the health sciences."
48,"Software Engineer, Data","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We’re on a mission to understand and structure the world’s medical data, starting by making sense of the terabytes of clinician notes contained within the electronic health records of the world’s largest health systems.

We’re seeking exceptional Data Engineers to work on data products that drive the core of our business-a backend expert able to unify data, and build systems that scale from both an operational and an organizational perspective.
As a Data Engineer you will:
Develop data infrastructure to ingest, sanitize and normalize a broad range of medical data, such as electronics health records, journals, established medical ontologies, crowd-sourced labelling and other human inputs.
Build performant and expressive interfaces to the data
Build infrastructure to help us not only scale up data ingest, but large-scale cloud-based machine learning

We’re looking for teammates who bring:
Experience building data pipelines from disparate sources
Hands-on experience building and scaling up compute clusters
Excitement about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration.
A solid understanding of databases and large-scale data processing frameworks like Hadoop or Spark. You’ve not only worked with a variety of technologies, but know how to pick the right tool for the job.
A unique combination of creative and analytic skills capable of designing a system capable of pulling together, training, and testing dozens of data sources under a unified ontology.

Bonus points if you have experience with:
Developing systems to do or support machine learning, including experience working with NLP toolkits like Stanford CoreNLP, OpenNLP, and/or Python’s NLTK.
Expertise with wrangling healthcare data and/or HIPAA.
Experience with managing large-scale data labelling and acquisition, through tools such as through Amazon Turk or DeepDive."
49,Senior Data Engineer,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,None Found,None Found,None Found,None Found,None Found,"Do you wake up early, eager to see how many of your ETL cron jobs failed overnight? Of course you don't! You put an end to all of those sleepless nights by leveraging new technologies to schedule workflow dependencies and set up alerting ages ago. You've been getting a full 8-hours of sleep ever since!

Updater's Data Team is an independent function that takes a strategic approach to ensuring the business is leveraging data across all units to drastically improve business outcomes. We are a group of Data Engineering and Data Science/Analytics professionals, working together closely to ensure we are a data-driven company. By building and maintaining the infrastructure that powers all of our Data Science, Product, Marketing, Sales, and Success analytical and operational functions, Data Engineering is one of the most highly leveraged teams at Updater!

What You'll Do

As a Senior Data Engineer at Updater, you'll lead the development of an innovative data environment by leveraging industry best practices and cutting edge approaches to build a robust and scalable data platform. You're excited to come on board and work with our Engineering & Tech Ops teams to understand our data model and construct a thoughtful approach to building a scalable data architecture that provides a foundation for teams across the company to self-serve their data needs.

You will lead the development, documentation, access, and quality of our centralized data warehouse solution (our stack includes Airflow, Fivetran, and Snowflake!), and will balance ready access to existing data features while incorporating new features in a timely manner. You'll collaborate with Engineering on ensuring data capture for new product initiatives, with our business partners across all major functions at the organization to anticipate data needs, and proactively develop data solutions to address the toughest business problems.

Our ideal candidate isn't satisfied until a project is seen through to meaningful impact. If you love working with data and want to see your work impact the entire organization, we want to talk to you!

What You Have

You're a senior engineer who has extensive experience leading data engineering and data warehousing initiatives, and are excited to be a part of an industry-leading company growing at an extremely fast pace! We're looking for someone with:


Humility and a sense of humor
Strong communication skills, deep levels of empathy, and a keen eye for innovation and pragmatism
3+ years experience building and maintaining data pipelines & data models as part of a Data Warehouse solution
Expert Python and SQL skills
Ability to manage full project lifecycles
Experience working closely with Engineering, Product, and Analytics/Data Science teams
Experience with modern ETL tools (Fivetran, Alooma, Stitch, etc)
Experience with modern DWH environments (Snowflake, AWS Redshift and S3, BigQuery, etc)
Experience with git, GitHub, and the pull request workflow

Bonus Points for:

Experience with workflow management tools (Airflow, Luigi, etc)
Experience with streaming data services (Kafka, Kinesis, Firehose, etc.)

Compensation & Benefits:
We're looking for top-tier talent, and we offer compensation packages that include competitive base salary & stock options. Our comprehensive benefits programs include health care, dental, transportation subsidies, and Flexible Paid Time Off (PTO).

About Updater:
Updater makes moving easier for the 17 million households that relocate every year in the US. With Updater, users seamlessly transfer utilities, update accounts and records, forward mail, and much more. Hundreds of the most prominent real estate companies in the US (from real estate brokerages to multifamily and relocation companies) rely on Updater's real estate products to save clients hours with a branded and personalized Updater moving experience.

Headquartered in New York City, Updater has raised nearly $100 million from leading investors, including SoftBank Capital, IA Ventures, Commerce Ventures, Second Century Ventures (the strategic investment arm of the National Association of Realtors®), and more. Updater ranked #3 on Crain's 2016 Best Places to Work in NYC, ranking as the highest rated tech company on the list, and ranked #7 in 2018. For more information, please visit www.updater.com ( http://www.updater.com/ ).

Updater is proud to be an equal opportunity employer and will consider all qualified applicants regardless of color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital or family status, disability, gender identity or expression, veteran status, actual or presumed belonging to an ethnic group, or any other legally protected status. If you have a disability or special need that requires accommodation, please let us know."
50,Senior Data Engineer,"San Francisco, CA 94109",San Francisco,CA,94109,None Found,None Found,None Found,None Found,None Found,None Found,"is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee of this job. Duties, responsibilities and activities may change at any time with or without notice.
About You
5+ years of data engineering in a production environment
3+ years of experience coding in a data capacity (Python, PHP, Java, etc)
5+ years of experience SQL (we use Postgres and RedShift)
You have hands-on experience with MPP databases and query optimization (Snowflake and Amazon Redshift preferred).
You have basic knowledge of non-relational technologies like Hadoop and Spark (development experience in these environments highly desirable).
You have an excellent understanding of data warehousing concepts, ETL and data modeling.
You have extensive experience building and maintaining production data pipelines.
Production experience with machine learning (scikit, SyPy, tensorflow, etc) a plus, but not required
Ability to work independently
Outstanding communication skills (Written and Verbal)
Let’s do this

We're building a diverse and inclusive work environment where we learn from each other. We welcome people of diverse backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and a fun place to work. Come join the community at Grove. It's a heck of a lot of fun, and we'd love to tell you more about it.

If you require reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please direct your inquiries to Talent@grove.co.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
51,"Senior Data Engineer, Data Infrastructure","Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Founded by Dr. Priscilla Chan and Mark Zuckerberg in 2015, the Chan Zuckerberg Initiative (CZI) is a new kind of philanthropy that's demonstrating technology to help solve some of the world's toughest challenges – from eradicating disease, to improving education, to reforming the criminal justice system. Across three core Initiative focus areas of Science, Education and Justice and Opportunity, we're pairing engineering with grantmaking, impact investing, policy work, and movement building, to help build an inclusive, just and balanced future for everyone.

----------
Our Values
----------

We believe we can help build a future for everyone.


We aim to be daring, but humble: We look for bold ideas — regardless of structure and stage — and help them scale by pairing engineers with domain authorities to build tools that accelerate the pace of social progress.
We want to learn fast, but build for the long-term: We want to iterate fast and help bring new solutions to the table, but we also realize that important breakthroughs often take decades, or even centuries.
Stay close to the real problems: We engage directly in the communities we serve because no one understands our society's challenges like those who live them every day.

Our success is dependent on building teams that include people from different backgrounds and experiences who can challenge each other's assumptions with fresh perspectives. To that end, we look for a diverse pool of applicants including those from historically marginalized groups — women, people with disabilities, people of color, formerly incarcerated people, people who are lesbian, gay, bisexual, transgender, and/or gender nonconforming, first and second generation immigrants, veterans, and people from different socioeconomic backgrounds.

---------------
The Opportunity
---------------

By pairing engineers with leaders in our education, science, and justice and opportunity teams, we can bring technology to the table in new ways to help drive solutions. We are uniquely positioned to design, build, and scale software systems to help educators, scientists, and policy experts better address the myriad challenges they face. Our technology team is already helping schools bring personalized learning tools to teachers and schools across the country and supporting scientists around the world as they develop a comprehensive reference atlas of all cells in the human body.

The Infrastructure organization works on building shared tools and platforms to be used across all of the Chan Zuckerberg Initiative. Members of the data infrastructure engineering team have an impact on all of CZI's initiatives by enabling the technology solutions used by other engineering teams at CZI to scale. A person in this role will build these technology solutions and help to cultivate a culture of shared standard methodologies and knowledge around data engineering.

--------
You will
--------


Analyze and improve efficiency, stability, and security of CZI data warehouses across our Education, Science, and Justice & Opportunity initiatives
Design and build tools and libraries, to help 5+ engineering teams manage and query their data efficiently
Evangelize and educate teams across CZI on standard methodologies around data privacy and security, data operations, as well as improving data operations efficiency
Help teams improve their data operations by automating manual processes, optimizing data storage and delivery, re-designing infrastructure for scalability
Design and build ETL infrastructure used by our initiatives and across the organization
Work with Data Scientists and Data Analysts to optimize deriving insights from our data
Provide operational oversight for our data and optimized Data DevOps for the organization

--------
You have
--------


Experience with a scripting language such as Python, PHP, Ruby, or Perl
Experience with a systems language such as C, C++, C#, Go, Java or Scala
Experience developing data solutions on Snowflake, Redshift, Cosmos DB, or BigQuery
Experience with developing and maintaining custom or structured ETL processes and data pipelines
Experience working with relational SQL and NoSQL databases
Experience working with large disconnected datasets and help teams extract value from them
Experience working with cloud services such as AWS, Google Cloud, Azure

"
52,Big Data Engineer- Contractor,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,"
Responsible for design, implementation, and ongoing support of the Big Data platforms (Hadoop, HBase, HIVE, Spark), ensure high availability and reliability
Migrating large data sets between data centers and the cloud (such as AWS SQL server and Hadoop)
Design, test and implement cloud BI / DW infrastructure
Understand and support AWS native big data / analytics system
Use Streaming, Spark & Big Data technologies to enrich and transform data for real time ingestion and build low latency feeds.
",None Found,"
Thorough understanding of Hadoop ecosystem (HDFS, YARN, Hive, Pig, MapReduce, Spark, Spark2, Sqoop, Solr, kafka, oozie)
Strong experience in setting up, configurating, upgrading and managing security for Hadoop clusters, setting up Ranger policies for HDFS and Hive
Experience in managing Hadoop cluster with Ambari and developing custom tools/scripts to monitor the Hadoop Cluster health
Strong knowledge and hands on experience related to mission critical backup and recovery
Strong experience with load balancing and high volume, high availability environments
Able to automate administrative tasks using scripting languages (Python, Shell, Ansible)
Strong working experiences of implementing Big Data processing using MapReduce algorithms and Hadoop/Spark APIs
Experience building workflow to perform predictive analysis, muilti-dimensional analysis, data enrichments etc
Understanding of software development methodologies and coding standards.
A burning desire to master new technologies and apply them to real world challenges
","Auction.com is the nation’s leading online real estate marketplace focused exclusively on the sale of residential bank-owned and foreclosure properties via online auctions and live trustee sale events. By offering access to exclusive properties and technology designed to seamlessly connect buyers and sellers, Auction.com empowers residential real estate investors and financial institutions to achieve optimal, mutually beneficial results – to go beyond the bid.

Big Data Engineer- short term project.

Are you looking to work on a challenging short term project with a Leading Online Real Estate Organization that has an amazing company culture. If this sounds like a good fit for you, please read on to view the information about this exciting/challenging project.

What you will be working on;

Responsibilities:

Responsible for design, implementation, and ongoing support of the Big Data platforms (Hadoop, HBase, HIVE, Spark), ensure high availability and reliability
Migrating large data sets between data centers and the cloud (such as AWS SQL server and Hadoop)
Design, test and implement cloud BI / DW infrastructure
Understand and support AWS native big data / analytics system
Use Streaming, Spark & Big Data technologies to enrich and transform data for real time ingestion and build low latency feeds.

Requirements:

Thorough understanding of Hadoop ecosystem (HDFS, YARN, Hive, Pig, MapReduce, Spark, Spark2, Sqoop, Solr, kafka, oozie)
Strong experience in setting up, configurating, upgrading and managing security for Hadoop clusters, setting up Ranger policies for HDFS and Hive
Experience in managing Hadoop cluster with Ambari and developing custom tools/scripts to monitor the Hadoop Cluster health
Strong knowledge and hands on experience related to mission critical backup and recovery
Strong experience with load balancing and high volume, high availability environments
Able to automate administrative tasks using scripting languages (Python, Shell, Ansible)
Strong working experiences of implementing Big Data processing using MapReduce algorithms and Hadoop/Spark APIs
Experience building workflow to perform predictive analysis, muilti-dimensional analysis, data enrichments etc
Understanding of software development methodologies and coding standards.
A burning desire to master new technologies and apply them to real world challenges

Nice to have:

Relational design, understand business requirements and perform data design reviews
Data migration ETL concepts, open source ETL tools
Working experience at a web or internet start-up experience
Knowledge of the real estate industry

To all recruitment agencies: Auction.com does not accept agency resumes unless you are part of our preferred partner network. Please do not forward resumes to our jobs alias, Auction.com employees or any other company location. Auction.com is not responsible for any fees related to unsolicited resumes."
53,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description

Our Data Engineering team builds and maintains a secure, scalable, flexible and user-friendly analytics hub that allows us to make informed and data-driven decisions. They also construct and curate business-critical data sets that allow us to realize the value of all the data we collect.
A Data Engineer utilizes a multidisciplinary approach to providing ETL solutions for the business, combining technical, analytical, and domain knowledge. The perfect applicant for this role has strong development skills, experience transforming and profiling data to determine risks associated with proposed analytics solutions, a willingness to continually interface with analysts in order to determine an optimal approach, and an eagerness to explore data sources to understand the availability, utility, and integrity of our data.
What you'll own:
Data pipeline / ETL development:
Building and enhancing data curation pipelines using tools like SQL, Python, Glue, Spark and other AWS technologies
Focus on data curation on top of datalake data to produce trusted datasets for analytics teams
Data Curation:
Processing and cleansing data from a variety of sources to transform collected data into an accessible and curated state for Analysts and Data Scientists
Migrating self-serve data pipeline to centrally managed ETL pipelines
Advanced SQL development and performance tuning
Some exposure to Spark, Glue or other distributed processing frameworks helpful
Work with business data stewards & analytics team to research and identify data quality issues to be resolved in the curation process
Data Modeling:
Design and build master dimensions to support analytic data requirements
Replacing legacy data structures with new datasets sourced from streaming data feeds from the core product and other operational systems
Design, build and support pipelines to deliver business critical datasets
Resolve complex data design issues & provide optimal solutions that meet business requirements and benefit system performance
Query Engine Expertise & Performance Tuning:
Assist Analytics teams with tuning efforts
Curated dataset design for performance
Orchestration:
Management of job scheduling
Dependency management mapping and support
Documentation of issue resolution procedures
Data Access
Design and management of data access controls mapped to curated datasets
Leveraging devops best practices, such as IAC and CI/CD to build upon a scalable and extensible data environment

Experience you'll need:
Strong experience designing and building end-to-end data pipelines
Extensive SQL development experience
Knowledge of data management fundamentals and data storage principles
Data modeling:
Normalization
Dimensional/OLAP design and data warehousing
Master data management patterns
Modeling trade-offs impacting data management & processing/query performance
Knowledge of distributed systems as it pertains to data storage, data processing and querying
Extensive experience in ETL and DB performance tuning
Hands on experience with a scripting language (Python, bash, etc.)
Some experience with Hadoop, Spark, Kafka, Impala, or other big data technologies helpful

Familiarity with the technology stacks available for:
Metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Data management, data processing and curation:
Postgres, Hadoop, Hive, Impala, Presto, Spark, Glue, etc.
Experience in data modeling for batch processing and streaming data feeds; structured and unstructured data
Experience in data security / access management, data cataloging and overall data environment management

Experience with cloud services such as AWS and APIs helpful
You’d be a great fit if your current track record looks like this:
5+ years of progressive experience data engineering and data warehousing
Experience with a variety of data management platforms (e.g. RDBMS (Postgres), Hadoop (CDH, EMR))
Experience with high performance query engines (Hive, Impala, Presto, Athena, MPP engines like RedShift)
Strong capability to manipulate and analyze complex, high-volume data from a variety of sources
Effective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language
Ability to problem solve independently and prioritize work based on the anticipated business value

Qualifications

null

Additional Information

All your information will be kept confidential according to EEO guidelines."
54,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"San Francisco, CA

About Curology


At Curology, we're revolutionizing dermatology by making effective skincare accessible to everyone.
We're part telemedicine startup/part skincare lab—and completely focused on helping hundreds of thousands of people get medical care previously available to only a tiny percentage of the population.
Our consumer-facing app powers 1:1 chats with real dermatology providers and enables users to engage with a vibrant community, and our customized skincare formulation help people see life-changing improvements in their skin.
We are successful in what we do. As a business, we have grown more than 3x over the last year!

Responsibilities


Design, develop and scale data pipelines (currently using Airflow, AWS Glue, Spark, and Redshift Spectrum)
Work closely with the data science, product and marketing teams to conceive and implement new data features
Develop internal tooling for automating the deployment of data infrastructure
Manage the monitoring of our applications
Research and implement improvements to how we manage our infrastructure and data pipelines

Skills and Experience


3+ years of experience working in a Software Engineering/DevOps/Data Engineering role
Must have experience with AWS, shell scripting/UNIX, and Python
Familiarity with an ETL tools like Airflow, AWS Glue, and Redshift Spectrum preferred
Experience managing and automating AWS deployments
Previous experience in distributed systems, data processing, and analytics is a plus
Strong foundation in programming, algorithms, and software application design
Passion for solving challenging problems and iterating quickly
Passion for learning and always improving yourself and the team around you

Curology encourages applications from people of all races, religions, national origins, genders, sexual orientations, gender identities, gender expressions and ages, as well as veterans and individuals with disabilities."
55,Experienced Big Data Engineer,"San Francisco Bay Area, CA",San Francisco Bay Area,CA,None Found,None Found,"
BS in Computer Science, Statistics or Data Analytics or equivalent through experience
2+ years of experience in Scala
2+ years of experience in Cassandra
5+ years of experience as a professional software developer with increasing levels of responsibility
Experience with Big Data tools and concepts such as Kafka/Spark
Knowledge of Big Data Architectures
Experience with Big Data caching architectures
Extensive experience with noSQL and SQL
Experience with High performance queries and data filtering
Experience with Various machine learning concepts a plus
Excellent oral and written communication skills.",None Found,None Found,None Found,"
BS in Computer Science, Statistics or Data Analytics or equivalent through experience
2+ years of experience in Scala
2+ years of experience in Cassandra
5+ years of experience as a professional software developer with increasing levels of responsibility
Experience with Big Data tools and concepts such as Kafka/Spark
Knowledge of Big Data Architectures
Experience with Big Data caching architectures
Extensive experience with noSQL and SQL
Experience with High performance queries and data filtering
Experience with Various machine learning concepts a plus
Excellent oral and written communication skills.","Remote or South San Francisco Bay, California
CipherTrace is a rapidly-growing leader in the cryptocurrency analytics, intelligence and security market. Our cryptocurrency investigation, anti-money laundering, and compliance software operates on a global scale to assist banks, cryptocurrency exchanges, law enforcement agencies, regulators, ICOs and businesses as they negotiate cases involving fraud, human
trafficking, money laundering, and other illicit activity where cryptocurrency is involved. You will be working on a team based out of our Los Gatos, CA office or 100% remotely.
Description:
We are looking for a developer who is comfortable working in a big data environment using the current state-of-the-art tools of the trade and has a passion for solving big data problems. Your primary focus will be the development of efficient data pipelines and robust ETL workflows that ingest and process large amounts of data.
The ideal candidate is passionate about Bitcoin, Ethereum, Smart Contracts (Solidity, Web3) and has worked with data scientists in an effort to productize their work.
Requirements
The ideal candidate should be a passionate, self-motivated, team player with experience working in a quickly evolving environment and a who has a willingness to work and learn in a collaborative organization.
Qualifications:
BS in Computer Science, Statistics or Data Analytics or equivalent through experience
2+ years of experience in Scala
2+ years of experience in Cassandra
5+ years of experience as a professional software developer with increasing levels of responsibility
Experience with Big Data tools and concepts such as Kafka/Spark
Knowledge of Big Data Architectures
Experience with Big Data caching architectures
Extensive experience with noSQL and SQL
Experience with High performance queries and data filtering
Experience with Various machine learning concepts a plus
Excellent oral and written communication skills.
If you are interested, send your resume to contact@ciphertrace.com"
56,"Senior/Principal Data Engineer, Business Intelligence","San Francisco, CA",San Francisco,CA,None Found,None Found,"
BS in Computer Science, Engineering or a related technical role or equivalent experience
Advanced working knowledge and ability to write complex SQL and HQL queries in an HDFS environment
Extensive hands-on experience working with Python and PySpark for the purposes of data transformations and ETL
Strong familiarity with Kimball, OLAP, and EDW data design methodologies
5+ years experience in ETL, Data Engineering, or BI fields with concentration on data transformations
Understanding of various data extraction and transformation techniques with data sourced in HDFS, MongoDB, and Postgres
Working familiarity with Pentaho, Airflow, or Oozie
Familiar with Data Visualization standard methodologies
Ability to succeed in a dynamic, Agile environment
Strong prioritization and time-management skills
Dedication to team goals that include support of live 24/7 production systems
A consummate collaborator, able to establish good relationships with technical, product, and business owners
A champion of quality, able to QA and vouch for the integrity of the report output
",None Found,None Found,None Found,None Found,"Rally Health™ is all about putting health in the hands of the individual. It's our mission, and it drives everything we do, which is to empower people with easy-to-use online and mobile tools that help them take charge of their health and health care, from improving their diet and fitness to selecting health benefits, and choosing the right doctor at the right price for their needs.

Our culture is built on a deep and sincere dedication to helping people live healthier lives. To do this, we are committed to innovating continuously at every level. We know that some of the things we do are not going to work, and that's okay. We're not trying to build something that is churn and burn. We're building something that supports people over their lifetime. Every day, we get to work with amazing people on something that directly impacts the lives of millions of people for the better.

At Rally Health, we believe that two heads truly are better than one. Rallyers understand the importance of communication and collaboration, ensuring that we work we produce is the best that it can be. Every opinion is valid and valued, and we share ideas that elevate the way we work. We know that the big picture and the small details are tied together, and we keep both in mind. Everything we do is executed with our users in mind, so we make sure that all of our work has a human touch. Here at Rally, we take advantage of the opportunity to build strong relationships with each other, because it makes us better.

About the team:
Do you love data and supporting an organization that is dedicated to making it easier and more rewarding for people to handle their health and wellness? Rally's mission is to help people to become healthier every day by putting health and wellness tools and crucial information in their own hands. We are looking for expert Business Intelligence professionals to join our data team, which also includes engineers and scientists, to drive our analytics, reporting, and business intelligence.

Centralized Data Team assists internal and external stakeholders with their data needs, ranging from sending extracts to building insightful custom BI dashboards. Data Team Extracts, Transforms, and Loads data into our Enterprise Data Warehouse, utilizing various technologies, such as Hive, Spark, Python, HQL, etc. In addition, Data Team operates in HDFS environment and works with platforms such as Airflow, Databricks, and Redshift.

Your day to day:
As an integral member of our team, the Data Engineer would be responsible for the design, development, implementation, and support of critical enterprise E2E Data ETL solutions in Hadoop and/or Databricks, sourcing data from HDFS, Amazon Redshift, MongoDB, Postgres or Kafka based push pipelines and utilizing Python, Spark, and Hive. These data solutions would assist internal and external stakeholders with their tactical and strategic decisions.

Your core responsibilities:

Analyze data from consumers interactions with their healthcare insurers and providers, monitor trends and develop strategies and opportunities to improve their health and lower their costs.
Partner with employers and healthcare insurers, provide phenomenal great UI for covered members and their families to manage both their health and healthcare options
Develop actionable insights for population health management and positive recommendations for ways individuals can improve their health and manage their costs
Responsible for the design, development, implementation and support of critical enterprise E2E Business Intelligence ETL solutions in Hadoop, sourcing data from HDFS, Amazon Redshift, MongoDB or Postgres environments and utilizing Python, Spark and Hive.
Handle the product's or project's conception, design initial product specifications and lead scheduling, estimating and securing of resources
Provide technical guidance to other internal and external teams
Help to train new employees and stay ahead of industry trends and issues
Maintaining business partner engagement and setting expectations
Assessing current processes and recommending changes as needed
Documenting and communicating technical specifications to ensure that proper and optimized techniques, queries, data standards, and final outputs are understood and incorporated into data and analytics processes
Participate in business analysis activities to gather required reporting and dashboard requirements
Translate business requirements into specifications that will be used to implement the required user-friendly environments, reports and dashboards, built from potentially multiple data sources

About you:
Minimum Qualifications:

BS in Computer Science, Engineering or a related technical role or equivalent experience
Advanced working knowledge and ability to write complex SQL and HQL queries in an HDFS environment
Extensive hands-on experience working with Python and PySpark for the purposes of data transformations and ETL
Strong familiarity with Kimball, OLAP, and EDW data design methodologies
5+ years experience in ETL, Data Engineering, or BI fields with concentration on data transformations
Understanding of various data extraction and transformation techniques with data sourced in HDFS, MongoDB, and Postgres
Working familiarity with Pentaho, Airflow, or Oozie
Familiar with Data Visualization standard methodologies
Ability to succeed in a dynamic, Agile environment
Strong prioritization and time-management skills
Dedication to team goals that include support of live 24/7 production systems
A consummate collaborator, able to establish good relationships with technical, product, and business owners
A champion of quality, able to QA and vouch for the integrity of the report output

Preferred qualifications:

Knowledge of Scala

Why join Rally? On top of an innovative work atmosphere and a chance to help people change their lives, we offer competitive pay, daily catered lunches, and an extensive benefits ( https://audaxhealth.app.box.com/s/1plpv5iki7j54za76uxn4uzqq579nqxk/file/480621545411 ) package for all full-time employees (including medical, dental, vision and 401(k)). In addition, offer the ability to grow, while truly making an impact in the healthcare system.

Rally knows that we are strongest when our employee population reflects the diversity of the world around us, and we are a place where all voices are valued. A diverse workforce will enrich us with the talent, energy, perspective and inspiration we need to achieve our mission. Rally Health believes in a policy of equal employment and opportunity for all people. It is our policy to recruit, hire, train, and promote individuals in all job titles, and administer all programs, without regard to race, color, religion, national origin or ancestry, citizenship, sex, age, marital status, pregnancy, childbirth or related medical conditions, personal appearance, sexual orientation, gender identity or expression, family responsibilities, genetic information, disability, matriculation, political affiliation, veteran status, union affiliation, or any other category protected by applicable federal, state or local laws.

Individuals with disabilities and veterans are encouraged to apply. Applicants who require an accommodation related to the application and/or review process should notify Talent Acquisition (recruiting@rallyhealth.com ( recruiting@rallyhealth.com )).

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
57,Senior Data Engineer - Java,"San Francisco, CA 94108",San Francisco,CA,94108,None Found,None Found,None Found,None Found,None Found,None Found,"and grow your career.
Autonomy, flexibility and a flat corporate structure.

ABOUT PLUM
After decades of experience in institutional financial services, the founders of Plum recognized the need to challenge the status quo which makes the commercial real estate (CRE) lending process opaque and cumbersome. Plum is a more agile and advisory financial technology company that combines data intelligence with best-in-class financial expertise to modernize CRE lending.

Our CEO, Bill Fisher, has decades of experience building successful startup businesses, including GetSmart.com, Xing and Trivago. Our team includes senior leaders and talent from AIG, Goldman Sachs, KKR, McKinsey, PWC, Bank of America, Meridian Capital, A10, JP Morgan, Freddie Mac, US Bank, Wells Fargo and PNC Real Estate.

In April 2018, Plum announced a Series B equity investment by the $35 billion hedge fund, Elliott Management. Plum’s Series A round in August 2015 was led by Renren Inc, who has backed other fintech companies including SoFi, LendingHome and Motif Investing. This followed an earlier seed investment by QED Investors, a pre-eminent VC firm led by the founders of Capital One, whose portfolio includes Prosper, Orchard and ApplePie Capital.
Plum is headquartered in the heart of San Francisco’s Financial District in an airy, industrial loft, close to all forms of public transportation.

Women and minorities are encouraged to applyNo Relocation"
58,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Rocket Lawyer
We believe everyone deserves affordable and simple access to legal services.
Rocket Lawyer is one of the largest and most widely used online legal service platforms in the world. Rocket Lawyer has helped over 20 million people create over 3 million legal documents, and answer over 30,000 legal questions.

We are in a unique position to enhance and expand the Rocket Lawyer platform to a scale never seen before in the company’s history, to capture audiences worldwide. We are expanding our engineering team to take on this challenge!

About the Role
Rocket Lawyer is looking for a Senior Data Engineer that will contribute in all aspects of creating an analytical data driven environment. The core data engineering team is responsible for the building out the data pipeline, gathering internal and external data, generating metrics, managing and monitoring batch and streaming jobs, and implementing analytical tools to drive strategic decision making.
A Day in the Life
Evangelize Modern Big Data Practices
Design warehouse schemas that accurately represent our business, and facilitate analysis and building of reports
Help build batch and streaming data ingestion pipeline using Hadoop, Hive, Pig, Storm, and Kafka Streams
Write ETL jobs to transform raw data into business information to drive decision making
Develop analytical environment using internal and external reporting tools
Integrate internal and external data with warehouse and external tools
Experience
Excellent technical skills including expert knowledge of the Hadoop ecosystem
Experience of the analysis, design and development of Data Warehouse and Big Data solutions, including analyzing source systems, developing ETL design patterns and templates, ETL development, data profiling and data quality issues resolution.
Experience of team and technical leadership
Excellent communication skills and presentation skills
Expert Level experience of the The Hadoop System
Strong SQL, Java, and Python skills
Database (relational & NoSQL), Data Warehouse knowledge
Stream processing experience (Storm, Kafka Streams)
Passion and enthusiasm for learning new technologies and technique
Comfortable with Linux
BS or MS in computer science
Detail oriented and organized
Desire to learn broad set of technologies
Benefits and Perks
Fully paid Medical, Dental and Vision insurance for full-time employees
Unlimited PTO; take time when you need it and come back refreshed
Competitive salary packages
Commuter/Transit Program
401k program
Flexible Spending Accounts (Medical and Dependent Care)
Life insurance and disability benefits
Your choice of a Mac or PC
Monthly onsite masseuse sessions
“Red Friday” catered lunches
Company sponsored events, both on- and off-site"
59,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"------------
About Wellio
------------

We believe that by using technology, collaboration, innovation, and diversity, we can transform the way people eat and cook.

We are a growing foodTech company with the goal of empowering families to eat healthier meals. We are building an AI platform that uses advanced machine learning algorithms and statistical techniques to both generate personalized meal recommendations and reinforce better eating habits.

To achieve our goal, we have assembled a team of smart, empirically motivated, disagreeable givers who care about making a difference in the world.

As a member of our data pipeline and operations team team, you'll work with other data engineers, data scientists, and product managers to enable the highest quality data products in our Food Intelligence Platform.

----------------
What you will do
----------------


Collaborate with a team of data scientists, data engineers, and product managers to develop food-related data products.
Design, build, improve, and maintain a performant and highly scalable data platform to support both internal and external use cases. We give broad leeway to implement your best architectural decisions. Use cases for this core data include:
two-way flow of data to user-facing apps via microservices,
structuring both core data and data exhaust for training machine learning models.
Work with our data scientists to design and build a models platform that leverages and supports online learning and machine teaching to develop industry-leading models in the food and health space.
Work with data scientists and engineers to build a CI/CD system for our ever-improving models that allows their performance to be monitored and improved in an automated manner

--------------------
Basic qualifications
--------------------


You are proficient in writing production-quality code (preferably in Python) and in software design best practices (minimum 3 years experience)
You are familiar with the software development lifecycle and interested in helping to maintain high quality software through CI/CD, TDD, and code reviews.
Experience designing, building, and maintaining RESTful APIs
You previously worked with GCP, AWS, or another PaaS
You have previously worked with data scientists and have experience integrating machine learning models into production code (minimum 1 year experience)
Experience with relational and non-relational databases
Experience in system design and architecture

------------------------
Preferred qualifications
------------------------


Experience building both streaming and scheduled data pipelines (experience with Airflow/Cloud Composer is a plus)
Experience with containers and orchestration (e.g., Docker, Kubernetes)
Experience with serverless technologies (e.g., Cloud Run, Amazon Fargate, Lambda Functions)

---------
About you
---------


You enjoy collaborating as part of a small, agile, and dynamic team where everyone is valued
You believe ""done is better than perfect"" and enjoy helping to ship features and quantify the right areas for iterative improvement.
You enjoy both learning and teaching, and enjoy working with others from whom you can learn and who can learn from you.
You have a passion for food and nutrition, and are excited to use your skills to help our customers nourish themselves
You believe that work should be fun and enjoy working with others
You like to try new things and believe in failing fast
You like chocolate, especially dark chocolate

If you got this far, perhaps you're the person we're looking for. We look forward to your application."
60,Azure Data Engineer,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,"At least 5 years of consulting or client service delivery experience on Azure
",DevOps on an Azure platform,None Found,None Found," Proven ability to build, manage and foster a team-oriented environment
","Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
61,Senior Data Engineer (SF),"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,"Experience working with Python/Pandas in a production environment.
Experience working with SQL and NoSQL data stores such as PostgreSQL & Redis
Experience working with AWS services include RDS, S3, and EC2
Experience working in Agile teams.
Very strong technical and analytical skillset (R, Python, SQL, Tableau, Predictive Analytics, Forecasting, Data Mining, Machine Learning, Text Analytics, Algorithm Designs, Bayesian Methods etc.)",None Found,None Found,None Found,"Love live music? Want to be part of one of the few tech startups currently making a real impact on the music industry?
At Lyte, we help fans get into their favorite events, with a ticketing experience that doesn't suck - no scalpers or fake tickets.
Want to watch the software you build immediately get put to use at the highest levels of the music industry? If there is popular music event or festival that is sold out, we are most probably close partners, think the biggest names in the industry! To name a few, Coachella, Burning Man, Electric Forest, Newport Folk Festival, AfroPunk, and BottleRock. Don't see your favorite event or artist here yet? Help make it a reality.
Want to join a team of hard working, fun loving people who are still a little weird and know how to both ship rock solid code and not take life too seriously? The Lyte engineering team is scrappy crew of technology professionals who have experience across the early stage startup realm.
In addition to being technically savvy, we love music and events and when we are not pushing code, we are actively trying to have fun in life. Some of our team members are in bands, and some just like to play flute by themselves out in the mountains.
Qualities that we are looking for in our team:
Passionate about building predictive models and analyzing complex datasets
Ability to collaborate closely with back-end and product teams to deliver a holistic experience that's optimized around providing the best user experience possible.
Able to find simple solutions to complex problems, excel at solving real issues for real people and want to push the boundaries of their engineering abilities.
What you will have ownership for delivering:
Help drive the design and architecture of algorithmic services to improve prediction capabilities
Using NLP (Natural language processing) to process and match messy datasets
Researching, building and developing statistical machine learning models and recommendation systems.
Collaborating with product management and engineering department to understand company needs and devise possible solutions.
Provide leadership to more junior software engineers and data scientists
Skills needed to be successful:
Experience working with Python/Pandas in a production environment.
Experience working with SQL and NoSQL data stores such as PostgreSQL & Redis
Experience working with AWS services include RDS, S3, and EC2
Experience working in Agile teams.
Very strong technical and analytical skillset (R, Python, SQL, Tableau, Predictive Analytics, Forecasting, Data Mining, Machine Learning, Text Analytics, Algorithm Designs, Bayesian Methods etc.)"
62,Senior Data Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"WePay’s API’s provide a flexible and robust payments system that seamlessly integrates into the software platforms of our customers while shielding them from fraud, so they can deliver the end-to-end user experiences they want without taking on the overhead they don’t want.

We are looking for an exceptional Senior Data Engineer to join our high-performing BI & Analytics team. Your work will focus on developing the data platform to support WePay’s analytics and data science capabilities and making data accessible to the entire company for enhanced data driven-decision making. You will contribute significantly to the vision of our analytics and data environment by using the cutting-edge cloud and big data technologies.
What You Will Do
Design and build analytics data marts and data warehouse for high-performance analytics, and provide ongoing optimization and enhancements
Partner with internal stakeholders to understand business requirements and develop data solutions to address their needs
Work with engineering team to ensure a scalable and streamlined process to extract data from the production environment into the analytics warehouse
Develop reporting and visualization tools using in-house tools as well as third-party tools
Define performance metrics and develop dashboards to evaluate business performance
Conduct ad hoc analyses to support key corporate initiatives
What We Are Looking For
Bachelor’s or Master’s degree in computer science or relevant technical field
MUST have 4+ years of industry experience as a Data Engineer, Business Intelligence Engineer or Data Scientist
4+ years experience in Python or Java
4+ years of experience in MySQL, NoSQL, BigQuery
2+ years experience in some of the following: Hadoop, Airflow, Kafka, Spark
Experienced with Tableau, Chartio or other BI tools is preferred
Fast-learner; ability to adapt to new development environments
Detailed-oriented with strong organizational skills
Creative problem solvers with can-do attitude
Excellent communication skills
About WePay

WePay’s mission is to make commerce seamless. Our products help software companies integrate payments into their applications – thereby empowering small businesses and individuals to get paid easily and quickly using their go-to apps and software. Our customers include BigCommerce, GoFundme, Meetup and Freshbooks, just to name a few. By joining forces with JPMorgan Chase, a global financial services firm with over $2.5 trillion in assets that serves millions of customers worldwide, WePay is now able to connect our customers seamlessly into a range of banking services beyond payments. WePay is a unique place to work and offers the best of both worlds. WePay has a FinTech startup culture that emphasizes transparency, collaboration and career growth, with the ability to work on small, nimble teams. However, now combined with the power of JPMorgan Chase, employees are also able to create change at scale and have an opportunity to truly disrupt and shape FinTech.

You can find more information at wepay.com

To all recruitment agencies: WePay does not accept agency resumes. Please do not forward resumes to our jobs alias, WePay employees, or any other company location. WePay is not responsible for any fees related to unsolicited resumes."
63,"Senior Data Engineer, Data Ocean Platform","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,"
You will perform a key role in the evolution of a highly performant data platform and associated services, applying industry standards to enable highly available, extensible data services for the SIE platform
You will play a senior role in design, development and quality related tasks, working closely with other teams.
You will design and develop internal tools to increase awareness of Sony's Data Ocean to improve both the visibility of data in the ocean as well as establishing systems and procedures make it easier to consume
You will be part of a team creating transformation processes (ETL) to reshape data so that it can be optimally consumed by data lake users.
This position requires extensive hands-on technical domain expertise in data infrastructure along with excellent communication skills
You will develop test scripts and scenarios to verify that your code meets Sony standards.
You will participate in product road-map discussions and identify key areas for improvement.
Willing to do what others do not do.
Be Amazing
","PlayStation isn't just the Best Place to Play —it's also the Best Place to Work. We've thrilled gamers since 1994, when we launched the original PlayStation. Today, we're recognized as a global leader in interactive and digital entertainment. The PlayStation brand falls under Sony Interactive Entertainment, a wholly-owned subsidiary of Sony Corporation.

Senior Data Engineer, Data Ocean Platform

San Francisco, CA

This is a senior level position with SIE's Data Ocean Platform engineering team to provide services and improvements within our next generation data platform serving the global, fast growing, PlayStation Network customer base.

In the Data Ocean team we socialize ideas, multi functionally, to support the growth and usability of the platform as well as to support teams within the SIE organization.

Let's talk, if:
---------------


You are passionate about making distributed systems better and have desire to continually learn.
You have deep expertise in system engineering and have successfully improved system performance.
You have experience tackling complicated issues.
You have implemented rigorous standard methodologies for measuring, understanding, predicting, and improving the system.
You are willing to build a strong partnership with other SIE teams to help influence technology decisions.
You have aggressively contributed to a development process establishing and influencing quality engineering standards.
You enjoy developing new processes to refine the platform, increasing performance and reducing costs.

Requirements:
-------------


You will perform a key role in the evolution of a highly performant data platform and associated services, applying industry standards to enable highly available, extensible data services for the SIE platform
You will play a senior role in design, development and quality related tasks, working closely with other teams.
You will design and develop internal tools to increase awareness of Sony's Data Ocean to improve both the visibility of data in the ocean as well as establishing systems and procedures make it easier to consume
You will be part of a team creating transformation processes (ETL) to reshape data so that it can be optimally consumed by data lake users.
This position requires extensive hands-on technical domain expertise in data infrastructure along with excellent communication skills
You will develop test scripts and scenarios to verify that your code meets Sony standards.
You will participate in product road-map discussions and identify key areas for improvement.
Willing to do what others do not do.
Be Amazing

Qualifications
--------------


BS Degree in Engineering, Computer Science or equivalent experience.
5+ years' experience in software development using Java, Scala or Python programming, design, and analysis.
Experience with distributed processing systems such as Apache Spark.
Experience delivering high performance, active-active, scalable services.
Strong foundation in data engineering, data structures and software design.
Experience with storage formats such as, Parquet / ORC / AVRO
Experience with a hive compatible meta store catalog such as AWS Lake Formation, Glue, Apache Atlas, Netflix's Metacat.
Interest in working with the data science and machine learning teams.
Hands-on experience in cloud-based web services, at enterprise scale, AWS is preferable.
Experience with agile TDD development methodologies.
Possess the drive and passion for quality with the ability to inspire, excite and motivate other team members.
Strong verbal and written communication skills, and be able to work with others at all levels, effective at working with geographically remote and culturally diverse teams.

Sony is an Equal Opportunity Employer. All persons will receive consideration for employment without regard to race, color, religion, gender, pregnancy, national origin, ancestry, citizenship, age, legally protected physical or mental disability, covered veteran status, status in the U.S. uniformed services, sexual orientation, gender identity, marital status, genetic information or membership in any other legally protected category.

We strive to create an inclusive environment, empower employees and embrace diversity. We encourage everyone to respond.

We sincerely appreciate the time and effort you spent in contacting us and we thank you for your interest in PlayStation.

#LI-CD1"
64,eCommerce Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
BS or MS degree in Computer Science or a related technical field
4+ years of Python experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale
Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases.
Experience with specific AWS technologies (such as Glue, S3, Redshift, EMR, and Kinesis) a plus",None Found,None Found,None Found,"
BS or MS degree in Computer Science or a related technical field
4+ years of Python experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale
Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases.
Experience with specific AWS technologies (such as Glue, S3, Redshift, EMR, and Kinesis) a plus","The next great shift in consumer behavior driven by technological disruption is underway, and it’s happening in the food & beverage industry.

As other sectors have shifted to eCommerce-first business models in recent years, food & beverage has continued to rely predominantly on traditional brick & mortar models, but this is changing rapidly. New technologies are transforming every aspect of reaching consumers, from the rise of digital marketing and online grocery platforms, to the creation of supply chain tools that enable speedy at-home delivery. According to the Food Marketing Institute, 70% of U.S. consumers will shop online for groceries by 2024, with an estimated annual spend of $100 billion.

To deepen our efforts to seize this opportunity and lead the food & beverage industry into its remarkable next chapter, PepsiCo – a global company with powerhouse brands including Frito-Lay, Gatorade, Pepsi-Cola, Quaker, and Tropicana – is expanding its Global eCommerce Team. We’re looking for the greatest minds in data & analytics, software development, machine learning optimization, and next-generation supply chain. Given PepsiCo’s incredible reach – our foods and beverages are enjoyed more than one billion times a day in more than 200 countries and territories, and our value chain involves diverse partners ranging from farmers and food scientists to retailers and logistics specialists – the challenges we’re addressing are complex and the solutions will be deeply impactful.

Although PepsiCo is a large multinational, the PepsiCo Global eCommerce Team prides itself on having the entrepreneurial, action-oriented culture of an exciting startup business. Our group includes startup founders, Silicon Valley veterans, food & beverage experts, and seasoned executives from digital transformation leaders such as Amazon and Walmart. Our goal is to build the technological products and capabilities that will reinvent our industry and make us the #1 food & beverage business in eCommerce for decades to come

This role is to build very large, scalable platforms using cutting edge data technologies, from the ground up and plan to leverage the most recent Big Data technologies, working closely with product engineering teams and data scientists to tackle problems in personalization, content discovery, search, advertising and content production, taking machine learning models in production and enabling data science research and analysis even amongst non-scientists in the company.
Design and implement data infrastructure and processing workflows required to support data science, machine learning, BI and reporting in AWS
Build robust, efficient and reliable data pipelines consisting of diverse data sources
Design and develop real time streaming and batch processing pipeline solutions
Own the data expertise and data quality for the pipelines
Drive the collection of new data and refinement of existing data sources
Identify shared data needs, understand their specific requirements, and build efficient and scalable pipelines to meet various needs
Build data stores for feature variables required for machine learning
Qualifications/Requirements
Preferred Qualifications:
BS or MS degree in Computer Science or a related technical field
4+ years of Python experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale
Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases.
Experience with specific AWS technologies (such as Glue, S3, Redshift, EMR, and Kinesis) a plus
Role can also be based in our NYC location
Relocation Eligible: Not Applicable
Job Type: Regular

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity

Our Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901 - 4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance.

If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy

Please view our Pay Transparency Statement"
65,Data Scientist/Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"POSITION SUMMARY

Aktana is looking for an experienced Data Scientist/Data Engineer to join our Data Science team in San Francisco. The Data Science team builds mathematical and behavioral economics models underpinning Aktana’s core solutions for actionable analytics and behavioral intelligence. The set of problems that we tackle is incredibly diverse and complex. They cut across optimization, prediction, modeling, inference, and behavioral science. We research and develop the algorithms and models that make our solution intelligent, as well as implementing, scaling and maintaining the code that powers our production systems.

The ideal candidate is a critical thinker with experience and background in commercial life science domain, passionate about solving mathematical and behavioral problems with data, and is excited about working in a fast-paced, innovative and collaborative environment.

LOCATION

San Francisco, CA

REPORT TO

Chief Science Officer

RESPONSIBILITIES

Building and maintaining data pipelines that are crafted with model building as a primary driving factor. Work with Data Scientists, Product Managers, Customer Success and Services teams to frame a problem, both mathematically and within the business context

Write production-level code; collaborate with Engineering team to implement algorithms in production and productize common solutions

Perform exploratory data analysis to gain a deeper understanding of the problem

Construct and fit statistical, machine learning and optimization models

Utilize commonly used computing and database environments to get the data that you need and implement a working prototype of the formulated model.

Learn and apply new methodologies in the intersection of applied math/ probability/statistics/machine learning/computer science.

Make intelligent approximations to the model if required to make it scalable.

Analyze experimental and observational data; communicate findings; facilitate launch decisions

Identify data sources that could be used to test assumptions

REQUIRED SKILLS/EXPERIENCES

Proven track record of developing algorithms for production-ready recommendation or prediction systems using languages and big data platforms such as Scala, Python, R, Java, Spark, Cassandra, and Hadoop.

Experience working in an agile software development environment.

Passion for solving unstructured and non-standard mathematical and behavioral problems.

End-to-end experience with data, including querying, aggregation, analysis, and visualization.

Experience implementing machine learning algorithms.

Experience with analytics for commercial life science and pharma-specific analyses a big plus.

Excellent communication and presentation skills, being able to explain complex problems and the solutions applied, feeling comfortable in being part of the sales process, supporting the sales team, engaging with customers and presenting technical solutions to a nontechnical audience.

Experience in data science and data analytics for life science industry especially in GTM strategy and execution a big plus

High-energy self-starter with a passion for your work, attention to detail, and a positive attitude

Great team player, willingness to collaborate and communicate with others to solve a problem.

EDUCATION

M.S. or Ph.D. in Statistics, Operations Research, Mathematics, Computer Science, or other quantitative disciplines with at least 2 years of experience


ABOUT AKTANA

Committed to customer success and innovation, Aktana is at the forefront of transforming how life sciences companies share information with healthcare providers (HCPs). Our proprietary platform harnesses machine learning algorithms to enable marketing and sales teams to seamlessly coordinate and optimize multichannel engagement with HCPs. Today 10 of the top 20 global biopharmaceutical companies are using Aktana to go to market smarter.

Aktana is growing fast and looking for exceptional talent to join our team. We value hard work, transparency, and collaboration – and we like to have fun too! Headquartered in San Francisco, we have offices around the world, including Philadelphia, New York, London, Sydney, Sao Paolo, Barcelona, Tokyo, Osaka, and Shanghai.


Linked In Category: (Up to 3)"
66,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Data is our fuel at Turo. It is ever-more abundant and valuable, but it's a raw material. Harnessed by data scientists and machine learning engineers, it propels Turo on its mission to put the world's 1.5+ billion cars to better use, delighting our customers with matching the right car for their next adventure from an exceptionally diverse selection, and at the same time helping our marketplace remain safe.

About You

At Turo, you will have the opportunity to use the latest technologies to build robust scalable solutions for collecting, analyzing large data sets, creating and maintaining data pipelines, data structures and reports. In this role, you'll partner closely with software engineers, data analysts, and data scientists to power analytical data products, experimentation, and machine learning models.

Responsibilities


On a daily basis, you will work with members of the team to update our data engineering roadmap and execute upon those initiatives
Build new technology stack for highly scalable and available data pipelines used by Turo Product, Engineering, Data Scientists, Marketing, Customer Operations, and Finance teams
Design canonical data models for various business domains. Formulate a vision to connect all data in the Turo ecosystem
Develop, deploy and maintain workflow management tools such as Airflow, Jenkins etc in cloud environments.
Develop, deploy and maintain streaming solution such as Apache Spark streaming, Kafka and Apache Flink in cloud environments.
Using cloud technology such as AWS, Kubernetes, Docker, Redshift, EMR
Data security automation
Data microservices development

Requirements


Past experience building ETL processes
Strong programmer who views their code as a craft
Experience with a workflow manager — Airflow, Luigi, Jenkins, etc.
Experience working with data tools in the public cloud (AWS, GCP, Azure)
Able to understand technical details and communicate with other engineers, as well as communicating with less technical members from other teams.
Enjoys mentoring & teaching other engineers
3+ years of relevant experience

Benefits


Competitive salary and equity for all full-time employees
Employer paid medical, dental, and vision insurance
Generous paid time off, paid holidays, paid volunteer time off, and paid parental leave
Weekly catered lunch with a fully-stocked kitchen
Company-sponsored happy hours and team events
Turo host matching and vehicle reimbursement program
Turo travel credit every month

About Turo

Turo is the world's largest peer-to-peer car sharing marketplace where you can book any car you want, wherever you want it, from a vibrant community of trusted hosts across the US, Canada, the UK, and Germany. Guests choose from a totally unique selection of nearby cars, while hosts earn extra money to offset the costs of car ownership. A pioneer of the sharing economy and the travel industry, Turo is a safe, supportive community where the car you book is part of a story, not a fleet. Discover Turo at https://turo.com ( https://turo.com/ ), the App Store, and Google Play, and check out our blog, Field Notes ( https://blog.turo.com/ ).

Turo has raised $450M to date from top-tier investors, including IAC, Daimler AG, Kleiner Perkins, GV, Canaan Partners, August Capital, and Shasta Ventures.

Turo cultivates a tight-knit team of smart, critical thinkers who care about their work and their colleagues. Our recruiting team is always on the lookout for supportive, down-to-earth, pioneering, and efficient candidates to grow our team's talent and enrich our culture.

Read more ( https://medium.com/@andre_haddad/connecting-the-dots-to-a-compelling-not-cultish-company-culture-35dc871cba08 ) about the Turo culture according to Turo CEO, Andre Haddad.

We're an equal opportunity employer and value diversity at our company. We don't discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, or disability status. When in doubt, please apply!"
67,"Software and Data Engineer, Music Works","Emeryville, CA",Emeryville,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Explore and Discover Nielsen! With offices located in 110 countries, we are a global independent measurement and big data analytics company focused on your future.

Gracenote, a Nielsen Company is the leading provider of entertainment metadata and media recognition technology that powers discovery features for top TV, music, sports and automotive platforms.

We are presently looking for a Software and Data Engineer to join our Music Works team. We’re looking for people with great programming skills that are quick learners, self-starters, and take ownership of their work.

This Music Works team is responsible for building a system that provides an industry wide source of truth for musical works and associated recordings. We seek someone who is passionate about using the best software processes and tools to help the music industry solve some of its most important problems.

IN THIS ROLE YOU WILL:
Elaborate detailed designs from high-level architecture
Create reusable software building blocks that can be assembled into applications, microservices, and APIs to support platform capabilities
Collect and analyze data to draw insights and identify solutions for our Music Works product
Build data ETL systems
Collaborate with and mentor team members to develop coding standards and design patterns

WHAT DO WE OFFER…

The chance to work alongside a highly skilled technical team allowing you to come up with designs, implement them, and solve interesting software problems
A welcoming atmosphere that will instantly make you feel comfortable within our team-oriented and fun environment
To be a part of a company that invests in programming tools that will make the development experience easier
A creative, musical culture featuring employee concerts and performances year round

FOR THIS ROLE WE ARE LOOKING FOR INDIVIDUALS THAT HAVE:
A Computer Science degree or equivalent required
8+ years experience working with the following technologies
Java, C#, or Go are desirable to transition from data analytics to development
Some relational or non-relational database system
Working as a member of a team of software developers
3+ years experience directly dealing with data analysis and/or data ETL
Fluent with SQL, no-SQL architectures
Experience with Cloud (AWS)
Machine Learning (Python or R)
Ability and passion for learning new technology
Teamwork and team building skills that will contribute directly to team success on assigned projects

#LI-GN

Gracenote, a Nielsen company, is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class"
68,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"At Afresh, we’re solving the big problems around food waste and the fresh food supply chain-focusing first in grocery. We use cutting-edge AI (we’ve been published in ICML!) combined with thoughtful design to enhance decision-making and optimize store workflows. The results are powerful: in live deployments in grocery stores, we have demonstrated the potential to double profits and reduce food waste by 50%+!
What will you be doing?
Building fast and reliable data pipelines that enable training machine learning models over billions of historical data points collected from tens of thousands of retail stores across the US.
Integrating with our customers’ infrastructure, which includes a variety of contemporary and legacy IT systems. Setting up data feeds from the customers’ systems.
Maintaining transactional, analytic, and NoSQL databases and data lakes over terabytes of data.
Collaborating with an interdisciplinary team of experts in machine learning, data scientists, design, software engineering, and business process optimization
What skills and experience do you need?
Bachelors or Masters in Computer Science or equivalent.2+ years of work experience.
Strong programming and problem-solving skills.
Expert-level knowledge of databases, data lakes, data pipelines, SQL. Experience with big data frameworks such as Apache Spark is a big plus.
Experience working with Microsoft Azure, Amazon Web Services, and other cloud providers.
Familiarity with statistical concepts and/or devops expertise are a plus. Experience with data visualization is a plus.
Background

About 30-40% of food produced worldwide is thrown away, causing nearly a trillion dollars of economic losses, trillions of gallons of wasted water, and billions of tons of greenhouse gas emissions. In the US, about 40% of all food waste occurs at the retail level and downstream, largely driven by insufficient technology and manual processes.

Afresh seeks to tackle some of these big problems around food waste. Born out of Stanford's Computer Science PhD program, Afresh is the first Fresh food supply chain company. We bring the cutting edge of artificial intelligence to Fresh food to minimize food waste.

Our machine learning-powered supply chain solutions are tailored for the nuances of perishables. Our first product is a store-level replenishment tool that optimizes the ordering of items in Fresh categories - produce, meat, deli, dairy, bakery, and prepared foods. The goal is to minimize waste and maximize in-stock rate, and consequently, profit.

So far, the results are awesome! Like we said above, in live deployments, we have demonstrated the potential to double profits and reduce food waste by 50%+.

We're growing fast: we're in partnership with 4 large regional grocers representing 500+ stores and >$10B in revenue. Our backers include Innovation Endeavors (former Google CEO Eric Schmidt’s firm) and Baseline Ventures (first money in Stitchfix, SoFi, Heroku, Instagram).

Interested? Email us at Careers@afreshtechnologies.com"
69,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Honor is now one of the fastest-growing, non-medical home care companies in the U.S. Why? Because we realized that by combining our amazing technology and operations with the local, personal touch of our partner agencies, we could make real progress in transforming this fast-growing, $30BN industry. This unique approach is powering our huge growth - we have cutting-edge machine learning, a beautiful, well-designed app, and industry-leading design, paired with a strong sales, marketing, and support engine.

We're looking for passionate developers to join our data team. Your work will directly impact the lives of people receiving care, people delivering care, and the families that are normally left in the dark about the entire care process. Every part of our business is hungry for analytics. As a key part of a small data team, you'll have the chance to help shape our analytics infrastructure.

In this role, you will:

Design and implement systems for capturing and storing data
Plan and maintain data lakes and data warehouses
Build and maintain ETL pipelines
Design and implement ETL infrastructure
Collaborate closely with data scientists, analysts, and the entire product team

The ideal candidate will have:

Expert-level SQL knowledge
Hands-on experience with analytics databases
Production coding experience
Working knowledge of Python
3+ years of industry experience with data processing systems
Experience with AWS or comparable cloud computing platforms

Honor is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy), national origin, age, disability, genetic information, political affiliation or belief."
70,Cloud Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"31 May 2018
As a Cloud Data Engineer, you will guide customers on how to ingest, store, process, analyze and explore/visualize data on the Google Cloud Platform, AWS and Azure. You will work on data migrations and transformational projects, and with customers to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential platform issues. In this role you will be the main Data Engineer working with DB Best’s most strategic cloud customers. Together with the team you will support customer implementation of Google Cloud, AWS or Azure products through: architecture guidance, best practices, data migration, capacity planning, implementation, troubleshooting, monitoring and much more.
REQUIREMENTS
Act as a trusted technical advisor to customers and solve complex Big Data challenges
Create and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders
Travel up to 30% of the time
Communicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities
Minimum qualifications
BA/BS degree in Computer Science, Mathematics or related technical field, or equivalent practical experience
Experience with data processing software (such as Hadoop, Spark, Pig, Hive) and with data processing algorithms (MapReduce, Flume)
Experience working with technical customers
Experience in writing software in one or more languages such as Java, C++, Python, Go and/or JavaScript
Preferred qualifications
Experience working data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments
Experience in technical consulting
Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments such as Amazon Web Services, Azure and Google Cloud Platform
Experience working with big data, information retrieval, data mining or machine learning as well as experience in building multi-tier high availability applications with modern web technologies (such as NoSQL, MongoDB, SparkML, Tensorflow)"
71,Senior Data Engineer,"San Mateo, CA 94401",San Mateo,CA,94401,None Found,None Found,None Found,None Found,None Found,"
A BS or MS degree in Computer Science or Computer Engineering, or equivalent experience, with a strong record of academic achievement
5+ years experience with data engineering in Java and / or Scala
Experience with Spark and other big data technologies, large-scale data processing and ETL pipelines
Experience with SQL and relational databases, such as Postgres
Experience with developing and deploying applications on Linux infrastructure in AWS
Enthusiasm about working in a small, entrepreneurial organization","Motif Capital is modernizing fundamental investment research. We combine traditional and non-traditional data sources, containing structured and unstructured data, deriving quantitative insights to power our clients' long-term investments. We are a small team, dedicated to scaling through automation in order to keep our clients' money invested in their portfolios, not their advisor.
As a Senior Data Engineer, you will be a key driver of our data and analytics platform. Not only will you develop and operate our data infrastructure, you will work closely with analysts to gain understanding of the underlying data and structure it for ease of analysis and use. You will search out new and interesting data sources, which when combined with our existing, proprietary research will generate unique investment insights. This data will feed our analytics and machine learning pipelines, to continuously update and adapt our portfolios to changing micro- and macro-economic conditions.
Job Requirements:
A BS or MS degree in Computer Science or Computer Engineering, or equivalent experience, with a strong record of academic achievement
5+ years experience with data engineering in Java and / or Scala
Experience with Spark and other big data technologies, large-scale data processing and ETL pipelines
Experience with SQL and relational databases, such as Postgres
Experience with developing and deploying applications on Linux infrastructure in AWS
Enthusiasm about working in a small, entrepreneurial organization
Bonus points:
Experience with financial datasets, knowledge of markets and investments
Firm grasp of applied statistical concepts and techniques including probability, regression, and time series analysis
Experience with Numerical / Scientific Python (NumPy, SciPy, Pandas)
Knowledge of machine learning pipelines using Scala / Spark or Python / scikit-learn
About Motif Capital:
Motif Capital Management is a next-generation global equity investment manager that specializes in the management of thematic investment strategies for financial institutions such as private wealth management, investment companies, endowments, and family offices. Our unique disciplined, scientific, and transparent approach to thematic investing relies on combining data-driven insights with objective fundamental research, algorithmic portfolio design and cutting-edge technology & analytics. Our goal is to work with our institutional partners to act on the economic, socio-political, and technological forces that are shaping the global economy for the benefit of their clients’ portfolios. Learn more at www.motifcapital.com.
Motif Capital Management, Inc. is an SEC registered investment advisory firm located in San Mateo, California. The company is privately held and a wholly owned subsidiary of Motif Investing Inc.

About Motif Investing:
Motif Investing is an online broker-dealer that is transforming the way retail investors invest and manage wealth. Motif Investing offers self-directed investors a concept-driven trading platform that enables them to trade “motifs”—intelligently-weighted basket of stocks and ETFs built around themes, investing styles or multi-asset models— for low fees. Based in San Mateo, the company's investors include Goldman Sachs, JPMorgan Chase, Balderton Capital, Renren, Foundation Capital, Ignition Partners, Norwest Venture Partners, and Wicklow Capital, with notable board members including former SEC Chairman Arthur Levitt and former Boston Consulting Group CEO Carl Stern. Learn more at www.motifinvesting.com."
72,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About us:

Want to infuse a $25B sector of the insurance and real estate industry with predictive analytics and a tech-forward customer experience? States Title is intelligently transforming closings by applying machine intelligence to the age-old processes and procedures in the $25B Title and Settlement industry. Our streamlined, efficient algorithms have revolutionized the title and escrow process and allowed us to scale rapidly. We are poised to transform this industry, repurposing the billions wasted in rote, manual tasks to make homeownership easier and less risky, helping people invest time and money into more meaningful parts of their lives.

You’re fired up to:

Design, build, and maintain data pipelines for extraction, transformation, and loading of data from a wide variety of data sources to various data services.
Build a next-generation enterprise data lake with raw production data as source of truth and always-on, versioned data pipelines.
Document data sources, data pipelines, and data infrastructure to share knowledge and understanding of the solutions being implemented.
Work with teams across the organization to assist with data-related technical issues and support their data infrastructure needs.
You definitely have:

Experience with scripting languages, particularly Python.
Experience with cloud services, particularly Azure: Blob Storage, VMs, Data Factory, SQL Data Warehouse, HDInsight, etc.
Ability to write complex SQL joins in your sleep and experience with stored procedures, triggers, etc.
Experience with the command line for Linux systems (also preferably Windows).
Knowledge of database modeling and data warehousing concepts.
You might even have:

Experience with big data platforms and tools like Spark, Hive, Presto, Kafka, etc.
Experience with compiled languages like Java, Scala, C#, etc.
Familiarity with ML concepts (supervised learning, feature engineering, etc) and experience with ML frameworks (Scikit-learn, TensorFlow, SparkML, etc).
Familiarity with data lake architecture.
Knowledge of DevOps practices and experience with related tooling (containers, infrastructure-as-code, observability, etc).
We want the work you do here to be the best work of your life.

We believe the most valuable investment we can make - and the greatest boost we can give to your career - is to build an outstanding team of colleagues who are passionate about our mission.

We currently offer the following benefits and will continually evolve them with the goal of efficiently attracting, retaining, and leveraging the very highest quality talent.

Our passionate, capable team will always be our #1 benefit
Learn something new every day
Get more done than you would anywhere else
Highly competitive salaries and stock option grants
Health, dental, and vision benefits for you and your family
Flexible work hours
Unlimited vacation policy
A modern, helpful 401(k) plan
Wellness and commuter benefits"
73,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Company Overview

Sundae's mission is to help homeowners get the best outcome when it's time to sell a house that needs some love. For too long, the process for selling outdated and damaged houses has been terrible for sellers: local property investors capitalize on homeowner distress to purchase houses cheap and resell them for huge profits.

Sundae is putting those profits back in the hands of the homeowner. We've developed a trusted brand, a model to predict when sellers will need our help, and a best-in-class sales and marketing team to reach them. Our scale and efficiency allow us to redistribute economics to sellers in the form of the most competitive price so that Sundae can become the largest wholesaler of houses in the United States.

We are a Founders Fund-backed company with product-market fit, scaling revenue, and a very significant target market (think 5-6% of US property transactions). Our team is comprised of seasoned leaders from real estate and marketplace businesses and with significant high-growth company experience (Airbnb, Dropbox, Chime, LendingHome).

About the Role

We're seeking an exceptional Software/Data Engineer to join us as we define, prototype, and build data systems and pipelines to power our predictive models and analysis capabilities.

As an early member of the engineering team, you'll have an outsized impact. You'll lead the development of projects that define the future of the company.

This is not a ""maintain existing platform"" or ""make minor tweaks to current code base"" kind of role. We are effectively building from the ground up and plan to leverage the best open-source stack. If you enjoy building new things without being constrained by technical debt, this is the job for you!

Responsibilities


You will be responsible for the ingestion and feature extraction of multiple third party data sources
You will build services that continuously monitor for events and propagate the changes to downstream services
You will help define/design data integrations, data quality frameworks and design/evaluate open source/vendor tools
You will develop our strategy for long term Data Platform architecture

Qualifications


Familiarity with ETL scheduling technologies such as Airflow or Luigi
3+ years of Python or Java development experience
3+ years of SQL experience (No-SQL experience is a plus)
3+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
AWS experience desired but not necessary
BS or MS degree in Computer Science or a related technical field

What We Offer


One of the most energetic teams that truly cares about the company and the people in the company
Freedom to help craft your role to your career goals and interests
Competitive salary based on experience, with medical and dental benefits
Free snacks and drinks
Monthly happy hours and company events
A challenging and fulfilling opportunity to join one of the most experienced teams in real estate and marketplace innovation with a mission to help homeowners in their time of need

Compensation & Benefits


Opportunity to revolutionize the real estate industry with a mission to help those in need
Scale with a rapidly growing organization, with tons of opportunity for growth
Work with a team of fun and motivated individuals in a highly collaborative culture
Competitive cash salary and compelling equity package based on experience, with medical and dental benefits

We here at Sundae strive to build a workforce comprised of individuals with diverse backgrounds, abilities, minds, and identities that will help us to grow, not only as a company but also as individuals. Sundae is an Equal Opportunity Employer.

Follow us on LinkedIn ( https://www.linkedin.com/company/sundaehq )

Watch our overview video ( https://www.youtube.com/watch?v=1PziMGJqMbY&t )

Watch our customer testimonial ( https://www.youtube.com/watch?v=C8Bjy5dAiw0 )"
74,SENIOR DATA ENGINEER,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,"Bachelor's Degree in Computer Science, and 3+ years of experience working in data architecture and data modeling.4-5+ years is preferredAWS Analytics stack or similarExperience building reliable production systemsAbility to talk to and collect requirements from teams of data scientists","San Francisco
Academia is building a new Open Science distribution and peer review platform for Academic papers. Guided by a mission to accelerate the world’s research, Academia aims to make every academic paper ever published available for free online and accessible by anyone in the world. Academia has become the world's leading software platform for reading academic papers.

Academia has built a network of Academics around the world who access our platform on the web and a native mobile app to discover new research, collaborate, recommend and peer review papers, and gather feedback on draft research by participating in real-time collaborative sessions. Academia’s platform has registered more than 80 million users (growing at over 100,000 per day), and has over 60 million unique visitors every month, including over 10M logged-in monthly active users (MAU). We have over 100,000 paying premium subscribers, which has contributed to a doubling in revenue for both of the last two years.

Academia is looking for an expert Senior Data Engineer to join our team. In this role, you will be working closely with data scientists, other engineering teams, and business domain experts, to design and implement our Data Warehouse model. You will design, implement and scale data pipelines that transform billions of records into actionable data models that enable data insights.

You will lead initiatives to formalize data governance and management practices, rationalize our information lifecycle and key company metrics. You will provide mentorship and hands-on technical support to build trusted and reliable domain-specific datasets and metrics.

You will have deep technical skills, be comfortable contributing to a nascent data ecosystem, and building a strong data foundation for the company. You will be a self-starter, be detail and quality oriented, and be passionate about having a huge impact at Academia.

RequirementsBachelor's Degree in Computer Science, and 3+ years of experience working in data architecture and data modeling.4-5+ years is preferredAWS Analytics stack or similarExperience building reliable production systemsAbility to talk to and collect requirements from teams of data scientists


About Academia

Academia has attracted $34 million in investment from leading VC firms including Khosla Ventures, True Ventures, Spark Capital, and Tencent. Our work has garnered favorable attention from trade and mainstream media including UK Times, Fortune, Wired, EdTech, Venture Beat, San Francisco Business Times, The Economist, The Washington Post, TechCrunch, Scientific American, and Forbes.

Academia is a proud equal opportunity employer and we are committed to hiring and supporting a diverse workforce. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
75,Senior Data Engineer,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,"4+ years of relevant experience in data engineering building out ETL systems and pipelines
Experience with SQL/NoSql databases such Postgres, Redshift, MongoDb, and Neo4j
Experience processing event stream to produce production usable data",None Found,"Participate in sprint planning and design process
Evaluate and improve data infrastructure
Work with data science team to develop ETL infrastructure and best practices
Work with data science and feature teams to develop and maintain ETL pipelines
Consult with feature teams to develop effective ETL to power their features",None Found,None Found,"As a Data Engineer, at Autodesk Construction Solutions you’ll work on BuildingConnected’s data infrastructure to help build and maintain it so that we continue to hold the leading network where pre-construction is done. From helping general contractors find subcontractors via recommendations and search to building machine learning systems that eliminate manual data entry, you'll be integral to boosting our application as the source of truth in construction. You'll have the autonomy to experiment with cutting edge tech, designing systems and holding ownership for their implementation. In your day-to-day, you'll work closely with the data science, analytics, and engineering teams to ensure our data infrastructure is consistently improving to better serve our customers.

Responsibilities
Participate in sprint planning and design process
Evaluate and improve data infrastructure
Work with data science team to develop ETL infrastructure and best practices
Work with data science and feature teams to develop and maintain ETL pipelines
Consult with feature teams to develop effective ETL to power their features

Minimum Qualifications
4+ years of relevant experience in data engineering building out ETL systems and pipelines
Experience with SQL/NoSql databases such Postgres, Redshift, MongoDb, and Neo4j
Experience processing event stream to produce production usable data

Preferred Qualifications
Proficient with Python, PySpark, SQL, and at least one workflow manager such as Airflow. Bonus points for previously maintaining infrastructure for Airflow/Spark clusters
Prior experience with machine learning/data science systems
Experience developing and maintaining solutions within AWS (EMR, EC2, RDS, Lambda, Redshift, etc.)

About Autodesk
With Autodesk software, you have the power to Make Anything. The future of making is here, bringing with it radical changes in the way things are designed, made, and used. It's disrupting every industry: architecture, engineering, and construction; manufacturing; and media and entertainment. With the right knowledge and tools, this disruption is your opportunity. Our software is used by everyone - from design professionals, engineers and architects to digital scientists, students and hobbyists. We constantly explore new ways to integrate all dimensions of diversity across our employees, customers, partners, and communities. Our ultimate goal is to expand opportunities for anyone to imagine, design, and make a better world.

#ACSCareers

At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law.
To all recruitment agencies: Autodesk does not accept unsolicited headhunter and agency resumes. Autodesk will not pay fees to any third-party agency or company that does not have a signed agreement with Autodesk, Inc."
76,Data Scientist / Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Data Scientist / Data Engineer (San Francisco, CA)
Location: San Francisco, California, United States
Full-time
The New York Times describes Thunder as ""an ad engine to put Mad Men out of business."" We're changing how digital ads are created and distributed by automating much of what people thought couldn't be done by computer. Our technology retrieves all relevant content about an advertiser across the web to intelligently design a beautiful set of ads for desktop, tablet, and mobile devices all in under a minute.


THE JOB

Thunder is looking for a talented Data Scientist with a track record working with Big Data and Distributed Systems to manage a cutting-edge infrastructure used by the world’s largest digital advertisers. We’re using Big Data in groundbreaking ways to uncover customer insights, personalize customer experiences and fix digital advertising. You will contribute as a key member of the Product Engineering team where you will be driving product and engineering innovation to better leverage Thunder's growing personal graph. We are looking for a self-starter who thrives with ambiguity and loves solving challenging problems.


RESPONSIBILITIES

Design and develop Big Data and real-time analytics solutions using industry standard technologies
Collaborate with internal business and product teams to identify product features that can be powered by advanced data analytics
Use various machine learning and statistical techniques to analyze data, build models and identify requirements for operationalizing those models into production services
Work with external customers on challenging data analysis problems


QUALIFICATIONS

Ideal candidates will have hands-on, operational experience building and operating large-scale data analytics services and thrive working in a fast-paced startup environment.

5 -7 years of hands-on experience with using advanced statistics techniques and machine learning to build operational production services
Strong understanding of machine learning, recommendation systems, predictive analytics, and multivariate analysis
Strong computer science fundamentals including data structures, algorithms, distributed systems and common design patterns
Strong database and data engineering experience with hands-on experience building services that leverage a variety of database systems including SQL, Redshift Spectrum, Druid, Hadoop, Hive, HBase, Spark, Kafka, AWS Kinesis, MongoDB
B.S. or M.S. in Computer Science, Computer Engineering, Mathematics, Statistics, Applied Mathematics or related experience"
77,Big Data Engineer - Associate Manager,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,"
Minimum 5 plus years of hands-on technical experience implementing Big Data solutions utilizing Hadoop (or other Data Science and Analytics platforms.)
Minimum 5 plus years of experience with a full life cycle development from functional design to deployment
Minimum 5 plus years of hands-on technical experience with delivering Big Data Solutions in the cloud with AWS or Azure
Minimum 5 plus years of hands-on technical experience in developing solutions utilizing at least two of the following:
Kafka based streaming services
R Studio
Cassandra , MongoDB
MapReduce, Pig, Hive
Scala, Spark
knowledge on Jenkins, Chef, Puppet
Bachelor's degree or equivalent (minimum 12 years) work experience. (If Associate’s Degree, must have minimum 6 years work experience)
Candidates must be able to travel Monday - Thursday on a weekly basis. This is also referred to as 100% travel.
",None Found,None Found,None Found,None Found,"Accenture is a leading global professional services company, providing a broad range of services and solutions in strategy, consulting, digital, technology and operations. Combining unmatched experience and specialized skills across more than 40 industries and all business functions – underpinned by the world’s largest delivery network – Accenture works at the intersection of business and technology to help clients improve their performance and create sustainable value for their stakeholders. With approximately 469,000 people serving clients in more than 120 countries, Accenture drives innovation to improve the way the world works and lives. Visit us at www.accenture.com.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward.
Analytics professionals create new insights from predictive statistical modeling activities that target and deliver value to our clients.
Job Description
A professional at this position level within Accenture has the following responsibilities:
Adapts existing methods and procedures to create alternative solutions to moderately complex problems.
Understands the strategic direction set by senior management as it relates to team goals.
Uses own judgment to determines optimal solution to recommend.
Primary upward interaction is with direct supervisor or teams leads. Generally interacts with peers and/or management levels at a client and/or within Accenture.
Determines methods and procedures on new assignments with minimal guidance.
Decisions often impact the team in which they reside and occasionally impact other teams.
Manages medium-small sized teams and/or work efforts (if in an individual contributor role) at a client or within Accenture.

Basic Qualifications
Minimum 5 plus years of hands-on technical experience implementing Big Data solutions utilizing Hadoop (or other Data Science and Analytics platforms.)
Minimum 5 plus years of experience with a full life cycle development from functional design to deployment
Minimum 5 plus years of hands-on technical experience with delivering Big Data Solutions in the cloud with AWS or Azure
Minimum 5 plus years of hands-on technical experience in developing solutions utilizing at least two of the following:
Kafka based streaming services
R Studio
Cassandra , MongoDB
MapReduce, Pig, Hive
Scala, Spark
knowledge on Jenkins, Chef, Puppet
Bachelor's degree or equivalent (minimum 12 years) work experience. (If Associate’s Degree, must have minimum 6 years work experience)
Candidates must be able to travel Monday - Thursday on a weekly basis. This is also referred to as 100% travel.
Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills
All of our consulting professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a federal contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
78,Data Engineer - Health Data Engineering,"San Francisco, CA 94103",San Francisco,CA,94103,None Found,None Found,None Found,None Found,None Found,None Found,"DATA ENGINEER - HEALTH DATA ENGINEERING TEAM

Are you an engineer who’s passionate about defending online users against abuse, spam, and manipulation? Will you be proud to work on a real-time, scalable pipelines that process terabytes of data to enable training and analysis of Machine Learning models, product analysis, and experimentation? If so, you should join us. Health is Twitter’s top priority and we need your help!

Who We Are
The mission of the Health organization at Twitter is to keep our users safe from negative experiences in a highly adversarial environment. This aligns with our company's #1 priority: growing the collective health, openness, and civility of public conversation.

The Health Data Engineering team is responsible for designing, implementing, and maintaining data pipelines powering the most fundamental datasets used by the entire Health organization at Twitter. This team is also in charge of the best practices around building scalable, production-ready data processing solutions, as well as researching and implementing the most efficient mechanisms for data access. Health Data Engineering team will be partnering closely with all the engineering teams in the Health org to understand and improve its data production and consumption needs. We work on some of the world’s most highly-scaled distributed systems, handling hundreds of millions of tweets, engagements, and model-driven decisions each day. This team is foundational to making the most out of the data we have.

What You’ll Do
Here are some examples of what you’ll find yourself doing daily:

Directly contribute to the design and code of the data pipelines operating on production data
Improve approaches to efficiently handle ever-increasing volumes of data
Lead end-to-end design and implementation of common components that accelerate and improve our ability to write efficient and reliable data pipelines
Maintain efficiency and reliability of production of the critical datasets
Evaluate and propose the best tooling and processes for data access and analysis
Provide design and review support to the engineering teams working on data processing
Continuously evaluate team’s processes to maintain a positive and efficient engineering culture
Who You Are
You have experience working in an environment that supports data analysis, experimentation, and Machine Learning modeling or its integration into a product.
You have a solid understanding of backend and distributed systems and strong experience working with MapReduce-based architectures.
You have experience in working with large volumes of data.
You have a broad knowledge of the data infrastructure ecosystem.
You are familiar with standard software engineering methodology, e.g. unit testing, code reviews, design documentation.
You enjoy working in a collaborative environment and interact effectively with others.
You ground your decisions with data and reasoning and can adapt to new information to make informed choices.
You bring thoughtful perspectives, empathy, creativity, and a positive attitude to solve problems at scale.


Here’s all the legal good stuff:

We are committed to an inclusive and diverse Twitter. Twitter is an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, age, disability, veteran, genetic information, marital status or any other legally protected status.

San Francisco applicants: Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
79,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Cuyana is looking for a Data Engineer to join our growing team in our SF Union Square Headquarters ( https://www.cuyana.com/visit-us-sf ). The Data Engineer will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.

The ideal candidate is an experienced data pipeline builder who enjoys working with data systems and building them from the ground up. They will support our software developers, database architects, analysts and data scientists on initiatives and will ensure optimal architecture is consistent throughout ongoing projects. Must be self-directed and comfortable supporting the needs of multiple teams, systems and products.

RESPONSIBILITIES


Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data schema and delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies
Familiar with building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work with and gather requirements from stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Create data tools for analytics and data science team members that assist them in building and optimizing our data pipeline
Giving Back is something we do as part of the Cuyana Community (e.g. helping out teammates during peak season, retail events, etc. + community volunteer events)

REQUIREMENTS


Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Experience building and optimizing data pipelines, architectures and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Ability to validate and make sense of ecommerce analyses
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Familiarity with Entity Relationship Diagrams
Experience ensuring a high degree of data quality and monitoring
Ability to operate at both the infrastructure and column level of the data pipeline
A successful history of manipulating, processing and extracting value from large disconnected datasets
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Familiar with fulfilling data requirements of an expanding internal systems landscape
Team player who thrives in fast paced environments
A passion for and connection to Cuyana's philosophy and mission
Flexible, adaptable and ready to roll up sleeves to take on new projects outside of day-to-day scope, pending business needs
Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field
5+ years of experience in a Data Engineer role and using the following software/tools:
Relational SQL and NoSQL databases
Tuning indexing and overall performance of queries
Snowflake, BigQuery, Hive, or similar
Data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
AWS cloud services: EC2, EMR, RDS, Redshift
Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

Cuyana Values


You are Good People – you are honest and inclusive, you are gracious
You are Committed to Excellence – you like to challenge the status quo, you are a creative problem solver, you can do a lot with a little
You have a Growth Mindset – you are agile and flexible to business needs, you welcome feedback and give it constructively, you own results for yourself and your team

"
80,Data Engineer,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,None Found,None Found,None Found,None Found,None Found,"DATA ENGINEER
About Slync.io (www.slync.io)
We are revolutionizing the trillion dollar global trade space. Our powerful and versatile platform enables global supply chains to connect through the cloud tapping into artificial intelligence/microservices technologies.
Our first to market and innovative approach to solve multi-billion dollar logistics problems is utilized by leading freight forwarders, logistics, manufacturing, and retailers through our easy to use cutting edge applications which gives major stakeholders in global trade the ability to cost-effectively facilitate through the extremely cumbersome, time-consuming and very expensive supply chain process.
Slync was one of the startups in the SAP.io Foundry.
We are headquartered in San Francisco.

DESCRIPTION:
Slync.io is looking for a Data Engineer with 3+ years of experience to join our quickly growing team in San Francisco! As an early engineer, you will receive outstanding equity, along with a very solid comp package, which includes great benefits. We are located in a beautiful downtown location, only a block from the Ferry Building. Close to BART, Caltrain and great places to eat!
You will work with ex-Salesforce engineers who were vital in building some of their top products and systems. You will learn some of the most cutting edge technologies in this innovative space from top engineers, where your work will be implemented as part of a growing product that will have an immediate impact in the growth of the company
RESPONSIBILITIES:
Design, build and integrate new cutting edge databases and data warehouses, develop new data schemas and figure out new innovative ways of storing, integrating, and representing our data.
Research, architect, build and test robust, highly available and massively scalable systems, software, and services.
Contribute to the AI/Machine Learning integration process and platform.
Contribute to the core design of data architecture, data models and schemas, and implementation plan.
Optimize and execute on requests to pull, analyze, interpret and visualize data.
Design and develop a new framework and automation tools to enable teams to consume and understand data faster.
Write well-tested, production ready code.
Improve the efficiency, reliability, and latency of our data system.
Create automated, highly reliable data pipelines.
Test all code written and ensure production readiness before shipping.
You have a high sense of urgency to deliver projects as well as troubleshoot and fix data queries/ issues.
You are always on the lookout to automate and improve existing data processes for quicker turnaround and high productivity.
Will run ETL processes on a large scale sensitive datasets.

REQUIREMENTS:
B.S. or above in Computer Science or a related field with 3+ years of experience in data driven technology.
Experience with building scalable and reliable data pipelines using Big Data engine technologies.
Development experience on GCP platform/ tools and/ or alternate cloud platforms like AWS is highly desirable.
3+ years writing complicated database SQL queries (Oracle, PostgresQL, Hive, etc).
3+ years of programming experience is necessary; Java experience highly desirable.
Working experience in Big data/ Hadoop Ecosystem of Tools (Spark, Hive, Pig, MapReduce).
Proficient in data modeling and data warehouse concepts.
Experience building/ maintaining data pipelines in a data warehouse, data lake environment preferably on a cloud platform.
Experience implementing operational best practices such as monitoring, alerting, metadata management.
Experience using ETL or Data Virtualization for scalable data integration.

Slync.io is excited to offer a full-time role with comprehensive medical, dental, and vision insurance. We offer pre-tax flexible spending accounts for both health and transportation, daily catered meals and happy hour every Friday. We also offer flexible work hours and the opportunity to work remotely.

OUR AWARDS AND MENTIONS:
Slync was named ""Best in Show"" at Transparency '18, transportation and logistics conference. Slync is a proud member of the 2018 cohort of the SAP.io foundry in San Francisco. Check us out in the news! https://slync.io/news.
Slync is an Equal Opportunity Employer. Individuals seeking employment at Slync are considered without regards to race, ethnicity, color, age, sex, religion, national origin, ancestry, pregnancy, sexual orientation, gender identity, gender expression, genetic information, physical or mental disability, registered domestic partner status, caregiver status, marital status, veteran or military status, citizenship status, or any other legally protected category.
wDq9WRUqo5"
81,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
Cloud solution implementation experience with Azure Data Lake and Spark preferred
Minimum 8 years hands-on experience with SQL
At least one year of experience in scripting languages such as Python
Demonstrated experience in a cloud-based -computing environment such as AWS, Azure, or Google Cloud Platform
Big data processing techniques, preferred
Can work independently in ambiguous environment",None Found,None Found,None Found,None Found,"Lead Data Engineer

San Francisco, CA

About the role. . .
In order to continue and accelerate our growth, we are looking for a Lead Data Engineer with Cloud Solutions background to add to our Bay Area-based team.
Lead Engineer is responsible for building a large-scale data pipeline in cloud platform. This may involve in automation of manual processes to cloud environment. Candidate would direct the initiatives for creation of data sets. Delivering client value and ensuring high client satisfaction.
Core responsibilities for this position include, but are not limited to the following:
Extracts data from various databases; perform exploratory data analysis, cleanses, massages, and aggregates data
Employs scaling & automation to data preparation techniques - Introduces incremental improvements to data analysis, visualization, and presentation techniques to communicate discoveries
Researches relevant emerging empirical methods and quantitative tools
Possesses in-depth business knowledge in order to initiate and drive discussions with business partners to identify business issues needing analytic solutions
Leads innovative packaging and presentation of insights to business and broader analytics community
Develops processes to automate and scale insights operationalization
Develops and drives multiple cross-departmental projects
Establishes brand and team as subject matter experts in advanced analytics across departments.
Mentors data scientists in pioneering techniques and business acumen
Required Qualifications:
Cloud solution implementation experience with Azure Data Lake and Spark preferred
Minimum 8 years hands-on experience with SQL
At least one year of experience in scripting languages such as Python
Demonstrated experience in a cloud-based -computing environment such as AWS, Azure, or Google Cloud Platform
Big data processing techniques, preferred
Can work independently in ambiguous environment
About Logic20/20. . .
Logic20/20 is one of Seattle’s fastest growing full service consulting firms. Our core competency is creating simplicity and efficiency in complex solutions. Although we make it look like magic, we succeed by combining methodical and structured approaches with our substantial experience to design elegant solutions for even the most intricate challenges. Our rapid growth is in response to our ability to deliver consistently for our clients, which is directly related to the quality of the people we hire.
The past four years, we’ve been in the top 10 “Best Companies to Work For” ….. why? Our team members are highly self-motivated, comfortable conceiving strategies on the fly, and enjoy working both individually and as part of a team. Our environment is very high-energy and demanding, and individuals with remarkable enthusiasm and a can-do attitude are joining our team. We have lots of fun, focus on our employees and our clients, and work to bring our best to every opportunity."
82,Principal Data Engineer,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,"6+ years of relevant experience in data engineering building out ETL systems and pipelines
Master's degree in computer science, mathematics, or engineering
Leadership experience in the form of management or leading projects
Proficient with Python, PySpark, SQL, and at least one workflow manager such as Airflow. Bonus points for previously maintaining infrastructure for Airflow/Spark clusters
Experience processing event stream to produce production usable data
Experience with SQL/NoSql databases such Postgres, Redshift, MongoDb, and Neo4j
Prior experience with machine learning/data science systems
Experience developing and maintaining solutions within AWS (EMR, EC2, RDS, Lambda, Redshift, etc.)",None Found,"Participate in sprint planning and design process
Evaluate and improve data infrastructure
Work with data science team to develop ETL infrastructure and best practices
Work with data science and feature teams to develop and maintain ETL pipelines
Consult with feature teams to develop effective ETL to power their features",None Found,None Found,"As a founding Data Engineer at Building Connected, an Autodesk company, you’ll build and maintain the data infrastructure that makes BuildingConnected the leading network where pre-construction is done. We're driving progress across our application by leveraging data on the construction industry. From helping general contractors find subcontractors via recommendations and search to building machine learning systems that eliminate manual data entry, you'll be integral to boosting our application, as the source truth in construction. You'll have the autonomy to experiment with cutting edge tech, designing systems and holding ownership for their implementation. In your day-to-day, you'll work closely with the data science, analytics, and engineering teams to ensure our data infrastructure is consistently improving to better serve our customers.

Responsibilities
Participate in sprint planning and design process
Evaluate and improve data infrastructure
Work with data science team to develop ETL infrastructure and best practices
Work with data science and feature teams to develop and maintain ETL pipelines
Consult with feature teams to develop effective ETL to power their features

Minimum Qualifications
6+ years of relevant experience in data engineering building out ETL systems and pipelines
Master's degree in computer science, mathematics, or engineering
Leadership experience in the form of management or leading projects
Proficient with Python, PySpark, SQL, and at least one workflow manager such as Airflow. Bonus points for previously maintaining infrastructure for Airflow/Spark clusters
Experience processing event stream to produce production usable data
Experience with SQL/NoSql databases such Postgres, Redshift, MongoDb, and Neo4j
Prior experience with machine learning/data science systems
Experience developing and maintaining solutions within AWS (EMR, EC2, RDS, Lambda, Redshift, etc.)

About Autodesk
With Autodesk software, you have the power to Make Anything. The future of making is here, bringing with it radical changes in the way things are designed, made, and used. It's disrupting every industry: architecture, engineering, and construction; manufacturing; and media and entertainment. With the right knowledge and tools, this disruption is your opportunity. Our software is used by everyone - from design professionals, engineers and architects to digital scientists, students and hobbyists. We constantly explore new ways to integrate all dimensions of diversity across our employees, customers, partners, and communities. Our ultimate goal is to expand opportunities for anyone to imagine, design, and make a better world.

At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law.
To all recruitment agencies: Autodesk does not accept unsolicited headhunter and agency resumes. Autodesk will not pay fees to any third-party agency or company that does not have a signed agreement with Autodesk, Inc."
83,Staff Data Engineer,"Oakland, CA",Oakland,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Company Overview

Mosaic is building upon its success in solar and home improvement lending, making it easy by leveraging technology and financial innovation, with a goal of providing access to clean energy for everyone. We are looking to collaborate with passionate, thoughtful people who want to make a real social impact while working to solve climate change. Come join our team centrally located in beautiful Uptown Oakland and help build the movement towards 100% clean energy for all.

The Opportunity

To complement our rapid growth, we are actively seeking a bright and talented Staff Data Engineer to join our growing Data Engineering team. In this role, you will work with a team of outstanding Data Engineers impacting the growth of renewable energy through building e-commerce and financial services applications.

Your day-to-day


You bring deep and broad expertise around complex data centric projects involving data pipelines, orchestration, Data persistence, Cloud Infrastructure and end user-usability
You will mentor and guide team members based on the strength of your experience and ideas
Represent the team in architecture discussions across the technology landscape and platform
Design and build robust data pipelines using scripting in SPARK, Airflow, Python and SQL
Design data warehouse/data marts in AWS Redshift and other databases as appropriate
Use Optimization techniques in data load and query processing
Validate and build audit, balance, and control of mission-critical data pipelines
Fix bugs, work collaboratively with team members
You will build and support projects by doing production support by rotation
Take ownership of code and systems developed by others
Onboard and mentors new team members to ensure their success
Assesses scope of epics/stories and is able to plan, execute, and deliver commitments

consistently

Help evaluate and select tools, languages and frameworks used by the team

What you bring to the team


Masters or equivalent in CS/Engineering or another comparable discipline
You have at least 7+ years of technical experience and strong data warehouse & data modeling skills
Very strong skills in Python, SQL, SPARK, Redshift, Airflow, AWS
Familiarity with Agile methods (we use agile tools)
Team player, agile, highly accountable, curious, willing to learn, implement and teach
Ability to juggle multiple responsibilities and deliver to timelines

Bonus Points


Experience with open source tools such as Kafka/Kinesis is a plus
Experience in any JVM based language

Why Mosaic

As a customer focused and driven-to-win organization, there are many exciting reasons to join the Mosaic team. We provide competitive salaries, quality healthcare and an enjoyable, be-yourself office environment. We are deeply mission and vision driven, have a generous PTO plan, and support flexible schedules when needed. Mosaic has a dynamic, fast-paced, and entrepreneurial environment, which requires a professional, flexible, self-starter attitude. We believe in hiring the best, the brightest, and cultivating a culture of collaboration and appreciation.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
84,"Cloud Data Engineer, Revenue Science","San Francisco, CA 94103",San Francisco,CA,94103,None Found,None Found,None Found,None Found,None Found,None Found,"Who We Are:


As data engineers in Revenue Science, our mission is to build real-time and offline solutions to make data accessible and reliable while leveraging the largest-scale data processing technologies in the world - and then apply them to the Revenue’s most critical and fundamental data problems.


Learn more about some of the challenges we tackle on this team:

Building a Petabyte-scale Data Warehouse (Google Cloud Next '18) https://youtu.be/APBF9Z3uBCc
How Twitter Migrated its On-Prem Analytics to Google Cloud (Google Cloud Next '18) https://youtu.be/sitnQxyejUg


What You’ll Do:


As a member of the Data Engineering team, you will build and own mission-critical data pipelines that are ‘source of truth’ for Twitter’s fundamental revenue data, as well as modern data warehouse solutions, while collaborating closely with Ads Data Science team.


You will be a part of an early stage team and have a significant stake in defining its future with a considerable potential to impact all of Twitter’s revenue and hundreds of millions of users.

You will be among the earliest adopters of bleeding-edge data technologies, working directly with Revenue Science and Revenue Platforms teams to integrate your services at scale.

Your efforts will reveal invaluable business and user insights, leveraging vast amounts of Twitter revenue data to fuel numerous Revenue teams including Ads Analytics, Ads Experience, Ads Data Science, Marketplace, Targeting, Prediction, and many others.


Who You Are:


You are passionate about data and driven to take the data organization challenges at the scope of entire Twitter’s Revenue.


What you’ll need:

Strong programming and algorithmic skills
Experience with data processing (such as Hadoop, Spark, Pig, Hive, MapReduce etc).
Proficiency with SQL (Relational, Redshift, Hive, Presto, Vertica)


Nice to have:

Experience writing Big Data pipelines, as well as custom or structured ETL, implementation and maintenance
Experience with large-scale data warehousing architecture and data modeling
Proficiency with Java, Scala, or Python
Experience with GCP (BigQuery, BigTable, DataFlow)
Experience with Druid or Apache Flink
Experience with real-time streaming (Apache Kafka, Apache Beam, Heron, Spark Streaming)
Ability in managing and communicating data warehouse project plans to internal clients"
85,Senior Big Data Engineer,"Emeryville, CA",Emeryville,CA,None Found,None Found,None Found," BS in Computer Science, Engineering, or related technical discipline or equivalent combination of training and experience 6+ years core Java experience: building business logic layers and back-end systems for high-volume pipelines Experience with spark streaming and scala Current experience in Spark, Hadoop, MapReduce and HDFS, Cassandra / HBase Understanding of data flows, data architecture, ETL and processing of structured and unstructured data Current experience using Java development, SQL Database systems, and Apache products Experience with high-speed messaging frameworks and streaming (kafka, akka,reactive) Current experience developing and deploying applications to a public cloud (AWS, GCE) Experience with DevOps tools (GitHub, TravisCI, Jira) and methodologies (Lean, Agile, Scrum, Test Driven Development) Experience with data science and machine/deep learning a plus Ability to work quickly with an eye towards writing clean code that is efficient and reusable Ability to build prototypes for new features that will delight our users and are consistent with business goals Ability to iterate quickly in an agile development process Ability to learn new technologies and evaluate multiple technologies to solve a problem Excellent written and verbal communication skills in English Strong work ethic and entrepreneurial spirit",None Found,None Found,None Found,"Senior Big Data Engineer

At ZapLabs, we work to build and improve a platform that helps real estate professionals work effectively, and helps delight home buyers and sellers with an excellent experience. We do that by combining great technology with great people – and we’re looking for a Senior Big Data Engineer to join our team.

What we’re looking for:
You’re a talented, creative, and motivated engineer who loves developing powerful, stable, and intuitive apps – and you’re excited to work with a team of individuals with that same passion. You’ve accumulated years of experience, and you’re excited about taking your mastery of Big Data and Java to a new level. You enjoy challenging projects involving big data sets and are cool under pressure. You’re no stranger to fast-paced environments and agile development methodologies – in fact, you embrace them. With your strong analytical skills, your unwavering commitment to quality, your excellent technical skills, and your collaborative work ethic, you’ll do great things here at ZapLabs.

What you’ll do:
As a Senior Big Data Engineer, you’ll be responsible for designing and building high performance, scalable data solutions that meet the needs of millions of agents, brokers, home buyers, and sellers. You’ll design, develop, and test robust, scalable data platform components. You’ll work with a variety of teams and individuals, including product engineers to understand their data pipeline needs and come up with innovative solutions. You’ll work with a team of talented engineers and collaborate with product managers and designers to help define new data products and features.

Skills, accomplishments, interests you should have: BS in Computer Science, Engineering, or related technical discipline or equivalent combination of training and experience 6+ years core Java experience: building business logic layers and back-end systems for high-volume pipelines Experience with spark streaming and scala Current experience in Spark, Hadoop, MapReduce and HDFS, Cassandra / HBase Understanding of data flows, data architecture, ETL and processing of structured and unstructured data Current experience using Java development, SQL Database systems, and Apache products Experience with high-speed messaging frameworks and streaming (kafka, akka,reactive) Current experience developing and deploying applications to a public cloud (AWS, GCE) Experience with DevOps tools (GitHub, TravisCI, Jira) and methodologies (Lean, Agile, Scrum, Test Driven Development) Experience with data science and machine/deep learning a plus Ability to work quickly with an eye towards writing clean code that is efficient and reusable Ability to build prototypes for new features that will delight our users and are consistent with business goals Ability to iterate quickly in an agile development process Ability to learn new technologies and evaluate multiple technologies to solve a problem Excellent written and verbal communication skills in English Strong work ethic and entrepreneurial spirit

Nice to haves: Experience mentoring or acting in a lead capacityHere at ZapLabs, we love to celebrate and share with one another—from volunteering and having pizza movie nights to playing on our company sports teams and hosting our annual baking competitions and international potlucks.
#LI-LY1
HTF1"
86,Senior Data Engineer - Healthline,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"San Francisco, CA or Remote, US

The Healthline Data Engineering team is committed to creating a positive contribution to the health and wellness of our users by using data. We have an opportunity to build a data platform, in collaboration with the product and business intelligence teams. We are using some of the leading edge cloud technologies from Google to process all the data that is generated by our millions of users. We strive to be a fast-moving team that supports each other along the way, which requires each of us to be committed to the team and aligned with our goals/value.

As a Senior Data Engineer, you will be working closely with the Sr. Director of Data Engineering as one of the very first members of our data team. It is an opportunity to have significant input into the technical direction and architecture of our new data platform. You will also work closely with product management and various business owners to build an infrastructure that is critical to the success of the company.

What You'll Do:

Build and support our data pipelines to help power our dashboards, reports and analytics teams
Work together with the engineering and product team come up with better and more efficient ways to collect and process the data generated
Collect and process data from our ad, event aggregation, and various 3rd party systems

Who You Are:

Bachelor's Degree in Computer Science, Engineering or a related field
5+ years of applied data engineering experience
Proficiency with Python and SQL required
Proficiency with MySql
Experience with BigQuery
Experience with GCP and it's services

Even Better:

Knowledge of statistics
AWS Data Pipeline
Data Studio
Tableau
Adtech and/or CDP experience a big plus

About Us:
Healthline.com is the fastest growing health information brand, the 2nd largest health site in the US (per comScore) and reaches over 200 million people a month globally. The business is high growth and solidly profitable, employing ~300 people in San Francisco, New York and the UK. We are a purpose-driven organization with a vision to create a stronger, healthier world.

Healthline is a certified Great Place to Work and has been named to the Top 5 in Ad Age's Best Places to Work in 2019. Check out these links to learn more about what it's like to work at Healthline Media.

GPTW review ( http://reviews.greatplacetowork.com/healthline ) / Top 5 in Ad Age's Best Places to Work ( https://adage.com/article/best-places-to-work/ad-age-places-work-2019/316115/ ) / LinkedIn ( https://www.linkedin.com/company/healthline-networks-inc./ )

Founded in 2000, Red Ventures is a portfolio of growing digital businesses that bring consumers and brands together through integrated e-commerce, strategic partnerships and many proprietary brands including Healthline, Bankrate, AllConnect.com and Reviews.com. Headquartered south of Charlotte, NC, Red Ventures has over 3000 employees in offices across the US, as well as London and Sao Paulo.

Healthline Media is committed to a policy of Equal Employment Opportunity and will not discriminate on any legally recognized basis, including but not limited to race, color, religion, sex, sex stereotyping, pregnancy (which includes pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), gender, gender identity, gender expression, national origin, age, mental or physical disability, ancestry, medical condition, marital status, military or veteran status, citizenship status, sexual orientation, genetic information, or any other status protected by applicable law. We also provide reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act as amended and applicable state and local law. No person shall be excluded from participation in, be denied the benefits of, or be subjected to discrimination on the basis of these factors. If you require an accommodation in the application process, please advise your recruiter."
87,Big Data Engineer,"San Francisco, CA 94107",San Francisco,CA,94107,None Found,None Found,None Found,None Found,None Found,None Found,"As a member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation & engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You’ll work in a collaborative, trusting, thought-provoking environment—one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.
This role requires a wide variety of strengths and capabilities, including:

BS/BA degree or equivalent experience
Advanced knowledge of application, data and infrastructure architecture disciplines
Understanding of architecture and design across all systems
Working proficiency in developmental toolsets
Knowledge of industry wide technology trends and best practices
Ability to work in large, collaborative teams to achieve organizational goals, and passionate about building an innovative culture
Understanding of software skills such as business analysis, development, maintenance and software improvement
Technical proficiency in Java, Spring, and Spring Boot
Hands on experience with web technologies (e.g. HTTP, XML, REST, HTML, etc.)
Experience with NoSQL databases (Cassandra, MongoDB, etc.).
Experience with distributed streaming platform (Kafka), Data protection, replication, reconciliation, and distribution
Experience with Spark/Spark Streaming (Big Data)
Experience with web-based version control tools (GIT, Bitbucket)
Experience in DevOps - build, deployment, integration, code management and similar tools like Jenkins, Maven, automated deployment etc.
Understanding of design patterns and their application
Good to have Hands on experience developing and deploying applications to cloud platforms namely AWS & Cloud Foundry
Ability to work collaboratively in teams and develop meaningful relationships to achieve common goals
Extensive experience with horizontally scalable and highly available system design and implementation, with focus on performance and resiliency and extensive experience profiling, debugging, and performance tuning complex distributed systems
Excellent logical reasoning and analytical skills
Demonstrated professional writing/communication skills
Strong organization, interpersonal and management skills
Passionate about the digital landscape with desire for continuous learning and development
Team player that is able to work with diverse groups across an organization
Our Consumer & Community Banking Group depends on innovators like you to serve nearly 66 million consumers and over 4 million small businesses, municipalities and non-profits. You’ll support the delivery of award winning tools and services that cover everything from personal and small business banking as well as lending, mortgages, credit cards, payments, auto finance and investment advice. This group is also focused on developing and delivering cutting edged mobile applications, digital experiences and next generation banking technology solutions to better serve our clients and customers.

When you work at JPMorgan Chase & Co., you’re not just working at a global financial institution. You’re an integral part of one of the world’s biggest tech companies. In 15 technology centers worldwide, our team of 50,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $11B annual investment in technology enables us to hire people to create innovative solutions that are transforming the financial services industry.

At JPMorgan Chase & Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you’re looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you."
88,Infrastructure & Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Jane Technologies- Infrastructure & Data Engineer

Jane is building the future of eCommerce.

Jane is an MIT-founded, high growth, and rapidly expanding technology company in the cannabis industry. As the cannabis industry's first complete real-time marketplace, we aim to provide consumers with a confident, safe and simple shopping experience. Users can browse local products in real-time, compare by price, proximity or popularity and place orders at local stores for pickup or delivery. Our platform integrates directly with POS systems at retail locations and leverages this real time data to provide an ""it just works"" experience for both the retail operators and end consumers. Additionally, Jane provides key data insights to industry stakeholders via our growing analytics platform.

Culture is the single most important component of Jane's success to date. A successful candidate will thrive in our environment of mutual support, relentless pursuit of excellence, creativity, and complete lack of ego. To learn more about who we are, our culture, and whether this is the right place for you, read our Key Values profile: https://www.keyvalues.com/jane ( https://www.keyvalues.com/jane ). Check out our product at: https://www.iheartjane.com/ ( https://www.iheartjane.com/ )

About Us:

We are a full stack company, i.e. we are building Point-Of-Sale (POS) integration, analytics systems, and user experiences
We are a small close-knit team of highly technical engineers with diverse backgrounds
We have a strong engineering culture, which values lean development, data-driven practices, and open-source
We are rapidly growing 20% month over month and are always tackling challenging and interesting technical problems

What You'll do:

Create and optimize our ETL and data pipelines that span dozens of data sources and third-party systems
Build scalable and resilient services
Own and automate all aspects of our data infrastructure
Implement reliable NPL/NLU solutions to extract value from our real-time customer data and POS
Contribute to our continuous efforts to improve our infrastructure and processes

You Have:

Bachelor's degree or equivalent experience
6+ years of relevant experience
Strong Computer Science fundamentals
Ability to write clean and maintainable code, preferably in Python and/or Go
Knowledge of SQL and experience working with relational databases and data modeling
Experience with data wrangling libraries (Pandas, Numpy)
Hands-on experience building and scaling systems that support microservice-oriented architectures and related technologies (e.g. Kubernetes, Kafka, Celery/RabbitMQ, nginx, Redis, Airflow, etc)
Experience with AWS cloud services
Experience in Infrastructure as Code (e.g. Terraform, SaltStack, Ansible, Chef, Puppet)

What We Offer:

Competitive salary and equity
Beautiful office space within walking distance to the surf break at Pleasure Point in Santa Cruz
Medical Health Insurance, Dental Insurance

How To Apply:

Your resume (PDF or Markdown/text preferred)
Links to some of your work (if possible) - (GitHub or similar preferred)
(Optional) An example of something that inspires you

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
89,Senior Big Data Engineer / Big Data Engineering Manager,"San Mateo, CA",San Mateo,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Title: Big Data Engineering Manager / Senior Big Data Engineer

Location: San Mateo, CA

Reporting to: Senior Director of Data

Headquartered in the Bay Area with offices in Toronto, Canada; Jaipur, India and Austin, Texas, venture-funded Punchh is the world leader in innovative digital marketing products for brick and mortar retailers, combining AI technologies, mobile-first expertise, and omnichannel communications designed to dramatically increase customer lifetime value. Leading global chains in the restaurant, health and beauty sectors rely on Punchh to grow revenue by building customer relationships at every stage to becoming brand loyalists, including more than 100 different chains representing more than $12B in annual spend, 30,000 locations globally, 26M+ consumers, and 1M+ transactions daily. Punchh boasts a customer list that includes Pizza Hut, Quiznos, Coffee Bean & Tea Leaf and many more.

About this role

Reporting to the Head of Data Science, the data eng manager plays a critical role in leading Punchh's data innovations. He/she will help create cutting-edge big data solutions by leveraging his/her prior industrial experience.

This role requires close collaborations with machine learning, software engineering and product, serving not only internal teams but also our business clients.

What You'll Do


Become company's domain expert in Big Data and related technologies. Continuously improve our internal data infrastructure.
An inspirational hands-on mentor to the other functions of the data team, guiding team members on establishing the best industrial practices.
Own and project-manage Punchh's internal data pipeline supporting machine learning, BI products, and analytics.
Represent Punchh's expertise in advanced technologies in a variety of media outlets.
Work with large data sets and implement sophisticated data pipelines with both raw and structured data.
Manage and optimize our internal data pipeline that supports marketing, customer success and data science to name a few.
Work with the senior leadership to define Punchh's long term strategies in advanced technologies.
Occasional business travels are required.

What You'll Need


7+ years of experience as a big data engineering professional, developing scalable big data solutions.
2+ years of experience as either people manager or technical lead manager of a big data team.
Advanced degree in computer science, engineering or other related fields.
Extensive knowledge with cloud technology, e.g. AWS and Azure.
Excellent programming background in Python and Java, with hands-on experience with Kafka and Spark.
Extensive experience in using big data technologies to build data products.
Exceptional communication skills and ability to articulate a complex concept with thoughtful, actionable recommendations.
Strong problem solving skills with demonstrated rigor in maintaining a complex data pipeline.
Excellent software engineering background. High familiarity with software development life cycle.

Advanced knowledge of big data technologies, such as programming language (Python, Java), relational (Postgres, mysql), NoSQL (Mongodb), Hadoop (EMR) and streaming (Kafka, Spark).

Benefits


Healthcare coverage, FSA, HSA
Life and AD&D insurance
401K
Competitive salaries, bonus and stock options
Professional development
EAP
Maternity and Paternity (Bonding) Leave
PTO
Paid holidays
Free lunch every single day, social events, plus a well stocked refrigerator

Punchh is proud to provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics.

We also provide reasonable accommodations to individuals with disabilities in accordance with applicable laws.

Notice to recruiters and placement agencies: If you are a recruiter or placement agency, please do not submit résumés to any person or email address at Punchh prior to having a signed agreement with Human Resources. Punchh is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Also, any résumés sent to us without an agreement in place will be considered your company's gift to Punchh and may be forwarded to our Talent Acquisition team."
90,"Senior Data Engineer, Energy Platform - San Francisco, CA","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,"Legal authorization to work in the US on a full-time basis for anyone other than current employer
Bachelor's degree in a relevant technical discipline.
8+ years of experience with demonstrable proficiency in one or more DB and data pipeline tooling: mySQL, PostgreSQL, OSI Pi historian, TimescaleDB, Streamsets, Apache Drill, Apache Parquet, Dremio …
You are passionate about building scalable, high performing, data pipelines, and analytic catalogs.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc
Proficient with Containerized environments and workloads
Excellent analytical, problem-solving, and troubleshooting skills.
Experience architecting, deploying, and supporting production applications.
You care deeply about performance, accessibility and API design.
Great communication skills.
Work locations may include Houston, TX or San Francisco, CA","Job Description
If you want to bring your technical expertise, ‘challenge-accepted!’ mentality and passion for building amazing products with real-world impact for hundreds of millions of people come join our group! We have been looking for you! We have an amazing team with deep backgrounds across technology and energy.

Shell New Energies

Shell is leading the transition towards a low-carbon future. We aim to cut the net carbon footprint of our energy products in half by 2050. Our New Energies business, set up in 2016, supports this ambition. New Energies is an emerging opportunity, in which we plan to invest on average $1-2 billion a year until 2020 as we look for commercial investments in new and fast-growing segments of the energy industry.

Shell New Energies focuses on two areas: new fuels and power. New Fuels consists of investments in hydrogen, biofuels, and electric vehicle charging. In Power, we are building up positions across the full electricity value chain, including in renewable generation, retail energy, distributed energy resources, power trading and marketing, and grid services. Within these focus areas, we look for ways to connect customers with new business models for mobility and energy services, enabled by digital technologies and decentralization of energy systems. Development of our IoT platform for Shell New Energy will be paramount in achieving these goals.

Energy Platform Team

The Energy Platform team is a nimble, cross-functional, deeply technical and passionate group that embodies the speed and agility of a startup while embracing the scale of one of the largest companies in the world. Achieving a balance between agility and global scale provides unique opportunities, and the Energy Platform Team borrows from best-in-class product development, continuous delivery, and commercialization techniques while adapting them to the unique global context within Shell.

The Energy Platform team is empowered to coordinate and align Shell’s energy management platform objectives, strategies, and execution approaches across the company, as well as to design, deliver and maintain a mission-critical component of Shell’s ability to deliver differentiated products, offerings,and capabilities across its expanding global footprint.

We are looking for a Senior Data Engineer with the ability to bring their expertise and excitement for solving complex problems while building one of the largest IoT platforms in the world. There will be no shortage of opportunities to lead, eat, drink, and be merry with the most dynamic team ever assembled in the energy industry.

Build, on a daily basis, real-time and big data processing pipelines, optimizing for scalability and performance, under multiple datastore concepts (Relational, NoSQL, Graph).
Proficient in building large scale ETL jobs, leveraging big data infrastructure (Hadoop, Spark, Kafka) and modern container orchestration environments (Kubernetes).
Comfortable working in a fast-paced environment building, running, testing and shipping data pipelines to serve ML/AI workloads under a common API.
Willing to work with a cross-functional team of market analysts, data scientists & software developers to translate their data needs into features inside the Energy Platform.
Create data tooling that assists data scientists and analysts in building low latency, scalable and resilient pipelines for machine learning and optimization workloads.
Enthusiast of data quality, lifecycle and provenance management, helping establish a DataOps centric culture within Engineering teams.
Advanced working knowledge of query authoring and tooling for cross source data aggregation: APIs, 3rd party DB, Object Storages, messaging bus.
You have a proven history of working on large scale ETL jobs for data wrangling and cleansing of IoT time-series & telemetry data, as well as IIoT unstructured datasets, focusing on serving machine learning orchestrations.
You have experience with Analytical Expression Compiling languages, for runtime analytics during SQL querying.
Requirements
Legal authorization to work in the US on a full-time basis for anyone other than current employer
Bachelor's degree in a relevant technical discipline.
8+ years of experience with demonstrable proficiency in one or more DB and data pipeline tooling: mySQL, PostgreSQL, OSI Pi historian, TimescaleDB, Streamsets, Apache Drill, Apache Parquet, Dremio …
You are passionate about building scalable, high performing, data pipelines, and analytic catalogs.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc
Proficient with Containerized environments and workloads
Excellent analytical, problem-solving, and troubleshooting skills.
Experience architecting, deploying, and supporting production applications.
You care deeply about performance, accessibility and API design.
Great communication skills.
Work locations may include Houston, TX or San Francisco, CA
Company Description
Shell is a global group of energy and petrochemical companies with about 84,000 employees across more than 70 countries. We aim to meet the world’s growing need for more and cleaner energy solutions in ways that are economically, environmentally and socially responsible. We have expertise in exploration, production, refining and marketing of oil and natural gas, and the manufacturing and marketing of chemicals. As a global energy company operating in a challenging world, we set high standards of performance and ethical behaviors. We are judged by how we act and how we live up to our core values of honesty, integrity and respect for people. Our Business Principles are based on these. They promote trust, openness, teamwork and professionalism, as well as pride in what we do and how we conduct business.Building on our core values, we aspire to sustain a diverse and inclusive culture where everyone feels respected and valued, from our employees to our customers and partners. A diverse workforce and an inclusive work environment are vital to our success, leading to greater innovation and better energy solutions.
Disclaimer
Please note: We occasionally amend or withdraw Shell jobs and reserve the right to do so at any time, including prior to the advertised closing date.

Before applying, you are advised to read our data protection policy. This policy describes the processing that may be associated with your personal data and informs you that your personal data may be transferred to Royal Dutch/Shell Group companies around the world.

The Shell Group and its approved recruitment consultants will never ask you for a fee to process or consider your application for a career with Shell. Anyone who demands such a fee is not an authorised Shell representative and you are strongly advised to refuse any such demand.

Shell participates in E-Verify.

All qualified applicants will receive consideration for employment without regard to race, color, sex, national origin, age, religion, disability, sexual orientation, gender identity, protected veteran status, citizenship, genetic information or other protected status under federal, state or local laws.

Shell is an Equal Opportunity Employer - Minorities/Females/Veterans/Disability.
Employment TypeFull Time
Skillpool
IT Data and Analytics, Information
Work LocationCalifornia - San Francisco
No. of Positions
1
Job Expires01-Nov-2019"
91,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"201 Third Street (61049), United States of America, San Francisco, California

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer

Your days will include:
Working with product owners to understand desired application capabilities and testing scenarios

Continuously improving software engineering practices

Working within and across Agile teams to design, develop, test, implement, and support technical solutions across development tools and technologies

Leading the craftsmanship, availability, resilience, and scalability of your solutions

Bringing a passion to stay on top of tech trends, experiment with and learn new technologies

Encouraging innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity.

Provide active mentorship/guidance to fellow members of the agile tech team and participate in internal and external technology conference & communities.

Coordinate and scale the evolving cloud based solutions with product development teams both internal and external to Capital One.

Basic Qualifications:
Bachelor’s Degree

At least 4 years of software development experience

At least 2 years of coding experience in one of the following languages: Python or Java.

At least 3 years of experience designing, architecting or developing on Distributed Systems

At least 2 years of experience with Cloud computing (AWS)

Preferred Qualifications:
3+ years of building highly available, scalable, lossless distributed systems serving huge data at a minimum latency with high throughput.

2+ years of experience with Apache Spark, Flink etc.

1+ years of No-Sql Datastores.

At this time, Capital One will not sponsor a new applicant for employment"
92,Sr. Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,"
Ingestion of data from multiple, unstructured sources using multiple analytics tools
Implementing ETL process
Monitoring performance and advising any necessary infrastructure changes
Defining data retention policies",None Found,"
Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.","Job Title: Data Engineer

Location: San Francisco, Chicago, San Jose, Palo Alto, Austin, TX

Terms: Full-time, Contract, Contract-2-Hire

About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.

What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.

As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do

Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:

Cloud
Analytics
Digitization
Infrastructure
Security

Sr. Data Engineer
Job Description
Responsibilities
Ingestion of data from multiple, unstructured sources using multiple analytics tools
Implementing ETL process
Monitoring performance and advising any necessary infrastructure changes
Defining data retention policies

Requirements
3+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct query performance tuning
Strong skills in one of the scripting language (Python, Ruby, Bash)
1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)

We are Growing Rapidly: 2019 Highlights

Trianz is growing rapidly. Here are some highlights.

Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.

Won the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.

Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.

Featured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.

Achieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.

Talk to us, Join us & Develop into Leaders
Come join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.
 We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
 Equal Opportunity Employer
Trianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law)."
93,Senior Data Engineer,"San Francisco, CA 94124",San Francisco,CA,94124,None Found,None Found,None Found,None Found,None Found,None Found,"Skip is looking for a Senior Data Engineer to create well-designed data processing systems that will support business strategy, fleet operations, and vehicle hardware improvements. Your work will directly impact Skip’s core user experience by giving stakeholders at Skip full visibility into customer behavior, effectiveness of new product launches and overall health of the global fleet. You need a strong software engineering foundation, a sense of ownership and a desire to learn and grow.

Skip's mission is to energize cities by making mobility accessible to everyone. We believe this requires designing every aspect of a micromobility network from the ground up. This includes custom vehicle hardware, the software-defined fleet management system, and ground operations for safety and recharging. Our success will make it easier for everyone to work, play, and connect in their communities.
WHAT YOU'LL DO:
Shape the data roadmap at Skip
Build our data infrastructure pipelines and data warehouse using products on the Google Cloud Platform including but not limited to Big Query, Pub/Sub and Dataflow
Optimize our vehicle telematics logging, storage, and retrieval
Work in cross functional pods alongside backend engineers, product managers, and UX designers using agile development to continuously deliver value to millions of users
Partner closely with Data Scientist to create insights to enable teams to make better strategic decisions
Present findings to the executive team
YOU SHOULD HAVE:
BS/MS in STEM field (CS, Math, Physics etc)
4+ years of experience building data infrastructure in a production setting using a modern data stack including SQL and NoSQL databases and both real-time and batch data processing
Passion for the micromobility industry
Excellent communication skills
WHAT WE OFFER:
The satisfaction of delivering an amazing experience for millions of people, from complete strangers to your friends and family.
A culture built around putting the customer first, prioritizing dependability, safety, and transparency.
The opportunity to learn about and solve difficult technical challenges, such as fleet management for hundreds of thousands of light electric vehicles.
Personal, professional, and leadership growth at a fast-growing startup at its inflection point.
A cross-functional work environment that includes experts in diverse fields like government policy, hardware engineering, mobile and cloud software, supply chain logistics, and trust and safety.
Competitive salaries and benefits, including coverage for health, dental, and vision insurance
WHY JOIN SKIP?

Designing from the ground up is important for supply chain and fleet management, especially when it comes to reliability, safety, business management, and a great rider experience. Our leadership team has the most experience in designing light electric vehicles from the ground up. Our founders previously were co-founders at Boosted, where they designed and built the first reliable micromobility vehicle and presented their work at TED.

We know our customers aren’t just our riders, but also the public and city governments. We helped create the first scooter sharing permit in the US, were the first to share data on scooter usage with cities, and have been at the front of transparent operation around fleet management and vehicle safety. The result is deeper collaboration with cities, fewer complaints from the public, and a better experience for our riders.

We are backed by some of the world’s best investors, including Accel, Menlo, Y Combinator, Initialized, A Capital, and Paul Graham.

Skip is an equal employment opportunity employer. We are dedicated to providing an inclusive, open, and diverse work environment."
94,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Juvo was founded with an overarching mission: to establish financial identities for the billions of people worldwide who are creditworthy, yet financially excluded. In partnership with mobile network operators, Juvo's proprietary Identity Scoring technology uses data science, machine learning and game mechanics to create an identity-based relationship with anonymous prepaid users, opening up access to otherwise unattainable mobile financial services.

Since emerging from stealth in September 2016, Juvo has increased its global reach five times over, from 100 million to 500 million, and has steadily grown its operations and employee base worldwide with 100 employees today. To date, Juvo has enabled over 400M transactions in 25 countries and 4 continents, with 1M active subscribers a day. Juvo's mobile operator partners include Telefonica, Millicom, Sprint, Deutsche Telekom and Cable & Wireless.

In 2017, Juvo completed a $40 million USD Series B funding round with funding from Samsung NEXT and top-tier VCs including NEA, Wing Venture, and Freestyle Capital. Early investors in the company include the former CEOs of AT&T Wireless, NYSE, Sprint, Telefonica International and Vodafone Group. Juvo is frequently profiled in top tier tech and business press and our proprietary technology, Identity Scoring, is award winning.

About the Job
-------------

Juvo is looking for a Senior Data Engineer to be a driving force in the design and development of data solutions on our AWS hosted platform. You will join a high performing team of both engineers & scientists and you'll collaborate with Juvo's internal stakeholders regarding technological developments for our scalable financial platform meant to serve billions of people in the underbanked world. Data Engineering is instrumental in abstracting, modularizing and stress-testing our Data Science stack, allowing us to operationalize data science work across 4 continents and 50 countries, touching 500M end users over the next two years.

At Juvo, you'll be responsible for turning one of the World's most interesting and valuable data sets into financial identities at the core of the global financial platform we're building to trigger fair, sustainable economic growth.

Responsibilities
----------------


Contribute to the design and implementation of scalable ETL and data processing systems to go into our big data ecosystem including data collection, cleaning, processing, ETL and the creation of a common data lake.
Work with engineering and infrastructure architects to improve data strategy, quality and governance.
Work closely with both business and partner stakeholders to quickly deliver high-quality applications.
Collaborate with architects, product owners, data scientists and test engineers to help bring data science R&D projects into Production.
Build and scale Internal Analytics, Reporting, and Decisioning platform on top of a common data lake.
Manage data infrastructure to grow and support the Data Science and Engineering team in relation to the construction of performant data products.
Establish the technical direction for the team, driving the necessary changes and making appropriate technology choices working collaboratively with the Data Science, Product and Engineering Teams.
Maintain and scale production environments for ML-based data products.

Qualifications
--------------


5+ years of experience as a Data Engineer with AWS services
2+ years of experience writing production level code in Python.
2+ years of experience with the following infrastructure frameworks; Terraform, Docker, and Kubernetes.
5+ years of experience designing data warehouse/data lake and ETL architectures with big data technologies such as Spark, Spark Streaming, Hive, Storm, Sqoop, Kafka, Hbase, and HDFS.
Experience with SQL/NoSQL systems such as MongoDB, Cassandra, Solr, Elasticsearch, Redshift, DynamoDB, etc.
Experience in Agile processes and scrum.
BS/MS/PhD in Computer Science or related quantitative fields.
Highly organized, structured work approach and dependable.
Experience in enterprise-scale software architecture is preferred.

Perks & Recreation


Work towards a mission that matters – join us in creating the YES economy
Competitive cash and equity compensation
Great medical/dental/vision benefits, with dependent coverage
Pre-tax commuter benefits
401(k) available
Paid holidays and flexible paid time off
Monthly reimbursement for internet or mobile phones
Conveniently located office in the Financial District of San Francisco
Fully stocked kitchens with organic and healthy snacks
Weekly catered lunch
Your choice of the best and newest tech (Apple products, Sennheiser Noise Canceling headphones, Stand-up desks, etc)
Employee discount on Samsung products (Samsung is an investor)!

Juvo is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. Juvo is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation."
95,Big Data Engineer,"Foster City, CA",Foster City,CA,None Found,None Found,"
2 years of work experience with a Bachelor's Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD, or PhD)",None Found,None Found,None Found,None Found,"Job Description

Job Description
Cybersecurity is at the beating heart of our culture. Our diligence and expertise is what makes us the undisputed leader in electronic payments. We’ve made it our priority to create a top-tier Security Architecture team, poised to defend us against any potential cyber threats. CyberSecurity Architecture team is looking for a Data Engineer with Hadoop admin experience. Candidate will be a part of the Security Data Engineering team and involved with administering Hadoop-ecosystem and Data Engineering activities. The position will be based at Visa's headquarters in Foster City, California.
As a Data Engineer, you will be responsible for helping to blueprint and deliver modelled attributes, data assets, and self-serve workflows that solve clients' business objectives. You will get the chance to leverage your business acumen and technical knowledge of big data and data mining techniques. Based on deep understanding and knowledge of big-data engineering techniques, you will develop and maintain data and tools to enable data scientists to draw fact based insights and build models This function is critical in building market-relevant client solutions and intellectual property for Visa.
You'll need to have excellent communication and cross-group collaboration skills, be able to make forward progress despite ambiguous circumstances, be a self-starter, a quick learner of new technologies and have experience in Data Platform engineering team to maintain DataQuality and Integrity.

Qualifications

Basic Qualifications
2 years of work experience with a Bachelor's Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD, or PhD)
Preferred Qualifications
4-5 years of experience with Bachelor's degree or 2-3 years of experience with Master's degree in Computer.
Experience with Data Engineering/ETL/Administration experience in production ETL pipelines, utilizing big data engineering techniques that enable statistical solutions to solve business problems.
Design and develop different architectural models for our scalable data processing as well as scalable data storage
Experience with various Hadoop distribution like hortonworks.
Experience in scripting languages (Shell/Perl/Python/Java etc.) and preferred Java Programming experience.
Experience with no-sql database administration & development like mongoDB is a must
Ability to create and manage big data pipeline using kafka, flume & Spark
Experience building large-scale distributed applications and services
Experience with Scala programming is plus
Experience with agile development methodologies
Previous exposure to financial services, credit cards or Security analytics is a plus, but not required
Extensive experience with SQL and big data technologies (Hadoop, Python , Java, Spark, Hive etc.) tools for large scale data processing, data transformation and machine learning pipelines
Experience with data visualization and business intelligence tools like Tableau, Microstrategy, or other programs highly desired
Some proficiency with SAS as a statistical package is also highly desirable
Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is very helpful
Demonstrated intellectual and analytical rigor, strong attention to detail, team oriented, energetic, collaborative, diplomatic, and flexible style
Additional Information

Essential Functions
Work with cross functional teams to fully understand business requirements and desired business outcomes
Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions
Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists
Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users
Execute data engineering projects ranging from small to large either individually or as part of a project team
Ensure project delivery within timelines and budget requirements
Experience on working with very large data sets and knowledge of building programs that leverage Massively Parallel Processing (MPP) Data warehouse platforms.
The engineer will have significant knowledge of Big Data technologies and tools with the ability to share ideas among a collaborative team.
Responsible for loading data from several disparate datasets, documentation, performance testing and debugging applications.
Travel Requirements
This position requires the incumbent to travel for work “0-5%” of the time
Physical Requirements
This position will be performed in an office setting. The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers, reach with hands and arms, and bend or lift up to 25 pounds.
Visa will consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law."
96,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Granite Media is seeking data engineering leaders to help us build a brighter future for professional media.
Headquartered in San Francisco, we are a profitable early-stage media company. We aim to make quality storytelling and journalism sustainable through use of modern technologies, data science, and business innovation. We have a talented 10 person team and each member has a proven track record of success in their field. The founding team has more than a decade of experience in the industry and a previous successful exit.
This job is either in San Francisco or remote (US only) and offers a package with both competitive salary and equity.
HOW YOU’LL MAKE AN IMPACT
You will drive data pipeline development in a system already processing hundreds of millions of events per day. We live for data and cannot overstate how important this function is to our business. Our data warehouse is the backbone of our machine learning platform, automated marketing and revenue systems, and multivariate testing platform.
WHAT YOU’LL DO
Lead data pipeline development, including streaming pipelines which use Apache Beam / Cloud Dataflow.
Help dictate data architecture.
Help develop and deploy machine learning models.
Develop proactive monitoring solutions.
REQUIREMENTS
Proven track record of exceptional communication skills
Computer Science degree or equivalent
4-8 years development experience in data-rich environments
Experience developing, deploying, and monitoring high-volume data pipelines
Professional Java experience
Professional Python experience
BONUS ROUND
Experience in any of the following fields or technologies:
Technical lecturing or blog entry writing
Data architecture
Project or team management
DevOps practices
Machine learning
BigQuery
Apache Beam
Docker
Looker

Intrigued? Reach out to learn more (jobs@granitemedia.com).

Note: Granite Media does not accept unsolicited resumes from search firm recruiters. Fees will not be paid in the event that a candidate submitted by a recruiter without an agreement in place is hired; such resumes will be deemed the sole property of Granite Media. Granite Media is an equal opportunity employer. All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law."
97,"Senior Data Engineer, Data Warehouse","San Francisco, CA 94103",San Francisco,CA,94103,None Found,None Found,None Found,"
Design, build, and maintain data pipelines (ETL/ELT) and database schemas/objects
Work with analysts to understand business needs and drive the project on its own and mentor the team
Collaborate on code reviews, internal processes, and software enhancements, and understanding of ETL/ELT/SQL/Python coding standards
Responsible for maintaining technical documentation to support new and existing solutions",None Found,"
4+ years in the industry as a data engineer or backend engineer
You will understand data warehousing concepts (modeling, tuning, maintenance)
You are highly proficient with SQL, and have the desire to mentor and teach others SQL
You are proficient in Python or any other scripting languages and software development
You have a strong understating/experience in distributed data processing/Traditional RDBMS/MPP/NoSQL systems, and data modeling
You are driven by a curiosity and motivation to explore data
You have experience dealing with DevOps concerns for data pipelines
You have excellent listening and interpersonal skills","Do you have a passion for cutting-edge technology? Do you thrive in fast-paced, dynamic environments? Do you love using your technical prowess to tackle industry-impacting challenges?

Quantcast is hiring a Senior Data Engineer to work on our Data Warehouse team in our San Francisco headquarters.

As the real-time pulse of the Internet, Quantcast runs the world’s largest AI-driven insights and measurement platform directly quantifying over 100 million web destinations. Using machine learning to drive human learning, Quantcast provides brand marketers and publishers with meaningful audience insights, predictive targeting and measurement solutions across the customer journey.

We are looking for a seasoned data engineer to build and grow large-scale analytical data platforms from the ground up. You will have opportunities to work end to end from collaborating with the business to designing and developing data pipelines to data modeling, all in the cloud.
Responsibilities
Design, build, and maintain data pipelines (ETL/ELT) and database schemas/objects
Work with analysts to understand business needs and drive the project on its own and mentor the team
Collaborate on code reviews, internal processes, and software enhancements, and understanding of ETL/ELT/SQL/Python coding standards
Responsible for maintaining technical documentation to support new and existing solutions
Requirements
4+ years in the industry as a data engineer or backend engineer
You will understand data warehousing concepts (modeling, tuning, maintenance)
You are highly proficient with SQL, and have the desire to mentor and teach others SQL
You are proficient in Python or any other scripting languages and software development
You have a strong understating/experience in distributed data processing/Traditional RDBMS/MPP/NoSQL systems, and data modeling
You are driven by a curiosity and motivation to explore data
You have experience dealing with DevOps concerns for data pipelines
You have excellent listening and interpersonal skills
Bonus Points
Experience with AWS Cloud ecosystem
Experience with Spark/Kafka/Kinesis
Experience with Pentaho, PostgreSQL, Salesforce, Redshift, Big Data systems
Reporting tool experience: Tableau and/or Looker or superset
Git, Docker, Jenkins, Jira experience
Knowledge of Adtech industry
Quantcast owns and operates the world’s largest audience insights and measurement platform on the open internet. Fueled by live data drawn from more than 100 million web and mobile destinations, Quantcast applies machine learning technology to help marketers, publishers, and agencies grow their brands by better understanding and predicting consumer interactions in real-time.

Founded in 2006, Quantcast is headquartered in San Francisco and has employees in over 20 offices across 10 countries. We are committed to building an inclusive and diverse environment where everyone can be their authentic self."
98,Senior Data Engineer (Java and Python),"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,"
Contribute to the architecture, design and development of our Python and Java data workflow management platform
Design and develop tools to wrangle datasets of small and large volumes of data into cleaned, normalized, and enriched datasets
Work with the product group and data engineering to develop new functionality
Should have knowledge about OO Python and Java.
Experience working independently, or with minimal guidance
Experience with agile software development.
Strong problem solving and troubleshooting skills
Experience working with Distributed Systems design and implementation.","
B.S. in computer science, software engineering, computer engineering, electrical engineering, or related area of study",None Found,"Infostretch is seeking a Senior Data Engineer with hands on experience in Java and Python. This position is based in San Francisco, CA.
Job Responsibilities:
Contribute to the architecture, design and development of our Python and Java data workflow management platform
Design and develop tools to wrangle datasets of small and large volumes of data into cleaned, normalized, and enriched datasets
Work with the product group and data engineering to develop new functionality
Should have knowledge about OO Python and Java.
Experience working independently, or with minimal guidance
Experience with agile software development.
Strong problem solving and troubleshooting skills
Experience working with Distributed Systems design and implementation.
Positive points:
RDBMS SQL and NOSQL, structured and unstructured data.
Big data and cloud processing experience, e.g. HDFS, Spark
Experience working in a cloud-based environment, such as GCP or AWS
Experience with pandas and numpy
Proficiency in multiple programming languages
Education Qualification:
B.S. in computer science, software engineering, computer engineering, electrical engineering, or related area of study"
99,"Software Engineer, Ads Metrics Quality","San Francisco, CA 94103",San Francisco,CA,94103,None Found,None Found,None Found,None Found,None Found,None Found,"We're looking for data engineer with experience in big data systems and excellent data analytical skills who is passionate about data quality. You'll lead the continuous improvement of our ads metrics quality, a critical area waiting for you to make a huge impact. You'll have the opportunity to collaborate with people from other engineer and non-engineer teams, play a big role in creating the ads metrics quality roadmaps, and have the flexibility to shape the scope of your role.

What you'll do:

Work on critical reporting/billing workflows to ensure the accuracy of our data
Investigate any data discrepancy issues across the ads data stacks
Build monitoring and alert solutions to guardrail ads metrics quality
Proactive investigation on identified gaps to understand the size and impact
Identify risks and controls for ad metrics

What we're looking for:

3+ years experience
Experience with big data technologies (Hadoop, Hive, Presto, or Spark)
Experience in analyzing metric or product data, deriving insights from it and generating reports

#LI-GK1"
100,Data Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,"Use data engineering expertise to design and build solutions/products for analyzing data collection from various data sources.
Create data tools for analytics and technical team members to assist them in building and optimizing the product.
Conduct advanced statistical analysis to determine trends and significant data relationships.
Work with data and machine learning experts to strive for greater functionality in our data and model life cycle management systems.
Other duties as assigned
",None Found,"Experience with relational SQL and NoSQL databases required
BS degree in Computer Science, Informatics, Information Systems or another related field and a minimum of 2 years’ experience in AWS, Python required
Years of experience can be substitute for the education","About Nevro
Nevro (NYSE: NVRO) is a public multinational medical technology company headquartered in Redwood City, California. We have developed HF10™ therapy, an innovative, evidence-based neuromodulation platform. We started with a simple mission to help more patients suffering from chronic pain. At each stage of development, our research was subject to the highest levels of scientific rigor, resulting in a new therapy that has impacted the lives of over 45,000 patients around the world. The Nevro® Senza® SCS System received CE mark in 2010, TGA approval in 2011, FDA approval in 2015, and is commercially available in Europe, Australia, and the United States.
Job Summary & Responsibilities
The Data Engineer shall be a foundational member in research and product development team to collaborate on the design and implementation of improving and automating clinical data to analyze, identify and report data and trends.
Use data engineering expertise to design and build solutions/products for analyzing data collection from various data sources.
Create data tools for analytics and technical team members to assist them in building and optimizing the product.
Conduct advanced statistical analysis to determine trends and significant data relationships.
Work with data and machine learning experts to strive for greater functionality in our data and model life cycle management systems.
Other duties as assigned
Role Requirements
Experience with relational SQL and NoSQL databases required
BS degree in Computer Science, Informatics, Information Systems or another related field and a minimum of 2 years’ experience in AWS, Python required
Years of experience can be substitute for the education
Skills and Knowledge
Proficient in data visualization skills with technologies such as PowerBI, Tableau
Experience with Data and Model pipeline and workflow management tools
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Have built processes supporting data transformation, data structures, metadata, dependency and workload management.
Nice to have skill would include: experience with RESTful APIs and cross-platform development
Attention to detail and organization/ documentation skills
Able to work effectively under pressure, independently, and within a collaborative team-oriented environment using sound judgment in decision making
Strong analytic skills related to working with unstructured datasets.
Project management and interpersonal skills.
Experience supporting and working with cross-functional teams in a dynamic environment
LI-TS1"
101,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Juvo was founded with an overarching mission: to establish financial identities for the billions of people worldwide who are creditworthy, yet financially excluded. In partnership with mobile network operators, Juvo's proprietary Identity Scoring technology uses data science, machine learning and game mechanics to create an identity-based relationship with anonymous prepaid users, opening up access to otherwise unattainable mobile financial services.

Since emerging from stealth in September 2016, Juvo has increased its global reach five times over, from 100 million to 500 million, and has steadily grown its operations and employee base worldwide with 100 employees today. To date, Juvo has enabled over 400M transactions in 25 countries and 4 continents, with 1M active subscribers a day. Juvo's mobile operator partners include Telefonica, Millicom, Sprint, Deutsche Telekom and Cable & Wireless.

In 2017, Juvo completed a $40 million USD Series B funding round with funding from Samsung NEXT and top-tier VCs including NEA, Wing Venture, and Freestyle Capital. Early investors in the company include the former CEOs of AT&T Wireless, NYSE, Sprint, Telefonica International and Vodafone Group. Juvo is frequently profiled in top tier tech and business press and our proprietary technology, Identity Scoring, is award winning.

About the Job
-------------

Juvo is looking for a Data Engineer to be a driving force in the design and development of data solutions on our AWS hosted platform. You will join a high performing team of both engineers & scientists and you'll collaborate with Juvo's internal stakeholders regarding technological developments for our scalable financial platform meant to serve billions of people in the underbanked world. Data Engineering is instrumental in abstracting, modularizing and stress-testing our Data Science stack, allowing us to operationalize data science work across 4 continents and 50 countries, touching 500M end users over the next two years.

At Juvo, you'll be responsible for turning one of the World's most interesting and valuable data sets into financial identities at the core of the global financial platform we're building to trigger fair, sustainable economic growth.

Responsibilities
----------------


Contribute to the design and implementation of scalable ETL and data processing systems to go into our big data ecosystem including data collection, cleaning, processing, ETL and the creation of a common data lake.
Work with engineering and infrastructure architects to improve data strategy, quality and governance.
Work closely with both business and partner stakeholders to quickly deliver high-quality applications.
Collaborate with architects, product owners, data scientists and test engineers to help bring data science R&D projects into Production.
Build and scale Internal Analytics, Reporting, and Decisioning platform on top of a common data lake.
Manage data infrastructure to grow and support the Data Science and Engineering team in relation to the construction of performant data products.
Establish the technical direction for the team, driving the necessary changes and making appropriate technology choices working collaboratively with the Data Science, Product and Engineering Teams.
Maintain and scale production environments for ML-based data products.

Qualifications
--------------


BS/MS/PhD in Computer Science or related quantitative fields.
2+ years experience in enterprise-scale software architecture is preferred
At least 3 years of experience in data engineering and software development.
2+ years of experience designing data warehouse/data lake and ETL architectures with big data technologies such as Spark, Spark Streaming, Hive, Storm, Sqoop, Kafka, Hbase, and HDFS.
2+ years of experience with AWS services.
Experience with SQL/NoSQL systems such as MongoDB, Cassandra, Solr, Elasticsearch, Redshift, DynamoDB, etc.
Fluent in at least one of the following programming languages Scala, Python, Java.
Familiar with infrastructure frameworks such as Terraform, Docker, Kubernetes, etc.
Experience in Agile processes and scrum.
Highly organized, structured work approach and dependable.

Perks & Recreation


Work towards a mission that matters – join us in creating the YES economy
Competitive cash and equity compensation
Great medical/dental/vision benefits, with dependent coverage
Pre-tax commuter benefits
401(k) available
Paid holidays and flexible paid time off
Monthly reimbursement for internet or mobile phones
Conveniently located office in the Financial District of San Francisco
Fully stocked kitchens with organic and healthy snacks
Weekly catered lunch
Your choice of the best and newest tech (Apple products, Sennheiser Noise Canceling headphones, Stand-up desks, etc)
Employee discount on Samsung products (Samsung is an investor)!

Juvo is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender, race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law. Juvo is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation."
102,"Senior Data Engineer, Core Systems","San Francisco, CA 94107",San Francisco,CA,94107,None Found,None Found,None Found,None Found,None Found,None Found,"About us

Metromile is the leading pay-per-mile car insurance company in the U.S. We're disrupting a $250 billion auto insurance category by offering an entirely new type of insurance that charges customers based on the miles they drive. We believe paying for what you use is more fair and more affordable. With pay-per-mile car insurance, low-mileage drivers see huge savings. We also believe in the power of data science and machine learning to make car insurance better and less complicated.

We're proud to say that Metromile has been named on Glassdoor's Best Place to Work in their 2018 Employees' Choice Awards list for the second year in a row!

About the role:
Metromile is on the cusp of implementing a new data warehousing solution, to help build the next generation of Data & Analytics platform. This is a fantastic opportunity to learn a lot and grow your career, while working in the cutting-edge space of using Telematics data to drive Insurance decisions.

Metromile is seeking an experienced Data Engineer with proven experience in designing data ingestion pipelines to join our Data & Analytics team. This role would contribute to the vision for data infrastructure and business intelligence tools, architect table schemas and data storage, upgrade our existing data infrastructure, implement new data engineering solutions, and work to gather and store new data from across the company to power our decision making processes.

This role would work predominantly with the Analysts, SME's and other data engineers on the team, but would also allow for opportunities to architect how data is managed across the entire company, giving qualified candidates the chance to be involved in a large variety of projects. Because of the important role that data plays in making decisions within the company, this role is at the critical intersection of driving the future of our business while also ensuring that day-to-day operations continue to be successful.

You will:

Design data models, data ingestion pipelines and implement scalable ETL / ELT processes.
Collaborate with partners across business functions to define, implement and maintain vital business metrics.
Architect database, data integration processes and table optimizations.
Engage in self-driven investigation into new and upcoming technologies/techniques for data management and retrieval.
Some familiarity with data serialization formats such as json, avro, and protobuf is a nice to have.

About you:

3-5+ years professional experience.
Experience with toolsets available in the Cloud (AWS / Azure / GCP etc) used for Data Storage, Ingestion.
Possess expertise in designing data ingestion pipelines.
Strong software development fundamentals in Java and Python for building and shipping production data pipelines.
Expert knowledge of databases, specifically relational (e.g. SQL), column store (e.g. Redshift/Snowflake). Experience with unstructured (HBase/Cassandra) is a nice to have.
Experience with configuration driven platforms (e.g. Airflow).

Nice to have:

Experience with storage and processing of sensor data (for example GPS time series) from external systems.
You have some experience with touch point/experimentation systems: Segment, Mixpanel, Optimizely
Familiarity with data replication (MySQL, Postgres etc) and any solutions like StitchData or Fivetran.
Experience implementing Enterprise Container Platforms like Docker.

What's in it for you:

Competitive salary plus equity
Robust benefit options (health, dental, vision, 401K)
Commuter and well-being benefits
Generous parental leave
Catered lunches and a fully stocked kitchen
Monthly social events (Movies, game nights, park days, etc.)
Mac equipment and adjustable workstations

Metromile is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
103,Senior Data Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Engaging, starry-eyed Enterprise Communications company in Redwood City seeks Senior Data Engineer for serious long-term relationship.


Us: Fun-loving, small, and diverse team with a real customer challenge to solve.
You: Passionate, geeky, curious, and enjoys like-minded companionship.

Want to make music together?

So Who Are We?

We're a small, diverse engineering team seeking to change the Enterprise Communications industry. Have you ever had a bad experience with your HR group? Been unable to find information in a dreary portal? Confused about initiatives your company is undertaking? You're not alone. Most internal communications haven't kept up with the life-changing technology innovations that have occurred in recent years. With our dedicated team of technologists, creatives, and strategists we've set out to modernize employer-to-employee communications. We believe life at work can be better than it is, and we're on a multi-year mission to improve everyone's relationship with their employer.

At GuideSpark we've created a new SaaS offering that enables employers to drive organizational impact by giving employees the right information at the right time, while personalizing and orchestrating that information to achieve the best-possible outcomes. That offering, GuideSpark Communicate Cloud, is rapidly growing in adoption. In a very short time we have built a tremendous roster of A-List clients from Fortune 1000 companies (e.g. Adobe) to fast growing startups (e.g. SurveyMonkey). Our fast-paced, entrepreneurial culture is driving triple digit growth and we're looking for great people to join our team!

As if a successful product wasn't enough, we're fun people, too. We work and play together, everyone is welcome to bring their personality to work to add color and texture to the growing symphony!

Check us out here: https://www.guidespark.com/ ( https://www.guidespark.com/ )

And Who Are You?

You're that special someone who enjoys working with others in a diverse mix of skills and creativity. You believe work is about more than just punching a clock. You think solving hard problems in order to help real-world customers is fun. You value good communications. You enjoy solving the thorny issues of data collection, transformation, and application. You are capable of making a positive impact, and you love proving it.

You're someone who is:

Passionate about Data!
Understands how to analyze and develop algorithms to get deep insights and great value from the Data for customers and business decisions.
Enjoys being instrumental in establishing technologies, standards, processes and architecture for Data.
Knows that Data doesn't matter unless it's accurate, consistent, and traceable at any point throughout the system.

So How Will We Play Together?

Can you hear the growing chorus? The Sun is rising and we stand ready to meet you with hand outstretched. Come use your well-honed skills to help us create, build, and grow our successful initial offering!

At GuideSpark you'll:

Design, develop, implement and maintain complex Data Processing and Data Lake Pipelines that are scalable, optimized, and fault-tolerant.
Define and develop guidelines, standards, and processes to ensure Data Quality and Integrity.
Build comprehensive Information Models and Frameworks.
Identify opportunities to develop key insights, forecasts, statistical models, segmentation schemes, and Data-driven analyses to drive customer value.
Mentor and collaborate with your peers.
Work with stakeholders including the Executive, Product, Enablement, and Design teams to assist with Data-related technical issues and support their Data infrastructure needs.
Architect Data to work with various Machine Learning algorithms, and work with those algorithms to further gather, iterate, refine, and derive insights in a virtuous cycle.

Alright! Time To Make Some Music!

You're bringing your whole self to the symphony:

Strong OOP and software design knowledge.
Hands-on experience designing and developing RESTful APIs.
Recent experience with Scalability and Data Pipelines.
Advanced working SQL knowledge and experience working with Relational and non-Relational databases.
Expertise in Object Oriented and Functional Programming skills such as Java, Ruby, Microsoft .Net, Python, Unix/Linux/Perl scripting.
Experience with designing complex Data Models and Data Engineering Solutions for Data Warehouses from various heterogeneous Data Sources.
Experience debugging performance issues in large-scale production environments.
Experience with Git and Jira.
Bachelor's or Master's degree in Computer Science, Mathematics, Statistics or a related field, and 5+ years of related experience.

GuideSpark is an E-Verify and Equal Opportunity Employer"
104,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Alto is a modern pharmacy changing the way people manage and fill their prescriptions with its tech-driven, patient-centric, online pharmacy. Alto provides same-day, free delivery for prescriptions, seven days a week. More importantly, Alto offers price transparency, personalized mobile support, and real-time coordination with doctors and insurance companies. Our mission is to fulfill medicine's true purpose - to improve the quality of life for everyone who needs it.

We focus solely on patients' healthcare needs, and we are changing the pharmacy landscape.

We are a Series C startup with over $74M in funding and a current run rate of over $120M (4x a year ago), projected to break $300M by the end of 2019. Start-up tracker, CB Insights, recently listed Alto as one of the top 50 private companies around the world on a path to a $1 billion valuation. Alto is also featured in The New York Times (2/19) article ""These 50 Start-Ups May Be the Next ""Unicorns"" ( https://www.nytimes.com/2019/02/10/technology/these-50-start-ups-may-be-the-next-unicorns.html )"".

We are looking for a data engineer to be a key member of our quickly growing team. Our analytics team is responsible for using detailed analyses to guide product strategy and for enabling teams across Alto to answer key business questions. As the first data engineer at Alto, you will design and build the systems we need to support high-quality, data-driven decisions.

We've taken the first step in building out our analytics platform by instrumenting several of our core workflows, and we're looking for someone to help us take the next step in defining the future systems that will be needed to support and scale analytics.

Example projects of an Alto data engineer include:

Instrumenting one of our operational processes like prescription refill requests so that we can better understand if patients needing refills are successfully getting those medications or if there are problems in the process we can tackle to make sure they do.
Building out a data integration for our phone system so we can better understand the business metrics for our outbound and inbound patient calls.
Deciding what data stores we need to store various types of analytics, and how we will enable teams across Alto to work with that data.

You will:

Build the instrumentation, data integrations, and tools that will allow our analytics team to answer key business questions accurately and efficiently.
Design and implement the infrastructure and data models that will allow our analytics systems to scale
Collaborate with engineers and analysts to develop the technical strategy for our analytics and reporting systems
Work cross-functionally with product, operations, and growth teams to support data analysis across all business functions
Contribute to defining our team and company data culture through peer collaboration, coaching, and input into the team processes we adopt as we grow the analytics team together

You are an ideal candidate if you:

Have 3+ years of experience in a data engineering role
Are familiar with one or more backend programming languages, particularly Ruby, Go, or Python
Are proficient in SQL
Are passionate about solving real user problems with data-driven solutions
Care deeply about helping teammates across the organization have the ability to answer key business questions
It's a plus if you have experience implementing a data warehousing system, but not required

Alto Pharmacy is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. We are an E-Verify company."
105,"Engineering Manager, Anti-Evil","San Francisco, CA",San Francisco,CA,None Found,None Found,"
3+ years managing high performing engineering teams.
5+ years of experience as a Software Engineer or Data Engineer developing production code.
Experience in one or more of the following areas: machine learning / artificial intelligence, pattern recognition or large-scale data mining.
Experience with Kafka, Hadoop/Hbase/Pig or MapReduce/Bigtable
Programming experience in one or more of the following languages: C, C++, Java, JavaScript or Python; designing, implementing, testing, and maintaining highly complex systems and subsystems
Results-oriented with a strong customer and business focus.
Entrepreneurial and self-directed, innovative, biased towards action in fast-paced environments.
Able to communicate and discuss complex topics with technical and non-technical audiences.
Able to tackle ambiguous and undefined problems.
",None Found,"
Architect & define strategies that help keep Reddit safe.
Leverage ML models to identify offending content and take appropriate actions.
Increase efficiency through automation, improved signals, workflow streamlining and system optimization.
Participate in the full development cycle: design, develop, QA, experiment, analyze, and deployment.
Collaborate across disciplines to find technical solutions to complex challenges.
",None Found,None Found,"""The front page of the internet,"" Reddit brings over 330 million people together each month through their common interests, inviting them to share, vote, comment, and create across thousands of communities. Come for the cats, stay for the empathy.

Generating billions of events and terabytes of data a day, we're in the unique position to revolutionize how we protect Redditors.

You will be joining the Anti-Evil Engineering team as a leader, working closely with ops to defend Reddit's integrity by fighting spam, abuse, bots, and fraud. Anti-evil team members are motivated to find innovative, state of the art solutions, leveraging ML models, data systems and user insights to pursue the highest possible quality and safety standards for users across our products.

If ensuring the safety of users on the 4th most popular website in the US excites you, then you've found the right place. We're a small, tightly knit company with one of the highest user: employee ratios in the industry, giving you a tremendous opportunity for both scope and impact.

Responsibilities:

Architect & define strategies that help keep Reddit safe.
Leverage ML models to identify offending content and take appropriate actions.
Increase efficiency through automation, improved signals, workflow streamlining and system optimization.
Participate in the full development cycle: design, develop, QA, experiment, analyze, and deployment.
Collaborate across disciplines to find technical solutions to complex challenges.

Qualifications:

3+ years managing high performing engineering teams.
5+ years of experience as a Software Engineer or Data Engineer developing production code.
Experience in one or more of the following areas: machine learning / artificial intelligence, pattern recognition or large-scale data mining.
Experience with Kafka, Hadoop/Hbase/Pig or MapReduce/Bigtable
Programming experience in one or more of the following languages: C, C++, Java, JavaScript or Python; designing, implementing, testing, and maintaining highly complex systems and subsystems
Results-oriented with a strong customer and business focus.
Entrepreneurial and self-directed, innovative, biased towards action in fast-paced environments.
Able to communicate and discuss complex topics with technical and non-technical audiences.
Able to tackle ambiguous and undefined problems.

"
106,Senior Data Engineer,"San Francisco, CA 94107",San Francisco,CA,94107,None Found,None Found,None Found,"
Design, architect and support new and existing data and ETL pipelines and recommend improvements and modifications
Design, build, and support the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Responsible for ingesting data into our data lake and data warehouse and providing frameworks and services for operating on that data using various toolsets
Responsible for extending data analytics platform to include real-time streaming analytics, big data analytics, and machine learning analytics capabilities
Work with the team to define and build datasets for dashboard/reporting, and pre-processed datasets for advanced analytics and machine learning
Work with security to implement data privacy and data security requirements to ensure solutions stay compliant to security standards and frameworks (such as ISO, SOC-II, etc.)
Contribute to the core design of data architecture, data models and schemas, and implementation plan
Use best practices in terms of testing, monitoring, alerting, auto-recovery, design patterns, security etc.
",None Found,"
BS or MS in Computer Science, Engineering, or a related technical discipline or equivalent experience
Experience in implementing complex ETL pipelines preferably in connection with mySQL, 3rd party systems (Salesforce, mixPanel,…), AWS S3, and cloud data warehouses
Work with cloud data warehousing for ad-hoc and advanced analytics such as Redshift, Athena, BigQuery, or Snowflake
Experience in designing and implementing data lakes and familiar with dimensional data modeling & schema design in data warehouses
Experience with BI and data visualization tools like Looker, Tableau or other BI tools
Proficient in Python programming
Familiar with cloud data platforms like AWS or GCP
Experience with cloud ETL tools like Stitch or Alooma and workflow management tools like Airflow or Luigi
Familiarity of how mysql/replication/clustering works
Excellent communication skills, particularly translating between technical and non-technical stakeholders
","Sauce Labs provides the world's largest automation cloud for testing web and native/hybrid mobile applications. Founded by the original creator of Selenium, Sauce Labs helps companies accelerate software development cycles, improve application quality and deploy with confidence across 500+ browser / OS platforms. Join us in making the world a better place for continuous integration and software development. We're building a next generation infrastructure as a service platform and looking for passionate Senior Data Engineer to join our Data Analytics team.

Our Data Analytics team consists of data scientists, data engineers, and data analysts working together to unlock the values of our data and to uncover new revenue opportunities as well as to create more compelling and differentiating user experience. As a Senior Data Engineer in the team, you will be playing a key role in architecting and building our next generation data analytics platform.

Responsibilities:

Design, architect and support new and existing data and ETL pipelines and recommend improvements and modifications
Design, build, and support the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Responsible for ingesting data into our data lake and data warehouse and providing frameworks and services for operating on that data using various toolsets
Responsible for extending data analytics platform to include real-time streaming analytics, big data analytics, and machine learning analytics capabilities
Work with the team to define and build datasets for dashboard/reporting, and pre-processed datasets for advanced analytics and machine learning
Work with security to implement data privacy and data security requirements to ensure solutions stay compliant to security standards and frameworks (such as ISO, SOC-II, etc.)
Contribute to the core design of data architecture, data models and schemas, and implementation plan
Use best practices in terms of testing, monitoring, alerting, auto-recovery, design patterns, security etc.

Requirements:

BS or MS in Computer Science, Engineering, or a related technical discipline or equivalent experience
Experience in implementing complex ETL pipelines preferably in connection with mySQL, 3rd party systems (Salesforce, mixPanel,…), AWS S3, and cloud data warehouses
Work with cloud data warehousing for ad-hoc and advanced analytics such as Redshift, Athena, BigQuery, or Snowflake
Experience in designing and implementing data lakes and familiar with dimensional data modeling & schema design in data warehouses
Experience with BI and data visualization tools like Looker, Tableau or other BI tools
Proficient in Python programming
Familiar with cloud data platforms like AWS or GCP
Experience with cloud ETL tools like Stitch or Alooma and workflow management tools like Airflow or Luigi
Familiarity of how mysql/replication/clustering works
Excellent communication skills, particularly translating between technical and non-technical stakeholders

"
107,Business Intelligence Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,"
BA/BS in a quantitative field
1-2 years of work experience as a data analyst, data engineer, or in a highly analytical role
Experience writing SQL queries and using a BI tool
Experience with a scripting language (preferably Python) for data processing and analysis a plus
Experience using the command line and git
Strong grasp of statistics and experience conducting rigorous data analyses
Experience developing models and visualizations in Looker a plus
Experience at an e-commerce or fintech company a plus
",None Found,"Who is Credible?

We believe life's changes create financial needs for people and that the traditional financial system often puts up unnecessary obstacles. People celebrate major milestones like going to college, getting married, and buying a home. And most of the time, these milestones come with financial implications.

At Credible, we have built a company with the mission of bringing transparency, choice, simple processes and savings to accessing credit for life's important moments. What you see is what you get. We are committed to being upfront, honest, and clear about your options. There are no mysteries, no hidden fees, and no secret clauses.

Credible is a fast-growing Australian Securities Exchange (ASX) listed Fintech company that has world class management, has raised multiple rounds of funding, is generating significant revenue and is disrupting the lending market and helping people save money and get out of debt faster.

About the role

Our Business Intelligence team is looking for a Business Intelligence Data Engineer who is passionate about data, analytics, and business strategy. You will help the team learn more about our business, teach others in the company about analytics, and improve the use of our data. You'll be an integral part of providing data-driven insights that inform significant company decisions.

You Will:

Build data pipelines and python-based ETL tools for getting, processing, and delivering data
Partner with teams across the organization to understand their analytics needs and create dashboards and reporting that allow them to execute more effectively
Work with business leaders to define key metrics and build reporting to monitor and understand performance along those metrics
Conduct in-depth data analyses that lead to actionable insights, owning the entire process from ideation to execution to presentation of findings to stakeholders
Develop data models in our data warehouse that enable performant, intuitive analysis
Become an expert on all aspects of Credible's data and analytics infrastructure
Be the driving force behind the adoption and effective use of our BI tool within every team at Credible

Education and Experience:

BA/BS in a quantitative field
1-2 years of work experience as a data analyst, data engineer, or in a highly analytical role
Experience writing SQL queries and using a BI tool
Experience with a scripting language (preferably Python) for data processing and analysis a plus
Experience using the command line and git
Strong grasp of statistics and experience conducting rigorous data analyses
Experience developing models and visualizations in Looker a plus
Experience at an e-commerce or fintech company a plus

Personality and Values:

The capacity to juggle multiple priorities effectively within a fast-paced environment is critical
You're a highly motivated self-starter with the ability to work efficiently with minimal supervision.
Anticipate business needs and think with a business owner mindset – think critically about analyses, don't just complete them
Passion for spreading the value of data throughout the company and communicating insights to a broad audience with varying levels of technical expertise

Why you want to work at Credible

We are a fast moving, fun-loving, seriously smart group of people who really care about impacting the lives of our customers. We empower our employees to make decisions, take risks, drive our business and make changes when we don't get it right. These are our values:


Exceed Customer Expectations: We provide an exceptional experience to each and every customer that compels them to share it with others.
Take Ownership: We are trusted to make decisions that are in the best interests of our customers and our business. We think and act like owners. We care – and that makes all the difference.
Be Curious: We are curious, ask questions, seek to understand and try new things.
Do the Right Thing: We earn trust by being transparent, respectful and honest with each person with whom we interact.
Get Results: Results fuel our excitement and we know how our personal accomplishments tie to the success of the company
Be Bold: We are courageous and take risks that scare us. Our enthusiasm for experimenting is how we will find the next breakthrough.

Our benefits: We offer competitive compensation, generous benefits, free food and a flexible vacation policy.

But mainly, you want to work at Credible because you believe in our mission and want to have a major role in delivering on it! We look forward to getting to know you.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
108,Data & Analytics- Data Engineer,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,"
Bachelor’s degree in Computer Engineering, Computer Science, Information Systems or related discipline
 3+ years relevant experience
Experience in capturing end users requirements and align technical solutions to the business objectives
Understanding of different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)
Understanding of data architecture concepts such as data modeling, metadata, workflow management, ETL/ELT, real-time streaming), data quality
 3+ years of experience working with SQL
Experience with setting up and operating data pipelines using Python or SQL
 1+ years of experience working on AWS, GCP or Azure
Experience working with data warehouses such as Redshift, BigQuery and Snowflake
Exposure to open source and proprietary cloud data pipeline tools such as Airflow, Glue and Dataflow
Experience working with relational databases
Experience with data serialization languages such as JSON, XML, YAML
Experience with code management tools (e.g. Git, SVN) and DevOps tools (e.g. Docker, Bamboo, Jenkins)
Strong analytical problem-solving ability
Great presentation skills, written and verbal communication skills
Self-starter with the ability to work independently or as part of a project team
Capability to conduct performance analysis, troubleshooting and remediation",None Found,None Found,None Found,None Found,"About Slalom
Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of what’s possible—together.
Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to nearly 4,500 employees. We were named one of Fortune’s 100 Best Companies to Work For in 2017 and are regularly recognized by our employees as a best place to work. You can find us in 25 cities across the U.S., U.K., and Canada.
Data Engineer Consultant
As a Data Engineer for Slalom Consulting, you'll work in small teams to deliver data pipelines and data models for our clients. You will design and build highly scalable and reliable modern data platforms including data lakes and data warehouse using Amazon Web Services, Azure, Google Cloud. Your work will include a variety of core data warehousing tools, Hadoop, Spark, event stream platforms, and ETL tools such as Airflow. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics.
Who are you?
You have passion for data!
You’re a smart, collaborative person who is excited about technology and driven to get things done.
You’re not afraid to be bring your authentic self to work.
You embrace a continuous learner mentality.
Who are we?
We are engineers, makers, planners, architects, and designers.
 We choose to imagine things made better, and then set out on a journey to realize what’s possible.
We’ll never trade the upside of wonder for the comfort of the familiar or the safety of convention.
What technologies will you be using?
Every element of a modern data & analytics stack. It’s about using the right technologies to solve problems and playing with new technologies to figure out how to apply them intelligently. We work with technologies across the board.
Why do we work here?
Each of us came to Slalom because we wanted something different. We wanted to make a difference, we wanted autonomy to own and drive our future while working with some of the best companies in San Francisco leveraging the coolest technologies. At Slalom, we found our people.
Qualifications:
Bachelor’s degree in Computer Engineering, Computer Science, Information Systems or related discipline
 3+ years relevant experience
Experience in capturing end users requirements and align technical solutions to the business objectives
Understanding of different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)
Understanding of data architecture concepts such as data modeling, metadata, workflow management, ETL/ELT, real-time streaming), data quality
 3+ years of experience working with SQL
Experience with setting up and operating data pipelines using Python or SQL
 1+ years of experience working on AWS, GCP or Azure
Experience working with data warehouses such as Redshift, BigQuery and Snowflake
Exposure to open source and proprietary cloud data pipeline tools such as Airflow, Glue and Dataflow
Experience working with relational databases
Experience with data serialization languages such as JSON, XML, YAML
Experience with code management tools (e.g. Git, SVN) and DevOps tools (e.g. Docker, Bamboo, Jenkins)
Strong analytical problem-solving ability
Great presentation skills, written and verbal communication skills
Self-starter with the ability to work independently or as part of a project team
Capability to conduct performance analysis, troubleshooting and remediation"
109,Data engineer,"Emeryville, CA",Emeryville,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Details
Position : Data engineer
MS SQL as primary. .NET / Java backend expertise will be plus
2+ years of experience in Google Cloud or Azure
1+ years of experience with Cassandra or Kafka
Certification in AWS or database administration"
110,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Scoop

Scoop brings co-workers and neighbors together to enjoy a smooth carpooling experience—unlocking new opportunities to create friendships, improve their well-being, and make the most of their valuable time.

Learn more in Crunchbase: https://news.crunchbase.com/news/scoop-raises-60m-for-corporate-carpooling-as-gridlock-ruins-america/

Engineering @ Scoop

Few companies get to face such diverse technical challenges as Scoop, and we’ve built a team of people excited to face these challenges together while investing in each others’ growth.

Scoop’s engineering team may move bits and pixels, but we also put real, live human beings in cars together. We’re touching problems academics have written about for years, and have data that no other company has ever collected.

But Scoop knows engineering is not a lone discipline. We’re a small team with varied backgrounds: big companies, VC-backed startups, bootcamps, academia. We like to build together, and we like to learn together. Our entire team and process are built around helping you grow and be successful. And we’d love to tell you more about the impact you could have at Scoop.
In this role, you will:
Architect, develop, and deploy infrastructure for large scale ETL pipelines with data processing frameworks
Productionizing machine learning - from research into fault-tolerant, production-scale deployments
Manage data workflows
Work closely with the Data Science, Analytics, Product and Platform teams to understand business needs and create a data platform that serves business needs
You should:
5+ years or equivalent experience
Be proficient in Python and/or Scala (Scala preferred)
Experience designing and operating distributed systems
Emphasis in high code quality, high code clarity, automated testing, and engineering best practices
Be able to clearly communicate complex technical concepts

Preferred Qualifications:
Mobility/Transportation domain knowledge
Worked with open sourced projects like Spark, Kafka or Airflow
Life @ Scoop

Founded in 2015 and based in downtown SF, our team mixes technology and elbow grease every day, with one statistic in our crosshairs: 80% of Americans drive alone to work. At Scoop, we envision a world where commuters feel empowered — starting with a choice to make their commute a meaningful part of their day. We embody that same spirit within our own culture, empowering every team member to make this the most meaningful experience of their career.

Walk into Scoop and you’ll find a furry, tail-wagging welcoming committee. In many ways these fluffy faces exemplify the energy that flows through our office. They are a reminder that while we’re focused and driven, we shouldn’t take ourselves too seriously. They also help bridge the gap between our homes and our workplace, just like a Scoop carpool.

The atmosphere overall is dynamic and unique. It’s influenced by our backgrounds at successful startups, big tech companies, and premier consulting firms — blended and crafted into what feels natural and right for this company. It plays out in our balance of scrappy and strategic, frameworks and fast thinking.

At Scoop, we’re all united by our desire to change the way people get to work — and committed to enjoying the journey together along the way."
111,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,"
Bachelor's degree required or equivalent combination of education and experience.
","
Ability to physically perform general office requirements
Must be able to perform essential responsibilities with or without reasonable accommodations
","Senior Quality Engineer

Who we are:
Our Company was founded on the idea that there are patterns in people's behavior that, with the right logic, can be used to predict future outcomes. We are a small but rapidly growing organization that works in partnership with our customers to create solutions that are simply not found anywhere else. We work in groups rather than in structured corporate hierarchies; our culture is creative and entrepreneurial where everyone contributes to company goals in very real way. We are a hardworking group, but we have a lot of fun with what we do and are looking for new people with a similar mindset to join the organization.

What we do:
Our proprietary software-as-a-service helps automotive dealerships and sales teams better understand and predict exactly which customers are ready to buy, the reasons why, and the key offers and incentives most likely to close the sale. Its micro-marketing engine then delivers the right message at the right time to those customers, ensuring higher conversion rates and a stronger ROI.

As aM continues to expand its engineering community and mature in its Agile approach, the infrastructure required to rapidly build, test, and deliver assets will become increasingly complex.

What you will do:
Responsible for processing structured and unstructured data and designing, developing and supporting data and analytics solutions using one or more technology.


Full project life-cycle experience
Understanding of Agile software development and project management. Working knowledge of Scrum.
A self-starter with an enthusiasm for technology
Design, build, improve, and manage data pipelines and ETL jobs
Experience using tools such as Airflow or Luigi, Oozie
Implement and optimize data solutions in data warehouses and big data repositories, including cross-cloud data pipelines
Build data expertise and own data quality for the awesome pipelines you build
The ability to apply design patterns, best-practices and novel approaches to overcoming technical challenges.
Experience building high-performance data back-end services
Thorough understanding of Python, Scala is a plus
Back-end data modeling
Working experience of data analytics and partner with data science to deploy operational metrics and machine learning algorithms for core products
Implementation experience with real-time and batch data processing frameworks – Apache Beam, Spark Streaming
Working knowledge of
Hadoop or Spark, distributed systems
GIT
Docker
Kubernetes or other orchestrator
Analytical skills and the ability to pay attention to detail
Develop apps that run on a cloud infrastructure - Azure, AWS, G-cloud
Strong communication skills required to be able effectively communicate with both technical and non-technical teammates and stakeholders
The ability to work and thrive in a start-up environment

Who you are:
Senior Data Engineer with 4+ years' experience building enterprise-level data and analytics products. Subject matter expert / go to person for one or more applied technologies. Should have the ability to mentor more junior team members. Should be self – directed and able to bring best practices as well as fresh new ideas to the team.

Education:

Bachelor's degree required or equivalent combination of education and experience.

Physical Requirements:

Ability to physically perform general office requirements
Must be able to perform essential responsibilities with or without reasonable accommodations

Travel:

Occasional travel including domestic or international trips may be required.

Expected Hours of Work:
This is a full-time position. Generally, work is performed Monday through Friday, though holidays and weekends may be required.

We believe in equal employment opportunities:
The company provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, the company complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

The company expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of the company's employees to perform their job duties may result in disciplinary actions up to and including discharge."
112,Senior Data Engineer - Data Engineering,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Slack is looking for expert data engineers to join our Data Engineering team. In this role, you will be working cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data Warehouse model. You will design, implement and scale data pipelines that transform billions of records into actionable data models that enable data insights.

You will lead initiatives to formalize data governance and management practices, rationalize our information lifecycle and key company metrics. You will provide mentorship and hands-on technical support to build trusted and reliable metrics.

You have deep technical skills, be comfortable contributing to a nascent data ecosystem, and building a strong data foundation for the company. You are a self-starter, detail and quality oriented, and passionate about having a huge impact at Slack.

What you will be doing
Translate business requirements into data models that are easy to understand and used by different disciplines across the company
Design, implement and build pipelines that deliver data with measurable quality under the SLA
Partner with business domain experts, data analysts and engineering teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service
Be a champion of the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements
Own and document foundational company metrics with a clear definition and data lineage
Identify, document and promote best practices
What you should have
5+ years of experience working in data architecture, data modeling, master data management, metadata management
Recent accomplishments working with relational as well as NoSQL data stores, methods and approaches (logging, columnar, star and snowflake, dimensional modeling)
Proven track record in scaling and optimizing schemas, performance tuning SQL and ETL pipelines in OLAP and Data Warehouse environments
Demonstrated skills with either Python or Java programming language
Familiar with data governance frameworks, SDLC, and Agile methodology
Excellent written and verbal communication and interpersonal skills, and ability to effectively collaborate with technical and business partners
Hands-on experience with Big Data technologies (e.g Hadoop, Hive, Spark) is a big plus
Bachelor's degree in Computer Science, Engineering or a related field, or equivalent training, fellowship, or work experience


Slack is an Equal Opportunity Employer and participant in the U.S. Federal E-Verify program. Women, minorities, individuals with disabilities and protected veterans are encouraged to apply. Slack will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance.

Slack is a layer of the business technology stack that brings together people, data, and applications – a single place where people can effectively work together, find important information, and access hundreds of thousands of critical applications and services to do their best work. From global Fortune 100 companies to corner markets, businesses and teams of all kinds use Slack to bring the right people together with all the right information. Slack is headquartered in San Francisco, CA and has ten offices around the world. For more information on how Slack makes teams better connected, visit slack.com.
Ensuring a diverse and inclusive workplace where we learn from each other is core to Slack’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and a pleasant and supportive place to work.
Come do the best work of your life here at Slack."
113,Senior Data Engineer,"San Francisco, CA 94110",San Francisco,CA,94110,None Found,None Found,None Found,None Found,None Found,None Found,"Earnest empowers people with the financial capital they need to live better lives.

We use technology, data, and design to:

Build more affordable products
Deliver them to more people
Engage through more human experiences

As a Senior Data Engineer, you will report to the Head of Data and work closely with other members of the Data team to maintain and improve data pipelines, deploy predictive models as a service, and work on our data infrastructure.

Team Philosophy:

We believe that data engineers should not write ETL, unless data pipelines are necessary to support the services that we build and maintain. Instead, your time is better spent writing tooling and creating abstractions to support the work of analysts and data scientists on the team, so that other users in the company can write and maintain their own ETL.
Everyone on the team is considered and treated as a Senior, so you are given a lot of ownership over the projects you're working on. With this in mind, we expect you to write code that is well tested, modular, and maintainable. We frown at instances of destructive fire fighting, caused by badly designed architecture or mismanaged projects.
We like learners. Hence, we create an environment where Data Scientists learn the skills and abstraction patterns of Software Engineers and Data Engineers learn the iterative model development workflow seen in Machine Learning. Expect an environment where you will be constantly learning and challenged to interact with tools, languages, frameworks that are outside of your area of expertise.

Tools, Frameworks, and Languages you will work with:

Languages: Python, Scala, and occasionally R and Node.js
Storage: Postgres, Redshift, S3
Compute: Spark, Athena, EC2
Workflow Management: Airflow
Infrastructure: Terraform, Kubernetes

Potential Projects:

Deploy trained machine learning models as a Python microservice.
Help migrate legacy ETLs into Airflow
Write tooling to make ETL accessible to engineers on other teams.

Ideal background and expertise:

5+ years of experience as a full-time professional data engineer
Experience with server-side concepts, e.g. microservices, database, caching, performance, monitoring and scalability
3+ years of experience with Python or Scala
Working experience with OLTP databases such as Postgres, MySQL
Extensive experience making data available in OLAP databases such as Redshift or Snowflake
Experience with orchestration tools such as Airflow (preferred), Luigi, Ooozie

Nice to Have:

Familiarity with Business Intelligence tools such as Tableau, Looker, Superset
Domain experience developing software for Fintech, Banking, or related Consumer Financial Services companies

Earnest Perks & Benefits:

Health, Dental, & Vision benefits plus savings plans
Employee Stock Purchase Plan
401(k) plan to help you save for retirement plus a company match
Tuition reimbursement program
$1000 flight on each Earnie-versary to anywhere in the world and 25 days of annual PTO

Earnest provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, sexual orientation, disability, genetics, gender identity or expression, or veteran status. Qualified applicants with criminal histories will be considered for the position in a manner consistent with the Fair Chance Ordinance."
114,Data Engineer,"Berkeley, CA",Berkeley,CA,None Found,None Found,"
Proficiency in Python
Proficiency in SQL and familiarity with non-relational databases
A technical stack that includes most or all of: Python, Postgres/PostGIS, Elasticsearch, AWS, GIS, Tensorflow, git
Experience creating/managing large databases and pipelines
At least 2 years of relevant full-time professional work experience in the private sector, writing code in a production environment
The ability to work well with technical experts who aren’t software people and to effectively communicate engineering, concepts, requirements, and processes to non-engineers
Excitement about joining a fast-growing early-stage company, comfort with a dynamic work environment, and eagerness to take on a range of responsibilities
Enjoyment in getting the details right
",None Found,"
Help develop KoBold’s proprietary software called Machine Prospector™
Design aspects of the data system to store and quickly process and access a wide variety of geoscience data
Integrate geophysical, geochemical, geologic, and geographic data from around the world into a well-structured proprietary database
Create tools for ingesting unstructured datasets and extracting key features
Work with data scientists to implement and improve algorithms and predictive models
Develop interactive visualizations to enable the data team and geoscience teams to rapidly view and interrogate the model results and underlying data
",None Found,None Found,"About the company
KoBold Metals is using machine learning to discover new sources of the critical materials needed for electric vehicles. Our Machine Prospector™ software predicts the locations of potential ore deposits by applying advanced scientific computing techniques to geoscience data from around the globe. We use our models to decide where to acquire land and to guide our geologists’ exploration program in the field.

We are exploring for new sources of critical battery materials, such as cobalt and nickel. As electric vehicles become widespread, many new mines will be needed, but before we can mine new deposits, we have to find them. KoBold’s team combines the world’s top mineral explorers with an outstanding team of data scientists and engineers, and our objective is to discover many new ethical sources of battery materials.

KoBold Metals is backed by Andreessen Horowitz and Breakthrough Energy Ventures.

About the position
In this role, you will help develop software that accelerates discovery of new supplies of cobalt, and you will help build a worldwide geospatial dataset that underlies our digital exploration program. As part of KoBold’s digital exploration team, you will work closely with both KoBold’s team of world-renowned geoscientists and our data team. Ultimately, your role is to help build data tools to solve challenging scientific problems and to enable the widespread adoption of electric vehicles. As one of the first members of this team, you will help build these tools from the ground up.

Responsibilities
The Data Engineer will:

Help develop KoBold’s proprietary software called Machine Prospector™
Design aspects of the data system to store and quickly process and access a wide variety of geoscience data
Integrate geophysical, geochemical, geologic, and geographic data from around the world into a well-structured proprietary database
Create tools for ingesting unstructured datasets and extracting key features
Work with data scientists to implement and improve algorithms and predictive models
Develop interactive visualizations to enable the data team and geoscience teams to rapidly view and interrogate the model results and underlying data
Qualifications
A great Data Engineer candidate will have:

Proficiency in Python
Proficiency in SQL and familiarity with non-relational databases
A technical stack that includes most or all of: Python, Postgres/PostGIS, Elasticsearch, AWS, GIS, Tensorflow, git
Experience creating/managing large databases and pipelines
At least 2 years of relevant full-time professional work experience in the private sector, writing code in a production environment
The ability to work well with technical experts who aren’t software people and to effectively communicate engineering, concepts, requirements, and processes to non-engineers
Excitement about joining a fast-growing early-stage company, comfort with a dynamic work environment, and eagerness to take on a range of responsibilities
Enjoyment in getting the details right
It is helpful but not required to have:

Experience with geospatial databases, analyses, and/or visualizations
Experience with planning and implementing information security measures
A bachelor’s degree or higher in the physical sciences, engineering, computer science, or mathematics
KoBold Metals is an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity for people of any race, color, ancestry, religion, sex, gender identity, sexual orientation, marital status, national origin, age, citizenship, marital status, disability, or veteran status.

This position is full-time, exempt.

Location: KoBold Metals has an office in Berkeley, CA; partial remote work is an option.

Interested applicants should contact info@koboldmetals.com."
115,Data Engineer- Marketplace,"San Francisco, CA 94103",San Francisco,CA,94103,None Found,None Found,None Found,None Found,None Found,None Found,"At Uber, we ignite opportunity by setting the world in motion. We take on big problems to help drivers, riders, delivery partners, and eaters get moving in more than 600 cities around the world.

We welcome people from all backgrounds who seek the opportunity to help build a future where everyone and everything can move independently. If you have the curiosity, passion, and collaborative spirit, work with us, and let’s move the world forward, together.
About the Role

Market Data and Simulation team builds the data platform foundation for all the products like core pricing, rider promotion, surge pricing, driver incentives, matching, subscription (e.g. monthly pass). Their query engine, data, metrics and insights cover Uber’s entire knowledge about the dynamics of Marketplace, rider, drivers in realtime. Engineers get to work on really large scale streaming systems such as Flink, Samza, Spark. You could work on deep optimization of our real-time query engine, The project we do typically yields impact to more than one product line. Our team always has a global view of Uber’s Marketplace, it is the place where one can quickly learn the domain knowledge about how a dynamic Marketplace works.
What You’ll Do

Build distributed data systems at a huge scale with state-of-the-art technology (Flink, Samza, Spark).
Build distributed backend systems serving realtime analytics and machine learning features at Uber scale.
Design and develop new tools to empower fast data driven decisions

What You’ll Need
3+ years of full-time engineering experiences, in particular, with large scale distributed systems.
General coding chops with Java, Python or Scala
Proficiency in Spark/MapReduce / Flink (Realtime Streaming), development and expertise with data processing technologies.
Knowledge of Hadoop related technologies such as HDFS, Kafka, Hive, and Presto.
Knowledge of SQL and data analysis
Experience with large-scale data warehousing architecture and data modeling.
Machine learning will be good to have.
A team player attitude. You believe that you can achieve more on a team - that the whole is greater than the sum of its parts. You rely on others' candid feedback for continuous improvement and you help others by returning the favor.

Bonus points if
BS/MS/PhD in Computer Science or a related field"
116,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,"
Extensive experience working with at least one big data tool such as Spark, Redshift/Snowflake, Airflow/Luigi, etc
Basic working knowledge other big data technologies
Strong computer science fundamentals
Experience building and maintaining critical, reliable ETL pipelines
Experience writing production-level code in Python, Go, sparkSQL, PySpark, or Scala, Fluent in SQL
Great communicator who can easily break down complex concepts to non-technical stakeholders
Bonus points if you have experience in any of the following areas: Containerization, Kubernetes, working with data scientists and can understand their needs, and/or machine-learning and/or deep learning platforms",None Found,None Found,None Found,"We have a simple mission at Calm: To make the world a happier and healthier place.

The heart of Calm is digital but the brand is expanding offline into a variety of products and services that bring more peace, clarity and perspective into people’s busy lives. We are building Calm into the Nike of the Mind. We believe Calm can become one of the most valuable and meaningful brands in the world.

Over 50 Million people have downloaded the app and we are growing by 85,000 new downloads a day. The company is profitable and headquartered in San Francisco, CA.

Calm was co-founded by Alex Tew (Million Dollar Homepage) and Michael Acton Smith (Mind Candy, Moshi Monsters, Firebox).


At Calm we are currently building a state-of-the-art data system using Kafka, Spark, Spark Streaming, Redshift, and Airflow, that is run in the cloud on AWS, deployed via Docker and Kubernetes, and has a codebase written with Go, Python, SQL and PySpark. We are looking for a data engineer with a solid computer science background and industry experience who can help us define, design, build, and optimize our data ecosystem. You will help us integrate different data sources, improve the efficiency, reliability, and latency of our data system, help automate data pipelines, and improve our data model and overall architecture. Bonus points if you have experience working with data scientists, building machine-learning platforms (e.g., for recommender systems), and understand the real-world, business use and impact of data. Calm has an extremely data-driven culture and your work will be critical to our success as a company.
What you will do:
Build data integrations within our data platform and between partners
Write well-tested, production ready code in Python, Go, and/or SQL
Improve the efficiency, reliability, and latency of our data system
Create automated, highly reliable data pipelinesHelp define and craft our data model
Onboard onto Airflow and Redshift and assist with maintenance
Define, design, and build data testing and quality frameworks
Test all code written and ensure production readiness before shipping
Work cross-functionally with our product and growth teams on complex and exciting projects that propel Calm’s business
Skills needed:
Extensive experience working with at least one big data tool such as Spark, Redshift/Snowflake, Airflow/Luigi, etc
Basic working knowledge other big data technologies
Strong computer science fundamentals
Experience building and maintaining critical, reliable ETL pipelines
Experience writing production-level code in Python, Go, sparkSQL, PySpark, or Scala, Fluent in SQL
Great communicator who can easily break down complex concepts to non-technical stakeholders
Bonus points if you have experience in any of the following areas: Containerization, Kubernetes, working with data scientists and can understand their needs, and/or machine-learning and/or deep learning platforms
BenefitsCompetitive salary and equityUnlimited PTOWe pay your medical, dental, & vision insurance premiums401KCommuter benefitsLife insurance and disability benefitsApple equipment
- Fun, energetic work environment, and daily perks - snacks, drinks, catered lunch twice a weekOpportunity to work with a product focused on making the world happier and healthier
And much more!"
117,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"---------------------------------------------------
Build the data infrastructure to help Flexport grow
---------------------------------------------------

----------------

The opportunity:
----------------

Flexport's data engineering team is just getting started on building the infrastructure that our global company, spread across three continents, relies on daily as we change an industry. Your role as a senior data engineer will be to come in and lay the groundwork to help make sure we have reliable data sources and a resilient infrastructure that will stand the test of time. If we can get this right, we will be providing insight into a major part of our global economy that we've never seen before. We're just getting started and are looking for people who are excited about the journey.

Our pipelines are built using Python and SQL, scheduled in Apache Airflow, glued with bash. Near Real-Time Data Warehouse in Postgres and Data Lake in Redshift. All of our infrastructure is based on AWS. Our BI data is only 15 minutes behind production servers.

---------

You will:
---------


You will develop resilient pipelines from a variety of data sources, both internal and external
You will implement Comprehensive Testing and Continuous Integration frameworks for schema, data, and functional processes/pipelines
You will design the scheduling framework (Airflow) to automatically recognize and set-up object dependencies for robust and timely data delivery
You will be part of a close-knit data engineering team that ships new code every day

----------------

You should have:
----------------


You have 5+ years experience developing data frameworks using Python
You have 5+ years of using SQL for data manipulation in a fast-paced work environment
You have experience with Airflow is strongly preferred
Data Warehousing development and fundamentals preferred
You have a passion for business-oriented data development

---------------

About Flexport:
---------------

We believe global trade can move the human race forward. That's why it's our mission to make global trade easier for everyone. We aim to do this by building the Operating System for Global trade - a strategic model combining advanced technology and data analytics, logistics infrastructure, and supply chain expertise. Flexport today connects almost 10,000 clients and suppliers across 109 countries, including established global brands like Georgia-Pacific as well as emerging innovators like Sonos. Started in 2013, we've raised over $1.3B in funding from SoftBank Vision Fund, Founders Fund, GV, First Round Capital and Y Combinator. We're excited about the three big ways we're moving forward after our recent $1B investment from SoftBank Vision Fund ( https://www.flexport.com/blog/flexport-secures-usd1-billion-in-funding-led-by-softbank-vision-fund ) in February 2019.

-----------------------------------------------------------
Worried about not having any freight forwarding experience?
-----------------------------------------------------------


Don't be! We're building the first Operating System for Global Trade. That's why it's incredibly important for us to bring people from diverse backgrounds and experiences ( https://www.flexport.com/careers/diversity-and-inclusion ) together with our industry veterans to help move the freight forwarding industry forward.
What's freight forwarding and why does it matter? Freight forwarding is the coordination and shipment of goods from one place to another and it's what makes global trade possible. Flexport is on a mission to make global trade easier for everyone because we believe it can help connect the world and break down economic barriers ( https://www.youtube.com/watch?v=3jvRNATITmQ ).
We know this industry is complex. That's why we invest in education starting day one with Flexport Academy ( https://www.youtube.com/watch?v=KQxp0Y8kSi8&t=2s ), a one week intensive onboarding program designed specifically to set every new Flexport employee up for success.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
118,eCommerce Data Engineer Operations Manager,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
5+ years experience in an operations or finance supervisor / manager role
Experience working with ERP systems – SAP preferred; especially knowledge of setup / implementation and understanding of the meta data
Prefer accounting background, and experience with financial audit procedures
Deep understanding of data, data relationships, how data is used to drive systems behavior, and reporting
Experience defining requirements and working with engineering teams to develop applications, reports, analytics tools, or back-end data processing jobs
Prefer experience with user administration for one or more shared applications such as Finance, HR, Order Management, Customer Service, or similar:
Managing user accounts, setting access privileges and roles
Supporting end user inquiries, debugging data issues / collaborating with engineers to fix problems
Managing 3rd party vendors, especially SaaS solutions
Prefer experience with software and vendor selection / RFP process: gathering requirements, writing an RFP, interacting with vendors, defining proof of concept or customized demos, developing a vendor scorecard, managing a cross functional team to review proposals and arrive at a decision
Solid understanding of ticketing software used for issue tracking - such as Jira, Zendesk, ServiceNow, or Pivotal Tracker - prefer knowledge of setup and configuration to support workflows specific to company requirements
Experience providing frontline support to a user community – either internal or external customers: issue intake, triage, analysis, escalation, resolution, and communications processes",None Found,"
Research and document data sources:
ERP and finance
Data feeds from resellers such as Amazon and Walmart
Syndicated data including online behavior and demographics
Supply chain including purchase orders, shipments, invoices, etc
Staffing:
Develop staffing plan for your team
Assessment of candidate skills and team fit
Mentor new team members on best practices around data management and support
Develop documentation and train team members on use of our solutions
Develop requirements and document in the form of high-level PRDs for initial concept, and stories for engineering use in development; collaborate with engineers to build best in class solutions
Maintain product roadmap for data engineering
Define business roles for monitoring and alerting rules, and work with engineers to automate
End-user support for data engineering:
Finance
Operations
Business Units
Other engineering teams that use our data repositories for analytics, reporting, and other applications
Systems monitoring: work with engineering team to define metrics / dashboards / reports, which you will use daily to keep an eye on our systems; we want to know about issues before our customers do!",None Found,"
5+ years experience in an operations or finance supervisor / manager role
Experience working with ERP systems – SAP preferred; especially knowledge of setup / implementation and understanding of the meta data
Prefer accounting background, and experience with financial audit procedures
Deep understanding of data, data relationships, how data is used to drive systems behavior, and reporting
Experience defining requirements and working with engineering teams to develop applications, reports, analytics tools, or back-end data processing jobs
Prefer experience with user administration for one or more shared applications such as Finance, HR, Order Management, Customer Service, or similar:
Managing user accounts, setting access privileges and roles
Supporting end user inquiries, debugging data issues / collaborating with engineers to fix problems
Managing 3rd party vendors, especially SaaS solutions
Prefer experience with software and vendor selection / RFP process: gathering requirements, writing an RFP, interacting with vendors, defining proof of concept or customized demos, developing a vendor scorecard, managing a cross functional team to review proposals and arrive at a decision
Solid understanding of ticketing software used for issue tracking - such as Jira, Zendesk, ServiceNow, or Pivotal Tracker - prefer knowledge of setup and configuration to support workflows specific to company requirements
Experience providing frontline support to a user community – either internal or external customers: issue intake, triage, analysis, escalation, resolution, and communications processes","PepsiCo’s eCommerce division is expanding into the San Francisco bay area of California, with a focus on leading-edge technology in the areas of machine learning, big data, eCommerce, data engineering, and more. This is an exciting opportunity to help build a new team, define our operating processes, quality metrics and controls, and support systems.
The Data Engineering team is looking for a seasoned manager to lead operations. You will collaborate with team members from Finance, Engineering, Product, and PepsiCo Business Units to gain in-depth knowledge of our data sources, data requirements and goals for data analysis and other data usage; define requirements for data transformations and other data processing; data quality metrics and data audit steps; as well as first level support for our systems.
Responsibilities:
Research and document data sources:
ERP and finance
Data feeds from resellers such as Amazon and Walmart
Syndicated data including online behavior and demographics
Supply chain including purchase orders, shipments, invoices, etc
Staffing:
Develop staffing plan for your team
Assessment of candidate skills and team fit
Mentor new team members on best practices around data management and support
Develop documentation and train team members on use of our solutions
Develop requirements and document in the form of high-level PRDs for initial concept, and stories for engineering use in development; collaborate with engineers to build best in class solutions
Maintain product roadmap for data engineering
Define business roles for monitoring and alerting rules, and work with engineers to automate
End-user support for data engineering:
Finance
Operations
Business Units
Other engineering teams that use our data repositories for analytics, reporting, and other applications
Systems monitoring: work with engineering team to define metrics / dashboards / reports, which you will use daily to keep an eye on our systems; we want to know about issues before our customers do!
Qualifications/Requirements
Preferred Qualifications:
5+ years experience in an operations or finance supervisor / manager role
Experience working with ERP systems – SAP preferred; especially knowledge of setup / implementation and understanding of the meta data
Prefer accounting background, and experience with financial audit procedures
Deep understanding of data, data relationships, how data is used to drive systems behavior, and reporting
Experience defining requirements and working with engineering teams to develop applications, reports, analytics tools, or back-end data processing jobs
Prefer experience with user administration for one or more shared applications such as Finance, HR, Order Management, Customer Service, or similar:
Managing user accounts, setting access privileges and roles
Supporting end user inquiries, debugging data issues / collaborating with engineers to fix problems
Managing 3rd party vendors, especially SaaS solutions
Prefer experience with software and vendor selection / RFP process: gathering requirements, writing an RFP, interacting with vendors, defining proof of concept or customized demos, developing a vendor scorecard, managing a cross functional team to review proposals and arrive at a decision
Solid understanding of ticketing software used for issue tracking - such as Jira, Zendesk, ServiceNow, or Pivotal Tracker - prefer knowledge of setup and configuration to support workflows specific to company requirements
Experience providing frontline support to a user community – either internal or external customers: issue intake, triage, analysis, escalation, resolution, and communications processes
Relocation Eligible: Not Applicable
Job Type: Regular

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity

Our Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901 - 4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance.

If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy

Please view our Pay Transparency Statement"
119,Data Engineer,"San Mateo, CA",San Mateo,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description
Overview:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Job Location: San Mateo, CA
Responsibilities:
1.Create and maintain optimal data pipeline architecture,
2.Assemble large, complex data sets that meet functional / non-functional business requirements.
3.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
4.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
5.Engages in building solutions leveraging technologies including but not limited to SQL, Node.js and AWS services such as EC2, S3, ELB, Kinesis, SNS, Redshift, Data Pipelines.
6.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
7.Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
8.Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications:
1. At least Bachelor’s Degree in Computer Science, software engineer or relevant technical field is required. Min 3+ years experience in data engineer role.
2.Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
3.Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
4.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
5.Strong analytic skills related to working with unstructured data sets.
6.Build processes supporting data transformation, data structures, metadata, dependency and workload management.
7.Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
8.Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience : 0-2 years.
Please rush your resume to info@zillionix.com"
120,Data Engineer,"San Francisco, CA 94107",San Francisco,CA,94107,None Found,None Found,None Found,None Found,None Found,None Found,"JOB DESCRIPTION
As a key member of our Consulting team within Professional Services, the Data Engineer is responsible for the data pipeline part of our data product implementation. Our team of data product managers, solution architects, and data engineers works directly with customers, driving requirements gathering, the project design, and the QA process. Data Engineers implement data warehouse data models and dimensional data models, deliver ETL in SQL, and write multidimensional queries in MAQL.
The right candidate for this position is customer oriented with strong communication and technical skills. You must be able to demonstrate deep technical expertise and clearly articulate technical topics even to non-technical audiences.
This role is best suited for someone who has 2+ years of work experience, has a quantitatively-oriented degree (of any variety) from a top-tier school, and who thrives in extremely analytical, technically-oriented, consultative role. This role is located in San Francisco, CA and no relocation assistance is offered.
RESPONSIBILITIES
In collaboration with architects, implement data solutions that solve business problems
Implement effective and scalable end-to-end data pipeline solutions
Optimize SQL queries for scale
Design conceptual, logical and physical data models
Build metrics and reports
Address functional requirements and quality attributes
Possess a comprehensive understanding of the GoodData products and services

QUALIFICATIONS
2+ years of overall experience
Fluency in SQL and experience in Extract, Transform, and Load (ETL) development in SQL
Understanding of dimensional and normalized database models and their applications
Having a programming mentality and problem-solving skills

BONUS POINTS
Experience in the Business Intelligence (BI) / Data Warehousing (DW) industry
IT consulting skills and experience
Experience working in professional services organization
Experience with columnar databases
Experience with Ruby
Experience with JavaScript
Experience with REST APIs
Interest in machine learning

ADDITIONAL INFORMATION
At GoodData, you won’t just grow your career. You’ll be a part of a movement that is changing how people work and make decisions.
Why you should join GoodData:
Help drive the future of business
Solve meaningful problems
Build and promote great technology
Work with brilliant people
We are committed to creating a diverse work environment and proud to be an Equal Opportunity Employer. All your information will be kept confidential according to EEO guidelines.
No relocation assistance is offered for this opportunity at this time, and local candidates will be given priority consideration.
Agency Recruiters Take Note:
Please do not spam us with unsolicited candidate resumes. Unless you have a fully executed agreement with us, we will consider them a gift. Do not send resumes to any of the executives."
121,"Data Engineer, Data Warehouse","San Francisco, CA 94103",San Francisco,CA,94103,None Found,None Found,None Found,"
Work with business and engineering stakeholders to understand their needs, and translate these into data processing and data modeling requirements.
Design and implement performance and scalable data storage models to facilitate analytics needs of engineers and business customers
Design and implement data ETL (extract, transform, load) processes to support and extend the company’s Data Warehouse
Participate in DevOps, utilizing your technical and logical reasoning skills to troubleshoot and solve high-stakes technical and operational problems for our internal customers.
Leverage your advanced analytics skills to help business customers answer questions based on Quantcast’s vast and rich data set on internet activity.",None Found,"
Advanced (Master/PhD) degree in Computer Science, Electrical Engineering, or related highly quantitative field (such as Math or Statistics)
Minimum of 1 year of experience
Experience with SQL, and with doing analytics on structured data in databases.
Experience with performance optimization for relational databases - including query optimization, database indexes.
Experience working with data migration, data cleaning/curation, or ETL (extraction, transformation and loading)
Experience working with dimensional data modeling and relational data storage","The Data Platform team owns the data backbone of Quantcast’s advertising business. This means that we build systems that ingest, refine, and make available, tens to hundreds of terabytes of data on a daily basis, and serve up petabyte-scale datasets for use in Quantcast’s industry-leading audience targeting and automation systems.

As the real-time pulse of the Internet, Quantcast uses machine learning to drive human learning. We’ve built one of the largest computing infrastructures on the planet, collecting a constant stream of data from over 100 billion events a day, across 100+ million directly-measured destinations. In addition to this global network, Quantcast has engineered premier map-reduce and real-time processing clusters at a massive scale, processing an average of 30+ PB of data every single day for various machine learning and data mining applications.

We’re looking for qualified engineers to drive our efforts in data warehousing, data modeling, data analytics and data processing (ETL/ELT), to provide the company with a clean, consistent and comprehensive view of our business. Quantcast is a fundamentally data-driven company, and we view improved data insight as a critical component of the scaling and direction of the company as a whole.

Responsibilities
Work with business and engineering stakeholders to understand their needs, and translate these into data processing and data modeling requirements.
Design and implement performance and scalable data storage models to facilitate analytics needs of engineers and business customers
Design and implement data ETL (extract, transform, load) processes to support and extend the company’s Data Warehouse
Participate in DevOps, utilizing your technical and logical reasoning skills to troubleshoot and solve high-stakes technical and operational problems for our internal customers.
Leverage your advanced analytics skills to help business customers answer questions based on Quantcast’s vast and rich data set on internet activity.
Requirements
Advanced (Master/PhD) degree in Computer Science, Electrical Engineering, or related highly quantitative field (such as Math or Statistics)
Minimum of 1 year of experience
Experience with SQL, and with doing analytics on structured data in databases.
Experience with performance optimization for relational databases - including query optimization, database indexes.
Experience working with data migration, data cleaning/curation, or ETL (extraction, transformation and loading)
Experience working with dimensional data modeling and relational data storage
Quantcast owns and operates the world’s largest audience insights and measurement platform on the open internet. Fueled by live data drawn from more than 100 million web and mobile destinations, Quantcast applies machine learning technology to help marketers, publishers, and agencies grow their brands by better understanding and predicting consumer interactions in real-time.

Founded in 2006, Quantcast is headquartered in San Francisco and has employees in over 20 offices across 10 countries. We are committed to building an inclusive and diverse environment where everyone can be their authentic self."
122,Data Engineer - CIMD - Marcus by Goldman Sachs Engineering,"San Francisco, CA 94104",San Francisco,CA,94104,None Found,None Found,None Found,None Found,None Found,None Found,"MORE ABOUT THIS JOB
CONSUMER (MARCUS BY GOLDMAN SACHS)
Marcus by Goldman Sachs is the firm’s consumer business, combining the entrepreneurial spirit of a startup with 150 years of experience. Today, Marcus has $50 billion in deposits, $5 billion in loan balances and 4 million customers across our lending and deposits businesses, as well as the personal financial management app, Clarity Money. Through the use of insights and intuitive design, we provide customers with powerful tools and products that are grounded in value, transparency and simplicity. We are backed by our unique team, comprised of individual contributors from leading agile technology companies, fintechs and consumer financial services companies, allowing us to disrupt the industry, while helping consumers take control of their financial lives.

RESPONSIBILITIES AND QUALIFICATIONS
HOW YOU WILL FULFILL YOUR POTENTIAL
Design and develop data ingest and transform processesDevelop data visualizations using BI tools and web-based technologiesWork as part of a global team using Agile software methodologiesPartner with Marcus risk, product, acquisition and servicing teamsUse Marcus data to drive change throughout the Marcus business

QUALIFICATIONS
Minimum 3 years of relevant professional experienceBachelor’s degree or equivalent requiredExperience with SQL and relational databasesProficient at Python, Spark and the Hadoop ecosystemSelf-starter, motivated, and good communication skills Strong sense of ownership and driven to manage tasks to completion

CONSUMER AND INVESTMENT MANAGEMENT DIVISION (CIMD)
The Consumer and Investment Management Division includes Goldman Sachs Asset Management (GSAM), Private Wealth Management (PWM) and our Consumer business (Marcus by Goldman Sachs). We provide asset management, wealth management and banking expertise to consumers and institutions around the world. CIMD partners with various teams across the firm to help individuals and institutions navigate changing markets and take control of their financial lives.
ABOUT GOLDMAN SACHS
The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

Â© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet."
123,Senior Data Engineer,"San Mateo, CA",San Mateo,CA,None Found,None Found,"
Master's or Bachelor's Degree in Computer Science or equivalent experience
Self-motivated individual with strong problem solving skills
Comfortable and adaptable in a fast-paced and informal environment
Excellent verbal and written communication skills with the ability to interact at all levels of the organization
Detail-oriented, analytical, methodical
Creative thinker committed to driving quality forward
Passionate about data and what we can extract from it
Strong interest in functional programming
Familiar with distributed systems (sharding, lambda arch., streaming arch., etc.)
Experience with big data (Cassandra, Hbase, Hadoop, Redshift, etc.)
Excellent grasp of statistics
Comfortable working in a distributed team
2+ year of experience with Apache Kafka
1+ year of experience with distributed streaming architecture
2+ years of experience building analytics systems
",None Found,None Found,None Found,None Found,"Rakuten Ready (formerly Curbside) builds a next generation cloud platform to connect stores and restaurants for mobile ordering. Rakuten Ready platform bridges the divide between online and bricks-and-mortar stores and restaurants, delivering mobile convenience for shoppers in the communities in which they live, work and play. Customers include Nordstrom, CVS, Pizza Hut and Boston Market. Rakuten Ready fosters a work environment that is respectful, fair and welcoming and we hire according to these values.

We are searching for a Senior Data Engineer to join our team! This position requires a self-motivated individual with excellent problem solving skills who can think of, put forward, design and implement efficient and scalable solutions to challenging problems. The prime directive is to be a key contributor in the development of our platform and its analytics. Moreover, significant contribution to the architecture of the system along with coaching of more junior members of the team is expected. Responsibilities may include, but are not limited to the following:


Follow the evidence to track down / root cause issues arising from a real world location-based service
Develop and maintain tools for discovering and analyzing issues and reporting of location based services problems
Prepare detailed and concise technical reports on a weekly basis documenting top issues and progress toward determining root cause and/or corrective action implementation/verification
Design and carry out experiments/simulations to study factors that influence the accuracy of a location based service;
Aggregate location data to study factors that influence the accuracy of a location based service;
Develop appropriate test plans and production monitoring plans

Qualifications:

Master's or Bachelor's Degree in Computer Science or equivalent experience
Self-motivated individual with strong problem solving skills
Comfortable and adaptable in a fast-paced and informal environment
Excellent verbal and written communication skills with the ability to interact at all levels of the organization
Detail-oriented, analytical, methodical
Creative thinker committed to driving quality forward
Passionate about data and what we can extract from it
Strong interest in functional programming
Familiar with distributed systems (sharding, lambda arch., streaming arch., etc.)
Experience with big data (Cassandra, Hbase, Hadoop, Redshift, etc.)
Excellent grasp of statistics
Comfortable working in a distributed team
2+ year of experience with Apache Kafka
1+ year of experience with distributed streaming architecture
2+ years of experience building analytics systems

Preferred Qualifications


1+ year of experience in measurement, data processing or data analysis
1+ year of experience with Clojure

"
124,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Must have 3 years of corporate experience

The Data Engineering team at Glu builds core data infrastructure and applications in support of all areas of our business, including our studio teams, user acquisition, monetization and finance. Glu is passionate about maximizing the value that data and analytics can provide to the business and is aggressively investing in new capabilities. Our team covers a lot of ground from data ingestion through to machine learning applications.

We leverage a cutting-edge tech stack to build both batch systems (YARN+Spark/Hive) and stream processing applications (Kinesis/Flink/Spark Streaming/Druid) that operate efficiently at high scale. The ideal candidate has a strong engineering background and has built robust data platforms and pipelines and takes complete ownership of their area of expertise. This is a fantastic opportunity to use your engineering skills to make a material impact on a highly valued analytics platform.
You'll most often be:
Maintaining, streamlining and hardening existing data pipelines, from ingestion, through ETL and batch processing in order to reliably process billions of records per day.
Build data support for our personalization and experimentation efforts, solving problems from statistical test automation to building real-time M/L applications.
Working with Analytics and Product Management to ensure optimal data design and efficiency.
And your skills and experience include:
Bachelor's degree in computer science/mathematics/engineering, or other fields with proven engineering experience.
More than 3 years of software engineering experience, especially working on back-end data infrastructure.
Proficiency with at least one of the following languages: Java, Python, Scala.
Proficiency with Spark and/or similar tools in Hadoop/YARN environment and comfortable with Linux operating system.
Proficiency with SQL and SQL-like languages, especially Hive.
Experience with AWS Ecosystem, especially Kinesis, EMR, Lambda, and Glue.
Bonus Points
Experience with high-scale machine learning, I.e. Spark M/L, Sagemaker, etc.
Knowledge of NoSQL application data stores i.e. Druid, HBase, Cassandra, DynamoDB, Redis.
https://i.imgur.com/4k9wIxl.jpg

https://i.imgur.com/4k9wIxl.jpg"
125,Big Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
A bachelor's degree in Computer Science or related field with at least 2–5 years of experience with Big Data Hadoop tools such as Spark, Hive, Kafka and MapReduce, Scala, Python, Pyspark, Scikit
Experience working within a Linux computing environment, and use of command-line tools including knowledge of shell/Python scripting for automating common tasks
Experience in AI/ML on big data platforms preferred:
Solid applied statistics skills, such as identifying distributions, statistical testing, regression, etc.
Proficiency querying both structured and unstructured data
Experience with Python and Deep Learning packages (Keras, Tensorflow, mxnet) and NLP packages (nltk, spacy)
Familiarity with analytical techniques such as random forest, gradient boosting, SVM, NLP
Experience deploying models in production environments
Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space
Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage
Excellent written and oral communication skills; must be able to effectively articulate technical concepts to non-technical audiences
Software development experience using:
Database programming using any flavor of SQL
Expertise in relational and dimensional modeling",None Found,"
Understand problems from a client's point of view, build and execute solid analytics work plans, gather and organize large and complex data sets, perform relevant analyses (data exploration and statistical modeling), manage priorities and deadlines, foster teamwork in interactions, develop client relationships with client counterparts, and communicate hypotheses and findings in a structured way
The role would involve big data pre-processing and reporting workflows including collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into business insights
Test various machine learning models on big data, and deploy learned models for ongoing scoring and prediction",None Found,None Found,"Company Overview
Fractal Analytics is a strategic AI partner to Fortune 500 companies with a vision to power every human decision in the enterprise. Fractal is building a world where individual choices, freedom, and diversity are the greatest assets. An ecosystem where human imagination is at the heart of every decision. Where no possibility is written off, only challenged to get better. We believe that a true Fractalite is the one who empowers imagination with intelligence.
We are seeking a strong candidate with advanced analytics experience to fill an exciting Big Data Engineer position, in San Francisco, CA. In this role, you will be a valuable expert and will help design and build analytics methodologies, solutions, and products to deliver value to our clients in collaboration with cross-functional teams. As an exceptional candidate, you will show an analytical curiosity.
Responsibilities
Understand problems from a client's point of view, build and execute solid analytics work plans, gather and organize large and complex data sets, perform relevant analyses (data exploration and statistical modeling), manage priorities and deadlines, foster teamwork in interactions, develop client relationships with client counterparts, and communicate hypotheses and findings in a structured way
The role would involve big data pre-processing and reporting workflows including collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into business insights
Test various machine learning models on big data, and deploy learned models for ongoing scoring and prediction
Qualifications
A bachelor's degree in Computer Science or related field with at least 2–5 years of experience with Big Data Hadoop tools such as Spark, Hive, Kafka and MapReduce, Scala, Python, Pyspark, Scikit
Experience working within a Linux computing environment, and use of command-line tools including knowledge of shell/Python scripting for automating common tasks
Experience in AI/ML on big data platforms preferred:
Solid applied statistics skills, such as identifying distributions, statistical testing, regression, etc.
Proficiency querying both structured and unstructured data
Experience with Python and Deep Learning packages (Keras, Tensorflow, mxnet) and NLP packages (nltk, spacy)
Familiarity with analytical techniques such as random forest, gradient boosting, SVM, NLP
Experience deploying models in production environments
Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space
Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage
Excellent written and oral communication skills; must be able to effectively articulate technical concepts to non-technical audiences
Software development experience using:
Database programming using any flavor of SQL
Expertise in relational and dimensional modeling
Nice to Have:
Operating knowledge of cloud computing platforms (AWS, especially EMR, EC2, S3, SWF services and the AWS CLI)
Experience building recommendation engines (both offline and online validation metrics)

Fractal Analytics provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
kna6P0AxzC"
126,Staff Search Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
5+ years of experience as a Data Engineer or Software Engineer developing production code.
Strong understanding of search, relevancy and ML / AI techniques.
Experience with Solr (any experience with Spark, Hadoop or similar large-scale data technologies is a plus)
Results-oriented with a strong customer and business focus.
Entrepreneurial and self-directed, innovative, biased towards action in fast-paced environments.
Able to communicate and discuss complex topics with technical and non-technical audiences.
Able to tackle ambiguous and undefined problems.
",None Found,"
Develop the core search infrastructure at Reddit.
Apply Machine Learning / Artificial Intelligence to retrieve relevant results with optimal ranking.
Dig into how Reddit's customer behavior is unique and go beyond generic relevance algorithms.
Participate in the full development cycle: design, develop, qa, experiment, analyze, and deploy.
Collaborate across disciplines to find technical solutions to complex challenges.
",None Found,None Found,"The front page of the internet,"" Reddit brings over 330 million people together each month through their common interests, inviting them to share, vote, comment, and create across thousands of communities. Come for the cats, stay for the empathy.

Generating billions of events and terabytes of data a day, we're in the unique position to revolutionize content discovery on the internet. We are overhauling Reddit's search and relevancy infrastructure as we unleash the value of an exponentially growing petabyte-scale dataset.

You'd leverage state of the art technology to lead this effort from the ground level, delivering highly personalized experiences across our extremely diverse user-base.

If applying ML / AI in production to improve Reddit's content hotness rankings and search relevance while impacting our bottom line through ad targeting/utilization excites you, then you've found the right place.

Responsibilities:

Develop the core search infrastructure at Reddit.
Apply Machine Learning / Artificial Intelligence to retrieve relevant results with optimal ranking.
Dig into how Reddit's customer behavior is unique and go beyond generic relevance algorithms.
Participate in the full development cycle: design, develop, qa, experiment, analyze, and deploy.
Collaborate across disciplines to find technical solutions to complex challenges.

Qualifications:

5+ years of experience as a Data Engineer or Software Engineer developing production code.
Strong understanding of search, relevancy and ML / AI techniques.
Experience with Solr (any experience with Spark, Hadoop or similar large-scale data technologies is a plus)
Results-oriented with a strong customer and business focus.
Entrepreneurial and self-directed, innovative, biased towards action in fast-paced environments.
Able to communicate and discuss complex topics with technical and non-technical audiences.
Able to tackle ambiguous and undefined problems.

"
127,Senior Data Engineer - Business Systems,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,None Found,None Found,None Found,None Found,None Found,"At Uber, we ignite opportunity by setting the world in motion. We take on big problems to help drivers, riders, delivery partners, and eaters get moving in more than 600 cities around the world.

We welcome people from all backgrounds who seek the opportunity to help build a future where everyone and everything can move independently. If you have the curiosity, passion, and collaborative spirit, work with us, and let’s move the world forward, together.
About the Role

As a Senior Data Engineer in the Business Applications team, you will be responsible for the engineering and implementation of scalable technical solutions around data engineering. This solution will have significant impact to Uber’s results in areas of user growth, safety and global expansion. Senior Data Engineers will solve extremely challenging technical problems for Uber Eats, Uber Freight, Uber for Business, and others.
What You’ll Do

Help Sales leaders, Finance, and Sales Operations implement and maintain data architecture, pipelines, and integrations
Develop a deep understanding of the business and their requirements to develop sound technical solutions
Design, build, and maintain data pipelines between our product, CRM, HR, and other relevant systems
Assist in core operational tasks, including high level design, reporting, documentation, and systems updates
Pro-active communication to users and management on issues
Improve and build solutions to address architectural gaps or technical debt
Propose best technologies for data processing and data framework
Play an active role in positively influencing the design direction
Participate in story planning, standups and retrospectives
Check in and deploy code and configurations; participate in release management
Execute with a team of solution architects, program managers and engineers
Produce documentation and tutorials that enable other teams
Mentor engineers; Drive enforcement of standards, tools and methodologies

What You’ll Need

7+ years of SQL (Hive, Oracle, MySQL, etc.) experience
5+ years of building and launching new data models that provide intuitive analytics for the analysts and customers
Design, build and launch extremely efficient & reliable data pipelines for movement of data to and from
5+ Experience with Hadoop/Hive, Vertica, Presto, Impala and data warehouse technologies
5+ years of custom or structured (Mulesoft, Informatica, etc…) ETL design, implementation, and maintenance
Experienced with Xactly API and administration would be a highly preferred
Experienced with Salesforce CRM, Workday, Fieldglass, and its various APIs is preferred
Experienced with at least 1 of the following languages: Java, Go, Python
Passion for technology, industry research and enjoys solving business problems
Excellent verbal, written, communication, interpersonal and presentation skills
Bachelor’s degree or higher / equivalent work experience

Bonus Points

Familiar with Salesforce, Workday, and Fieldglass administration

About the team

The Business Applications team’s mission is to empower Uber’s staff to be more productive and successful through our technical solutions, service delivery and innovation. Our team supports Sales and Community Operations teams within Uber Eats, Uber Freight and Uber for Business. We also support Uber’s Law Enforcement Response Team improving safety of riders/drivers across the globe. Our team invests in innovation, new business line expansion and integration with external and internal systems."
128,"Software Engineer, Data","San Francisco Bay Area, CA",San Francisco Bay Area,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Lime is a smart-mobility provider that offers cities an array of mobility products including Lime-E e-assist bicycles and Lime-S electric scooters. Lime aims to revolutionize mobility in cities and campuses by empowering residents with cleaner, more efficient, and affordable transportation options that improve urban sustainability.

As a Data Engineer at Lime, you will own the infrastructure that processes every single data point we collect. We're building an on-demand service that stretches into the physical world, so you will be presented with a vast array of exciting, open-ended data challenges. You will lay the groundwork for how our entire organization runs data pipelines, trains statistical models, and turns analytical reports into concrete actions. Your work will directly drive many of our important business decisions. You'll be working alongside a group of engineers, designers, PMs, and operators from top schools and with experience building systems and apps at companies like Facebook, Twitter, Uber, and Square.

What You'll Do
Ensure system uptime for all of our data infrastructure (i.e. our central data warehouse, ETL systems, and realtime processing)
Democratize data within the organization by formulating and internally distributing best practices for building data pipelines
Jump into at least one engineering/product unit to build data pipelines and produce actionable insights for that team
Embrace your role as a founding member of our data infrastructure team
About You
Degree in Computer Science or equivalent technical background
2+ years experience in the industry, via internships or full-time positions
Experience working with big data technology like Redshift, Snowflake, Hadoop, or Hive
You understand how to harness realtime data streams using technologies like AWS Kinesis, Kafka, Storm, or Spark
What We Offer
Opportunity to revolutionize transportation in your hometown with the leader in urban mobility solutions
A position that offers a variety of career and resume building experiences with the fastest growing startup of 2018
You will have the chance to scale with a rapidly growing organization, with tons of opportunity for growth
Play a role in the transformation of urban mobility and sustainability
Work with a team of fun and motivated individuals
Competitive salary and benefits
We here at Lime strive to build a workforce comprised of individuals with diverse backgrounds, abilities, minds, and identities that will help us to grow, not only as a company, but also as individuals. Lime is an Equal Opportunity Employer.

#LI-AB2"
129,Sr. Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Must have 6 years of corporate experience

The Sr Data Engineering team at Glu builds core data infrastructure and applications in support of all areas of our business, including our studio teams, user acquisition, monetization and finance. Glu is passionate about maximizing the value that data and analytics can provide to the business and is aggressively investing in new capabilities. Our team covers a lot of ground from data ingestion through to machine learning applications.
We leverage a cutting-edge tech stack to build both batch systems (YARN+Spark/Hive) and stream processing applications (Kinesis/Flink/Spark Streaming/Druid) that operate efficiently at high scale. The ideal candidate has a strong engineering background and has built robust data platforms and pipelines and takes complete ownership of their area of expertise. This is a fantastic opportunity to use your engineering skills to make a material impact on a highly valued analytics platform
You'll most often be:
Take ownership of and develop critical new features for our next-generation analytics platform, supporting Glu's worldwide studios and central functions such as marketing and finance.
Build scalable, accurate and extensible stream processing applications using cutting-edge technology such as Spark Streaming and Apache Flink.
Implement complex and highly scalable end-to-end data pipelines, using Elastic Beanstalk, Kinesis, EMR, Spark, Hive, Druid, Cassandra.
Build data support for our personalization and experimentation efforts, solving problems from statistical test automation to building real-time M/L applications.
And your skills and experience include:
Bachelor's degree in computer science/mathematics/engineering, or other fields with proven engineering experience.
More than 6 years of software engineering experience, especially working on back-end data infrastructure.
Proficiency with at least one of the following languages: Java, Python, Scala.
Experience with distributed stream processing technologies such as Flink, Spark Streaming and/or Kafka Streams.
Experience with AWS Ecosystem, especially Kinesis, EMR, Lambda, and Glue.
Knowledge of NoSQL application data stores i.e. Druid, HBase, Cassandra, DynamoDB, Redis. .
Bonus Points
Experience with high-scale machine learning, I.e. Spark M/L, SageMaker, etc.
Experience with SQL and SQL-like languages, especially Hive.
Experience with CI/CD process, testing framework, and containerization technology
Experience building data-rich web applications, especially with technologies like Angular, Node.js, and Elastic Beanstalk
https://i.imgur.com/4k9wIxl.jpg

https://i.imgur.com/4k9wIxl.jpg"
130,"Senior Visualization & Data Engineer, Tableau","San Francisco, CA 94103",San Francisco,CA,94103,None Found,"
Bachelor's Degree in a relevant field with 3+ years of professional experience OR Master’s Degree with 2+ years of professional experience
Strong experience in analytics in a fast-paced, big-data environment
Strong experience with Tableau versions 2018.1 and above
Hands-on experience with creating solution driven, compelling and well-organized views and dashboards in Tableau
Experienced in Tableau best practices for dashboard development and data source management (e.g. certified data sources, creating template workbooks, optimizing workbook performance)
Experience with troubleshooting and performance tuning of Tableau workbooks
Working knowledge of statistical methods and mathematical models
Ability to communicate effectively with business partners, engineers, and analysts",None Found,"
Absorbing and managing complex analytical requests
Prioritizing requests through management of team backlog
Coordinating across multiple engineering teams
Assisting in data analysis to drive proactive planning for future development
Participating in data modeling & visualization exercises
Build solution driven views and dashboards in Tableau to support the IT Enterprise Applications team
Conduct training for Tableau and hold office hours for internal Tableau users to help them with advanced questions / scenarios
Support and utilize all Tableau tools including Tableau Desktop, Tableau Server, and Tableau Prep
Serve as the secondary on-call for the internal Tableau Server deployment, which will involve server maintenance tasks and ad-hoc user requests
Help Business Analysts build dashboards using techniques for advanced analytics, interactive dashboard design, and visual best practices to convey the story within the data
Open collaboration through timely communication is a must!
Providing input and guidance for team growth and process definition
Help teams who own sites tracked in Google Analytics to understand and make sense of their GA data to drive business outcomes",None Found,None Found,"Twitter is a growing organization that faces new challenges as it evolves. The IT Enterprise Applications Team focuses on the building blocks of applications and tools that improve the productivity of employees across the company.


Who We Are

At Twitter, our IT Business Intelligence team focuses on implementing and supporting applications to help our internal customers meet their operational and reporting needs. We are responsible for establishing processes for continual improvement, guiding administration, and partnering with multiple stakeholders on a project-to-project basis to help them through their reporting and visualization needs. Our mission is to keep our business running effectively while delivering the data required to assist in analytical insights.


Who You Are

To continually strengthen our growing team, we are looking for an experienced visualization and data engineer with a deep background in Tableau visualization and report development, as well as Google Analytics. You will partner with business and engineering teams to prioritize requests and consistently deliver excellent results against time-sensitive priorities. You have practiced interpersonal communication skills, possess a broad range of data related experience, are team-oriented, and you thrive in a fast paced environment.


Roles & Responsibilities

Absorbing and managing complex analytical requests
Prioritizing requests through management of team backlog
Coordinating across multiple engineering teams
Assisting in data analysis to drive proactive planning for future development
Participating in data modeling & visualization exercises
Build solution driven views and dashboards in Tableau to support the IT Enterprise Applications team
Conduct training for Tableau and hold office hours for internal Tableau users to help them with advanced questions / scenarios
Support and utilize all Tableau tools including Tableau Desktop, Tableau Server, and Tableau Prep
Serve as the secondary on-call for the internal Tableau Server deployment, which will involve server maintenance tasks and ad-hoc user requests
Help Business Analysts build dashboards using techniques for advanced analytics, interactive dashboard design, and visual best practices to convey the story within the data
Open collaboration through timely communication is a must!
Providing input and guidance for team growth and process definition
Help teams who own sites tracked in Google Analytics to understand and make sense of their GA data to drive business outcomes


Qualifications

Bachelor's Degree in a relevant field with 3+ years of professional experience OR Master’s Degree with 2+ years of professional experience
Strong experience in analytics in a fast-paced, big-data environment
Strong experience with Tableau versions 2018.1 and above
Hands-on experience with creating solution driven, compelling and well-organized views and dashboards in Tableau
Experienced in Tableau best practices for dashboard development and data source management (e.g. certified data sources, creating template workbooks, optimizing workbook performance)
Experience with troubleshooting and performance tuning of Tableau workbooks
Working knowledge of statistical methods and mathematical models
Ability to communicate effectively with business partners, engineers, and analysts


Preferred Qualifications

Tableau Server Administration experience (e.g. server setup and maintenance; supporting high availability clusters, server migrations, etc.)
Experience with MySQL, Vertica, and/or other database systems (MySQL highly preferred)
Advanced experience with Google Analytics’ data model
Ability to write Bash, Python scripts for monitoring"
131,Data Engineer - SF,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"WHO WE ARE
Looker is on a mission to bring better insights and data-driven decisions to every business. Everything we do is aimed at making sure our customers love every aspect of Looker, from our products and technologies to our ease of doing business and our support. We are looking for curiously brilliant individuals to join our team as we reinvent data analytics. Get data-driven and see yourself at Looker.

WHAT WE'VE GOT GOING ON
Looker is searching for a Data Engineer with 5+ years of experience in implementing modern data architectures. With an exciting new round of funding and a new team, you will be a key owner in bringing our infrastructure to the next level. You will be working on one of the biggest greenfield opportunities at Looker: A major overhaul of our data, pipeline, and security architecture, as well as our modeling layer. Looker’s amazing success is just getting started and you will be working for a company that truly values the power of data. Your first projects will include helping to scale our data infrastructure and optimizing our warehouse performance and footprint. You will be working with other engineers and analysts on the Data & Analytics team to support both our internal warehousing efforts and prototyping new best practices for our Professional Services and Customer Love department.

WHAT WE NEED YOU TO DO
Partner on the design the next implementation of our secure, global data architecture
Execute engineering tasks with maturity in variety of languages including JVM-based, ruby, and other scripting languages
Own medium-to-large size data engineering projects
Mentor junior staff by providing opportunities to execute on your engineering agenda
Own optimization and monitoring projects for our Massively Parallel Processing (MPP) Database and Pipeline technologies
Set standards and create documentation for self-serve data pipeline services supporting core engineering and professional services use cases
Work with a variety of AWS, GCP, and Azure technologies
Work with stream and queue-based solutions
WHAT YOU BRING TO LOOKER
5+ years experience with Data Warehouse Systems
Java and Python experience in production
Familiarity with Spark and/or MapReduce
Strong familiarity with batch processing and workflow tools such as Airflow, NiFi, Azkaban
A strong desire to show ownership of problems you identify and proven ability to empower others to get more done
Familiarity with modern BI and exploration tools.
Understanding of hardware performance
Familiarity with Linux systems
Some familiarity with streaming approaches preferred
CS Degree preferred
Some experience preferred with Node, Ruby, or Scala
#LI-SK1"
132,Senior Data Engineer,"Emeryville, CA",Emeryville,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Art.com

Art.com Inc. is made up of talented people who are original thinkers and who love a culture where innovation, creativity and results are valued. Since its inception in 1998, Art.com has grown to be the largest online specialty retailer of high quality art, a position of strength that presents excellent opportunities for professional development and advancement for our employees.

Discover your potential

As an innovative and growing company, Art.com Inc. consistently strives to remain competitive in attracting and retaining employees whose work-hard, play-hard attitude allows them to thrive in achieving both personal and professional goals. Our performance-based culture has been developed around this approach. You can also expect challenging, rewarding work, competitive salaries, great benefits, and tremendous opportunities for career growth.

Emeryville, CA

You will be a part of an agile team of highly motivated and talented engineers, and you will be building the core data platform that powers the next generation data, analytics, and machine learning solutions. You will be using the latest cloud-based technologies to build the data lake and data platform. You will work across disparate platforms, data sources and formats, and you will work closely with Product Managers, Growth Marketers and other Engineers.

Responsibilities

Analyze, design, develop and implement solutions for data lake components.
Analyze data, system and data flows and develop new data models in production.
Design, build and deploy efficient data integration pipelines for extraction, loading and transformation of data.
Define and manage SLA for all data sets, including quality.
Work with data from disparate environments (SQL, NoSQL, Columnar, etc.) data stores on premises and on the cloud
Write scripts/programs to extract from and publish data using APIs (SOAP, REST etc.), working with data in various formats such as relational, JSON, XML etc.
Develop the next generation analytics platform on the cloud
Work with business and technology groups, especially with Growth Marketers, to understand business processes and convert business needs into technical solutions

Must Haves

Bachelor’s and/or advanced degree in Computer Science, Information Systems or related field from an accredited college/university or equivalent work experience
Experience in the data engineering space building data pipelines
Experience in writing and optimizing SQL scripts
Experience in scripting (Python)
Experience with SQL (MySQL, SQL Server, Redshift etc.) and NoSQL databases (Cassandra etc.)
Experience in ETL design, implementation, and optimization
Experience building and scaling for high data volume and velocity
Proven ability to work independently; designing, developing and deploying solutions, and to deliver projects on time with minimal direction
Ability to work seamlessly within a team as well as manage individual tasks
Excellent business and communication skills and the ability to identify and communicate data issues and insights
Experience with versioning tools like Git or SVN

Nice to Haves

Exposure to dimensional modeling, schema design
Experience in distributed processing – Spark, MapReduce
Experience working with marketing data providers
Experience with data cleansing methodologies and tools
Experience with ETL systems
Experience with cloud data systems such as AWS Redshift, Google BigQuery or Snowflake
Exposure to BI/Visualization tools such as Tableau, Looker, MicroStrategy, Cognos"
133,Senior Data Engineer - Loyalty Offers Platform,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Who is Mastercard?
We are the global technology company behind the world’s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless ®. We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.
Job Title
Senior Data Engineer - Loyalty Offers Platform
Offers team in the Loyalty group at Mastercard is at the heart of providing end consumers highly personalized card linked offers by marrying offers from our advertising network with anonymized transaction data. We are looking for a strong, innovative engineers who can bring unique perspectives and innovative ideas to all areas of development, and are interested in continuing to improve our platform through the ever changing technology landscape. In this role, you will work on multiple projects, which are critical to Mastercard’s role.


An ideal candidate is someone who works well with a cohesive team-oriented group of developers, who has a passion for developing great software, who enjoys solving problems in a challenging environment, and who has the desire to take their career to the next level. If any of these opportunities excite you, we would love to talk.


Responsibilities:


Design, develop, test, deploy, maintain, and improve our software

Manage individual project priorities, deadlines, and deliverables

Ensure the final product is highly performant, responsive, and of the highest quality

Actively participate in agile ceremonies including daily scrum, story pointing, story elaboration, and retrospectives

Qualifications:


Proficiency in, at least, one modern programming language such as Java or Python

Strong Computer Science fundamentals in object-oriented design, data structures, algorithm design, problem solving, and complexity analysis

Relational Databases (e.g MySql, Postgres)

Basic Shell scripting and knowledge of Linux/Unix systems

Experience in designing & developing software at scale

Preferred:


Bachelor’s or Master’s degree in Computer Science or a related engineering field

Interest and ability to learn new coding languages, frameworks, and paradigms as needed
Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.
If you require accommodations or assistance to complete the online application process, please contact reasonable.accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly."
134,"Data Engineer - Senior Consultant - San Francisco, CA","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.

We are...

 • The Industry-recognized data and analytics leaders
 • Passionate problem solvers across a broad spectrum of technologies and industries
 • Value seekers for measurable business outcomes
 • Continuous learners through training and education
 • Focused on a work-life balance with an unlimited paid time off policy

Data engineers are challenged with building the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally. Our engagements typically target a variety of use cases across data engineering, data science, data governance, and visualization.
Data engineers deliver value through....
Hands-on, self-directed design and development of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools
 Demonstration of technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven insights
Collaboration with team members, business stakeholders and data SMEs to elicit requirements and to develop business metrics and analytical insights
Internal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO
A data engineer's skills include, but are not limited to...
Bachelors Degree and 5+ years of work experience
5+ years of professional IT work experience
SQL, SQL, SQL!
Programming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)
Linux / Windows (Command line)
Big Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop, Spark)
Cloud Platforms (AWS, Azure, Google Cloud Platform)
Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)
Data Integration Tools (Ab Initio, DataStage, Informatica, SSIS, Talend)
Databases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)
Markup Languages (JSON, XML, YAML)
Code Management Tools (Git/GitHub, SVN, TFS)
DevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)
Testing / Data Quality (TDD, unit, regression, automation)
Solving complex data and technology problems
Leading technical teams of 2+ consultants
Ability to design components of a larger implementation
Excellent communication to narrate data driven insights and technical approach

If this sounds like you, let’s talk!

Candidates must be comfortable with a national travel model to client locations weekly (M-TH is typical).

Clarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.

#LI-NT1
GLDR"
135,Data Engineer,"San Mateo, CA",San Mateo,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are changing the way healthcare uses data. Currently, about 80% of healthcare data is underused because it is too messy and unstructured for humans to efficiently analyze. The healthcare industry needed an intelligent technology that could extract insights quickly and accurately. And that is where we came in. Apixio augments the ability to read, decipher, and understand patient information. This ultimately translates into better care delivery, lower costs, and streamlined processes.

The opportunity at Apixio:
Apixio is a mission-driven data science company passionate to make the practice of medicine more science than art. We are looking for a high-level Data Engineer, who shares our vision, to join our winning team and help us make a difference. This role will be filled by someone who can gain a clear understanding of the current infrastructure and be able to understand how to align with the needs and objectives of the science team. Abilities for training and testing will come into play with their motivation to make constant improvements.

We will be asking you to:

Build and maintain Scala spark / Hadoop pipelines at Apixio that are involved in our training / testing infrastructure for machine learning models
Build and maintain Spark pipelines for ETL of data that write to our data warehouse and maintain the Apixio data warehouse
Contribute to design and implementation of software pipelines that enable Apixio to update its internal proprietary algorithms in a scalable and reliable manner
Contribute to our core code base that performs machine learning in a production setting

To be effective at this role you will need to be:

Curious, willing to learn new things and try new things
Data Driven
Communicative - (willing to stand by decisions / ideas that they have)

We are assuming that you have:
A degree in computer science


Experience working with Scala / Java (min 2 years) in a professional setting
Experience working with Big Data Technologies (Hadoop, spark, etc… )

About Apixio:
We are a mission-driven company developing insights from data for a healthier world. We are helping people make better decisions using artificial intelligence to deliver quality healthcare. We are the technology leader in our space and growing at a 60% year over year rate and show no signs of slowing down. Our success is driven by a team of experienced, passionate and fun engineers, data scientists and business professionals that are working to solve some of the most complex problems, with some of the most innovative technologies and techniques in AI and machine learning. We are well funded by leading organizations such as Bain Capital and SSM, and serve more than 35 national and regional health plan and provider clients, and growing.

What Apixio can offer you:

Meaningful work to improve the healthcare industry
Competitive compensation, including pre-IPO equity
Exceptional benefits, including medical, dental and vision, FSA
401k
Catered, free lunches
Parties, picnics and Friday Happy Hour
Generous Vacation Policy
Free Parking
Subsidized Gym membership/Wellness Program
Modern open office in beautiful San Mateo, CA

"
136,Data Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Location: Redwood City

Job Description:
The data team as Invoice2go is dedicated to supporting the rest of the company by transforming our raw data into high quality and high fidelity insights. We're building data sets for an array of uses including performance analysis, personalization, growth and marketing communications. Engineers on this team are critical to making sure Invoice2go continues to be one of the top business apps in fintech.

What You'll Be Doing


Using Databricks and Spark to transform our largest data feeds into a columnar storage format for easy, cost-effective access in S3
Experimenting with Spectrum to allow queries that run on both Redshift and S3
Optimizing our ETL process to make sure we can provide faster data without sacrificing quality

Who We Are Looking For


Someone who can collaborate with product owners to experiment, iterate and deliver new tools
Can learn new languages as needed, depending on the best framework or tool for the job
Strong understanding of schema design and modeling for large datasets.
Someone who loves what they do, is passionate about software development and wants to contribute to the company culture everyday
Experience with SQL and Python is helpful
Someone that owns their work, from conception to release and beyond.

------

------

-----------
ABOUT US...
-----------

Invoice2go is the world's top selling invoicing app, but we haven't stopped there. Equipping business owners with the most straightforward way to run a business, Invoice2go brings together all the tools needed to get the job done: From winning jobs, tracking estimates and payments, and offering the ability to pay any way.

We strive to give small businesses control over their time and their business.

Invoice2go was founded in Australia by Chris Strode, a small business owner who came from a family of tradespeople, and wanted to help them streamline their invoicing.

Today, we are backed by $60 million in funding from Accel, Ribbit Capital and OCV Partners, and trusted by business owners across 160 countries to send $24 billion in invoicing every year. The company employs a world-class team from it's offices in Redwood City, California and Sydney, Australia.

We're working hard to solve big challenges for the smallest of businesses, and we're always looking for talented people to join our team!!

Invoice2go is an equal opportunity employer. In accordance with applicable law, we prohibit discrimination against any applicant or employee based on any legally-recognized basis, including, but not limited to: veteran status, uniformed service member status, race, color, religion, sex, age (40 and over), pregnancy (including childbirth, lactation and related medical conditions), national origin or ancestry, physical or mental disability, genetic information (including testing and characteristics) or any other consideration protected by federal, state or local law. Our commitment to equal opportunity employment applies to all persons involved in our operations and prohibits unlawful discrimination by any employee, including supervisors and co-workers."
137,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,"BS/MS in Computer Science, Engineering, Mathematics or equivalent
3+ years experience with Spark and Scala; Experience with manipulating large datasets and building efficient data pipelines
Experience solving day to day data related issues
Well versed in SQL
Superb communication, organization, and problem-solving skills
Experience in scalable, high performance systems
Experience with Linux and AWS
","Construct and maintain high volume, efficient, reliable data pipelines for our programmatic buying and analytics systems
Extract data from various reporting platforms for analysis
Ensure the resulting data is organized appropriately so reporting queries from users and applications can run against this data
Tune performance of existing data systems
Ensure high-availability of data sources for our programmatic buying platform.
Collaborate with others across product management, engineering and AdOps as needed to meet company objectives
Work with data science to prepare and process data for machine learning models
Interface with engineers, business stakeholders and analysts to understand data needs.
Define and manage all data sets
",None Found,None Found,"Senior Data Engineer
ZypMedia has built an enterprise grade comprehensive advertising platform – from the ground up – specifically for some of the nation's largest broadcasters and media companies to plan, buy, execute, and manage programmatic campaigns for their local clients across all digital channels, including over-the-top (OTT). Our products are used by thousands of users at some of the leading media companies and our platform processes 100 billion ad calls daily deciding every 100ms on which ad to show to which user.
We are looking for a Senior Data Engineer who is self-motivated, a problem solver and passionate about data engineering.
The Data Engineering team builds data pipelines for large amounts of data and organizes reporting data for end users and clients. This person will own the responsibility of having our reporting used by thousands of users up all the time.
Responsibilities:
Construct and maintain high volume, efficient, reliable data pipelines for our programmatic buying and analytics systems
Extract data from various reporting platforms for analysis
Ensure the resulting data is organized appropriately so reporting queries from users and applications can run against this data
Tune performance of existing data systems
Ensure high-availability of data sources for our programmatic buying platform.
Collaborate with others across product management, engineering and AdOps as needed to meet company objectives
Work with data science to prepare and process data for machine learning models
Interface with engineers, business stakeholders and analysts to understand data needs.
Define and manage all data sets
Experience & Skills Required:
BS/MS in Computer Science, Engineering, Mathematics or equivalent
3+ years experience with Spark and Scala; Experience with manipulating large datasets and building efficient data pipelines
Experience solving day to day data related issues
Well versed in SQL
Superb communication, organization, and problem-solving skills
Experience in scalable, high performance systems
Experience with Linux and AWS
Bonus Points:
Experience with agile development methodologies, CI/CD, scalable infrastructure
Knowledge of AdTech
Significant startup company experience"
138,Lead Big Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Lead Big Data Engineer. Scroll down to learn more about the position’s responsibilities and requirements.

#LI-DNI
What You’ll Do
Lead, design and implement innovative analytical solution using Hadoop, NoSQL, and other Big Data related technologies
Work with product and engineering teams to understand requirements, evaluate new features and architecture to help drive decisions
Build collaborative partnerships with architects, technical leads and key individuals within other functional groups
Perform detailed analysis of business problems and technical environments and use this in designing quality technical solution
Actively participate in code review and test solutions to ensure it meets best practice specifications
Build and foster a high performance engineering culture, mentor team members and provide team with the tools and motivation to make things happen
Work with stakeholders and cross-functional teams to develop new solutions or enhance existing solution
What You Have
5+ years of software development and deployment experience with at least 2 years of hands-on experience with Hadoop applications (e.g. administration, configuration management, monitoring, debugging, and performance tuning)
Strong experience building data ingestion pipelines (simulating Extract, Transform, Load workload), data warehouse or database architecture
Solid, in depth understanding of Hadoop, Spark and Hive
Strong experience with data modeling
Hands-on development experience using open source big data components such as Hadoop, Hive, Pig, Spark, HBase, Hawk, Oozie, Mahout, Flume, Kafka, ZooKeeper, Sqoop etc. preferably with Hortonworks distro
Microsoft Azure: Experience designing, deploying, and administering scalable, available, and fault tolerant systems on Microsoft Azure using HDInsights or Analytics Platform System (APS)
Experience with Azure Management Portal, Azure Machine Learning, and Azure SQL Server
Solid understanding of DevOps principles and practices
Experience with Scala/Java/Python (any combination)
Analytical approach to problem-solving with an ability to work at an abstract level and gain consensus; excellent interpersonal, leadership and communication skills
Data-oriented personality. Motivated, independent, efficient and able to handle several projects; work under pressure with a solid sense for setting priorities
Ability to work in a fast-paced (startup like) agile development environment
Friendly, articulate, and interested in working in a fun, small team environment
Experience working in the retail industry with a large-scale enterprise organization, ecommerce, marketing and CRM applications will be a plus
BS or MS in Computer Science or equivalent; MS preferred
What We Offer
Medical, Dental and Vision Insurance (Subsidized)
Health Savings Account
Flexible Spending Accounts (Healthcare, Dependent Care, Commuter)
Short-Term and Long-Term Disability (Company Provided)
Life and AD&D Insurance (Company Provided)
Employee Assistance Program
Unlimited access to LinkedIn learning solutions
Matched 401(k) Retirement Savings Plan
Paid Time Off
Legal Plan and Identity Theft Protection
Accident Insurance
Employee Discounts
Pet Insurance
EPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring"
139,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"BS/CS, MS/CS or equivalent.
Proficient coding skills in Python/ scripting languages. Expertise with Big Data technologies on AWS, i.e. EMR, Spark, Parquet, Kinesis, Hive, Presto, Airflow, etc.
Bonus points for working with Infrastructure-as-Code (e.g. Terraform).
Huge plus with experience on running ML/AI models in production on large scale data.
Prior healthcare experience is an advantage.
Excellent written and oral communication skills.
Ability to set and manage priorities judiciously.
Ability to articulate ideas to both technical and non-technical audiences. Exceptionally self-motivated and directed.
Superior analytical, evaluative, and problem-solving abilities.
",None Found,"Create and maintain optimal data pipeline & platform architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Work on fast iteration cycles and tight deadlines using an agile methodology.
",None Found,None Found,"The Data Platform Team is looking for a savvy Data Engineer to join our growing team of data experts. As a Data Engineer, you will design & build mission-critical data pipelines and platform that enable analytics for our internal & external customers on one of the biggest specialty healthcare data. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.

Responsibilities
Create and maintain optimal data pipeline & platform architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Work on fast iteration cycles and tight deadlines using an agile methodology.
Qualifications
BS/CS, MS/CS or equivalent.
Proficient coding skills in Python/ scripting languages. Expertise with Big Data technologies on AWS, i.e. EMR, Spark, Parquet, Kinesis, Hive, Presto, Airflow, etc.
Bonus points for working with Infrastructure-as-Code (e.g. Terraform).
Huge plus with experience on running ML/AI models in production on large scale data.
Prior healthcare experience is an advantage.
Excellent written and oral communication skills.
Ability to set and manage priorities judiciously.
Ability to articulate ideas to both technical and non-technical audiences. Exceptionally self-motivated and directed.
Superior analytical, evaluative, and problem-solving abilities.
To apply, please email your resume to:
jobs@veranahealth.com"
140,Data Engineer,"Emeryville, CA 94608",Emeryville,CA,94608,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer

Company Profile

MycoWorks is a biotechnology company that makes fine leathers from mycelium (mushroom) for applications in luxury fashion, footwear, and beyond. We are currently shipping product, and we are backed by some of the leading venture capital firms in Silicon Valley. Our mission is to impact the world with new materials that have superior performance to animal leathers and plastic. Our products currently match or exceed animal leathers in their performance, so we are embarking on an aggressive scale-up plan to meet extraordinary customer demand while also continuously innovating to create new materials and methods of manufacturing.

We are looking for talented and enthusiastic individuals to join our team. You must be excited by delivering amazing product to customers, a fast-paced and ever-evolving work environment, advanced manufacturing, and revolutionizing the fashion, apparel, and luxury industries through environmentally sustainable processes and products.

MycoWorks is committed to building a diverse team and creating an inclusive environment for all employees. Women, people of color, individuals with disabilities, veterans, formerly incarcerated, and LGBQT candidates are strongly encouraged to apply.


Job Summary

MycoWorks is seeking a person to fill the role of Data Engineer. This position will report to the data team lead and will develop, construct, test and maintain architectures including databases and processing systems. This person will also need to solution and and oversee ways to improve data reliability, efficiency, and quality.


Required Job Qualifications

5+ years experience in a software engineering role

Experience with schema design and dimensional data modeling

Structured approach to problem solving: ability to take a broad challenge and break it down into solvable components

Engineer’s eye for automation and fault tolerance, deep empathy for users of the data

Demonstrated ability to work independently, prototype rapidly, and meet deadlines

Strong communication and collaboration skills to effectively explain, influence, and advise with interdisciplinary partners


Essential Functions

The functions and duties described below may not be the only requirements of the position. It will be necessary to follow other instructions and perform other duties as required by MycoWorks team members.

Build fault-tolerant pipelines that ingest and refine large diverse datasets into simplified, accessible data models in production

Build tools and fundamental data sets that encourage self-service

Be responsible for data accuracy and integrity, implement data quality code and SLAs

Improve and maintain the data infrastructure

Work with varied structured, semi-structured (streaming and SOA messages), and unstructured datasets (including images)

Reports to the VP of Quality and/or the Senior Data Scientist, Quality


Compensation and Benefits


Competitive salary and equity packages

Comprehensive medical, dental, and vision benefits

401(K) retirement plan

Flexible and generous vacation policy"
141,"Developer, Marketing Systems","San Francisco, CA 94107",San Francisco,CA,94107,None Found,"Bachelor's degree in Computer Science/Engineering, Analytics, Statistics or equivalent work experience.
4+ years of work experience in Data Engineering, ETL Development and Data Analytics.
4+ years of hands-on experience using SQL and scripting language such as Unix Shell or Python
3+ years of hands-on experience developing on a Linux platform.
2+ years of hands-on experience working in traditional RDBMS such as Oracle, DB2.
1+ years of hands-on experience working in Hadoop using HIVE, HDFS, TEZ, MapReduce, Sqoop.
1+ years of hands-on experience working scripting language such as Python or SAS with BASE SAS, SAS MACRO, and SAS STAT.
Experience with Spark, PySpark, Zeppelin and Jupyter Notebook is nice to have.
Experience and knowledge of cloud technologies is preferred.
Demonstrated experience implementing and automating ETL processes on large data sets.
Experience with report development and supporting data requirements for reporting.
Strong knowledge of Hadoop / Big Data architecture and operational workings.",Strong communications skills.,None Found,"Bachelor's degree in Computer Science/Engineering, Analytics, Statistics or equivalent work experience.
4+ years of work experience in Data Engineering, ETL Development and Data Analytics.
4+ years of hands-on experience using SQL and scripting language such as Unix Shell or Python
3+ years of hands-on experience developing on a Linux platform.
2+ years of hands-on experience working in traditional RDBMS such as Oracle, DB2.
1+ years of hands-on experience working in Hadoop using HIVE, HDFS, TEZ, MapReduce, Sqoop.
1+ years of hands-on experience working scripting language such as Python or SAS with BASE SAS, SAS MACRO, and SAS STAT.
Experience with Spark, PySpark, Zeppelin and Jupyter Notebook is nice to have.
Experience and knowledge of cloud technologies is preferred.
Demonstrated experience implementing and automating ETL processes on large data sets.
Experience with report development and supporting data requirements for reporting.
Strong knowledge of Hadoop / Big Data architecture and operational workings.",None Found,"Job Overview:

The Marketing Analytic Systems data pipeline delivers value to various business partners by providing tools and capabilities to analyze large data sets for research, analytics, campaign management, reporting and tactical/strategic decision making. Our systems development team is looking for a Developer who has passion to work with data and to build solutions that supports our Analytic Systems Solution stack, which includes Hortonworks Hadoop distribution, Google cloud platform (GCP), SAS (Linux environment) and Tableau Server/Desktop. The successful candidate would have extensive experience as a data engineer or ETL developer building and automating data transformation and loading procedures. Strong knowledge and experience using Hive, SQL, SAS and Hadoop to conduct data profiling/discovery, data modelling and process automation is required. The candidate must be comfortable working with data from multiple sources; Hadoop, DB2, Oracle, flat files. The projects are detail intensive, requiring the accurate capture and translation of data requirements (both tactical and analytical needs) and validation of the working solution. We work in a highly collaborative environment working closely with cross functional team members; Business Analysts, Product Managers, Data Analysts and Report Developers. Perform other duties as assigned.

Essential Functions:
Design, develop and implement end-to-end solutions on Hortonworks Hadoop distribution and google cloud platform; strong ability to translate business requirements into technical design plan.
Automate, deploy and support solutions scheduled on Crontab or Control-M. Deployment includes proper error handling, dependency controls and necessary alerts. Triage and resolve production issues and identify preventive controls.
Build rapid prototypes or proof of concepts for project feasibility.
Document technical design specifications explaining how business and functional requirements are met. Document operations run book procedures with each solution deployment.
Identify and propose improvements for analytics eco-system solution design and architecture.
Participate in Hadoop and SAS product support such patches and release upgrades. Provide validation support for Hadoop and SAS products, including any changes to other infrastructure, systems, processes that impact Analytics infrastructure.
Participate in full SDLC framework using Agile/Lean methodology.
Support non-production environments with the Operations and IT teams.
Regular, dependable attendance & punctuality.
Qualifications:

Education/Experience:
Bachelor's degree in Computer Science/Engineering, Analytics, Statistics or equivalent work experience.
4+ years of work experience in Data Engineering, ETL Development and Data Analytics.
4+ years of hands-on experience using SQL and scripting language such as Unix Shell or Python
3+ years of hands-on experience developing on a Linux platform.
2+ years of hands-on experience working in traditional RDBMS such as Oracle, DB2.
1+ years of hands-on experience working in Hadoop using HIVE, HDFS, TEZ, MapReduce, Sqoop.
1+ years of hands-on experience working scripting language such as Python or SAS with BASE SAS, SAS MACRO, and SAS STAT.
Experience with Spark, PySpark, Zeppelin and Jupyter Notebook is nice to have.
Experience and knowledge of cloud technologies is preferred.
Demonstrated experience implementing and automating ETL processes on large data sets.
Experience with report development and supporting data requirements for reporting.
Strong knowledge of Hadoop / Big Data architecture and operational workings.
Communication Skills:
Strong communications skills.
Mathematical Skills:
Basic math functions such as addition, subtraction, multiplication, division, and analytical skills.
Reasoning Ability:
Must be able to work independently with minimal supervision and make sound decisions.
Physical Demands:
This position involves regular walking, standing, sitting for extended periods of time, hearing, and talking.
May occasionally involve stooping, kneeling, or crouching.
May involve close vision, color vision, depth perception, focus adjustment, and viewing computer monitor for extended periods of time.
Involves manual dexterity for using keyboard, mouse, and other office equipment.
May involve moving or lifting items under 10 pounds.
Other Skills:
Ability to collaborate with internal and cross functional teams.
Ability to multi-task and meet deadlines.
Ability to work with diverse teams and multiple technologies.
Work Hours:
Ability to work a flexible schedule based on department and company needs.

Company Profile:

As the fastest growing part of Macy's Inc. business, macys.com is achieving record sales and broadening our workforce. Macys.com offers the entrepreneurial culture of a web business with the stability and support of the best brand in retailing. Creativity and ingenuity partner with business acumen and tech savvy to build a unique business poised for substantial growth. If you're interested in being a part of that growth and want to know what it's really like to work at macys.com, get an inside look at http://ecommerce.macysjobs.com/

Our employees have long-term opportunities and are encouraged to utilize their Supervisors and Human Resources for cross-functional movement to further their careers. At macys.com we are committed to giving back to the community by partnering with local charitable organizations. By skillfully combining the power of digital technology and omnichannel integration with the best in retailing, macys.com is reaching new heights.



This job overview is not all inclusive. In addition, Macy’s, Inc. reserves the right to amend this job overview at any time. Macy’s is an Equal Opportunity Employer, committed to a diverse and inclusive work environment. Macy’s, Inc. – including Macy’s and Bloomingdale’s – will consider for employment qualified applicants with criminal convictions in a manner consistent with SFPC Art. 49 and LA MC ch.XVIII Art. 9."
142,Data Engineer,"South San Francisco, CA 94080",South San Francisco,CA,94080,None Found,None Found,None Found,None Found,None Found,None Found,"Who we are:
Calico is a research and development company whose mission is to harness advanced technologies to increase our understanding of the biology that controls lifespan, and to devise interventions that enable people to lead longer and healthier lives. Executing on this mission will require an unprecedented level of interdisciplinary effort and a long-term focus for which funding is already in place.

Position Description:
Great software engineering and data science are increasingly crucial to biology. We are in the midst of an explosion in the quantity and quality of biological and medical data that are transformative to our understanding of biology and disease. But the tools to store, process, and analyze these data are often primitive, and in some cases don't yet exist. Calico is seeking an exceptional data engineer to join our computing group and be a part of changing that story.

In this role, you will work closely with computational and research scientists to define strategies and implement systems for modeling, collecting, storing, and accessing diverse scientific data and metadata. Collaborating with other scientists and engineers, you will design, build, and maintain databases and data warehouses that underpin our scientific endeavors and accelerate our ability to ask new, sophisticated questions spanning multiple organisms, data modalities, and timescales. You will not only build tools to support existing scientific workflows, but also help set the vision for future data generation and collection efforts.

If you are passionate about data, passionate about biology, and passionate about their intersection—this is the job for you.

What you'll do:

Work with computational and research scientists to understand common analysis use cases and data access needs.
Design strategies for data storage and integration across different data sources (both internal and external) for multiple use cases.
Implement, document, and maintain processing pipelines, databases, and data warehouse infrastructure.
Work closely with full-stack engineers to develop APIs and GUIs for accessing and visualizing scientific data.
Set data engineering vision and drive both independent and collaborative software development projects end-to-end.
Contribute to a range of projects, from one-off solutions to long-term, complex systems.
Build out core infrastructure, tooling, and software development processes.

Position requirements:

5+ years working with contemporary ETL tools and frameworks.
3+ years building Python-based backend systems.
Fluent knowledge of SQL.
Experience implementing RESTful APIs, GraphQL, and other programmatic interfaces to complex multidimensional data.
Experience deploying high-performance data backends in the cloud with Amazon Web Services, Heroku, Google Cloud Platform, or a similar service.
Firm grasp on software testing and test-driven development.
Demonstrated success in owning projects end-to-end, including working with non-technical stakeholders to define requirements and seek feedback.

Nice to have:

Worked with machine learning tools and infrastructure, e.g. TensorFlow and PyTorch.
Built back-ends for high-dimensional graph or network data.
Worked in biology or life sciences, and have familiarity with databases and data types used by computational biologists.
Built software with technologies like ElasticSearch, GraphQL, and Google Cloud Platform.

Some projects you may contribute to:

Data warehouse—a system to extract, transform, and load public and private datasets into a single repository, then making these data available for analysis visually with either off-the-shelf or custom-built GUIs.
Exploratory data visualization & analysis tools—apps to help scientists explore and understand diverse, complex, and multidimensional data.
Data platform—a modern, React (front-end) and Python (back-end) application that our scientists use to manage and process experimental data.
Automation—software to ingest and transform data from custom high-throughput instrumentation.

"
143,Senior Business Intelligence Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Team at Womply advances the state of our data, and empowers the company to make better decisions from our data. We’re seeking a talented and motivated Sr. BI Engineer to join our team. As a Sr. BI Data Engineer, you will hold the keys to the infrastructure that powers our current and future Data Products. We expect you to help us build and leverage the latest technologies to tap into our firehose of data, and your systems will open up new paths for analysis and discovery. You will have the opportunity to make a big impact, and work with extremely talented peers on a fast paced, high energy team.

You will develop ETL and data modeling solutions from our Data Lake into our Data Warehouse, to help us evolve our data-driven philosophy and become a world-class data organization. You will own the design, execution, and ongoing support of critical data warehousing projects enabling accurate reporting and advanced analytics for all of Womply’s internal business units.

You will have to be self-sufficient - we are a startup, so everyone might do a bit of everything to get things done. We look for people who take pride in their work, execute on it, and deliver phenomenal results.

In order to be successful in this role, you will be responsible for:

Building and maintaining the data pipelines from various data sources, while maintaining high accuracy, consistency, and reliability
Leveraging our data foundation to design and implement innovative solutions to our hardest data problems, and ensure their quality and effectiveness with robust verification process
Driving innovation by recommending and adopting new tools and technologies that provide competitive data advantages for the company
Developing, documenting, and implementing data models for analytics
Partnering with business stakeholders, product managers, business intelligence analysts, engineering, and data scientists to understand reporting requirements and bring ideas to production
You must have:

3-8 years in software engineering
Experience with Data Warehousing, Architecting Pipelines, and Data Modeling
Team-oriented, self-motivated, success-driven, roll-up-your-sleeves attitude
Strong intellectual curiosity and demonstrated ability to understand and question the data
Healthy Skepticism to challenge the status quo so we can improve
Technically proficient in:
Languages - Python / Scala / Ruby / SQL / Bash
Technologies - Snowflake/Athena, AWS, Airflow
Nice to have:

Spark
Come build something amazing at Womply:

Womply helps small businesses thrive in a digital world. Our software makes it easy for small businesses to boost their online reputations, engage their customers, and monitor the health of their businesses with data and technology they can’t get anywhere else. We’re one of the fastest growing software companies in the country, serving more than 100,000 small businesses across 400+ business verticals in every corner of America.

We’re a fanatically values-based company with $50 million raised to accelerate our growth. We work hard and push each other to be the best, but we also have fun and don’t take ourselves too seriously. If you want to win and make a big impact, let’s talk. We’re hiring in the Bay Area and Lehi, Utah for engineering, DevOps, design, data science, sales, marketing, business development, account management, and more.

PLEASE NOTE - Direct applicants ONLY. Any recruiter/3rd party submissions we receive will be considered a gift.
More:

Work at Womply

Life at Womply

How we work

Our values

Benefits

Diversity"
144,AWS Data Engineer,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,"At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.","DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud",None Found,None Found," Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
§ Certified AWS Developer - Associate
§ Certified AWS DevOps – Professional (Nice to have)
§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
145,Data Engineer - Business Systems,"San Francisco, CA 94103",San Francisco,CA,94103,None Found,None Found,None Found,None Found,None Found,None Found,"We're looking for someone who is passionate about building the integrations that powers how we make decisions at Asana. This role acts as a foundational pillar to the entire business and spans Sales, Marketing, Finance, HR applications and systems.

The Business Technology team at Asana acts as the engine to the business team; up-leveling our business partners by delivering secure, scalable, and efficient systems, processes, data and insights that help Asana achieve its mission. As a Business Technology Data Engineer, you will be foundational to how the business operates and the value provided with each thoughtful integration between systems leads to tremendous gains for the business. You will expose new insights that help Asana make better decisions, enable complex workflows that help us move more efficiently and effectively as an organization, and expand the value that each system can provide to an end user.

What you'll achieve


Partner closely with cross-functional teams, including Data Engineering, Data Science, and Engineering to build data infrastructure, processes, and tooling
Invest in the infrastructure that supports data-driven workflows for analysts, business teams, and fellow business technology teammates
Architect and implement robust data systems that will scale with Asana's rapid growth

About you


2+ years of experience working with distributed data technologies
Background in a technical and/or quantitative field of study, such as computer science, math, physics or statistics
Experience with Python
Experience with Enterprise IPAAS platforms: Snaplogic, Dell Boomi, Mulesoft, etc. (we use Snaplogic!)
Experience with databases such as MySQL, PostgreSQL, and Oracle preferred
Experience with cloud technologies such as AWS, Azure, and Google Cloud
Ability to demonstrate strong teamwork with your internal team, business stakeholders, and business leaders
Experience with highly scalable ETL / ELT / Data Lake technologies nice to have
Experience working with server-side concepts such as containers, microservices, caching, performance monitoring, and API design nice to have.

About us

Asana is a leading work management platform, helping more than 70,000 organizations and millions of users across 195 countries organize and manage all of their work, including AB-InBev, Airbnb, KLM Air France, NASA, Overstock.com, Uber, Viessmann Group, and Vox Media. Asana has been named a Top 5 Best Place to Work by FORTUNE three years in a row, and one of Glassdoor's and Inc.'s Best Places to Work. Headquartered in San Francisco with offices in New York, Dublin, Sydney, Vancouver, and Reykjavík, Asana is always looking for curious, collaborative people to be a part of our inclusive culture and help us achieve our mission.

Our goal is to ensure that Asana upholds an inclusive environment where all people feel that they are equally respected and valued, whether they are applying for an open position or working at the company. We welcome applicants of all educational backgrounds, gender identities and expressions, sexual orientations, religions, ethnicities, ages, citizenships, socioeconomic statuses, disabilities, and veteran statuses, and we'd love to learn about what you can add to our team."
146,Big Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Our data platform enables analytics, experimentation, machine learning models, streaming, reporting infrastructure and systems metrics which powers and drives innovation at UserTesting. Our team of data engineers and scientists is focused on creating a competitive advantage for UserTesting and our customers through novel data infrastructure, metrics, insights and data services. We are a small but rapidly growing team that builds and leverages state-of-the-art analytics systems, especially around video and Natural Language Processing (NLP).

As a Big Data Engineer, you’ll design, develop & tune data products, applications and integrations on large scale data platforms with an emphasis on performance, reliability and scalability and most of all quality. You’ll support our Machine Learning efforts by building large-scale distributed infrastructure for rapid experimentation, training, and inference. You are passionate about applying cutting-edge machine learning to real-world problems and building the required frameworks and tools to do so.

You will play a key role in the evolution of our Data Platform, duties include:


Work closely with product and design to discover and build solutions that help our customers build great user experiences
Collaborate with engineers who are both remote and co-located in our Mountain View, San Francisco, and Atlanta offices
Work effectively within a team environment, to regularly solicit and act on feedback, focus on root causes, and continually strive to improve
Enhance our customer-facing platform, tester panel distribution systems, video playback tools, and mobile device recording capabilities
Advocate and lead-by-example best practices for code quality in architecture and design, maintainability, performance, and scalability
Lead on promoting just-right solutions to build for the future while also avoiding costly premature optimizations

Requirements

At least 5 years of software development experience.
At least 3 years of experience of using Big Data systems.
Strong in one or more languages (Python/Ruby/Scala/Java/C++)
Strong experience on a professional software development team building highly scalable, distributed systems in the cloud
Experience in REST API design and implementation
Experience with messaging, queuing, and workflow systems, especially Kafka or Amazon Kinesis
Experience with non-relational, NoSQL databases and various data-storage systems, especially: Cassandra, ElasticSearch/Solr, Neo4j, etc.

Preferred Qualifications

Experience working with Machine Learning, especially NLP
Experience with software development on top of Deep Learning Frameworks, especially Tensorflow/Keras
Data engineering knowledge including ETL, DataWarehouse, Data Visualization, etc.
Data modeling experience with columnar data formats
Experience integrating with CI tools programmatically
Experience with Docker, registries and container deployment services (e.g., AWS ECS, Kubernetes).

Additional Information
Besides a great work environment and the opportunity to change the world, we offer competitive salaries, benefits, plenty of perks, as well as equity participation.

UserTesting is an Equal Opportunity Employer and participant in the U.S. Federal E-Verify program. Women, minorities, individuals with disabilities and protected veterans are encouraged to apply. We welcome people of different backgrounds, experiences, abilities, and perspectives. UserTesting will consider qualified applicants with criminal histories in a manner consistent with the
San Francisco Fair Chance Ordinance.
[http://sfgsa.org/modules/showdocument.aspx?documentid=11600]

Learn more about what it is like to work at UserTesting at:
https://www.usertesting.com/about-us/jobs
[https://www.usertesting.com/about-us/jobs]"
147,Big Data Engineer - Machine Learning,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,"
2-5 yrs. experience with Big Data Projects using multiple types of structured and unstructured data. Specifically, ML technologies including Python (multiple versions), Spark, Hadoop, Docker
Proficiency in querying large relational databases and querying languages SQL/NoSQL
Demonstrable experience on good coding practices in a continuous integration context, model evaluation, and experimental design
Prior experience in test driven development and deployment in Cloud environments.
End-to-end understanding of applications and algorithms solutions for the development, implementation and scaling, execution, validation, monitoring, and improvement of data science solutions
Ability to work with a global team, playing a key role in communicating problem context to the remote teams
Bachelor's degree or higher in computer science or related field.
","Tiger Analytics is an advanced analytics consulting firm. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our consultants bring deep expertise in Data Science, Machine Learning and AI. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are one of the highest rated employers on Glassdoor.

RESPONSIBILITIES
Work with client stakeholders to understand key business problems and create Scalable Machine Learning systems that are highly performant
Provide solutions for the deployment, execution, validation, monitoring, and improvement of data science solutions.
Build and manage reusable production data pipelines to implement machine learning models
Write production quality code and libraries that can be packaged as containers, and readily installed and deployed
Troubleshoot production machine learning model issues, including recommendations for retrain, revalidate, and improvements
Collaborate with Technology team and Data Scientist to build data and model pipelines and help running machine learning tests and experiments.
Requirements
2-5 yrs. experience with Big Data Projects using multiple types of structured and unstructured data. Specifically, ML technologies including Python (multiple versions), Spark, Hadoop, Docker
Proficiency in querying large relational databases and querying languages SQL/NoSQL
Demonstrable experience on good coding practices in a continuous integration context, model evaluation, and experimental design
Prior experience in test driven development and deployment in Cloud environments.
End-to-end understanding of applications and algorithms solutions for the development, implementation and scaling, execution, validation, monitoring, and improvement of data science solutions
Ability to work with a global team, playing a key role in communicating problem context to the remote teams
Bachelor's degree or higher in computer science or related field.
Benefits
Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, fast-growing, challenging and entrepreneurial environment, with a high degree of individual responsibility. Standard company benefits include best in class flexi work environment with training opportunities."
148,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description
Job Title: Data Engineer
Location: San Francisco, CA, Austin, TX, San Jose, CA
Terms: Full-time
About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.
What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.
As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do
Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:
Cloud
Analytics
Digitization
Infrastructure
Security
Job Description
Overview
Data is the way our clients make decisions. It is the core to their business, helping create an experience for customers and providing insights into the effectiveness of our product launch & features.

As a Data Engineer , you will be a part of an early stage team that builds the data pipelines, collection, and storage, and exposes services that make data a first-class citizen. We are looking for a Data Engineer to build a scalable data platform. You'll have ownership of core data pipelines that powers top line metrics; You will also use data expertise to help evolve data models in several components of the data stack; You will help architect, building, and launching scalable data pipelines to support growing data processing and analytics needs. Your efforts will allow access to business and user behavior insights, using huge amounts of data to fuel several teams such as Analytics, Data Science, Marketplace and many others.

Responsibilities

Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth
Evolve data model and data schema based on business and engineering needs
Implement systems tracking data quality and consistency
Develop tools supporting self-service data pipeline management (ETL)
SQL and MapReduce job tuning to improve data processing performance

Experience

3+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct advanced performance tuning
Strong skills in scripting language (Python, Ruby, Bash)
1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)
Comfortable working directly with data analytics to bridge Lyft's business goals with data engineering

We are Growing Rapidly: 2019 Highlights
Trianz is growing above the average of the professional services industry. Here are some highlights.
Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.
Won the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.
Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.
Featured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.
Achieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.
Talk to us, Join us & Develop into Leaders
Come join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.
We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
Equal Opportunity Employer
Trianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law)."
149,Senior Data Engineer (Python / SQL / Informatica),"San Francisco, CA 94105",San Francisco,CA,94105,None Found,None Found,None Found,None Found,None Found,None Found,"At Castlight, our mission is to empower people to make the best choices for their health and to help companies make the most of their health benefits. We offer a health benefits platform that engages employees to make better healthcare decisions and can guide them to the right program, care, and provider. The platform also enables benefit leaders to communicate and measure their programs while driving employee engagement with targeted, relevant communications.
To date, Castlight has partnered with more than 240 large enterprise customers, spanning millions of lives, to improve healthcare outcomes, lower costs, and increase benefits satisfaction.
WHAT YOU'LL BE DOING:
Castlight’s Data Engineering Team is responsible for the end to end management of data lifecycle – from data acquisition (inbound), ingestion and validation, warehousing (reporting and analytics) and data distribution (outbound).
We are looking for seasoned data engineers who have a penchant for problem-solving with a passion for innovation and data quality to join our Data Engineering Team. The candidate must be comfortable dealing with large sets of data, disparate data sources, imperfect data sets and can impact data automation that will remove frictions in data processing. As data volume increases, the candidate must apply deep knowledge in query performance tuning, scalability and optimization to make the appropriate recommendation and help implement solutions.
To succeed in this role, you'll need to be a versatile engineer. You must have an innate desire to build things the right way. You should be pain-averse - if some process or system isn't as streamlined as it could be, you want to fix it! You will have a lot of interaction with the Customer Success team and other externally facing groups hence verbal and writing communications skills are required. You must be a champion of standardizing and maturing the business process in order to support scalability.
You will be working with cross-functional teams within and outside of the engineering department and with team members in SF and India. This is a full-time position at our San Francisco headquarters.
RESPONSIBILITIES:
Data Discovery - analyzing data values and data patterns to identify the relationships that link disparate data elements into logical units of information, or “business objects” (such as customer, patient or claim). Identify the transformation rules that have been applied to a source system to populate a target such as transactional entities, operational data store or data warehouse
Data Architecture and Modeling - Create logical and physical data models, including conceptual models. Define data attributes, including domain constraints and privacy attributes. Discover, explore, and visualize the structure of data sources. Discover or identify relationships between disparate data sources. Compare and synchronize the structure of two data sources
Data Governance/Stewardship – Record the business use for defined data. Identify opportunities to share and re-use data. Monitors the progress towards, and tuning of, data quality and data security target metrics. Ensures the quality, completeness and accuracy of data definitions. Identifies and manages the resolution of data quality and data security issues, such as uniqueness, integrity, accuracy, consistency, privacy and completeness in a cost-effective and timely fashion. Identify procedures for disaster recovery and data archiving to ensure effective protection and integrity of data assets.
Data quality - Ensure the stability, integrity and efficiency of data access and data quality across the organization via ongoing database support and maintenance.
Database architecture, administration and development - Work with application development staff to develop database architectures, coding standards, and quality assurance policies and procedures. Participate in testing and implementing database design and functionality, and tuning for performance.
Metadata - support information governance by providing reporting and traceability on data movement, modeling and business intelligence applications, as required by regulatory requirements. Analyze and view the impact of changes to the current information model, avoiding potentially disruptive modifications to existing processes.
Customer Implementation and Production Support – Assist Customer Implementation and Production support teams as DM SME during troubleshooting issues.
Data warehouse (Optional) – Knowledge in developing queries against Enterprise Data Warehouse star scheme is a nice to have. Participate in research and development of recommendations regarding database components, including hardware, database systems, ETL software, metadata management tools and database design solutions.
QUALIFICATIONS:
Minimum of 5 years of experience with Python.
Minimum of 5 years of experience writing SQL for querying databases. Ability to extract information from databases using complex query statements and advanced database tools.
Minimum of 5 years of experience analyzing and developing data requirements and data specifications; hands-on experience documenting understanding and analysis of databases and data models.
Minimum of 5 years of experience developing backend data sources for the reporting and analytics platform.
Minimum of 5 years of experience in logical data modeling.
Minimum of 5 years of technical experience with designing, building, installing, configuring and supporting database servers including database tuning and troubleshooting experience.
Minimum of 3 years of experience with Informatica.
Minimum of 2 years of project coordination / management experience.
Minimum of 1 year of team leadership / mentoring experience.
Experience with version control tools & release management.
BS in Computer Science or related Degree, or equivalent work experience."
150,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,"
Design, build, and support data-centric services including but not limited to event streaming, ETL pipelines, distributed data storage, and real-time data processing.
Work on high impact projects that optimize data availability and quality, and provide reliable access to data across the company.
Collaborate with partner teams to understand their business contexts and analytical challenges, and to transform and sprinkle data-driven fairy dust on their products.
Develop machine-learning software using analytical data models that can generalize across Mixmax customers, but can automatically adapt to each of their individual features.
Communicate strategies and processes around data modeling and architecture to multi-functional groups.
Work with fellow engineers to build out other parts of the data infrastructure, effectively communicating your needs and understanding theirs.
",None Found,"
Exceptional coding and design skills, particularly in Java/Scala or Python.
Extensive previous experience of working with large data volumes, including processing, transforming and transporting large-scale datasets for analytics and business purposes.
Extensive experience data warehousing and ETL pipelines.
Great communication and collaboration skills.
","-------------
Data Engineer
-------------

We're looking to hire our first data engineer to lay the foundation for all aspects of Mixmax's data infrastructure from end to end. You'll be a key communicator, working both cross-functionally in our San Francisco office and across our distributed organization.

As a data engineer you'll help start a team specifically focused on ensuring the company runs on accurate and repeatable data. This means being a member of a team that values continuous and collective learning, culture over process, data driven development and always asking tons of questions. We actively blog about our work, contribute to open source, sponsor Open Collectives, and host/present at meetups - we actively encourage you to do the same and under your own name.

Responsibilities:
-----------------

As a data engineer, you'll:

Design, build, and support data-centric services including but not limited to event streaming, ETL pipelines, distributed data storage, and real-time data processing.
Work on high impact projects that optimize data availability and quality, and provide reliable access to data across the company.
Collaborate with partner teams to understand their business contexts and analytical challenges, and to transform and sprinkle data-driven fairy dust on their products.
Develop machine-learning software using analytical data models that can generalize across Mixmax customers, but can automatically adapt to each of their individual features.
Communicate strategies and processes around data modeling and architecture to multi-functional groups.
Work with fellow engineers to build out other parts of the data infrastructure, effectively communicating your needs and understanding theirs.

Requirements and skills you possess:
------------------------------------


Exceptional coding and design skills, particularly in Java/Scala or Python.
Extensive previous experience of working with large data volumes, including processing, transforming and transporting large-scale datasets for analytics and business purposes.
Extensive experience data warehousing and ETL pipelines.
Great communication and collaboration skills.

Bonus points:

Previous experience with AWS like EC2, RDS, S3, Redshift, SNS, SQS, etc.
Previous experience with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra
Previous experience building out high volume, distributed event systems (such as working w/ Kafka or similar)

Bonus points ++:

Experience using terraform or other infrastructure as code
Contributor to open source technologies

Diversity and inclusion are core to our culture, and we are actively committed to building a more inclusive work environment. If you are a member of an underrepresented group in technology, we strongly encourage you to apply."
151,Principal Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are seeking a Principal Data Engineer in our Ads Engineering team, empowering cutting edge mobile advertising technologies to create the next generation of our Ads Platform.If you want to create amazing user experiences using the latest technologies, then this is the right job for you. You're an excellent communicator, happy to work with people from several different business units. You can translate business needs into technical requirements and implementation. We are problem solvers, constantly reviewing how and why we do things and learning from each other. We are experimental, trying out new tech and ideas and willing to take risks to drive the entire industry forward. We are sociable and fun, and we like to hang out together. We are passionate, some might even say quirky, and while we love what we do, our lives are about more than work. We love games and are obsessed with creating the best player experience.Responsibilities
Partner with senior engineers, architects and product owners to build scalable data pipeline and services.
Work with the product team to understand the business requirements and translate them into the development/design tasks.
Be a role model in engineering best practices and design/coding standard.
Choose the right technology stack to align with the use of cases and scalability.
Collaborate with the other team members across different teams.
Provide technical directions and mentor other engineers.
Player Profile
BA/BS degree in Computer Science, similar technical field of study or equivalent practical experience.
7+ years of hands-on experience in software design and development.
Advanced level expertise in Java Development.
2+ years of experience in working with relational databases such as MySQL, Postgres, etc.
2+ years of experience in NoSQL databases like Bigtable, Cassandra, HBase, etc.
Experience with schema design and data modeling.
Strong understanding of large-scale distributed data processing.
Experience with developing extract-transform-load (ETL).
Experience with distributed messaging systems like Kafka and RabbitMQ
Experience with distributed computing framework like Apache Spark and Flink.
Bonus points
Experience working with AWS or Google Cloud Platform (GCP)
Understanding of machine learning concepts and some experience in working with machine Learning libraries.
Experience in building data warehouse and data lake.
Knowledge of advertising platform.
Your PlatformActivision Blizzard Media is a rapidly growing business built to connect brands with our 350M + global player base across Activision, Blizzard, and King.We create premium, innovative advertising experiences for world-class casual mobile games like King's Candy Crush Saga. Looking ahead, we plan to launch advertising across the Activision Blizzard portfolio, including high growth areas like eSports. This is an exciting time to join this growing business.Our WorldActivision Blizzard, Inc. is the world’s largest interactive entertainment company, with operations across North America, Europe, and Asia. We are home to some of the most beloved entertainment franchises including Call of Duty®, Skylanders®, World of Warcraft®, Overwatch®, Diablo®, Candy Crush™, and Bubble Witch™. Our combined entertainment network delights hundreds of millions of monthly active users in 196 countries, making us the largest gaming network on the planet!We’re proud to be recognized as one of FORTUNE’s ""100 Best Companies To Work For®” for four consecutive years and have earned a spot on FORTUNE’s “Most Admired Companies,” and “Future 50” lists. Our 10,000+ global employees are some of the best and brightest talents across entertainment, media, and technology.The video game industry and therefore our business is fast-paced and will continue to evolve. As such, the duties and responsibilities of this role may be changed as directed by the Company at any time to promote and support our business and relationships with industry partners."
152,Data Engineer - Active Learning,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"------------
About Wellio
------------

We believe that using technology, collaboration, innovation, and diversity, we can transform the way people eat and cook.

We are a growing foodTech company with the goal of empowering families to eat healthier meals. We are building an AI platform that uses advanced machine learning algorithms and statistical techniques to generate personalized meal recommendations and reinforce better eating habits.

To achieve our goal, we have assembled a team of smart, empirically motivated, disagreeable givers who care about making a difference in the world.

As the first member on our newly-formed active learning team, you will help build the infrastructure that powers the data strategy for our Food Intelligence Platform. This includes building CI/CD frameworks for various ML models, as well as integrating with external and internal data labeling services and tools. You will work closely with members of our data science, engineering, and in-house culinary teams to build a state-of-the-art food intelligence platform. We are looking for creative people who are comfortable with ambiguity and enjoy learning and mentoring.

----------------
What you will do
----------------


Work with our data scientists to design and build our Food Intelligence Platform that leverages and supports online learning and machine teaching to develop industry-leading models in the food and health space.
Work with data scientists and engineers to establish a framework for CI/CD of existing and new Machine Learning models, allowing them to be monitored and improved in an automated manner.
Work with data scientists and engineers to establish a framework for active learning for our Food Intelligence Platform. This includes integration with 3rd party data labeling services.
Work with app developers, designers, and product managers to develop micro-apps to be shared with culinary experts to generate high-quality labeled data.

--------------------
Basic qualifications
--------------------


You have previously worked with data scientists and have experience integrating machine learning models into production code (minimum 1 year experience)
You are proficient in writing production-quality code (preferably in Python) and in software design best practices
You are familiar with the software development lifecycle and interested in helping maintain high quality software through CI/CD, TDD, and code reviews.
You previously worked with GCP, AWS, or another PaaS
Experience with relational and non-relational databases

------------------------
Preferred qualifications
------------------------


Experience building both streaming and scheduled data pipelines (experience with Airflow or Cloud Composer is a plus)
A good understanding of distributed computing
Experience with containers and orchestration (e.g., Docker, Kubernetes)
Experience designing, building, and maintaining RESTful APIs

---------
About you
---------


You enjoy collaborating as part of a small, agile, and dynamic team where everyone is valued.
Kaizen and KISS are engineering principles that you believe make sense!
You believe ""done is better than perfect"" and enjoy helping to ship features and quantify the right areas for iterative improvement.
You enjoy helping shape an engineering culture of a small company.
You enjoy both learning and teaching, and enjoy working with others from whom you can learn and who can learn from you.
You have a passion for food and nutrition, and are excited to use your skills to help our customers nourish themselves
You believe that work should be fun and enjoy working with others
You like to try new things and believe in failing fast
You like chocolate, especially dark chocolate

If you got this far, perhaps you're the person we're looking for. We look forward to your application."
153,"Innovation Data Engineer, Consultant","Oakland, CA",Oakland,CA,None Found,None Found,"
5+ years of experience with advanced analytics, business intelligence, data warehouse, big data, machine learning and/or artificial intelligence, including serving in integration and technical delivery roles, such as SQL/ETL developer, data scientist, and/or machine learning specialist
Proven experience with data visualization and insights tools, such as Tableau and/or SAS, from prototype development through production deployment
Experience working with large data sets, such as health information exchanges (HIE)
Demonstrated proficiency in design and writing advanced SQL/ETL queries and Oracle tools. Including, experience and hands on expertise developing batch and/or real-time data extract applications
Experience working with Information Architects, Data Analysts, DBAs and interface with multiple supporting business teams.
Experienced in both SDLC & agile development methodologies
Must be data-oriented with a highly curious mindset
Strong communication and presentation skills (PowerPoint) at all levels of leadership
Experience in healthcare highly preferred, but not required
Highly preferred Data Scientist experience:",None Found,"
Exploration of new/emerging AI & Machine Learning and Advanced analytic platforms through partnership with internal business and IT Stakeholders
Research, analysis, creation and presentation of innovative ideas and product concepts through pitch-decks and prototype development
Data mining of claims and membership data extracts using advanced SQL/ETL tools and techniques.
Ad-hoc data insights requests and translate into presentation as “data stories”
Expansion of data exploration capabilities through third party data sources (clinical, social determinates, etc.)
Research, document and provide findings on new and emerging technology concepts to broader audience.",None Found,None Found,"Description:

Blue Shield of California is seeking Innovation Data Engineer for the Health Innovation Technology department to play a key role in realizing goals and objectives for healthcare transformation. Reporting to the Director of Product Integration & Delivery, this individual will be responsible for data exploration, actionable insights discovery and related “art-of-possible” prototype development for new and emerging innovative technology and advanced analytic capabilities. This is a high-visibility role to share knowledge, agile problem-solving skills and creative solutions through partnership with application developers, product owners, and multiple stakeholders in a collaborative environment.

Responsibilities:
Exploration of new/emerging AI & Machine Learning and Advanced analytic platforms through partnership with internal business and IT Stakeholders
Research, analysis, creation and presentation of innovative ideas and product concepts through pitch-decks and prototype development
Data mining of claims and membership data extracts using advanced SQL/ETL tools and techniques.
Ad-hoc data insights requests and translate into presentation as “data stories”
Expansion of data exploration capabilities through third party data sources (clinical, social determinates, etc.)
Research, document and provide findings on new and emerging technology concepts to broader audience.

Qualifications

Qualifications
5+ years of experience with advanced analytics, business intelligence, data warehouse, big data, machine learning and/or artificial intelligence, including serving in integration and technical delivery roles, such as SQL/ETL developer, data scientist, and/or machine learning specialist
Proven experience with data visualization and insights tools, such as Tableau and/or SAS, from prototype development through production deployment
Experience working with large data sets, such as health information exchanges (HIE)
Demonstrated proficiency in design and writing advanced SQL/ETL queries and Oracle tools. Including, experience and hands on expertise developing batch and/or real-time data extract applications
Experience working with Information Architects, Data Analysts, DBAs and interface with multiple supporting business teams.
Experienced in both SDLC & agile development methodologies
Must be data-oriented with a highly curious mindset
Strong communication and presentation skills (PowerPoint) at all levels of leadership
Experience in healthcare highly preferred, but not required
Highly preferred Data Scientist experience:
o Machine learning frameworks experience, such as TensorFlow, Keras, Microsoft Azure ML, etc.
o Experience with common data science tool kits, such as R and/or Python
o Understanding of machine learning techniques and algorithms, such Boosted Decision Trees, Naive Bayes, SVM, Decision Forests, etc.
Education:
Requires a minimum of a Bachelor’s Degree in CS, Computer Engineering, Statistics, Machine Learning, Mathematics or Industrial Engineering. Master’s Degree strongly preferred.

Physical Requirements

Office Environment - roles involving part to full time schedule in Office Environment. Based in our physical offices and work from home office/deskwork – Activity level: Sedentary, frequency most of work day.
Please click here for further physical requirement detail."
154,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The customer journey starts with a question. And consumers expect answers. Yext puts businesses in control of their facts online with brand-verified answers in search. By serving accurate, consistent, brand-verified answers to consumer questions, Yext delivers authoritative information straight from the source — the business itself — no matter where or how customers are searching. Taco Bell, Marriott, Jaguar Land Rover, and businesses around the globe use the Yext platform to capture consumer intent and drive digital discovery, engagement, and revenue — all from a single source of truth. Yext's mission is to provide perfect answers everywhere.

We are looking for a savvy Senior Data Engineer to join our growing team of analytics experts. The Senior Data Engineer will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Senior Data Engineer will support our data analysts and the business at large on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. Understanding of some core business processes and previous experience supporting them is a big plus (order to cash, procure to pay, hire to retire etc).

The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.

Responsibilities


Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executives, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.

Minimum Requirements


3-5 years of advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases (3-5 years of experience).
BA/BS or similar college level education.
Experience building and optimizing big data data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Bachelor's degree in Computer Science, Statistics, Information Systems or another quantitative field.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with AWS cloud services
S3
EMR
Redshift
Experience with:
Python
Java

Compensation, Benefits & Perks

Yext offers the following exceptional benefits: competitive compensation, 401k, unlimited snacks, daily meal allowance, flexible hours/paid time off, and excellent health/dental/vision insurance. We treat our employees well and offer tremendous growth opportunities. Challenging work pushes our people to be creative in a casual environment that is caring, fun, and collaborative. We believe that when you have smart, happy people working together you can produce something special.

About

Yext has been named a Best Place to Work by Fortune and Great Place to Work®, as well as a Best Workplace for Women. Yext is headquartered in New York City with offices in Amsterdam, Berlin, Chicago, Dallas, Geneva, London, Miami, Milan, Paris, San Francisco, Shanghai, Tokyo and the Washington, D.C. area.

Yext is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ethnicity, religion, creed, national origin, ancestry, genetics, sex, pregnancy or childbirth, sexual orientation, gender (including gender identity or nonbinary or nonconformity and/or status as a trans individual), age, physical or mental disability, citizenship, marital, parental and/or familial status, past, current or prospective service in the uniformed services, or any characteristic protected under applicable law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires accommodation, please let us know.

#LI-MF1"
155,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,"
Provide senior-level contribution to the design, implementation and maintenance of complex data pipelines
Build reliable services for gathering & ingesting data from a wide variety of sources
Build performant and reliable data pipelines to validate, extract and normalize data from a wide variety of sources
Develop strategy, tools, and workflow for integrations and ingestion of data
Collaborate with cross-functional teams and stakeholders to understand data needs
Write quality, maintainable code with extensive test coverage in a fast-paced, agile software engineering environment
Mentor junior teammates and lead by example in demonstrating software engineering best practices",None Found,"
Hold a B.S. or M.S. in Computer Science, or equivalent degree
5-7+ years of proven working experience as a data engineer
Excellent software engineering skills and strong fundamentals in algorithms, data structures, predictive modeling and big data concepts
Strong programming fundamentals and proficiency in an object-oriented language such as Python or Scala
Excellent communication skills to collaborate with stakeholders in engineering, data science, and product
Experience with our stack (Python, PySpark, Airflow, AWS ecosystem) is preferred but not required
Experience building large-scale and complex data processing pipelines
A successful history of manipulating, processing and extracting value from large disconnected datasets
Strong analytical skills related to working with unstructured datasets","Data Acquisition & Ingestion Team Overview
The Data Acquisition & Ingestion team is responsible for collecting, ingesting, and normalizing the data that powers Helio. We are building an automated ingestion system that is able to scale and reliably onboard and extract value from hundreds of disparate data sources. We are also champions of maintaining high quality data practices across all of Helioâ€™s data pipelines.
Some of the technologies we leverage are Python 3, PySpark, AWS, Docker, Kubernetes, Postgres, Airflow, Jenkins and Git.
We are looking for a Senior Data Engineer to help us design and develop data pipelines to ingest, validate, extract, and normalize data across new and existing sources. The ideal candidate will be self-driven and comfortable balancing progress towards a longer-term roadmap while maintaining context and stability across a dynamic set of existing data sources. While the role is for an individual contributor, we are looking for someone who is excited and willing to mentor junior engineers.
CircleUp has been named one of the Top 5 Most Disruptive Companies in Finance by CNBC, one of the 50 Best FinTech Innovators by KPMG, Top 3 Most Innovative Companies within Data Science by Fast Company and one of America's Most Promising Companies by Forbes. We are backed by top-tier investors including Google Ventures, Union Square Ventures (backers of Etsy/Kickstarter), and the ex CEOs/Presidents of Goldman Sachs, Morgan Stanley, Thomson Reuters, the Stanford Endowment and Capital One.
Responsibilities
Provide senior-level contribution to the design, implementation and maintenance of complex data pipelines
Build reliable services for gathering & ingesting data from a wide variety of sources
Build performant and reliable data pipelines to validate, extract and normalize data from a wide variety of sources
Develop strategy, tools, and workflow for integrations and ingestion of data
Collaborate with cross-functional teams and stakeholders to understand data needs
Write quality, maintainable code with extensive test coverage in a fast-paced, agile software engineering environment
Mentor junior teammates and lead by example in demonstrating software engineering best practices
Requirements
Hold a B.S. or M.S. in Computer Science, or equivalent degree
5-7+ years of proven working experience as a data engineer
Excellent software engineering skills and strong fundamentals in algorithms, data structures, predictive modeling and big data concepts
Strong programming fundamentals and proficiency in an object-oriented language such as Python or Scala
Excellent communication skills to collaborate with stakeholders in engineering, data science, and product
Experience with our stack (Python, PySpark, Airflow, AWS ecosystem) is preferred but not required
Experience building large-scale and complex data processing pipelines
A successful history of manipulating, processing and extracting value from large disconnected datasets
Strong analytical skills related to working with unstructured datasets
Useful Traits for this Role
Communication, both technical and business-level, especially with external contractors
Detail-oriented, Business-sense and ability to manage ambiguity; able to synthesize detailed schema specifications from a newly identified source
Ability to understand, maintain, document, and be knowledgeable about a large variety of data sources; able to deal with a certain level of reactive context-switching
Proactive and driven; will identify gaps in our data model and will proactively work to improve it"
156,Data Engineer,"San Francisco, CA 94109",San Francisco,CA,94109,None Found,None Found,None Found,"
Writing scheduled Spark pipelines that perform sophisticated queries on the entirety of our datasets
Writing real-time pipelines that execute complex operations on incoming data
Synchronizing large amounts of data between unstructured and structured formats on various data sources
Creating testing and alerting for data pipelines
Building out our data infrastructure and managing dependencies between data pipelines
Defining and implementing metrics that provide visibility into our data quality",None Found,"
You have an undergraduate and / or graduate degree in computer science or a similar technical field, with a sound understanding of statistics
You have 1-2 years of industry experience as a data engineer
You have hands-on experience doing ETL and have written data pipelines in either Spark, Hadoop, or similar technologies
You have a sound understanding of SQL
You have worked with data lakes such as S3 or HDFS
You have worked with various databases, such as Postgres, Cassandra, or Redshift before, and understand their pros and cons
You have a working knowledge of the following technologies, or are not afraid of picking them up on the fly: Mesos, Chronos, Marathon, Jenkins
You are fluent in at least one scripting language (preferably NodeJS or python) and one compiled language (such as Scala, Java, or C)
You have great communication skills and ability to work with others
You are a strong team player, with a do-whatever-it-takes attitude","About Hive

Hive is a full-stack deep learning platform helping to bring companies into the AI era. We take complex visual challenges and build custom machine learning models to solve them. For AI to work, companies need large volumes of high quality training data. We generate this data through Hive Data, our proprietary data labeling platform with over 1,000,000 globally distributed workers, generating millions of high quality pieces of data per day. We then use this training data to build machine learning models for verticals such as Media, Autonomous Driving, Security, and Retail. Today, we work with some of the largest companies in the world to redefine how they think about unstructured visual data. Together, we build solutions that incorporate AI into their businesses to completely transform industries.

We are fortunate that investors like Peter Thiel (Founders Fund), General Catalyst, 8VC, and others see Hive's potential to be groundbreaking in AI business solutions. We have over 120 rock stars globally in our San Francisco and Delhi offices. Please reach out if you are interested in joining the AI revolution!

Data Engineer Role

In order to execute our vision, we need to grow our team of best-in-class data engineers. We are looking for developers who conduct impeccable data practices and implement high quality data infrastructures. We value hard workers who are comfortable improvising solutions to big data challenges while building a system that can stand the test of time. Our ideal candidate has experience building data infrastructure from the ground up, contributes innovative ideas and ingenious implementations to the team, and is capable of planning out scalable, maintainable data pipelines.

As a data engineer, you would at first work primarily on our Hive Media product, taking real-time data from hundreds of television streams and turning them into a combination of real-time and scheduled outputs, especially our signature ads feed. Your work would improve the quality of our results while reducing computational cost and latency. Expect truly novel challenges.
Responsibilities
Writing scheduled Spark pipelines that perform sophisticated queries on the entirety of our datasets
Writing real-time pipelines that execute complex operations on incoming data
Synchronizing large amounts of data between unstructured and structured formats on various data sources
Creating testing and alerting for data pipelines
Building out our data infrastructure and managing dependencies between data pipelines
Defining and implementing metrics that provide visibility into our data quality
Requirements
You have an undergraduate and / or graduate degree in computer science or a similar technical field, with a sound understanding of statistics
You have 1-2 years of industry experience as a data engineer
You have hands-on experience doing ETL and have written data pipelines in either Spark, Hadoop, or similar technologies
You have a sound understanding of SQL
You have worked with data lakes such as S3 or HDFS
You have worked with various databases, such as Postgres, Cassandra, or Redshift before, and understand their pros and cons
You have a working knowledge of the following technologies, or are not afraid of picking them up on the fly: Mesos, Chronos, Marathon, Jenkins
You are fluent in at least one scripting language (preferably NodeJS or python) and one compiled language (such as Scala, Java, or C)
You have great communication skills and ability to work with others
You are a strong team player, with a do-whatever-it-takes attitude
What We Offer You

We are a group of ambitious individuals who are passionate about creating a revolutionary machine learning company. At Hive, you will have a significant career development opportunity and a chance to contribute to one of the fastest growing AI startups in San Francisco. The work you do here will have a noticeable and direct impact on the development of Hive.

Our benefits include competitive pay, equity, health / vision / dental insurance, catered lunch and dinner, a corporate gym membership, etc.

Thank you for your interest in Hive."
157,"VP, Analytics","San Francisco, CA 94105",San Francisco,CA,94105,None Found,"Minimum of 10 years’ experience working in analytics
Strong business and strategic thinker; ensure analytics function supports key business objectives
Minimum of 5 years team leadership / people management experience, including recruiting, training, developing, staffing and performance management
Strong cross-functional leadership and execution
Healthcare data or population health experience
Experience working in, or with, Product Management organizations
Experience with statistical modeling techniques and common analytical frameworks
Ability to deliver consistent and accurate operational reporting
Technical expertise:
Expert with PostgreSQL
Experienced with Python
Experience with visualization platforms (Tableau ideally)
Experience with MPP databases (Redshift, Vertica)
Ideal candidates will also have:
Advanced degree (Quantitative MBA, CS, ML)
Experience with Apache Airflow or Pivotal GreenPlum",None Found,"
Develop and own strategic vision and roadmap for Analytics; think critically to test hypotheses and prioritize analyses that support key stakeholders and deliver highest value insights for the business
Partner cross-functionally with key stakeholders:
Product Management to deliver insights and influence Product roadmap development
Customer Success to develop and refine reporting packages and develop analytical tools for CS to share insights with customers, including reporting on Cost Savings (ROI)
Sales and Product Marketing to develop proof points for the market
Develop health economic analysis for potential white paper or journal publication
Build and manage healthcare claims tools (e.g. risk adjustment software), savings methodologies, and claims-based reporting
Build executive and product dashboards to provide visibility and access to key metrics, define targets for KPIs, and track progress against targets
Support the team in creating data models with accuracy and efficiency, identify the best methodology, and design outputs to showcase findings in a digestible way
Provide technical oversight to Analytics Infrastructure team:
Oversee work of our Data Engineer to keep workflow management and data pipelines up to date
Work with Engineering and Data Management to solve for technical dependencies
Provide strategic thought partnership to the leadership team on corporate growth opportunities as they relate to driving steerage and population health
Establish and maintain a culture of data driven decision making; leads Analytics function to ensure visibility and alignment with cross-functional leadership team
Inspire, lead, and grow our team, embodying our company culture and values",None Found,None Found,"VP, Analytics
Role:
Castlight Health is seeking a senior leader with specialized expertise in leading the Analytics function. Analytics at Castlight is a critical function that drives our Product Roadmap, ensures we demonstrate value to our customers, and enables our mission of steering users to the best care for them. Our team is focused on answering questions like: Are we steering users to the highest quality, lowest cost providers? Are we driving medical cost savings for our customers? Is our product delivering value and engaging users, and where should we invest to maximize product value?
Our team builds models that power customer reporting, defines metrics that power product analytics reporting, and researches new areas of opportunity for the product, as well as strategic growth vectors for the business. The team partners closely with key stakeholders across the major functions of the business, including Product, Customer Success, Sales, Marketing, and Engineering to define and tackle the most important questions facing the business.
We are looking for an Analytics leader who will be a member of the company’s leadership team and will provide values-based mentoring and leadership to Castlight’s talented group of analysts, data scientists and healthcare actuaries to achieve these key objectives and develop a best-in-class analytics team that spans Product Analytics, Customer Analytics, Healthcare Analytics, and Analytics Infrastructure functions.
Responsibilities:
Develop and own strategic vision and roadmap for Analytics; think critically to test hypotheses and prioritize analyses that support key stakeholders and deliver highest value insights for the business
Partner cross-functionally with key stakeholders:
Product Management to deliver insights and influence Product roadmap development
Customer Success to develop and refine reporting packages and develop analytical tools for CS to share insights with customers, including reporting on Cost Savings (ROI)
Sales and Product Marketing to develop proof points for the market
Develop health economic analysis for potential white paper or journal publication
Build and manage healthcare claims tools (e.g. risk adjustment software), savings methodologies, and claims-based reporting
Build executive and product dashboards to provide visibility and access to key metrics, define targets for KPIs, and track progress against targets
Support the team in creating data models with accuracy and efficiency, identify the best methodology, and design outputs to showcase findings in a digestible way
Provide technical oversight to Analytics Infrastructure team:
Oversee work of our Data Engineer to keep workflow management and data pipelines up to date
Work with Engineering and Data Management to solve for technical dependencies
Provide strategic thought partnership to the leadership team on corporate growth opportunities as they relate to driving steerage and population health
Establish and maintain a culture of data driven decision making; leads Analytics function to ensure visibility and alignment with cross-functional leadership team
Inspire, lead, and grow our team, embodying our company culture and values
Qualifications:

Minimum of 10 years’ experience working in analytics
Strong business and strategic thinker; ensure analytics function supports key business objectives
Minimum of 5 years team leadership / people management experience, including recruiting, training, developing, staffing and performance management
Strong cross-functional leadership and execution
Healthcare data or population health experience
Experience working in, or with, Product Management organizations
Experience with statistical modeling techniques and common analytical frameworks
Ability to deliver consistent and accurate operational reporting
Technical expertise:
Expert with PostgreSQL
Experienced with Python
Experience with visualization platforms (Tableau ideally)
Experience with MPP databases (Redshift, Vertica)
Ideal candidates will also have:
Advanced degree (Quantitative MBA, CS, ML)
Experience with Apache Airflow or Pivotal GreenPlum"
158,Senior Software/Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Aclima

Aclima delivers hyperlocal air pollution and climate emissions intelligence at unprecedented block-by-block resolution. Our air quality mapping and analysis platform empowers governments, companies, researchers and the public to diagnose air quality problems in support of reducing emissions and protecting public health, at both the local and global level.

Based on years of R&D, Aclima's platform combines our breakthrough mobile and stationary sensing technology, climate science and machine learning to provide next-generation diagnostics of critical air pollutants — from CO2 and methane to particulate matter. With this new layer of Environmental Intelligence, Aclima makes the invisible visible, charting a new course for improving human and planetary health.

Aclima was recognized as a 2017 World Economic Forum Technology Pioneer, as well as being named one of the prestigious 2018 Global Cleantech 100 companies. Aclima is headquartered in San Francisco, CA. For more information, visit blog.aclima.io.

Software at Aclima

We operate across the software stack, working with our hardware and firmware engineers who design and build our data collection platform, to the technical operations team who fabricates technology and maintains and operates our instruments in cities across the world. The software team integrates work across these disciplines and more.

The software team is responsible for running data collection, data analysis systems, and for serving data to our interfaces and products. Additionally, we support teams inside Aclima, building tooling that ranges from flashing firmware in the field to optimizing routing algorithms for mobile data collection. We are looking for a passionate Senior-level Engineers to help build our software team. We seek smart and motivated people who have a strong interest in environmental science and big data. You'll collaborate with data scientists, climatologists, hardware engineers, and designers in a fast-paced, purpose-driven atmosphere with flexible hours and competitive perks.

Our stack includes: Google Cloud Platform (BigQuery, PubSub, DataFlow, CloudSQL), Python (Flask), ElasticSearch, MapBox, Kubernetes, Docker, CircleCI, Git, Go

Primary Responsibilities

You will be responsible for building tooling and infrastructure to enable more rapid iteration on our data collection and serving objectives. For example, you might work on a tool that routes drivers in the field to optimize data collection, based on environmental and traffic data. Or you might be introducing and migrating to data tooling which supports a DAG of data jobs. Or you could be working to scale services that present data to our teams and customers. You will be responsible for taking projects from concept to production, and so might need to operate at all levels of the stack and be able to work independently.

You will be in communication with fellow engineers, and must be able to justify your technical decisions. However, you will also often interface with non-technical stakeholders, and must be able to communicate your approach to them, and also solicit and implement user feedback. Finally, you will interact regularly with executive staff, and be involved in strategic decision-making.

As a member of our growing software team, you will also participate in defining future direction. Depending on your direction, you could help to evolve our interview process, advocate for tooling for cross-team collaboration, or champion standards for code review or continuous deployment.

Responsibilities


Collaborate with engineers, data scientists, designers and product managers to build, maintain, and improve Aclima's data platform
Architect scalable backend systems, services, and APIs
Provide data engineering support to the data science team to productize their output
Identify gaps in automation and workflows, and build tooling to address them
Collaborate with senior leaders to chart a long term vision for maintaining and scaling our systems

Qualifications


5+ years of experience designing, building, scaling and maintaining production services
Ability to identify and diagnose problems and then build and deploy solutions
Experience with championing and deploying new technologies and approaches
Desire for continuous learning of new technologies, languages, and platforms
Bonus points for: Experience with embedded systems and firmware
Deep knowledge of Python and Flask
Familiarity with cloud environments, especially Google Cloud Platform, and Docker/k8s
Knowledge of data processing systems, e.g. Airflow or Google Dataflow

Equal Employment Opportunity

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

Aclima has made the Kapor Capital Founders' Commitment ( https://www.kaporcapital.com/founders-commitment/ ).

Full-time position only. Out of area candidates will be considered.

Compensation and Benefits

Aclima employees work incredibly hard, and we work incredibly hard to thank them. We offer a competitive compensation package and excellent benefits. You'll appreciate our challenging work environment, exceptional colleagues, strong business momentum, and the ability to make a difference. Benefits include: medical/dental/vision/Life/LTD, 401K, vacation, commuter and wellness benefit, company events and an extraordinary culture.

Learn More About Aclima


Driving Science: Mapping Air Pollution in Oakland​ ( https://blog.aclima.io/aclima-google-edf-map-air-quality-oakland-aad28c67859b ) - Aclima blog
The High-Res Future of Anti-Pollution Tech​ ( https://www.google.com/about/stories/future-of-anti-pollution-tech/ ) - Google blog
Getting Hyper-Local: Mapping Street Level Air Quality Across California​ ( https://www.blog.google/products/maps/getting-hyper-local-mapping-street-level-air-quality-across-california/ ) - Google blog
Incredibly Detailed Maps Show The Pollution From California's Fires ( https://www.fastcompany.com/40493798/these-incredibly-detailed-maps-show-the-pollution-from-californias-fires )​ - Fast Company

To Apply:
Please submit your resume and a cover letter explaining why you are the ideal candidate for this role."
159,Sr Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
BS or MS degree in Computer Science or a related technical field
6+ years of Python or Java development experience, Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases
6+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale
Experience with specific AWS technologies (such as Glue, S3, Redshift, EMR, and Kinesis) a plus
Experience writing production code for Python or JVM-based systems, but you know a few other languages and like the right tool for the job.
Knowledge of machine-learning tools and techniques.
Experience optimizing larger applications to increase speed, scalability, and extensibility.
Proven self-starter who can move projects forward by filling in the gaps on Agile teams, from leading a design session to doing some test automation, to mentoring a teammate struggling with a new technology",None Found,None Found,None Found,"
BS or MS degree in Computer Science or a related technical field
6+ years of Python or Java development experience, Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases
6+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale
Experience with specific AWS technologies (such as Glue, S3, Redshift, EMR, and Kinesis) a plus
Experience writing production code for Python or JVM-based systems, but you know a few other languages and like the right tool for the job.
Knowledge of machine-learning tools and techniques.
Experience optimizing larger applications to increase speed, scalability, and extensibility.
Proven self-starter who can move projects forward by filling in the gaps on Agile teams, from leading a design session to doing some test automation, to mentoring a teammate struggling with a new technology","As other sectors have shifted to eCommerce-first business models in recent years, food & beverage has continued to rely predominantly on traditional brick & mortar models, but this is changing rapidly and a period of extraordinary disruption is now underway. New technologies are transforming every aspect of reaching consumers, from the rise of digital marketing and online grocery platforms, to the creation of supply chain tools that enable speedy at-home delivery.

To seize this opportunity and lead the food & beverage industry into its remarkable next chapter, PepsiCo – the international food & beverage powerhouse with annual net revenue exceeding $64 billion and beloved brands including Frito-Lay, Gatorade, Pepsi-Cola, Quaker, and Tropicana – is expanding its Global eCommerce Team. As it needs the greatest minds in data & analytics, software development, machine learning optimization, and next-generation supply chain. Although PepsiCo is a large multinational, the PepsiCo Global eCommerce Team prides itself on having the entrepreneurial, action-oriented culture of an exciting startup business. As part of our group, alongside Silicon Valley veterans, founders of successful startup companies, and food & beverage experts to address a wide variety of the fascinating technical challenges facing our industry.

Given PepsiCo’s incredible global reach – our foods and beverages are enjoyed more than one billion times a day in more than 200 countries and territories, and our value chain involves diverse partners ranging from farmers and food scientists to retailers and logistics specialists – the challenges we’re addressing are complex and the solutions will be deeply impactful. The goal of the PepsiCo Global eCommerce Team is to build the technological products and capabilities that will reinvent our industry and make us the #1 food & beverage business in eCommerce for decades to come.

Accountabilities:
Lead problems assessment of eCommerce challenges to lead development and design of technology solutions across functions involving computer hardware and software
Lead technology project evaluations as well as proposal feasibility with the different eCommerce businesses
Apply theoretical expertise and innovation to create or apply new technologies to apply to the entire digital landscape
Act as a consultant to the broader business users, management, vendors, and technicians to determine technology needs and system requirements
Build new technologies and algorithms to optimize any business process
Develop data set processes and projects requirements
Use large data sets to resolve major business and functional issues whisle improving data reliability, efficiency and quality
Optimize processes implementing new technology and automations across eCommerce businesses and eCommerce functions
Qualifications/Requirements
BS or MS degree in Computer Science or a related technical field
6+ years of Python or Java development experience, Experience with multiple data technologies and concepts such as Airflow, Kafka, Hadoop, Hive, Spark, MapReduce, SQL, NoSQL, and Columnar databases
6+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale
Experience with specific AWS technologies (such as Glue, S3, Redshift, EMR, and Kinesis) a plus
Experience writing production code for Python or JVM-based systems, but you know a few other languages and like the right tool for the job.
Knowledge of machine-learning tools and techniques.
Experience optimizing larger applications to increase speed, scalability, and extensibility.
Proven self-starter who can move projects forward by filling in the gaps on Agile teams, from leading a design session to doing some test automation, to mentoring a teammate struggling with a new technology
Relocation Eligible: Not Applicable
Job Type: Regular

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity

Our Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901 - 4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance.

If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy

Please view our Pay Transparency Statement"
160,Senior Data Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Course Hero is scaling! We are looking for a motivated, and progressive Senior Data Engineer that will help build our next-generation Data Platform. We are searching for someone who has a devops mentality and passionate about innovating, optimizing and automating data at scale.

You can expect high impact and wide-ranging responsibilities: As a Senior Data Engineer you will have deep hands-on experience working with Data Scientists, Machine Learning experts, and Search Engineers. The scalable systems you build will enable our product designers to bring powerful data products to the Course Hero platform.

Check out these videos to learn more about our engineering culture ( https://www.youtube.com/watch?v=Hpa0bVeJpIE ), and our company mission ( https://www.youtube.com/watch?v=pmzuj0MW_Dk ).

Here are some ways you'll make an impact:

5+ years relevant data experience
Work with a team of passionate data engineers and scientists to enable data mining, deep learning, statistical modeling, predictive analytics, machine learning, and NLP.
Empower engineers and to innovate and discover insights by removing the technical barriers that come with processing big data.
Ensure compliance with the organization's high bar for data quality and modeling standards, across the product and related business areas.
Provide tools to empower internal teams across the organization (sales, operations, finance, engineering, etc.) to make data-driven decisions.
Institute development best practices to ensure the team produces high quality, well architected and supportable code through a continuous delivery model.
Participate in the on-call rotation and document administration and response procedures through runbooks & playbooks.

Are you our Star Senior Data Engineer?


Worked with Machine Learning experts and Data Scientists to enable self-service data ingestion, transformation, visualization, reporting and advanced analytics (machine learning, AI).
Strategic thinker and thrive operating in a broad scope, from conception through continuous operation.
Robust Ops foundation - you're always thinking ""What happens if this fails"" when you build things.
Bachelor's or Master's degree in computer science, mathematics, economics, engineering, or other related fields.
Expert in Mysql and other relational database technologies.
Hands-on experience working with large scale data ingestion, ETL processing, storage, Hadoop ecosystems, Spark, non-relational databases (NoSQL, MongoDB, Cassandra), and messaging systems (Kafka, Kinesis, RabbitMQ).
Written scripts in one or more languages such as Python or Go.

Bonus Points:

Understand open source software like Kafka, Arvo, ElasticSearch, NGINX, Kubernetes, and Docker.
Familiar with Python analytics libraries or use of R language.
Experience with standard IT security practices such as Access, Authorization, and Key Management.
Performed hands on work using AWS stack (i.e. S3, Redshift, EC2, SNS, SQS, SES, DynamoDB,Kinesis).

About Us:
At Course Hero, we have an awesome team and a truly engaging culture. We are customer-focused, collaborative, responsible, gritty and we love to learn. Our bold mission is to help students graduate confident and prepared!

We are not the only ones that think we're onto something big. Course Hero has been recognized as the 245th Fastest Growing Company in North America ( https://www.prnewswire.com/news-releases/course-hero-ranked-number-245-fastest-growing-company-in-north-america-on-deloittes-2018-technology-fast-500-300751425.html ) on Deloitte's 2018 Technology Fast 500 and also 2018's One of the Best Places to Work in the Bay Area ( https://www.bizjournals.com/sanfrancisco/feature/best-places-to-work/2018/best-places-to-work-bay-area-2018-top-workplaces.html ) by the San Francisco Business Times and the Silicon Valley Business Journal. Read up on some of our recent news coverage ( https://www.coursehero.com/press-room/ ), blog ( https://www.coursehero.com/blog/ ), and learn moreabout us ( https://www.coursehero.com/about-us/ ) to see what it is like to work with our team.

Benefits & Perks!


Competitive salary and stock options
Full medical coverage (medical, dental, vision)
401(k) program with match
Education Reimbursement
Quarterly team events and outings (Sporting Events, Escape Rooms, Go-Kart Racing, Karaoke, Bowling and much more!)
Free lunches twice a week, on-site cafe discount, plus an endless snack and drink supply
Onsite gym – Pacific Shores Center – Classes – Pool – Spa – Rock Wall - Massages!
Commuter benefits, shuttle service from Redwood City, and cell-phone allowance
Local move benefit to move within 10 miles of our office!
8 hours per quarter paid time for volunteering for a cause of your choice
Front row seat to Master Educator lectures – check out the videos on our LinkedIn Career Page

"
161,Data Engineer,"San Francisco, CA 94107",San Francisco,CA,94107,None Found,None Found,None Found,None Found,None Found,None Found,"***********
About Envoy
***********

Envoy makes workplaces work better. With a focus on the details, we craft beautiful, modern software that elevates the workplace experience. Companies like Google, Tesla, GitHub, Slack, Stripe and Pinterest, have worked with Envoy to welcome over 30 million visitors to more than 10,000 locations around the world. We are proudly backed by Andreessen Horowitz, Menlo Ventures, Initialized Capital, and many others.

Our mission is to challenge the status quo of workplace technology. This idea started at the front desk, where we set a new standard for visitor sign-in. Now, we're looking around the office—to the mailroom, meeting rooms and beyond—and asking how can we make this better, too? We envision a world where technology is woven through our workplaces, all of it working together to make our time there delightful.

If this world sounds exciting, we'd love for you to help us build the Office OS.

--------
You will
--------


Build out our data transformation pipeline (we use Airflow, dbt, Spark, Redshift) that powers all our analysis, research, and reporting, and ensure it is reliable, fast, and scalable
Build full-stack data products that power key tools and apps (Flask, React, Vega)
Drive key decisions on standards, frameworks, and tools across the data team
Serve as the domain expert on the technologies that power our data stack, and ensure that best practices are always being followed in both code and process
Learn and implement innovative technologies to continuously improve the value that our data platform can deliver
Explore data from all sides of the business (Sales, Success, Marketing, Finance) and drive insights through modeling (marketing attribution, churn prediction, account health, lead scoring, etc.)

--------------------
You have
--------------------


5+ years of software engineering experience or coding experience
BS / MS in Computer Science or a related technical field
Mastery of SQL
Experience with Airflow (or another orchestration framework)
Experience with dbt, ideally worked on a large dbt project
Experience with Python

------------
Bonus points
------------


Have built ETL pipelines from scratch and scaled them up to process high volumes of data
Have built and maintained APIs
Have built real-time data pipelines using streaming technologies (Kafka, Kinesis)
Deployed ML models in a production environment
Experience with AWS data services (Redshift, EMR, S3, etc.)

If this kind of work sounds interesting, we'd love to hear from you! We're open to all backgrounds and levels of experience, and we believe that great people can always find a place. People do their best work when they can be themselves, so we value uniqueness. We never discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status or disability status."
162,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Komodo Health is addressing the global burden of disease through the world's most actionable healthcare map. Our solutions drive a more transparent, efficient and productive healthcare ecosystem. We value our culture of encouraging growth, collaboration, and constructive debate as well as delivering innovative solutions that ""wow"" our customers.

The Data Engineering (DE) team is looking for a Senior Data Engineer to help build out our data pipelines and data infrastructure, helping us develop data processing that is scalable, reliable and automated. The ideal candidate will be comfortable going from the whiteboard design of a system all the way to the nuances of code implementation.

This is an opportunity to join a growing company, and be a part of a team of folks accomplished in diverse Engineering disciplines; focused on using the best of what lies at the forefront of technology and skills to address complex, real-world problems in the Healthcare and Life Science space. Some of the tools we use are: Python, Spark, AWS, Kubernetes, Docker, Postgres, Git, Airflow and Flask.

RESPONSIBILITIES
----------------


Design, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources.
Create automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines.
Evaluate new technologies for continuous improvements in Data Engineering.
Collaborate closely with the product team to build out new data features.
Work with the data scientists to implement descriptive, forecasting and predictive algorithms and models using latest technologies.

REQUIREMENTS
------------


MS in Computer Science or related degrees.
5+ years of relevant experience with data engineering or similar experience.
Experience with...
Building and deploying large-scale data processing pipelines.
Python, Scala or Java coding experience, proficiency with at least one of them is required.
Workflow & pipeline systems, like Airflow, Luigi, etc.
Distributed systems for data processing tools such as Spark, Hadoop, Kubernetes, etc.
SQL and Relational Databases, like PostgreSQL.
Continuous integration and automation tools and processes, like Jenkins.
You've been through the planning, launching and refactoring phases of code you wrote.
A serious passion for data.
History of excellence and responsibility in previous engineering positions.
Ability to work as part of a collaborative team in a fast-paced environment.
Sincere interest in working at a startup and scaling with the company as we grow.

NICE TO HAVE


Experience or interest in the life sciences industry.
Basic knowledge (college level or hands on) of machine learning and statistics.
Experience with graph databases.

BENEFITS
--------


Competitive salary and equity compensation
Full Medical, Dental, and Vision benefits
401k plan with employer match
Flexible hours and a ""use as you need"" vacation policy
Opportunities to attend industry conferences and events
Great office location(s) in SF/SOMA and NY/FLAT IRON

"
163,Data Engineer,"Emeryville, CA",Emeryville,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Engineer role will require partnering with key internal (intra-team and cross-disciplinary) stakeholders to implement automated reporting deliverables and work with our broader team to contribute to and evolve our data automation, visualization, warehousing and reporting services. The ideal candidate will be able to help support the team in development work by unifying multiple data sources and evolving our process to create and maintain reporting and analytical tools. Additionally, they will be able to act as subject matter expert and be a partner to both our internal and client teams.
They will work with the broader Analytics team to map data sources, create a data lake, develop SQL reporting views to consolidate dimensions and metrics across tables, link the reporting view to visualization tools (Datorama), and customize the reporting templates to meet the client teams’ needs. Additionally, this role will require the Data Engineer to maintain and update existing reports based on requests and changing technologies.
What you’ll be responsible for:
Ability to develop, build, and maintain data architecture, including data warehouse/data lake, that leverages existing automation
Work closely with internal insights, reporting & data visualization teams to drive data accuracy, reporting advancement and data warehousing efforts
Own key client data visualization builds and overall maintenance
Take lead of data and technology-centric projects and contribute to the evolution of processes, working with the team as well as independently with the goal of driving efficiency
Drive implementation, validation and ongoing support of data visualization tools and recurring reports
Ability to build custom data models for data mining and reporting
Serve the client teams as an expert on internal data visualization best practices
Grow credibility with internal teams by exceeding expectations and delivering valuable tools
Develop and refine documentation around our tech platforms and data processes
Contribute to the company's knowledge base by being the forward-thinking data technology expert with a mind for strategic growth
Manage, mentor and coach broader analytics team on technical skills and platform work
Drive advancements in our data visualization platforms including accuracy, efficiency and aesthetics - The Data Engineer will assist in onboarding new client reporting by following internal processes, maintain and update existing reports based on requests and changing technologies.
You’ll need to have:
A Bachelor's degree or equivalent years’ experience required (Computer Science, Computer Engineering, Data Analysis/Marketing, or related field); Master’s degree preferred
Expertise with data visualization tools such as Datorama, Tableau, Excel, Google Sheets, Google Data Studio required
Advanced/Intermediate experience developing custom SQL
Experience in developing data warehouses/data lakes from scratch"
164,AWS Big Data Engineer,"San Rafael, CA",San Rafael,CA,None Found,None Found,"
Hadoop, HDFS, Hive, Python, REST API/ SOAP API, Spark2, Oozie WFs
Advanced level expertise in any one modern software development language, for example, Java, Python, Ruby, Node.js, etc.
Keen understanding of big data and parallelization accompanied with a stellar record of delivery
Experience working within the AWS Big Data/Hadoop Ecosystem (EMR is preferred), AWS Glue
Experience with on-premises to cloud migrations including re-hosting, re-platforming and re-factoring
Experience with orchestration template technologies such as AWS CloudFormation",None Found,"
Design, build and maintain Big Data workflows/pipelines to process billions of records into and out of our data lake
Fine tune application performance
Troubleshoot and resolve data processing issues
Engage in application design and data modeling discussions
Participate in developing and enforcing data security policies
Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Hive, Hadoop, Spark, Python, Elastic Search, Storm, Kafka, Oozie WFs etc. in both an on premise and cloud deployment model to solve large scale processing problems
Provide technical leadership in the area of big data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics, user access, and security
Proficiency in Amazon AWS big data technologies including S3, RDS, RedShift, Elasticsearch, Lambda, AWS Glue
Conduct code reviews in accordance with team processes and standards
Works on a geographically dispersed team embracing Agile and DevOps strategies for themselves and others while driving adoption to enable greater technology and business value",None Found,None Found,"As Autodesk continues to transform as a Subscription Offering Product company, we are making investments to change business processes and modernize our Sales, Marketing and Support systems. As part of this initiative, Autodesk is seeking a highly talented and experienced Principal Big Data Engineer, to join advanced engineering 'Marketing Automation Platform'.

As a Principal Big Data Engineer, you will help ensure the satisfaction of our consumers of the data by analyzing and translating business and technical requirements into a cloud architectural blueprint that outlines solutions to achieve organizational objectives.

As a technical leader within the Marketing Engineering group, you will take ownership of the Marketing architecture for processing and analyzing data across the Platform. Act as subject matter expert on AWS and other cloud providers, responsible for end to end cloud architectures, cloud migrations, SaaS and multi-cloud providers integrations.

The ideal candidate will implement a variety of solutions to ingest data into, process data within, and expose data from a Data Lake that enables our data analysts and scientists to explore data in the ad-hoc manner as well as quickly implement data-driven models that generate accurate insights in an automated fashion.

Responsibilities
Design, build and maintain Big Data workflows/pipelines to process billions of records into and out of our data lake
Fine tune application performance
Troubleshoot and resolve data processing issues
Engage in application design and data modeling discussions
Participate in developing and enforcing data security policies
Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Hive, Hadoop, Spark, Python, Elastic Search, Storm, Kafka, Oozie WFs etc. in both an on premise and cloud deployment model to solve large scale processing problems
Provide technical leadership in the area of big data systems development including data ingestion, data curation, data storage, high-throughput data processing, analytics, user access, and security
Proficiency in Amazon AWS big data technologies including S3, RDS, RedShift, Elasticsearch, Lambda, AWS Glue
Conduct code reviews in accordance with team processes and standards
Works on a geographically dispersed team embracing Agile and DevOps strategies for themselves and others while driving adoption to enable greater technology and business value

Minimum Qualifications
Hadoop, HDFS, Hive, Python, REST API/ SOAP API, Spark2, Oozie WFs
Advanced level expertise in any one modern software development language, for example, Java, Python, Ruby, Node.js, etc.
Keen understanding of big data and parallelization accompanied with a stellar record of delivery
Experience working within the AWS Big Data/Hadoop Ecosystem (EMR is preferred), AWS Glue
Experience with on-premises to cloud migrations including re-hosting, re-platforming and re-factoring
Experience with orchestration template technologies such as AWS CloudFormation

Preferred Qualifications
Bachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience
5 + years experience working within the AWS Big Data/Hadoop Ecosystem (EMR is preferred)
Managing traditional enterprise platforms for application runtimes, integration middleware and relational databases

About Autodesk
With Autodesk software, you have the power to Make Anything. The future of making is here, bringing with it radical changes in the way things are designed, made, and used. It's disrupting every industry: architecture, engineering, and construction; manufacturing; and media and entertainment. With the right knowledge and tools, this disruption is your opportunity. Our software is used by everyone - from design professionals, engineers and architects to digital scientists, students and hobbyists. We constantly explore new ways to integrate all dimensions of diversity across our employees, customers, partners, and communities. Our ultimate goal is to expand opportunities for anyone to imagine, design, and make a better world.

At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law.
To all recruitment agencies: Autodesk does not accept unsolicited headhunter and agency resumes. Autodesk will not pay fees to any third-party agency or company that does not have a signed agreement with Autodesk, Inc."
165,Senior Software Engineer - Data,"San Francisco, CA 94103",San Francisco,CA,94103,None Found,None Found,None Found,None Found,None Found,None Found,"As a Senior Data Engineer, you'll play an important role as a technical leader responsible for designing, building and improving frameworks and platforms that empower other teams at Clever to manage and process the large amounts of data from multiple internal and external sources and extract meaningful insights to inform both our users' decisions and our product strategy. You will have the opportunity to inspire and evolve the technology stack and processes behind Clever's data analysis.

A Day in the Life

You will be responsible for designing and building reusable frameworks, architecture and practices for data collection, aggregation, visualization, and analysis across the company, with an emphasis on creating self-service tools for engineers, data scientists and analysts. You will be working on projects like:


Building and scaling ETL pipeline architecture for collecting and aggregating business and product data from multiple sources.
Managing Clever's data warehouse and optimizing for scale as we grow our user base and product surface area.
Recommending and implementing new tools and integrations that enable Clever teams to generate data insights more efficiently.
Working closely with product managers and data scientists to define and implement data analytics best practices.

Things We're Looking For

You may be a fit for this role if you have:

5+ years of relevant experience (3+ years of data engineering experience preferred, 2+ years of software engineering).
SQL database management and query optimization expertise.
Experience with modern data science/engineering tools, programming languages and practices (e.g. Jupyter Notebooks, Python).
Optional experience with AWS managed infrastructure is a plus (e.g. AWS Lambda, Kinesis, Redshift).

What Clever offers


A chance to revolutionize the way schools use technology
Competitive salary and significant equity in well-funded, high-growth company
Catered lunch daily
Generous vacation and holidays
Top-notch healthcare, vision, & dental coverage
Unlimited amazon.com credits for learning
An incredible team of fun, bright coworkers

"
166,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,"
Design, implement and build ETL pipelines that deliver data with measurable quality
Understand the data needs, including schema, frequency of the data (either streaming, daily, hourly etc) by interacting with machine learning engineers and data scientists
Develop Scala (or equivalent) scripts to generate enriched tables or logs using distributed scalable platforms (Databricks)
Interact with client engineers to discuss granularity of details of the request
",None Found,None Found,"Join Tubi, a premium streaming service leading the charge in making entertainment accessible to all. With thousands of movies and TV shows from studios like Paramount, Lionsgate and MGM, we provide the largest catalog to millions of viewers, all for free. We're looking for great people who are creative thinkers, self-motivators and impact-makers with a passion to help us shape the future of TV.

About the role:
In this Data Engineering role, you will work closely with product engineering teams and data scientists to tackle problems in personalization, content discovery, search, advertising and content production. Some of the challenges you will take on will include streamlining feature engineering, helping data scientists take machine learning models in production and enabling data science research and analysis even amongst non-scientists in the company.

Responsibilities:

Design, implement and build ETL pipelines that deliver data with measurable quality
Understand the data needs, including schema, frequency of the data (either streaming, daily, hourly etc) by interacting with machine learning engineers and data scientists
Develop Scala (or equivalent) scripts to generate enriched tables or logs using distributed scalable platforms (Databricks)
Interact with client engineers to discuss granularity of details of the request

Your background:

2+ years of a proven track record in a data engineering or related role.
Ability to take ownership of the design, implementation, and maintenance of scalable end to end data pipelines.
Ability to quickly evaluate and make trade-off decisions on adopting emerging technologies.
Strong knowledge of Scala and Apache Spark 2.0+.
Building real-time data pipelines using Redshift, S3, Kinesis, Spark structured streaming, Akka streams, and similar stacks on leading cloud platforms.
Experience with DAG workflow schedulers like Airflow.
A passion for shipping production quality code with good test coverage using testing frameworks for testing Spark code.
Excellent problem solving skills.
Ability to interpret and analyze data is a must. Consequently, mathematical inclination is a major plus.

Benefits:

A tight-knit team of passionate people and a tech-first business
Autonomy and end-to-end ownership
In addition to VC funding, Tubi generates healthy revenue
We offer very competitive pay, equity, full medical, dental & vision benefits, catered lunch and dinner, and gym subsidies
Work with other fellow AVOD enthusiasts
Opportunity for internal growth
Open PTO
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. We are an E-Verify company.

"
167,"Data Engineer (SQL, Python)","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Title: Data Engineer
Location: San Francisco, CA

Terms: Full-time, Direct hire

About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.

What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.

As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do

Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:

Cloud
Analytics
Digitization
Infrastructure
Security

Job Description
Strong data engineer able to:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements,
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources,
Work with stakeholders including Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs,
Experience building and optimizing data pipelines, architectures and data sets,
A successful history of manipulating, processing and extracting value from large disconnected datasets.
A plus:
Experience in creating reports and dashboards in Tableau.
Required

Proficiency and fluency with SQL and Python

Good to have: Presto, Tableau, Cloud technologies. Understanding of Big Data Technologies. Scuba
We are Growing Rapidly: 2018 Highlights
Trianz is growing above the average of the professional services industry. Here are some highlights.

Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.

Won the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.

Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.

Featured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.

Achieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.

Talk to us, Join us & Develop into Leaders
Come join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.
 We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
 Equal Opportunity Employer
Trianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law)."
168,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"GSN Games is looking for a Data Engineer to join our team in San Francisco!

About the Data Team:
The GSN Data team provides the data, analytics, and algorithms necessary to make GSN Games a fun and rich experience for players. We are building a next generation data platform and analytics suite to support the nearly one billion records a day coming from our portfolio of gaming apps, databases, supporting web services, third party APIs, and everything in between.

What You’ll Do:
We are building a next generation “big data” analytics platform for social gaming. Our BI platform contains trillions of data points and we ingest over 1 billion records a day from various internal and external data sources including databases, mobile devices, web services and 3rd party APIs. You will be responsible for building highly scalable multithreaded applications to gather, store and maintain massive datasets. The applications you build will help power a growing library of dashboards, reports, and interactive data tools enabling critical decision making at all levels of the company including the management team.

About You:

3-5 years experience as a Data Engineer in a large scale environment
Exceptional experience and proficiency with Python
Strong proficiency in SQL, including advanced queries and performance tuning (Vertica a plus)
Strong working knowledge of ETL technologies such as Airflow
Experience with distributed computing (e.g. Apache Beam / Spark)
Experience with object oriented programming in Java/C++/C#
Experience developing software against 3rd party APIs / RESTful web services and working with vendors to identify and correct issues and drive enhancements
Experience at implementing the full life cycle of massive datasets including ETL, cleaning, data analysis and deployment in the Cloud (AWS or GCP)
Strong analytical ability and debugging skills

Bonus Points!

Familiarity with machine learning technologies (in python or scikit-learn)
Familiarity with BigQuery
Previous experience with Tableau
Experience in the gaming industry

"
169,Data Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,"
4-6 years of hands-on database skills with such technologies as PostgresSQL/RedShift, Glue, or equivalent
3+ years experience with Spark/pySpark, Pandas, numpy, pysql
4-6 years coding in Python with solid understanding of Python best practices (PEP8)
2+ years experience with cloud services like AWS/GCP
Comfortable supporting analytics tools such as Tableau, PowerBI, etc
Excellent detail-oriented, problem solving skills and the ability to quickly learn and apply new concepts, principles and solutions
Must have excellent communication skills (verbal and written)
Ability to balance competing priorities in a very dynamic and fast-paced environment
",None Found,None Found,None Found,"About the Job
As a rapidly growing start-up, we are implementing the tools and processes we use to run the company and our IoT product base. As a Data Engineer, you will roll up your sleeves to integrate and expand the core systems that run our platform.

About You
Candidates need to have 5+ years of hands-on experience, with proven results in designing, building, integrating and rolling out impactful data infrastructure in support of a growing company.

What You’ll Do

Design and support the application integration between various applications and systems.
Provide analytical rigor to thoroughly analyze business requirements and translate the results into good technical designs.
Design and develop systems automation with future expansion in mind.
Establish the documentation of integration, develops, and maintains technical specification documentation for all processes.

Required Skills and Experience:

4-6 years of hands-on database skills with such technologies as PostgresSQL/RedShift, Glue, or equivalent
3+ years experience with Spark/pySpark, Pandas, numpy, pysql
4-6 years coding in Python with solid understanding of Python best practices (PEP8)
2+ years experience with cloud services like AWS/GCP
Comfortable supporting analytics tools such as Tableau, PowerBI, etc
Excellent detail-oriented, problem solving skills and the ability to quickly learn and apply new concepts, principles and solutions
Must have excellent communication skills (verbal and written)
Ability to balance competing priorities in a very dynamic and fast-paced environment

Preferred Skills:

Experience with GIS services and datasets
Implementation and integration of ticketing and workflow systems like ZenDesk, Jira or PagerDuty.
Implementation and integration of CRM systems like Salesforce, HubSpot, or Copper
Experience with data aggregation services such as Snowflake, Segment, or equivalent
Experience with developing user authentication and authorization solutions and knowledge of security compliance is a plus
Knowledge in Transportation or Logistics domain.

"
170,"Senior Data Engineer, Analytics Instrumentation","San Francisco, CA 94105",San Francisco,CA,94105,None Found,"5 - 8+ years’ experience in global software development and deployment
Experience setting up big data platform service providers like Qubole
Experience with AWS Services, which includes setting up S3 policies, IAM Roles, Enabling Cloudwatch Alarms, understanding AWS cost etc.
Experience setting up Big Data Technologies like, HDFS, MapReduce, Hbase, Pig, Hive, Sqoop, Oozie, Spark, Cloudera manager, Kafka & Splunk
Experience setting up schedulers like Oozie and Airflow
Experience with building CI/CT/CD pipelines for AWS Services like Lambda, Glue, ECS and Firehoses
Experience with AWS including S3, EC2, Lambda, Kinesis, Firehose, Step functions, Cloudwatch, Cloud formation templates
Experience with Programming Languages - Java, Python, Linux Shell Scripts, PL/SQL
Experience with Databases: Databases: Oracle, MySQL, SQL Server, PostgreSQL & DynamoDb
Experience with Web Services like REST & SOAP
Bachelor's Degree in Computer Science or equivalent work experience
Excellent analytical and problem solving abilities, demonstrated ability to think outside the box
Scrum experience",None Found,"
Consult with project stakeholders to understand business problems and design solutions to meet those problems
Understand, utilize and influence the data collection architecture
Develop and optimize data workflows and aggregations
Develop and optimize internal data validation tools and services
Work with remote teams to support and ensure project deliveries
Work with analysts to ensure data collection and usage follows business requirements
Work with Product Managers and stakeholders to ensure data integrity between systems
Work directly with developers, architects and business analysts to deploy and test the next generation of data collection that will transform Autodesk
Critically evaluate information gathered from multiple sources and reconcile conflicts
Dissect high-level data into details and communicate information in a manner understood by relevant audiences
Test solutions and ensure they meet business requirements and are ""fit-for-purpose,"" present and validate solution with user",None Found,None Found,"Autodesk is becoming a cloud company, and a cloud company is a data company. The Web and Product Analytics team is laying the data foundation for Autodesk, used to enable data insights, data science and data products. You will be joining a team passionate about data and delivering the best tools and infrastructure for data collection, processing and aggregation. As an Senior Data Engineer, you will get an opportunity to work with cutting edge technologies in AWS and the Big Data World.

Responsibilities

Consult with project stakeholders to understand business problems and design solutions to meet those problems
Understand, utilize and influence the data collection architecture
Develop and optimize data workflows and aggregations
Develop and optimize internal data validation tools and services
Work with remote teams to support and ensure project deliveries
Work with analysts to ensure data collection and usage follows business requirements
Work with Product Managers and stakeholders to ensure data integrity between systems
Work directly with developers, architects and business analysts to deploy and test the next generation of data collection that will transform Autodesk
Critically evaluate information gathered from multiple sources and reconcile conflicts
Dissect high-level data into details and communicate information in a manner understood by relevant audiences
Test solutions and ensure they meet business requirements and are ""fit-for-purpose,"" present and validate solution with user

Minimum Qualifications
5 - 8+ years’ experience in global software development and deployment
Experience setting up big data platform service providers like Qubole
Experience with AWS Services, which includes setting up S3 policies, IAM Roles, Enabling Cloudwatch Alarms, understanding AWS cost etc.
Experience setting up Big Data Technologies like, HDFS, MapReduce, Hbase, Pig, Hive, Sqoop, Oozie, Spark, Cloudera manager, Kafka & Splunk
Experience setting up schedulers like Oozie and Airflow
Experience with building CI/CT/CD pipelines for AWS Services like Lambda, Glue, ECS and Firehoses
Experience with AWS including S3, EC2, Lambda, Kinesis, Firehose, Step functions, Cloudwatch, Cloud formation templates
Experience with Programming Languages - Java, Python, Linux Shell Scripts, PL/SQL
Experience with Databases: Databases: Oracle, MySQL, SQL Server, PostgreSQL & DynamoDb
Experience with Web Services like REST & SOAP
Bachelor's Degree in Computer Science or equivalent work experience
Excellent analytical and problem solving abilities, demonstrated ability to think outside the box
Scrum experience

About Autodesk
With Autodesk software, you have the power to Make Anything. The future of making is here, bringing with it radical changes in the way things are designed, made, and used. It’s disrupting every industry: architecture, engineering, and construction; manufacturing; and media and entertainment. With the right knowledge and tools, this disruption is your opportunity. Our software is used by everyone - from design professionals, engineers and architects to digital artists, students and hobbyists. We constantly explore new ways to integrate all dimensions of diversity across our employees, customers, partners, and communities. Our ultimate goal is to expand opportunities for anyone to imagine, design, and make a better world.

At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law.
To all recruitment agencies: Autodesk does not accept unsolicited headhunter and agency resumes. Autodesk will not pay fees to any third-party agency or company that does not have a signed agreement with Autodesk, Inc."
171,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,"
5 years of software development experience
Experience with building scalable and reliable data pipelines using Big Data engine technologies like Spark, AWS EMR, Redshift, etc.
Experience with scalable data integration technologies like ETL or Data Virtualization
Competence with SQL
","
Lead projects, including working across organizational boundaries to drive work to completion.
Design and develop framework to automate ingestion and integration of structured data from a wide variety of enterprise data sources, at scale
Design and develop data pipeline components and integrate them with the Formation Platform
Design data quality monitoring and automated data cleaning
",None Found,None Found,"Formation is seeking a Senior Data Software Engineer to help develop our flagship product. We are using reinforcement learning to solve interesting problems at scale. You will be contributing to the development and maintenance of complex data pipelines as well as the Formation Data Platform.
Key Responsibilities:

Lead projects, including working across organizational boundaries to drive work to completion.
Design and develop framework to automate ingestion and integration of structured data from a wide variety of enterprise data sources, at scale
Design and develop data pipeline components and integrate them with the Formation Platform
Design data quality monitoring and automated data cleaning
Skills and Experience:

5 years of software development experience
Experience with building scalable and reliable data pipelines using Big Data engine technologies like Spark, AWS EMR, Redshift, etc.
Experience with scalable data integration technologies like ETL or Data Virtualization
Competence with SQL
Bonus Points:

SaaS experience
Experience using Scala, or willingness to learn Scala and demonstrated competence with functional programming
Experience with cloud technologies such as AWS or Google Cloud
About Formation

Formation distills complex customer data into uniquely tailored experiences; we orchestrate physical and digital exchanges into one seamless journey. Our business is building lasting, trusted relationships between people and brands—and making it look easy.

We're already reaching millions of people a day, and we're just getting started. Our founding leadership is equal parts business, design, and engineering—because we believe differing perspectives + passionate discourse achieve the greatest outcomes. We are collectively talented, but also humble. We give our whole selves. We love learning new things.

Formation is committed to inclusion and diversity and is an equal opportunity employer. All applicants will receive consideration without regard to race, color, religion, gender, gender identity, sexual orientation, national origin, disability, or veteran status.

If you're up for the challenge of a lifetime, we're looking for outstanding talent to join our team."
172,Data Engineer,"Oakland, CA",Oakland,CA,None Found,None Found,None Found,"
BS / MS in Mathematics, Computer Science or an Engineering discipline from a top university.
3+ years of experience in data science and/or statistics, using languages such as Python and C++.
2+ years of experience in SQL and relational databases (PostgreSQL preferred), and good understanding of relational concepts.
Expertise with GIS analysis and technology
Experience with scalable cloud-based solutions (e.g. AWS, Redshift, S3, Spark, GCP)
Experience using libraries such as shapely, pandas, numpy, PySAL Intermediate-level experience working with spatial databases, specifically postgreSQL databases and postGIS functionality; also interested in experience with similar technologies including RDBMS/ArcSDE/geodatabases.","Create and maintain automated data flows
Identify, design, and implement internal process improvements in automating manual processes.
Query, analyze and transform large spatial datasets
Find and build real-world data sets for training various models, doing quality checks and anomaly detection.
Create data validation, analytics, visualization and monitoring tools for our Data Science team that assist them in building and optimizing our data flows.

",None Found,None Found,"Our Mission:
To eliminate congestion, improve traffic flow and give back time to everyone.

Role Mission:
Your main mission is to build the data infrastructure that enables us to collect, store and efficiently process traffic and mapping data for every road and every road user in the US. You will be responsible for spatial data analytics, building automation tools, as well as creating processes for data monitoring and anomaly detection. Scalable data pipelines provide the fuel that enable our Data Science team to accurately model traffic patterns allowing us to optimize traffic signals. Your work will impact millions of people each day: with better traffic signal timing we will lower travel times by up to 25%, reduce emissions by up to 22% and enable everyone to spend more time on the things that are most important to them and with the people who are most important to them.

Key Responsibilities:
Create and maintain automated data flows
Identify, design, and implement internal process improvements in automating manual processes.
Query, analyze and transform large spatial datasets
Find and build real-world data sets for training various models, doing quality checks and anomaly detection.
Create data validation, analytics, visualization and monitoring tools for our Data Science team that assist them in building and optimizing our data flows.

Required Skills:
BS / MS in Mathematics, Computer Science or an Engineering discipline from a top university.
3+ years of experience in data science and/or statistics, using languages such as Python and C++.
2+ years of experience in SQL and relational databases (PostgreSQL preferred), and good understanding of relational concepts.
Expertise with GIS analysis and technology
Experience with scalable cloud-based solutions (e.g. AWS, Redshift, S3, Spark, GCP)
Experience using libraries such as shapely, pandas, numpy, PySAL Intermediate-level experience working with spatial databases, specifically postgreSQL databases and postGIS functionality; also interested in experience with similar technologies including RDBMS/ArcSDE/geodatabases.

Bonus:
Experience building high performance batch and real-time data processing pipelines (MapReduce, Hadoop, HBase/Cassandra, Beam, Spark, Samza etc)
Experience building and managing large-scale geospatial data systems (mapping, navigation, GIS, routing, fleet management)


To apply for this role please email us here. Please check out our application tips prior to applying."
173,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Us:
Live experiences help people cross today’s digital divide and focus on what truly connects us - the here, the now, the once-in-a-lifetime moment that brings us together. To fulfill Gametime’s vision to unite the world through shared experiences, we deliver fans an extraordinary experience for discovering and purchasing last-minute tickets to live events.

With platforms on iOS, Android, mobile web and desktop supporting more than 100,000 events across the US and Canada, we are reimagining the event ticket experience in a mobile-first world.

The Role:
As a Senior Data Engineer at Gametime, you will have the opportunity to work not only within the data team but across the entire business. You’ll be at the forefront of building scalable systems for product, marketing, engineering, finance, and customer support to handle the high volume of data we collect as one of the fastest-growing startups in the Bay Area.
What you'll do/own:
Design and implement scalable data pipelines and data storage on AWS using Kinesis, Redshift, S3, and a Spark based streaming architecture
Create scalable and low latency code for data products using MongoDB, Redis, Elasticsearch or similar
Scale and maintain our Analytics Databases to power our dashboards
Collaborate with our data scientists to productionize their models
Implement Comprehensive Testing and Continuous Integration frameworks for schema, data, and functional processes/pipelines
Our ideal candidate has:
BS in Computer Science or equivalent experience or field
At least 4 years experience using Redshift or a similar Data Warehouse
At least 4 years experience programming, preferably with Python or Go
Hands-on experience with SQL, ETL, Data Warehousing and Data Orchestration
Familiarity with scheduling frameworks, preferably Airflow
Familiarity with real-time/batch distributed systems like Kinesis, Kafka, Spark, professional experience with Redis and Elastic search
Familiarity with business intelligence/analytics tools like Tableau or Periscope Data
What we can offer you:
Competitive SF Bay Area total comp package
Fully paid medical, dental and vision insurance
Monthly commuter, cell phone, and Friday lunch stipends
Monthly credits for events on Gametime ($1,200/yr)
New equipment and multi-monitor setup provided
Company-paid lunches (M-Thu)
Unlimited snacks, drinks
Company swag
Company happy-hours, events and outings
Wellness programs
Tenure recognition
Apply for this job"
174,Principal Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Mission

As a Principal Data Engineer, you will have the opportunity to shape the future big data solution landscape for leading Fortune 500 organizations. This position is a senior level customer-facing role that needs deep expertise in Apache Spark along with breadth of big data solution architecture experience. On a weekly basis, you will guide customers through architecture, design and implementation activities while strategically aligning their technical roadmap for expanding the usage of the Databricks platform.

At Databricks we work on some of the most complex distributed processing systems and our customers challenge us with interesting new big data and AI requirements. ""Teamwork makes the dream work"" is a fundamental growth value at Databricks, so this role will work internally within a multi-functional team including Account Executives and Customer Success Engineers, all while having a direct channel to the original creators of Apache Spark in Engineering.

Outcomes


Guide strategic customers as they design and implement Big Data projects ranging from transformations to data science and AI through on-site and remote engagements
Provide technical leadership in a post-sales capacity for customers to support successful understanding, evaluation and adoption of Databricks
Identify and drive new initiatives that enable customers to succeed in turning their data into value
Build reference architectures, frameworks, solutions, how-to's, and prototypes for customers
Provide escalated level of support for critical customer operational issues
Architect, implement, and/or validate migration of workloads from 3rd party databases and data platforms to Apache Spark.
Plan and coordinate with Account Executives, Customer Success Engineers and Solution Architects for expanding the use of Databricks platform within strategic enterprise customers on a weekly basis

Competencies


Deep hands-on technical expertise with Apache Spark
Minimum 5+ years of design and implementation experience in Big Data technologies (Hadoop ecosystem, Kafka, NoSQL databases)
3-5 years in customer-facing pre-sales, technical architecture or consulting role
Open to travel up to 30% per month
Familiarity with data architecture patterns (data warehouse, data lake, streaming, Lambda/Kappa architecture)
Outstanding verbal and written communication skills; Comfortable with talking up and down the IT chain of command including directors, managers, architects and developers
Passionate about learning new technologies and making customers successful
Excellent presentation and whiteboarding skills
Comfortable coding Python, Scala or Java
Familiarity with AWS/EC2 cloud deployment models (Public vs. VPC)

Preferred Competencies


BS / MS in Computer Science or equivalent
Proven track record within a data platform software vendor in a consulting/services function
Experience working as or with Data Scientists
Experienced with performance tuning, troubleshooting, and debugging Spark and/or other big data solutions
Familiarity with database and analytics technologies in the industry including Data Warehousing/ETL, Relational Databases, or MPP
Located in any of the following metro areas: San Francisco, New York, Boston, or DC

Benefits


Medical, dental, vision
401k Retirement Plan
Unlimited Paid Time Off
Catered lunch (everyday), snacks, and drinks
Gym reimbursement
Employee referral bonus program
Awesome coworkers
Maternity and paternity plans

About Databricks

Databricks' mission is to accelerate innovation for its customers by unifying Data Science, Engineering and Business. Founded by the original creators of Apache Spark™, Databricks provides a Unified Analytics Platform for data science teams to collaborate with data engineering and lines of business to build data products. Users achieve faster time-to-value with Databricks by creating analytic workflows that go from ETL and interactive exploration to production. The company also makes it easier for its users to focus on their data by providing a fully managed, scalable, and secure cloud infrastructure that reduces operational complexity and total cost of ownership. Databricks, venture-backed by Andreessen Horowitz, NEA and Battery Ventures, among others, has a global customer base that includes Salesforce, Viacom, Shell, and HP. For more information, visit www.databricks.com.

Apache, Apache Spark and Spark are trademarks of theApache Software Foundation ( http://www.apache.org/ )."
175,Senior Big Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent
4+ years of hands-on experience in Java and Spring frame work
Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment
4+ years of hands-on experience in building data pipeline with Java and Spark
4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation
2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL
Experience working in an Agile/Scrum environment
Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders
Proficient understanding of distributed computing principles
Strong written and verbal communications",None Found,"
Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications
Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus
Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses
Design, build and launch new data extraction, transformation and loading processes in production
Create new systems and tools to enable the customer to consume and understand data faster
Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process)
Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems
Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business
Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale
Study data, identify patterns, make sense out of it and convert it to algorithms
Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions
Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance",None Found,None Found,"12 Feb 2019
DB Best is a global full-service company, operating at the cutting edge of the latest cloud technologies. DB Best is involved in hundreds of projects with top-tier customers worldwide. Currently, we have several ongoing engagements with a leading global retailer in the beauty industry. We’ve been working with this customer for over 5 years and have established a strong relationship with them.
We have already significantly improved their entire enterprise reporting platform, including ETL processes, enterprise data warehouse, data marts and data visualization components for this customer. Now, our customer is looking for faster and better decision making and advanced analytics. So, they consider leveraging Big Data technologies and they turned to DB Best to help them develop and maintain this new system.
So, we’re looking for an experienced Senior Big Data Engineer in San Francisco, CA to join our star team.
Actually, we invite you to join one of the most skillful teams at DB Best. Currently, our distributed team includes over 60% of senior developers. So, working with this team of industry experts is really a great pleasure.
This huge opportunity is unlike any project that you’ve worked before. Come join the team and let’s discover together how big, big data can be…
Seeking Big Data Superhero
As a Big Data Senior Developer, you will be working with clients to implement leading edge data analytics and cloud solutions across a range of industries. We are looking for software engineers with a strong understanding of the full data development lifecycle, including requirements gathering, solution design, development, and production deployment. The ideal candidate will have solid Spark & Scala development experience with the desire and passion to continue to learn big data technologies.
Responsibilities
The exciting things you will get to do once employed as a Senior Big Data Engineer:
Participate in technical planning & requirements gathering phases including design, coding, testing, troubleshooting, and documenting big data-oriented software applications
Be Responsible for the ingestion, maintenance, improvement, cleaning, and manipulation of data in the business’s operational and analytics databases. The ability to troubleshoot any existent issues will be a big plus
Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our data warehouses
Design, build and launch new data extraction, transformation and loading processes in production
Create new systems and tools to enable the customer to consume and understand data faster
Build, implement and support the data infrastructure; ingest and transform data (ETL/ELT process)
Implements, troubleshoots, and optimizes distributed solutions based on modern big data technologies like Spark, Scala, Storm, and Kafka in both an on premise and cloud deployment model to solve large scale processing problems
Define and build large-scale near real-time streaming data processing pipelines that will enable faster, better, data-informed decision making within the business
Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale
Study data, identify patterns, make sense out of it and convert it to algorithms
Designs and plans BI, and other Visualization Tools capturing and analyzing data from multiple sources to make data-driven decisions, as well as debugs, monitors, and troubleshoots solutions
Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance leading to improvement in overall improvement in data governance across the business, promoting informed decision-making, and ultimately improving overall business performance
Required Qualifications
The experience and qualifications we hope you bring to the Senior Big Data Engineer position:
Bachelor’s Degree with a minimum of 7+ year’s relevant experience or equivalent
4+ years of hands-on experience in Java and Spring frame work
Minimum of 4 years Java/J2EE software development experience in a complex enterprise system environment, building software from conception to production deployment
4+ years of hands-on experience in building data pipeline with Java and Spark
4+ years of hands-on experience with various messaging systems, such as Kafka, Spark data manipulation, pipeline creation
2+ years’ experience with traditional relational databases such as Oracle, SQL Server, PostgreSQL, MySQL
Experience working in an Agile/Scrum environment
Need someone who is a self-starter and team player, capable of working with a team of Architects, Developers, Business/Data Analysts, QA, and client stakeholders
Proficient understanding of distributed computing principles
Strong written and verbal communications
Preferred Qualifications
Hands-on experience working with Business Intelligence and Reporting
Hands-on experience working within AWS, Azure, Google, or other Cloud Platform based on IaaS and PaaS Solutions.
Experience with integration of data from multiple data sources
Hands-on experience with DevOps solutions like: Puppet, AWS CloudFormation, Docker and Microservices
Intelligence & Visualization Space (Tableau, Power BI, Qlik, Cognos, SAP BOBJ, etc.)
Passion for working with open-source technologies as well as commercial platforms
Benefits
As the Microsoft, Amazon, Google, and IBM partner, DB Best provides you with:
Working with the industry-leading partners and customers — you will not be limited with just the one project we covered here
Flexible work hours, up to 32 days off annually
Friendly teams, experienced colleagues, and perfect work equipment
Opportunities for career growth and raising professional skills
Travel opportunities
Certainly, DB Best is a great place to work. Join our top-tier team of engineers, developers, and experts who are continually pushing the boundaries of what is and what can be. DB Best is the perfect environment for you to grow your skills, understand new technologies, and work with some of the world’s most important companies in creating the future of the cloud. We’re always seeking inquiring minds, so send in your resume today."
176,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"EasyPost is a San Francisco based start-up that provides an API to revolutionize the entire shipping process for e-commerce companies. Any online merchant can plug in our state of the art technology into their existing framework to provide a world-class shipping experience for consumers. Today, we help hundreds of leading e-commerce companies ship and track packages. We also have a fulfillment business that offers end to end shipping solutions.

We are looking for a Senior Data Engineers with experience in or who are comfortable in a polyglot environment to join the easypost team. You will be a key member of our small but growing engineering team making important technical decisions that will shape the company's future. If you love to code and work on unique challenges within a collaborative team of developers to build meaningful products, then we'd love to meet you!

About You:

5+ years of professional software development experience
Expert in SQL and deep understanding of relational databases
Strong experience in performance tuning SQL and ETL pipelines in OLTP, OLAP, and Data Warehouse environments
Expert knowledge of data architecture, data modeling, and building data tools for ETL
Experience with modern big data technologies - e.g. Hadoop, Hive, Spark
Integrations experience with REST, HTTP/HTTPS protocols, and 3rd party APIs a plus
Strong desire to work in a fast-paced, start-up environment with multiple releases a day
A passion for working as part of a team - you love connecting and collaborating with others

What We Offer:

Competitive salary and equity
Comprehensive medical, dental, and vision and commuter benefits
Flexible work schedule and paid time off
Collaborative culture with a supportive team
The opportunity to make massive technical contributions at a fast-growing start-up
A great place to work with unlimited growth opportunities

Do you want to come join us? You'd be a critical member of our growing engineering team to solve some of the hardest problems out there. Please send in an application and we will contact you.

EasyPost is proud to be an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
177,Data Engineer - Bioinformatics,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Ancestry:
When you join Ancestry, you join our family tree. Backed by history, science, and technology, we’re creating a new world of connection, innovation, and understanding. Whether it’s reuniting long-lost relatives through DNA or unearthing new family stories from historical records, Ancestry empowers life-changing experiences. With over 20 billion digitized historical records, 100 million family trees, and 15+ million DNA kits sold, Ancestry is bringing the power of personal discovery to people around the world.

The Opportunity:
The DNA Science team’s goal is to deliver the next generation of consumer genomic tests to shed light on human diversity, origins, relationships, and health. As a Scientific Data Engineer, you will help deliver on this mission by using your data engineering and scientific skills to support research and new product development for Ancestry’s direct-to-consumer DNA test. In this critical role, you will work on fast paced multidisciplinary teams to build a scalable data platform used by Ancestry’s R&D team.
What You Will Do
Bring understanding of genomics, product development, and machine learning to build end-to-end cloud-based data ecosystem for Ancestry’s R&D scientific and genealogy data.
Develop tools to locate, manipulate, QC and ensure interoperability of large diverse datasets
Collaborate with engineers, scientists, and stakeholders to define technical and business requirements
Design and own solutions to maintain high quality standards
Interface with data customers to continuously improve data processes
Develop and enforce policies for proper use of sensitive datasets
Collaborate with team members across the company to integrate with existing data sources
Proactively communicate plans and updates to colleagues and stakeholders

Who You Are
4+ years of experience building data systems and managing datasets with Master’s degree in Computer Science, Genetics, Physics, Engineering or other quantitative field, or an equivalent combination of education and experience.
Expertise in several of the following: Python, R, SQL, git, and Hadoop/Spark in a distributed Unix environment, AWS,
Ability to design and implement complex data management systems
Experience with biomedical ontologies and controlled vocabularies (such as Gene Ontology, Human Phenotype Ontology, etc) for metadata management and data integration.
Experience with public databases and tools commonly used in genomics, such as dbGAP, NCBI, EBI, ENSEMBL.
Familiarity with methods used to analyze genomic data, such as ancestry inference, GWAS, NGS, and machine learning toolkits
Self-starter with strong communication skills and sense of ownership to complete projects independently while thriving in a team environment
Able to apply feedback in a professional manner
Scientific or genomics data management experience in pharma, clinical diagnostics, or biotech
Experience designing, building, and supporting complex software and distributed compute systems
Experience working with HPC job schedulers (ex. SLURM, SGE), and public cloud experience

GD-Sponsored
IND1
#LI-HK1

Additional Information:
Ancestry is an Equal Opportunity Employer that makes employment decisions without regard to race, color, religious creed, national origin, ancestry, sex, pregnancy, sexual orientation, gender, gender identity, gender expression, age, mental or physical disability, medical condition, military or veteran status, citizenship, marital status, genetic information, or any other characteristic protected by applicable law. In addition, Ancestry will provide reasonable accommodations for qualified individuals with disabilities.

All job offers are contingent on a background check screen that complies with applicable law. For San Francisco office candidates, pursuant to the San Francisco Fair Chance Ordinance, Ancestry will consider for employment qualified applicants with arrest and conviction records.

Ancestry is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at Ancestry via-email, the Internet or in any form and/or method without a valid written search agreement in place for this position will be deemed the sole property of Ancestry. No fee will be paid in the event the candidate is hired by Ancestry as a result of the referral or through other means"
178,Data Engineer- Python,"San Francisco, CA",San Francisco,CA,None Found,None Found, 5+ years of experience in core JAVA and SQL,None Found,None Found,None Found,None Found,"As a Senior Consultant, you will focus on managing the information supply chain from acquisition to ingestion, storage and the provisioning of data to points of impact by modernizing and enabling new capabilities. Information value is enhanced through enterprise-scale applications that enable visualization, consumption and monetization of both structured and unstructured data. Big data is becoming one of the most important technology trends that has the potential for dramatically changing the way organizations use information to enhance the customer experience and transform their business models.
Work you'll do

Senior Consultants work within an engagement team. Key responsibilities will include:
 Function as integrators between business needs and technology solutions, helping to create technology solutions to meet clients’ business needs.
 Identifying business requirements, requirements management, functional design, prototyping, process design (including scenario design, flow mapping), testing, training, defining support procedures and supporting implementations.

The Team

Analytics & Cognitive

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.


The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.


Analytics & Cognitive will work with our clients to:
 Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
 Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
 Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements


Qualifications

Required:

 5+ years of experience in core JAVA and SQL
 3+ years of experience in Python& Unix Shell Scripting
 3+ years of experience in building scalable and high performance data pipelines using Apache Hadoop, Map Reduce, Pig & Hive
 Experience with bigdata cross platform compatible file formats like Apache Avro & Apache Parquet
 Experience in Apache Spark is a plus
 1+ years of experience with data lake implementations, core modernizations and data ingestion

 1 or more years of hands on experience designing and implementing data ingestion techniques for real time and batch processes for video, voice, weblog, sensor, machine and social media data into Hadoop ecosystems and HDFS clusters.
 2+ years of experience leading workstreams or small teams
 Willingness for weekly client-based travel, up to 80-100% (Monday — Thursday/Friday)
 Bachelor’s Degree or equivalent professional experience

 Preferred:

AWS Certification, Hadoop Certification or Spark Certification
Experience with Cloud using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)
Experience with data integration products like Informatica Power Center Big Data Edition (BDE), IBM BigInsights, Talend etc.
Experience designing and implementing reporting and visualization for unstructured and structured data sets
Experience in designing and implementing scalable, distributed systems leveraging cloud computing technologies like AWS EC2, AWS Elastic Map Reduce and Microsoft Azure
Experience designing and developing data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching
Knowledge of data, master data and metadata related standards, processes and technology
Experience working with multi-Terabyte data sets
Experience with Data Integration on traditional and Hadoop environments
Ability to work independently, manage small engagements or parts of large engagements.
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint).
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment.
Eagerness to mentor junior staff.
An advanced degree in the area of specialization is preferred.

How you’ll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.


Benefits

At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitte’s culture

Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.


Corporate citizenship

Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.


Recruiter tips

We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.

#LI:PTY
#IND:PTY"
179,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,"
Thrives in a fast-paced, startup environment, is adaptable and versatile
3+ years of python
Experience in python data libraries (pandas, luigi, dask, etc)
Solid understanding of data structures and algorithms
Building distributed data systems
",None Found,None Found,None Found,"
San Francisco, CA
Data department

Alice helps small business owners launch and grow. Since launching in May 2017, our goal is to help 6 million owners succeed through our free digital platform,
helloalice.com [http://www.helloalice.com]. Alice serves all entrepreneurs and we wholeheartedly believe that if underserved business owners -- women, people of color, veterans, the LGBTQ+ community, entrepreneurs with disabilities, people in small towns, and immigrants -- are provided better access to resources, they can change the world. Our vision is to be financially successful while making a positive impact on economies and job creation.

Alice is growing a diverse team, and we’re looking for a Data Engineer to help us fulfill our mission of connecting all entrepreneurs to the resources they need to grow and scale their businesses. We’re leading a movement to connect every founder, regardless of geography, capitalization, prior experience, or cultural constraints, to the experts, tools, knowledge, and communities that will propel their companies forward. Alice is a Series A-backed company with teams in Houston, TX and San Francisco, CA. We are helping hundreds of thousands of owners a week in all fifty states. Led by co-founders Elizabeth Gore and Carolyn Rodz, we are hiring self-starters who can build the plane while flying it.

Engineering is key to scaling our mission, and we’re growing our data engineering team to build new features, improve our AI, and better serve entrepreneurs from around the world. We believe in working fast, but smart, and work toward measurable results. As the first member of the data team you will be able to have a huge impact on the technologies, architecture, and tools we use. Data is key to Alice’s success and this position will have big visibility throughout the organization.

Required Skills/Experiences:

Thrives in a fast-paced, startup environment, is adaptable and versatile
3+ years of python
Experience in python data libraries (pandas, luigi, dask, etc)
Solid understanding of data structures and algorithms
Building distributed data systems

Desired Skills / Experiences

AWS and Google Cloud management
Ansible, Terraform or other cloud orchestration libraries
RabbitMQ, Kafka or other queuing systems
gRPC or other RPC libraries
Protobufs
Machine learning and statistics libraries
Scraping websites for relevant information

Our company values are important to us. We thought we would share them with you as someone interested in joining our team.

INNOVATE ALWAYS. Seek out unique perspectives, diverse experiences, and disconnected dots. These are the seeds of big ideas and exceeded expectations, made even better through constant collaboration.
EMBRACE FAILURE. Learn from the inevitable failures that result from innovation, and move forward quickly with a pioneering spirit. Champion the doers, celebrate their contributions to the team, and don't shy away from difficult conversations.
DRIVE THE MISSION FORWARD. Everything we do is through the inclusive lens of helping all business owners launch and grow, regardless of who they are or where they come from. Hold yourself to the highest standards of quality and equality in everything you do.
EVERYONE TAKES OUT THE TRASH. No task is too small for the success of our company or our owners. Always be thoughtful and practice extreme kindness toward our team, partners, and owners.
SIMPLIFY AND COMMIT. Maintain a bias toward efficiency, acting quickly and testing often. Recognize the opportunity cost of every decision, commit to deadlines, be on time, and search for simple, smart solutions.

Alice is an Equal Employment Opportunity employer that will consider all qualified applicants, regardless of race, color, religion, gender, sexual orientation, marital status, gender identity or expression, national origin, genetics, age, disability status, protected veteran status, or any other characteristic protected by applicable law.

www.helloalice.com [http://www.helloalice.com/] // Twitter
[https://twitter.com/HelloAlice] // Facebook
[http://www.facebook.com/aliceconnects] // Instagram
[https://www.instagram.com/helloalice_com/] // LinkedIn
[https://www.linkedin.com/company/25067438/]"
180,Lead Data Engineer,"San Francisco, CA 94107",San Francisco,CA,94107,None Found,None Found,None Found,None Found,None Found,None Found,"ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.

As a Lead Data Engineer, here's what we'll be looking for you to bring:

Hands-on Engineering Leadership
Proven track record of Innovation and expertise in Data Engineering
Tenure in coding, architecting and delivering complex projects
Deep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others
Deep understanding of streaming data architectures and technologies for real-time and low-latency data processing
Deep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies
Understanding of how to architect solutions for data science and analytics such as productionizing machine learning models and collaborating with data scientists
Understanding of agile development methods including: core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Passion for software craftsmanship
A rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..
Strong stakeholder management and interaction experience at different level
There's no typical day or engagement for our Lead Data Engineers. Here’s what you’ll do:

Be the SME. Develop modern data architectural approaches to meet key business objectives and provide end to end data solutions
You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems.
On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.
It could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.
Whatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.
You have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.
You recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.
A few important things to know:
Projects are almost exclusively on customer site, so candidates should be flexible and open to travel.

Candidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.

Not quite ready to apply? Or maybe this isn’t the right role for you? That’s OK, you can stay in touch with AccessThoughtWorks, our learning community (click ""contact me about recruitment opportunities"" to hear about jobs in the future).

It is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment."
181,"Data Engineer - Consultant - San Francisco, CA","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.

We are...

 • The Industry-recognized data and analytics leaders
 • Passionate problem solvers across a broad spectrum of technologies and industries
 • Value seekers for measurable business outcomes
 • Continuous learners through training and education
 • Focused on a work-life balance with an unlimited paid time off policy

Data engineers are challenged with building the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally. Our engagements typically target a variety of use cases across data engineering, data science, data governance, and visualization.
Data engineers deliver value through....
Hands-on, self-directed design and development of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools
Demonstration of technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven insights
Collaboration with team members, business stakeholders and data SMEs to elicit requirements and to develop business metrics and analytical insights
Internal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO
A data engineer’s skills include, but are not limited to...
Bachelor degree AND 3+ years of professional work experience
SQL, SQL, SQL!
Programming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)
Linux / Windows (Command line)
Big Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop, Spark)
Cloud Platforms (AWS, Azure, Google Cloud Platform)
Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)
Data Integration Tools (Ab Initio, DataStage, Informatica, SSIS, Talend)
Databases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)
Markup Languages (JSON, XML, YAML)
Code Management Tools (Git/GitHub, SVN, TFS)
DevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)
Testing / Data Quality (TDD, unit, regression, automation)
Solving complex data and technology problems
Leading technical teams of 2+ consultants
Ability to design components of a larger implementation
Excellent communication to narrate data driven insights and technical approach
Must reside in the San Francisco, CA bay area

If this sounds like you, let’s talk!

We prefer candidates who are open to a national travel model (M-TH weekly); however, we will consider candidates that do not prefer to travel, but they must be willing to commute anywhere within the bay area.

Clarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.


GLDR
#LI-NT1"
182,Data Architect,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
Authority in everything data related. Know exactly how data is collected, analyzed, and delivered. Also familiar with both the technical and user-facing sides of databases.
5+ years work experience as a Data Architect or Data Engineer or similar role
In-depth understanding of database structure principles used for analytics such as star schemas and SCD
Expertise in database performance optimization
Expertise in building processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Experience using the following software/tools:
Expertise in Redshift, Snowflake, or similar cloud databases
Experience with object-oriented/object function scripting languages
Experience with ETL tools: Alooma, Matillion, Fivetran, Talend, etc.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Airflow, Data Factory, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Familiarity with data visualization tools (e.g. Tableau, Looker, Power BI)
Demonstrated analytical skills
Problem-solving attitude
B.S. in Computer Science, Information Systems or another quantitative field",None Found,None Found,None Found,None Found,"Your Opportunity
We are looking for a Data Architect to run and evolve our data practice within the Go-To-Market Data & Analytics Team. We use data to (i) define and measure performance and productivity metrics across Go-To-Market Functions (ii) identify and monitor leading indicators and predictive models, and (iii) deliver insights to Go-To-Market teams to ultimately improve core business processes and outcomes. The team partners with Executives and their teams within Sales, Marketing, Pre-and-Post Technical Sales, Customer Success, and Alliances & Channels. We also partner on multi-functional projects with Product and G&A.

Who You Are:
A creative and forward-thinking data professional
An experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up
You thrive in a fast-paced growth environment
Results and Team oriented
What You'll Do
Design, develop and build database design and architecture to power Go-To-Market analytics products
Build, optimize and maintain conceptual and logical database models
Design data pipeline architecture and ensure successful creation of the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and other technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Create data monitoring models for each product and work with business and analytics teams to create models ahead of new releases
Examine and identify database structural necessities by evaluating operations, applications, and programming.
Monitor the system performance by performing regular tests, fixing performance issues, and integrating new features.
Work with business partners to assist with data-related technical issues and support their data infrastructure needs
Recommend solutions to improve new and existing database systems.
Educate staff members through training and individual support.
Your Qualifications
Authority in everything data related. Know exactly how data is collected, analyzed, and delivered. Also familiar with both the technical and user-facing sides of databases.
5+ years work experience as a Data Architect or Data Engineer or similar role
In-depth understanding of database structure principles used for analytics such as star schemas and SCD
Expertise in database performance optimization
Expertise in building processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Experience using the following software/tools:
Expertise in Redshift, Snowflake, or similar cloud databases
Experience with object-oriented/object function scripting languages
Experience with ETL tools: Alooma, Matillion, Fivetran, Talend, etc.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Airflow, Data Factory, etc.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Familiarity with data visualization tools (e.g. Tableau, Looker, Power BI)
Demonstrated analytical skills
Problem-solving attitude
B.S. in Computer Science, Information Systems or another quantitative field
Please note that visa sponsorship is not available for this position.
Our Office
Our office is in the tech-rich urban center of San Francisco, with easy commute access and a plethora of good eats. We provide competitive compensation, equity and big-company benefits (medical, dental, etc.)—all while maintaining the energy, agility, and fun of a start-up.
About Us
New Relic (NYSE: NEWR) is the industry’s largest and most comprehensive cloud-based instrumentation platform built to create more perfect software. The world’s best software and DevOps teams rely on New Relic to move faster, make better decisions and create best-in-class digital experiences. If you run software, you need to run New Relic. We’re proudly trusted by more than 50% of the Fortune 100.
Founded in 2008, we’re a global company focused on building a culture where all employees feel a deep sense of belonging, where every ‘Relic’ can bring their whole self to work and feel supported and empowered to thrive. We’re consistently recognized as a distinguished employer and are committed to building world-class products and an award winning culture. For more information, visit newrelic.com.
Our Hiring Process
In compliance with applicable law, all persons hired will be required to verify identity and eligibility to work and to complete employment eligibility verification. Note: Our stewardship of the data of thousands of customers’ means that a criminal background check is required to join New Relic.
We will consider qualified applicants with arrest and conviction records based on individual circumstances and in accordance with applicable law including, but not limited to, the San Francisco Fair Chance Ordinance.
Headhunters and recruitment agencies may not submit resumes/CVs through this website or directly to managers. New Relic does not accept unsolicited headhunter and agency resumes, and will not pay fees to any third-party agency or company that does not have a signed agreement with New Relic.
New Relic is an equal opportunity employer. We eagerly seek applicants of diverse background and hire without regard to race, color, gender identity, religion, national origin, ancestry, citizenship, physical abilities (or disability), age, sexual orientation, veteran status, or any other characteristic protected by law.

Interested in the details of our privacy policy? Read more here: https://newrelic.com/termsandconditions/applicant-privacy-policy

#LI-SP2"
183,Data Engineer,"San Francisco, CA 94102",San Francisco,CA,94102,None Found,None Found,None Found,None Found,None Found,None Found,"----------------
Role Description
----------------

In this role you will build very large, scalable platforms using cutting edge data technologies. This is not a ""maintain existing platform"" or ""make minor tweaks to current code base"" kind of role. We are effectively building from the ground up and plan to leverage the most recent Big Data technologies. If you enjoy building new things without being constrained by technical debt, this is the job for you!

----------------
Responsibilities
----------------


You will help define company data assets (data model), spark, sparkSQL and hiveSQL jobs to populate data models
You will help define/design data integrations, data quality frameworks and design/evaluate open source/vendor tools for data lineage
You will work closely with Dropbox business units and engineering teams to develop strategy for long term Data Platform architecture

------------
Requirements
------------


BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients.
Experience designing, building and maintaining data processing systems
Experience working with either a Map Reduce or a MPP system on any size/scale

------------------
Benefits and Perks
------------------


100% company paid individual medical, dental, & vision insurance coverage
401k + company match
Market competitive total compensation package
Free Dropbox space for your friends and family
Wellness Reimbursement
Generous vacation policy
10 company paid holidays
Volunteer time off
Company sponsored tech talks (technology and other relevant professional topics)

"
184,Senior Data Engineer Sandipan,"San Mateo, CA",San Mateo,CA,None Found,None Found,"
Proven expertise in production software development
7+ years of experience programming in Java, Python, SQL, or C/C++
Proficient in SQL, NoSQL, relational database design and methods for efficiently storing & retrieving data
Strong analytical and science skills
Creative problem solver
Excellent verbal and written communications skills
Strong team player capable of working in a demanding start-up like environment building the next generation Analytical Ecosystem..
Experience building complex and non-interactive systems (batch, real-time, distributed, etc.) Strong experience in working with and managing large distributed computing cluster (combining various technologies & frameworks like Hadoop, Nifi, streamsets, spark, kafka etc.)
","
Prior Data Platform Engineering experience
Experience with Hadoop, Hive, Pig, Avro, Thrift, Protobufs and JMS: ActiveMQ, RabbitMQ, JBoss, etc.
Dynamic and/or functional languages (e.g., Python, Ruby, Scala, Clojure)
Experience designing and tuning high performance systems
Prior experience with data warehousing and business intelligence systems
Experience with Elasticsearch, SolrWeb, and Lucene
Experience with Star Schema, fact vs dimensions, updates/restatements and views
Professional or academic background that includes mathematics, statistics, machine learning and data mining for optimizing the data platform
Linux expertise
Prior work and/or research experience with unstructured data and data modeling
Familiarity with different development methodologies (e.g., agile, waterfall, XP, scrum, etc.)
Demonstrate understanding of ""var"" vs. ""val"", use of multi-return methods, ability to write clean, legible scala code that solves a complex problem
Firm understanding on python memory mode, classes, sub classing, designing classes for re-use, static string constants rather than in-line constants
Understanding of various analytic and visualization utilities available in R
Configure a Jenkins build, create/update a Jira ticket, enable Automated Tests in gradle/maven build
Ability to leverage additional security tools like Ranger, Knox, Sentry to further harden a cluster or secure data access
Understanding of how to segregate data based on access control rules, when and how to encrypt data (whole record vs individual fields) when and how to mask fields, etc.
Able to create and deploy a Samza job via YARN or Mesos, read from a streaming source (like Kafka) and produce some filtered or enhanced output
Able to create a storm topology to filter or transform a steam of data. Ability to track state and isolation in Trident or similar
Able to connect DStream to Kafka or Flume (or similar) queue, filter or transform data and write back to DStream on a different topic/queue
Implementation of D3, Tableau or R graphing technologies that produce an intuitive view of the underlying data
Implement a graph (line or pie etc.) backed by a live (changing) data set, something like ""requests per minute"" or similar
Understand basic modeling techniques, tools sets. Implement simple Python or R analytic routines
",None Found,None Found,None Found,"As a Data Engineer, you will provide technical leadership to the team that designs and develops path-breaking large scale cluster data processing systems.

Design and develop code, scripts and data pipelines that leverage structured and unstructured data integrated from multiple sources. Software installation and configuration. Participate in and help lead requirements and design workshops with our clients. Develop project deliverable documentation. Mentor junior members of the team in software development best practices. Other duties as assigned.

Additionally, as a senior member of our development & Ops team, you will help establish thought leadership in the big data space by contributing best practices, white papers, technical commentary and representing our section as one Leads.

Job Qualifications:

Proven expertise in production software development
7+ years of experience programming in Java, Python, SQL, or C/C++
Proficient in SQL, NoSQL, relational database design and methods for efficiently storing & retrieving data
Strong analytical and science skills
Creative problem solver
Excellent verbal and written communications skills
Strong team player capable of working in a demanding start-up like environment building the next generation Analytical Ecosystem..
Experience building complex and non-interactive systems (batch, real-time, distributed, etc.) Strong experience in working with and managing large distributed computing cluster (combining various technologies & frameworks like Hadoop, Nifi, streamsets, spark, kafka etc.)

Preferred Knowledge, Skills and Abilities:

Prior Data Platform Engineering experience
Experience with Hadoop, Hive, Pig, Avro, Thrift, Protobufs and JMS: ActiveMQ, RabbitMQ, JBoss, etc.
Dynamic and/or functional languages (e.g., Python, Ruby, Scala, Clojure)
Experience designing and tuning high performance systems
Prior experience with data warehousing and business intelligence systems
Experience with Elasticsearch, SolrWeb, and Lucene
Experience with Star Schema, fact vs dimensions, updates/restatements and views
Professional or academic background that includes mathematics, statistics, machine learning and data mining for optimizing the data platform
Linux expertise
Prior work and/or research experience with unstructured data and data modeling
Familiarity with different development methodologies (e.g., agile, waterfall, XP, scrum, etc.)
Demonstrate understanding of ""var"" vs. ""val"", use of multi-return methods, ability to write clean, legible scala code that solves a complex problem
Firm understanding on python memory mode, classes, sub classing, designing classes for re-use, static string constants rather than in-line constants
Understanding of various analytic and visualization utilities available in R
Configure a Jenkins build, create/update a Jira ticket, enable Automated Tests in gradle/maven build
Ability to leverage additional security tools like Ranger, Knox, Sentry to further harden a cluster or secure data access
Understanding of how to segregate data based on access control rules, when and how to encrypt data (whole record vs individual fields) when and how to mask fields, etc.
Able to create and deploy a Samza job via YARN or Mesos, read from a streaming source (like Kafka) and produce some filtered or enhanced output
Able to create a storm topology to filter or transform a steam of data. Ability to track state and isolation in Trident or similar
Able to connect DStream to Kafka or Flume (or similar) queue, filter or transform data and write back to DStream on a different topic/queue
Implementation of D3, Tableau or R graphing technologies that produce an intuitive view of the underlying data
Implement a graph (line or pie etc.) backed by a live (changing) data set, something like ""requests per minute"" or similar
Understand basic modeling techniques, tools sets. Implement simple Python or R analytic routines

Job Abilities:
Must be able to sit for long periods of time working on computers. Must be able to interact and communicate with the senior management in meetings. Must be able to write programming code in applicable languages and write project documentation in English.

Education:
Bachelor's Degree or foreign equivalent in Computer Science or related technical field followed by six (6-8) years of progressively responsible professional experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).

OR

Master's Degree or foreign equivalent in Computer Science or related technical field. Four (4-5) years of experience programming in Java, Python or C/C++. Experience with production software development lifecycle. Experience with Linux, SQL, relational database design and methods for efficiently retrieving data. Experience building complex and non-interactive systems (batch, distributed, etc.).

Employer will accept any suitable combination of education, training, or experience."
185,Data Engineer,"Emeryville, CA",Emeryville,CA,None Found,None Found,None Found,None Found,"
Collaborate with architects to implement data solutions that solve business problems
Design conceptual, logical and physical data models
Implement effective and scale-able end-to-end data pipeline solutions
Import data from a multitude of external systems
Build metrics and reports",None Found,"
Bachelor's degree in Computer Science/Engineering
2+ years professional development experience
Fluency in SQL, preferably MySQL
Familiar with data storage mechanisms, e.g. Hadoop, NoSQL, etc.
Experienced in ETL development
Understanding of dimensional and normalized database models and their applications
Good knowledge of the Linux operating system, networking, and toolset (bash, ssh, vim, etc.), especially text processing commands (sed, awk, etc.)
Experience in OOP (e.g. C++, Python, etc.)
Strong software engineering best practices (unit testing, code reviews, design documentation)
Good verbal and written communication skills","We combine the power of Big Data, technology, and lean economics. We discover the information people are searching for and provide it. We help transform lives.

Callisto Media will be unmatched in providing products, services, and experiences to a diverse universe. From mainstream populations to groups that traditional companies believe are too small or economically unfeasible to address, we will meet their needs.

Today, we're the fastest growing company in the $140 billion global publishing industry, and our primary method for meeting peoples' needs is through books. But creating books for them is only the beginning.

Data Engineer to join our small engineering team. Design, code, test, and support projects to manage the company’s data that will directly impact publishing efforts.

Responsibilities
Collaborate with architects to implement data solutions that solve business problems
Design conceptual, logical and physical data models
Implement effective and scale-able end-to-end data pipeline solutions
Import data from a multitude of external systems
Build metrics and reports
Requirements
Bachelor's degree in Computer Science/Engineering
2+ years professional development experience
Fluency in SQL, preferably MySQL
Familiar with data storage mechanisms, e.g. Hadoop, NoSQL, etc.
Experienced in ETL development
Understanding of dimensional and normalized database models and their applications
Good knowledge of the Linux operating system, networking, and toolset (bash, ssh, vim, etc.), especially text processing commands (sed, awk, etc.)
Experience in OOP (e.g. C++, Python, etc.)
Strong software engineering best practices (unit testing, code reviews, design documentation)
Good verbal and written communication skills
Other useful skills/experience:
Experience using Git
Knowledge of JavaScript
Knowledge of Business Intelligence (BI) / Data Warehousing (DW) principles and software (e.g. Tableau, Looker, etc.)
Experience with cloud based SaaS platforms
Experience with clustered and/or distributed systems
Experience developing within large codebases
Experience with web development: CGI, HTML, Javascript, Apache
Experience with REST APIs
Join us! We are a team where data drives our decisions. Where culture matters. We put our customers first and they embody our core values. We’re entrepreneurial and focused in our approach. We challenge each employee to experiment and drive results while feeling empowered to do their best work each and every day.

Callisto Media offers a competitive salary, full benefits, 401k, stock options, for full-time employees, as well as a friendly working environment. This is a full-time, onsite position."
186,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Kiva:
Kiva is an international nonprofit working in more than 80 countries, with a mission of expanding financial access to help underserved communities thrive. We do this by crowdfunding loans for entrepreneurs, women and students on our kiva.org lending platform, and by addressing the underlying barriers to financial access around the world through innovative projects and partnerships. Our organization combines the culture and technological passion of an internet start-up with the compassion and heart of a nonprofit to create impact and opportunity at global scale. With offices in San Francisco, Portland, New York, Nairobi and Bangkok, Kiva's team includes 100+ employees and 400+ volunteers worldwide. Our team is growing as we pursue exciting new opportunities to create a financially inclusive world.

About Data Science @Kiva

The newly formed Data Science team at Kiva is tasked with creating impact by using data to solve problems around financial inclusion. This translates to a host of interesting and challenging technical problems spanning analytics, machine learning, data engineering and infrastructure. The team is currently building out algorithms to power personalization on kiva.org and is looking to scale data science for other kinds of problems. Our current ecosystem of infrastructure, tools and skill sets includes AWS, GCP, Kubernetes, Snowflake, Fivetran, Looker, Google Cloud Composer (Airflow), Snowplow, Python, Kotlin, R, PHP, and Hyperledger. We are looking for smart and motivated individuals to join our growing team.

About the role

The team is looking for our first data engineer to help architect, design, build, maintain and evolve our data platform & machine learning infrastructure. This is an opportunity to be an early influencer and a significant contributor to the direction and growth of the team. As a Senior Data Engineer, you will:


Build a machine learning and data science platform that will influence the experience of millions of lenders and borrowers around the world.
Make decisions and trade-offs that will drive the data engineering roadmap.
Collaborate with data scientists & machine learning engineers to build new data driven products.
Build an ecosystem of tools to scale data science at Kiva.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with various parts of the organization (Engineering, Product, Operations etc.) on solving data related technical issues.
Help foster a spirit of innovation and collaboration both within the engineering team and across the organization.
Work to create impactful and sustainable solutions to complex problems by taking bold and measured risks.
Balance your technical excellence with a high E.Q., showing up with a sense of empathy, awareness, and responsibility.
Share the knowledge you gain generously with your peers to perpetuate a culture of engineering excellence.

About You


You have a BS in a quantitative discipline (computer science, engineering, physics, mathematics or a related field) or comparable work experience.
You have the ability to communicate findings and recommendations to technical and non-technical audiences.
You love solving complex and challenging data and ML infrastructure problems.
You have built data platforms to enable machine learning driven products.
You have 3+ years of experience deploying and scaling machine learning models.
You have 5+ years of experience implementing complex data pipelines that support analytics & machine learning.
You have coding (Python & SQL) and design skills.
You have experience in distributed systems and architectures.
You are willing to learn, take initiative and wear multiple hats as required.

This role will be based in San Francisco or Portland, and will report to the Director of Data Science based in San Francisco. At this time, we can only consider applicants with authorization to work in the United States on a permanent, full-time basis; unfortunately, we cannot provide visa sponsorship.

What We Offer


An opportunity to improve real lives, solve hard problems, and change the world
Friendly, supportive, and adventurous environment with a team of engaged colleagues
A comprehensive, industry-leading benefits package
Opportunities to connect with and learn from colleagues and partners around the world

A diverse and inclusive workplace where we learn from each other is an integral part of Kiva's culture. We actively welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and a great place to work. Join us and help us achieve our mission!"
187,Senior Business Intelligence Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Team at Womply advances the state of our data, and empowers the company to make better decisions from our data. We're seeking a talented and motivated Sr. BI Engineer to join our team. As a Sr. BI Data Engineer, you will hold the keys to the infrastructure that powers our current and future Data Products. We expect you to help us build and leverage the latest technologies to tap into our firehose of data, and your systems will open up new paths for analysis and discovery. You will have the opportunity to make a big impact, and work with extremely talented peers on a fast paced, high energy team.

You will develop ETL and data modeling solutions from our Data Lake into our Data Warehouse, to help us evolve our data-driven philosophy and become a world-class data organization. You will own the design, execution, and ongoing support of critical data warehousing projects enabling accurate reporting and advanced analytics for all of Womply's internal business units.

You will have to be self-sufficient - we are a startup, so everyone might do a bit of everything to get things done. We look for people who take pride in their work, execute on it, and deliver phenomenal results.

In order to be successful in this role, you will be responsible for:


Building and maintaining the data pipelines from various data sources, while maintaining high accuracy, consistency, and reliability
Leveraging our data foundation to design and implement innovative solutions to our hardest data problems, and ensure their quality and effectiveness with robust verification process
Driving innovation by recommending and adopting new tools and technologies that provide competitive data advantages for the company
Developing, documenting, and implementing data models for analytics
Partnering with business stakeholders, product managers, business intelligence analysts, engineering, and data scientists to understand reporting requirements and bring ideas to production

You must have:

3-8 years in software engineering
Experience with Data Warehousing, Architecting Pipelines, and Data Modeling
Team-oriented, self-motivated, success-driven, roll-up-your-sleeves attitude
Strong intellectual curiosity and demonstrated ability to understand and question the data
Healthy Skepticism to challenge the status quo so we can improve
Technically proficient in:
Languages - Python / Scala / Ruby / SQL / Bash
Technologies - Snowflake/Athena, AWS, Airflow

Nice to have:

Spark

Come build something amazing at Womply:
Womply helps small businesses thrive in a digital world. Our software makes it easy for small businesses to boost their online reputations, engage their customers, and monitor the health of their businesses with data and technology they can't get anywhere else. We're one of the fastest growing software companies in the country, serving more than 100,000 small businesses across 400+ business verticals in every corner of America.

We're a fanatically values-based company with $50 million raised to accelerate our growth. We work hard and push each other to be the best, but we also have fun and don't take ourselves too seriously. If you want to win and make a big impact, let's talk. We're hiring in the Bay Area and Lehi, Utah for engineering, DevOps, design, data science, sales, marketing, business development, account management, and more.

PLEASE NOTE - Direct applicants ONLY. Any recruiter/3rd party submissions we receive will be considered a gift.

More:
Work at Womply ( https://womply.com/jobs/ )

Life at Womply ( https://womply.com/life-at-womply/ )

How we work ( https://womply.com/how-we-work )

Our values ( https://womply.com/values/ )

Benefits ( https://womply.com/benefits )

Diversity ( http://womply.com/diversity/ )"
188,Google Cloud Data Engineer Trainer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
Experience producing applications/products ideally cloud based products
Comfortable presenting and delivering multiple-day-long training session both in the classroom and online
Comfortable in training enterprise level clients on how to develop or diversify their current skill set
Basic proficiency with command-line tools and Linux operating system environments
Systems Operations experience, including deploying and managing applications, either on-premises or in a public cloud environment
Comfortable and confident in at least one of the following languages and technologies:
Python
SQL
C++
Java
PHP
Ruby
GO
Node.JS
.NET
Proficient in building data pipelines using programming models like Apache Beam
Experience working with HADOOP
Familiar with numerical libraries and popular machine learning frameworks such as scikit-learn or tensorflow
Certified on one of the following:
Google Cloud Platform
Amazon Web Services
Microsoft Azure",None Found,"
Deliver Google Cloud Certification courses both in the classroom and online
Help to aggressively grow the range of related courses.
Maintain and evolve existing classroom course content and respond to post-training questions
Through the delivery of great training, encourages repeat bookings onto other courses
Work with Jellyfish Agency to guide our clients and internal teams on cloud best practices
Support new business opportunities
Help create marketing materials, assets and other promotional material",None Found,None Found,"Overview
The Role
We are looking for a Google Cloud Engineer guru to join the team and help us build our training portfolio within Google Cloud Platform as well as Amazon Web Services.
We require an experienced Cloud Engineer professional who has a proven track-record for delivering high quality products using GCP / AWS technologies.
You will be instrumental in helping the team identify opportunities for new Cloud Engineer and Data Engineer courses to add to the portfolio and take the lead in delivering the current Google Cloud Certification courses offered by the agency in partnership with Google, this should be as a minimum the Cloud engineer track courses (https://cloud.google.com/training/data-ml).
You will need to have in-depth knowledge and strong practical working experience of the Google Cloud Platform, plus ability to teach other cloud products.
Ideally you will also have a good understanding of the competitors on the landscape who compete directly with the Google Cloud, including other providers such as Amazon & Microsoft Azure.
A key part of this role will involve leading the creation of the Google Cloud Engineer Platform training material being used for both classroom and online courses. You will also be responsible for identifying opportunities for new courses to add to the portfolio and for obtaining the up-to-date knowledge and certifications required to deliver the existing Google Cloud courses..
You will be delivering face-to-face training to clients in the US, but also expect regular travel to deliver courses overseas.

This is an excellent opportunity for the right candidate to build their own profile within the industry, we will be positioning and promoting you as a thought leader and providing time for you to create both written / video content for publication online, creating compelling case studies of work carried out by the agency, leading round-table events, public speaking engagements, hosting events at our training venue and being proactive in sourcing good opportunities for increasing the awareness of the training proposition.
To help keep your skills sharp there will also be the chance to work alongside the agency’s Cloud practitioners. This opportunity can help guide our trainers on product developments and initiatives.

This is an excellent opportunity for the individual to build their own profile within the industry and to refine their public speaking skills. The ideal candidate should be comfortable at collaborating with an integrated team to develop outstanding training solutions.

Candidates must hold relevant cloud certifications, or be in a position to have passed them prior to joining the team.
Responsibilities
Responsibilities
Deliver Google Cloud Certification courses both in the classroom and online
Help to aggressively grow the range of related courses.
Maintain and evolve existing classroom course content and respond to post-training questions
Through the delivery of great training, encourages repeat bookings onto other courses
Work with Jellyfish Agency to guide our clients and internal teams on cloud best practices
Support new business opportunities
Help create marketing materials, assets and other promotional material
Qualifications
Knowledge, Skills & Experience Requirements
Experience producing applications/products ideally cloud based products
Comfortable presenting and delivering multiple-day-long training session both in the classroom and online
Comfortable in training enterprise level clients on how to develop or diversify their current skill set
Basic proficiency with command-line tools and Linux operating system environments
Systems Operations experience, including deploying and managing applications, either on-premises or in a public cloud environment
Comfortable and confident in at least one of the following languages and technologies:
Python
SQL
C++
Java
PHP
Ruby
GO
Node.JS
.NET
Proficient in building data pipelines using programming models like Apache Beam
Experience working with HADOOP
Familiar with numerical libraries and popular machine learning frameworks such as scikit-learn or tensorflow
Certified on one of the following:
Google Cloud Platform
Amazon Web Services
Microsoft Azure
Skill Requirements
Can clearly communicate processes, strategies and general requirements to both internal and external stakeholders.
Capable of explaining complex concepts in simple terms
Gather delegate requirements and integrating them with the course learning outcomes
Facilitate user led learning through practical workshops
Ability to promote user interactions and engagement"
189,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
At least 4-5 years of experience in Scala/Java or Python programming
AWS data products (Data pipelines, Redshift, Pinpoint, S3, etc)
Experience with recognized industry patterns, methodologies, and techniques
",None Found,None Found,None Found,None Found,"About Skillz:
Skillz is driving the future of entertainment by accelerating the convergence of sports, video games and media for an exploding mobile-first audience worldwide. The company's platform empowers mobile game developers and players with democratized access to fun, fair and skill-based competition for real prizes ( https://venturebeat.com/2019/01/31/skillz-top-10-mobile-esports-players-won-8-million-in-2018/ ), shifting the paradigm to make eSports accessible to anyone, anywhere with a mobile device.

Skillz helps developers build multi-million dollar game franchises by turning content into competitive social gaming properties for the world's 2.6 billion gamers. The company has already worked with 13,000 game developers, leveraging its patented technology to host over 800 million tournaments for 18 million players worldwide.

This year, Skillz was recognized as one of Fast Company's Most Innovative Companies ( https://www.entrepreneur.com/slideshow/313595 ) and CNBC Disruptor 50 ( https://www.cnbc.com/2019/05/14/skillz-2019-disruptor-50.html ) (for the second time ( https://www.cnbc.com/2017/05/16/skillz-2017-disruptor-50.html )). In 2018, Skillz was listed as one of Forbes' Next Billion-Dollar Startups ( https://www.forbes.com/next-billion-dollar-startups/#6334ab494441 ) and Entrepreneur Magazine's 100 Brilliant Companies ( https://www.entrepreneur.com/slideshow/313595 ). In 2017, Inc. Magazine ranked Skillz the No. 1 fastest-growing private company in America ( https://www.entrepreneur.com/slideshow/313595 ).

The company is backed by leading venture capitalists, media companies, and professional sports luminaries, ranging from Liberty Global, Accomplice, Wildcat Capital, Telstra Ventures, and a founder of Great Hill Partners to the owners of the New England Patriots, Milwaukee Bucks, New York Mets, and Sacramento Kings.

Who we're looking for:
You're ready to take the next step in your Data Engineering career - to a fast-moving, successful company building out their next-generation streaming analytics infrastructure! You love data consistency and integrity. You consider yourself scrappy and a technologist, passionate about data infrastructure... with your attention to detail and insistence on doing things correctly, you know you can make a big impact on a small team! You're an excellent communicator and know that you grow faster from being able to mentor others.

What You'll Do:

Build new systems to provide real-time streaming analytics and event processing pipeline based on fast data architecture
Build enterprise grade data lake to support both business analytical needs and next generation data infrastructure
Building data integration toolkit for backend services
Support our data science team in deploying new algorithms for matchmaking, fraud and cheat detection
Find better ways to move massive amounts of data from a variety of sources to formats consumable by reporting systems and people
Improve monitoring and alarms that impact data integrity replication lag
Support our product development team in creating new events to measure/track

Your Skillz:
Basic Qualifications:

At least 4-5 years of experience in Scala/Java or Python programming
AWS data products (Data pipelines, Redshift, Pinpoint, S3, etc)
Experience with recognized industry patterns, methodologies, and techniques

Bonus


Familiarity with Agile engineering practices
4+ years of experience with Spark, Scala and/or Akka
4+ years of experience with Spark Streaming, Storm, Flink, or other Stream Processing technologies
2+ years of experience working with Kafka or similar data pipeline backbone
4+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python
3+ years' experience with NoSQL implementation (ElasticSearch, Cassandra, etc. a plus)
At least 4-5 years of experience with Unix/Linux systems with scripting experience
Familiarity with Alooma, Snowflakes
Familiarity with Kinesis, Lamda
Prior experience in gaming
Prior experience in finance

Skillz embraces diversity and is proud to be an equal opportunity employer. As part of our commitment to diversifying our workforce, we do not discriminate on the basis of age, race, sex, gender, gender identity, color, religion, national origin, sexual orientation, marital status, citizenship, veteran status, or disability status, and we operate in compliance with the San Francisco Fair Chance Ordinance."
190,Senior Data Engineer,"Oakland, CA",Oakland,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Recent years have made us all too familiar with the destruction that natural disasters can bring about, and the increasing frequency and intensity with which they are occurring. Every year we are faced with the devastation and harsh impact that wildfires, hurricanes, and floods have on millions of lives. Despite mounting losses, conventional methods of risk modeling at best paint an incomplete picture of these threats, and at worst, get them completely wrong. Better outcomes require better preparedness. Better preparedness requires better modeling, which requires better data and better approaches to working with that data.

Zesty.ai uses novel data gathering and machine learning approaches to produce higher quality information about the risks to property from catastrophes like floods and wildfires. While AI alone may not be able to thwart these disasters, it can help us be more prepared for them, and ultimately that will lead to better outcomes.

We have partnered with leading insurance carriers and reinsurers to underwrite risk more accurately, provide customers a smoother purchasing experience and manage inspections more cost-effectively.

As a Senior Data Engineer, you have strong interest/experience working with remote sensing and imagery. You will be building the platform that delivers processed data and imagery to our machine learning systems. You will be doing advanced imagery manipulation and GIS processing. You thrive in a collaborative, creative environment that moves fast and are comfortable learning new skills and working with unfamiliar technologies.

Are you ready to help us change the world?

What You’ll Do:
Working on data cleansing, batch processing, data transformations, and other data manipulations to enable our data science efforts
Advanced image manipulation
Geometry/geographic calculations
Serving as part of the core team for the technology stack
Partnering closely with the Founders to bring a disruptive AI based technology platform to the insurance and real estate markets
Investigating and prototyping new technologies
What You Bring to the zesty.ai Team:
Bachelor’s Degree is required
At least 2 years of employment as a data engineer in a professional setting, or other relevant development experience
Expert in python
Expert working with cloud platforms (AWS, Google Cloud, etc)
Experience with Airflow or other workflow management software
Ability to define data model and data storage strategies, including knowledge of distributed data systems
Ability to manage multiple/competing priorities and make the right tradeoffs and timely delivery of features
Experience or familiarity with geography, geometry and GIS systems
Experience working with satellite/remote imagery
Relevant education (Coding Bootcamp, and/or Bachelors in Computer Science) or equivalent experience
Must be legally eligible to work in the U.S.
Why Zesty.ai:

Be part of a well-funded early-stage start-up
Market competitive comp and equity incentives to give you a stake in our future
Medical, Vision and Dental for you and your dependents
Pre-tax commuter & parking benefits
Flexible Time Off
An upbeat and collaborative work culture
Company-sponsored outings
Free One Medical membership



All your information will be kept confidential according to EEO guidelines."
191,Consulting Data Engineer,"San Francisco, CA 94105",San Francisco,CA,94105,None Found,None Found,None Found,None Found,None Found,None Found,"At Ravel we develop the legal profession’s most innovative products for legal analytics and research, using the latest in visualization and text mining. Lawyers use our products to forecast how judges will rule, find critical cases, and make data-driven decisions. We're working with great technology and the challenges that get us fired up involve large-scale heterogeneous text and data mining, beautiful UI, and machine learning. Industry leader LexisNexis acquired Ravel in 2017.


What you will be doing:

Build and scale data infrastructure that powers batch and real-time data processing of billions of records
Design and build scalable data ingestion and enrichment pipelines (machine learning inference)
Automate and handle life-cycle of the systems and platforms that process our data.
Provide visibility into the health of our data platform (comprehensive view of data flow, resources usage, data lineage, etc)
Evolve maturity of our data quality and monitoring systems
Mentor fellow teammates on algorithms, data structures, design patterns, and best practices

What Are We Looking For:
7+ years of Software Engineering experience
Bachelor’s degree in computer science or equivalent practical experience
Passion for big data and creative ideas for what to do with it
Expertise in any of the following programming language: Java, Scala, C++, Python
Experience working with data technologies that power analytics (e.g. Hadoop, Spark, Kafka, etc). Experience with SQL and/or No-SQL Experience with any cloud solutions (AWS, GCP, Azure)
Bonus:
Experience working with Elasticsearch
Experience working with Neptune DB
Understanding of machine learning

LexisNexis Legal & Professional (www.lexisnexis.com) is a leading global provider of content and technology solutions that enable professionals in legal, corporate, tax, government, academic and non-profit organizations to make informed decisions and achieve better business outcomes. As a digital pioneer, the company was the first to bring legal and business information online with its Lexis Nexis services. Today, LexisNexis Legal & Professional harnesses leading-edge technology and world-class content, to help professionals work in faster, easier and more effective ways. Through close collaboration with its customers, the company ensures organizations can leverage its solutions to reduce risk, improve productivity, increase profitability and grow their business. Part of RELX Group plc, LexisNexis Legal & Professional serves customers in more than 100 countries with 10,000 employees worldwide. LexisNexis, a division of RELX Group, is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. If a qualified individual with a disability or disabled veteran needs a reasonable accommodation to use or access our online system, that individual should please contact 1.877.734.1938 or accommodations@relx.com.
RSRLNLP"
192,"Data Strategy Specialist - Business & Data Analysis, Cloud, AWS, Azure, Big Data","San Francisco, CA 94105",San Francisco,CA,94105,None Found,None Found, 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:,None Found,None Found,None Found,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The North America Data Strategy & Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.

 Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model

Required Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:
o Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling
o Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.
o Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel
 Able to travel up to 100% (Mon-Thu)

Optional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model
 Experience in compiling business cases and roadmaps for data, analytics and technology investments

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
193,"Staff Big Data Engineer (Java, Spark, Kafka, AWS) - Data Science team","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Ancestry:
When you join Ancestry, you join our family tree. Backed by history, science, and technology, we’re creating a new world of connection, innovation, and understanding. Whether it’s reuniting long-lost relatives through DNA or unearthing new family stories from historical records, Ancestry empowers life-changing experiences. With over 20 billion digitized historical records, 100 million family trees, and 15+ million DNA kits sold, Ancestry is bringing the power of personal discovery to people around the world.

Ancestry is seeking a Staff level Software Developer to join our Big Data as a Service team. In this role, you will deliver products and services to help our Data Science and Product Analytics teams to facilitate research to improve our Ancestry customers experience. As such you will play a key role in engineering a secure, scalable and highly available platform used by our internal scientists using a variety of technologies such as AWS EMR, Spark, Kafka, Kinesis, Kubernetes, Docker and many more.

Who you are (at a minimum):
7+ years of experience in software development (at a minimum, in Java & preferred to also have Scala)
2+ years of experience working with large scale distributed frameworks (Spark and/or MapReduce)
2+ years of experience with cloud (AWS and cloud provisioning frameworks such as Terraform and orchestration tools such as Airflow will be given priority)
Has demonstrated understanding of SOLID principles, and can work effectively in an Agile environment
Has demonstrated technical leadership abilities, leading the team to increase quality, and helps promote team discussions for maximum results
One who has been a lead or who has taken the lead for a concept-through completion system/platform build.
Takes responsibility for scrum team results
Guides the team to solve highly scalable issues, and guides team to maximize productivity/risk reductions
Provides guidance for the team, and has ability to interact with product owner and other technical leaders through entire product lifecycle
Bachelor's in Computer Science or Computer Engineering is required. Master's degree in computer science, computer engineering, or other technical disciplines, or equivalent work experience, is preferred.

Will be given priority to those who have any & all of the following:
Working knowledge of streaming engines, such as Kafka or Kinesis.
Familiarity with contemporary data science workflows and techniques is a huge plus.

#bigdata #kafka #mapreduce #streamingdata #scala #java #search #terraform #distributedsystems #kinesis #aws

GD-Sponsored
IND1
#LI-AH1

Additional Information:
Ancestry is an Equal Opportunity Employer that makes employment decisions without regard to race, color, religious creed, national origin, ancestry, sex, pregnancy, sexual orientation, gender, gender identity, gender expression, age, mental or physical disability, medical condition, military or veteran status, citizenship, marital status, genetic information, or any other characteristic protected by applicable law. In addition, Ancestry will provide reasonable accommodations for qualified individuals with disabilities.

All job offers are contingent on a background check screen that complies with applicable law. For San Francisco office candidates, pursuant to the San Francisco Fair Chance Ordinance, Ancestry will consider for employment qualified applicants with arrest and conviction records.

Ancestry is not accepting unsolicited assistance from search firms for this employment opportunity. All resumes submitted by search firms to any employee at Ancestry via-email, the Internet or in any form and/or method without a valid written search agreement in place for this position will be deemed the sole property of Ancestry. No fee will be paid in the event the candidate is hired by Ancestry as a result of the referral or through other means"
194,Visualization & Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Samsara
-------------

Samsara, founded in 2015, is a leader in Industrial IoT and our mission is to increase the efficiency, safety, and sustainability of the operations that power our economy. Our solutions combine hardware and software to bring real-time visibility, analytics, and AI to operations across various industries. Samsara's fast-growing team is headquartered in San Francisco, with offices in San Jose, Atlanta, and London. Our team has raised over $530M from Andreessen Horowitz, General Catalyst, Tiger Global, and Dragoneer.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Samsara is growing fast… like, really fast! To support our growth, we are standing up a new analytics & operations capability to solve some of Samsara's most pressing business problems with a data-first mindset. The team will tackle cross-functional projects across People Ops, Customer Success, Recruiting, Engineering, Sales, and more with the core mission of unlocking real value through business acumen, data, and a bias toward action. Examples of a few cross-functional strategic projects you will work on include: how can Samsara optimize its recruiting operations as we hire another 1000 employees? Where should Samsara open its next office? How can we enable our customer success team to manage 10K+ customer accounts at scale?
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

About the role
--------------

Are you an entrepreneurially-minded analytics expert looking to unlock business value through data? Can you manipulate large, disparate datasets and package up your output in easy-to-digest BI dashboards? We are looking for a talented Visualization & Data Engineer to solve Samsara's most pressing business problems as part of a newly formed analytics & operations capability.
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The Visualization & Data Engineer will be a core technical contributor to the team with deep expertise in combining disparate data sources (e.g., via SQL and API calls) and building accessible dashboards (e.g., in Tableau) to drive business outcomes.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The Visualization & Data Engineer will work on:
---------------------------------------------------


Writing SQL queries and API calls to merge cross-functional datasets

--------------------------------------------------------------------------


Developing automated processes to clean, structure, and store data

------------------------------------------------------------------


Conducting quantitative analyses to solve business problems (e.g., likelihood to churn models to prioritize customer account management)

----------------------------------------------------------------------------------------------------------------------------------------


Creating dashboards for management / operational teams (e.g., Tableau visualizations)

-------------------------------------------------------------------------------------

An ideal candidate has:
-----------------------


Strong proficiency in SQL and accessing data via API calls

----------------------------------------------------------


Experience with data manipulation/processing, preferably in Python (e.g., with Pandas)

--------------------------------------------------------------------------------------


Experience with data visualization, preferably in Tableau

---------------------------------------------------------


2+ years experience as a data scientist, data engineer, or related role

-----------------------------------------------------------------------


Preferred: Experience with statistical analysis and machine learning

--------------------------------------------------------------------


Preferred: BA / MS degree in Computer Science, Statistics, or related discipled

-------------------------------------------------------------------------------

------

At Samsara, we welcome all. All sizes, colors, cultures, sexes, beliefs, religions, ages, people. We depend on the unique approaches of our team members to help us solve complex problems. We are committed to increasing diversity across our team and ensuring that Samsara is a place where people from all backgrounds can make an impact.
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
195,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,"
The position would primarily work with the statistics and engineering teams, along with day to day communication with the product and marketing team.
Design and develop AWS systems to collect and manage business events at scale.
Respond to request from the product, engineering and marketing team for timely data analysis.
Participate in multi-team planning for future data collection needs.
",None Found,"
Strives to deliver clean, maintainable and testable code
Has a passion for user privacy
Is open to learning new languages and frameworks
Has a soft spot for JavaScript
","We are looking for an experienced data engineer to be responsible for the design, development and maintenance of large data collection and processing systems at Brave. This position will focus on scaling our backend systems to handle the increased load of data collection, processing and presentation.

-----------------

Responsibilities:
-----------------


The position would primarily work with the statistics and engineering teams, along with day to day communication with the product and marketing team.
Design and develop AWS systems to collect and manage business events at scale.
Respond to request from the product, engineering and marketing team for timely data analysis.
Participate in multi-team planning for future data collection needs.

-------------

Requirements:
-------------

Strong AWS skills with extensive knowledge of the data collection, storage and processing systems that AWS provides. Experience building ETL pipelines at scale.

Familiarity with Redshift, Kinesis, Athena, Glue, Data Pipeline, MongoDB, Postgresql.

Strong JavaScript and SQL skills.

Extensive knowledge of modern open source software develop practices and tools i.e. Git, Github, Unit and integration testing, pull requests, code review etc…

Additional qualities:

Strives to deliver clean, maintainable and testable code
Has a passion for user privacy
Is open to learning new languages and frameworks
Has a soft spot for JavaScript

Benefits


Competitive salary
4 weeks (20 days) of paid vacation per year
Excellent medical coverage
Generous 401k plan
Stock option grant
Travel and conference budgets
Commuters benefit (On­site only)
Hip office in the SoMA neighborhood of SF

Candidates must be legally authorized to work in the United States or Canada."
196,"Data Engineer, Data Solutions","San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Affirm is reinventing credit to make it more honest and friendly, giving consumers the flexibility to buy now and pay later without any hidden fees or compounding interest.

As a Data Engineer, you will be working cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data model. You will architect, design, implement data pipelines enabling insights from our product for partners, data scientists, analysts and decision across Engineering, Product, Marketing, Product and Finance organizations.

Data engineers also need to guarantee compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data models and pipelines. This role will require collaboration across the company.
This is a fun role for problem solvers, who can intuitively anticipate problems and can also look beyond immediate issues. They will be a self-starter, detail-oriented and quality-oriented and passionate about taking initiatives.
What You'll Do
Design, implement and build data models and ETL pipelines that deliver quality data meeting SLA requirements.
Partner with product, data analyst, engineering teams and other data engineers to build foundational, trusted and documented datasets enabling self-service and Single Source of Truth.
Be an advocate for data governance, security, privacy, quality and retention.
Own and document data pipelines and data lineage.
Identify, document and promote best practices.
Identify and work with Infrastructure teams to address Data Platform gaps.
What We Look For
BS, MS or PhD in Computer Science, Engineering or a related technical field.
Experience in data management disciplines including data modeling, data quality and other areas directly relevant to data engineering.
Experience working with cross-functional teams and collaborating with business or product or engineering stakeholders in data management and analytics initiative.
Ability to work on multiple areas like ETLs, Data modeling & design, writing complex SQL queries etc.
Passionate about various technologies including but not limited to SQL/No SQL/MPP databases etc.
Hands-on experience with Data Warehouse technologies (Snowflake, Redshift) and Big Data technologies (e.g Spark)
Proficiency with programming languages is a big plus (e.g. Python)
Excellent written and verbal communication and interpersonal skills; able to effectively collaborate with technical and business partners.
At Affirm, ""People Come First"" is a core value and that’s why diversity and inclusion are vital to our priorities as an equal opportunity employer. You can learn more about our D&I efforts here.

We also consider qualified applicants with arrest and conviction records for positions in accordance with applicable laws, including the San Francisco Fair Chance Ordinance."
197,Software Engineer,"Redwood City, CA",Redwood City,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Auction.com is the nation’s leading online real estate marketplace focused exclusively on the sale of residential bank-owned and foreclosure properties via online auctions and live trustee sale events. By offering access to exclusive properties and technology designed to seamlessly connect buyers and sellers, Auction.com empowers residential real estate investors and financial institutions to achieve optimal, mutually beneficial results – to go beyond the bid.

Senior Data Engineer

Position Summary
At Auction.com, we are embarking on a journey to transform the real estate market with technological innovations. A critical prerequisite for this transformation is a robust data infrastructure. As a senior data engineer, you will help us design and implement our big data environment that is real-time, stable and scalable. You will work with a talented data engineering team to improve our data processing pipeline and developing new capabilities to support mission-critical initiatives. You will mentor junior engineers and help evaluating new technology along the way. You impact will be felt across the team as well as the entire Auction.com organization.

Responsibilities/Duties

Make major contribution to the implementation of our real time big data initiative
Build and automate productized data processing pipelines in AWS big data platform
Help improve our development process and standards through mentoring and leading-by-example
Help define and implement data ingestion contracts
Create data environment to support our data analytics, reporting and data science teams

Knowledge, Skills and Abilities

In-depth understanding of modern big data technology, including Hadoop and Spark
Knowledge of real time data streaming and aggregation architectural patterns and practice
Proficient in programming languages such as Python, Scala and Java
Familiarity with NoSQL as well as SQL databases
Data modeling and machine learning skill is a plus

Education/Experience

Bachelor's Degree in computer science, data science or related fields
Familiarity with agile developmental process
Previous experience developing data product required
Hands-on experience with real time data streaming, aggregation and presentation strongly preferred
Previous experience with production ETL pipeline development required
At least a years’ experience with AWS cloud or another cloud platform

To all recruitment agencies: Auction.com does not accept agency resumes unless you are part of our preferred partner network. Please do not forward resumes to our jobs alias, Auction.com employees or any other company location. Auction.com is not responsible for any fees related to unsolicited resumes."
198,Staff Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Komodo Health is addressing the global burden of disease through the world's most actionable healthcare map. Our solutions drive a more transparent, efficient and productive healthcare ecosystem. We value our culture of encouraging growth, collaboration, and constructive debate as well as delivering innovative solutions that ""wow"" our customers.

The Data Engineering (DE) team is looking for Staff Data Engineers to help us lead projects that will further our tech stack related to data, helping us develop data processing that is scalable, reliable and automated.

This is an opportunity to join a growing company, and be a part of a team of folks accomplished in diverse Engineering disciplines; focused on using the best of what lies at the forefront of technology and skills to address complex, real-world problems in the Healthcare and Life Science space. Some of the tools we use are: Python, Spark, AWS, Kubernetes, Docker, Postgres, Git, Airflow and Flask.

RESPONSIBILITIES
----------------


Design, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources.
Create automation systems and tools to configure, monitor, and orchestrate our data infrastructure and our data pipelines.
Evaluate new technologies for continuous improvements in Data Engineering.
Collaborate closely with the product team to build out new data features.
Work with the data scientists to implement descriptive, forecasting and predictive algorithms and models using latest technologies.

REQUIREMENTS
------------


BS, MS in CS or related degrees.
5+ years of relevant experience with data engineering or similar experience.
Experience with...
Building and deploying large-scale data processing pipelines.
Python, Scala or Java coding experience, proficiency with at least one of them is required.
Workflow & pipeline systems, like Airflow, Luigi, etc.
Distributed systems for data processing tools such as Spark, Hadoop, Kubernetes, etc.
SQL and Relational Databases, like PostgreSQL.
Continuous integration and automation tools and processes, like Jenkins.
You've been through the planning, launching and refactoring phases of code you wrote.
A serious passion for data.
History of excellence and responsibility in previous engineering positions.
Ability to work as part of a collaborative team in a fast-paced environment.
Sincere interest in working at a startup and scaling with the company as we grow.

NICE TO HAVE


Experience or interest in the life sciences industry.
Basic knowledge (college level or hands on) of machine learning and statistics.
Experience with graph databases.

BENEFITS
--------


Competitive salary and equity compensation
Full Medical, Dental, and Vision benefits
401k plan with employer match
Flexible hours and a ""use as you need"" vacation policy
Opportunities to attend industry conferences and events
Great office location(s) in SF/SOMA and NY/FLAT IRON

"
199,Sr. Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"As a Senior Data Engineer, you will be working cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data Warehouse model. You will be working in our Business Technology Data and Analytics function and have an opportunity to build a sound data foundation and processes that will scale with the company’s growth. You will architect, design, implement data pipelines enabling insights from our Product and Corporate Systems for key partners, data scientists and decision makers at Slack. You would also be responsible for enhancing and maintaining our data warehouse.

Data engineers also need to guarantee compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines. This would enable faster data access, integrated data reuse and vastly improved time-to-solution for Slack’s data and analytics initiatives. The newly hired data engineer will be the key interface in operationalizing data and analytics on behalf of the Finance, Sales, Product and organizational outcomes. This role will require both creative and collaborative working with IT and the wider business.

This is a fun role for problem solvers, who can intuitively anticipate problems and can also look beyond immediate issues. In short, we look for people who take pride in the craft and want to be part of creating and defining the teams operating model and contribution to the company. They will be a self-starter, detail and quality oriented, and passionate about having a huge impact at Slack. If this role has your name written all over it, please contact us with a resume so that we explore further.

Responsibilities

Design, implement and build pipelines that deliver data with measurable quality under the SLA
Partner with Data Engineers, Data architects, domain experts, data analysts and other teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service
Be a champion of the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements
Own and document data pipelines and data lineage
Identify, document and promote best practices
Support and Maintain analytics tech ecosystem (data warehouse, ETL and BI tools)
Requirements

BS or MS degree in Computer Science or Engineering discipline.
At least 8 years or more of work experience in data management disciplines including data integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.
At least 4 years of experience working in cross-functional teams and collaborating with business stakeholders in Sales or Finance in support of a departmental and/or multi-departmental data management and analytics initiative.
Very strong experience in dimensional modeling, supporting data warehouse, scaling and optimizing, performance tuning and ETL pipelines
Deep understanding of relational as well as big data setup. Preferred: Prior experience with Snowflake, ETL tools (eg Informatica, Matillion, Snaplogic), Hive, Presto, Dimensional Modeling.
Problem solver with excellent interpersonal skills with ability to make sound complex decisions in a fast-paced, technical environment.
Ability to work on multiple areas like Data pipeline ETL, Data modeling & design, writing complex SQL queries etc.
Capable of planning and executing on both short-term and long-term goals individually and with the team.
Passionate about various technologies including but not limited to SQL/No SQL/MPP databases etc.
Hands-on experience with Data Warehouse technologies (Snowflake, Redshift) and Big Data technologies (e.g Hadoop, Hive, Spark)
Proficiency with programming languages is a big plus (e.g. Python)
Excellent written and verbal communication and interpersonal skills, able to effectively collaborate with technical and business partners
Excellent understanding of trade-offs
Demonstrated ability to navigate between big-picture and implementation details
Slack is a layer of the business technology stack that brings together people, data, and applications – a single place where people can effectively work together, find important information, and access hundreds of thousands of critical applications and services to do their best work. From global Fortune 100 companies to corner markets, businesses and teams of all kinds use Slack to bring the right people together with all the right information. Slack is headquartered in San Francisco, CA and has ten offices around the world. For more information on how Slack makes teams better connected, visit slack.com.
Ensuring a diverse and inclusive workplace where we learn from each other is core to Slack’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and a pleasant and supportive place to work.
Come do the best work of your life here at Slack."
200,Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,"
Experience building and maintaining large-scale analytics systems;
Experience in software development and delivery;
Expertise in one or more programming languages such as Python, Scala, Java, Javascript, etc. and related frameworks (Flask, Django, Spring, AngularJS, React).
Good knowledge of architecting, developing, and maintaining of cloud technologies (AWS, Azure, GCP)\nExperience with Big Data stack (Kafka, Hadoop, Storm, Spark, Hive, Airflow, ElasticSearch, TensorFlow);
Experience with relational (Postgres, MySQL) and/or non-relational (Cassandra, MongoDB) databases;
Strong machine learning or stats knowledge a plus;
Experience in Supply Chain / Retail / eCommerce;
Excellent communication skill and a strong team player.
BS/MS in Computer Science, Computer Engineering or related technical discipline;
Strong analytical and quantitative problem-solving ability.
","
Experience building and maintaining large-scale analytics systems;
Experience in software development and delivery;
Expertise in one or more programming languages such as Python, Scala, Java, Javascript, etc. and related frameworks (Flask, Django, Spring, AngularJS, React).
Good knowledge of architecting, developing, and maintaining of cloud technologies (AWS, Azure, GCP)\nExperience with Big Data stack (Kafka, Hadoop, Storm, Spark, Hive, Airflow, ElasticSearch, TensorFlow);
Experience with relational (Postgres, MySQL) and/or non-relational (Cassandra, MongoDB) databases;
Strong machine learning or stats knowledge a plus;
Experience in Supply Chain / Retail / eCommerce;
Excellent communication skill and a strong team player.
BS/MS in Computer Science, Computer Engineering or related technical discipline;
Strong analytical and quantitative problem-solving ability.
","
Partner in building the infrastructure required for optimal extraction, transformation, visualization, and loading of data from a wide variety of data sources using SQL and big data technologies;
Improve our Machine Learning systems by contributing to all phases of algorithm development including ideation, prototyping, design and production;
Scale our data processing pipelines to handle and maintain complex processes in an efficient and reliable way that are available, scalable, and fault tolerant';
Build scalable production systems for data collection, data transformation, feature extraction, model training, and scoring, using distributed software tools;
Work with data science to define data ingestion standards and assist with data-related technical issues;
Partner with product development team to understand various opportunities and use cases;
Maintain specifications and metadata; create and maintain documentation;
Build Communities-of-Practice in key data technologies.
",None Found,None Found,"Avenue Code is the leading software consultancy focused on delivering end-to-end development solutions for digital transformation across every vertical. We’re privately held, profitable, and have been on a solid growth trajectory since day one. We care deeply about our clients, our partners, and our people. We prefer the word ‘partner’ over ‘vendor’, and our investment in professional relationships is a reflection of that philosophy. We pride ourselves on our technical acumen, our collaborative problem-solving ability, and the warm professionalism of our teams.

About the Opportunity:
We are seeking an energetic and talented Data Scientist Engineer to deliver high value, high-quality business capabilities to our data technology platform. With a background in both software development and machine learning, you will collaborate with software engineers, data scientists and domain experts in Supply Chain and Inventory management to develop data and machine learning products to support our company. You will be an integral member engineering team delivering across multiple business functional areas. You will build data analysis infrastructure for effective prototyping and visualization of various data-driven approaches.

Key Responsibilities:

Partner in building the infrastructure required for optimal extraction, transformation, visualization, and loading of data from a wide variety of data sources using SQL and big data technologies;
Improve our Machine Learning systems by contributing to all phases of algorithm development including ideation, prototyping, design and production;
Scale our data processing pipelines to handle and maintain complex processes in an efficient and reliable way that are available, scalable, and fault tolerant';
Build scalable production systems for data collection, data transformation, feature extraction, model training, and scoring, using distributed software tools;
Work with data science to define data ingestion standards and assist with data-related technical issues;
Partner with product development team to understand various opportunities and use cases;
Maintain specifications and metadata; create and maintain documentation;
Build Communities-of-Practice in key data technologies.

Technical Skills/Qualifications:

Experience building and maintaining large-scale analytics systems;
Experience in software development and delivery;
Expertise in one or more programming languages such as Python, Scala, Java, Javascript, etc. and related frameworks (Flask, Django, Spring, AngularJS, React).
Good knowledge of architecting, developing, and maintaining of cloud technologies (AWS, Azure, GCP)\nExperience with Big Data stack (Kafka, Hadoop, Storm, Spark, Hive, Airflow, ElasticSearch, TensorFlow);
Experience with relational (Postgres, MySQL) and/or non-relational (Cassandra, MongoDB) databases;
Strong machine learning or stats knowledge a plus;
Experience in Supply Chain / Retail / eCommerce;
Excellent communication skill and a strong team player.
BS/MS in Computer Science, Computer Engineering or related technical discipline;
Strong analytical and quantitative problem-solving ability.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records

Does this sound like you?
Apply now to become an Avenue Coder!"
201,Senior Data Engineer,"San Francisco Bay Area, CA",San Francisco Bay Area,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Location: Santa Clara or San Francisco

Upwork ($UPWK) is the world's largest freelancing website. Each year $1.7 billion of work happens through Upwork, allowing businesses to get more done and helping professionals break free of traditional time and place boundaries and work anytime, anywhere on projects they love. At Upwork, you'll help build on this momentum. Together, we'll create economic and social value on a global scale, providing a trusted online workplace for businesses to connect with extraordinary talent and work without limits.

Our Data Management team is tasked with building and supporting the business analytics infrastructure at Upwork that analysts and partners use on a daily basis to make critical business decisions. We are looking for a passionate engineer who has a love of data technologies to empower our teams in leveraging data to drive and measure impact. In this role, you will develop our next generation data infrastructure to support our future business growth.

Your responsibilities:
Based on Upwork’s unique ecosystem and data needs, partner with our architects to research new cloud-based data technologies and conduct PoC to make recommendations suitable for our overall blueprint of data infrastructure
Develop and maintain our end-to-end data platform, from data acquisition, real-time data stream, to data lake, enterprise data warehouse and front-end data tools and applications.
Create and evolve a data platform that enables your peer data management engineering teams to create data products that serve both internal business units and external clients and partners to maximize the value generated from our data assets in a user-friendly, self-serve, robust, efficient and scalable way.
Partner with cross-functional engineering peers to streamline the flow of data in the whole company
Provide guidance on development and implementation of data quality rules and validation processes, leading engineering excellence initiatives to improve data quality and issue resolution
What it takes to catch our eye:
8+ years combined experience in DW/BI and Big Data engineering role in medium and large size companies
A proven record of building complex data platform from ideation to implementation
Strong knowledge and experience of the Big Data ecosystem and Cloud-based solutions
Expert in designing, implementing, and operating efficient, scalable, and reliable data transformation pipelines
Solid experience in database modeling, architecture, design, and implementation
Familiar with data visualization tools such as Looker, Domo, or similar
Experience with marketplace businesses, especially as it pertains to working with large datasets inherent to two-sided marketplaces
Able to influence, lead, and communicate effectively across engineering teams, business units and other partners to negotiate priority, scope, and design solution
Excellent oral, written, and presentation skills, including the ability to work in person, virtually, and in a globally-staffed environment
Fluent and effective in working with a globally distributed team
Demonstrate a strong understanding of development processes and agile methodologies
Come change how the world works.

At Upwork you’ll help shape the future of work. From our offices in San Francisco, Mountain View and Chicago, together we’re creating exciting new opportunities for a world of professionals. You’ll be part of a vibrant culture built on shared values: Inspire a boundless future of work, Put our community first, Have a bias towards action, and Build amazing teams. Along the way you’ll have fun and enjoy the perks of a people-first company: Work from Home Wednesday's, daily breakfast and lunch, regular in-office happy hours, top-notch benefits … and more. Check out Upwork’s spotlight on The Muse for a glimpse of our daily work/life balance.

Upwork is proudly committed to recruiting and retaining a diverse and inclusive workforce. As an Equal Opportunity Employer, we never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics."
202,Senior Data Engineer,"San Francisco, CA",San Francisco,CA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"At Vonage, Business Intelligence is the team working on everything related to data, from ETL and data warehouses all the way to the reporting platform where other teams can build reports and get answers they need from our data.

Currently, we're looking for an experienced Data Engineer, to help us improve data processing systems that handle hundreds of GBs of data every day.

What will be your role

As a Senior Data Engineer, you'll become a senior member of the Business intelligence team, responsible for designing and building systems for processing the ~500GB of logs generated by the Nexmo API platform every day and loading it into our data warehouse and other systems which consume aggregated data.

As a Senior Data Engineer, you will:

Design and implement data loading and aggregation frameworks and jobs that will be able to handle hundreds of GBs of json files, using Python, Airflow and Snowflake
Build best practice ETLs to transform raw data into easy to use dimensional data for self service reporting
Improve our deployment and testing infrastructure within AWS, using tools like Jenkins, Puppet and Docker
Work closely with the Product, Infrastructure and Core teams, to make sure data needs are considered during product development and to guide decisions related to data
Mentor junior team members

----------------------------
What we're looking for
----------------------------

We're looking to strengthen our team by adding an experienced ETL / Data engineering professional, who has practical experience building performant data pipelines at scale, modeling data for self-serve use and working in a dynamic, startup-like environment.

Ideally, you'll have:

5 years+working experience building data processing, ETL and DWH solutions
Experience with software development using Python(preferred), Java or Scala
Experience working with relational databases (MySQL or Postgres preferred) and DWH technologies (Snowflake and Redshift preferred)
Experience working as part of a larger engineering team, within an agile framework and using version control systems
Familiarity with cloud technologies and working with cloud infrastructure (AWS preferred) and exposure to deployment and infrastructure automation would be highly beneficial
Exposure to distributed data processing systems with tools like Kubernetes, Hadoop, Spark, Kafka or ELK a large plus

----------
Why Vonage
----------

Now acquired and part of Vonage, Nexmo is a global API platform for cloud communications that handles over 90 million requests every day. Customers like Airbnb, Viber, Whatsapp, Snapchat, and many others depend on our APIs and SDKs to connect with their customers all over the world.

Company built by engineers
--------------------------


Nexmo was a tech company, built by engineers at the intersection of cloud and telecommunications and this is reflected everywhere in our culture.
Even though we've grown a lot in recent years, we've managed to maintain a startup-like working atmosphere, with very flat hierarchies and a very pragmatic approach to getting things done

Interesting engineering challenge
---------------------------------


We're in the process of migrating our DWH from Redshift to Snowflake, with a lot of room for re-design and refactoring along the way
We appreciate initiative and ownership, so you'll be encouraged to design, implement and own parts of our data processing system, end to end

Agile development using modern technologies
-------------------------------------------


We iterate quickly, use the scrum framework, do frequent code reviews and have a very open and pragmatic work culture within the team
Our data platform platform is built in the cloud, using technologies like Airflow, Snowflake, Kafka, S3, GCDS, ELK stack and Tableau

"
