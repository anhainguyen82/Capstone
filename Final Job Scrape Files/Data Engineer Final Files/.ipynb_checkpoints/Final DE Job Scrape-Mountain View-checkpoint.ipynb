{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=Mountain+View%2C+CA&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Engineer - Operations</td>\n",
       "      <td>Santa Clara, CA 95051</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>95051</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Master the MapR Converged Platform, including MapR-FS, MapR-DB Binary and JSON Tables, MapR-Streams and the Hadoop Eco-System products and maintain proficiency and currency as the technology evolves and advances.Achieve and maintain proficiency with MapR cluster and Hadoop Framework sizing, installation, debugging, performance optimization, cluster migration, security and automation.Achieve proficiency with MapR DB Binary and Json Tables sizing, performance tuning and multi-master replication.Achieve proficiency with MapR Event Stream sizing, performance tuning and multi-master replication.Assist the Sales Team (Sales Rep and Sales Engineer) in positioning and selling MapR products and Service offerings.Work closely with MapR sales in scoping and estimating projects.Write, deliver and present formal SOW’s (Statement of Work).Deliver SOW content in formal, hands-on, onsite customer engagements and ensure the on-time delivery and quality of MapR Professional Service engagements.Be a technical voice to MapR’s customers and community via blogs, Hadoop User Groups (HUG’s) and participation at leading industry conferences.Stay current in best practices, tools, and applications used in the delivery of professional service engagements.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years experience administering any flavor of LinuxStrong scripting skills (Bash or Python prefered)Familiarity with commercial IT infrastructures including storage, networking, security,</td>\n",
       "      <td>Job Description:\\nSenior Data Engineer- Operations\\nSenior Data Engineers will report into MapR’s Professional Services Organization. MapR Data Engineers are responsible for delivering a variety of engineering services to MapR customers throughout North America. Assignments will vary based on the candidate’s skills and experience. Typical assignments may involve cluster sizing, installation, configuration, health checks, performance tuning, data migration, security and automation. MapR’s growing customer base includes most of the Fortune 50 companies which makes the work assignments technically challenging yet rewarding. This role provides a significant opportunity to learn and apply big data technologies and solve related complex problems. MapR Data Engineers report to the Director of Data Engineering located at company headquarters in San Jose, CA.\\nResponsibilities:\\nMaster the MapR Converged Platform, including MapR-FS, MapR-DB Binary and JSON Tables, MapR-Streams and the Hadoop Eco-System products and maintain proficiency and currency as the technology evolves and advances.Achieve and maintain proficiency with MapR cluster and Hadoop Framework sizing, installation, debugging, performance optimization, cluster migration, security and automation.Achieve proficiency with MapR DB Binary and Json Tables sizing, performance tuning and multi-master replication.Achieve proficiency with MapR Event Stream sizing, performance tuning and multi-master replication.Assist the Sales Team (Sales Rep and Sales Engineer) in positioning and selling MapR products and Service offerings.Work closely with MapR sales in scoping and estimating projects.Write, deliver and present formal SOW’s (Statement of Work).Deliver SOW content in formal, hands-on, onsite customer engagements and ensure the on-time delivery and quality of MapR Professional Service engagements.Be a technical voice to MapR’s customers and community via blogs, Hadoop User Groups (HUG’s) and participation at leading industry conferences.Stay current in best practices, tools, and applications used in the delivery of professional service engagements.\\nRequirements:\\n5+ years experience administering any flavor of LinuxStrong scripting skills (Bash or Python prefered)Familiarity with commercial IT infrastructures including storage, networking, security,\\nvirtualization and systems management\\nFamiliarity with either Ansible, Puppet or ChefProficiency in basic Java or Scala programming preferred but not requiredBachelor's degree in CS or equivalent experienceFamiliarity with Hadoop and the Hadoop Eco-System framework a significant advantage\\n(MapR is willing to train otherwise promising candidates)\\nStrong verbal and written communication skills are requiredAbility to professionally manage multiple priorities with minimal supervision and deliver on\\nschedule\\nWillingness to travel about 70%\\nIdeal candidate will have any/all of the following:\\nRHCE certification, Bash or Python scripting, Automation using Ansible, Puppet or Chef, basic knowledge of Hadoop, Hive, or Spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biomedical Data Engineer - Health Technologies</td>\n",
       "      <td>Santa Clara Valley, CA 95014</td>\n",
       "      <td>Santa Clara Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>95014</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: Aug 21, 2019\\nWeekly Hours: 40\\nRole Number: 200092872\\nThe Health Technologies Team conceives and proves out innovative technology for Apple’s future products and features in health.\\nWe are seeking a highly capable Biomedical Data Engineer to join a multi-disciplinary team. Successful candidates will be able to integrate with our research study leads, data scientists, and engineers to develop and support effective data analysis and machine learning workflows.\\nKey Qualifications\\nExperience with software engineering frameworks:\\nExcellent coding skills in Python (e.g. Spark, Pandas, Jupyter )\\nWeb Service APIs (e.g., AWS, REDCap, XNAT)\\nDesigning and maintaining (non-)relational databases (e.g. Postgres, Cassandra, MongoDB)\\nLinux, MacOS based development frameworks\\nVersion control frameworks (Git, virtualenv)\\nExperience using task scheduling system (eg: Airflow, Luigi or equivalent)\\nFamiliarity with best practices for information security, including safe harbor privacy principles for sensitive data\\nExperience with biomedical sensors/platforms for measuring physiological signals (time-series data) in the health, wellness and/or fitness realms\\nDescription\\n• Work closely with team members and study staff to design, build, launch and maintain systems for storing, aggregating and analyzing large amounts of data\\n• Process, troubleshoot, and clean incoming data from human studies\\n• Create ETL pipelines to automate data ingestion and transformation, with hooks for QA, auditing, redaction and compliance checks per data management specifications\\n• Create and populate databases with existing and incoming clinical data\\n• Architect data models and create tools to harmonize disparate data sources\\n• Incorporate and comply with regulations as they pertain to electronic and clinical data and databases\\nEducation &amp; Experience\\nBS/MS in Computer Science, Engineering, Informatics, or equivalent with relevant 4+ years industry experience with biomedical, health or sensitive data.\\nAdditional Requirements\\nYou will thrive in our fast-paced environment if you are highly organized and able to multitask.\\nFlexible thinking, adaptability to change and comfort with ambiguity are hallmarks of successful people on our team.\\nWe look forward to witnessing your excellent communication and interpersonal skills during the interview process.\\nApple is known for heterogeneous and cohesive teams. A proven ability to work seamlessly with others is required to acclimate quickly to our culture.\\nWe highly value your analytical mind and problem-solving skills, and expect a stellar attention to detail.\\nYou will let the customer experience guide your decision making. You will design with Apple’s culture and values, inclusion for all and privacy, as fundamental requirements.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a candidate with 5+ years of experience in a Data Engineer role, with Bachelor or Master’s degree in Computer Science or related field\\n2+ years of hands on experience in building and optimizing big data pipelines using Apache Spark in Python programming languages preferred.\\n2+ years of hands on experience of building &amp; integrating data pipelines with machine learning models preferred.\\n2 years of experience with any of the cloud platforms such as AWS , Google, Azure etc. for building and deploying data pipelines preferred.\\nHighly desirable to have experience on data quality analysis and detection.\\nExperience with big data tools: Kafka, Cassandra, Hadoop, Storm and Spark\\nHighly Proficient in Python programming skills with Java programming language skills as a huge plus.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Design &amp; Build data pipelines for handling real-time data streams and integrating with machine learning models\\nUnderstand and analyze large, complex data sets to identify anomalies and data quality issues\\nDesign and Build Data quality and anomaly detection framework for IoT timeseries data\\nBuild data pipeline with Apache Spark\\nWork with cross functional stakeholders to assist with their data needs via optimal &amp; reusable data pipelines\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer\\nEquinix is one of the fastest growing data center companies, growing connectivity between clients worldwide. That’s why we're always looking for creative and forward-thinking people who can help us achieve our goal of global interconnection. With 200 data centers in over 24 countries spanning across 5 continents, we are home to the Cloud, supporting over 1000 Cloud and IT services companies that are directly engaged in technological innovation and development. We are passionate about further evolving the specific areas of software development, software and network architecture, network operations and complex cloud and application solutions.\\nAt Equinix, we make the internet work faster, better, and more reliably. We hire talented people who thrive on solving hard problems and give them opportunities to hone new skills, try new approaches, and grow in new directions. Our culture is at the heart of our success and it’s our authentic, humble, gritty people who create The Magic of Equinix. We share a real passion for winning and put the customer at the center of everything we do.\\nResponsibilities\\nDesign &amp; Build data pipelines for handling real-time data streams and integrating with machine learning models\\nUnderstand and analyze large, complex data sets to identify anomalies and data quality issues\\nDesign and Build Data quality and anomaly detection framework for IoT timeseries data\\nBuild data pipeline with Apache Spark\\nWork with cross functional stakeholders to assist with their data needs via optimal &amp; reusable data pipelines\\nQualifications\\nWe are looking for a candidate with 5+ years of experience in a Data Engineer role, with Bachelor or Master’s degree in Computer Science or related field\\n2+ years of hands on experience in building and optimizing big data pipelines using Apache Spark in Python programming languages preferred.\\n2+ years of hands on experience of building &amp; integrating data pipelines with machine learning models preferred.\\n2 years of experience with any of the cloud platforms such as AWS , Google, Azure etc. for building and deploying data pipelines preferred.\\nHighly desirable to have experience on data quality analysis and detection.\\nExperience with big data tools: Kafka, Cassandra, Hadoop, Storm and Spark\\nHighly Proficient in Python programming skills with Java programming language skills as a huge plus.\\nEquinix is an equal opportunity employer. All applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, or status as a qualified individual with disability.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Mountain View, CA 94043</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>94043</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nHave a passion for working on big data with professional experiences in data mining, statistical analysis, predictive modeling, and data manipulation\\nAdvanced degree (MS or PhD) in science, engineering, or statistics field\\n5+ years of experience in analyzing large data sets to drive significant business decisions for online/e-commerce domain\\n5+ years delivery experience in advanced modeling environment using Python, H2O, Spark, Java\\nStrong understanding of machine learning including supervised learning (Neural Network, Gradient Boosting, Logistics Regression) and unsupervised learning (clustering, entropy analysis, isolation forest, etc.)\\nStrong problem-solving and communication skills\\nData engineer experience is a plus\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nWork with business partners and stakeholders to understand the business, formulate the problems, and come up with the solutions\\nDesign, develop and implement advance machine learning models to solve E-commerce Fraud problems\\nWork with large volumes of data; extract and manipulate large datasets using standard tools such as Python, H2O, Hadoop, Spark, SQL, and Java\\nCollaborate with engineers to implement machine learning models on one of the best data platforms in the industry\\nAnalyze data to detect new fraud patterns and implement quick rules and solutions\\nMeasure and report the business impact of models and rules\\nCommunicate the concepts and results of the models and analyses to audience with different background\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Coupang is one of the largest and fastest growing e-commerce platforms on the planet. Our mission is to create a world in which Customers ask \"How did I ever live without Coupang?\" We are looking for passionate builders to help us get there. Powered by world-class technology and operations, we have set out to transform the end-to-end Customer experience -- from revolutionizing last-mile delivery to rethinking how Customers search and discover on a truly mobile-first platform. We have been named one of the \"50 Smartest Companies in the World\" by MIT Technology Review and \"30 Global Game Changers\" by Forbes.\\n\\nCoupang is a global company with offices in Beijing, Los Angeles, Seattle, Seoul, Shanghai, and Silicon Valley.\\n\\nJob Overview:\\nThe E-commerce Fraud Detection System team at Coupang is looking for a Principle/Senior Data Scientist with a solid background in machine learning, data science, data analysis, and data engineering. You will work closely with a team of data scientists, business analysts and engineers to ensure we detect frauds in e-commerce business and take actions to protect our business.\\n\\nKey Responsibilities:\\n\\nWork with business partners and stakeholders to understand the business, formulate the problems, and come up with the solutions\\nDesign, develop and implement advance machine learning models to solve E-commerce Fraud problems\\nWork with large volumes of data; extract and manipulate large datasets using standard tools such as Python, H2O, Hadoop, Spark, SQL, and Java\\nCollaborate with engineers to implement machine learning models on one of the best data platforms in the industry\\nAnalyze data to detect new fraud patterns and implement quick rules and solutions\\nMeasure and report the business impact of models and rules\\nCommunicate the concepts and results of the models and analyses to audience with different background\\n\\nQualifications:\\n\\nHave a passion for working on big data with professional experiences in data mining, statistical analysis, predictive modeling, and data manipulation\\nAdvanced degree (MS or PhD) in science, engineering, or statistics field\\n5+ years of experience in analyzing large data sets to drive significant business decisions for online/e-commerce domain\\n5+ years delivery experience in advanced modeling environment using Python, H2O, Spark, Java\\nStrong understanding of machine learning including supervised learning (Neural Network, Gradient Boosting, Logistics Regression) and unsupervised learning (clustering, entropy analysis, isolation forest, etc.)\\nStrong problem-solving and communication skills\\nData engineer experience is a plus\\n\\nPerks:\\n\\nAutonomy to make decisions in a rapidly growing company\\n15 days PTO + 15 national holidays off\\n401K matching\\nPre-IPO stock options\\nMobile &amp; fitness reimbursement\\nFree Catered Lunch daily\\n\\nCoupang is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex or gender (including pregnancy, gender identity, gender expression, sexual orientation, transgender status), national origin, age, disability, medical condition, HIV/AIDS or Hepatitis C status, marital status, military or veteran status, use of a trained dog guide or service animal, political activities, affiliations, citizenship, or any other characteristic or class protected by the laws or regulations in the locations where we operate.\\n\\nIf you need assistance and/or a reasonable accommodation in the application or recruiting process due to a disability, please contact us at usrecruiting@coupang.com ( usrecruiting@coupang.com ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Campbell, CA</td>\n",
       "      <td>Campbell</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Us\\n--------\\n\\nWith electric vehicles (EVs) expected to be 25% of vehicle sales by 2025 and more than 50% by 2040, electric mobility is in the midst of a tipping point and becoming reality. ChargePoint is at the center of the e-mobility revolution, powering the world's leading EV charging network and most complete set of hardware, software and mobile solutions for every EV charging need. Whether it's to ride, driver or deliver on electric fuel, we bring together people, vehicle fleets, businesses, automakers, policymakers, utilities and other stakeholders to make e-mobility a reality globally.\\n\\nOur fanatical focus on charging and 10+ years in business has made us an industry leader. Supported by more than half a billion dollars in funding from investors, including Quantum Energy Partners, GIC, Clearvision, Daimler Trucks &amp; Buses, Daimler, Siemens, Linse Capital, American Electric Power, Canada Pension Plan Investment Board, Chevron Technology Ventures, Rho Capital Partners, BMW i Ventures and Braemar Energy Ventures, ChargePoint offers a once-in-a-lifetime chance to be part of creating an all-electric future and shaping a trillion-dollar market. Join the team that built the EV charging industry and make your mark on how people and goods will get everywhere they need to go, in any context, for generations to come.\\n\\nReports To\\n----------\\n\\nSoftware Engineering Manager\\n\\nWhat You Will Be Doing\\n----------------------\\n\\nBeing a leading EV charging network we have a lot of data that is used to enhance end-user experience, optimize network operations and placement of charging stations, and improve internal operations and manufacturing processes. This is just the beginning and we see a huge opportunity to leverage data to predict failures before they happen, build delightful products and enable electrification of all forms of transportation.\\n\\nA Data Engineer in this role will work with several teams across the company to identify use cases/scenarios for optimization, build a data lake to enable efficient querying and analytics, and use data science and ML to analyze the data and build data solutions.\\n\\nWhat You Will Bring to ChargePoint\\n----------------------------------\\n\\n\\nFundamental understanding of data science and ML\\nExperience building data pipelines, analyzing big data &amp; building data solutions\\nBias to action\\nBe an evangelist for data analysis and data-oriented decision making\\n\\nRequirements\\n------------\\n\\n\\n1-3 years of object-oriented programming &amp; SQL experience\\n1-3 years of experience with big data technologies (e.g., Hadoop, Spark, etc.) and data viz and ETL tools (e.g., Pentaho, Tableau etc.)\\nRelevant coursework and experience in data science and ML\\nSolid software engineering fundamentals\\nFundamental understanding of RDBMS, NoSQL databases and big data solutions\\nAbility to work with ambiguity, create quick PoCs and engineer an E2E solution\\nStrong scripting or programming skills for automating repetitive tasks\\n\\nPreferred\\n---------\\n\\n\\nExperience with AWS\\nExperience building data lake or data solutions from scratch\\n\\nLocation\\n--------\\n\\nCampbell, CA\\n\\nWe are committed to an inclusive and diverse team. ChargePoint is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status.\\n\\nIf there is a match between your experiences/skills and the Company needs, we will contact you directly.\\n\\nChargePoint is an equal opportunity employer.\\nApplicants only - Recruiting agencies do not contact.\\n\\n#LI-SH1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Sunnyvale, CA 94087</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>94087</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Strong quantitative background (Computer Science, Math, Physics, and/or Engineering), or at least 5 years of industry experience in a quantitative roleExperience programming with any of the following: Python, Go, C++, Scala, JavaExperience establishing pipelines using distributed processing and no-SQL technologies such as Apache Spark, Kafka, Elasticsearch, Presto, MongoDBEffectively participate in the team’s planning, code reviews, KPI reviews, and design discussions leading to continuous improvement in these areas.You are passionate about working across the entire technology stack, from hardware, to embedded SW, to local software, to cloud-based services to optimize capability and drive reliability.</td>\n",
       "      <td>Strong quantitative background (Computer Science, Math, Physics, and/or Engineering), or at least 5 years of industry experience in a quantitative roleExperience programming with any of the following: Python, Go, C++, Scala, JavaExperience establishing pipelines using distributed processing and no-SQL technologies such as Apache Spark, Kafka, Elasticsearch, Presto, MongoDBEffectively participate in the team’s planning, code reviews, KPI reviews, and design discussions leading to continuous improvement in these areas.You are passionate about working across the entire technology stack, from hardware, to embedded SW, to local software, to cloud-based services to optimize capability and drive reliability.</td>\n",
       "      <td>Build highly scalable data pipelines to handle ingestion and processing of robot, manufacturing, &amp; engineering dataEnable internal users by giving them APIs, services, and applications to allow them to access and interact with their dataWork closely with core engineering teams to consistently evolve data models &amp; data schemas based on growing business and engineering needsBe a leader in the data space and help people understand what they could be getting from their data that they currently are not</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job: Engineering\\nPrimary Location: United States-California-US-CA-Sunnyvale\\nSchedule: Full-time\\nRequisition ID: 191586\\n\\nDescription\\n\\nCompany Description:\\nWho is Intuitive Surgical? The numbers tell an amazing story. Learn more about our company.\\n\\nJoining Intuitive Surgical means joining a team dedicated to using technology to benefit patients by improving surgical efficacy and decreasing surgical invasiveness, with patient safety as our highest priority.\\n\\nPatients First. Always. We build the world’s best surgical robotic systems.\\nOur surgical robots are deployed worldwide and help nearly a million people per year be cured of cancer and other ailments. Those people get back to their families and lives faster with less pain.\\n\\nAre you passionate about technology? Do you want to have a direct impact on helping people live better lives?\\n\\nAbout the role:\\nWe’re looking for a Senior Data Engineer who is passionate and motivated to make an impact in creating a robust and scalable data platform for product analytics. You will have ownership of core engineering data pipelines that power metrics for engineering, manufacturing, post-market, marketing, and business teams with the purpose of driving data discoverability, accessibility, and analysis. You will be a catalyst of change and will be responsible for helping to re-invent the way the Instruments &amp; Accessories business unit uses data. To be successful in the long run you will need to build sophisticated systems and retool antiquated processes to enable the broader engineering teams to move quickly with confidence.\\n\\nResponsibilities:\\nBuild highly scalable data pipelines to handle ingestion and processing of robot, manufacturing, &amp; engineering dataEnable internal users by giving them APIs, services, and applications to allow them to access and interact with their dataWork closely with core engineering teams to consistently evolve data models &amp; data schemas based on growing business and engineering needsBe a leader in the data space and help people understand what they could be getting from their data that they currently are not\\nQualifications\\n\\nSkills, Characteristics, and Technology:\\nStrong quantitative background (Computer Science, Math, Physics, and/or Engineering), or at least 5 years of industry experience in a quantitative roleExperience programming with any of the following: Python, Go, C++, Scala, JavaExperience establishing pipelines using distributed processing and no-SQL technologies such as Apache Spark, Kafka, Elasticsearch, Presto, MongoDBEffectively participate in the team’s planning, code reviews, KPI reviews, and design discussions leading to continuous improvement in these areas.You are passionate about working across the entire technology stack, from hardware, to embedded SW, to local software, to cloud-based services to optimize capability and drive reliability.\\n\\nBonus points:\\nExperience with the following: Apache Airflow, Elasticsearch, Presto, Snowflake, Kubernetes, PandasExperience installing, configuring, and managing AWS or on-premises server infrastructureExperience working with Docker development and deployment workflows\\nAbout the Advanced Analytics Team:\\nWe are small group looking for strong technical leads that want an opportunity to move fast and help develop data platforms and analytics that will enable broader engineering teams to create better products for our patients. This is a new group at the company and our mandate is to promote significant change that will improve the productivity of our engineering teams in analyzing and understanding our products. We have the potential to overlap with many teams and as such emphasize tearing down organizational silos and a strong culture of collaboration. Nothing is taboo. If you see a high-leverage solution to a long-standing problem, tear it all down and build the right solution provided you have the will and a good plan to execute!\\n\\nWe are an AA/EEO/Veterans/Disabled employer.\\nWe will consider for employment qualified applicants with arrest and conviction records in accordance with fair chance laws.\\n\\n#RH1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Redwood City, CA</td>\n",
       "      <td>Redwood City</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nCloud solution implementation experience with Azure Data Lake and Spark preferred\\nMinimum 8 years hands-on experience with SQL\\nAt least one year of experience in scripting languages such as Python\\nDemonstrated experience in a cloud-based -computing environment such as AWS, Azure, or Google Cloud Platform\\nBig data processing techniques, preferred\\nCan work independently in ambiguous environment</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Lead Data Engineer\\n\\nSan Francisco, CA\\n\\nAbout the role. . .\\nIn order to continue and accelerate our growth, we are looking for a Lead Data Engineer with Cloud Solutions background to add to our Bay Area-based team.\\nLead Engineer is responsible for building a large-scale data pipeline in cloud platform. This may involve in automation of manual processes to cloud environment. Candidate would direct the initiatives for creation of data sets. Delivering client value and ensuring high client satisfaction.\\nCore responsibilities for this position include, but are not limited to the following:\\nExtracts data from various databases; perform exploratory data analysis, cleanses, massages, and aggregates data\\nEmploys scaling &amp; automation to data preparation techniques - Introduces incremental improvements to data analysis, visualization, and presentation techniques to communicate discoveries\\nResearches relevant emerging empirical methods and quantitative tools\\nPossesses in-depth business knowledge in order to initiate and drive discussions with business partners to identify business issues needing analytic solutions\\nLeads innovative packaging and presentation of insights to business and broader analytics community\\nDevelops processes to automate and scale insights operationalization\\nDevelops and drives multiple cross-departmental projects\\nEstablishes brand and team as subject matter experts in advanced analytics across departments.\\nMentors data scientists in pioneering techniques and business acumen\\nRequired Qualifications:\\nCloud solution implementation experience with Azure Data Lake and Spark preferred\\nMinimum 8 years hands-on experience with SQL\\nAt least one year of experience in scripting languages such as Python\\nDemonstrated experience in a cloud-based -computing environment such as AWS, Azure, or Google Cloud Platform\\nBig data processing techniques, preferred\\nCan work independently in ambiguous environment\\nAbout Logic20/20. . .\\nLogic20/20 is one of Seattle’s fastest growing full service consulting firms. Our core competency is creating simplicity and efficiency in complex solutions. Although we make it look like magic, we succeed by combining methodical and structured approaches with our substantial experience to design elegant solutions for even the most intricate challenges. Our rapid growth is in response to our ability to deliver consistently for our clients, which is directly related to the quality of the people we hire.\\nThe past four years, we’ve been in the top 10 “Best Companies to Work For” ….. why? Our team members are highly self-motivated, comfortable conceiving strategies on the fly, and enjoy working both individually and as part of a team. Our environment is very high-energy and demanding, and individuals with remarkable enthusiasm and a can-do attitude are joining our team. We have lots of fun, focus on our employees and our clients, and work to bring our best to every opportunity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Big Data Engineer</td>\n",
       "      <td>San Jose, CA 95134</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95134</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nYou will design and create multi-tenant systems capable of loading and transforming a large volume of structured and semi-structured fast moving data\\nBuild robust and scalable data infrastructure (both batch processing and real-time) to support needs from internal and external users\\nBuild Data Pipelines\\nRun ETL into Hadoop/Elastic Search</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Location: San Jose, CA or Atlanta, GA\\nFor over 10 years, Zscaler has been disrupting and transforming the security industry. Our 100% purpose built cloud platform delivers the entire gateway security stack as a service through 150 global data centers to securely connect users to their applications, regardless of device, location, or network in over 185 countries protecting over 3,500 companies and 100 Million threats detected a day.\\nWe work in a fast paced, dynamic and make it happen culture. Our people are some of the brightest and passionate in the industry that thrive on being the first to solve problems. We are always looking to hire highly passionate, collaborative and humble people that want to make a difference.\\nAs a Big Data Engineer, you will work on building the next generation of Zscaler's security analytics platform. You will play a crucial role in building a platform to collect and ingest several billion (and growing) log events from Zscaler's globally distributed security infrastructure and provide actionable insights to customers and Zscaler's security researchers.\\nResponsibilities:\\nYou will design and create multi-tenant systems capable of loading and transforming a large volume of structured and semi-structured fast moving data\\nBuild robust and scalable data infrastructure (both batch processing and real-time) to support needs from internal and external users\\nBuild Data Pipelines\\nRun ETL into Hadoop/Elastic Search\\nRequired:\\n3+ years of experience in Python or Java development a must (Strong Scala would skills would be acceptable as well)\\n3+ years experience in application big data development (Spark, Kafka, Storm, Kinesis, &amp; building data pipelines)\\nAbility to troubleshoot and find complex performance issues with queries on the Spark platform (Spark SQL)\\nFamiliarity with implementing services following REST model\\nExcellent interpersonal, technical and communication skills\\nAbility to learn, evaluate and adopt new technologies\\nBachelor's Degree in computer science or equivalent experience\\nHighly Desirable:\\nExperience working with data processing infrastructure\\nExperience with data serialization techniques and data stores for persisting events\\nWhat You Can Expect From Us:\\nAn environment where you will be working on cutting edge technologies and architectures\\nA fun, passionate and collaborative workplace\\nCompetitive salary and benefits, including equity\\nThe pace and excitement of working for a Silicon Valley Unicorn\\nWhy Zscaler?\\nPeople who excel at Zscaler are smart, motivated and share our values. Ask yourself: Do you want to team with the best talent in the industry? Do you want to work on disruptive technology? Do you thrive in a fluid work environment? Do you appreciate a company culture that enables individual and group success and celebrates achievement?\\n\\nIf you said yes, we’d love to talk to you about joining our award-winning team!\\nLearn more at zscaler.com or follow us on Twitter @zscaler. Additional information about Zscaler (NASDAQ : ZS ) is available at http://www.zscaler.com. All qualified applicants will receive consideration for employment without regard to race, sex, color, religion, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability.\\n#LI-JM1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Principal Application Data Engineer - SQL Specialist</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>You are intellectually curious. You like building cool stuff. And you're nice.\\n\\nAt Noodle, we are not just building AI applications. We are going deep into industries that have yet to leverage AI at scale such as steel mills, distribution &amp; logistics companies or consumer packaged goods. Our applications fit and integrate deeply into the supply chain starting with planning to manufacturing to delivery. The applications we build have to not only integrate into the existing software in these industries but have to talk to each other so really drive the value from AI. Turns out, we are one of the pioneers here charting a new course. This means that science behind building the software and the AI behind it has not settled. You will be part of a team that is charting this new course figuring out how to adapt software engineering best practices to delivering AI applications that fit within legacy software in non-tech industries. This is going to be a bumpy ride and we are looking for people who are not afraid of the unknown, are experts at their craft and can adapt and learn as we create a suite of new AI applications.\\n\\nResponsibilities\\n\\n\\nWork collaboratively with an interdisciplinary team of management consultants, product managers, data scientists, data engineers, software engineers, UX designers to understand application data engineering requirements &amp; convert requirements into functional &amp; technical specifications.\\nDrive rapid application prototyping cycles from technical requirements, data-pipeline design, programming, debugging, and performance optimization.\\nDesign, develop and evolve Noodle's AI Application Databases, Data Models &amp; Data Pipelines.\\nAnalyze complex Enterprise data, exploring entity relationships, performing data quality checks &amp; validations.\\nChampion engineering and operational excellence, establishing metrics and process for regular assessment and improvement as well as mentoring of Junior Data Engineers.\\nProvide thought leadership and improve Noodle's Data Architecture interactively through innovations and adaptation of best practices &amp; industry standards.\\n\\nQualifications\\n\\n\\nBS/BE/B.Tech or Advanced degree in a relevant field (Computer Science and Engineering, Technology and related fields). Master' s degree a plus.\\n6-9 years of industry experience in data driven software development.\\n7+ years of real-world experience in architecting &amp; developing scalable data driven AI applications, data warehousing &amp; business intelligence solutions.\\nExpert in writing SQL queries, procedures and user-defined functions.\\nDemonstrated proficiency as a lead with database development, automation, performance tuning, optimization and management projects.\\nReal life work experience with relational databases: PostgreSQL, MS SQL Server, Oracle etc.\\nStrong understanding of Big Data concepts and scenarios where it works. This includes understanding of the nature of distributed systems and its pitfalls.\\nStrong knowledge of database design and data modeling concepts.\\nWorking knowledge of database security and data encryption.\\nExperience working with version control systems like: bitbucket, github, etc.\\nExperience working in a hybrid environment involving on-premise infrastructure and cloud databases will be a plus.\\nHands-on experience with modern ETL tools, orchestration platform like Airflow and Python data processing libraries will be considered as an added advantage.\\nFamiliarity with agile software development practices and DevOps is desirable.\\n\\nPassion for learning and a desire to grow – Noodlers are life-long learners!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Integrated Marketing Analytics Manager</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Department: Marketing &amp; Customer Insights (MCI)\\nTitle: Integrated Marketing Analytics Manager\\nPosition Role/Description:\\nThe Marketing &amp; Customer Insights (MCI) organization has a dual mandate of providing objective customer research, analysis, and marketing effectiveness measurement while advancing the use of Adobe Marketing Cloud technologies to enable and track customer experiences across surfaces.\\nWithin MCI, the Advanced Analytics team was established to focus on developing deep customer insights to support integrated marketing planning across channels. The group closely partners with Global and Field Marketing, IT, business unit leaders and other corporate functions to enhance understanding of our customers and their digital journey.\\nThe team maintains a highly visible and strategically important role in delivering and facilitating understanding of insights to inform business strategies, and to track the performance of marketing specific activities against expectations. The work the team delivers is informed by the business needs for strategic customer understanding, and may range from media investment performance analysis, deep customer journey investigations, to customer segmentation or targeting overlays.\\nThe talent mix of the team is part analyst and part data engineer. This role will focus on the data side. We are looking for a self-starter with marketing science experience and a wide range of technical skills to advance our analytics and execution capabilities. This includes advancing our marketing measurement strategy across all of our online and offline marketing channels. You will also develop our next generation of data and analytics stacks to guide our customer insights work.\\nA balance of technical and analytics skills as well as a working knowledge of marketing processes is key. A team-player, collaborative, mindset is essential.\\nResponsibilities:\\n· Ability to structure problems into data and analytics plan and execute.\\n· Hands-on creation of time-series models\\n· Ability to partner with a cross-functional team of technical and analytical partners\\n· Possible engagement with Adobe Marketing Cloud teams to evolve capabilities within the products.\\n· An understanding of effective visualization techniques for complex concepts or a background in visualizing “big data” use cases.\\n· Partner with media teams, campaign strategy teams, marketing and web analysts to gather inputs to inform business problems and analytical approaches.\\n· Provide ad hoc analysis and communicate results and insights with team and executives\\nWhat you need to succeed:\\n· Intellectual curiosity, flexibility, and high attention to detail\\n· Undergraduate degree in a quantitative field preferred\\n· Experience with SQL, R, Python\\n· Project/program management experience\\n· Good intuition with information and data; experience synthesizing large data sets to generate insights\\n· Outstanding communication skills to provide actionable reporting via written and in-person presentations. Must be able to clearly communicate results and actionable recommendations to constituencies at various levels of management\\n· Strong interpersonal skills and the ability to work as part of a diverse team and build cooperative, productive relationships with colleagues and stakeholders around the world\\nDesirable Skills:\\n· Understanding in one or more marketing channels (e.g. web, mobile, display, search)\\n· Experience in machine learning tools and techniques\\n· Experience developing and working on larger scale analytics / big data implementations\\n· Track record of consistent delivery and execution\\nAt Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.\\nIf you’re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.\\nAdobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Engineer- Python</td>\n",
       "      <td>San Jose, CA 95113</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95113</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of experience in core JAVA and SQL</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a Senior Consultant, you will focus on managing the information supply chain from acquisition to ingestion, storage and the provisioning of data to points of impact by modernizing and enabling new capabilities. Information value is enhanced through enterprise-scale applications that enable visualization, consumption and monetization of both structured and unstructured data. Big data is becoming one of the most important technology trends that has the potential for dramatically changing the way organizations use information to enhance the customer experience and transform their business models.\\nWork you'll do\\n\\nSenior Consultants work within an engagement team. Key responsibilities will include:\\n Function as integrators between business needs and technology solutions, helping to create technology solutions to meet clients’ business needs.\\n Identifying business requirements, requirements management, functional design, prototyping, process design (including scenario design, flow mapping), testing, training, defining support procedures and supporting implementations.\\n\\nThe Team\\n\\nAnalytics &amp; Cognitive\\n\\nIn this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.\\n\\n\\nThe Analytics &amp; Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy &amp; Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.\\n\\n\\nAnalytics &amp; Cognitive will work with our clients to:\\n Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms\\n Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions\\n Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements\\n\\n\\nQualifications\\n\\nRequired:\\n\\n 5+ years of experience in core JAVA and SQL\\n 3+ years of experience in Python&amp; Unix Shell Scripting\\n 3+ years of experience in building scalable and high performance data pipelines using Apache Hadoop, Map Reduce, Pig &amp; Hive\\n Experience with bigdata cross platform compatible file formats like Apache Avro &amp; Apache Parquet\\n Experience in Apache Spark is a plus\\n 1+ years of experience with data lake implementations, core modernizations and data ingestion\\n\\n 1 or more years of hands on experience designing and implementing data ingestion techniques for real time and batch processes for video, voice, weblog, sensor, machine and social media data into Hadoop ecosystems and HDFS clusters.\\n 2+ years of experience leading workstreams or small teams\\n Willingness for weekly client-based travel, up to 80-100% (Monday — Thursday/Friday)\\n Bachelor’s Degree or equivalent professional experience\\n\\n Preferred:\\n\\nAWS Certification, Hadoop Certification or Spark Certification\\nExperience with Cloud using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)\\nExperience with data integration products like Informatica Power Center Big Data Edition (BDE), IBM BigInsights, Talend etc.\\nExperience designing and implementing reporting and visualization for unstructured and structured data sets\\nExperience in designing and implementing scalable, distributed systems leveraging cloud computing technologies like AWS EC2, AWS Elastic Map Reduce and Microsoft Azure\\nExperience designing and developing data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching\\nKnowledge of data, master data and metadata related standards, processes and technology\\nExperience working with multi-Terabyte data sets\\nExperience with Data Integration on traditional and Hadoop environments\\nAbility to work independently, manage small engagements or parts of large engagements.\\nStrong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint).\\nStrong problem solving and troubleshooting skills with the ability to exercise mature judgment.\\nEagerness to mentor junior staff.\\nAn advanced degree in the area of specialization is preferred.\\n\\nHow you’ll grow\\n\\nAt Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.\\n\\n\\nBenefits\\n\\nAt Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.\\nDeloitte’s culture\\n\\nOur positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.\\n\\n\\nCorporate citizenship\\n\\nDeloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.\\n\\n\\nRecruiter tips\\n\\nWe want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.\\n\\n#LI:PTY\\n#IND:PTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Senior Data Engineer - ETL Specialist</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>You are intellectually curious. You like building cool stuff. And you're nice.\\n\\nAt Noodle, we are not just building AI applications. We are going deep into industries that have yet to leverage AI at scale such as steel mills, distribution &amp; logistics companies or consumer packaged goods. Our applications fit and integrate deeply into the supply chain starting with planning to manufacturing to delivery. The applications we build have to not only integrate into the existing software in these industries but have to talk to each other so really drive the value from AI. Turns out, we are one of the pioneers here charting a new course. This means that science behind building the software and the AI behind it has not settled. You will be part of a team that is charting this new course figuring out how to adapt software engineering best practices to delivering AI applications that fit within legacy software in non-tech industries. This is going to be a bumpy ride and we are looking for people who are not afraid of the unknown, are experts at their craft and can adapt and learn as we create a suite of new AI applications.\\n\\nResponsibilities\\n\\n\\nWork collaboratively with an interdisciplinary team of management consultants, product managers, data scientists, data engineers, software engineers, UX designers to understand and implement application data engineering requirements.\\nExecute rapid application prototyping cycles from breaking down technical requirements in tasks, data pipeline design, programming and debugging to optimization of database code.\\nOrchestrate complex Enterprise data processing workflows, performing data quality checks &amp; validations.\\nLiaise with Customer teams &amp; Internal stakeholders on the Application data pipeline requirements.\\nChampion engineering and operational excellence, follow best practices and coding guidelines to deliver highly scalable application data processing components.\\nHelp improve Noodle Data Pipeline Architecture through innovations and adaptation of best practices &amp; industry standards.\\n\\nQualifications\\n\\n\\nBS/BE/B.Tech or Advanced degree in a relevant field (Computer Science and Engineering, Technology and related fields). Master' s degree a plus.\\n6+ years of industry experience in data driven software projects where 4+ years are on developing real-world scalable data pipelines to ingest, including data quality validations.\\nDemonstrated proficiency with modern ETL tools / frameworks and data pipeline orchestration platforms like Airflow.\\nHands-on work experience in proficient querying - relational databases like PostgreSQL, MS SQL Server, Oracle, etc.\\nWorking knowledge of Python (scripting), bitbucket or github (version control.)\\nComfortable working with both high performance on-premises database installations and cloud instances (AWS or Azure) is a plus.\\nGood knowledge of database design and security, data modeling concepts and data encryption, data devops is a huge plus.\\n\\nPassion for learning and a desire to grow – Noodlers are life-long learners!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience with Cloud-based service and development environment, such as AWS or GCP\\nProficiency with programming languages such as Python and Java\\nProficient understanding of distributed computing principles\\nGood knowledge of Big Data querying databases, such as BigQuery, BigTable and MongoDB</td>\n",
       "      <td>\\nExperience with Cloud-based service and development environment, such as AWS or GCP\\nProficiency with programming languages such as Python and Java\\nProficient understanding of distributed computing principles\\nGood knowledge of Big Data querying databases, such as BigQuery, BigTable and MongoDB</td>\n",
       "      <td>\\nSelecting and integrating big data frameworks required to provide requested capabilities\\nBuild analysis pipelines as well as visualization dashboards\\nMonitoring performance and iterate the solution fast</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Shape Security\\n\\nWe are security and web experts, pioneers, evangelists, and elite researchers. We believe in the power of the Internet to be a positive force; our mission is to protect every website and mobile app from cybercriminals. Shape’s founders fought cybercrime at the Pentagon, Google, and other leading security companies. We are backed by some of the most prominent leaders and investors in the technology industry including Kleiner Perkins, Google Ventures, and more. Come be a part of our unparalleled team that is responsible for making the Internet a safer place for everyone.\\n\\nPosition Summary\\n\\nWe are looking for a Big Data Engineer that will work on the storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company.\\nResponsibilities\\nSelecting and integrating big data frameworks required to provide requested capabilities\\nBuild analysis pipelines as well as visualization dashboards\\nMonitoring performance and iterate the solution fast\\nSkills and Qualifications\\nExperience with Cloud-based service and development environment, such as AWS or GCP\\nProficiency with programming languages such as Python and Java\\nProficient understanding of distributed computing principles\\nGood knowledge of Big Data querying databases, such as BigQuery, BigTable and MongoDB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Pleasanton, CA 94588</td>\n",
       "      <td>Pleasanton</td>\n",
       "      <td>CA</td>\n",
       "      <td>94588</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join our team and experience Workday!\\nIt's fun to work in a company where people truly believe in what they're doing. At Workday, we're committed to bringing passion and customer focus to the business of enterprise applications. We work hard, and we're serious about what we do. But we like to have a good time, too. In fact, we run our company with that principle in mind every day: One of our core values is fun.\\nJob Description\\nJob Responsibilities:\\nAnalyze, Design and Build robust and scalable Data Engineering solutions for structured and unstructured data conducive to Business Insights, Machine Learning, Reporting and Analytics.\\nConsistently build and manage evolving Data Model &amp; Data Schema based on business and engineering requirements.\\nBuild Scalable and reusable Abstractions/Data Pipelines to create, ingest or access data sets (batch, streaming or low latency APIs).\\nImprove the data quality and reliability of the pipelines using innovative solutions (monitoring and failure detection).\\nEvaluate new technologies and build prototypes for continuous improvements in Data Engineering.\\nWork effectively using scrum with multiple team members to deliver analytical solutions to the business and Engineering functions.\\nProvide technical leadership, mentor other Engineering team members and manage a small team of engineers.\\nMaintain high level of technical skills and strive to expand on technical knowledge through self-education or company provided education.\\nDemonstrated academic or professional Statistics and Mathematics skills.\\nJob Skills:\\nExperience with designing complex Data Models and Data Engineering Solutions for large scale Data Warehouse/Data Lakes from various heterogeneous Data Sources (Salesforce, Eloqua, Workday, Marketo, Gainsight, Adobe Analytics or other Sales, Marketing, Finance and Human Capital Management Applications).\\nExperience with Data Integration, Business Intelligence and Analytics tools like Pentaho Data Integration, Snaplogic Data Integration, Tableau Software and other open source and self-service analytics tool.\\nStrong Database Management system knowledge; Experience with Microsoft SQL Server, Amazon Redshift, Amazon Sagemaker, PostgreSQL, MySQL, Oracle and NoSQL Databases are Preferred.\\nExpertise in Object Oriented and/or Functional Programming skills (Java/Scala, Microsoft .Net, Espresso, Python, Unix/Linux/Perl scripting, SQL/PLSQL)\\nExperience with Agile development methodologies (Scrum, Pair Programming). Certified Scrum Product Owner Certification will be Preferred.\\nMinimum Education Requirements:\\nBachelor’s/Master’s degree in Computer Science, Computer Engineering, Electrical Engineering, or related field of study.\\nMinimum Experience Requirements:\\n8+ years of experience with very large scale and complex Data Management Projects\\n#LI-SB7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sr Data Engineer - HPC UI</td>\n",
       "      <td>Sunnyvale, CA 94086</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>94086</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description:\\nSenior Data Engineer – High Performance Computing UI\\nTarget Data Sciences is revolutionizing the way Target retail uses data. Located in Sunnyvale, CA, it’s just across the street from the local train station in the heart of Silicon Valley. Originally opened in 2014, the Sunnyvale office is now home to more than 150 team members who work to make Target a more modern data-driven retail company. To learn more, view this article: https://corporate.target.com/article/2016/11/meet-target-sunnyvale\\nTeam Introduction\\nThe High-Performance Computing Group at Target not only aims to enable teams at Target to stream, filter, transform, and analyze high-bandwidth data in real-time, but also provide tools for data analysts and other team members to analyze and take action on their data streams.\\nWhat will you be doing:\\nAs a Senior UI Engineer of the High-Performance Computing Research Group at Target, you are to design and implement user experiences that enable data analysts and engineers to use our high-performance streaming platform to its maximum potential. You’ll receive hands-on experience and exposure to designing and building real-time, state dependent, data driven UI applications.\\nResponsibilities:\\nArchitect new features and reusable UI components using ReactJS\\nCollaborating with team in coming up with neat and usable user interfaces\\nOptimize UI applications for scalability, stability, and speed.\\nWhat you will learn:\\nYou will architect, build, and maintain our UI application, meaning you’ll be receive exposure to a large variety of cutting edge front-end/data technologies.\\nYou will get exposure to the full-stack of software systems (down to the computer architecture level) while collaborating with our talented engineers to tackle challenging problems.\\nRequirements:\\nBachelor’s degree (preferably in CS, EE or related field)\\nExperience in developing real-world UI applications\\nStrong Proficiency in HTML5, CSS3, and JavaScript.\\nGood understanding of RESTFUL APIs and asynchronous request handling.\\nExperience or exposure to front-end tooling such as package managers, CSS-preprocessing, file-bundlers such as webpack etc.\\nAbility to use code version tool such as Git.\\nExperience with ReactJS.\\nA natural talent for design and attention to detail\\nPreferred Requirements:\\nSome exposure to low level programing languages and system/computer architecture knowledge.\\nWire-framing and UI design skills\\nAmericans with Disabilities Act (ADA)\\nTarget will provide reasonable accommodations (such as a qualified sign language interpreter or other personal assistance) with the application process upon your request as required to comply with applicable laws. If you have a disability and require assistance in this application process, please visit your nearest Target store or Distribution Center or reach out to Guest Services at 1-800-440-0680 for additional information.\\nQualifications:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Newark, CA</td>\n",
       "      <td>Newark</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor or Masters in Software Engineering or Computer Science\\n+4 years of experience in Data Engineering and Business Intelligence.\\nProficient in IoT tools such as MQTT, Kafka, Spark\\nProficient with AWS, S3, Redshift\\nExperience with Presto and Parquet/ORC\\nProficient with Apache Spark and data frame.\\nExperienced in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python,\\nExperience with Columnar database such as Redshift, Vertica\\nGreat verbal and written communication skills.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Leading the future of luxury mobility\\n\\nLucid’s mission is to inspire the adoption of sustainable energy by creating the most captivating luxury electric vehicles, centered around the human experience. Working at Lucid Motors means having a shared vision to power the future in revolutionary ways. Be part of a once-in-a-lifetime opportunity to transform the automotive industry.\\n\\nWe are looking for a Data Engineer, Streaming who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If building large and building fast, working with a very talented team of engineers and collaborating with the brightest mind in the Automotive industry is what you like, Lucid is the best to experience it.\\nThe Role\\nHands-on design and develop streaming and IoT data pipelines.\\nDeveloping streaming pipeline using MQTT, Kafka, Spark Structure Streaming\\nOrchestrate and monitor pipelines using Prometheus and Kubernetes\\nDeploy and maintain streaming jobs in CI/CD and relevant tools.\\nPython scripting for automation and application development\\nDesign and implement Apache Airflow and other dependency enforcement and scheduling tools.\\nHands-on data modeling and data warehousing\\nDeploy solution using AWS, S3, Redshift and Docker/Kubernetes\\nDevelop storage and retrieval system using Presto and Parquet/ORC\\nScripting with Apache Spark and data frame.\\nQualifications\\nBachelor or Masters in Software Engineering or Computer Science\\n+4 years of experience in Data Engineering and Business Intelligence.\\nProficient in IoT tools such as MQTT, Kafka, Spark\\nProficient with AWS, S3, Redshift\\nExperience with Presto and Parquet/ORC\\nProficient with Apache Spark and data frame.\\nExperienced in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python,\\nExperience with Columnar database such as Redshift, Vertica\\nGreat verbal and written communication skills.\\nBe part of something amazing\\n\\nCome work alongside some of the most accomplished minds in the industry. Beyond providing competitive salaries, we’re providing a community for innovators who want to make an immediate and significant impact. If you are driven to create a better, more sustainable future, then this is the right place for you.\\n\\nAt Lucid, we don’t just welcome diversity - we celebrate it! Lucid Motors is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, national or ethnic origin, age, religion, disability, sexual orientation, gender, gender identity and expression, marital status, and any other characteristic protected under applicable State or Federal laws and regulations.\\n\\nTo all recruitment agencies: Lucid Motors does not accept agency resumes. Please do not forward resumes to our careers alias or other Lucid Motors employees. Lucid Motors is not responsible for any fees related to unsolicited resumes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As Verizon’s media unit, our brands like Yahoo, TechCrunch and HuffPost help people stay informed and entertained, communicate and transact, while creating new ways for advertisers and partners to connect. With technologies like XR, AI, machine-learning, and 5G, we’re transforming media for tomorrow, too. We're creators and coders, dreamers and doers creating what's next in content, advertising and technology.\\nAbout the role:\\nFlurry is a leader in mobile application analytics, empowering companies of all sizes to unlock mobile growth. Flurry empowers app makers to improve acquisition, engagement, retention and revenue with the world’s most adopted mobile app analytics solution. Over 40,000 companies - including Angry Birds, McDonald’s, Microsoft, Samsung and Baidu - use Flurry across a quarter of a million applications. Flurry operates a high-scale, high performance big data operation, delivering behavioral analytics insights on 10 billion daily end-user app sessions.\\nWhat we’re looking for:\\nFlurry is looking for a senior content marketing manager to join and lead our content efforts. You'll develop a deep understanding of our target audiences, the mobile landscape, and Flurry technology—and use that knowledge to tell compelling stories that build brand awareness and thought leadership.\\nWhat you’ll do:\\nIdeate, research, write and edit content for the industry-leading Flurry blog with a focus on educating the market, increasing brand awareness and driving demand\\nDevelop and execute a content strategy to create and promote long-form offerings including ebooks, guides, infographics, etc. that increase engagement and drive leads\\nIdentify, own, and track key content metrics, including content-driven lead creation\\nWork with our analysts, data scientists and other collaborators to identify relevant and timely industry topics, influencers, and subject-matter experts to aid in content creation\\nDevelop and maintain an editorial style guide to ensure all copy is consistent with Flurry’s messaging, voice, and tone\\nCross-team collaboration on marketing programs, including campaigns and larger initiatives that may span coordination across other Verizon Media groups\\nWhat would make you a strong fit:\\nBachelor’s degree with 4-6 years of content marketing, writing, and/or editorial experience in SaaS, data and analytics, or the mobile application space\\nExcellent writer and researcher—you can tell stories and communicate technical concepts to a wide variety of audiences without using jargon\\nOrganized and self-motivated with the ability to define goals and prioritize work based on what’s most impactful to the business and our target audience\\nFamiliarity with SEO best practices, content categorization, distribution, promotion, and measurement\\nVerizon Media is proud to be an equal opportunity workplace. All qualified applicants will receive consideration for employment without regard to, and will not be discriminated against based on age, race, gender, color, religion, national origin, sexual orientation, gender identity, veteran status, disability or any other protected category. Verizon Media is dedicated to providing an accessible environment for all candidates during the application process and for employees during their employment. If you need accessibility assistance and/or a reasonable accommodation due to a disability, please email ApplicantAccommodation@verizonmedia.com or call 408-336-1409. Emails/calls received for non-disability related issues, such as following up on an application, will not receive a response.\\nCurrently work for Verizon Media? Please apply on our internal career site.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Machine Learning Lead</td>\n",
       "      <td>Sunnyvale, CA 94087</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>94087</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Masters or equivalent degree in a computational science or engineering with 4+ years of experience in machine learning/data engineer role;Familiarity with distributed computing frameworks (e.g., Hadoop/Spark) and relational data base (e.g., Oracle, MySQL), and knowledge of NoSQL database;Strong implementation experience with a programming language (e.g., Jave/C++/Scala) and a scripting language (e.g., Python/Perl/Ruby), and familiarity with Linux/Unix/Shell environments;Strong hands-on skills in building scalable and reliable data pipelines for sourcing, cleaning and manipulating large volumes of data;Basic understanding of machine learning/statistics.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Position Description\\nWe are looking for outstanding machine learning/data engineers with skills in distributed computing system and experience working with very large scale of data, who will work closely with machine learning/data scientists in the team, and contribute to Advertising Technology in driving the future of ad targeting, personalization, relevance, ranking, and campaign optimization.\\n\\nJoin us if you want to be spending your time on:\\nBuilding, maintaining and monitoring scalable data pipelines to support modeling and optimization products;Analyzing, identifying, and debugging data related issues to ensure quality and stability of models and applications;Working with machine learning/data scientists to build prototypes and perform experiments;Working with engineering team to productize new features and products;Performing ad hoc data related analysis.\\nMinimum Qualifications\\nMasters or equivalent degree in a computational science or engineering with 4+ years of experience in machine learning/data engineer role;Familiarity with distributed computing frameworks (e.g., Hadoop/Spark) and relational data base (e.g., Oracle, MySQL), and knowledge of NoSQL database;Strong implementation experience with a programming language (e.g., Jave/C++/Scala) and a scripting language (e.g., Python/Perl/Ruby), and familiarity with Linux/Unix/Shell environments;Strong hands-on skills in building scalable and reliable data pipelines for sourcing, cleaning and manipulating large volumes of data;Basic understanding of machine learning/statistics.\\nAdditional Preferred Qualifications\\nExperience building and maintaining large scale data pipelines in online advertising, recommender system, ecommerce or relevant areas;Experience building and/or maintaining machine learning models and pipelines;Familiarity with job scheduler (e.g., Jenkins/Azkaban/Airflow);Experience with Elastic Search/Solr.\\nCompany Summary\\nThe Walmart eCommerce team is rapidly innovating to evolve and define the future state of shopping. As the world’s largest retailer, we are on a mission to help people save money and live better. With the help of some of the brightest minds in technology, merchandising, marketing, supply chain, talent and more, we are reimagining the intersection of digital and physical shopping to help achieve that mission.\\nPosition Summary\\nThis position is in the data science team under the Advertising Technology organization. The mission of the Advertising Technology organization is to advance Walmart eCommerce by driving higher value for our customers and vendor partners. Walmart is investing in building a world class advertising platform and the Ads team is responsible for defining and performance advertising products that drive discovery, sales and profits. The team operates an end to end advertising platform that includes a scalable ad service that serves hundreds of millions of impressions each day, sophisticated ad matching algorithms, real-time reports, self-service interface for end to end program management etc.\\n\\nWe are a highly motivated group of Big Data Geeks, Data Scientists and Applications Engineers, working in small agile group to solve sophisticated and high impact problems. We are building smart data systems that ingest, model and analyze massive flow of data from online and offline user activity. We use cutting edge machine learning, data mining and optimization algorithms on ad relevance, ranking and campaign optimization.\\n\\nJoin us if you want to be spending your time on:\\nGathering and analyzing data, identifying modeling and optimization problems, devising solutions and building prototypes;Formulating machine learning/statistical approaches while paying attention to business metrics, designing features from the rich data available from many sources, training, evaluating, and deploying models;Researching and implementing methodologies to measure the impact of the technologies;Initiating and proposing unique and promising modeling projects, developing new and innovative algorithms and technologies, pursuing patents where appropriate;Developing high-performance algorithms for precision targeting, user engagement prediction, and ad relevance/ranking; testing and implementing these algorithms in scalable, product-ready code; interacting with other teams to define interfaces and understanding and resolving dependencies;Staying current on published data mining, machine learning and modeling techniques and competing technologies and sharing these findings with scientists and engineers in the organization;Maintaining world-class academic credentials through publications, presentations, external collaborations and service to the research community.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Mateo, CA</td>\n",
       "      <td>San Mateo</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are changing the way healthcare uses data. Currently, about 80% of healthcare data is underused because it is too messy and unstructured for humans to efficiently analyze. The healthcare industry needed an intelligent technology that could extract insights quickly and accurately. And that is where we came in. Apixio augments the ability to read, decipher, and understand patient information. This ultimately translates into better care delivery, lower costs, and streamlined processes.\\n\\nThe opportunity at Apixio:\\nApixio is a mission-driven data science company passionate to make the practice of medicine more science than art. We are looking for a high-level Data Engineer, who shares our vision, to join our winning team and help us make a difference. This role will be filled by someone who can gain a clear understanding of the current infrastructure and be able to understand how to align with the needs and objectives of the science team. Abilities for training and testing will come into play with their motivation to make constant improvements.\\n\\nWe will be asking you to:\\n\\nBuild and maintain Scala spark / Hadoop pipelines at Apixio that are involved in our training / testing infrastructure for machine learning models\\nBuild and maintain Spark pipelines for ETL of data that write to our data warehouse and maintain the Apixio data warehouse\\nContribute to design and implementation of software pipelines that enable Apixio to update its internal proprietary algorithms in a scalable and reliable manner\\nContribute to our core code base that performs machine learning in a production setting\\n\\nTo be effective at this role you will need to be:\\n\\nCurious, willing to learn new things and try new things\\nData Driven\\nCommunicative - (willing to stand by decisions / ideas that they have)\\n\\nWe are assuming that you have:\\nA degree in computer science\\n\\n\\nExperience working with Scala / Java (min 2 years) in a professional setting\\nExperience working with Big Data Technologies (Hadoop, spark, etc… )\\n\\nAbout Apixio:\\nWe are a mission-driven company developing insights from data for a healthier world. We are helping people make better decisions using artificial intelligence to deliver quality healthcare. We are the technology leader in our space and growing at a 60% year over year rate and show no signs of slowing down. Our success is driven by a team of experienced, passionate and fun engineers, data scientists and business professionals that are working to solve some of the most complex problems, with some of the most innovative technologies and techniques in AI and machine learning. We are well funded by leading organizations such as Bain Capital and SSM, and serve more than 35 national and regional health plan and provider clients, and growing.\\n\\nWhat Apixio can offer you:\\n\\nMeaningful work to improve the healthcare industry\\nCompetitive compensation, including pre-IPO equity\\nExceptional benefits, including medical, dental and vision, FSA\\n401k\\nCatered, free lunches\\nParties, picnics and Friday Happy Hour\\nGenerous Vacation Policy\\nFree Parking\\nSubsidized Gym membership/Wellness Program\\nModern open office in beautiful San Mateo, CA\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Senior Big Data Engineer / Big Data Engineering Manager</td>\n",
       "      <td>San Mateo, CA</td>\n",
       "      <td>San Mateo</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Title: Big Data Engineering Manager / Senior Big Data Engineer\\n\\nLocation: San Mateo, CA\\n\\nReporting to: Senior Director of Data\\n\\nHeadquartered in the Bay Area with offices in Toronto, Canada; Jaipur, India and Austin, Texas, venture-funded Punchh is the world leader in innovative digital marketing products for brick and mortar retailers, combining AI technologies, mobile-first expertise, and omnichannel communications designed to dramatically increase customer lifetime value. Leading global chains in the restaurant, health and beauty sectors rely on Punchh to grow revenue by building customer relationships at every stage to becoming brand loyalists, including more than 100 different chains representing more than $12B in annual spend, 30,000 locations globally, 26M+ consumers, and 1M+ transactions daily. Punchh boasts a customer list that includes Pizza Hut, Quiznos, Coffee Bean &amp; Tea Leaf and many more.\\n\\nAbout this role\\n\\nReporting to the Head of Data Science, the data eng manager plays a critical role in leading Punchh's data innovations. He/she will help create cutting-edge big data solutions by leveraging his/her prior industrial experience.\\n\\nThis role requires close collaborations with machine learning, software engineering and product, serving not only internal teams but also our business clients.\\n\\nWhat You'll Do\\n\\n\\nBecome company's domain expert in Big Data and related technologies. Continuously improve our internal data infrastructure.\\nAn inspirational hands-on mentor to the other functions of the data team, guiding team members on establishing the best industrial practices.\\nOwn and project-manage Punchh's internal data pipeline supporting machine learning, BI products, and analytics.\\nRepresent Punchh's expertise in advanced technologies in a variety of media outlets.\\nWork with large data sets and implement sophisticated data pipelines with both raw and structured data.\\nManage and optimize our internal data pipeline that supports marketing, customer success and data science to name a few.\\nWork with the senior leadership to define Punchh's long term strategies in advanced technologies.\\nOccasional business travels are required.\\n\\nWhat You'll Need\\n\\n\\n7+ years of experience as a big data engineering professional, developing scalable big data solutions.\\n2+ years of experience as either people manager or technical lead manager of a big data team.\\nAdvanced degree in computer science, engineering or other related fields.\\nExtensive knowledge with cloud technology, e.g. AWS and Azure.\\nExcellent programming background in Python and Java, with hands-on experience with Kafka and Spark.\\nExtensive experience in using big data technologies to build data products.\\nExceptional communication skills and ability to articulate a complex concept with thoughtful, actionable recommendations.\\nStrong problem solving skills with demonstrated rigor in maintaining a complex data pipeline.\\nExcellent software engineering background. High familiarity with software development life cycle.\\n\\nAdvanced knowledge of big data technologies, such as programming language (Python, Java), relational (Postgres, mysql), NoSQL (Mongodb), Hadoop (EMR) and streaming (Kafka, Spark).\\n\\nBenefits\\n\\n\\nHealthcare coverage, FSA, HSA\\nLife and AD&amp;D insurance\\n401K\\nCompetitive salaries, bonus and stock options\\nProfessional development\\nEAP\\nMaternity and Paternity (Bonding) Leave\\nPTO\\nPaid holidays\\nFree lunch every single day, social events, plus a well stocked refrigerator\\n\\nPunchh is proud to provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics.\\n\\nWe also provide reasonable accommodations to individuals with disabilities in accordance with applicable laws.\\n\\nNotice to recruiters and placement agencies: If you are a recruiter or placement agency, please do not submit résumés to any person or email address at Punchh prior to having a signed agreement with Human Resources. Punchh is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Also, any résumés sent to us without an agreement in place will be considered your company's gift to Punchh and may be forwarded to our Talent Acquisition team.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Big Data Engineer, Catalog DataWorks</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree or higher in computer science or math is required.Data engineering Knowledge including ETL, Machine learning Models, NoSQL, Hive/SparkSQL/Athena, NoSQLStrong computer science fundamentals - algorithms, data structures , design patterns and programming (Java , Scala / Python)At least 5 years of software development experience.At least 3 years of experience of using Big Data systems.\\n\\nAmazon's Catalog DataWorks team is looking for highly motivated data engineers. We are embarking on multiple new initiatives to re-organize Amazon's catalog of billions of products, in new and interesting views, that drive several features Amazon's customers love. Today, these views drive hundreds of popular features like product recommendations, clustering of similar products, and shopping with Alexa. We will build a new near real-time Catalog Data Lake on AWS, to enable engineers and scientists across Amazon to solve customer problems faster. Come join us on this exciting journey!\\n\\nAs a data engineer on this team, you will own the Catalog Data Lake end-to-end. You will work closely with business partners to synthesize technical requirements.Your focus is at the team level on a major portion of existing or new data architecture (e.g., large or significant dataset, mid-size data solutions). You create coherent Logical Data Models that drive physical design.Your responsibilities may range from optimizing operational data storage to processing semi-structured data streams to building self-service business intelligence infrastructure for analysts. You take on projects and make enhancements that improve data processes (e.g., data auditing solutions, management of manually maintained tables, automating, ad-hoc or manual operation steps). You will use industry technologies like Spark, MapReduce, NoSQL, Parquet as well as modern AWS offerings like EMR, Glue, Athena, and Redshift. We are fortunate to be at the cusp of innovation in both the e-commerce business as well as cloud technology. As a key stakeholder, you will constantly develop ETLs, Queries, new patterns, algorithms, models for ranking, anomaly, pricing, etc.\\n\\nExperience in building/working on large scale distributed systems like Hadoop, Spark in the cloud.Experience with industry standards big data technologies (Spark, Kafka, Hive, Presto, NoSQL, or AWS equivalent)Data modeling experience with columnar data formats (Parquet, ORC etc.)ETL patterns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Senior Data Engineer (SWE)</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDesign and implement product features\\nTranslate business requirements into data models. Augment the data warehouse to incorporate new data and support new analytics requirements\\nCraft optimal data processing architecture and systems for new data pipelines\\nImplement instrumentation to measure query performance\\nRewrite and optimize relational queries (SQL) to improve product performance\\nWork with structured and semi-structured data\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5+ years of experience in the data warehouse and data analytics space\\nMUST: experience with SQL query rewrite, query optimization\\nExperience working in data architecture and relational data modeling\\nExperience with large scale analytic databases (e.g., Redshift, Snowflake, Teradata, Vertica)\\nStrong experience in scaling and optimizing schemas, performance tuning SQL, OLAP and Data Warehouse environments\\nStrong in relational algebra\\nProficiency with Java or related programming language\\nBS in computer science or related fields. MS/PhD a plus\\n</td>\n",
       "      <td>As more companies adopt public cloud infrastructure and the increase sophistication and harm caused by cyber attacks, the ability to safeguard companies from these threats have never been more urgent.\\n\\nLacework's novel approach to security fundamentally converts cyber security into a big data problem. We are a startup based in Silicon Valley that applies large scale data mining and machine learning to public cloud security. Within a cloud environment (AWS, GCP, Azure), our technology captures all communication between processes/users/external machines and uses advanced data analytics and machine learning techniques to detect anomalies that indicate potential security threats and vulnerabilities.\\n\\nThe company is led by an experienced team who have built large scale systems at Google, Paraccel (Amazon Redshift), Pure Storage, Oracle, and Juniper networks. Lacework is well funded by a tier one VC firm and is based in Mountain View, CA.\\nIn this role, you are a software engineer building security features into our product in Java and Go. Because you have expertise in databases, you will design data models that forms the basis of our anomaly detection algorithms and event investigation tools. You will extend our existing data models to incorporate new data sets and to support new use cases.\\n\\nResponsibilities:\\n\\nDesign and implement product features\\nTranslate business requirements into data models. Augment the data warehouse to incorporate new data and support new analytics requirements\\nCraft optimal data processing architecture and systems for new data pipelines\\nImplement instrumentation to measure query performance\\nRewrite and optimize relational queries (SQL) to improve product performance\\nWork with structured and semi-structured data\\n\\nRequirements:\\n\\n5+ years of experience in the data warehouse and data analytics space\\nMUST: experience with SQL query rewrite, query optimization\\nExperience working in data architecture and relational data modeling\\nExperience with large scale analytic databases (e.g., Redshift, Snowflake, Teradata, Vertica)\\nStrong experience in scaling and optimizing schemas, performance tuning SQL, OLAP and Data Warehouse environments\\nStrong in relational algebra\\nProficiency with Java or related programming language\\nBS in computer science or related fields. MS/PhD a plus\\n\\nLacework supports Green Card and H-1B sponsorship. We also have an office in Vancouver and sponsor Canadian work permits/permanent residence.\\n\\nLacework is an Equal Opportunity Employer. It is the policy of Lacework to provide equal employment opportunity to all persons, regardless of age, race, religion, color, national origin, sex, political affiliations, marital status, non-disqualifying physical or mental disability, age, sexual orientation, membership or non-membership in an employee organization, or on the basis of personal favoritism or other non-merit factors, except where otherwise provided by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Sunnyvale, CA 94085</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>94085</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Senior Data Engineer\\nSunnyvale, US- Full Time\\nLotusFlare is a product company with engineering offices in Silicon Valley, Belgrade and Kiev. Our solution is adopted by leading telecom providers and lifts their product infrastructure into the digital age. We are replacing traditional business backends with an engagement-centric dynamic product stack. Our cloud-native SaaS platform running on leading public clouds as well as supporting on-premise private and hybrid clouds is based on latest technologies picked from the CNCF stack.\\nLotusFlare, founded by early product and growth staff from Facebook, is backed by leading investors including Social Capital and Google Ventures. LotusFlare provides a platform for telecom operators to create a 100% digital customer experience where subscribers can choose and port a mobile number, order SIMs, and devices, track shipping, choose and purchase plans, discover and consume content, pay bills, receive loyalty rewards, and access customer service. LotusFlare also provides a Growth platform to drive user acquisition, engagement, and monetization on digital products. LotusFlare's clients include leading companies such as Verizon Wireless, Telenor, Ooredoo, Singtel, Maxis, Globe, LinkedIn, and Skype.\\nRESPONSIBILITIES\\nAs Senior Data Lead Engineer on our team, you would be responsible for designing, building a shipping and maintaining our critical data platform. In this role, you will lead the development of our Data Warehouse/Data Lake mentoring our cross-functional team of engineers and Data Scientists. Besides architecting and implementing you would also be responsible for collaborating with our Growth/Marketing teams to provide useful insights using various ML driven models.\\nREQUIREMENTS\\nBA/BS in Computer Science or other equivalent technical disciplines\\nYou have strong programming skills in Java/Scala or equivalent\\n5+ Years of experience creating ETL pipelines using Spark/Hadoop/Kafka/ClickHouse\\n2+ Years experience leading and/or mentoring junior engineers\\nExperience writing complex SQL and ETL processes\\nHands-on Experience with AWS\\nKnowledge of ClickHouse, Apache Spark, Kafka, and similar technology stacks\\nMachine learning expertise is a plus\\nBenefits\\nCompetitive base salary and stock options\\nFull medical/dental/vision package\\nGenerous vacation time and paid holidays\\nPerks\\nHigh impact work; influence the strategic direction of the company\\nAccess to the latest stacks, modernized tools\\nAccelerated career growth and opportunity\\nSharp, motivated co-workers in a flexible office environment\\nLUmFcmxiw1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bearing.ai - Data Engineer Palo Alto, CA</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>AI is the new electricity: Just as electricity transformed numerous industries starting 100 years ago, AI is now poised to do the same. AI Fund is a startup studio building new AI companies from the ground up. Our companies bridge AI technology and applications, focusing on industries and problems that we believe move the world forward. Founded by Dr. Andrew Ng, our team merges a deep understanding of AI technology with a proven ability to start new ventures. We have a unique business model that prepares founding members of a portfolio company with technical validation of the idea, experienced back office operational support and our team’s AI know how as well as connections within the AI community. This is an excellent opportunity for you to get in on the ground floor of an exciting AI Venture Firm and through it, grow its portfolio.\\n\\n\\nDogpatch Shipping is an early-stage startup at the forefront of bringing AI to the maritime shipping industry. This is a trillion dollar industry that moves 90% of the goods we interact with on a daily basis, but has traditionally lagged far behind other industries in adopting new technologies. At Dogpatch Shipping, we’re changing that. We’re building an AI-enabled product that solves the shipping industry’s biggest pain point and we already have some of the world’s biggest shippers as our partners.\\n\\nWe are currently looking for an exceptionally talented Data Engineer to add to our small team. This person will be responsible for leading our Data Engineering efforts in addition to guiding our initial Dev Ops efforts and contributing to the team’s Machine Learning model development.\\n\\nSince this person will be joining an early-stage startup at the ground level they’ll need to be able to wear multiple hats and thrive while working in a dynamic environment.\\nPrimary responsibilities:\\nAssess data quality coming from our partners, develop a data quality framework and work with our partners to optimize data quality moving forward\\nArchitect, develop and optimize ongoing ETL pipelines for that data. These pipelines must be built to support scale\\nHelp develop and validate complex machine learning models\\nDesign and implement overall data security strategy for Dogpatch Shipping Mentor/support other technical staff on data modeling and ETL related issues\\nEffectively communicate data strategy with a wide range of stakeholders\\nHelp interview candidates and further build out the team\\nWhat You Must Bring:\\nMust Haves:\\n3+ years of experience with data warehouse technical architecture\\n3+ years of experience with Python, Spark, ETL/Data engineering, Docker/Kubernetes, automation/devops in AWS\\nS3, EC2, EMR, RDS, Redshift, Glue, Lambda, EKS, Sagemaker experience\\nExperience setting up security for Redshift, S3, and EC2\\nBasic knowledge in machine learning model development\\nEntrepreneurial grit - You are ready to roll up your sleeves and willing to be scrappy in order to carry out plans under time and resource constraints\\nBachelor’s or Master’s degree in computer science or software engineering\\nNice to Haves:\\nExperience with data visualization tools such as TableauKnowledge of the shipping industry or experience with logistics products\\nWhat We Have to Offer:\\nFree snacks, drinks, lunches and dinners daily\\n401(k) plan\\nUntracked PTO including a week off between Christmas and New Year’s\\nVariety of medical, dental and vision coverage to choose from\\nCollegial atmosphere\\nDirect and transparent communication styleInnovative environment\\nWork with global AI Leaders\\nBe part of a great cause to improve lives everywhere through artificial intelligence\\nThis is a full-time position based in Palo Alto, California. You must already have, or be able to obtain, authorization to work in the United States.\\n\\nAt AI Fund, we are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants without regard to race, color, religion, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, gender expression, genetic information (including characteristics and testing), military and veteran status, and any other characteristic protected by applicable law. AI Fund believes that diversity and inclusion among our employees is critical to our success as a company, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool. Selection for employment is decided on the basis of qualifications, merit, and business need.\\n\\nThis is a full-time position based in Palo Alto, California. You must already have, or be able to obtain, authorization to work in the United States.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.\\nJob Description\\nAbout Arm\\n\\nArm® technology is at the heart of a computing and connectivity revolution that is transforming the way people live and businesses operate. Our advanced, energy-efficient processor designs are enabling the intelligence in 86 billion silicon chips and securely powering products from the sensor to the smartphone to the supercomputer. Our large footprint in the stack of technologies that enables IoT applications, creates an unparalleled opportunity to be at the forefront of the Machine Learning and data driven revolution.\\nWe are a fast growing, diverse and dynamic team of enthusiastic professionals who share a vision and real passion for building a technology foundation for this emerging industry. We are looking for a hardworking and hands-on data engineer as part of our data science team within ISG organization.\\nThe role will be based in San Jose with occasional travel to other Arm sites, customers and partners.\\n\\n What will I be accountable for?\\n\\nYou work closely with internal/external key partners across the globe to understand and document requirements to implement and optimize data workflow\\nYou take lead in designing pipelines for ETL and data access (e.g. scrape, clean, prepare, catalogue data)\\nYou will consolidate and govern data from different sources\\nYou appreciate operating in agile environment and being the interface to the product team\\nYou will successfully maintain data process and model deployment by applying best software practices\\nYou are the database wizard and enjoy enabling the data science with relevant and consolidated data\\nYou will be curious and open-minded about different tools and frameworks, and you can adopt the right one for the task at hand\\nJob Requirements\\nWhat skills, experience, and qualifications do I need?\\n\\nBachelor’s degree in Computer Science, Engineering or related field, with at least four years of experience in a professional setting\\nYou are experienced and passionate to design, develop, unit test, code review, build and produce the deployment artifacts.\\nProven coding experience with at least one scripting language like Python and another language like Java, Node; JavaScript will be a plus\\nDeep understanding of trade-offs for different methods to scale and stabilize the scheme of database engines\\nNice to have:\\nBuilt pipeline to harvest data and put them in a format that is easily accessible by different part of the organization\\nHands-on experience with cloud compute experience such as AWS\\nDemonstrable strong interpersonal skills, excellent written and spoken English to communicate openly and effectively\\n\\nAt Arm, we are guided by our core beliefs that reflect our rare culture and guide our decisions, defining how we work together to defy ordinary and shape extraordinary:\\nWe not I\\nTake daily responsibility to make the Global Arm community thrive.\\nNo individual owns the right answer. Brilliance is collective.\\nInformation is crucial, share it.\\nRealise that we win when we collaborate — and that everyone misses out when we don’t.\\nPassion for progress\\nOur differences are our strength. Widen and mix up your network of connections.\\nDifficult things can take unexpected directions. Stick with it.\\nMake feedback positive and expansive, not negative and narrow.\\nThe essence of progress is that it can’t stop. Grow with it and own your own progress.\\nBe your brilliant self\\nBe quirky not egocentric.\\nRecognise the power in saying ‘I don’t know’.\\nMake trust our default position.\\nHold strong opinions lightly.\\n\\n#LI-PW1\\n\\nBenefits\\nYour particular benefits package will depend on position and type of employment and may be subject to change. Your package will be confirmed on offer of employment. Arm’s benefits program provides permanent employees with the opportunity to stay innovative and healthy, ensure the wellness of their families, and create a positive working environment.\\n\\nAnnual Bonus Plan\\nDiscretionary Cash Awards\\n401(k), 100% matching on first 6% eligible earnings\\nMedical, Dental &amp; Vision, 100% coverage for employee only, shared cost for dependents\\nBasic Life and Accidental Death and Dismemberment Insurance (AD&amp;D)\\nShort Term (STD) and Long Term (LTD) Disability Insurance\\nVacation, 20 days per year with option to buy 5 more.\\nHolidays, 13 days per year\\nSabbatical, 20 paid days every four-years of service\\nSick Leave, 7 days per year\\nVolunteering, four hours per month (TeamARM)\\nOffice location dependent: café on site, fitness facilities, team and social events\\nAdditional benefits include: Flexible Spending Accounts for health and dependent care, EAP, Health Advocate, Business Travel Accident Program &amp; Commuter programs.\\nARM, Inc. (USA) participates in E-Verify. For more information, please refer to www.dhs.gov/E-Verify\\nAbout Arm\\nArm® technology is at the heart of a computing and connectivity revolution that is transforming the way people live and businesses operate. From the unmissable to the invisible; our advanced, energy-efficient processor designs are enabling the intelligence in 86 billion silicon chips and securely powering products from the sensor to the smartphone to the supercomputer. With more than 1,000 technology partners including the world’s most famous business and consumer brands, we are driving Arm innovation into all areas compute is happening inside the chip, the network and the cloud.\\n\\nWith offices around the world, Arm is a diverse community of dedicated, innovative and highly talented professionals. By enabling an inclusive, meritocratic and open workplace where all our people can grow and succeed, we encourage our people to share their unique contributions to Arm's success in the global marketplace.\\n\\nAbout the office\\nThe Arm San Jose office is nestled in California's Silicon Valley, just south of San Francisco. Serving as Arm’s North America headquarters, the office houses employees serving all divisions of Arm. World-renowned Stanford University is seen as a hub of innovation, helping to make San Jose one of the US’s fastest growing cities.\\n\\nSan Jose, California USA\\n\\nArm Inc.\\n\\n150 Rose Orchard Way\\n\\nSan Jose, CA 95134-1358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Intern - IT Big Data Engineer</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description\\nOur Mission\\nAt Palo Alto Networks® everything starts and ends with our mission:\\n\\nBeing the cybersecurity partner of choice, protecting our digital way of life.\\n\\nWe have the vision of a world where each day is safer and more secure than the one before. These aren’t easy goals to accomplish – but we’re not here for easy. We’re here for better. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.\\nYour Career\\nOur AI &amp; Analytics group is responsible for working with various business owners/stakeholders from Sales, Marketing, GCS, Infosec, Operations, and Finance to solve difficult business problems which will have a direct impact on the metrics defined to showcase the progress of Palo Alto Networks.\\nWe leverage the latest technologies from Cloud &amp; Big Data ecosystem to improve business outcomes and create through prototyping, Proof-of-Concept projects, and application development. We are seeking an intern for our team who will work closely with other Principal Engineers, Product Managers, Data Engineers &amp; Data Scientists. In this role, you will work in the AI &amp; A team to design, and deliver our next generation data services.\\nOur Summer Internship Program from May-August or June-September provides you:\\n1:1 mentorship\\nFun and engaging events that inspire your intellectual curiosity\\nOpportunities to expand your knowledge and work on challenging projects\\nConnections to other interns, recent grads, and employees across the company as well as our leaders\\nYour Impact\\nBuild Internal or external customer facing applications in Cloud &amp; Hadoop environments.\\nDevelop low latency, high throughput data pipelines in Hadoop and Cloud using leading edge technologies like Spark, Kafka, Beam, and other cloud specific technologies.\\nPartner with data analysts, product owners to gather business and system requirements\\nBuild and Integrate applications with other external applications\\nDevelop rich front-end experiences using technologies such as Tableau, Angular JS, JavaScript, Node.js, React framework, etc.\\nAssist with design, development of features to perform predictive analytics on big data sets\\nYour Experience\\nCurrently working towards a B.A./B. S/B.E./MS in a technical area (Computer Science, Engineering, etc.)\\nBasic understanding of software design and development\\nBasic understanding of developing cloud-based services (GCP, AWS, etc.)\\nGood working knowledge of at least one contemporary programming language such as Python, Java, Scala, and JavaScript.\\nBasic understanding of data analysis methods and approaches\\nExcellent oral and written communication skills\\nRequirements – All applicants must be pursuing a 4-year Undergraduate Degree, a 2-year Master’s Degree or a Doctorate degree and returning to school in the fall. You must have authorization to work within the United States.\\n\\n*Please note that we will not sponsor applicants for work visas for this position*\\nThe Team\\nOur Data engineering team is responsible for deep diving into data, figuring out issues with the quality of data and coming up with advanced techniques to bring insights out of the data to solve business problems.\\nOur Commitment\\nWe’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.\\nWe are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.\\nPalo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Director, Business Analytics</td>\n",
       "      <td>Redwood City, CA</td>\n",
       "      <td>Redwood City</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Code: 4806\\nGrade: L\\n\\nDirector, Business Analytics\\n\\nImprovement, Analytics, and Innovation Services\\nWe are a newly formed department within Stanford tasked with providing management and operational consulting services to a highly complex university administration with hundreds of business processes and dozens of major systems. Our team of internal consultants strives to create, instill, and sustain a business improvement discipline that delivers transformed and efficient processes through a data-driven approach.\\n\\nThe Business Analytics team is instrumental in executing our vision by providing essential data services both internally within our department as well as to our various clients. This team is composed of data professionals that cover a broad spectrum of skills such as data visualization, data engineering, statistics, machine learning, and optimization. We are in search of a seasoned director who will lead a current team of three (data engineer, senior data scientist, and business intelligence analyst) on a multitude of projects as well as identify new opportunities with a diverse internal clientele that span across university functions from administration to research. In additional to the technical data skills you bring, we also value your ability to manage and develop a successful and productive team.\\n\\nThe team will support projects across Stanford that range from identifying the impact of collaborative research, building an automated way to classify university expenditures, to identifying opportunities to improve the student services staff onboarding experience.\\n\\nTake a closer look at what we’re up to https://www.youtube.com/user/StanfordUniversity and dive deeper into our vision: https://ourvision.stanford.edu/\\n\\nYour responsibilities include:\\nManage, coach, and lead a team of data professionals with varying skill levels and backgrounds\\nBuild lasting relationships with our clients by providing data-driven solutions to their most impactful business problems\\nMake strategic decisions on our project portfolio, establish practical project timelines, and allocate resources in order to deliver solutions to our clients\\nCreate an environment for your team to succeed and remove any barriers that are detrimental to productivity\\nManage the performance and career development of the team\\nMarket our services to Stanford by speaking at various outlets and community meet-ups\\nImplement guidelines, policies, and standards for the team to follow\\nPerform business analysis and capture requirements from clients regarding their various data needs\\nAs necessary, get hands-on with various aspects of work done by the team such as writing scripts for data analysis or building a data visualization or interactive dashboard\\n\\nKnowledge, skills, and abilities you bring:\\nBroad knowledge in the data analytics space from data pipeline development and business intelligences to statistics and machine learning.\\nDemonstrates rigorous attention to detail when conducting analysis on a business problem as well as when delivering a solution\\nImpeccable ability to find the most appropriate communication style depending on the audience, e.g. technical level with developers, strategic level with business managers\\nFearlessness to dive into the gritty technical details of a particular solution\\nKeen ability to translate complex concepts around data into layman terms\\nSolid understanding of scripting languages such as Python, R including relevant packages used in data analysis\\nSolid understanding of systems architecture and various forms of database technologies such as Oracle, PostgreSQL, MongoDB, etc.\\nHands-on experience with business intelligence tools such as Tableau and Oracle BI\\nAbility to design reporting data models that are precisely aligned with the business questions being asked\\nAn eye for good data visualization design\\nUnderstanding of UI/UX as it relates to building interactive dashboards\\nExperience using git to track source code\\nExperience working closely with data scientists and report developers\\nExperience eliciting, interpreting, and documenting user requirements in the context of data analysis\\nAbility to lead multiple activities in a diverse environment\\n\\nWorking at Stanford:\\nImagine a world without search engines or social platforms. Consider lives saved through first-ever organ transplants and research to cure illnesses. Stanford University has revolutionized the way we live and enrich the world. Supporting this mission is our diverse and dedicated 17,000 staff. We seek talent driven to impact the future of our legacy. Our culture and unique perks empower you with:\\nFreedom to grow. We offer career development programs, tuition reimbursement, audit a course. Join a TedTalk, film screening, or listen to a renowned author or global leader speak.\\nA caring culture. We provide superb retirement plans, generous time-off, and family care resources. http://stanfordcareers.stanford.edu/pay-and-rewards/a-competitive-edge\\nLegacy and Innovation. Our new state-of-the-art campus in Redwood City is a vibrant workplace with many great amenities to take in across our 35–acre site. Attend a meeting in our new multi–faceted conference rooms. Or get in a great workout at our impressive 28,000 square–foot Recreation &amp; Wellness Center complete with a six–lane rooftop pool and indoor basketball court.\\nEnviable resources. Enjoy free commuter programs, ridesharing incentives, discounts, and more!\\n\\nConsistent with its obligations under the law, the University will provide reasonable accommodation to any employee with a disability who requires accommodation to perform the essential functions of the job.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Data Engineer, Apple Pay Security</td>\n",
       "      <td>Santa Clara Valley, CA 95014</td>\n",
       "      <td>Santa Clara Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>95014</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: May 28, 2019\\nRole Number: 200063186\\nImagine what you could do here. At Apple, new ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Do you have a passion for Security technologies? Do you love significant challenges? Join the team that provides software security technologies to help users protect their accounts. Our vision is to transform your smartphone into a device that secures your digital life without sacrificing your privacy. We are seeking a talented engineer who will develop libraries and platforms to empower data scientists to rapidly build and deploy robust models in production. You will also craft robust and scalable software. The role requires handling multiple assignments, communicating across functional areas, and driving projects to completion. We need you to show initiative and be able to take ownership of a problem area to find workable solutions.\\nKey Qualifications\\nStrong software design and development skills\\nExcellent analytical, programming and debugging skills\\nExperience with Scala or Java\\nExperience building scalable, distributed data-intensive applications with modern data tools. Knowledge of Hive, Spark, Cassandra, Kafka is a plus\\nExperience building data pipelines is a plus\\nHighly motivated and organized, with the ability to accept ambiguity and deliver exceptional results on tight schedules\\nKnowledge of SQL and/or NoSQL databases is a plus\\nExperience with security and cryptography is a plus\\nDescription\\nWe are looking for an expert data engineer who will work on our data science libraries and platforms to support new products. You will work hand in hand with data scientists, security engineers, program managers and business partners to identify problems, define scalable solutions, execute plans and results on a regular basis.\\nEducation &amp; Experience\\nBS or MS in Math, Computer Science or equivalent industry experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Engineer (Outward, Inc.)</td>\n",
       "      <td>San Jose, CA 95112</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>95112</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor's degree or equivalent work experience\\n2+ years of Python development experience\\n2+ years of SQL (Hive, Oracle, MySQL, PostgreSQL) experience\\nProfessional experience using XML\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nWork with Engineers, Product Owners, and Designers to understand their data needs\\nAutomate frequently requested analyses using Python\\nEvaluate and define critical business metrics and identify new levers to help move these metrics\\nDesign and evaluate A/B experiments\\nMonitor key product metrics and identify root causes behind anomalies\\nBuild and analyze dashboards and reports\\nInfluence product teams through a presentation of data-based recommendations\\nCommunicate state of business and experiment results to product teams</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Location: San Jose, CA\\n\\nAbout Outward, Inc.\\n\\nOutward, Inc. is based in San Jose, CA and is a wholly owned subsidiary of Williams Sonoma, Inc. ( www.outwardinc.com )\\n\\nAt Outward Inc. our vision is to 'lower the friction' with regards to all aspects of the customer journey for our parent company and our retail customers. We do this by offering new technology solutions that enable new experiences and top-notch visualizations of their products. We are continuously pushing the boundaries of how 3D and AR/ VR technologies will drive the next generation shopping experience.\\n\\nThrough our portfolio of premium lifestyle brands - our mission is to deepen consumer connections with the products that matter and deliver an innovative experience.\\n\\nWe are positioned as a technology leader in the visual merchandising space for retail, with a focus on improving customer experiences with next-generation product visualizations.\\n\\nCome and join a growing team of engineers as we solve technological riddles and push the envelope of what can be done on the web!\\n\\n\\nResponsibilities\\n\\nWork with Engineers, Product Owners, and Designers to understand their data needs\\nAutomate frequently requested analyses using Python\\nEvaluate and define critical business metrics and identify new levers to help move these metrics\\nDesign and evaluate A/B experiments\\nMonitor key product metrics and identify root causes behind anomalies\\nBuild and analyze dashboards and reports\\nInfluence product teams through a presentation of data-based recommendations\\nCommunicate state of business and experiment results to product teams\\nMinimum Qualifications\\n\\nBachelor's degree or equivalent work experience\\n2+ years of Python development experience\\n2+ years of SQL (Hive, Oracle, MySQL, PostgreSQL) experience\\nProfessional experience using XML\\n\\n\\nNice-to-have Qualifications\\n\\n2+ years of experience with data visualization and data-mining\\nExperience analyzing data to identify deliverables, gaps and inconsistencies\\nExperience initiating and driving projects to completion with minimal guidance\\nExperience communicating the results of analyses\\nExperience with AWS services like lambda, Cloud Formation, RDS, EC2, IAM\\nOutward's Benefits &amp; Perks:\\n\\nMedical, Dental, Vision\\n401K\\nPaid time off\\nCompany-sponsored team events such as regular staff parties\\nFantastic new headquarters\\nFully stocked kitchens with catered lunches twice a week\\nOn-site gym &amp; game room\\nOffice dogs! Bring your furry pal with you to work\\nFlexible working hours\\nFriendly, caring co-workers and management\\n\\n\\nThis position will not offer relocation assistance or remote work.\\nOutward, Inc. is an Equal Opportunity Employer.\\nOutward, Inc. will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the California Fair Employment Act (AB 1008), or other applicable state or local laws and ordinances.\\n\\n#LI-JQ1 | rev 8.9.19\\n\\n-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Senior Splunk Architect</td>\n",
       "      <td>Pleasanton, CA 94588</td>\n",
       "      <td>Pleasanton</td>\n",
       "      <td>CA</td>\n",
       "      <td>94588</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Provide guidance on how to best deploy and maintain our Splunk environment\\nDesign and support the automated deployment of security infrastructure\\nInstall, build and maintain Splunk and related Apps and add-ons\\nLead cross-team efforts with other internal organizations\\nSupport On-call duties for a variety of security tools\\nLead key deliverables and objectives for the Security Data Engineering team\\n</td>\n",
       "      <td>Provide guidance on how to best deploy and maintain our Splunk environment\\nDesign and support the automated deployment of security infrastructure\\nInstall, build and maintain Splunk and related Apps and add-ons\\nLead cross-team efforts with other internal organizations\\nSupport On-call duties for a variety of security tools\\nLead key deliverables and objectives for the Security Data Engineering team\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join our team and experience Workday!\\nIt's fun to work in a company where people truly believe in what they're doing. At Workday, we're committed to bringing passion and customer focus to the business of enterprise applications. We work hard, and we're serious about what we do. But we like to have a good time, too. In fact, we run our company with that principle in mind every day: One of our core values is fun.\\nJob Description\\nJoin our team and experience Workday!\\nIt's fun to work in a company where people truly believe in what they're doing. At Workday, we're committed to bringing passion and customer focus to the business of enterprise applications. We work hard, and we're serious about what we do. But we like to have a good time, too. In fact, we run our company with that principle in mind every day: One of our core values is fun.\\nJob Description\\nAre you looking to join one of the hottest cloud companies in the world? Do you enjoy administering enterprise grade, large-scale, security solutions in a rapidly-growing global infrastructure?\\nJoin Workday's Security Engineering team as a Sr. Security Data Engineer. This position is responsible for the administration of security tools with a focus on Splunk. You will be responsible for working closely with the Security Operations, Network, Security, and IT teams in architecting and administering security solutions. This is a technical role with the expectation that you are fluent in Linux system administration and Splunk configurations.\\nThe team that you would be on designs, deploys and manages all internal and external customer facing security services. You'll be an inventive engineer, with a taste for challenging problems that lesser engineers shy away from. You'll revel in deploying and administering elegant solutions using whatever languages, tools and hardware deemed most appropriate.\\nYour training and development budget will see you mandated to attend at least one major off-site training course annually (SANS, Splunk Educations, etc) and at least one major security conference (Blackhat, Defcon, .conf, RSA, CanSecWest, etc) as well as having budget for local conferences and events. You'll be encouraged to keep your skills up to date with other events such as internal red/blue team events, hackathons, membership of various groups and societies. You'll be provided a budget to grow a reference library for you and your team. You will have a lab and development pipeline to run proof of concept projects in. We also reserve one afternoon a week for side projects.\\nResponsibilities\\nProvide guidance on how to best deploy and maintain our Splunk environment\\nDesign and support the automated deployment of security infrastructure\\nInstall, build and maintain Splunk and related Apps and add-ons\\nLead cross-team efforts with other internal organizations\\nSupport On-call duties for a variety of security tools\\nLead key deliverables and objectives for the Security Data Engineering team\\nRequirements\\nMinimum of B.S. Degree in STEM field required.\\n6+ Years of experience in IT/Security\\nExpert in Splunk Architecture (ideally certified)\\nProficiency in Linux administration (ideally RHEL/CentOS)\\nProficient skills in at least 3 of the following:\\nVersion Control (ideally Git)\\nExperience with Automated deployment (ideally Chef, Ansible)\\nExperience with Continuous integration tools (ideally Jenkins, Bamboo)\\nExperience with Python scripting\\nExperience open-sourcing and supporting home-grown tools\\nExperience in Security Operations\\nHost Based IDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Senior Data Engineer, Customer Experience</td>\n",
       "      <td>Los Gatos, CA 95032</td>\n",
       "      <td>Los Gatos</td>\n",
       "      <td>CA</td>\n",
       "      <td>95032</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Los Gatos, California\\nData Science and Engineering\\nNetflix is revolutionizing entertainment and shaping the evolution of storytelling around the world. With over 150 million members in 190 countries, we are focused on delivering an incredible customer experience.\\n\\nWhen someone encounters a problem watching their favorite Netflix show, the data products we build and teams we support get members back to streaming as soon as possible. We collect millions of data points via phone, chat, and social media to assess customer feedback in 25 different languages. This allows us to measure the effectiveness of different customer service strategies, detect issues to proactively assist members with personalized support, and test different paths for members to easily discover useful resources.\\n\\nIn this role, you will collaborate with a team of data engineers to build reliable, scalable data pipelines using Apache Spark/Flink. These pipelines will power analytic dashboards, custom viz applications with different storage engines (Snowflake, Druid, Elasticsearch), A/B experiments, feature generation for training production ML models, and NLP driven insights to better understand customer issues.\\nWho you are:\\nYou have a strong background in distributed data processing (Batch or Streaming).\\nYou have extensive data modeling skills. You design structures that are adaptable to changes in the source data or business processes.\\nYou are a technical thought leader with a perspective on how to build great data products. You can adopt and help evolve our engineering best practices.\\nYou are proficient in at least one major programming language and are passionate about writing clean, supportable code.\\nYou are an advocate for data quality. You have a strong opinion on when data audits, unit tests, and documentation can be most effective.\\nYou have strong SQL skills.\\nYou have strong communication skills to effectively partner with data scientists and engineering stakeholders.\\nYou are curious about the rapidly evolving technologies in this domain. You are eager to learn and master new tech when it can have a big impact on our team.\\nYou can relate to many of the aspects of the Netflix culture and love to operate independently while collaborating and giving/receiving strong, candid feedback to your team members.\\nAPPLY NOW\\nShare this listing:\\nLINK COPIED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Siri - Performance and Reliability Team Data Engineer</td>\n",
       "      <td>Santa Clara Valley, CA 95014</td>\n",
       "      <td>Santa Clara Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>95014</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: Sep 20, 2019\\nWeekly Hours: 40\\nRole Number: 200067345\\nPlay a part in the next revolution in human-computer interaction. Contribute to a product that is redefining mobile and desktop computing. Create groundbreaking technology for large scale systems, spoken language, big data, and artificial intelligence. And work with the people who created the intelligent assistant that helps millions of people get things done — just by asking. The vision for the Siri Performance and Reliability team is to empower Siri organization to improve Siri quality by using data as the voice of our customers. Join us, and impact hundreds of millions of customers across a plethora of Apple of devices!\\nKey Qualifications\\n4+ years of experience in developing jobs in the MapReduce/Hadoop ecosystem, especially with Spark/Scala.\\nExpert knowledge of one or more object-oriented programming languages (Scala, Java, C++).\\nAbility to use several scripting languages (Python, Ruby, Bash, etc.).\\nThorough understanding of the Hadoop ecosystem (HBase, HDFS, Hive, MapReduce), Spark, Solr, Kafka.\\nExperience with Batch and Streaming data processing\\nExperience with SQL and basic database knowledge for modifying queries and tables.\\nWorking knowledge of the fundamentals of probability and basic statistics.\\nStrong interpersonal skills and experience working on multi-functional projects.\\nAn obsession with quality.\\nDescription\\nThe Siri team is looking for a talented, broadly-skilled developer who is a creative problem-solver, thrives in a fast-paced environment, can work well across teams and organizations, and has a passion for quality. We thrive in automation, designing and implementing frameworks and other infrastructure for data analysis, creating tools with elegant and effective user interfaces.\\nIf you're interested, you're probably a strong programmer with excellent problem-solving and interpersonal skills. You also have a passion to make the best products possible is the key for success in our group. You will create design patterns to ensure your metrics can be easily understood and reused in different contexts. You will also contribute to the design of our full system architecture. Create unit and functional tests to validate that your code continues to work in a fast-paced environment. You will provide technical expertise to other teams, advising best practices and highlighting risks.\\nAs a Data Engineer on the Siri Performance and Reliability, you will have significant responsibility and influence in improving Siri by using data to measure user perceived latency, errors and failures along with user abandonments. You will develop large scale data processing and analytical solutions. You will collaborate with our quality initiative leaders to ensure the system is meeting the needs, and iterate as well as innovate based on observations and requirements gathering.\\nOur engineers collaborate with many internal sub-teams, such as engineering, design, QA, operations, and project management, and will be working in a heterogeneous environment. A successful candidate will have experience in large-volume data ingestion, processing, and analysis in near real-time. Design, implement, and manage scalable data models and pipelines used by all Siri teams. Build analytical solutions to enable data analysts to perform accurate and consistent analysis efficiently. Thus deep technical capabilities, strong communication skills and a knack to use hard data to triage issues is a must have requirement.\\nEducation &amp; Experience\\nMS in EE/CS/CE or equivalent experience\\nAdditional Requirements\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>SQL Data Analyst</td>\n",
       "      <td>Mountain View, CA 94039</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>94039</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>What are we looking for?\\nWe are looking for a technically savvy database-oriented Analyst or Data-Engineer with good people skills and the ability to pick up new business concepts and technologies.\\n\\nThe ideal candidate will possess:\\nA strong to very strong working knowledge of SQL.\\nAn ability to write and troubleshoot complex SQL procedures.\\n\\n• The desire to understand business events through data.• An understanding of Data Warehousing and ETL techniques.\\nHigh level understanding in at least one scripting language such as Ruby, Shell, Python.\\nAn interest in learning large data set processing with MapReduce/Hadoop/Pig/etc.\\nLinux skills are a plus.\\nGood client relations skills strongly preferred.\\n\\nWho are we?\\nRaybeam, Inc. is a software engineering and consulting company focused on strategic consulting, business intelligence, and online/database marketing for the past twenty years. We have offices near Boston and San Francisco and support a strong list of clients including Google, Facebook, Microsoft, eBay, Disney, One Kings Lane, Beachbody and Hilton Worldwide.\\n\\nWhat do we do?\\nWe provide technology solutions by architecting and developing enterprise systems using a variety of programming languages, tools and platforms. This can range from building data warehouses, to web applications to implementing reporting platforms. We work in small teams, own the projects that we work on, and have direct input into the business decisions of our clients.\\n\\nWhat do you get from working for Raybeam?\\nA fun, supportive work environment that promotes camaraderie and growth.\\nThe chance to travel and network with important figures in the industry.\\nThe chance to have input into business decisions of our clients.\\nThe opportunity to learn technologies that you've always wanted but never had the chance.\\n\\nIf you are interested in applying for the position please click on the link below to take a short quiz.\\n\\nhttp://careerseval.raybeam.com/sign_in\\n\\nPlease note that Raybeam, Inc. is currently unable to provide sponsorship or OPT extension work and we will only consider local candidates. Recent grads are encouraged to apply, and an MBA is desirable. Thank You.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Hadoop – Senior Data Engineer</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Experience in Hadoop distributions (Apache / Cloudera / Hortonworks)\\nAbility to deploy and maintain multi-node Hadoop cluster\\nExperience working with Big Data eco-system including tools such as Hadoop, Map Reduce, Yarn, Hive, Pig, Impala, Spark , Kafka, Hive, Impala and Storm to name a few\\nKnowledgeable in techniques for designing Hadoop-based file layout optimized to meet business needs\\nUnderstands the tradeoffs between different approaches to Hadoop file design\\nExperience with techniques of performance optimization for both data loading and data retrieval\\nExperience with NoSQL Databases – HBase, Apache Cassandra, Vertica, or MongoDB\\nAble to translate business requirements into logical and physical file structure design\\nAbility to build and test rapidly Map Reduce code in a rapid, iterative manner\\nAbility to articulate reasons behind the design choices being made</td>\n",
       "      <td>Strong data analysis and SQL skills in Relational/Columnar/Big data environments (Joins, Union, rank, group by, order etc.)\\nDemonstrate excellent written and verbal communication skills\\nExperience with Agile implementation methodology and working in a globally distributed team structure\\nAble to work independently in a fast-paced environment\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Details\\nJob Code\\nJPSC-7224\\nPosted Date\\n04/19/18\\nExperience\\n7 Years\\nPrimary Skills\\nHadoop,Hive,Pig,Impala,Map Reduce,KAFKA,Spark,Yarn,HIVE SQL\\nRequired Documents\\nResume\\nOverview\\nRole: Hadoop – Senior Data Engineer\\nDuration: 6 Months\\nLocation: San Jose, CA\\n\\nOnly W2, No C2C at this moment\\n\\nImportant:\\n6 months right to hire will go into the work order\\nprepare your candidate for multiple interviews including a client interview\\n-local candidates only - rate is all inclusive - if your candidate is not local but willing to relocate, please make a note of this in the summary section of the resume.\\n-3 - 6 years of experience\\n\\nResponsibilities and required skills:\\nStrong data analysis and SQL skills in Relational/Columnar/Big data environments (Joins, Union, rank, group by, order etc.)\\nDemonstrate excellent written and verbal communication skills\\nExperience with Agile implementation methodology and working in a globally distributed team structure\\nAble to work independently in a fast-paced environment\\nHadoop Skills:\\nKey words for Primary – HIVE SQL , SPARK and Sqoop, Rest optional\\nExperience in Hadoop distributions (Apache / Cloudera / Hortonworks)\\nAbility to deploy and maintain multi-node Hadoop cluster\\nExperience working with Big Data eco-system including tools such as Hadoop, Map Reduce, Yarn, Hive, Pig, Impala, Spark , Kafka, Hive, Impala and Storm to name a few\\nKnowledgeable in techniques for designing Hadoop-based file layout optimized to meet business needs\\nUnderstands the tradeoffs between different approaches to Hadoop file design\\nExperience with techniques of performance optimization for both data loading and data retrieval\\nExperience with NoSQL Databases – HBase, Apache Cassandra, Vertica, or MongoDB\\nAble to translate business requirements into logical and physical file structure design\\nAbility to build and test rapidly Map Reduce code in a rapid, iterative manner\\nAbility to articulate reasons behind the design choices being made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Senior Data Engineer, Apple Media Products</td>\n",
       "      <td>Santa Clara Valley, CA 95014</td>\n",
       "      <td>Santa Clara Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>95014</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: Nov 1, 2018\\nRole Number: 200007033\\nApple is seeking a highly skilled data engineer to join the Data Engineering team within Apple Media Products. AMP (home to Apple Music, App Store, iTunes and more) has some of the most compelling data in the world. We are looking for a talented engineer who is motivated by challenging problems and well versed with big data technologies. This is a unique opportunity to join a focused team and work collaboratively with other groups to make a significant impact.\\nKey Qualifications\\nExperience in high level programming languages such as Java, Scala, or Python.\\nProficiency with databases and SQL is required.\\nProficiency in data processing using technologies like Spark Streaming, Spark SQL, or Map/Reduce.\\nExpertise in Hadoop related technologies such as HDFS, Azkaban, Oozie, Impala, Hive, and Pig.\\nExpertise in developing big data pipelines using technologies like Kafka, Flume, or Storm.\\nExperience with large scale data warehousing, mining or analytic systems.\\nAbility to work with analysts to gather requirements and translate them into data engineering tasks\\nAptitude to independently learn new technologies.\\nDescription\\nAs a member of the Data Engineering team, you will have significant responsibility and influence in shaping its future direction. This role is inherently cross-functional and the ideal candidate will work across disciplines. We are looking for someone with a love for data and ability to iterate quickly on all stages of data pipeline. This position involves working on a small team to develop large scale data pipelines and analytical solutions using BigData technologies. Successful candidates will have strong engineering skills and communication, as well as, a belief that data driven processes lead to great products. You will need to have a passion for quality and an ability to understand complex systems.\\nEducation &amp; Experience\\nBachelor's degree or equivalent work experience in Engineering, Computer Science, Business Information Systems.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Redwood City, CA</td>\n",
       "      <td>Redwood City</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Auction.com is the nation’s leading online real estate marketplace focused exclusively on the sale of residential bank-owned and foreclosure properties via online auctions and live trustee sale events. By offering access to exclusive properties and technology designed to seamlessly connect buyers and sellers, Auction.com empowers residential real estate investors and financial institutions to achieve optimal, mutually beneficial results – to go beyond the bid.\\n\\nSenior Data Engineer\\n\\nPosition Summary\\nAt Auction.com, we are embarking on a journey to transform the real estate market with technological innovations. A critical prerequisite for this transformation is a robust data infrastructure. As a senior data engineer, you will help us design and implement our big data environment that is real-time, stable and scalable. You will work with a talented data engineering team to improve our data processing pipeline and developing new capabilities to support mission-critical initiatives. You will mentor junior engineers and help evaluating new technology along the way. You impact will be felt across the team as well as the entire Auction.com organization.\\n\\nResponsibilities/Duties\\n\\nMake major contribution to the implementation of our real time big data initiative\\nBuild and automate productized data processing pipelines in AWS big data platform\\nHelp improve our development process and standards through mentoring and leading-by-example\\nHelp define and implement data ingestion contracts\\nCreate data environment to support our data analytics, reporting and data science teams\\n\\nKnowledge, Skills and Abilities\\n\\nIn-depth understanding of modern big data technology, including Hadoop and Spark\\nKnowledge of real time data streaming and aggregation architectural patterns and practice\\nProficient in programming languages such as Python, Scala and Java\\nFamiliarity with NoSQL as well as SQL databases\\nData modeling and machine learning skill is a plus\\n\\nEducation/Experience\\n\\nBachelor's Degree in computer science, data science or related fields\\nFamiliarity with agile developmental process\\nPrevious experience developing data product required\\nHands-on experience with real time data streaming, aggregation and presentation strongly preferred\\nPrevious experience with production ETL pipeline development required\\nAt least a years’ experience with AWS cloud or another cloud platform\\n\\nTo all recruitment agencies: Auction.com does not accept agency resumes unless you are part of our preferred partner network. Please do not forward resumes to our jobs alias, Auction.com employees or any other company location. Auction.com is not responsible for any fees related to unsolicited resumes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Data Engineer - Ad Platforms</td>\n",
       "      <td>Santa Clara Valley, CA 95014</td>\n",
       "      <td>Santa Clara Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>95014</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: May 30, 2019\\nWeekly Hours: 40\\nRole Number: 200063498\\nAt Apple, we work every day to create products that enrich people’s lives. Our Advertising Platforms group makes it possible for people around the world to easily access informative and imaginative content on their devices while helping publishers and developers promote and monetize their work. Today, our technology and services power advertising in Search Ads in the App Store and Apple News. Our platforms are highly-performant, deployed at scale, and setting new standards for enabling effective advertising while protecting user privacy.\\nWe are looking for an ambitious and versatile engineer who will be a key member of our team, delivering data solutions to important business problems. Responsibilities range from core storage and processing capabilities, to mission-critical pipelines, to supporting online serving architectures, and leading edge, privacy preserving machine learning platforms. You will have the opportunity to define, refine, and/or refactor approaches, designs, and architectures to meet the data engineering challenges we must solve. You will join a team of world-class data engineers hungry to apply leading-edge technologies to deliver extraordinary experiences to our customers. You will play a meaningful role building data products that deliver on Apple's privacy commitments and change the way advertising works with data. You will collaborate closely with the business to deliver relevant data and insight to inform our strategy and decisions.\\nYou should have experience in data engineering roles, ideally within the ads or media space. You will have an excellent understanding of scalable approaches and thrive working in Agile environments. The ability to be a good team player under tight deadline constraints is key to success.\\nKey Qualifications\\nYou are a clear and effective communicator, and enjoy collaborative problem solving\\nYou love working on a shared codebase that supports web-scale, mission critical applications; and the discipline that it requires\\nYou understand modern data engineering approaches, stay on top of developments, and are aware of what leading players are doing\\nYou have a demonstrated ability to implement and extend highly performant, resilient, reliable, and understandable data pipelines\\nYou have experience with Spark, Hadoop, Kafka or other distributed systems\\nYou have deep expertise in Python, Java, Scala, SQL, and/or other relevant languages and frameworks\\nYou have experience with Oracle, Postgres, mySQL, or other relational databases\\nYou have worked in cloud environments and are familiar with object stores, and other common cloud-native data storage and processing frameworks\\nYou have worked in CI/CD environments\\nYou have experience with pipelines and architectures that support machine learning development platforms and production applications\\nYou are familiar with A/B and other online testing applications\\nYou understand statistics and are capable of using data analysis techniques to understand data quality, profile system loads, understand the relationships between business metrics, and similar\\nDescription\\nAt Ad Platforms, we are constantly developing data products to provide amazing user experiences and drive value for publishers and developers. We are looking for a data engineer to deliver our future data systems, by working to:\\n- Design and implement new pipelines, storage, and processing solutions using modern, Distributed Systems approaches and technologies\\n- Advocate for the design and implementation approaches you propose\\n- Follow best practices in storage, processing, copy/synchronization, etc. appropriate to the scale and maturity of our products\\n- Work in cross-functional teams to prototype new concepts and deliver end-to-end systems in an agile setting\\n- Produce high quality systems with excellent reliability and scalability\\n- Work closely with partners across the organization to create and build data-driven products\\nEducation &amp; Experience\\nBachelors degree in Computer Science, Distributed Systems, Software Engineering, or related field and experience designing, building, maintaining, and extending web-scale production data systems.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Big Data Engineer - Masters (Co-Op) – United States</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>What You’ll Do\\nDesign and deliver automated transformation of large data sets leveraging MapReduce, streaming, and other emerging technologiesLeverage HBase, Elasticsearch, etc. to ingest transformed data at scaleCollaborate with security experts to deliver high-impact web-based APIs\\n Implement high-volume data integration solutions Analyze, monitor, and optimize for performance\\nProduce and maintain high-quality technical documentation\\nWho You'll Work With\\nJoin us as we transform the world of tomorrow. Develop creative ideas on how to work better and smarter. Influence and participate in top-priority projects that have a real impact.\\nWho You Are\\nCurrently enrolled in an accredited university co-op program pursuing a Master’s degree in Computer Science, Computer Engineering, Electrical Engineering, or a related major such as Math, PhysicsMinimum of a 3.0 GPA or equivalentTrack record of developing technology to enable large scale data transformationStrong Java experience and hands-on Hadoop ecosystem experience – HBase, Hive, Spark, etc.Possess knowledge of software engineering best practicesPassion for solving hard problems and exploring new technologiesExcellent communication and technical documentation skills\\nWhy Cisco\\n#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.\\nWe embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong!) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box!\\nBut “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)\\nDay to day, we focus on the give and take. We give our best, we give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart. Because without diversity of thought and a commitment to equality for all, there is no moving forward.\\nSo, you have colorful hair? Don’t care. Tattoos? Show off your ink. Like polka dots? That’s cool. Pop culture geek? Many of us are. Passion for technology and world changing? Be you, with us!\\n\\nThis position is available to Master’s level Students. Positions are located East Coast, West Coast and Central US. Not all positions offer sponsorship or are available at all locations. Relocation is available for some locations and or positions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Principal Data Engineer</td>\n",
       "      <td>Mountain View, CA 94043</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>94043</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nProactively drive the execution of our data engineering, fraud detection roadmap\\nDrive design and implementation of durable &amp; efficient software solutions that handles massive amount of structured and unstructured data\\nDevelop and scale data infrastructure that powers batch and real-time data processing\\nBuild strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs\\nResearch, analyze and select technical approaches for solving difficult and challenging development and integration problems\\nCoach and mentor other engineers in process and methodologies\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nComputer Science or engineering degree\\n8+ years software development experience\\n3+ years working experience with big data ecosystem(Spark, Hive, HBase, etc.), solid experience with ETL or Data Warehouse\\nPrior experience working with structured and unstructured dataand Cloud technologies like AWS, Docker, Kubernetes\\nSolid experience with handling large data volumes in a distributed environment and web data management\\nGood at one or more of program languages (JAVA, Scala, Python, etc.)\\nExcellent communication and presentation skills\\nAbility to think outside-the-box and challenge conventional wisdoms\\n</td>\n",
       "      <td>Coupang is one of the largest and fastest growing e-commerce platforms on the planet. Our mission is to create a world in which Customers ask \"How did I ever live without Coupang?\" We are looking for passionate builders to help us get there. Powered by world-class technology and operations, we have set out to transform the end-to-end Customer experience -- from revolutionizing last-mile delivery to rethinking how Customers search and discover on a truly mobile-first platform. We have been named one of the \"50 Smartest Companies in the World\" by MIT Technology Review and \"30 Global Game Changers\" by Forbes.\\n\\nCoupang is a global company with offices in Beijing, Los Angeles, Seattle, Seoul, Shanghai, and Silicon Valley.\\n\\nJob Overview:\\nThe eCommerce Fraud Detection team is dedicated in making Coupang's eCommerce business fraud free. We are seeking a talented, enthusiastic and technology-proficient Big Data engineer, who is eager to participate in design and implementation of a large-scale, highly efficient data platform, batch and real-time pipelines and tools. You will work closely with a team of data scientists, business analysts and engineers to ensure we detect fraud in e-commerce business and take actions with them to protect our business.\\n\\nKey Responsibilities:\\n\\nProactively drive the execution of our data engineering, fraud detection roadmap\\nDrive design and implementation of durable &amp; efficient software solutions that handles massive amount of structured and unstructured data\\nDevelop and scale data infrastructure that powers batch and real-time data processing\\nBuild strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs\\nResearch, analyze and select technical approaches for solving difficult and challenging development and integration problems\\nCoach and mentor other engineers in process and methodologies\\n\\nRequirements:\\n\\nComputer Science or engineering degree\\n8+ years software development experience\\n3+ years working experience with big data ecosystem(Spark, Hive, HBase, etc.), solid experience with ETL or Data Warehouse\\nPrior experience working with structured and unstructured dataand Cloud technologies like AWS, Docker, Kubernetes\\nSolid experience with handling large data volumes in a distributed environment and web data management\\nGood at one or more of program languages (JAVA, Scala, Python, etc.)\\nExcellent communication and presentation skills\\nAbility to think outside-the-box and challenge conventional wisdoms\\n\\nPreferred:\\n\\nExperience in eCommerce or payment related field\\nExperience in development of Petabyte-Scale NoSQL database and Messaging Platform\\n\\nPerks:\\n\\nAutonomy to make decisions in a rapidly growing company\\n15 days PTO + 15 national holidays off\\n401K matching\\nPre-IPO stock options\\nMobile &amp; fitness reimbursement\\nCatered Lunch\\n\\nCoupang is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex or gender (including pregnancy, gender identity, gender expression, sexual orientation, transgender status), national origin, age, disability, medical condition, HIV/AIDS or Hepatitis C status, marital status, military or veteran status, use of a trained dog guide or service animal, political activities, affiliations, citizenship, or any other characteristic or class protected by the laws or regulations in the locations where we operate.\\n\\nIf you need assistance and/or a reasonable accommodation in the application or recruiting process due to a disability, please contact us at usrecruiting@coupang.com ( usrecruiting@coupang.com ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Data Engineer, Partnership Analytics</td>\n",
       "      <td>Menlo Park, CA</td>\n",
       "      <td>Menlo Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.\\nFacebook is looking for exceptionally talented and experienced data engineers to join the Partnerships Analytics team. The Partnerships team at Facebook works with leading content creators, publishers, and businesses in entertainment, sports, news, and many other verticals. This role is a unique opportunity to work with one of the most important datasets in the world to create analytics tools and systems that enable field organizations, analysts and clients alike. You will work with some of the brightest minds in the industry, and get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match. The ideal candidate will have a passion for problem solving and a belief in the power of incremental change in a fast-paced environment. The role will report to the Data Infrastructure Lead and is based in Menlo Park.\\nRESPONSIBILITIES\\nDevelop methods to unlock access to data for stakeholders across the Partnerships Organization\\nDesign, build and launch efficient &amp; reliable data pipelines to move and transform data (both large and small amounts)\\nOptimize and maintain existing pipelines, ensuring that data arrives accurately and on-time\\nBuild and ensure data tables can serve as the source of truth for various high-level priorities\\nCreate scripts to automate operational processes\\nWork cross-functionally to define problem statements, collect data, and make recommendations\\nBuild data expertise and own data quality for allocated areas of ownership\\nDefine and manage SLA for all data sets in allocated areas of ownership\\nWork with data infrastructure to triage infra issues and drive to resolution\\nSupport on-call shift as needed to support the team\\nMINIMUM QUALIFICATIONS\\nBS/BTech in Computer Science, Math or related field\\n4+ years experience in the data warehouse space\\n4+ years experience in custom ETL/data pipeline design, implementation and maintenance\\n4+ years of SQL (Oracle, Vertica, Hive, etc.) or relational database experience (Oracle, MySQL) writing queries\\nExperience with programming languages, Python\\nExperience with data architecture, data modeling, schema design and software development\\nExperience with large data sets, Hadoop, and data visualization tools\\nExperience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholders\\nPREFERRED QUALIFICATIONS\\nExperience with packages such as R, Tableau, SPSS, SAS, STATA, etc.\\nFamiliar with version control systems (git, mercurial, etc.)\\nFacebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Cloud Data Engineer - GCP</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Responsibilities :\\nStrong Cloud experience and at least 6 months of GCP hands-on project experience in\\nCloud Migration (IAAS)BigData/BigQuery/DataProc/CloudStorage/Object Storage etcKnowledge of Containers &amp; Microservices would be a great plusStrong GCP cloud &amp; data migration experience\\n- Google Cloud Certified - Professional Data Engineer – Certificate is a MUST\\nBig Data cloud migration to GCP experience is a GREAT PLUSStrong client communication skills &amp; Interpersonal skillsShould have experience with the distributed team\\n\\nPosition :Cloud Data Engineer - GCP\\n\\nLocation :San Jose\\n\\nLast Date To Apply :September 30, 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Big Data Engineer- Contractor</td>\n",
       "      <td>Redwood City, CA</td>\n",
       "      <td>Redwood City</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nResponsible for design, implementation, and ongoing support of the Big Data platforms (Hadoop, HBase, HIVE, Spark), ensure high availability and reliability\\nMigrating large data sets between data centers and the cloud (such as AWS SQL server and Hadoop)\\nDesign, test and implement cloud BI / DW infrastructure\\nUnderstand and support AWS native big data / analytics system\\nUse Streaming, Spark &amp; Big Data technologies to enrich and transform data for real time ingestion and build low latency feeds.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nThorough understanding of Hadoop ecosystem (HDFS, YARN, Hive, Pig, MapReduce, Spark, Spark2, Sqoop, Solr, kafka, oozie)\\nStrong experience in setting up, configurating, upgrading and managing security for Hadoop clusters, setting up Ranger policies for HDFS and Hive\\nExperience in managing Hadoop cluster with Ambari and developing custom tools/scripts to monitor the Hadoop Cluster health\\nStrong knowledge and hands on experience related to mission critical backup and recovery\\nStrong experience with load balancing and high volume, high availability environments\\nAble to automate administrative tasks using scripting languages (Python, Shell, Ansible)\\nStrong working experiences of implementing Big Data processing using MapReduce algorithms and Hadoop/Spark APIs\\nExperience building workflow to perform predictive analysis, muilti-dimensional analysis, data enrichments etc\\nUnderstanding of software development methodologies and coding standards.\\nA burning desire to master new technologies and apply them to real world challenges\\n</td>\n",
       "      <td>Auction.com is the nation’s leading online real estate marketplace focused exclusively on the sale of residential bank-owned and foreclosure properties via online auctions and live trustee sale events. By offering access to exclusive properties and technology designed to seamlessly connect buyers and sellers, Auction.com empowers residential real estate investors and financial institutions to achieve optimal, mutually beneficial results – to go beyond the bid.\\n\\nBig Data Engineer- short term project.\\n\\nAre you looking to work on a challenging short term project with a Leading Online Real Estate Organization that has an amazing company culture. If this sounds like a good fit for you, please read on to view the information about this exciting/challenging project.\\n\\nWhat you will be working on;\\n\\nResponsibilities:\\n\\nResponsible for design, implementation, and ongoing support of the Big Data platforms (Hadoop, HBase, HIVE, Spark), ensure high availability and reliability\\nMigrating large data sets between data centers and the cloud (such as AWS SQL server and Hadoop)\\nDesign, test and implement cloud BI / DW infrastructure\\nUnderstand and support AWS native big data / analytics system\\nUse Streaming, Spark &amp; Big Data technologies to enrich and transform data for real time ingestion and build low latency feeds.\\n\\nRequirements:\\n\\nThorough understanding of Hadoop ecosystem (HDFS, YARN, Hive, Pig, MapReduce, Spark, Spark2, Sqoop, Solr, kafka, oozie)\\nStrong experience in setting up, configurating, upgrading and managing security for Hadoop clusters, setting up Ranger policies for HDFS and Hive\\nExperience in managing Hadoop cluster with Ambari and developing custom tools/scripts to monitor the Hadoop Cluster health\\nStrong knowledge and hands on experience related to mission critical backup and recovery\\nStrong experience with load balancing and high volume, high availability environments\\nAble to automate administrative tasks using scripting languages (Python, Shell, Ansible)\\nStrong working experiences of implementing Big Data processing using MapReduce algorithms and Hadoop/Spark APIs\\nExperience building workflow to perform predictive analysis, muilti-dimensional analysis, data enrichments etc\\nUnderstanding of software development methodologies and coding standards.\\nA burning desire to master new technologies and apply them to real world challenges\\n\\nNice to have:\\n\\nRelational design, understand business requirements and perform data design reviews\\nData migration ETL concepts, open source ETL tools\\nWorking experience at a web or internet start-up experience\\nKnowledge of the real estate industry\\n\\nTo all recruitment agencies: Auction.com does not accept agency resumes unless you are part of our preferred partner network. Please do not forward resumes to our jobs alias, Auction.com employees or any other company location. Auction.com is not responsible for any fees related to unsolicited resumes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Senior Application Support Analyst (Temp. 6 months)</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Named as one of Fortunes’ 100 Fastest Growing Companies for 2019, EPAM is committed to providing our global team of 30,100+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential.\\n\\nDescription\\n\\nYou are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Application Support Analyst. Scroll down to learn more about the position’s responsibilities and requirements.\\n\\nEPAM Systems is seeking a Cloud Platform/Big Data Support Specialist to provide enterprise-level support to customers of a major cloud service provider. As a cloud support specialist, you will work in a team of experienced support engineers to resolve customer's concerns and issues for using cloud platform and big data products.\\n\\nYou would use your technical expertise and communication skills to understand customer's problem, provide technical assistance, then guide them to resolution. You would also participate in discussion with product engineers to share your insights on customer needs and issues to help product improvements. You will be fully exposed to the cutting edge technologies of a prominent cloud service provider, and play a key role in the growth of their cloud computing products.\\n\\n#LI-DNI\\nWhat You’ll Do\\nProvide technical assistance and support over e-mail, chat and phone as part of a global 24x7-support organization\\nProvide initial response to customer's inquiry, troubleshoot, provide updates, identify root case, and resolve the issue to the satisfaction of customer\\nHandle escalation from customer and lead to satisfactory resolution\\nCo-work with engineers across technical and product domains to resolve complex cross-domain issues\\nConsult with senior engineers and subject matter experts (SME) to accelerate problem resolution\\nHand-off or take-over cases to/from other geographical region to provide around-the-clock issue resolution for premium customers\\nFollow communication guidelines and security policies when communicating with customer\\nCategorize support requests for support and service analytics\\nProduce support documents, perform knowledge sharing and training\\nKeep technical skills up to date with latest cloud technologies\\nWhat You Have\\nA degree in an associated field and/or other advanced certification along with significant experience\\nStrong analytical / troubleshooting / problem solving skills\\nStrong verbal and written communication skills\\nExcellent customer service skills\\nAbility to perform job functions under stress and pressure\\nCommitment to continuous self-learning\\nRegular, reliable attendance\\n3+ years of experience as developer or a combination developer + big data engineer\\nProficient in at least one of the following development languages: Java, Python, .NET, Ruby, PHP, Go or Javascript (NodeJS)\\nHands on experience with RESTful APIs\\nExperience with relational databases (e.g. MySQL, PostgreSQL, etc.)\\nExperience with Big Data architectures and technologies and BI solutions\\nExperience in CI/CD, DevOps and related automation tools (e.g. Jenkins, Chef, Puppet, etc.)\\nAbility to read and understand code and able to write code samples to reproduce customer issues\\nAbility to read and understand logs and stack traces to troubleshoot issues\\nGood oral and written business communication skills in English (CEF Level C1 or above)\\nMust be able to work on the following shifts:\\nEarly week shift from 7:00 AM to 6:00 PM, Sunday to Wednesday\\nLate week shift from 7:00 AM to 6:00 PM, Wednesday to Saturday\\nYes, you will work 4 days and take 3 days off\\nMay need to work on public holidays. If worked on a public holiday, you will be provided with a day-off in lieu\\nNice to have\\nBA/BS degree preferred\\n2+ years of customer support experience preferably in Enterprise software support\\nExperience with PaaS and IaaS technologies\\nExperience with distributed computing frameworks (e.g. Hadoop, Spark, Flink, Storm, Samza, Beam, Airflow, Google Big Query, etc.)\\nExperience with distributed data stores (HBase, Cassandra, Riak, Google Bigtable, Amazon Dynamo DB, etc.) and/or distributed message brokers (Kafka, RabbitMQ, ActiveMQ, Google Pub/Sub, Amazon Kinesis, etc.)\\nExperience with ETL processes and tools (e.g. AWS Glue, Google Dataprep and/or Datafusion, MS SSIS, ODI, IPC, etc.)\\nExperience with any ML library (scikit-learn, XGBoost, pytorch, tensorflow, Spark mllib) or basic understanding of ML concepts\\nWhat We Offer\\nMedical, Dental and Vision Insurance (Subsidized)\\nHealth Savings Account\\nFlexible Spending Accounts (Healthcare, Dependent Care, Commuter)\\nShort-Term and Long-Term Disability (Company Provided)\\nLife and AD&amp;D Insurance (Company Provided)\\nEmployee Assistance Program\\nUnlimited access to LinkedIn learning solutions\\nMatched 401(k) Retirement Savings Plan\\nPaid Time Off\\nLegal Plan and Identity Theft Protection\\nAccident Insurance\\nEmployee Discounts\\nPet Insurance\\nEPAM welcomes all applicants and will consider qualified candidates with criminal history such as arrest and conviction records in a manner consistent applicable law, including the San Francisco Fair Chance Ordinance and Los Angeles Fair Chance Initiative for Hiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBig data tools (Spark, Hadoop)\\nRelational and NoSQL Databases (Postgres and Mysql)\\nAWS Cloud Services (EMR, RDS, Redshift, Kinesis, SQS, S3, Glue etc.)\\nWorkflow management tools (Airflow, AWS Data pipeline)\\nLanguage Skills (Python o R)\\nAbility to do analysis by SparkSQL, Athena, Redshift in Zeppelin or Jupyter notebook or Tableau\\n</td>\n",
       "      <td>\\nIndependently design, maintain, and enhance big data pipeline, including ETL and data analytics.\\nBuild and scale systems that orchestrate and execute complex workflows in big-data pipelines\\nSupport team members to proactively anticipate and resolve issues\\nWork with marketing and dev teams to build data-driven business\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nA Bachelors or Masters in Computer Science, MIS, Statistics, Economics, Mathematics, etc\\n5+ years of experience in a data engineering role\\nMust be a problem solver\\nMust thrive in a fast-paced working environment\\nExcellent communicator\\nSelf motivated\\n</td>\n",
       "      <td>Omlet Arcade is the community for mobile game streamers and e-sports. Every month we bring millions of gamers together to show off their gameplay and make new friends. We hatched up Omlet Arcade to serve the next generation of gamers; those who no longer need PCs or consoles due to the overwhelming power of the phones at their fingertips. This year has shown we're on the right path as AAA games like Fortnite and PUBG light up our mobile screens. In 2019, Omlet is taking it to the next level, bringing mobile e-sports to the world by empowering players with the tools they need to strut their skills, build amazing teams, and have a blast competing with each other.\\n\\nWe are looking for a Senior Data Engineer to join our Engineering ream, this person will be responsible for turning data into information, information into insights and insights into key business decisions, that steer the strategic direction of the company.\\n\\nResponsibilities:\\n\\nIndependently design, maintain, and enhance big data pipeline, including ETL and data analytics.\\nBuild and scale systems that orchestrate and execute complex workflows in big-data pipelines\\nSupport team members to proactively anticipate and resolve issues\\nWork with marketing and dev teams to build data-driven business\\n\\nRequirement:\\n\\nA Bachelors or Masters in Computer Science, MIS, Statistics, Economics, Mathematics, etc\\n5+ years of experience in a data engineering role\\nMust be a problem solver\\nMust thrive in a fast-paced working environment\\nExcellent communicator\\nSelf motivated\\n\\nSkills:\\n\\nBig data tools (Spark, Hadoop)\\nRelational and NoSQL Databases (Postgres and Mysql)\\nAWS Cloud Services (EMR, RDS, Redshift, Kinesis, SQS, S3, Glue etc.)\\nWorkflow management tools (Airflow, AWS Data pipeline)\\nLanguage Skills (Python o R)\\nAbility to do analysis by SparkSQL, Athena, Redshift in Zeppelin or Jupyter notebook or Tableau\\n\\nPerks &amp; Benefits:\\n\\nCompetitive Salary\\nMedical, Dental, Vision\\nUnlimited PTO\\nFree catered lunch through Door Dash 5 days per week\\nFriendly working environment\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Querying and manipulating large data sets for analytical purposes using SQL-like languages (Hive is strongly preferred)</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Seeking Data Engineer\\n\\n\\nDesign, build, and manage complex analytics data models in Hive/Hadoop for Analytics team across all customer journey from Acquisition, Engagement, and Retention. The analytics data marts will be used by data analysts in Analytics and other team to do deep dive analysis, build analytics dashboard, or other data science project. Design, build, deploy, and maintain new data models ETL pipeline with SQL query, Python, Oozie, and other script language. Create/maintain workflow and ensure overall data quality.\\n\\n\\nQualifications:\\nQuerying and manipulating large data sets for analytical purposes using SQL-like languages (Hive is strongly preferred)\\nExperience with Hadoop/big data environments to synthesize and analyze data.\\nProfessional experience in the data warehouse space\\nGood attention to detail and ability to QA multiple data sources\\nExperience working on building scalable ETL pipelines, data warehousing and schema modeling\\nExperience working with Oozie Workflow or others\\nExperience with script language such as Python\\nWe are a software company focused on emgergin tecnologies - delivering data, analystics, AI and RPA solutions that drive growth &amp; opportunities.\\n\\n\\nLearn more at www.dataflix.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Staff Software Engineer, Data Team (Big Fast Data)</td>\n",
       "      <td>Sunnyvale, CA 94087</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>94087</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Collaborating closely with application team architects and engineers to identify technologies and platforms suitable for their big data processing requirements, and then assisting those teams with onboarding, development, deployment, and debugging on those platforms</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Position Description\\n\\nDevelops Innovation strategies, processes, and best practices\\nDrives the execution of multiple business plans and projects\\nEnsures business needs are being met\\nLeads and participates in medium- to large-scale, complex, cross-functional projects\\nLeads the discovery phase of medium to large projects to come up with high level design\\nLeads the work of other small groups of six to ten engineers, including offshore associates, for assigned Engineering projects\\nPromotes and supports company policies, procedures, mission, values, and standards of ethics and integrity\\nProvides supervision and development opportunities for associates\\nSupports business objectives\\nTroubleshoots business and production issues\\nUtilizes industry research to improve Wal-Mart's technology environment\\n\\nMinimum Qualifications\\nBachelor's Degree in Computer Science or related field and 6 years experience building scalable ecommerce applications or mobile software\\nAdditional Preferred Qualifications\\nPlease add text\\nCompany Summary\\nThe Walmart eCommerce team is rapidly innovating to evolve and define the future state of shopping. As the world’s largest retailer, we are on a mission to help people save money and live better. With the help of some of the brightest minds in technology, merchandising, marketing, supply chain, talent and more, we are reimagining the intersection of digital and physical shopping to help achieve that mission.\\nPosition Summary\\nThe Walmart Labs Big Data Platforms team is seeking a big data engineer to serve in a consulting engineering and support role for the big data components of Walmart’s major application and project initiatives. The Big Data Platforms team operates multi-tenant persistent Hadoop-based platforms based on several Hadoop distributions, and also leads architecture and integration engineering for Walmart’s cloud-based big data platforms using both on-premise and commercially available cloud resource providers.\\n\\nWe’re looking for a software engineer that has extensive experience building and supporting big data applications using the Hadoop ecosystem and related technologies, both on traditional clusters and cloud platforms, to collaborate with Walmart’s internal product development teams to help them construct scalable and performant big data applications, and also help application teams troubleshoot big data problems when things go wrong.\\n\\nResponsibilities include:\\n\\nCollaborating closely with application team architects and engineers to identify technologies and platforms suitable for their big data processing requirements, and then assisting those teams with onboarding, development, deployment, and debugging on those platforms\\nProviding technical engineering and performance tuning assistance to a broad community of big data infrastructure users, such as software application engineers and data scientists, through research, investigation, collaboration, and hands-on debugging when necessary\\nInvestigating new big data tools and technologies for their potential application to common use cases; establishing best practices, developing design patterns, and writing documentation to disseminate new capabilities to a broad technical audience; working with platform engineers and product managers to specify and deliver new major technology features\\nEnsuring that application big data solutions adhere to best practices and enterprise standards for scalability, availability, efficiency, data lifecycle management, information security, fault tolerance, and disaster recovery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nJoin an agile SaaS team to design, develop and maintain features and iteratively deploy services using Infoblox’s cloud-based architecture\\nDesign and implement components of our Next Generation Platform\\nRecommend ways to improve data reliability, efficiency and quality\\nExpand and grow data platform capabilities to solve new data problems and challenges\\nBuild large-scale data processing systems using cloud computing technologies\\nBuild high-performance algorithms, prototypes, and proof of concepts\\nApply complex big data concepts with a focus on collecting, parsing, managing, and analyzing large sets of data to turn information into insights\\nWork closely with various cross functional product teams\\nStay current on key trends especially in the area of technologies and frameworks like: Mesos/Marathon, Kubernetes, Docker etc.</td>\n",
       "      <td>\\nBachelor’s degree in CS, CE or EE is required\\nMasters in CS, CE, or EE is preferred</td>\n",
       "      <td>\\n8+ years experience, 2+ in Big Data Engineering\\nProficient in Java, Scala, Golang, or Python\\nGood understanding of Microservices architecture\\nExpertise in BigData - MapReduce, HIVE, HBase, Spark streaming, Apache Flink, Storm, Kafka, In memory Database, JMS\\nExperience with NoSQL databases such as Cassandra/DynamoDB\\nGood exposure in application performance tuning, memory management, scalability\\nAbility to design highly scalable distributed systems, using different open source technologies\\nExperience building high-performance algorithms</td>\n",
       "      <td>Infoblox is the global leader in providing actionable network intelligence through network services, security and threat intelligence. We give companies total control and visibility of their network, allowing them to operate more efficiently and intelligently.\\nWe are looking for a Staff Software Engineer to join our SaaS Next Generation Platform Team in Santa Clara, CA. In this role, you will be responsible for developing, maintaining, evaluating and testing big data technologies. Our organization is extremely data driven where technical innovations happen and you will have an opportunity to use cutting edge technology across all stages of development lifecycle and be part of our exciting and innovative initiatives.\\nResponsibilities:\\n\\nJoin an agile SaaS team to design, develop and maintain features and iteratively deploy services using Infoblox’s cloud-based architecture\\nDesign and implement components of our Next Generation Platform\\nRecommend ways to improve data reliability, efficiency and quality\\nExpand and grow data platform capabilities to solve new data problems and challenges\\nBuild large-scale data processing systems using cloud computing technologies\\nBuild high-performance algorithms, prototypes, and proof of concepts\\nApply complex big data concepts with a focus on collecting, parsing, managing, and analyzing large sets of data to turn information into insights\\nWork closely with various cross functional product teams\\nStay current on key trends especially in the area of technologies and frameworks like: Mesos/Marathon, Kubernetes, Docker etc.\\nRequirements:\\n8+ years experience, 2+ in Big Data Engineering\\nProficient in Java, Scala, Golang, or Python\\nGood understanding of Microservices architecture\\nExpertise in BigData - MapReduce, HIVE, HBase, Spark streaming, Apache Flink, Storm, Kafka, In memory Database, JMS\\nExperience with NoSQL databases such as Cassandra/DynamoDB\\nGood exposure in application performance tuning, memory management, scalability\\nAbility to design highly scalable distributed systems, using different open source technologies\\nExperience building high-performance algorithms\\nEducation\\nBachelor’s degree in CS, CE or EE is required\\nMasters in CS, CE, or EE is preferred\\nIt’s an exciting time to be at Infoblox. We are the market leader in technology for network control. Our success depends on bright, energetic, talented people who share a passion for excellence in building the next generation of networking technologies—and having fun along the way. Infoblox offers a fast-paced, action-oriented environment. We promote a culture that embraces innovation, change, teamwork, and strong partnerships. Join the winning Infoblox team—our future looks bright, and so will yours. To check out what it’s like to be a Bloxer click here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Senior Data/Server Engineer</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Niantic, the developer behind popular games like Pokemon GO and Harry Potter: Wizards Unite is searching for a Senior Data Engineer with an extensive server infrastructure background. Join a group of experienced engineers to help build and scale Niantic's core data infrastructure. This is a nimble, motivated team responsible for building trust, owning data integrity, and supporting data-driven decision-making at Niantic.\\nResponsibilities\\nArchitect the data stack responsible for storing and processing enormous volumes of analytics data.\\nDesign efficient, extensible data models for use with Niantic's data pipelines and analytics systems.\\nImprove and extend core data competency for the User Acquisition pipeline to facilitate tools and reporting in support of growing UA efforts.\\nOrganize and secure data drawn from diverse sources and build streamlined ETL pipelines to transform and validate it.\\nMentor and offer technical guidance to data engineers, data scientists, and infrastructure engineers.\\nWork with the Product Team and Management to define a shared vision, an execution strategy, and communicate timeline and trade-offs.\\nQualifications\\n4+ years of experience developing and deploying robust, large-scale data pipelines.\\nA high degree of attention to detail and clear aptitude for finding and resolving data integrity issues.\\nProven success in securing and auditing data stores and implementing legal compliance, e.g. GDPR.\\nDeep knowledge of available data storage technologies such as Hadoop, Cassandra, Druid, and their trade-offs.\\nExcel in developing general purpose solutions to difficult problems and building elegant solutions.\\nStrong communicator to both technical and non-technical people and demonstrated ability to document technical design decisions.\\nExpert in Java or Scala, Python and SQL.\\nBS, MS, or PhD in Computer Science or a related technical field.\\nPlus If...\\nFamiliarity with mobile advertising, user acquisition and associated data processing and metrics (e.g. attribution, retention, CPI, ROAS).\\nDetailed knowledge of and experience with the large advertising networks, e.g. Google, Facebook, Twitter, Apple.\\nKnowledge of the Google data stack (e.g. Dataflow, BigQuery, BigTable).\\nProficient in the use of Airflow, Composer.\\nJoin the Niantic team!\\nNiantic is the world’s leading AR technology company, sparking creative and engaging journeys in the real world. Our products inspire outdoor exploration, exercise, and meaningful social interaction.\\nOriginally formed at Google in 2011, we became an independent company in 2015 with a strong group of investors including Nintendo, The Pokémon Company, and Alsop Louie Partners. Our current titles include pioneering global-control game Ingress, record-breaking AR game Pokémon GO, and recently released third title, Harry Potter: Wizards Unite.\\nNiantic is an Equal Opportunity and Affirmative Action employer. We believe that cultivating a workplace where our people are supported and included is essential to creating great products our community will love. Our mission emphasizes seeking and hiring diverse voices, including those who are traditionally underrepresented in the technology industry, and we consider this to be one of the most important values we hold close.\\nWe're a hard-working, fun, and exciting group who value intellectual curiosity and a passion for problem-solving! We have growing offices located in San Francisco, Sunnyvale, Bellevue, Los Angeles, London, Tokyo, Hamburg, and Zurich.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Santa Clara, CA 95054</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>95054</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\nJob Title: Data Engineer\\nLocation: San Francisco, CA, Austin, TX, San Jose, CA\\nTerms: Full-time\\nAbout Trianz\\nTrianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.\\nWhat We Stand For\\nOur clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.\\nAs a result, Trianz is focusing on three important themes in our engagement model with clients.\\nCrystallize business impact from a top management point of view\\nHelp Clients achieve results from strategy-by making execution predictable through innovative execution techniques\\nCreate a positive, enriching partnership experience in everything we do\\nIndustries, Clients &amp; Practices\\nTrianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:\\nCloud\\nAnalytics\\nDigitization\\nInfrastructure\\nSecurity\\nJob Description\\nOverview\\nData is the way our clients make decisions. It is the core to their business, helping create an experience for customers and providing insights into the effectiveness of our product launch &amp; features.\\n\\nAs a Data Engineer , you will be a part of an early stage team that builds the data pipelines, collection, and storage, and exposes services that make data a first-class citizen. We are looking for a Data Engineer to build a scalable data platform. You'll have ownership of core data pipelines that powers top line metrics; You will also use data expertise to help evolve data models in several components of the data stack; You will help architect, building, and launching scalable data pipelines to support growing data processing and analytics needs. Your efforts will allow access to business and user behavior insights, using huge amounts of data to fuel several teams such as Analytics, Data Science, Marketplace and many others.\\n\\nResponsibilities\\n\\nOwner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth\\nEvolve data model and data schema based on business and engineering needs\\nImplement systems tracking data quality and consistency\\nDevelop tools supporting self-service data pipeline management (ETL)\\nSQL and MapReduce job tuning to improve data processing performance\\n\\nExperience\\n\\n3+ years of relevant professional experience\\nExperience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)\\nProficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)\\nGood understanding of SQL Engine and able to conduct advanced performance tuning\\nStrong skills in scripting language (Python, Ruby, Bash)\\n1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)\\nComfortable working directly with data analytics to bridge Lyft's business goals with data engineering\\n\\nWe are Growing Rapidly: 2019 Highlights\\nTrianz is growing above the average of the professional services industry. Here are some highlights.\\nVoted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.\\nWon the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.\\nWon UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.\\nFeatured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.\\nAchieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.\\nTalk to us, Join us &amp; Develop into Leaders\\nCome join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.\\nWe are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!\\nEqual Opportunity Employer\\nTrianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Senior Scala Data Engineer</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Scala Data Engineer – HPE InfoSight\\n\\nHPE is seeking an outstanding software engineer to play a key role in helping the HPE InfoSight team build AI for the datacenter. HPE InfoSight allow HPE partners and customers to optimize, manage, and protect their datacenter infrastructure, while helping customer support, engineering, and sales to deliver more value to our customers.\\n\\nYou will be joining a small start up within HPE, agile, empowered team, focused on analyzing call-home data sent from HPE storage and enterprise products to provide business value through analytics. The team leverages a modern big-data and microservice-based technology stack for our end-to-end data processing, analysis, API, and web application – to provide our users with the insights they need to be successful.\\n\\nResponsibilities\\n\\nTechnical contributor as a full-stack developer in a small, cross-functional development team, focused on providing data analytics as a service to internal and external HP customers.Contribute to the continuous improvement of our IoT analytics platform, powered by Scala, Spark, Mesos, Akka, Cassandra, Kafka, Elasticsearch, and Vertica.Develop unit, integration, system or any tests that are needed to help the team deliver value quickly, with high quality, to our customers.Leverage big-data technologies for data analytics, including Hadoop/Spark, Vertica (SQL), and Elasticsearch.Develop automation for continuous delivery, testing, and monitoring of our application and infrastructure, using Mesosphere DCOS, Jenkins, Ansible, Kibana, and others.\\nEducation and Experience\\n\\nBachelor/Master's in Computer Science/Engineering, or equivalent, and a minimum of 5-7 years’ experience.\\n\\nKnowledge and Skills\\nTeam player with a passion for learning, programming, automation, and data analytics.Excellent programming skills, with experience or an interest in learning functional programming.Excellent analytical and problem solving skills.Excellent communications skills.\\nWe are looking for a candidate with some or all of the following:\\n\\nExperience building a data pipeline using Scala, Java, or Python, preferably with Spark and KafkaData analytics experience with SQL, NoSQL, Hadoop, or ideally Spark.Machine learning experienceLinux development or system administration experience, including Python or BASH scripting.Automation experience with Ansible, Chef, Puppet, or other.\\n1038195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Silicon Valley, CA</td>\n",
       "      <td>Silicon Valley</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in CS or related discipline\\n3 - 5 years of experience in using SQL and databases in a business environment\\n3 - 5 years of experience in data warehouse space\\n3 - 5 years of experience in custom ETL design, implementation, and maintenance\\n3 - 5 years of experience with schema design and data modeling\\n2 plus years of experience with programming or scripting languages (e.g. Python or shell scripting)\\nPreferred experience working with either a Map Reduce or an MPP system\\nPreferred experience working with cloud platforms such as AWS, Google Cloud Platform or MS Azure\\nExperience to analyze data to identify deliverables, gaps, and inconsistencies</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nCollaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions\\nDesign and build data extraction, transformation, and loading processes by writing custom data pipelines\\nDesign, implement and support a platform that can provide ad-hoc access to large datasets and unstructured data\\nModel data and metadata to support adhoc and pre-built reporting\\nTune application and query performance using performance profiling tools and SQL\\nBuild data expertise and own data quality for allocated areas of ownership\\nWork with data infrastructure to triage infrastructure issues and drive to resolution</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of what’s possible—together.\\n\\nFounded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to nearly 5,000 employees. We were named one of Fortune’s 100 Best Companies to Work For in 2018 and are regularly recognized by our employees as a best place to work. You can find us in 28 cities across the U.S., U.K., and Canada.\\n\\nJob Title: Data Engineer\\n\\nDo you like working with data? Do you want to use data to influence decisions for products and services being used by business and consumers every day? If yes, we want to talk to you. Slalom is seeking a Data Engineer to join our Data &amp; Analytics team.\\n\\nAs a Data Engineer, you should have expertise in the design, creation, management, and business use of extremely large datasets. You know and love working with analytic tools, can write excellent SQL, scripts (e.g. Python, PL/SQL) and ETL code (e.g. Alteryx, Informatica), and can use your technical skills and creative approaches to solve some unique problems in the BI space. In this role, you will be working across industry sectors such as retail, finance, healthcare and high-tech and you'll get an opportunity to solve some of the most challenging business problems.\\n\\nResponsibilities:\\nCollaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions\\nDesign and build data extraction, transformation, and loading processes by writing custom data pipelines\\nDesign, implement and support a platform that can provide ad-hoc access to large datasets and unstructured data\\nModel data and metadata to support adhoc and pre-built reporting\\nTune application and query performance using performance profiling tools and SQL\\nBuild data expertise and own data quality for allocated areas of ownership\\nWork with data infrastructure to triage infrastructure issues and drive to resolution\\nQualifications:\\n﻿Bachelor’s degree in CS or related discipline\\n3 - 5 years of experience in using SQL and databases in a business environment\\n3 - 5 years of experience in data warehouse space\\n3 - 5 years of experience in custom ETL design, implementation, and maintenance\\n3 - 5 years of experience with schema design and data modeling\\n2 plus years of experience with programming or scripting languages (e.g. Python or shell scripting)\\nPreferred experience working with either a Map Reduce or an MPP system\\nPreferred experience working with cloud platforms such as AWS, Google Cloud Platform or MS Azure\\nExperience to analyze data to identify deliverables, gaps, and inconsistencies\\n\\nSlalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Redwood City, CA</td>\n",
       "      <td>Redwood City</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Course Hero is scaling! We are looking for a motivated, and progressive Senior Data Engineer that will help build our next-generation Data Platform. We are searching for someone who has a devops mentality and passionate about innovating, optimizing and automating data at scale.\\n\\nYou can expect high impact and wide-ranging responsibilities: As a Senior Data Engineer you will have deep hands-on experience working with Data Scientists, Machine Learning experts, and Search Engineers. The scalable systems you build will enable our product designers to bring powerful data products to the Course Hero platform.\\n\\nCheck out these videos to learn more about our engineering culture ( https://www.youtube.com/watch?v=Hpa0bVeJpIE ), and our company mission ( https://www.youtube.com/watch?v=pmzuj0MW_Dk ).\\n\\nHere are some ways you'll make an impact:\\n\\n5+ years relevant data experience\\nWork with a team of passionate data engineers and scientists to enable data mining, deep learning, statistical modeling, predictive analytics, machine learning, and NLP.\\nEmpower engineers and to innovate and discover insights by removing the technical barriers that come with processing big data.\\nEnsure compliance with the organization's high bar for data quality and modeling standards, across the product and related business areas.\\nProvide tools to empower internal teams across the organization (sales, operations, finance, engineering, etc.) to make data-driven decisions.\\nInstitute development best practices to ensure the team produces high quality, well architected and supportable code through a continuous delivery model.\\nParticipate in the on-call rotation and document administration and response procedures through runbooks &amp; playbooks.\\n\\nAre you our Star Senior Data Engineer?\\n\\n\\nWorked with Machine Learning experts and Data Scientists to enable self-service data ingestion, transformation, visualization, reporting and advanced analytics (machine learning, AI).\\nStrategic thinker and thrive operating in a broad scope, from conception through continuous operation.\\nRobust Ops foundation - you're always thinking \"What happens if this fails\" when you build things.\\nBachelor's or Master's degree in computer science, mathematics, economics, engineering, or other related fields.\\nExpert in Mysql and other relational database technologies.\\nHands-on experience working with large scale data ingestion, ETL processing, storage, Hadoop ecosystems, Spark, non-relational databases (NoSQL, MongoDB, Cassandra), and messaging systems (Kafka, Kinesis, RabbitMQ).\\nWritten scripts in one or more languages such as Python or Go.\\n\\nBonus Points:\\n\\nUnderstand open source software like Kafka, Arvo, ElasticSearch, NGINX, Kubernetes, and Docker.\\nFamiliar with Python analytics libraries or use of R language.\\nExperience with standard IT security practices such as Access, Authorization, and Key Management.\\nPerformed hands on work using AWS stack (i.e. S3, Redshift, EC2, SNS, SQS, SES, DynamoDB,Kinesis).\\n\\nAbout Us:\\nAt Course Hero, we have an awesome team and a truly engaging culture. We are customer-focused, collaborative, responsible, gritty and we love to learn. Our bold mission is to help students graduate confident and prepared!\\n\\nWe are not the only ones that think we're onto something big. Course Hero has been recognized as the 245th Fastest Growing Company in North America ( https://www.prnewswire.com/news-releases/course-hero-ranked-number-245-fastest-growing-company-in-north-america-on-deloittes-2018-technology-fast-500-300751425.html ) on Deloitte's 2018 Technology Fast 500 and also 2018's One of the Best Places to Work in the Bay Area ( https://www.bizjournals.com/sanfrancisco/feature/best-places-to-work/2018/best-places-to-work-bay-area-2018-top-workplaces.html ) by the San Francisco Business Times and the Silicon Valley Business Journal. Read up on some of our recent news coverage ( https://www.coursehero.com/press-room/ ), blog ( https://www.coursehero.com/blog/ ), and learn moreabout us ( https://www.coursehero.com/about-us/ ) to see what it is like to work with our team.\\n\\nBenefits &amp; Perks!\\n\\n\\nCompetitive salary and stock options\\nFull medical coverage (medical, dental, vision)\\n401(k) program with match\\nEducation Reimbursement\\nQuarterly team events and outings (Sporting Events, Escape Rooms, Go-Kart Racing, Karaoke, Bowling and much more!)\\nFree lunches twice a week, on-site cafe discount, plus an endless snack and drink supply\\nOnsite gym – Pacific Shores Center – Classes – Pool – Spa – Rock Wall - Massages!\\nCommuter benefits, shuttle service from Redwood City, and cell-phone allowance\\nLocal move benefit to move within 10 miles of our office!\\n8 hours per quarter paid time for volunteering for a cause of your choice\\nFront row seat to Master Educator lectures – check out the videos on our LinkedIn Career Page\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Newark, CA 94560</td>\n",
       "      <td>Newark</td>\n",
       "      <td>CA</td>\n",
       "      <td>94560</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor or Masters in Software Engineering and Computer Science\\n8+ years of experience in design and development of large scale data platforms\\nExpert in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nProficient in Spark development with PySpark or Scala\\nExpert in Data streaming platforms such as Apache Kafka\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python, and/or C++</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a Staff Data Engineer, Big Data who is looking for a challenge, enjoys thinking big and looking to make their mark on an extremely fast growing company. If building large and building fast, working with a young and very talented team of engineers and collaborating with the brightest mind in the Automotive industry is what you like, Lucid is the best to experience it.\\nThe Role\\nLead Data Engineer and architect to design, implement a highly scalable system to ingest and process Petabytes of data per day.\\nHands-on design and develop applications for data pipeline and data management.\\nSet processes and policies for data governance and data pipeline\\nArchitect and implement best practices of big data tools such as Spark, Airflow, Kafka, Presto and Cassandra\\nLead and mentor junior Data and BI engineers.\\nSet and define the standards and best practices in data team\\nBe the point of reference for solving challenging technical problem.\\nArchitect and implement Machine Learning Pipelines for Data Science team\\n\\nQualifications\\nBachelor or Masters in Software Engineering and Computer Science\\n8+ years of experience in design and development of large scale data platforms\\nExpert in containerization, including Docker and Kubernetes\\nExpert in tools such as Apache Spark, Apache Airflow, Presto\\nProficient in Spark development with PySpark or Scala\\nExpert in Data streaming platforms such as Apache Kafka\\nExpert in design and implement reliable, scalable, and performant distributed systems and data pipelines\\nExtensive programming and software engineering experience, especially in Java, Python, and/or C++\\nExperience with running large-scale distributed computing infrastructure such as load balancing, Zookeeper, Micro service architecture\\nExperienced in security and access management\\nExperience with managing distributed databases like Elasticsearch, Cassandra\\nExperience with Columnar database such as Redshift, Vertica\\nGreat verbal and written communication skills.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Earnin:\\nEvery year, while Americans wait for their paychecks, more than $1 trillion of their hard-earned money is held up in the pay cycle. As a result, we accumulate over $50 billion in late and overdraft fees and turn to high-interest loans. Overdraft charges and bank fees often trap people in a cycle of debt that can lead to unhealthy decisions and falling victim to predatory businesses disguised as helpful services. We don't accept that.\\n\\nEarnin is an app that creates products that help people gain control of their finances. Cash Out lets people get paid as soon as they leave work, with no fees, interest, or hidden costs. With Health Aid, Earnin negotiates on behalf of community members to lower their total unpaid medical bill and work out a budget-friendly payment plan. Cash Back Rewards is a way for members to earn up to 10% cash back on purchases from over a thousand local and national businesses without needing a credit card or having to reach spend thresholds to earn cash rewards — and they can withdraw the money at any time. We also offer free tools to help avoid overdrafts, to remind people when recurring bills are due, and we're working on more! There is never any required cost to use any of these products or services, users can choose to tip what they think is fair to support the service and pay it forward to keep the movement going.\\n\\nEarnin is supported by funding partners including Andreessen Horowitz, Matrix Partners, Ribbit Capital, Felicis Venture, Thrive Capital, and others. Join us and help build a new financial system focused on fairness and people's needs.\\n\\nYou can help make a difference.\\n\\nAbout the Team:\\nWe are a data driven mobile financial tech company and we're looking for a Data Engineer to join us and help us build out our data infrastructure to aid in our mission of enabling people to gain access to their paycheck on demand.\\n\\nData engineers are an important function to interact with every team within Earnin and you will be interfacing heavily with our analytics, engineering, and data science teams to help them advance our product utilizing machine learning intelligence.\\n\\nAs a Data Engineer you will:\\n\\nFocus on designing, building, and launching efficient and reliable data infrastructure to scale and compute for our business\\nHelp us build a world class data lake/data warehouse, by building data pipelines\\nDesign and develop new systems and tools to enable folks to consume and understand data faster\\nUse your expert coding skills across a number of languages from Python, Java, C++, Go etc.\\nWork across multiple teams in high visibility roles and own the solution end-to-end\\nDesign, build and launch new data extraction, transformation and loading processes in production\\nWork with data infrastructure to triage infra issues and drive to resolution.\\n\\nSome skills we consider critical to being a Data Engineer:\\n\\nBS or MS degree in Computer Science or a related technical field\\nFamiliarity with Python\\nFamiliarity with Hadoop stack, Spark, AWS Glue, AWS Athena etc\\nDiverse data storage technologies (RDBMS, Sql Server, Mysql, ElasticSearch, dynamodb, s3 etc.)\\nDeep familiarity with schemas, metadata catalogs etc.\\nAbility to manage and communicate data warehouse plans to internal clients\\nStrong communication skills, including the ability to identify and communicate data driven insight\\n\\nEarnin does not unlawfully discriminate on the basis of race, color, religion, sex (including pregnancy, childbirth, breastfeeding or related medical conditions), gender identity, gender expression, national origin, ancestry, citizenship, age, physical or mental disability, legally protected medical condition, family care status, military or veteran status, marital status, registered domestic partner status, sexual orientation, genetic information, or any other basis protected by local, state, or federal laws. Earnin is an E-Verify participant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Data Engineer – Systems and Configuration</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDegree in Computer Science, Engineering, or related fields\\nStrong in data structures, algorithms, and systems\\nDeep understanding of interconnected systems behavior controlled through configuration\\nExperience with modeling system configuration and behavior from a manageability perspective\\nExperience with creating validated solution designs across multiple layers of the data center infrastructure stack in technical collaborations with partners\\nExperience with building and using configuration management systems and databases (e.g.: Chef, Puppet, BMC Remedy, etc.)\\nExperience with handling complex service request fulfillment and production issues\\nHeuristic problem solving with incomplete information\\nDeep domain knowledge and hands-on experience across a very broad spectrum of the backend, frontend, cloud, AI and data infrastructure platforms.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDegree in Computer Science, Engineering, or related fields\\nStrong in data structures, algorithms, and systems\\nDeep understanding of interconnected systems behavior controlled through configuration\\nExperience with modeling system configuration and behavior from a manageability perspective\\nExperience with creating validated solution designs across multiple layers of the data center infrastructure stack in technical collaborations with partners\\nExperience with building and using configuration management systems and databases (e.g.: Chef, Puppet, BMC Remedy, etc.)\\nExperience with handling complex service request fulfillment and production issues\\nHeuristic problem solving with incomplete information\\nDeep domain knowledge and hands-on experience across a very broad spectrum of the backend, frontend, cloud, AI and data infrastructure platforms.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Peritus\\nPeritus enables self-healing autonomous datacenters with automated, cognitive support for infrastructure software and hardware. It is a funded startup co-created at The Hive in Palo Alto, CA that delivers artificial intelligence based virtual support expert systems for datacenter service fulfillment and incident resolution.\\n\\nAs data center vendors move from on premise to the cloud their existing support system lacks the agility and cost-effectiveness for the cloud. Peritus significantly enhances operational efficiencies of existing support services, and enables managed service providers &amp; system vendors to offer new business continuity entitlements. Peritus assists &amp; automates a wide spectrum of decisions in system support including incident classification, routing, contract coverage, incident resolution recipes and orchestration of incident management between subject matter experts (SMEs).\\n\\nPeritus’ unique vectorization of system log data drives predictive modeling with highly granular feature extraction for early detection of system events. The platform’s advanced natural language processing (NLP) capabilities drive Peritus’ incident modeling and predictive capabilities. The core service fulfillment engine uses a combination of supervised and unsupervised methods to predict incident features from system log data. Peritus delivers automated orchestration of incident resolution through its close integration with existing incident management platforms.\\n\\nJob Description\\nWe are building a product that helps customers fulfill service requests as well as troubleshoot and diagnose infrastructure issues that cut across domains. The product needs to interface with configuration management systems/databases to glean insights into how systems are interconnected. Some of the key outcomes relate to the evolution of the interconnected systems over time through timestamped versioning of configuration trees, detection of anomalies via comparative analysis of the system with that of recommended best practices/solutions, and identification of interoperability issues by correlating configuration data across the infrastructure stack.\\n\\nResponsibilities\\nWe are looking to hire an engineering technical leader with deep systems knowledge. The role entails a deeper understanding of how data center systems interoperate, requiring familiarity with computing, storage, networking and virtualization products within a rack and across racks. The whole system behavior needs to be modeled from a manageability perspective to apply any corrective measures. The configuration infrastructure needs to enable building machine learning models that establish a baseline of a working system. Over time, the learning translates to automatic configuration of systems thereby enabling self-healing.\\n\\nCan you help connect the dots for users to understand the impact of configuration changes?\\n\\nQualifications &amp; Expertise\\nThe successful engineer would have a proven track record of building complex log analysis platforms:\\n\\nDegree in Computer Science, Engineering, or related fields\\nStrong in data structures, algorithms, and systems\\nDeep understanding of interconnected systems behavior controlled through configuration\\nExperience with modeling system configuration and behavior from a manageability perspective\\nExperience with creating validated solution designs across multiple layers of the data center infrastructure stack in technical collaborations with partners\\nExperience with building and using configuration management systems and databases (e.g.: Chef, Puppet, BMC Remedy, etc.)\\nExperience with handling complex service request fulfillment and production issues\\nHeuristic problem solving with incomplete information\\nDeep domain knowledge and hands-on experience across a very broad spectrum of the backend, frontend, cloud, AI and data infrastructure platforms.\\nPlease send your resume to jobs@peritus.ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Data Engineer, Analytics</td>\n",
       "      <td>Menlo Park, CA</td>\n",
       "      <td>Menlo Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.\\nDo you like working with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. In this role, you will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.\\n\\n\\nThis is a full-time position based in our office in Menlo Park.\\nRESPONSIBILITIES\\nManage data warehouse plans for a product or a group of products.\\nInterface with engineers, product managers and product analysts to understand data needs.\\nBuild data expertise and own data quality for allocated areas of ownership.\\nDesign, build and launch new data models in production.\\nDesign, build and launch new data extraction, transformation and loading processes in production.\\nSupport existing processes running in production.\\nDefine and manage SLA for all data sets in allocated areas of ownership.\\nWork with data infrastructure to triage infra issues and drive to resolution.\\nMINIMUM QUALIFICATIONS\\n2+ years experience in the data warehouse space.\\n2+ years experience in custom ETL design, implementation and maintenance.\\n2+ years experience working with either a MapReduce or an MPP system.\\n2+ years experience with object-oriented programming languages.\\n2+ years experience with schema design and dimensional data modeling.\\n2+ years experience in writing SQL statements.\\nExperience analyzing data to identify deliverables, gaps and inconsistencies.\\nExperience managing and communicating data warehouse plans to internal clients.\\nPREFERRED QUALIFICATIONS\\nBS/BA in Technical Field, Computer Science or Mathematics.\\nKnowledge in Python or Java.\\nFacebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Senior Application Data Engineer - SQL Specialist</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>You are intellectually curious. You like building cool stuff. And you're nice.\\n\\nAt Noodle, we are not just building AI applications. We are going deep into industries that have yet to leverage AI at scale such as steel mills, distribution &amp; logistics companies or consumer packaged goods. Our applications fit and integrate deeply into the supply chain starting with planning to manufacturing to delivery. The applications we build have to not only integrate into the existing software in these industries but have to talk to each other so really drive the value from AI. Turns out, we are one of the pioneers here charting a new course. This means that science behind building the software and the AI behind it has not settled. You will be part of a team that is charting this new course figuring out how to adapt software engineering best practices to delivering AI applications that fit within legacy software in non-tech industries. This is going to be a bumpy ride and we are looking for people who are not afraid of the unknown, are experts at their craft and can adapt and learn as we create a suite of new AI applications.\\n\\nResponsibilities\\n\\n\\nWork collaboratively with an interdisciplinary team of management consultants, product managers, data scientists, data engineers, software engineers, UX designers to understand &amp; implement application data engineering requirements.\\nExecute rapid application prototyping cycles from breaking down technical requirements in tasks, data-pipeline design, programming, debugging, and optimization of database code.\\nDesign, develop and evolve Noodle's AI Application Databases, Data Models &amp; Data Pipelines.\\nIn charge of database design, data modeling and data pipeline development activities for the Application.\\nAnalyze complex Enterprise data, exploring entity relationships, performing data quality checks &amp; validations.\\nChampion engineering, operational excellence, follow best practices and coding guidelines to deliver highly scalable application data components.\\nHelp improve Noodle Data Architecture iteratively through innovations and adaptation of best practices &amp; industry standards.\\n\\nQualifications\\n\\n\\nBS/BE/B.Tech or Advanced degree in a relevant field (Computer Science and Engineering, Technology and related fields). Master' s degree a plus.\\n6+ years of industry experience in data driven software development.\\n4+ years of real-world experience in architecting &amp; developing scalable data driven AI applications, data warehousing &amp; business intelligence solutions.\\nExpert in writing SQL queries, procedures and user-defined functions.\\nDemonstrated proficiency as a lead with database development, automation, performance tuning, optimization and management projects.\\nReal life work experience with relational databases: PostgreSQL, MS SQL Server, Oracle, etc.\\nGood understanding of Big Data concepts and scenarios where it works. This includes understanding of the nature of distributed systems and its pitfalls.\\nGood knowledge of database design and data modeling concepts.\\nWorking knowledge of database security and data encryption.\\nExperience working with version control systems like: bitbucket, github, etc.\\nExperience working in a hybrid environment involving on-premise infrastructure and cloud databases will be a plus.\\nHands-on experience with modern ETL tools and Orchestration platform like Airflow is desirable.\\nFamiliarity with agile software development practices and DevOps is an added advantage.\\n\\nPassion for learning and a desire to grow – Noodlers are life-long learners!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Sunnyvale, CA</td>\n",
       "      <td>Sunnyvale</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Do you want to be part of a growing data team with a huge impact on products used by millions of people around the world? Niantic, the developer of Ingress, Pokemon Go, and Harry Potter: Wizards Unite, is searching for a Senior Data Engineer. You'll lead efforts to scale and organize the data infrastructure, working closely with Machine Learning scientists and engineers to craft a highly scalable data pipeline capable of data-enabling Niantic’s next generation of augmented reality games. You'll form the cornerstone of a flexible, driven team to build game experiences which enrich millions of lives.\\nResponsibilities\\nYou'll design and implement elegant data models (tables, partitioning, dependencies ...) for use with Niantic's ML pipelines and analytics.\\nYou'll architect the technical stack for storing and processing large volumes of Niantic’s data.\\nYou'll craft ETL pipelines to ingest and structure data from diverse sources.\\nYou'll mentor and provide technical guidance to junior data engineers, with many opportunities to demonstrate leadership.\\nYou'll work hand-in-hand with the ML Science and ML Engineering teams to provide datasets which can be used in production ML systems.\\nQualifications\\nYou have a BS, MS, or PhD in Computer Science, or a related technical field.\\nYou have 4+ years of experience developing and deploying data pipelines.\\nYou've deployed large-scale data extraction pipelines which feed into ML models.\\nYou have extensive knowledge of large-scale data processing concepts and technologies.\\nYou have experience with cloud deployment of pipelines and orchestration tools (Airflow, Composer).\\nYou possess a deep knowledge of data storage and analysis technologies such as Hive, Presto, or Spark, and are comfortable with their trade-offs and optimizations.\\nYou know your way around Java or Scala, Python and SQL.\\nPlus If...\\nYou have knowledge of the Google data stack (Dataflow, Dataproc, BigTable, BigQuery, etc.).\\nYou have experience with design of data models which serve multiple applications underlying the same model (common schemas across multiple games).\\nYou have knowledge of ML models for classification, regression, and clustering.\\nYou have experience with deploying ETL pipelines on large user bases (10s of millions of users).\\nJoin the Niantic team!\\nNiantic is the world’s leading AR technology company, sparking creative and engaging journeys in the real world. Our products inspire outdoor exploration, exercise, and meaningful social interaction.\\nOriginally formed at Google in 2011, we became an independent company in 2015 with a strong group of investors including Nintendo, The Pokémon Company, and Alsop Louie Partners. Our current titles include pioneering global-control game Ingress, record-breaking AR game Pokémon GO, and recently released third title, Harry Potter: Wizards Unite. .\\nNiantic is an Equal Opportunity and Affirmative Action employer. We believe that cultivating a workplace where our people are supported and included is essential to creating great products our community will love. Our mission emphasizes seeking and hiring diverse voices, including those who are traditionally underrepresented in the technology industry, and we consider this to be one of the most important values we hold close.\\nWe're a hard-working, fun, and exciting group who value intellectual curiosity and a passion for problem-solving! We have growing offices located in San Francisco, Sunnyvale, Bellevue, Los Angeles, London, Tokyo, Hamburg, and Zurich.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Senior Data Engineer, Product</td>\n",
       "      <td>Los Gatos, CA 95032</td>\n",
       "      <td>Los Gatos</td>\n",
       "      <td>CA</td>\n",
       "      <td>95032</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nCreative thinker and strong problem solver with meticulous attention to detail.\\nAn aptitude to learn new technologies.\\nPassion for solving business problems using data.\\nExcellent written and verbal communication skills - Ability to communicate in a clear and effective manner with teams of diverse skills &amp; mindsets to influence the overall strategy of the product.\\nAbility to initiate and drive projects to completion with minimal guidance in a fast-paced dynamic environment.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>MS or BS in Computer Science, Engineering, Math or a related field or equivalent experience.\\n4+ years of industry experience building large-scale distributed systems.\\nStrong Software Engineering experience with exceptional skills in at least one high-level programming language (Python, Java, Scala or equivalent).\\nKnowledge &amp; experience with Spark, Hadoop, MapReduce, Kafka.\\nExperience with building stream-processing applications using Flink, Storm or Spark-Streaming.\\nProficiency with NoSQL databases, such as HBase, Cassandra, MongoDB.\\nExperience with databases and SQL.\\nExperience with Cloud Computing platforms like Amazon AWS, Google Cloud etc.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Los Gatos, California\\nData Science and Engineering\\nNetflix is the world's leading internet entertainment service which has revolutionized how people interact with TV shows and movies. With over a 148M subscribers in 190 countries we strive to connect the world through amazing, award-winning stories which can be viewed on any internet-connected device. As a data-driven company, we use data to deliver delightful experiences to our users.\\n\\nThe “Product-Analytics” data engineering team is responsible for enabling and empowering our partners in Product, Science &amp; Analytics by democratizing access to user interaction data.\\n\\nAs part of this engineering team, you will work on diverse data technologies such as Spark, Flink, Kafka, Cassandra &amp; others to build mission-critical, scalable and robust data pipelines and intuitive data products that power data discovery &amp; analysis in a self-service manner. You will be expected to partner effectively with our Engineering and Analytics teams to enable data-driven decisions which improve the experience of hundreds of millions of users worldwide.\\n\\nWho are you?\\nYou have a strong background in distributed data processing and software engineering.\\nYou build high-quality data products and frameworks which can be leveraged across multiple teams.\\nYou are knowledgeable about data modeling, data access, and data storage techniques.\\nYou know how to write distributed, high-volume services in Java, Scala or other programming languages.\\nYou appreciate agile software development principles and patterns.\\nYou understand the value of partnership within teams.\\nYou have limitless curiosity and are capable of taking on loosely defined problems.\\nEducation &amp; Experience\\nMS or BS in Computer Science, Engineering, Math or a related field or equivalent experience.\\n4+ years of industry experience building large-scale distributed systems.\\nStrong Software Engineering experience with exceptional skills in at least one high-level programming language (Python, Java, Scala or equivalent).\\nKnowledge &amp; experience with Spark, Hadoop, MapReduce, Kafka.\\nExperience with building stream-processing applications using Flink, Storm or Spark-Streaming.\\nProficiency with NoSQL databases, such as HBase, Cassandra, MongoDB.\\nExperience with databases and SQL.\\nExperience with Cloud Computing platforms like Amazon AWS, Google Cloud etc.\\nKey Qualifications\\nCreative thinker and strong problem solver with meticulous attention to detail.\\nAn aptitude to learn new technologies.\\nPassion for solving business problems using data.\\nExcellent written and verbal communication skills - Ability to communicate in a clear and effective manner with teams of diverse skills &amp; mindsets to influence the overall strategy of the product.\\nAbility to initiate and drive projects to completion with minimal guidance in a fast-paced dynamic environment.\\nAPPLY NOW\\nShare this listing:\\nLINK COPIED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>8+ years of back-end engineering experience, preferably Java Maven or Scala or Node.js\\n5+ years of experience in building IoT Bigdata and Analytics solution utilizing ETL tools, Kafka, Java or Scala, PL/SQL, Pentaho or Big Query SQL and Shell scripting\\n5+ years of Web development using Django or Angular JS, JavaScript frameworks, EXT JS, MongoDB or NoSQL or RDBMS\\n3+ years of API design, development and integration experience\\n2+ years of hands on experience creating applications and tools using Cloud Platform (example: Google Big Query, Cloud Storage, Cloud Functions, Pub/Sub, App Engine, Cloud SQL)\\n1+ year of experience in deploying AI models in preferably Google Cloud using Google Cloud Platform tools.\\n1+ year of experience in designing and scaling cloud-based applications (Google Cloud Platform and Amazon Web Services).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0    Data Engineer - Operations                          \n",
       "1    Biomedical Data Engineer - Health Technologies      \n",
       "2    Data Engineer                                       \n",
       "3    Principal Data Scientist                            \n",
       "4    Data Engineer                                       \n",
       "..             ...                                       \n",
       "181  Data Engineer, Analytics                            \n",
       "182  Senior Application Data Engineer - SQL Specialist   \n",
       "183  Senior Data Engineer                                \n",
       "184  Senior Data Engineer, Product                       \n",
       "185  Big Data Engineer                                   \n",
       "\n",
       "                         Location                City State         Zip  \\\n",
       "0    Santa Clara, CA 95051         Santa Clara         CA    95051        \n",
       "1    Santa Clara Valley, CA 95014  Santa Clara Valley  CA    95014        \n",
       "2    Sunnyvale, CA                 Sunnyvale           CA    None Found   \n",
       "3    Mountain View, CA 94043       Mountain View       CA    94043        \n",
       "4    Campbell, CA                  Campbell            CA    None Found   \n",
       "..            ...                       ...            ..           ...   \n",
       "181  Menlo Park, CA                Menlo Park          CA    None Found   \n",
       "182  Palo Alto, CA                 Palo Alto           CA    None Found   \n",
       "183  Sunnyvale, CA                 Sunnyvale           CA    None Found   \n",
       "184  Los Gatos, CA 95032           Los Gatos           CA    95032        \n",
       "185  Santa Clara, CA               Santa Clara         CA    None Found   \n",
       "\n",
       "        Country  \\\n",
       "0    None Found   \n",
       "1    None Found   \n",
       "2    None Found   \n",
       "3    None Found   \n",
       "4    None Found   \n",
       "..          ...   \n",
       "181  None Found   \n",
       "182  None Found   \n",
       "183  None Found   \n",
       "184  None Found   \n",
       "185  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Qualifications  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "2    We are looking for a candidate with 5+ years of experience in a Data Engineer role, with Bachelor or Master’s degree in Computer Science or related field\\n2+ years of hands on experience in building and optimizing big data pipelines using Apache Spark in Python programming languages preferred.\\n2+ years of hands on experience of building & integrating data pipelines with machine learning models preferred.\\n2 years of experience with any of the cloud platforms such as AWS , Google, Azure etc. for building and deploying data pipelines preferred.\\nHighly desirable to have experience on data quality analysis and detection.\\nExperience with big data tools: Kafka, Cassandra, Hadoop, Storm and Spark\\nHighly Proficient in Python programming skills with Java programming language skills as a huge plus.\\n   \n",
       "3    \\nHave a passion for working on big data with professional experiences in data mining, statistical analysis, predictive modeling, and data manipulation\\nAdvanced degree (MS or PhD) in science, engineering, or statistics field\\n5+ years of experience in analyzing large data sets to drive significant business decisions for online/e-commerce domain\\n5+ years delivery experience in advanced modeling environment using Python, H2O, Spark, Java\\nStrong understanding of machine learning including supervised learning (Neural Network, Gradient Boosting, Logistics Regression) and unsupervised learning (clustering, entropy analysis, isolation forest, etc.)\\nStrong problem-solving and communication skills\\nData engineer experience is a plus\\n                                                                     \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "181  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "182  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "183  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "184  \\nCreative thinker and strong problem solver with meticulous attention to detail.\\nAn aptitude to learn new technologies.\\nPassion for solving business problems using data.\\nExcellent written and verbal communication skills - Ability to communicate in a clear and effective manner with teams of diverse skills & mindsets to influence the overall strategy of the product.\\nAbility to initiate and drive projects to completion with minimal guidance in a fast-paced dynamic environment.                                                                                                                                                                                                                                                                                                                                     \n",
       "185  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "         Skills  \\\n",
       "0    None Found   \n",
       "1    None Found   \n",
       "2    None Found   \n",
       "3    None Found   \n",
       "4    None Found   \n",
       "..          ...   \n",
       "181  None Found   \n",
       "182  None Found   \n",
       "183  None Found   \n",
       "184  None Found   \n",
       "185  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Responsibilities  \\\n",
       "0    Master the MapR Converged Platform, including MapR-FS, MapR-DB Binary and JSON Tables, MapR-Streams and the Hadoop Eco-System products and maintain proficiency and currency as the technology evolves and advances.Achieve and maintain proficiency with MapR cluster and Hadoop Framework sizing, installation, debugging, performance optimization, cluster migration, security and automation.Achieve proficiency with MapR DB Binary and Json Tables sizing, performance tuning and multi-master replication.Achieve proficiency with MapR Event Stream sizing, performance tuning and multi-master replication.Assist the Sales Team (Sales Rep and Sales Engineer) in positioning and selling MapR products and Service offerings.Work closely with MapR sales in scoping and estimating projects.Write, deliver and present formal SOW’s (Statement of Work).Deliver SOW content in formal, hands-on, onsite customer engagements and ensure the on-time delivery and quality of MapR Professional Service engagements.Be a technical voice to MapR’s customers and community via blogs, Hadoop User Groups (HUG’s) and participation at leading industry conferences.Stay current in best practices, tools, and applications used in the delivery of professional service engagements.   \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "2    Design & Build data pipelines for handling real-time data streams and integrating with machine learning models\\nUnderstand and analyze large, complex data sets to identify anomalies and data quality issues\\nDesign and Build Data quality and anomaly detection framework for IoT timeseries data\\nBuild data pipeline with Apache Spark\\nWork with cross functional stakeholders to assist with their data needs via optimal & reusable data pipelines\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "3    \\nWork with business partners and stakeholders to understand the business, formulate the problems, and come up with the solutions\\nDesign, develop and implement advance machine learning models to solve E-commerce Fraud problems\\nWork with large volumes of data; extract and manipulate large datasets using standard tools such as Python, H2O, Hadoop, Spark, SQL, and Java\\nCollaborate with engineers to implement machine learning models on one of the best data platforms in the industry\\nAnalyze data to detect new fraud patterns and implement quick rules and solutions\\nMeasure and report the business impact of models and rules\\nCommunicate the concepts and results of the models and analyses to audience with different background\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "181  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "182  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "183  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "184  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "185  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Education  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "181  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "182  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "183  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "184  MS or BS in Computer Science, Engineering, Math or a related field or equivalent experience.\\n4+ years of industry experience building large-scale distributed systems.\\nStrong Software Engineering experience with exceptional skills in at least one high-level programming language (Python, Java, Scala or equivalent).\\nKnowledge & experience with Spark, Hadoop, MapReduce, Kafka.\\nExperience with building stream-processing applications using Flink, Storm or Spark-Streaming.\\nProficiency with NoSQL databases, such as HBase, Cassandra, MongoDB.\\nExperience with databases and SQL.\\nExperience with Cloud Computing platforms like Amazon AWS, Google Cloud etc.   \n",
       "185  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "\n",
       "                                                                                                                                                                                       Requirement  \\\n",
       "0    5+ years experience administering any flavor of LinuxStrong scripting skills (Bash or Python prefered)Familiarity with commercial IT infrastructures including storage, networking, security,   \n",
       "1    None Found                                                                                                                                                                                      \n",
       "2    None Found                                                                                                                                                                                      \n",
       "3    None Found                                                                                                                                                                                      \n",
       "4    None Found                                                                                                                                                                                      \n",
       "..          ...                                                                                                                                                                                      \n",
       "181  None Found                                                                                                                                                                                      \n",
       "182  None Found                                                                                                                                                                                      \n",
       "183  None Found                                                                                                                                                                                      \n",
       "184  None Found                                                                                                                                                                                      \n",
       "185  None Found                                                                                                                                                                                      \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FullDescriptions  \n",
       "0    Job Description:\\nSenior Data Engineer- Operations\\nSenior Data Engineers will report into MapR’s Professional Services Organization. MapR Data Engineers are responsible for delivering a variety of engineering services to MapR customers throughout North America. Assignments will vary based on the candidate’s skills and experience. Typical assignments may involve cluster sizing, installation, configuration, health checks, performance tuning, data migration, security and automation. MapR’s growing customer base includes most of the Fortune 50 companies which makes the work assignments technically challenging yet rewarding. This role provides a significant opportunity to learn and apply big data technologies and solve related complex problems. MapR Data Engineers report to the Director of Data Engineering located at company headquarters in San Jose, CA.\\nResponsibilities:\\nMaster the MapR Converged Platform, including MapR-FS, MapR-DB Binary and JSON Tables, MapR-Streams and the Hadoop Eco-System products and maintain proficiency and currency as the technology evolves and advances.Achieve and maintain proficiency with MapR cluster and Hadoop Framework sizing, installation, debugging, performance optimization, cluster migration, security and automation.Achieve proficiency with MapR DB Binary and Json Tables sizing, performance tuning and multi-master replication.Achieve proficiency with MapR Event Stream sizing, performance tuning and multi-master replication.Assist the Sales Team (Sales Rep and Sales Engineer) in positioning and selling MapR products and Service offerings.Work closely with MapR sales in scoping and estimating projects.Write, deliver and present formal SOW’s (Statement of Work).Deliver SOW content in formal, hands-on, onsite customer engagements and ensure the on-time delivery and quality of MapR Professional Service engagements.Be a technical voice to MapR’s customers and community via blogs, Hadoop User Groups (HUG’s) and participation at leading industry conferences.Stay current in best practices, tools, and applications used in the delivery of professional service engagements.\\nRequirements:\\n5+ years experience administering any flavor of LinuxStrong scripting skills (Bash or Python prefered)Familiarity with commercial IT infrastructures including storage, networking, security,\\nvirtualization and systems management\\nFamiliarity with either Ansible, Puppet or ChefProficiency in basic Java or Scala programming preferred but not requiredBachelor's degree in CS or equivalent experienceFamiliarity with Hadoop and the Hadoop Eco-System framework a significant advantage\\n(MapR is willing to train otherwise promising candidates)\\nStrong verbal and written communication skills are requiredAbility to professionally manage multiple priorities with minimal supervision and deliver on\\nschedule\\nWillingness to travel about 70%\\nIdeal candidate will have any/all of the following:\\nRHCE certification, Bash or Python scripting, Automation using Ansible, Puppet or Chef, basic knowledge of Hadoop, Hive, or Spark                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "1    Summary\\nPosted: Aug 21, 2019\\nWeekly Hours: 40\\nRole Number: 200092872\\nThe Health Technologies Team conceives and proves out innovative technology for Apple’s future products and features in health.\\nWe are seeking a highly capable Biomedical Data Engineer to join a multi-disciplinary team. Successful candidates will be able to integrate with our research study leads, data scientists, and engineers to develop and support effective data analysis and machine learning workflows.\\nKey Qualifications\\nExperience with software engineering frameworks:\\nExcellent coding skills in Python (e.g. Spark, Pandas, Jupyter )\\nWeb Service APIs (e.g., AWS, REDCap, XNAT)\\nDesigning and maintaining (non-)relational databases (e.g. Postgres, Cassandra, MongoDB)\\nLinux, MacOS based development frameworks\\nVersion control frameworks (Git, virtualenv)\\nExperience using task scheduling system (eg: Airflow, Luigi or equivalent)\\nFamiliarity with best practices for information security, including safe harbor privacy principles for sensitive data\\nExperience with biomedical sensors/platforms for measuring physiological signals (time-series data) in the health, wellness and/or fitness realms\\nDescription\\n• Work closely with team members and study staff to design, build, launch and maintain systems for storing, aggregating and analyzing large amounts of data\\n• Process, troubleshoot, and clean incoming data from human studies\\n• Create ETL pipelines to automate data ingestion and transformation, with hooks for QA, auditing, redaction and compliance checks per data management specifications\\n• Create and populate databases with existing and incoming clinical data\\n• Architect data models and create tools to harmonize disparate data sources\\n• Incorporate and comply with regulations as they pertain to electronic and clinical data and databases\\nEducation & Experience\\nBS/MS in Computer Science, Engineering, Informatics, or equivalent with relevant 4+ years industry experience with biomedical, health or sensitive data.\\nAdditional Requirements\\nYou will thrive in our fast-paced environment if you are highly organized and able to multitask.\\nFlexible thinking, adaptability to change and comfort with ambiguity are hallmarks of successful people on our team.\\nWe look forward to witnessing your excellent communication and interpersonal skills during the interview process.\\nApple is known for heterogeneous and cohesive teams. A proven ability to work seamlessly with others is required to acclimate quickly to our culture.\\nWe highly value your analytical mind and problem-solving skills, and expect a stellar attention to detail.\\nYou will let the customer experience guide your decision making. You will design with Apple’s culture and values, inclusion for all and privacy, as fundamental requirements.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "2    Data Engineer\\nEquinix is one of the fastest growing data center companies, growing connectivity between clients worldwide. That’s why we're always looking for creative and forward-thinking people who can help us achieve our goal of global interconnection. With 200 data centers in over 24 countries spanning across 5 continents, we are home to the Cloud, supporting over 1000 Cloud and IT services companies that are directly engaged in technological innovation and development. We are passionate about further evolving the specific areas of software development, software and network architecture, network operations and complex cloud and application solutions.\\nAt Equinix, we make the internet work faster, better, and more reliably. We hire talented people who thrive on solving hard problems and give them opportunities to hone new skills, try new approaches, and grow in new directions. Our culture is at the heart of our success and it’s our authentic, humble, gritty people who create The Magic of Equinix. We share a real passion for winning and put the customer at the center of everything we do.\\nResponsibilities\\nDesign & Build data pipelines for handling real-time data streams and integrating with machine learning models\\nUnderstand and analyze large, complex data sets to identify anomalies and data quality issues\\nDesign and Build Data quality and anomaly detection framework for IoT timeseries data\\nBuild data pipeline with Apache Spark\\nWork with cross functional stakeholders to assist with their data needs via optimal & reusable data pipelines\\nQualifications\\nWe are looking for a candidate with 5+ years of experience in a Data Engineer role, with Bachelor or Master’s degree in Computer Science or related field\\n2+ years of hands on experience in building and optimizing big data pipelines using Apache Spark in Python programming languages preferred.\\n2+ years of hands on experience of building & integrating data pipelines with machine learning models preferred.\\n2 years of experience with any of the cloud platforms such as AWS , Google, Azure etc. for building and deploying data pipelines preferred.\\nHighly desirable to have experience on data quality analysis and detection.\\nExperience with big data tools: Kafka, Cassandra, Hadoop, Storm and Spark\\nHighly Proficient in Python programming skills with Java programming language skills as a huge plus.\\nEquinix is an equal opportunity employer. All applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, or status as a qualified individual with disability.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "3    Coupang is one of the largest and fastest growing e-commerce platforms on the planet. Our mission is to create a world in which Customers ask \"How did I ever live without Coupang?\" We are looking for passionate builders to help us get there. Powered by world-class technology and operations, we have set out to transform the end-to-end Customer experience -- from revolutionizing last-mile delivery to rethinking how Customers search and discover on a truly mobile-first platform. We have been named one of the \"50 Smartest Companies in the World\" by MIT Technology Review and \"30 Global Game Changers\" by Forbes.\\n\\nCoupang is a global company with offices in Beijing, Los Angeles, Seattle, Seoul, Shanghai, and Silicon Valley.\\n\\nJob Overview:\\nThe E-commerce Fraud Detection System team at Coupang is looking for a Principle/Senior Data Scientist with a solid background in machine learning, data science, data analysis, and data engineering. You will work closely with a team of data scientists, business analysts and engineers to ensure we detect frauds in e-commerce business and take actions to protect our business.\\n\\nKey Responsibilities:\\n\\nWork with business partners and stakeholders to understand the business, formulate the problems, and come up with the solutions\\nDesign, develop and implement advance machine learning models to solve E-commerce Fraud problems\\nWork with large volumes of data; extract and manipulate large datasets using standard tools such as Python, H2O, Hadoop, Spark, SQL, and Java\\nCollaborate with engineers to implement machine learning models on one of the best data platforms in the industry\\nAnalyze data to detect new fraud patterns and implement quick rules and solutions\\nMeasure and report the business impact of models and rules\\nCommunicate the concepts and results of the models and analyses to audience with different background\\n\\nQualifications:\\n\\nHave a passion for working on big data with professional experiences in data mining, statistical analysis, predictive modeling, and data manipulation\\nAdvanced degree (MS or PhD) in science, engineering, or statistics field\\n5+ years of experience in analyzing large data sets to drive significant business decisions for online/e-commerce domain\\n5+ years delivery experience in advanced modeling environment using Python, H2O, Spark, Java\\nStrong understanding of machine learning including supervised learning (Neural Network, Gradient Boosting, Logistics Regression) and unsupervised learning (clustering, entropy analysis, isolation forest, etc.)\\nStrong problem-solving and communication skills\\nData engineer experience is a plus\\n\\nPerks:\\n\\nAutonomy to make decisions in a rapidly growing company\\n15 days PTO + 15 national holidays off\\n401K matching\\nPre-IPO stock options\\nMobile & fitness reimbursement\\nFree Catered Lunch daily\\n\\nCoupang is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex or gender (including pregnancy, gender identity, gender expression, sexual orientation, transgender status), national origin, age, disability, medical condition, HIV/AIDS or Hepatitis C status, marital status, military or veteran status, use of a trained dog guide or service animal, political activities, affiliations, citizenship, or any other characteristic or class protected by the laws or regulations in the locations where we operate.\\n\\nIf you need assistance and/or a reasonable accommodation in the application or recruiting process due to a disability, please contact us at usrecruiting@coupang.com ( usrecruiting@coupang.com ).                                                                                                  \n",
       "4    About Us\\n--------\\n\\nWith electric vehicles (EVs) expected to be 25% of vehicle sales by 2025 and more than 50% by 2040, electric mobility is in the midst of a tipping point and becoming reality. ChargePoint is at the center of the e-mobility revolution, powering the world's leading EV charging network and most complete set of hardware, software and mobile solutions for every EV charging need. Whether it's to ride, driver or deliver on electric fuel, we bring together people, vehicle fleets, businesses, automakers, policymakers, utilities and other stakeholders to make e-mobility a reality globally.\\n\\nOur fanatical focus on charging and 10+ years in business has made us an industry leader. Supported by more than half a billion dollars in funding from investors, including Quantum Energy Partners, GIC, Clearvision, Daimler Trucks & Buses, Daimler, Siemens, Linse Capital, American Electric Power, Canada Pension Plan Investment Board, Chevron Technology Ventures, Rho Capital Partners, BMW i Ventures and Braemar Energy Ventures, ChargePoint offers a once-in-a-lifetime chance to be part of creating an all-electric future and shaping a trillion-dollar market. Join the team that built the EV charging industry and make your mark on how people and goods will get everywhere they need to go, in any context, for generations to come.\\n\\nReports To\\n----------\\n\\nSoftware Engineering Manager\\n\\nWhat You Will Be Doing\\n----------------------\\n\\nBeing a leading EV charging network we have a lot of data that is used to enhance end-user experience, optimize network operations and placement of charging stations, and improve internal operations and manufacturing processes. This is just the beginning and we see a huge opportunity to leverage data to predict failures before they happen, build delightful products and enable electrification of all forms of transportation.\\n\\nA Data Engineer in this role will work with several teams across the company to identify use cases/scenarios for optimization, build a data lake to enable efficient querying and analytics, and use data science and ML to analyze the data and build data solutions.\\n\\nWhat You Will Bring to ChargePoint\\n----------------------------------\\n\\n\\nFundamental understanding of data science and ML\\nExperience building data pipelines, analyzing big data & building data solutions\\nBias to action\\nBe an evangelist for data analysis and data-oriented decision making\\n\\nRequirements\\n------------\\n\\n\\n1-3 years of object-oriented programming & SQL experience\\n1-3 years of experience with big data technologies (e.g., Hadoop, Spark, etc.) and data viz and ETL tools (e.g., Pentaho, Tableau etc.)\\nRelevant coursework and experience in data science and ML\\nSolid software engineering fundamentals\\nFundamental understanding of RDBMS, NoSQL databases and big data solutions\\nAbility to work with ambiguity, create quick PoCs and engineer an E2E solution\\nStrong scripting or programming skills for automating repetitive tasks\\n\\nPreferred\\n---------\\n\\n\\nExperience with AWS\\nExperience building data lake or data solutions from scratch\\n\\nLocation\\n--------\\n\\nCampbell, CA\\n\\nWe are committed to an inclusive and diverse team. ChargePoint is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status.\\n\\nIf there is a match between your experiences/skills and the Company needs, we will contact you directly.\\n\\nChargePoint is an equal opportunity employer.\\nApplicants only - Recruiting agencies do not contact.\\n\\n#LI-SH1  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ...  \n",
       "181  Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities — we're just getting started.\\nDo you like working with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. In this role, you will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.\\n\\n\\nThis is a full-time position based in our office in Menlo Park.\\nRESPONSIBILITIES\\nManage data warehouse plans for a product or a group of products.\\nInterface with engineers, product managers and product analysts to understand data needs.\\nBuild data expertise and own data quality for allocated areas of ownership.\\nDesign, build and launch new data models in production.\\nDesign, build and launch new data extraction, transformation and loading processes in production.\\nSupport existing processes running in production.\\nDefine and manage SLA for all data sets in allocated areas of ownership.\\nWork with data infrastructure to triage infra issues and drive to resolution.\\nMINIMUM QUALIFICATIONS\\n2+ years experience in the data warehouse space.\\n2+ years experience in custom ETL design, implementation and maintenance.\\n2+ years experience working with either a MapReduce or an MPP system.\\n2+ years experience with object-oriented programming languages.\\n2+ years experience with schema design and dimensional data modeling.\\n2+ years experience in writing SQL statements.\\nExperience analyzing data to identify deliverables, gaps and inconsistencies.\\nExperience managing and communicating data warehouse plans to internal clients.\\nPREFERRED QUALIFICATIONS\\nBS/BA in Technical Field, Computer Science or Mathematics.\\nKnowledge in Python or Java.\\nFacebook is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-ext@fb.com.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "182  You are intellectually curious. You like building cool stuff. And you're nice.\\n\\nAt Noodle, we are not just building AI applications. We are going deep into industries that have yet to leverage AI at scale such as steel mills, distribution & logistics companies or consumer packaged goods. Our applications fit and integrate deeply into the supply chain starting with planning to manufacturing to delivery. The applications we build have to not only integrate into the existing software in these industries but have to talk to each other so really drive the value from AI. Turns out, we are one of the pioneers here charting a new course. This means that science behind building the software and the AI behind it has not settled. You will be part of a team that is charting this new course figuring out how to adapt software engineering best practices to delivering AI applications that fit within legacy software in non-tech industries. This is going to be a bumpy ride and we are looking for people who are not afraid of the unknown, are experts at their craft and can adapt and learn as we create a suite of new AI applications.\\n\\nResponsibilities\\n\\n\\nWork collaboratively with an interdisciplinary team of management consultants, product managers, data scientists, data engineers, software engineers, UX designers to understand & implement application data engineering requirements.\\nExecute rapid application prototyping cycles from breaking down technical requirements in tasks, data-pipeline design, programming, debugging, and optimization of database code.\\nDesign, develop and evolve Noodle's AI Application Databases, Data Models & Data Pipelines.\\nIn charge of database design, data modeling and data pipeline development activities for the Application.\\nAnalyze complex Enterprise data, exploring entity relationships, performing data quality checks & validations.\\nChampion engineering, operational excellence, follow best practices and coding guidelines to deliver highly scalable application data components.\\nHelp improve Noodle Data Architecture iteratively through innovations and adaptation of best practices & industry standards.\\n\\nQualifications\\n\\n\\nBS/BE/B.Tech or Advanced degree in a relevant field (Computer Science and Engineering, Technology and related fields). Master' s degree a plus.\\n6+ years of industry experience in data driven software development.\\n4+ years of real-world experience in architecting & developing scalable data driven AI applications, data warehousing & business intelligence solutions.\\nExpert in writing SQL queries, procedures and user-defined functions.\\nDemonstrated proficiency as a lead with database development, automation, performance tuning, optimization and management projects.\\nReal life work experience with relational databases: PostgreSQL, MS SQL Server, Oracle, etc.\\nGood understanding of Big Data concepts and scenarios where it works. This includes understanding of the nature of distributed systems and its pitfalls.\\nGood knowledge of database design and data modeling concepts.\\nWorking knowledge of database security and data encryption.\\nExperience working with version control systems like: bitbucket, github, etc.\\nExperience working in a hybrid environment involving on-premise infrastructure and cloud databases will be a plus.\\nHands-on experience with modern ETL tools and Orchestration platform like Airflow is desirable.\\nFamiliarity with agile software development practices and DevOps is an added advantage.\\n\\nPassion for learning and a desire to grow – Noodlers are life-long learners!                                                                                                                                                              \n",
       "183  Do you want to be part of a growing data team with a huge impact on products used by millions of people around the world? Niantic, the developer of Ingress, Pokemon Go, and Harry Potter: Wizards Unite, is searching for a Senior Data Engineer. You'll lead efforts to scale and organize the data infrastructure, working closely with Machine Learning scientists and engineers to craft a highly scalable data pipeline capable of data-enabling Niantic’s next generation of augmented reality games. You'll form the cornerstone of a flexible, driven team to build game experiences which enrich millions of lives.\\nResponsibilities\\nYou'll design and implement elegant data models (tables, partitioning, dependencies ...) for use with Niantic's ML pipelines and analytics.\\nYou'll architect the technical stack for storing and processing large volumes of Niantic’s data.\\nYou'll craft ETL pipelines to ingest and structure data from diverse sources.\\nYou'll mentor and provide technical guidance to junior data engineers, with many opportunities to demonstrate leadership.\\nYou'll work hand-in-hand with the ML Science and ML Engineering teams to provide datasets which can be used in production ML systems.\\nQualifications\\nYou have a BS, MS, or PhD in Computer Science, or a related technical field.\\nYou have 4+ years of experience developing and deploying data pipelines.\\nYou've deployed large-scale data extraction pipelines which feed into ML models.\\nYou have extensive knowledge of large-scale data processing concepts and technologies.\\nYou have experience with cloud deployment of pipelines and orchestration tools (Airflow, Composer).\\nYou possess a deep knowledge of data storage and analysis technologies such as Hive, Presto, or Spark, and are comfortable with their trade-offs and optimizations.\\nYou know your way around Java or Scala, Python and SQL.\\nPlus If...\\nYou have knowledge of the Google data stack (Dataflow, Dataproc, BigTable, BigQuery, etc.).\\nYou have experience with design of data models which serve multiple applications underlying the same model (common schemas across multiple games).\\nYou have knowledge of ML models for classification, regression, and clustering.\\nYou have experience with deploying ETL pipelines on large user bases (10s of millions of users).\\nJoin the Niantic team!\\nNiantic is the world’s leading AR technology company, sparking creative and engaging journeys in the real world. Our products inspire outdoor exploration, exercise, and meaningful social interaction.\\nOriginally formed at Google in 2011, we became an independent company in 2015 with a strong group of investors including Nintendo, The Pokémon Company, and Alsop Louie Partners. Our current titles include pioneering global-control game Ingress, record-breaking AR game Pokémon GO, and recently released third title, Harry Potter: Wizards Unite. .\\nNiantic is an Equal Opportunity and Affirmative Action employer. We believe that cultivating a workplace where our people are supported and included is essential to creating great products our community will love. Our mission emphasizes seeking and hiring diverse voices, including those who are traditionally underrepresented in the technology industry, and we consider this to be one of the most important values we hold close.\\nWe're a hard-working, fun, and exciting group who value intellectual curiosity and a passion for problem-solving! We have growing offices located in San Francisco, Sunnyvale, Bellevue, Los Angeles, London, Tokyo, Hamburg, and Zurich.                                                                                                                                                                                                                \n",
       "184  Los Gatos, California\\nData Science and Engineering\\nNetflix is the world's leading internet entertainment service which has revolutionized how people interact with TV shows and movies. With over a 148M subscribers in 190 countries we strive to connect the world through amazing, award-winning stories which can be viewed on any internet-connected device. As a data-driven company, we use data to deliver delightful experiences to our users.\\n\\nThe “Product-Analytics” data engineering team is responsible for enabling and empowering our partners in Product, Science & Analytics by democratizing access to user interaction data.\\n\\nAs part of this engineering team, you will work on diverse data technologies such as Spark, Flink, Kafka, Cassandra & others to build mission-critical, scalable and robust data pipelines and intuitive data products that power data discovery & analysis in a self-service manner. You will be expected to partner effectively with our Engineering and Analytics teams to enable data-driven decisions which improve the experience of hundreds of millions of users worldwide.\\n\\nWho are you?\\nYou have a strong background in distributed data processing and software engineering.\\nYou build high-quality data products and frameworks which can be leveraged across multiple teams.\\nYou are knowledgeable about data modeling, data access, and data storage techniques.\\nYou know how to write distributed, high-volume services in Java, Scala or other programming languages.\\nYou appreciate agile software development principles and patterns.\\nYou understand the value of partnership within teams.\\nYou have limitless curiosity and are capable of taking on loosely defined problems.\\nEducation & Experience\\nMS or BS in Computer Science, Engineering, Math or a related field or equivalent experience.\\n4+ years of industry experience building large-scale distributed systems.\\nStrong Software Engineering experience with exceptional skills in at least one high-level programming language (Python, Java, Scala or equivalent).\\nKnowledge & experience with Spark, Hadoop, MapReduce, Kafka.\\nExperience with building stream-processing applications using Flink, Storm or Spark-Streaming.\\nProficiency with NoSQL databases, such as HBase, Cassandra, MongoDB.\\nExperience with databases and SQL.\\nExperience with Cloud Computing platforms like Amazon AWS, Google Cloud etc.\\nKey Qualifications\\nCreative thinker and strong problem solver with meticulous attention to detail.\\nAn aptitude to learn new technologies.\\nPassion for solving business problems using data.\\nExcellent written and verbal communication skills - Ability to communicate in a clear and effective manner with teams of diverse skills & mindsets to influence the overall strategy of the product.\\nAbility to initiate and drive projects to completion with minimal guidance in a fast-paced dynamic environment.\\nAPPLY NOW\\nShare this listing:\\nLINK COPIED                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "185  8+ years of back-end engineering experience, preferably Java Maven or Scala or Node.js\\n5+ years of experience in building IoT Bigdata and Analytics solution utilizing ETL tools, Kafka, Java or Scala, PL/SQL, Pentaho or Big Query SQL and Shell scripting\\n5+ years of Web development using Django or Angular JS, JavaScript frameworks, EXT JS, MongoDB or NoSQL or RDBMS\\n3+ years of API design, development and integration experience\\n2+ years of hands on experience creating applications and tools using Cloud Platform (example: Google Big Query, Cloud Storage, Cloud Functions, Pub/Sub, App Engine, Cloud SQL)\\n1+ year of experience in deploying AI models in preferably Google Cloud using Google Cloud Platform tools.\\n1+ year of experience in designing and scaling cloud-based applications (Google Cloud Platform and Amazon Web Services).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "\n",
       "[186 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_MountainView.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
