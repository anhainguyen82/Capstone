{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=Boston%2C+MA&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google Technical Architect</td>\n",
       "      <td>Boston, MA 02199</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02199</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum 5 years of Consulting or client service delivery experience on Google GCP\\n</td>\n",
       "      <td>DevOps on an GCP platform. Multi-cloud experience a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Google Cloud Platform (GCP) Technical Architect Delivery is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would also be responsible for developing and delivering Google GCP cloud solutions to meet todays high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Google GCP Technical Architect is a highly performant GCP Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data soltuions on cloud. Using Google GCP public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:Work with Sales and Bus Dev teams in providing Data and GCP Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS &amp; NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nBasic Qualifications\\nMinimum 5 years of Consulting or client service delivery experience on Google GCP\\nMinimum 10 years of experience in big data, database and data warehouse architecture and delivery\\nBachelors degree or 12 years previous professional experience\\nAble to travel 100% (M-TH)\\nMinimum of 5 years of professional experience in 2 of the following areas:\\nSolution/technical architecture in the cloud\\nBig Data/analytics/information analysis/database management in the cloud\\nIoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using GCP services etc.:\\nData Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core\\nStreaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam\\nData Warehousing &amp; Data Lake : BigQuery, Cloud Storage\\nAdvanced Analytics : Cloud ML engine, Google Data Studio, Tensorflow &amp; Sheets\\n\\nFamiliarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nCertified GCP Solutions Architect - Associate\\nCertified GCP Solutions Architect – Professional (Nice to have)\\nCertified GCP Big Data Specialty (Nice to have)\\nCertified GCP AI/ML Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an GCP platform. Multi-cloud experience a plus.\\nExperience developing and deploying ETL solutions on GCP\\nStrong in Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience beyond GCP a plus - AWS and Azure\\n\\nProfessional Skill Requirements\\nProven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Software Engineer/Data Engineer</td>\n",
       "      <td>Boston, MA 02101</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02101</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nStrong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.\\nFamiliarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured (Postgres, MySQL) and unstructured data sources (Elastic, Kafka, and the Hadoop ecosystem) as implemented at Internet-scale.\\nExperience writing and optimizing streaming and batch analytics.\\nExperience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.\\nBS/BA in Computer Science, Engineering, or relevant field experience.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Ideally, the successful candidate will be located near our NYC or College Park, MD office. However, there is the opportunity to work remotely based on role and level.\\nSoftware Engineer/Data Engineer\\nBlueVoyant is seeking a Software Engineer/Data Engineer to help us build a data analytics platform powerful enough to protect some of the world's biggest networks, and nimble enough to adapt to a quickly evolving product vision. We are solving interesting, exciting, and important problems with smart people.\\nQualifications for the Software Engineer/Data Engineer:\\nStrong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.\\nFamiliarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured (Postgres, MySQL) and unstructured data sources (Elastic, Kafka, and the Hadoop ecosystem) as implemented at Internet-scale.\\nExperience writing and optimizing streaming and batch analytics.\\nExperience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.\\nBS/BA in Computer Science, Engineering, or relevant field experience.\\nWhat you will do as a Software Engineer/Data Engineer:\\nWork closely with analysts to transform threat analytics into production-level code.\\nActively contribute to application architecture and product vision.\\nParticipate in requirements gathering and transformation from prototype to product design.\\nParticipate in daily development stand-up meetings and regular sprint planning and product demo meetings.\\nHelp us stay current on the latest data processing tools and trends.\\nIdeal candidates will:\\nThrive in our small, fast-paced, product-driven environment\\nCollaborate with teams from across the organization\\nDeliver features and fixes on tight schedules and under pressure\\nPresent ideas in business-friendly and user-friendly language\\nCreate systems that are maintainable, flexible and scalable\\nDefine and follow a disciplined development and engineering workflow\\nDemonstrate ownership of tasks with escalation as needed\\nBe a subject matter expert in one or more of the technologies employed\\nRelentlessly push for successful customer outcomes\\nPossess a strong interest or background in cyber security\\nGeneral responsibilities include:\\nParticipate in all stages of an agile software development lifecycle, including product ideation, requirements gathering, architecture, design, implementation, testing, documentation, and support\\nRefine our software development methodology based on agile/lean practices with continuous feedback and well-defined metrics to drive improvement\\nMaintain up-to-date knowledge of technology standards, industry trends, emerging technologies, and software development best practices\\nEnsure technical issues are quickly resolved and help implement strategies and solutions to reduce the likelihood of reoccurrence\\nIdentify competitive offerings and opportunities for innovation including assessments of risk/reward to the company.\\nAbout BlueVoyant\\nBlueVoyant is a global cybersecurity firm that provides Advanced Threat Intelligence, for large companies and a comprehensive Managed Security Service and Professional Services for small businesses, powered by one of the largest commercially available cyber threat databases in the world.\\nBy working with BlueVoyant, companies can gain unique and far-reaching visibility into malicious activity on their networks, in the dark web and across the internet, as well as real-time, automatable remediation services. Through our unique real-time external threat monitoring, predictive human and machine-sourced intelligence, and proactive managed security and incident response, BlueVoyant offers the private sector exceptional cyber defense capabilities.\\nCo-founded by CEO Jim Rosenthal, former Chief Operating Officer at Morgan Stanley, and Executive Chairman Tom Glocer, former Chief Executive Officer at Thomson Reuters, BlueVoyant has attracted a management team that comes from the world's preeminent intelligence, law enforcement, and private sector organizations. Other leaders include:\\nJim Penrose, COO, former EVP at Darktrace with 17 years at the NSA in key leadership roles.\\nGad Goldstein, Head of BlueVoyant Europe and Chairman of Israel, former division head (Major General equivalent) in the Israel Security Agency, Shin Bet.\\nRobert Hannigan, Chairman of BlueVoyant International, former Director of GCHQ.\\nAustin Berglas, Global Head of Professional Services, former head of the FBI's New York Cyber Branch.\\nDavid Etue, Global Head of MSS, former VP of Managed Services at Rapid7.\\nMilan Patel, Chief Client Officer, former CTO of the FBI Cyber Division.\\nRon Feler, Global Head of Threat Intelligence and Operations, former Deputy Commander of Unit 8200, the cybersecurity division of the Israel Defense Forces.\\nEldad Chai, CPO, former SVP of Products at Imperva.\\nJim Bieda, Senior Advisor, former NSA Deputy CTO.\\nBill Crumm, Senior Advisor, former NSA SIGINT Director and former Cybersecurity Head, Morgan Stanley.\\nDan Ennis, Senior Advisor, former Head of Threat Intelligence at the NSA\\nAll employees must be authorized to work in the United States or Israel. BlueVoyant provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, BlueVoyant complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.\\nCome work with us!\\nBlueVoyant is hiring software developers, infrastructure engineers, data science experts, and technologists of all types to build next generation predictive threat intelligence and advanced security monitoring solutions.\\nProjects currently in development include:\\nAn Internet-scale (multi-PB, &gt; 500k TPS) repository made up of unstructured, structured, and semi-structured data sources used for real time alerting, threat analysis, and research and development internally.\\nA powerful, enterprise-scale suite of products used to provide managed security services and Security Operations Center (SOC) functionality to small and medium sized enterprises around the globe.\\nA unique internal platform to support the data research needs of our analysts and SOC so they can quickly and effectively identify new threat actors and techniques.\\nrOZMxAY0kC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3-5 years experience with data engineering or data warehousing, or an equivalent data-oriented software engineering background\\n2-3 years experience with ETL technologies (e.g. Talend, Informatica, Matillion)\\n2-3 years experience with key AWS technologies (Redshift, S3, IAM; also Kinesis, Lambda, EMR, Spark, Hive)\\n3-5 years of experience with SQL\\nExperience in optimizing queries and tuning database performance\\nFamiliarity with data analysis and data science tools (Looker, Python, Jupyter, R) is a plus\\nPython or Java experience is a plus</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Implement data pipelines using agile, iterative processes to deliver value quickly\\nTreat data quality, reliability, and documentation as fundamental acceptance criteria for data pipelines\\nCollaborate with the team to make the best technology choices\\nDeliver solutions that ensure data accuracy, high availability, robust security, and that are built for rapid scaling\\nCreate and maintain data models, data catalogues, and data security.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Senior Data Engineer\\nBoston, MA\\nU.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time. No Corp to Corp.\\n\\nApply direct to: creposa@syrinx.com\\nSummary:\\nThe Data team is responsible for leading efforts to ingest, structure, and model our data to support critical decision making. Our enterprise data warehouse is based on AWS Redshift, supporting Looker analytical dashboards, and is backed by an AWS data lake. In addition to analytics, the data warehouse also supports an active and growing data science practice.\\n\\nThe ideal candidate will have experience building and supporting data platforms that help end users make sound business decisions. This includes data pipeline creation, data quality management, system reliability, job orchestration and recovery aka “Data Ops”. In additional to technical competency, they should be truly motived to empower end users.\\n\\nResponsibilities\\nImplement data pipelines using agile, iterative processes to deliver value quickly\\nTreat data quality, reliability, and documentation as fundamental acceptance criteria for data pipelines\\nCollaborate with the team to make the best technology choices\\nDeliver solutions that ensure data accuracy, high availability, robust security, and that are built for rapid scaling\\nCreate and maintain data models, data catalogues, and data security.\\nQualifications\\n3-5 years experience with data engineering or data warehousing, or an equivalent data-oriented software engineering background\\n2-3 years experience with ETL technologies (e.g. Talend, Informatica, Matillion)\\n2-3 years experience with key AWS technologies (Redshift, S3, IAM; also Kinesis, Lambda, EMR, Spark, Hive)\\n3-5 years of experience with SQL\\nExperience in optimizing queries and tuning database performance\\nFamiliarity with data analysis and data science tools (Looker, Python, Jupyter, R) is a plus\\nPython or Java experience is a plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Engineer, Delivery</td>\n",
       "      <td>Boston, MA 02110</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02110</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We’re reimagining sports and technology.\\nDraftKings is bringing sports fans closer to the games they love and becoming an essential part of their experience in the process. An industry pioneer since our founding in 2012, we believe we can continue to define what it means to be a technology company in sports entertainment. We love what we do and we think you will too.\\n\\nLove data? We do too.\\nAs a Senior Data Engineer, Delivery you’ll be a creative contributor, leader and mentor to our data analysis and scalability processes, and you will use your experience to provide key insights that help us make data-driven decisions. Analytical thinking drives our business and when you join our team, you’ll not only solve new problems every day, you’ll see your data solutions immediately improve our users’ experience.\\nWho are we a good fit for?\\nWe love working with talented people but more than that, we seek out compassionate co-workers with a collaborative spirit. Our work moves quickly and we’re great at coming together to find creative solutions to some of tech’s most interesting problems. If that sounds good to you, join us.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Newton, MA</td>\n",
       "      <td>Newton</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About the Data Engineer position\\nData fuels everything we do at Gravyty. It is part of our core value – to turn data into action for nonprofits to achieve their mission. We place high value in things like fast no-BS implementations, data quality/transparency, and building scalable data systems. As a Data Engineer at Gravyty you will be a core part of designing and implementing ETL (extract-transform-load) data flows for our customers. We are looking for a person who wants the fast-paced environment of a startup operating in the mission-driven world of nonprofits and wants to evolve into a senior data engineer with world-class abilities in ETL data workflows, database systems for processing, and big data applications.\\nData Engineer Responsiblities\\nScheduling jobs that recover from errors, such as hitting rate limits or data loading errors, and can do retries when failures occur\\nStoring data into various targets such as flat files, data stores, or outgoing REST APIs\\nDoing transformation tasks on data – such as cleansing, validation, filtering, partitioning, aggregation, calculating metrics, and reporting\\nUnderstand existing schemas and define the best possible new data functionality\\nHelping customers understand how processing is best done to achieve their non-technical objectives\\nUnderstanding the input/output formats of the data and constraints on data processing\\nData Engineer Requirements\\nAt least 1 year of full-time backend programming experience in a major programming language such as Python, Java, C#, Scala, PHP, or Ruby\\nGood practical knowledge of (ANSI) SQL, database design, and data modeling\\nExperience in programming ETL (extract-transform-load) workflows with a programming language such as Python or Scala\\nAs part of the job, be prepared to learn Python for writing ETL scripts and Django for integrating your work into web applications\\nKnowledge of programmatically fetching and sending data over REST APIs with some programming language such as Python, Ruby, Java, PHP, etc.\\nAt least a basic understanding and some experience in the topic of data quality and query performance\\nAdditional Information\\nOptional, but big pluses include doing prior work with nonprofit CRM systems such as Salesforce, Ellucian, Blackbaud, etc., and experience or interest in big data, including deployment of systems such as Hadoop.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Business Intelligence Engineer, Language Expansion</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)3+ years of relevant experience in one of the following areas: business intelligence, data engineering, or business analytics3+ years of hands-on experience in writing SQL queries across data setsExperience with Tableau, Matillion, Quicksight, matplotlib, and AWS services (Redshift, S3, AWS Glue, EMR, DynamoDB)Knowledge of distributed systems as it pertains to data storage and computingExperience with scripting languages (Python, Perl, Ruby), Java, or other programming languagesExperience writing queries on relational and NoSQL databasesExperience working with text data in multiple languages, either academic or professional\\n\\nInterested in Amazon Alexa? We are developing the speech and language solutions behind Amazon Alexa, Amazon Echo and other Amazon products and services. We’re working hard, having fun, and making history!\\n\\nThe Alexa Applied Modeling and Data Science team is looking for a creative Business Intelligence Engineer (BIE). As a BIE you will be responsible for helping us scale the business to expand to new languages and territories. You will work with product owners to find solutions to customer pain points, and generate unique insights backed by data and coupled with actionable conclusions. You will invent metrics, data schemas, and standards for Alexa’s critical-path data solutions.\\n\\nAs a BIE leader for the language expansion team, we look to you to build an analytics suite that will provide a detailed view into our natural language understanding (NLU) engines, developing predictive analytics models to forecast requirements for new languages and features. You will lead design, implementation, and successful delivery of large-scale, critical, or difficult data solutions involving a significant amount of work. You will share in the ownership of the technical vision and direction for advanced analytics and insight products. You will be a part of a team of top technical professionals developing complex systems at scale and with a focus on sustained operational excellence\\n\\nThis position will support multiple data sourcing products and programs, including the publicly available Alexa skill, Cleo, which gives Amazon customers from around the world an opportunity to teach Alexa their language and dialect. https://www.amazon.com/Amazon-Cleo/dp/B01N5QDE0Y\\n\\nKEY RESPONSIBILITIES\\nThe BIE must have a passion for data, efficiency, and accuracy. Specifically, they will:\\nBuild and optimize ETLs and their underlying SQL queries to efficiently, accurately, and with low-latency, load data to our product suite for use by customers.Influence big data solutions/access to data set(s) in team architecture and will be solely responsible for the efficient, secure, and performant queries underlying our analytics suite. As such, attention to detail, data integrity, and strong analytical skills (i.e., understanding business implications of what the data says) are required.Have knowledge of engineering and operational excellence best practices. Can make enhancements that improve data processes (e.g., data auditing solutions, management of manually maintained tables, automating, ad-hoc or manual operation steps).Understand how to make appropriate data trade-offs. Can balance customer requirements with technology requirements. Knows when to re-use code. Is judicious about introducing dependencies.Deliver pragmatic solutions. You do things with the proper level of complexity the first time (or at least minimize incidental complexity).Understand how to be efficient with resource usage (e.g., system hardware, data storage, query optimization, AWS infrastructure etc.)Write code that a Data Engineer or Software Development Engineer unfamiliar with the system can understand.Communicate proposals and results to a wide audience in a clear, data-driven mannerProvide day-to-day expert support for the Cleo teamProvide key support for on-going and new data collection effortsThrive in an agile, iterative environment where collaboration with Product Managers, Data Engineers, and Software Development Engineers is crucial for delivering the right information to our customers.Have demonstrated success in an environment which offers ambiguously defined problems, big challenges, and quick changes.\\n\\nMaster’s degree in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)Ability to communicate technical concepts to non-technical customers in a cross-functional environmentKnowledge of recent advances in distributed systems (e.g. MapReduce, MPP architectures, and NoSQL databases). You are proficient in a broad range of data design approaches and know when it is appropriate to use them (and when it is not).Experience with the full lifecycle of designing, developing, and maintaining softwareDemonstrated strength in data modeling, ETL development, and data warehousing.Experience building infrastructure to manage large data setsExperience with machine learning (ML) and natural language processing (NLP)Fluency in a foreign language\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Azure Data Engineer</td>\n",
       "      <td>Somerville, MA 02145</td>\n",
       "      <td>Somerville</td>\n",
       "      <td>MA</td>\n",
       "      <td>02145</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3-5 Years of experience architecting and building Data Lake, Azure Big Data architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures and data sets.Advanced hands-on SQL, USQL, Python, C#, Java, pySpark (2+ of these) knowledge and experience working with relational databases for data querying and retrieval.Experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Azure Data Bricks, Azure ML, SQL Data Warehouse, HDInsight..Experience with Design and Architecture of Apache Hadoop big data frameworks/tools: Hadoop, Kafka, Spark, etc.Experience with Design and Architecture of relational SQL and NoSQL databases, including MS SQL Server, Cosmos DBExperience with Design and Architecture of data security and Azure security, VM, VnetExperience with building processes supporting data transformation, data structures, metadata, dependency and workload management.Experience working with cross-functional teams in a dynamic environment.Experience building Big data pipeline with Java and/or Python a plus.3-5 Years of Experience with Hadoop based technologies (e.g. hdfs, Spark). Spark Experience is mustStrong SQL skills on multiple platform (preferred MPP systems)Leading development of Data Lake Architectures from scratchData Modeling tools (e.g. Erwin, Visio)3-5 years of Programming experience in Python, and/or JavaExperience with Continuous integration and deploymentStrong Unix/Linux skillsExperience in petabyte scale data environments and integration of data from multiple diverse sourcesCloud advanced analytics – Azure ML, machine learning, text analysis, NLPHealthcare experience, most notably in Clinical data, Epic, Payer data and reference data is a plus but not mandatory</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a not-for-profit organization, Partners HealthCare is committed to supporting patient care, research, teaching, and service to the community by leading innovation across our system. Founded by Brigham and Women’s Hospital and Massachusetts General Hospital, Partners HealthCare supports a complete continuum of care including community and specialty hospitals, a managed care organization, a physician network, community health centers, home care and other health-related entities. Several of our hospitals are teaching affiliates of Harvard Medical School, and our system is a national leader in biomedical research.\\n\\nWe’re focused on a people-first culture for our system’s patients and our professional family. That’s why we provide our employees with more ways to achieve their potential. Partners HealthCare is committed to aligning our employees’ personal aspirations with projects that match their capabilities and creating a culture that empowers our managers to become trusted mentors. We support each member of our team to own their personal development—and we recognize success at every step.\\n\\nOur employees use the Partners HealthCare values to govern decisions, actions and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability &amp; Service Commitment, Decisiveness, Innovation &amp; Thoughtful Risk; and how we treat each other: Diversity &amp; Inclusion, Integrity &amp; Respect, Learning, Continuous Improvement &amp; Personal Growth, Teamwork &amp; Collaboration.\\n\\nOverview\\nWe are looking for a self-motivated Data Engineer to join our data engineering team.Design, Develop, construct, test and maintain architectures such as Data Lake, large-scale data processing systemsBig data ecosystem related Tool selection and POC analysisGather and process raw data at scale that meet functional / non-functional business requirements (including writing scripts, REST API calls, SQL Queries, etc.)Develop data set processes for data modeling, mining and productionIntegrate new data management technologies (Collibra, Informatica DQ..) and software engineering tools into existing structuresThe candidate will be responsible for participating in building new Data Lake in Azure, Hadoop, expanding and optimizing our data platform and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up.The Data Engineer will support our Software Developers, Database Architects, Data Analysts and Data Scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.The right candidate will be excited by the prospect of optimizing and/or re-designing our data architecture to support next generation of products and data initiatives.\\nPrincipal Duties And Responsibilities\\nCreate and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements on Hadoop and relational data systemsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.Build the Hadoop infrastructure required for optimal extraction, transformation, and loading of data from traditional/legacy data sources.Work with stakeholders including the Management team, Product owners, and Architecture teams to assist with data-related technical issues and support their data infrastructure needs.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.\\n\\nQualifications\\n3-5 Years of experience architecting and building Data Lake, Azure Big Data architecture, Enterprise Analytics Solutions, and optimizing 'big data' data pipelines, architectures and data sets.Advanced hands-on SQL, USQL, Python, C#, Java, pySpark (2+ of these) knowledge and experience working with relational databases for data querying and retrieval.Experience with Design and Architecture of Azure big data frameworks/tools: Azure Data Lake, Azure Data Factory, Azure Data Bricks, Azure ML, SQL Data Warehouse, HDInsight..Experience with Design and Architecture of Apache Hadoop big data frameworks/tools: Hadoop, Kafka, Spark, etc.Experience with Design and Architecture of relational SQL and NoSQL databases, including MS SQL Server, Cosmos DBExperience with Design and Architecture of data security and Azure security, VM, VnetExperience with building processes supporting data transformation, data structures, metadata, dependency and workload management.Experience working with cross-functional teams in a dynamic environment.Experience building Big data pipeline with Java and/or Python a plus.3-5 Years of Experience with Hadoop based technologies (e.g. hdfs, Spark). Spark Experience is mustStrong SQL skills on multiple platform (preferred MPP systems)Leading development of Data Lake Architectures from scratchData Modeling tools (e.g. Erwin, Visio)3-5 years of Programming experience in Python, and/or JavaExperience with Continuous integration and deploymentStrong Unix/Linux skillsExperience in petabyte scale data environments and integration of data from multiple diverse sourcesCloud advanced analytics – Azure ML, machine learning, text analysis, NLPHealthcare experience, most notably in Clinical data, Epic, Payer data and reference data is a plus but not mandatory\\nSkills Required\\nExpertise in the Azure, Hadoop Data Lake and relational Data Warehouse platformsDemonstrated experience in Azure and Hadoop big data technologies (Cloudera, Hortonworks), Data Lake developmentExperience with real time data processing and analytics productsExperience with Azure Big data technologies (Azure Data Lake, Azure Data Factory, Azure Data Bricks, Azure ML, SQL Data Warehouse, HDInsight..)Azure certification preferredCloudera or Big Data certification would be preferredLarge data warehousing environments in at least two database platforms (Oracle, SQL Server, DB2, etc)Programming experience in Python, Java, SQL, good to have .Net, C#ETL, data processing expertise in Azure (Azure Data Factory, Data Bricks..), Hadoop (map-reduce, spark, sqoop) and SSIS, HealthCatalyst, InformaticaFamiliarity with data governance and data quality principles, good to have experience with data quality tools\\nAbility to independently troubleshoot and performance tune in large scale data lake, enterprise systemsKnowledge of data architecture principles, data lake, data warehousing, agile development, DevOps concepts and methodologiesUnderstanding of change management techniques, and the ability to apply themExcellent verbal and written communication skills, problem solving and negotiation skillsAct as an effective, collaborative team member\\nWorking Conditions\\n\\nOffice setting, with some local travel between Partners Healthcare System sitesMay require occasional travel for training\\n\\nEEO Statement\\n\\nPartners HealthCare is an Equal Opportunity Employer &amp; by embracing diverse skills, perspectives and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under law.\\n\\nPrimary Location: MA-Somerville-Assembly Row - PHS\\nWork Locations: Assembly Row - PHS 399 Revolution Drive Somerville 02145\\nJob: Business and Systems Analyst\\nOrganization: Partners HealthCare(PHS)\\nSchedule: Full-time\\nStandard Hours: 40\\nShift: Day Job\\nEmployee Status: Regular\\nRecruiting Department: PHS Enterprise Data &amp; Digital Health\\nJob Posting: Sep 30, 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Boston, MA 02111</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02111</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Exceptional analytical and creative problem solving abilities\\nIntrinsic sense of urgency with ability to prioritize workload, meet project deadlines and drive results\\nIntellectually curious and interested in exploring new ways to analyze data\\nAbility to manage and analyze data, determine trends, identify anomalous results, draw conclusions and communicate findings\\nAlways willing to wrestle even minor unexplained variances found in data to the ground\\nEffective communication skills (both verbal and written)\\nAbility to strike the right balance between working independently and collaboratively depending upon the individual task or project assigned</td>\n",
       "      <td>Act as primary architect and owner of our data infrastructure\\nPrincipal owner and administrator of our team’s SQL Server\\nDevelop and maintain jobs, stored procedures, SSIS packages and other ETL processes\\nTake the lead in assembling and structuring data for analysis and introducing automation techniques where appropriate\\nIdentify, design and implement technical process improvements, automate manual processes, optimize data delivery with a constant eye towards scalability\\nWork closely with Claims Department management and other business functions by developing and producing both recurring (production) and ad hoc reports and metrics, as well as assisting teams with data-related technical issues and support their data infrastructure needs\\nCreate new tools to help manage our business and optimize results\\nProvide other Claims Department analytical support and collaborate with other departments as projects require</td>\n",
       "      <td>Bachelor’s Degree in a technical discipline desired (engineering, statistics, computer science, economics, or mathematics). Master’s Degree a plus.\\nPrior experience working with insurance data preferred</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Plymouth Rock Company and its affiliated group of companies write and manage over $1.3 billion in personal and commercial auto and homeowner’s insurance throughout the Northeast, where we have built an unparalleled reputation for service. We continuously invest in technology, our employees thrive in our empowering environment, and our customers are among the most loyal in the industry.\\n\\nThe Plymouth Rock group of Companies employs 1,800 people and is headquartered in Boston, Massachusetts. Plymouth Rock Assurance Corporation holds an A.M. Best rating of \"A-/Excellent\".\\n\\nThe Data Engineer’s mission will be to provide the in-depth technical and analytical support required to propel our analytics team forward during an exciting time of expansion at the Plymouth Rock companies. The successful candidate will participate in a broad range of data management, business intelligence and other analytics-based activities designed to ensure Plymouth Rock maintains and extends its competitive edge with respect to loss management, claims customer service, expense control, and employee engagement.\\n\\nEssential Duties and Responsibilities:\\nAct as primary architect and owner of our data infrastructure\\nPrincipal owner and administrator of our team’s SQL Server\\nDevelop and maintain jobs, stored procedures, SSIS packages and other ETL processes\\nTake the lead in assembling and structuring data for analysis and introducing automation techniques where appropriate\\nIdentify, design and implement technical process improvements, automate manual processes, optimize data delivery with a constant eye towards scalability\\nWork closely with Claims Department management and other business functions by developing and producing both recurring (production) and ad hoc reports and metrics, as well as assisting teams with data-related technical issues and support their data infrastructure needs\\nCreate new tools to help manage our business and optimize results\\nProvide other Claims Department analytical support and collaborate with other departments as projects require\\n\\nTechnical Proficiency:\\nAdvanced SQL knowledge and comfort working with relational databases and complex query authoring\\nExperience managing data environments independently\\nExperience working with common business intelligence tools, preferably Tableau\\nAdvanced knowledge of Excel\\nExperience with object-oriented programming languages (such as Python, C++, etc.)\\nExperience working with big data and/or unstructured data a plus\\n\\nEducation and Experience:\\nBachelor’s Degree in a technical discipline desired (engineering, statistics, computer science, economics, or mathematics). Master’s Degree a plus.\\nPrior experience working with insurance data preferred\\n\\nSkills and Abilities:\\nExceptional analytical and creative problem solving abilities\\nIntrinsic sense of urgency with ability to prioritize workload, meet project deadlines and drive results\\nIntellectually curious and interested in exploring new ways to analyze data\\nAbility to manage and analyze data, determine trends, identify anomalous results, draw conclusions and communicate findings\\nAlways willing to wrestle even minor unexplained variances found in data to the ground\\nEffective communication skills (both verbal and written)\\nAbility to strike the right balance between working independently and collaboratively depending upon the individual task or project assigned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Boston, MA 02134</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02134</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBS in computer science, systems engineering, or a similar technical field with relevant work experience.\\nAn understanding of data model design, database schemas, and optimizing database applications.\\nA breadth of understanding of database technologies including both relational and non-relational solutions.\\nExperience manipulating large data sets of time-series and intermittent data.\\nExperience using version control software.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nInterface with software stakeholders to understand infrastructure and user requirements.\\nDevelop and support Etiometry’s data-warehouse solution both for clinical personnel and internal research staff.\\nBuild, test and deploy software and database upgrades into a production environment.\\nUtilize and improve Etiometry’s clinical data cleaning tools and techniques, which include data extraction, de-identification, and clinical measure normalization &amp; cleaning.\\nDesign and implement requirements for company research.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Position Summary\\n\\nWe are currently seeking a Data Engineer who will assist in the design and implementation of a hospital-based clinical data-warehouse system. Your role will be to support existing software and integrate new data sources with a focus on efficiency and availability. Etiometry has the world’s largest collection of pediatric intensive care unit monitoring data and the volume and data types are continually increasing. The data sets provide numerous storage and normalization challenges but can offer important insights into the efficacy of existing treatments and reveal new and innovative patient management strategies.\\n\\nResponsibilities\\n\\nInterface with software stakeholders to understand infrastructure and user requirements.\\nDevelop and support Etiometry’s data-warehouse solution both for clinical personnel and internal research staff.\\nBuild, test and deploy software and database upgrades into a production environment.\\nUtilize and improve Etiometry’s clinical data cleaning tools and techniques, which include data extraction, de-identification, and clinical measure normalization &amp; cleaning.\\nDesign and implement requirements for company research.\\nBasic Qualifications\\n\\nBS in computer science, systems engineering, or a similar technical field with relevant work experience.\\nAn understanding of data model design, database schemas, and optimizing database applications.\\nA breadth of understanding of database technologies including both relational and non-relational solutions.\\nExperience manipulating large data sets of time-series and intermittent data.\\nExperience using version control software.\\nDesired Qualifications\\n\\nExperience designing and developing database access layers for schemas that contain multiple databases containing unique data types and access requirements.\\nExperience working with Python as a primary development language with an emphasis on data management and processing.\\nExperience with MySQL and MongoDB\\nExperience producing software for a clinical setting that utilizes clinical patient data, e.g., labs, physiologic signals, and administrative data.\\nAn understanding of clinical (or any data-driven) research from a data aggregation and methodologies standpoint (study design, subject protections, and statistical analysis).\\nExperience with Agile software development methodologies, and continuous integration and delivery.\\nContact Us\\n\\nIf you are interested in this opportunity, please email careers@etiometry.com with your resume.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Boston, MA 02110</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02110</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About You\\nMore than a candidate that checks every box, we’re looking for people who are excited to work, learn, and grow at Sonos—no matter their background or how they identify. If that’s you, we hope you’ll apply for this role.\\nYou want to be part of a team.\\nYou come with new ideas and a unique point of view. You look forward to collaborating with a diverse team of individuals. You assume everyone’s best intentions, welcome a healthy debate, and embrace differing opinions. You eagerly seek and give help. Transparency tops your list of values, and you proactively contribute to a culture of respect and inclusion.\\nYou enjoy a challenge.\\nInquisitive and focused, you see every challenge as an opportunity. You’re ambitious and comfortable making mistakes because you learn from them and bounce back quickly. You would rather create the future than wait for it. You prioritize long-term value over short-term objectives.\\nYou love to listen.\\nYou approach every interaction with curiosity and a desire to understand. You want to make a positive impact in the world. You’re passionate about culture and know the power that music, film, podcasts, games, and stories have to bring people together.\\nWhat You’ll Do\\nBecome a data expert at Sonos, help to educate and evangelize data usage across Product Development.\\nInnovate and build data systems working with data science teams to unearth insights from large volumes of IoT data and tell compelling stories.\\nWork with stakeholders across the organization to design new instrumentation and processing frameworks, to provide insight into usage, monitor system reliability, and to diagnose customer issues.\\nMentor junior team members and lead complex projects that require collaboration with other Software Engineers, User Experience Designers, Product Managers, DevOps Engineers, and Test Engineers.\\nDocument and assist in the development of tools to automate our data workflow.\\nAsk questions, think on your feet, and absorb lots of information.\\nPassion and ability for solving intractable problems. Superior critical thinking skills.\\nSkills You’ll Need\\nBecome a data expert at Sonos, help to educate and evangelize data usage across Product Development.\\nInnovate and build data systems working with data science teams to unearth insights from large volumes of IoT data and tell compelling stories.\\nWork with stakeholders across the organization to design new instrumentation and processing frameworks, to provide insight into usage, monitor system reliability, and to diagnose customer issues.\\nMentor junior team members and lead complex projects that require collaboration with other Software Engineers, User Experience Designers, Product Managers, DevOps Engineers, and Test Engineers.\\nDocument and assist in the development of tools to automate our data workflow.\\nAsk questions, think on your feet, and absorb lots of information.\\nPassion and ability for solving intractable problems. Superior critical thinking skills.\\nMore About Sonos\\nSonos is a sound experience company. We pioneered multiroom wireless audio, made it sound amazing, and changed the way people listen, making it effortless for them to enjoy what they want, where they want, how they want.\\nToday we continue empowering listeners by developing new technologies, thoughtfully designing products, expanding our software platform, and crafting brilliant sound experiences while participating in a culture that values respect, transparency, collaboration, and ownership.\\nTogether we’re working to positively impact the world and inspire everyone to listen better—because listening brings people together, builds understanding, drives change, and makes us happier.\\nNotice to European Job Applicants: Information you submit as a part of your job application will be used in accordance with Sonos EU Job Applicant Privacy Notice.\\nNotice to U.S. Job Applicants: Sonos is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics.\\nFollow the links to review the EEO is the Law poster and its supplement. The pay transparency policy is available here. Sonos is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process, please send an e-mail to accommodations@sonos.com and let us know the nature of your request and your contact information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Engineer (SY19-20)</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Boston Public Schools seeks a junior Data Engineer to help create and optimize the data pipeline architecture for the Office of Data and Accountability. You will be a key member of the Office of Data and Accountability and deliver great support to the team’s core group of analysts. Over the past two years, the Office of Data and Accountability and the Office of Instructional and Information Technology have worked collaboratively to implement the Ed-Fi data standard with support from the Michael and Susan Dell Foundation. The work of this position will directly support the departments’ collaborative work in implementing the Ed-Fi data standard.\\n\\n\\nReports to: Director of Analytics, Office of Data and Accountability\\n\\n\\nKey responsibilities:\\n\\nImplement data pipelines using agile, iterative processes to deliver value quickly\\nTreat data quality, reliability, and documentation as fundamental acceptance criteria for data pipelines\\nPartner with data analysts to implement develop and deliver products that serve internal and external BPS stakeholders\\nCollaborate with the team to make the best technology choices\\nDeliver solutions that ensure data accuracy, high availability, robust security, and that are built for scaling\\nAssists team in the development and maintenance of best practices, methodologies, standards and frameworks\\nContinually seeks ways to improve processes, workflows and/or operations\\nAssist with other analytical tasks as assigned\\nQUALIFICATIONS - REQUIRED:\\n\\nBachelor’s degree in a related field - a quantitative field is a plus\\n2+ years of full time, professional work experience\\nAt least 2 years of experience with SQL - experience with T-SQL is a plus\\nExperience in optimizing queries and tuning database performance\\nMust have knowledge and experience in descriptive statistics and regression\\nExperience in cleaning data and building data pipelines - Python, R, or Java experience is a plus\\nUnderstands business value, context and priorities of the team's work\\nCollaborates in scoping, solution design and evaluation\\nAttention to detail - All downstream work is only as good as the quality and integrity of the data you're moving through the pipeline\\nAppreciation for clean design - You should appreciate the elegance of clean and simple designs that are not over-architected\\nGood communication skills - You'll have to talk to people from other departments to understand the playing field before you design anything and you should find satisfaction in helping colleagues solve problems\\nExcitement about working on back-end systems - You have to take pride in being the hero behind the scenes\\nA love of learning - Things change fast and you need to be able to quickly understand, evaluate, and learn new tools if necessary\\nCurrent authorization to work in the United States - Candidates must have such authorization by their first day of employment\\nQUALIFICATIONS - PREFERRED:\\n\\nExperience working in public schools, preferably in a large urban setting\\nExperience working with normalized databases and Microsoft SQL Server\\nKnowledge of Ed-Fi Data Standard\\nTERMS: Managerial, Tier C\\n\\nThis position subject to the City of Boston residency requirement.\\n\\n\\nPlease refer to www.bostonpublicschools.org/ohc (under \"Employee Benefits and Policies\") for more information on salary and compensation. Salaries are listed by Unions and Grade/Step.\\n\\n\\nNOTE: School-based managerial employees will work 223 days between July 1st and June 30th each year. The 223 day work-year will include the 180 days in which school is in session, and the additional days will be determined by the employee and the principal or headmaster of the school. This position subject to the City of Boston residency requirement.\\n\\n\\nSchool-based managerial employees are not eligible for vacation time or compensatory time. In the event of school cancellation due to snow or inclement weather, school-based managerial employees need not report to work.\\n\\n\\nThe Boston Public Schools, in accordance with its nondiscrimination policies, does not discriminate in its programs, facilities, or employment or educational opportunities on the basis of race, color, age, criminal record (inquiries only), disability, homelessness, sex/gender, gender identity, religion, national origin, ancestry, sexual orientation, genetics or military status, and does not tolerate any form of retaliation, or bias-based intimidation, threat or harassment that demeans individuals’ dignity or interferes with their ability to learn or work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Engineer/Senior Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nArchitect, build and automate production ETL workflows for the company.\\nDesign, build, and automate data transformation pipelines for batch and streaming services and applications.\\nArchitect schemas and data warehousing solutions for the company and our clients.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDemonstrated professional experience with Python, Spark, SQL and AWS services, including EMR, Athena and Lambda\\nStrong oral and written presentation skills.\\nPreferred degree in computer science, computer engineering or similar quantitative field. Advanced degree (M.Sc/Ph.D.) is a plus.\\n5+ years of relevant work experience.</td>\n",
       "      <td>Are you passionate about technology and data? Are you the kind of person that sees a need for help and raises their hand? Do you value building high quality solutions as much as you value supporting your teammates and the problems they’re working on? If the answer is a resounding yes to those questions, then you will probably be a great fit at SessionM!\\n\\nWe are looking to add a talented Data Engineer to our fast-growing Data Engineering team to help us build next-generation data ETL pipelines and warehousing solutions for our backend infrastructure.\\nResponsibilities\\nArchitect, build and automate production ETL workflows for the company.\\nDesign, build, and automate data transformation pipelines for batch and streaming services and applications.\\nArchitect schemas and data warehousing solutions for the company and our clients.\\nRequirements\\nDemonstrated professional experience with Python, Spark, SQL and AWS services, including EMR, Athena and Lambda\\nStrong oral and written presentation skills.\\nPreferred degree in computer science, computer engineering or similar quantitative field. Advanced degree (M.Sc/Ph.D.) is a plus.\\n5+ years of relevant work experience.\\nWhat we’re offering you\\nCompetitive pay, equity and excellent benefits.\\nOpportunity to get in early to build a world-class organization, shape the future of our team, our product, and our company.\\nExcitement of working on our high-volume - 4 billion transactions/day - and high-performance - 50 ms service response times - platform for some of the most exciting brands – Coca-Cola, Air Canada, Chipotle, L’Oreal to name a few.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Principal Big Data Engineer</td>\n",
       "      <td>Boston, MA 02212</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02212</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nYour expertise in languages such as Python, Ruby, Go, JavaScript and Java, as well as strong Unix shell skills\\nYour Big Data Skills with popular stacks like Hadoop and Spark\\nYour knowledge of AWS CloudFormation, OpenStack HEAT templates and Terraform\\nYour hands on experience in all phases of data modeling, from conceptualization to database optimization.\\nYour ability to map the systems and interfaces used to manage data, sets standards for data management, analyzes current state and conceives desired future state, and conceives projects needed to close the gap between current state and future goals\\nYour desire and aptitude for learning new technologies\\nYour excellent verbal and written communication skills\\nYour experience owning your systems end-to-end and you are hands-on in every aspect of the software development and support: requirement discussion, architecture, prototyping, development, debugging, unit-testing, deployment, support.\\nYour passion and intellectual curiosity to learn new technologies and business areas\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Fidelity Personal Investing has an opportunity for a hands-on Big Data Engineer, reporting to VP of Big Data Architecture to help anchor an exciting and fast-paced engineering team focused on designing and implementing large-scale distributed data processing systems using cutting edge cloud based open source and proprietary big data technologies.\\nIn this role, you will implement a variety of solutions to ingest data into, process data within, and expose data from a Data Lake that enables our data analysts and scientists to explore data in the ad-hoc manner as well as quickly implement data-driven models that generate accurate insights in an automated fashion. This position is a critical element to delivering Fidelity’s promise of creating the best customer experiences in financial services.\\nThe Expertise We’re Looking For\\nBachelor’s degree or higher in a technology related field (e.g. Engineering, Computer Science, etc.) required.\\n5+ years of hands-on experience applying principles, best practices and trade-offs of schema design to various types of database systems: relational (Oracle, MSSQL, Postgres, MySQL), NoSQL (HBase, Cassandra, MongoDB) and in-memory (e.g. VoltDB). Understanding data manipulation principles.\\n7+ years of hands-on experience in one or more modern Object Oriented Programming languages (Java, Scala, Python) including the ability to code in more than one programming language. Our engineers work across several of them, sometimes simultaneously.\\n7+ years of hands-on experience in building distributed back-end enterprise software platforms.\\n5+ years of working in Linux environment, ability to interface with the OS using system tools, scripting languages, integration frameworks, etc.\\n2+ years of hands-on experience in implementing batch and real-time Big Data integration frameworks and/or applications, in private or public cloud, preferably AWS, using various technologies (Hadoop, Spark, Impala, etc); debugging, identifying performance bottlenecks and fine-tuning those frameworks.\\n2+ years of experience with DevOps, Continuous Integration and Continuous Delivery (Maven, Jenkins, Stash, Ansible, Docker).\\nExperience and comfort executing projects in Agile environments (Kanban and Scrum).\\nThe Skills You Bring\\nYour expertise in languages such as Python, Ruby, Go, JavaScript and Java, as well as strong Unix shell skills\\nYour Big Data Skills with popular stacks like Hadoop and Spark\\nYour knowledge of AWS CloudFormation, OpenStack HEAT templates and Terraform\\nYour hands on experience in all phases of data modeling, from conceptualization to database optimization.\\nYour ability to map the systems and interfaces used to manage data, sets standards for data management, analyzes current state and conceives desired future state, and conceives projects needed to close the gap between current state and future goals\\nYour desire and aptitude for learning new technologies\\nYour excellent verbal and written communication skills\\nYour experience owning your systems end-to-end and you are hands-on in every aspect of the software development and support: requirement discussion, architecture, prototyping, development, debugging, unit-testing, deployment, support.\\nYour passion and intellectual curiosity to learn new technologies and business areas\\nThe Value You Deliver\\nBuild a strategy to re-invent systems and tools to create a continuous cycle of Innovation\\nCreate data monitoring models for each product and work with our marketing team to create models ahead of new releases\\nAbility to build data models supporting complex transformation\\nIdentifying and ingesting new data sources and performing feature engineering for integration into models\\nHow Your Work Impacts the Organization\\n\\nOur teams are flat, non-hierarchical structures, which run on agile principles. You’ll be driving multiple initiatives by means of designing architecture, defining best practices and evangelizing initiatives across this and other teams. Many of these initiatives are just starting allowing you the opportunity to drive and affect the foundational development work for years ahead. We invest in a broad range of technologies and experiment before delivering a production system. As a lead engineer, you’ll help lead our technology research, evaluation and PoC efforts, mentor and guide less senior members of our team.\\n\\nCompany Overview\\nAt Fidelity, we are focused on making our financial expertise broadly accessible and effective in helping people live the lives they want. We are a privately held company that places a high degree of value in creating and nurturing a work environment that attracts the best talent and reflects our commitment to our associates. For information about working at Fidelity, visit FidelityCareers.com.\\n\\nFidelity Investments is an equal opportunity employer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Engineer I</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>A Bachelor’s degree or higher in Computer Science, Engineering, Mathematics, Physics, or a related fieldProficiency in SQL and programming languages such as python2+ years of hands on experience in working with data including but not limited to Data Warehouse (DWH) environment with data integration/ETL of large and complex data sets, Data modeling skills such as Star/Snowflake schema design for DWH, or building large scale data-processing systems with experience in Big Data technologies such as MapReduce, Hadoop, Spark, Kafka or AWS equivalents.Familiarity with Database technologies such as AWS Redshift, Oracle, Teradata, or othersFamiliarity with Business Intelligence (BI) and Visualization platforms such as MicroStrategy and AWS QuicksightAbility to communicate effectively and work independently with little supervision to deliver on time quality productsWillingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision\\n\\nThis opportunity is within Audible’s Data Engineering group. The Data Engineering group owns technology platforms and datasets that enable systems and people to uncover new insights and fine-tune operations to meet business goals. We need your help designing and building these.\\n\\nKEY RESPONSIBILITIES\\nApply broad knowledge of technology options, technology platforms, design techniques and approaches across the Data Engineering ecosystem to build systems that meet business needsBuild systems and datasets using software engineering best practices, data management fundamentals, data storage principles, recent advances in distributed systems, and operational excellence best practicesAnalyze source systems, define underlying data sources and transformation requirements, design suitable data models and document the design/specificationsDemonstrate passion for quality and productivity by use of efficient development techniques, standards and guidelinesEffectively communicate with various teams and stakeholders, escalate technical and managerial issues at the right time and resolve conflictsPeer review work. Actively mentor other members of the team, improving their skills, their knowledge of our systems and their ability to get things done\\nHOW DOES AMAZON FIT IN?\\nWe're a part of Amazon, they are our parent company and it's a great partnership. You'll get to play with all of Amazon's technologies like EC2, SQS and S3 but it doesn't stop there. Audible's built on Amazon technology and you'll have insight into the inner workings of the world's leading ecommerce experience. There's a LOT to learn!\\n\\nIf you want to own and solve problems, work with a creative dynamic team, fail fast in a supportive environment whilst growing your career and working on a platform that powers web applications used by millions of customers worldwide we want to hear from you.\\n\\nHands on experience with BI and Visualization platforms such as MicroStrategy and AWS QuicksightExperience with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena\\nAudible is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Framingham, MA 01702</td>\n",
       "      <td>Framingham</td>\n",
       "      <td>MA</td>\n",
       "      <td>01702</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s Degree in Computer Science (or related field), Master’s degree preferred 7+ years of overall professional experience At least 2 years Looker development experience creating Looker ML and Looker</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Builds Looker visualization and Looker ML models to curated data layer in Snowflake Works with Product owners across Staples business teams and IT to define new KPI</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description\\nIntroduction:\\nThere’s never been a more exciting time to be on the Enterprise Data &amp; Analytics team at Staples! Staples is on an exciting journey to modernize the data landscape, consolidate data assets, and build a brand new Data Warehouse. The DW will support critical business process transformation, be in the cloud, integrate with Hadoop, and implement and enforce new data governance standards. The Analytics that will sit on top of the DW will be solution oriented, enforce definitions, provide self-service solutions, and focus heavily on data visualization techniques that are actionable by the business. The team is seeking top notch talent for BI reporting engineers experienced in Looker, Looker ML, data visualization/UX (Including visualizing R models). If you are interested in using your expertise and innovative skills to build something brand new, enjoy making architectural decisions, are a resourceful technologist who likes to wear many hats, consider yourself an ‘A’ player, have a passion for data, and like to mentor others, then we want to talk to you! A fantastic opportunity awaits you!\\n\\nEssential Responsibilities: Builds Looker visualization and Looker ML models to curated data layer in Snowflake Works with Product owners across Staples business teams and IT to define new KPI\\n measurements and best practice visualization Implement, decide and document UX data visualization standards Extract complex data from multiple sources into usable and meaningful reports and analyses Mentor and support other Developers and business partners Designs, documents, administers and maintains Looker ML models Create and maintain detailed documentation for development team Support user community on data analytics Perform training and knowledge transfer of technology Provides ongoing performance monitoring and tuning, and automation of production\\n activities related the BI environment Works in conjunction with business analysts and other users acting as a liaison between\\n technical and business departments in developing dashboards and other BI related\\n solutions. Analyze existing complex systems to underst\\n\\nQualifications\\nBasic: Bachelor’s Degree in Computer Science (or related field), Master’s degree preferred 7+ years of overall professional experience At least 2 years Looker development experience creating Looker ML and Looker\\n visualizations 2 or more years’ experience developing reporting solutions and Models in either Power BI,\\n Tableau or Microstrategy.\\n\\n\\nPreferred:\\n Experience with UX Design / Data Visualization strongly preferred Excellent verbal and written communication skills Possesses strong business acumen, with ability to assist in development of business\\n requirements definitions Interface with key customers from all functional areas - Analytic skills are needed Proven and recent experience of implementing solutions from requirements gathering and\\n process design to functional production deployment Expert T-SQL Programming skills Experience with dimensional data modeling Experience or knowledge of SSIS and ETL processing Experience with Microsoft Azure DB and Azure DW a big plus Experience with Hadoop tools including sqoop, Spark, and Hive Ability to manage multiple efforts concurrently Strong problem-solving skills Strong knowledge of emerging technologies and tools Strong organizational and time management skills Effective collaboration of tasks across the business and technical teams Able to work quickly and under tight delivery deadlines with focus on details Demonstrated ability to produce high quality deliverables Ability to establish and maintain positive and effective work relationships with management,\\n coworkers, and business users\\n\\nStaples is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, protected veteran status, disability, or any other basis protected by federal, state, or local law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Senior Software Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Our mission is to make biology easier to engineer and make to help data to driving the research in partnership of Ginkgo Bioworks and Joyn Bio. Ginkgo is constructing, editing, and redesigning the living world in order to answer the globe’s growing challenges in health, energy, food, materials, and more. Our bioengineers make use of an in-house automated foundry for designing and building new organisms. Today, our foundry is developing over 40 different organisms to make products across multiple industries. Joyn Bio is designing microbes for sustainable agriculture - Probiotics for Plants We're making the codebase, compiler, debugger, and data analysis for life. We're looking for an experienced Senior Software Data Engineer who is interested in architecting the software platform to support analytics and machine learning that will ultimately help to define how our bioengineering is performed at scale.\\n\\nGinkgo's programming languages of choice are Python and SQL, and DNA, but you must be someone who loves writing elegant code in any language. Most importantly, you should be passionate about making biology the next engineering discipline. The 20th century was all about bits and the awesome technology of computers. The 21st century is all about atoms and the awesome technology of biology, and Ginkgo is at the forefront of this revolution. As an experienced data pipeline builder and data wrangler who enjoys building data systems from the ground up, you’re excited by the prospect of optimizing (or even redesigning) Ginkgo’s data architecture to support our next generation of products and data initiatives. You’ll be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. You’ll also support our software developers, database architects, data analysts, and data scientists on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects.\\n\\nYou will work in close collaboration with a data science team at Joyn Bio to address data needs of the shared projects\\nResponsibilites\\nCreate and maintain optimal data pipeline architecture\\nIdentify, design, and implement internal process improvements, including: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, and more\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies\\nUse appropriate tools to analyze the data pipeline and provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics\\nWork with stakeholders including the Executive, Product, Data Science, Design, Computational Biology teams to assist with data-related technical issues and support their data infrastructure needs\\nKeep our data secure\\nDesired Experience and Capabilities\\nMaster’s degree in Computer Science, Statistics, Informatics, Information Systems or related quantitative field\\nAt least five years of data engineering experience\\nAdvanced knowledge of database design best practices, as well as experience working with relational databases, data warehouses, and big data platforms\\nProven capability of building and optimizing ‘big data’ data pipelines, architectures, and data sets\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement\\nStrong analytical skills in relation to working with unstructured datasets\\nExperience building processes that support data transformation, data structures, metadata, dependency, and workload management\\nWorking knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores\\nStrong project management and organizational skills\\nHigh level of comfort with supporting the data needs of multiple teams, systems, and products\\nStrong level of motivation and self-direction\\nDesired Software Tools/Expertise\\nBig data tools: Hadoop, Hive, Spark, Kafka, etc.\\nRelational SQL databases, including Redshift\\nData pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.\\nAWS cloud services: EC2, EMR, RDS, Redshift\\nObject-oriented/object function scripting languages: Python, Java, C++, Scala, etc.\\nLinux (working knowledge)\\nTo learn more about Ginkgo, check out some recent press:\\n\\nGinkgo Bioworks Is Turning Human Cells into On-Demand Factories (WIRED)\\nCan This Company Convince You to Love GMOs? (The Atlantic)\\nHundreds Of Millions Of Dollars Pour Into Hacking Microbes (Forbes)\\nThis food tech startup just raised $90 million to make it easier to invent new plant-based meats (Fast Company)\\n\\nWe also feel it’s important to point out the obvious here – there’s a serious lack of diversity in our industry and it needs to change. Our goal is to help drive that change. Ginkgo is deeply committed to diversity, equality, and inclusion in all of its practices, especially when it comes to growing our team. We hope to continue to build a company whose culture promotes inclusion and embraces how rewarding it is to work with engineers from all walks of life. Making biology easier to engineer is a tough nut to crack – we can’t afford to leave any talent untapped.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nYou have experience with extracting of text through parsing html, xml, json, text, pdf, word, and other types of documents\\nYour demonstrated experience developing RESTful API based web services on Windows and Linux\\nYou thrive in environments involving everything from Conceptual Design to Rapid Prototyping\\nYou are familiar with extracting data from REST APIs and parallel processing large datasets\\nYou have knowledge or interest in developing custom Data Pipelines to extract data, map data, transform data, and to load data in various data stores like Oracle, S3, and / or shared drives.\\nParticipating in problem solving, troubleshooting, performance turning, production support, and maintenance of existing APIs</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Do you love data? Do you thrive on developing multiple sources of data to create actionable business analytics? If so, this position could be a great match for you. We are looking for several accomplished data engineers to join our Advanced Data Analytics Team in Fidelity’s Asset Management organization.\\n\\nThis role is ideal for someone with an enterprise development background, with a strong technology and coding skills, looking to operate in a less constrained environment, as part of an accelerated development team.\\n\\nDue to the nature of the role, employees on this team work full time in our Boston office\\n\\nThe Team\\n\\nAsset Management Technology (AMT) provides worldwide technology and support to all the Investment Management, Research, Trading and Investment Operations functions. AMT is an integral partner for Asset Management to deliver creative, scalable, industry-leading investment tools that enable Asset Management to achieve competitive advantage globally.\\n\\nThe Expertise You Have\\n3-10 years professional development experience\\nDatabase development background, SQL, PL/SQL\\nStrong background with object oriented programming, with projects completed leveraging many of the following technologies including Python, Java or .Net, .Net Core, C#\\nExperience in Pattern Matching to extract data from Web Sites (Web Scraping), Text Documents, etc.\\nKnowledge of Cloud computing concepts (AWS) and working experience with deploying and managing applications in the Cloud\\nThe Skills You Bring\\nYou have experience with extracting of text through parsing html, xml, json, text, pdf, word, and other types of documents\\nYour demonstrated experience developing RESTful API based web services on Windows and Linux\\nYou thrive in environments involving everything from Conceptual Design to Rapid Prototyping\\nYou are familiar with extracting data from REST APIs and parallel processing large datasets\\nYou have knowledge or interest in developing custom Data Pipelines to extract data, map data, transform data, and to load data in various data stores like Oracle, S3, and / or shared drives.\\nParticipating in problem solving, troubleshooting, performance turning, production support, and maintenance of existing APIs\\n\\nCompany Overview\\n\\nAt Fidelity, we are focused on making our financial expertise broadly accessible and effective in helping people live the lives they want. We are a privately held company that places a high degree of value in creating and nurturing a work environment that attracts the best talent and reflects our commitment to our associates. We are proud of our diverse and inclusive workplace where we respect and value our associate for their unique perspectives and experiences. For information about working at Fidelity, visit FidelityCareers.com.\\n\\nFidelity Investments is an equal opportunity employer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Cambridge, MA 02139</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>MA</td>\n",
       "      <td>02139</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Spun out of MIT in 2014, Zylotech is a disruptive AI-powered customer data analytics company located in Central Square. We are ‘ZyloChamps’, big thinkers with a sense of fun who love working in an open environment full of start-up perks.\\n\\nWhy you'll love working here:\\n\\nZylotech is a fast-growing company that rewards autonomy and creativity. Our team works hard, is fast and collaborative, and we encourage personal growth. There are many opportunities to work with other departments where you can interact with some incredibly talented people.\\n\\nWhat you’ll do:\\n\\nArchitect, implement and deploy new data models and data processes in production.\\nManage data warehouse plans for a product or a group of products.\\nExplore client data, analyze and Implement ETL process leveraging open source frameworks\\nWrite python/spark scripts for Model verification and/or query database.\\nOrchestrate APIs and automate data processes\\nMonitor model performance and advising any necessary DevOps changes.\\nBuild quick POCs / prototypes around data problems\\nWhat we seek:\\n\\nA Hacker. We encourage hacking out creative ways to find and build simple and effective solutions to complex problems\\nAt least 2 years of Experience in Python and spark\\nExperience in Databases (NoSQL and SQL), preferably in PostgreSQL, MongoDB, ElasticSearch, Neo4j.\\nExperience in creating Data-warehouses and scheduling automated ETL jobs\\nExperience in Docker and Kubernetes\\nFlexible to learn, solve problems and continuously hack solutions\\nBackground in building, maintaining, and shipping sophisticated enterprise software systems through multiple release cycles, especially through upgrades and changes to APIs and data formats\\nExperience in an agile environment and a rapidly expanding product market\\nBachelor’s or Master’s degree in Computer Science or a related field of study.\\nImportant Personal Attributes:\\n\\nYou are curious about technology &amp; science.\\nTeam matters.\\nChallenging yourself is an intrinsic part of who you are.\\nSharing and debating ideas to get to the right answer is important to you.\\nYou seek to act and don’t wait to be told.\\nYou like to teach those around you.\\nYou are serious about your work but also believe that laughter and levity are important.\\nYou work hard and you also have passions outside of work that enrich your life.\\nWinning is important to you, but so is how you win.\\nYou take pride &amp; ownership in building a Product and company who is disrupting an industry.\\nAbout Zylotech:\\n\\nZylotech is a self-learning customer intelligence platform that helps marketers create complete customer profiles for targeting revenue opportunities more effectively. Powered by AutoML, the platform continuously unifies and enriches internal and external data, and performs ongoing micro-segmentation, pattern discovery, and recommendations, all while integrating with a variety of marketing clouds for on-demand accessibility. Zylotech’s cross-industry clients have reported up to a 6x increase in customer lift. For more information, visit Zylotech.com.\\n\\nZylotech is an equal employment opportunity employer and does not discriminate against any applicant because of race, creed, color, age, national origin, ancestry, religion, gender, sexual orientation, disability, genetic information, veteran status, military status, application for military service or any other class protected by state or federal law.\\n\\nTo all recruitment agencies: Zylotech does not accept agency resumes. Please do not forward resumes to our jobs alias, Zylotech employees or any other company location. Zylotech is not responsible for any fees related to unsolicited resumes. Unsolicited resumes received will be considered our property and will be processed accordingly.\\n\\nQualified Applicants must be legally authorized for employment in the United States. Qualified Applicants will not require employer-sponsored work authorization now or in the future for employment in the United States.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Azure Data Engineer</td>\n",
       "      <td>Boston, MA 02199</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02199</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of consulting or client service delivery experience on Azure\\n</td>\n",
       "      <td>DevOps on an Azure platform</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\n</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n People in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications\\n\\n Role &amp; Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers successfully delivering work efforts\\n\\n (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nBasic Qualifications\\nAt least 5 years of consulting or client service delivery experience on Azure\\nAt least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions\\nExtensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.\\nExtensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.\\n Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.\\n5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.\\nMinimum of 5 years of RDBMS experience\\nExperience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nMCSA Cloud Platform (Azure) Training &amp; Certification\\nMCSE Cloud Platform &amp; Infratsructiure Training &amp; Certification\\nMCSD Azure Solutions Architect Training &amp; Certification\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nIoT, event-driven, microservices, containers/Kubernetes in the cloud\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\nFamiliarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\n- Multi-cloud experience a plus - Azure, AWS, Google\\n\\nProfessional Skill Requirements\\n Proven ability to build, manage and foster a team-oriented environment\\n Proven ability to work creatively and analytically in a problem-solving environment\\n Desire to work in an information systems environment\\n Excellent communication (written and oral) and interpersonal skills\\n Excellent leadership and management skills\\n Excellent organizational, multi-tasking, and time-management skills\\n Proven ability to work independently\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nWe’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.\\nThe desire to learn and code in Scala\\nExperience in working in an Agile environment\\nExpert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.\\nA strong coding background in either Java, Python or Scala\\n</td>\n",
       "      <td>Founded in 2016 with only a handful of individuals, Quantexa purpose was built that through a greater understanding of context, better decisions can be made. 3 years, 6 locations and 180+ employees later we still believe that today. Working within industries such as Finance, Insurance, Energy and Government, we connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.\\n\\nOur success is driven by the talent of our staff and our commitment to quality. We are looking for Data Engineers to join us in tackling some of the industry’s most challenging problems.\\nWhat does a Data Engineer role at Quantexa look like?\\nIn order to be a successful data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.\\nBeing Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service.\\nWe want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.\\nRequirements\\nWhat do I need to have?\\nWe’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.\\nThe desire to learn and code in Scala\\nExperience in working in an Agile environment\\nExpert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.\\nA strong coding background in either Java, Python or Scala\\nExperience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.\\nPassion and drive to grow within one of the UK’s fastest growing Start-ups\\nBenefits\\nWhy join Quantexa?\\n\\nWe know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying “thank you” for all your hard work!\\nCompetitive Salary\\nCompany Bonus\\nExcellent private healthcare, Dental and Optic coverage, Life assurance, LTD and STD coverage\\n401k where we’ll match up to 5%\\nOnline training customized to your personal preferences\\nGenerous annual leave\\nAmazing working environment - Ranging from regular social events, free beverages and a very good location right by south station in Boston.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Data Engineer is an individual contributor responsible for the collection of requirements, management of individual tasks, creation of software applications, implementation of analytical algorithms, production of user interfaces, collection and curation of data, and participating in engineering processes within an agile framework.\\n\\nRequirements\\n------------\\n\\n\\nExperience in developing analytics and data processing across big data technologies\\nAbility to implement algorithms and produce analytical insight applications within a data engineering group\\nProficient in command line interaction with the unix OS\\nExpertise with Data modeling and Object Persistent Mapping\\nExperience with Cloud-based data analysis tools including Hadoop, Map/Reduce,Mahout, Acumulo, MongoDB, Impala, Pig, and neo4j\\nLarge data technology skills and experience with NoSQL databases such as MongoDB, HBase, CouchDB, etc.\\nClick stream analysis could be helpful\\n\\nProgramming skills:\\n-------------------\\n\\n\\nJava (fluent), Spring, Python, common UI technologies (bootstrap, jquery, GWA)\\nHands-on data movement work experience in Sqoop or equivalent\\nExperience with machine learning, algorithms, and data clustering in a major ML library\\nExperience with day-to-day hands on design and implementation\\nAgile development experience preferred\\nBA or BS degree in a quantitative discipline (Computer Science, Mathematics or Engineering); MS or PhD preferred\\n\\nSkill Matrix\\n------------\\n\\nSkill\\n\\nApprox. Exper. (Years)\\n\\nRecency\\n\\nJava development (JRE 1.6+)\\n\\n8\\n\\nCurrent\\n\\nOOP\\n\\n6\\n\\nCurrent\\n\\nREST API development\\n\\n5\\n\\nCurrent\\n\\nWeb UI (js, bootstrap, jquery)\\n\\n5\\n\\nCurrent\\n\\nMVC\\n\\n3\\n\\nCurrent\\n\\nSpring\\n\\n3\\n\\nCurrent\\n\\nMachine Learning Algorithms\\n\\n3\\n\\nCurrent\\n\\nHadoop (mpp development)\\n\\n2\\n\\nCurrent\\n\\nMap/Reduce\\n\\n2\\n\\nCurrent\\n\\nMongoDB, Hive, or HBase\\n\\n1\\n\\nCurrent\\n\\nPython\\n\\n1\\n\\nCurrent\\n\\nR\\n\\n0.5\\n\\nCurrent\\n\\nSqoop\\n\\n0.5\\n\\nMahout\\n\\n0+\\n\\nNeo4j\\n\\n0+\\n\\nPig\\n\\n0+\\n\\nImpala\\n\\n0+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\n\\nOur Data Engineering team builds and maintains a secure, scalable, flexible and user-friendly analytics hub that allows us to make informed and data-driven decisions. They also construct and curate business-critical data sets that allow us to realize the value of all the data we collect.\\nA Data Engineer utilizes a multidisciplinary approach to providing ETL solutions for the business, combining technical, analytical, and domain knowledge. The perfect applicant for this role has strong development skills, experience transforming and profiling data to determine risks associated with proposed analytics solutions, a willingness to continually interface with analysts in order to determine an optimal approach, and an eagerness to explore data sources to understand the availability, utility, and integrity of our data.\\nWhat you'll own:\\nData pipeline / ETL development:\\nBuilding and enhancing data curation pipelines using tools like SQL, Python, Glue, Spark and other AWS technologies\\nFocus on data curation on top of datalake data to produce trusted datasets for analytics teams\\nData Curation:\\nProcessing and cleansing data from a variety of sources to transform collected data into an accessible and curated state for Analysts and Data Scientists\\nMigrating self-serve data pipeline to centrally managed ETL pipelines\\nAdvanced SQL development and performance tuning\\nSome exposure to Spark, Glue or other distributed processing frameworks helpful\\nWork with business data stewards &amp; analytics team to research and identify data quality issues to be resolved in the curation process\\nData Modeling:\\nDesign and build master dimensions to support analytic data requirements\\nReplacing legacy data structures with new datasets sourced from streaming data feeds from the core product and other operational systems\\nDesign, build and support pipelines to deliver business critical datasets\\nResolve complex data design issues &amp; provide optimal solutions that meet business requirements and benefit system performance\\nQuery Engine Expertise &amp; Performance Tuning:\\nAssist Analytics teams with tuning efforts\\nCurated dataset design for performance\\nOrchestration:\\nManagement of job scheduling\\nDependency management mapping and support\\nDocumentation of issue resolution procedures\\nData Access\\nDesign and management of data access controls mapped to curated datasets\\nLeveraging devops best practices, such as IAC and CI/CD to build upon a scalable and extensible data environment\\n\\nExperience you'll need:\\nStrong experience designing and building end-to-end data pipelines\\nExtensive SQL development experience\\nKnowledge of data management fundamentals and data storage principles\\nData modeling:\\nNormalization\\nDimensional/OLAP design and data warehousing\\nMaster data management patterns\\nModeling trade-offs impacting data management &amp; processing/query performance\\nKnowledge of distributed systems as it pertains to data storage, data processing and querying\\nExtensive experience in ETL and DB performance tuning\\nHands on experience with a scripting language (Python, bash, etc.)\\nSome experience with Hadoop, Spark, Kafka, Impala, or other big data technologies helpful\\n\\nFamiliarity with the technology stacks available for:\\nMetadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\nData management, data processing and curation:\\nPostgres, Hadoop, Hive, Impala, Presto, Spark, Glue, etc.\\nExperience in data modeling for batch processing and streaming data feeds; structured and unstructured data\\nExperience in data security / access management, data cataloging and overall data environment management\\n\\nExperience with cloud services such as AWS and APIs helpful\\nYou’d be a great fit if your current track record looks like this:\\n5+ years of progressive experience data engineering and data warehousing\\nExperience with a variety of data management platforms (e.g. RDBMS (Postgres), Hadoop (CDH, EMR))\\nExperience with high performance query engines (Hive, Impala, Presto, Athena, MPP engines like RedShift)\\nStrong capability to manipulate and analyze complex, high-volume data from a variety of sources\\nEffective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language\\nAbility to problem solve independently and prioritize work based on the anticipated business value\\n\\nQualifications\\n\\nnull\\n\\nAdditional Information\\n\\nAll your information will be kept confidential according to EEO guidelines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Cloud Data Engineer</td>\n",
       "      <td>Boston, MA 02111</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02111</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Master’s degree in Computer Science, Engineering, or equivalent work experience\\nTwo to four years’ experience working with datasets with hundreds of millions of rows using a variety of technologies\\nIntermediate to expert level programming experience in Python and SQL in Windows and Mac/Linux environment\\nIntermediate level experience working with distributed computing frameworks, especially Spark\\nIntermediate level experience working with relational databases including PostgreSQL and Microsoft SQL Server\\nExperience working with contemporary data file formats like Apache Parquet, Avro, and columnar databases like RedShift\\nExperience working with distributed SQL query engines like Presto DB and Athena\\nExperience with Amazon Web Services including Redshift, S3, Kinesis, Glue, and DynamoDB\\nExperience analyzing data for data quality and supporting the use of data in an enterprise setting.\\nNice to have:\\nSome experience working with clustering and classification models\\nSome experience working with Trifacta\\nSome experience working with Google Analytics\\nSome familiarity working with RDFs and SparQL and some experience working with Graph Databases\\nExperience with enterprise search engine systems including ElasticSearch and Apache Solr</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Build and Maintain serverless data ingestion and refresh pipelines in terabyte scale using AWS cloud services – Amazon Glue, Amazon Redshift, Amazon S3, Amazon Athena, DynamoDB, and others\\nIncorporate new data sources from external vendors using flat files, APIs, web-scraping, and databases.\\nMaintain and provide support for the existing data pipelines using Python, Glue, Spark, and SQL\\nWork to develop and enhance the database architecture of the new analytic data environment that includes recommending optimal choices between relational, columnar, and document databases based on requirement\\nIdentify and deploy appropriate file formats for data ingestion into various storage and/or compute services via Glue for multiple use cases\\nDevelop real-time/near real-time data ingestion from web and web service logs from Splunk\\nMaintain existing processes and develop new methods to match external data sources to Homesite data using exact and fuzzy methods\\nImplement and use machine learning based data wrangling tools like Trifacta to cleanse and reshape 3rd party data to make suitable for use.\\nDevelop and implement tests to ensure data quality across all integrated data sources.\\nServe as internal subject matter expert and coach to train team members in the use of distributed computing frameworks for data analysis and modeling including AWS services and Apache projects\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.\\nOne thing that's stayed the same since our founding: our commitment to our customers, partners and employees.\\nJoin us on our journey as we continue to grow into a powerful contender in the field of insurance.\\nHomesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.\\nOne thing that's stayed the same since our founding: our commitment to our customers, partners and employees.\\n\\nJoin us on our journey as we continue to grow into a powerful contender in the field of insurance.\\n\\nWe’re looking for a Data Engineer to help us transform our data systems and architecture to support greater variety, volume, and velocity of data and data sources. You might be a good fit if:\\n\\nYou enjoy extracting data from a variety of sources and find ways to connect them and make them suitable for use in software systems and for the development of models and algorithms.\\n\\nYou enjoy interacting with new database systems and learning new data technologies and are interesting in developing your knowledge of new tools and techniques.\\n\\nYou are interested in automating data engineering efforts to minimize human interaction and optimizing data quality.\\n\\nYou have an interest in developing your knowledge of practical data science techniques and technologies in addition to your data engineering knowledge and experience.\\n\\n\\nThis role requires comprehensive data engineering skills and is not a SQL developer role though SQL is a required skill.\\nResponsibilities:\\nWe’re looking for an experienced data engineer to help us:\\nBuild and Maintain serverless data ingestion and refresh pipelines in terabyte scale using AWS cloud services – Amazon Glue, Amazon Redshift, Amazon S3, Amazon Athena, DynamoDB, and others\\nIncorporate new data sources from external vendors using flat files, APIs, web-scraping, and databases.\\nMaintain and provide support for the existing data pipelines using Python, Glue, Spark, and SQL\\nWork to develop and enhance the database architecture of the new analytic data environment that includes recommending optimal choices between relational, columnar, and document databases based on requirement\\nIdentify and deploy appropriate file formats for data ingestion into various storage and/or compute services via Glue for multiple use cases\\nDevelop real-time/near real-time data ingestion from web and web service logs from Splunk\\nMaintain existing processes and develop new methods to match external data sources to Homesite data using exact and fuzzy methods\\nImplement and use machine learning based data wrangling tools like Trifacta to cleanse and reshape 3rd party data to make suitable for use.\\nDevelop and implement tests to ensure data quality across all integrated data sources.\\nServe as internal subject matter expert and coach to train team members in the use of distributed computing frameworks for data analysis and modeling including AWS services and Apache projects\\nQualifications:\\nMaster’s degree in Computer Science, Engineering, or equivalent work experience\\nTwo to four years’ experience working with datasets with hundreds of millions of rows using a variety of technologies\\nIntermediate to expert level programming experience in Python and SQL in Windows and Mac/Linux environment\\nIntermediate level experience working with distributed computing frameworks, especially Spark\\nIntermediate level experience working with relational databases including PostgreSQL and Microsoft SQL Server\\nExperience working with contemporary data file formats like Apache Parquet, Avro, and columnar databases like RedShift\\nExperience working with distributed SQL query engines like Presto DB and Athena\\nExperience with Amazon Web Services including Redshift, S3, Kinesis, Glue, and DynamoDB\\nExperience analyzing data for data quality and supporting the use of data in an enterprise setting.\\nNice to have:\\nSome experience working with clustering and classification models\\nSome experience working with Trifacta\\nSome experience working with Google Analytics\\nSome familiarity working with RDFs and SparQL and some experience working with Graph Databases\\nExperience with enterprise search engine systems including ElasticSearch and Apache Solr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data Engineer (multiple roles, levels)</td>\n",
       "      <td>Boston, MA 02210</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02210</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nStrong analytic skills, especially in relation to working with structured and unstructured datasets\\nExcellent problem-solving skills with an ability to think outside the box\\nWorking SQL knowledge and experience working with relational databases\\nExperience with object-oriented/object-functional scripting languages: Python, Java, C++, Scala, etc.\\nClean coding habits, attention to detail and a focus on quality\\nExperience supporting and working with cross-functional teams in a dynamic environment\\nStrong oral and written communication, project management and organizational skills</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Company Overview\\nData Plus Math is a media measurement company that helps connect advertising exposures to real-world outcomes. Powered by millions of households of cross-screen viewing data, the company's TV and Video attribution platform is used by cable operators, national programming networks, agencies and marketers to measure which components of their advertising campaigns are driving results. We work with some of the largest media and entertainment companies, agencies and brands in the world to power the next generation of analytics and measurement for all of TV and Video.\\nResponsibilities:\\nIn this role, you will help create and optimize DPM's data pipeline architecture. You'll be an early and key member of a team building massively scalable systems to enable DPM to analyze petabytes of data. You will be a key partner of our data science, operations and product teams and deliver great support to those teams. If you are passionate about using (and sharpening) your skills to solve massively complex data problems, we'd like to hear from you.\\nMust have:\\nStrong analytic skills, especially in relation to working with structured and unstructured datasets\\nExcellent problem-solving skills with an ability to think outside the box\\nWorking SQL knowledge and experience working with relational databases\\nExperience with object-oriented/object-functional scripting languages: Python, Java, C++, Scala, etc.\\nClean coding habits, attention to detail and a focus on quality\\nExperience supporting and working with cross-functional teams in a dynamic environment\\nStrong oral and written communication, project management and organizational skills\\nNice to have:\\nAbility to lead or mentor other developers\\nGood understanding of database internals, query processing, query optimization and UML diagramming\\nWorking knowledge of message queuing, stream processing, and highly scalable data stores\\nExperience using object-relational mapping (ORM) frameworks\\nTraits\\nA desire to solve business problems with technology.\\nGreat communication skills, and the ability to influence stakeholders.\\nStrong interpersonal skills and exceptional character\\nInterest, willingness and demonstrated ability to quickly pick up new technology quickly\\nA self-starter who brings energy, passion, and creativity to work every day\\ntQQR1MfvJg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Quincy, MA 02169</td>\n",
       "      <td>Quincy</td>\n",
       "      <td>MA</td>\n",
       "      <td>02169</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>ABOUT PRESCRIBEWELLNESS\\nPrescribeWellnessâ€™s mission is to inspire collaboration among trusted pharmacists, healthcare institutions, and the communities they serve for better health across the country. We believe community pharmacists are poised to change the face of preventative healthcare. With an expanding roster of clinical services, the pharmacy is quickly becoming the most accessible wellness destination for patients between doctor visits. Pharmacies that are independent can have a greater focus on personal service and meaningful relationships, which supports lasting behavior change in their patients. We are passionate in our mission to inspire collaboration for better health, and are looking for energetic, passionate professionals to join our team.\\nEssential Duties and Responsibilities\\n\\nThe Data Scientist is responsible for assisting with modeling complex problems, discovering insights and identifying opportunities using statistical, algorithmic, mining and visualization techniques. Other duties include managing and building ETL processes and troubleshooting issues. Specific duties include the following:\\n\\nParticipate in use case feasibility discussions and translate business idea / business problems into analytics use case.\\no Explain insights from analyses and point out implications, risks, and usability constraints.\\no Provide support as needed to maintain and update models running in production environment.\\n\\nDevelop and maintain complex ETL processes and algorithms\\no Own and enhance ETL processes to load customer data into PrescribeWellnessâ€™s application\\no Monitor and troubleshoot processes on a daily basis\\no Document new and existing processes.\\n\\nBuild sophisticated predictive / prescriptive models to generate insights about customers products, sales, and operations for specific use cases.\\no Create and maintain the master data file and model predictive summary.\\no Support internal teams with data requests and BI Tool enhancements\\n\\nREQUIRED QUALIFICATIONS\\n\\nâ€¢ Get along with people and establish cooperative working relationships with all coworkers.\\nâ€¢ Effectively communicate information to and consult with others to complete work assignments.\\nâ€¢ Act in a responsible, trustworthy and ethical manner that considers the impact and consequences oneâ€™s actions or decisions.\\nâ€¢ Communicate ideas, thoughts, and facts in writing using proper grammar, spelling, document formatting and sentence structure.\\nâ€¢ Evaluate and analyze problems or tasks from multiple perspectives; adaptively employ problem solving methods to find creative or novel solutions; use logical, systematic and sequential processes to solve problems.\\nâ€¢ Complete assigned job tasks in an accurate and timely manner.\\nâ€¢ Carefully prepare for meetings and presentations; follow-up with others to ensure that agreements, tasks or commitments have been fulfilled.\\nâ€¢ Demonstrate commitment to achieving PrescribeWellnessâ€™ core business objectives of increasing the role of pharmacy and improving patient health in America.\\n\\n.\\nSKILLS, EDUCATION OR EXPERIENCE\\nâ€¢ Bachelorâ€™s degree in Business, Computer Science, Information Systems or related field.\\nâ€¢ 4+ years of data analysis experience.\\nâ€¢ Expertise in algorithm design, machine learning, and applied statistics.\\nâ€¢ Proven track record in use of SQL specifically in Oracle and working with data including: extracting information, validating data, creating and maintaining custom data structures.\\n\\nDESIRABLE QUALIFICATIONS\\nâ€¢ Passion for data and digging into the minutia of datasets.\\nâ€¢ Take calculated risks based on data-driven analytics.\\nâ€¢ Be a self-starter.\\nâ€¢ Enjoy working in a fast-paced environment.\\n\\nJUST A FEW OF OUR PERKS AND BENEFITS\\nÂ· A welcoming and inclusive environment\\nÂ· Professional development opportunities\\nÂ· Generous health, dental, and vision coverage\\nÂ· Competitive salary with employee stock option plan\\nÂ· 401K\\nÂ· Free bootcamp, meditation, stretch, yoga and Zumba classes on site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Senior AWS Big Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5+ years of AWS Solutions implementation, professional services experience, prefer Data Analytics space.\\nA passion for exploring data and extracting valuable insights.\\nProven analytical, problem solving, and troubleshooting expertise.\\nProficiency in SQL, preferably across a number of dialects (we commonly write MySQL, PostgreSQL, Redshift, SQL Server, and Oracle).\\nExposure to developer tools/workflow (e.g., git/github, *nix, SSH)Experience optimizing database/query performance.\\nExperience with AWS ecosystem (EC2, S3, RDS, Redshift).Experience with business intelligence tools with a physical model (e.g., MicroStrategy, Business Objects, Cognos).Experience with data warehousing.\\nExposure to NoSQL-based, SQL-like technologies (e.g., Hive, Pig, Spark SQL/Shark, Impala, BigQuery)\\nExcellent verbal and written communication skills\\nAbility to travel up to 70-80%</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nPossess In depth knowledge and hands on development experience in building Distributed Big Data Solutions including ingestion, caching, processing, consumption, logging &amp; monitoring) (Must Have)\\nStrong Development Experience in either one of the Distributed Big Data processing (bulk) engines preferably using Spark on EMR or related (Must Have)\\nStrong Development Experience on at least one or more event driven streaming platforms prefer Kinesis, Firehose, Kafka or related (Must Have)\\nStrong Data Orchestration experience using tools such has AWS Step Functions, Lambda, AWS Data Pipeline, Apache Airflow or related (Must Have)\\nAssess use cases for various teams within the client company and evaluate pros and cons and justify recommended tooling and component solution options using AWS native services, 3rd party and open source solutions (Must Have)\\nStrong experience on either one or more MPP Data Warehouse Platforms prefer AWS RedShift, PostgreSQL, Teradata or similar (Must Have)\\nStrong understanding and experience with Cloud Storage infrastructure and operationalizing AWS based storage services &amp; solutions prefer S3 or related (Must Have)\\nStrong technical communication skills and ability to engage a variety of business and technical audiences explaining features, metrics of Big Data technologies based on experience with previous solutions (Must Have)\\nStrong Data Cataloging experience preferably using AWS Glue (Nice to Have)\\nStrong Development Experience on at least one NoSQL OR Document databases (Nice to Have)\\nExperience on at least one or More Ingestion Integration tools Like Apache NIFI or Streamset or related (Nice to Have)\\nStrong Development Experience on at least one Caching Tools like Redis, Lucene, Memcached (Nice to Have)\\nStrong Understanding and experience in Big Data Audit Logging and Monitoring solutions (Nice to Have)\\nStrong Understanding of at least one or more Cluster Managers (Yarn, Hive, Pig, etc) (Nice to Have)Interface with client project sponsors to gather, assess and interpret client needs and requirements\\nAdvising on database performance, altering the ETL process, providing SQL transformations, discussing API integration, and deriving business and technical KPIs\\nDevelop a data model around stated use cases to capture client’s KPIs and data transformations\\nAssess, document and translate goals, objectives, problem statements, etc. to our offshore team and onshore management\\nDocument and communicate product feedback in order to improve user experience</td>\n",
       "      <td>Bachelor’s Degree in Computer Science or Equivalent\\nMinimum five years of Big Data Engineering on AWS experience</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Us\\n\\nWould you like to be part of a team focused on helping customers in a \"once in a generation\" shift to the cloud and AWS. NorthBay is a 300 person fast growing AWS Cloud-based Professional Services firm helping customers build solutions for data platforms and analytics, ML/Ai, DevOps, Database Migrations and custom application development and modernization. Do you have the business savvy and the technical background necessary to help grow NorthBay as a key technology provider to the Enterprise?\\n\\nSummary\\n\\nNorthBay is seeking technically savvy Senior Big Data Engineer to implement solutions for our customers working with our offshore engineering team. In this role, you will collaborate with NorthBay customers, some working onsite, understand requirements and needs, translate into specifications to develop solutions, drive work with offshore engineering teams, and deliver solutions and results to the customer. This includes assessing customer needs, re-engineering business intelligence processes, designing and developing data models, and sharing your expertise throughout the deployment process.\\nResponsibilities Include but Not Limited to:\\nPossess In depth knowledge and hands on development experience in building Distributed Big Data Solutions including ingestion, caching, processing, consumption, logging &amp; monitoring) (Must Have)\\nStrong Development Experience in either one of the Distributed Big Data processing (bulk) engines preferably using Spark on EMR or related (Must Have)\\nStrong Development Experience on at least one or more event driven streaming platforms prefer Kinesis, Firehose, Kafka or related (Must Have)\\nStrong Data Orchestration experience using tools such has AWS Step Functions, Lambda, AWS Data Pipeline, Apache Airflow or related (Must Have)\\nAssess use cases for various teams within the client company and evaluate pros and cons and justify recommended tooling and component solution options using AWS native services, 3rd party and open source solutions (Must Have)\\nStrong experience on either one or more MPP Data Warehouse Platforms prefer AWS RedShift, PostgreSQL, Teradata or similar (Must Have)\\nStrong understanding and experience with Cloud Storage infrastructure and operationalizing AWS based storage services &amp; solutions prefer S3 or related (Must Have)\\nStrong technical communication skills and ability to engage a variety of business and technical audiences explaining features, metrics of Big Data technologies based on experience with previous solutions (Must Have)\\nStrong Data Cataloging experience preferably using AWS Glue (Nice to Have)\\nStrong Development Experience on at least one NoSQL OR Document databases (Nice to Have)\\nExperience on at least one or More Ingestion Integration tools Like Apache NIFI or Streamset or related (Nice to Have)\\nStrong Development Experience on at least one Caching Tools like Redis, Lucene, Memcached (Nice to Have)\\nStrong Understanding and experience in Big Data Audit Logging and Monitoring solutions (Nice to Have)\\nStrong Understanding of at least one or more Cluster Managers (Yarn, Hive, Pig, etc) (Nice to Have)Interface with client project sponsors to gather, assess and interpret client needs and requirements\\nAdvising on database performance, altering the ETL process, providing SQL transformations, discussing API integration, and deriving business and technical KPIs\\nDevelop a data model around stated use cases to capture client’s KPIs and data transformations\\nAssess, document and translate goals, objectives, problem statements, etc. to our offshore team and onshore management\\nDocument and communicate product feedback in order to improve user experience\\nQualifications:\\n5+ years of AWS Solutions implementation, professional services experience, prefer Data Analytics space.\\nA passion for exploring data and extracting valuable insights.\\nProven analytical, problem solving, and troubleshooting expertise.\\nProficiency in SQL, preferably across a number of dialects (we commonly write MySQL, PostgreSQL, Redshift, SQL Server, and Oracle).\\nExposure to developer tools/workflow (e.g., git/github, *nix, SSH)Experience optimizing database/query performance.\\nExperience with AWS ecosystem (EC2, S3, RDS, Redshift).Experience with business intelligence tools with a physical model (e.g., MicroStrategy, Business Objects, Cognos).Experience with data warehousing.\\nExposure to NoSQL-based, SQL-like technologies (e.g., Hive, Pig, Spark SQL/Shark, Impala, BigQuery)\\nExcellent verbal and written communication skills\\nAbility to travel up to 70-80%\\nEducation and Experience:\\nBachelor’s Degree in Computer Science or Equivalent\\nMinimum five years of Big Data Engineering on AWS experience\\nThis position can be located in Boston or Virginia or NC or Columbus or Cincinnati, OH or Detroit or NY or NJ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Azure Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Role Overview:\\nAs an Azure Data Engineer you will collect, aggregate, store, and reconcile data in support of Client business decisions. You will help design and build data pipelines, data streams, reporting tools, information dashboards, data service APIs, data generators and other end-user information portals and insight tools. You will be a critical part of the data supply chain, ensuring that business partners can access and manipulate data for routine and ad hoc analysis to drive business outcomes using Advanced Analytics.\\n\\nKey Role Responsibilities:Day-to-day, you will:\\nTranslate business requirements to technical solutions using strong business insight.\\nAnalyzes current business practices, processes, and procedures as well as identifying future business opportunities for demonstrating Microsoft Azure Data &amp; Analytics PaaS Services.\\nSupport the planning and implementation of data design services, providing sizing and configuration assistance and performing needs assessments.\\nDelivery of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies.\\nDesign and Build Modern Data Pipelines and Data Streams.\\nDesign and Build Data Service APIs.\\nDevelop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics.\\nExpose data to end-users using Power BI, Azure API Apps or other modern visualization platform or experience.\\nImplement effective metrics and monitoring processes.\\nAble to travel approximately 80%\\n\\nYour technical/non-technical skills include:\\nDemonstrable experience of turning business use cases and requirements to technical solutions.\\nExperience in business processing mapping of data and analytics solutions.\\nAbility to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.\\nThe ability to apply such methods to take on business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration.\\nT-SQL is required.\\nKnowledge of Azure Data Factory, Azure Data Lake, Azure SQL DW, and Azure SQL, Azure App Service is required.\\nKnowledge of Lambda and Kappa architecture patterns.\\nKnowledge of Master Data Management (MDM) and Data Quality tools and processes.\\nStrong collaboration ethic and experience working with remote teams.\\n\\nNice to Haves:\\nAzure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics.\\nKnowledge of Python.\\nDemonstrated experience preparing data and building data pipelines for AI Use Cases (text, voice, image, etc.…)\\nDesigning and building Data Pipelines using streams of IOT data.\\nExperience with Git/TFS/VSTS is a requirement.\\nKnowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals.\\nWorking experience with Visual Studio, PowerShell Scripting, and ARM templates.\\n\\nPreferred Certifications:\\nMCSA\\n\\nPreferred Education Background:\\nYou likely possess a bachelor’s degree in Computer Science, Information Technology, Business, or another relevant field. An equivalent combination of education and experience will also suffice.\\n\\nPreferred Years of Work Experience:\\nYou likely have about 2-4+ years of relevant professional experience.\\n\\nAbout Avanade\\nAvanade leads in providing creative digital services, business solutions and design-led experiences for its clients, delivered through the power of people and the Microsoft ecosystem. Our professionals combine technology, business, and industry expertise to build and deploy solutions to realize results for clients and their customers. Avanade has 27,000 digitally connected people across 23 countries, bringing clients the best thinking through a collaborative culture that honors diversity and reflects the communities in which we operate. Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com.\\n\\nWhy Avanade?\\n14-time winner of Microsoft Partner of the Year\\n24,000+ certifications in Microsoft technology\\n90+ Microsoft partner awards\\n17 Gold Competencies\\n3,500 analytics professionals worldwide\\n1,000 data engineers\\nImplemented analytics systems for more than 550 clients\\n400 AI practitioners\\n300 cognitive service experts\\nWe champion creativity and free-thinking, embrace diversity, and believe innovation is powered by people working together to change things for the better.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>Cambridge, MA 02138</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>MA</td>\n",
       "      <td>02138</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Meet CarGurus—the #1 visited online car shopping website in the US. At CarGurus, we're building the world's most trusted and transparent automotive marketplace where it's easy to find great deals from top-rated dealers.\\n\\nFounded in 2006 by Langley Steinert (co-founder of TripAdvisor), CarGurus is a technology company with a passion for data and its power to simplify every aspect of the car shopping experience. Using proprietary technology, search algorithms and innovative data analytics, we provide unbiased validation on pricing, dealer reputation and vehicle history.\\n\\nWe're looking for a thoughtful, technical and deeply collaborative Machine Learning Engineer to work with our growing Analytics Engineering team! This position will provide foundational framework for the Data Science team. This includes building the infrastructure to allow the team to \"publish\" models into production, developing Python libraries to facilitate feature generation, and providing the data science team with product-based datasets to train their models.\\n\\nWhat you'll do:\\n\\nCollaborate with Data Scientists, Emerging Product Engineers, and Analysts\\nBe the interface between the data warehouse and the data science team, using transformation frameworks to automate dataset generation\\nHelp Data Scientists optimize, monitor, and deploy their models\\nBuild, maintain, and upgrade a platform for running machine learning models\\nDevelop APIs to serve model predictions in real time and at scale\\nLearn the business inside and out, interfacing with domain knowledge experts\\nParticipate in creating the team's roadmap, providing feedback on priority and business value.\\n\\nWho You Are:\\n\\n3+ years experience as a data engineer, machine learning engineer, or a software engineer with a passion for data.\\nTeam player who thrives in a collaborative environment.\\nSelf-starter, willing to jump in and learn something new to execute on a task.\\nExperienced in SQL, with ability to optimize database and query performance.\\nA natural detective, with a keen interest in solving business problems with data driven methods.\\nDeeply focused on delivering value to stakeholders, with a data-as-a-product mindset.\\nPassionate about creating production grade systems and data quality, supporting what you build.\\nFamiliarity with cloud computing and architectures.\\nExposure and experience with machine learning is a plus.\\n\\nTechnologies we use:\\nAirflow, AWS, Docker, Snowflake, Python, Spark, Kafka, Prometheus, Snowplow\\n\\nCarGurus Culture:\\nAt the core of our company culture is a spirit of innovation, curiosity and collaboration. True to our start-up roots, we're nimble, flexible and hardworking. We have a great respect for testing and learning and a healthy aversion to scheduling meetings to discuss meetings. Lunch is catered daily. Gym membership is free. Foosball and ping pong are played often. Now a publicly-traded company, we're as committed as ever to cultivating the culture that got us here.\\n\\nIn addition to the US, CarGurus operates sites in Canada, the UK and Germany with other markets on the horizon. Our offices are located in Cambridge, MA, Detroit, MI and Dublin, Ireland. If you'd like to learn more, please visit our careers page ( https://careers.cargurus.com/ ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Framingham, MA 01701</td>\n",
       "      <td>Framingham</td>\n",
       "      <td>MA</td>\n",
       "      <td>01701</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Framingham, MA / Providence, RI / Minneapolis, MN\\n\\nWho is Virgin Pulse?\\nVirgin Pulse, founded as part of Sir Richard Branson’s famed Virgin Group, helps organizations build employee health and wellbeing into the DNA of their corporate cultures. As the only company to deliver a powerful, mobile-first digital platform infused with live services, including coaching and biometric screenings, Virgin Pulse’s takes a high-tech-meets-high-touch-approach to engage employees in improving across all aspects of their health and wellbeing, every day – from prevention and building a healthy lifestyle to condition and disease management to condition reversal, all while engaging users daily in building and sustaining healthy habits and behaviors. A global leader in health and wellbeing, Virgin Pulse is committed to helping change lives and businesses around the world for good so that people and organizations can thrive, together. Today, more than 3100 organizations across the globe are using Virgin Pulse’s solutions to improve health, employee wellbeing and engagement, reduce costs and create strong workplace cultures.\\n\\nWho are our employees?\\nAt Virgin Pulse we’re passionate about changing lives for good. We want to make a difference in the world by helping people be healthy so they can perform at their best, every day, at work and home. Our award-winning solutions support leading employers in improving and simplifying the employee health and wellbeing journey and engaging people in all aspects of their health. But our world-class products and programs are nothing without our people – the employees who design, build, promote, sell, test and perfect the latest innovations in workplace health and wellbeing. Our people are our top priority and we invest in their health and happiness. At Virgin Pulse, we have so much more than a strong, supportive company culture – have a shared vision for a healthier, happier world.\\nWho you are.\\nAs a Data Engineer with Virgin Pulse you are capable of working on a fast-paced reporting pipeline. As a KEY member of the Data Systems &amp; Reporting team, you will work closely with our client services and ETL team to clean data, design Tableau workbooks, automate SQL scripts, and respond to daily reporting requests.\\nYou are passionate about top quality data software and knowledgeable about business insights, capable of sharing this knowledge with internal clients and customers. You understand SQL deeply and have extensive experience with data analytics using industry tools such as Tableau or Power BI. You also take pride in having an agile mindset and a deep commitment to the principles of agile software development. You should understand modern non-relational databases, and how they are different from traditional databases and take pride in your excellent verbal and written communication skills and desire for a strong team ethic.\\n\\nIn the role of Data Engineer for Virgin Pulse you will wear many hats but attention will be crucial in the following:\\nDemonstrate experience in Tableau or Power BI\\nDemonstrate capability in SQL\\nUsing Python to create clean, extensible code\\nDesigning and implementing data warehouse schemas for big data\\nEat, breathe, and think agile for all\\nYou embrace the full lifecycle of product development from inception to continual support\\n\\nYou are passionate about quality. Ensuring accuracy and elegance in your work is something you do every day. You think carefully about anything before writing code, you code with fastidious attention to detail, you test obsessively, and you take criticism enthusiastically and improve continually.\\n\\nWhat you bring to the team.\\nIn order to represent the best of what we have to offer you come to us with a multitude of positive attributes including:\\nYou have a degree in Computer Science, MiS or Mathematics\\nYou have outstanding problem solving and analytical skills\\nYou are an amazing communicator (written and verbal) and comfortable discussing software and features\\nYour ability to contribute to a planning or scheduling discussion mirrors your ability to listen and learn from your peers\\n\\nWhat makes you stand out.\\n3 years of a major BI tool such as Tableau or PowerBI\\n3 years of SQL experience\\n3 years of Python experience\\nExperience with one or more non-relational database (Vertica, MongoDB, Cassandra, etc)\\nExperience with Jira and Git\\nSecurity Competencies:\\nWork to ensure system and data security is maintained at a high standard, ensuring the confidentiality, integrity and availability of the Virgin Pulse application is not compromised. Ensure industry best practice coding standards are adhered to in particular ensure all code developed at Virgin Pulse is free from bugs and security vulnerabilities, such as those defined and published by OWASP.\\n\\nWhy work here?\\nWe believe a career should provide a collaborative and supportive work environment, strong employee culture and cutting-edge technology and services — so many reasons to love it here.We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to any protected class status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Wellesley, MA</td>\n",
       "      <td>Wellesley</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description:\\nThis Sr. Data Engineer role will be working with a group of Data Scientists and Data Engineers developing a Next Best Action recommendation system. They will be responsible for creating and managing data pipelines that support core system functionality and reporting. They will work within the team’s Agile processes to create a high-quality systems and processes, and they will collaborate with other teams to achieve the best possible results. 63450\\n\\nFundamental Components:\\nDevelops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.\\nWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.\\nCollaborates with data science team to transform data and integrate algorithms and models into automated processes.\\nUses knowledge in Hadoop architecture, HDFS commands and experience designing &amp; optimizing queries to build data pipelines.\\nUses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems.\\nBuilds data marts and data models to support Data Science and other internal customers.\\nIntegrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.\\nAnalyzes current information technology environments to identify and assess critical capabilities and recommend solutions.\\nExperiments with available tools and advises on new tools in order to determine optimal solution given the requirements dictated by the model/use case.\\n\\nBackground Experience:\\n7+ years writing SQL queries, understanding logic of existing queries, and developing relational databases.\\n5+ year with languages used for data engineering (Java, R, Python, H2O, etc.)\\n5+ years with Linux via CLI and Shell Scripting\\n3+ years of experience data engineering on Hadoop (HDFS, Hive, Pig, HBase, etc.)\\n#LI-DT1\\n\\nPotential Telework Position:\\nNo\\n\\nPercent of Travel Required:\\n0 - 10%\\n\\nEEO Statement:\\nAetna is an Equal Opportunity, Affirmative Action Employer\\n\\nBenefit Eligibility:\\nBenefit eligibility may vary by position. Click here to review the benefits associated with this position.\\n\\nCandidate Privacy Information:\\nAetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Data Engineer (Reporting, Analytics, NoSQL Databases)</td>\n",
       "      <td>Lexington, MA</td>\n",
       "      <td>Lexington</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>High competency in Java or similar JVM language.</td>\n",
       "      <td>High competency in Java or similar JVM language.</td>\n",
       "      <td>Strong development experience working with large scale data.\\nDeep knowledge of data structures and algorithms.\\nBuilding high-performance, massively-scalable, always-available Cloud-based systems.\\nSupport and troubleshoot systems occasionally outside of regular office hours as needed.\\nPlan and negotiate with peers to meet deadlines.\\nDeveloping, deploying and managing software across the full Continuous Delivery life-cycle.\\nManage your own time and priorities, without the need for micro-management.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview\\nThe Analytics and Reporting team at Mimecast looking for a data engineer to help develop frameworks and technology for the analytics platform powering insights into threat intelligence and customer experiences on our service. We are looking for someone who shares a passion for turning billions of events into valuable intelligence and reporting data. The ideal candidate will share a passion for tackling complexity with simplicity.\\nResponsibilities\\nStrong development experience working with large scale data.\\nDeep knowledge of data structures and algorithms.\\nBuilding high-performance, massively-scalable, always-available Cloud-based systems.\\nSupport and troubleshoot systems occasionally outside of regular office hours as needed.\\nPlan and negotiate with peers to meet deadlines.\\nDeveloping, deploying and managing software across the full Continuous Delivery life-cycle.\\nManage your own time and priorities, without the need for micro-management.\\nQualifications\\nEssential Skills and Experience\\nHigh competency in Java or similar JVM language.\\nExperience and great understanding of modern scalable data processing and storage technologies.\\nExperience working with Cassandra, Kafka, Spark or equivalent NoSQL databases.\\nExpert knowledge developing and debugging distributed applications under Linux environment.\\nStrong software engineering best practices.\\nSolid experience with concurrency, multithreading, server architectures, and distributed systems.\\nDevOps mindset.\\n\\nAs the ideal candidate, you would have knowledge in most of the following\\nKnowledge of container (Docker/Kubernetes) technologies (a plus).\\nKnowledge and/or Experience with MapReduce Framework, Hadoop, HDFS (a plus).\\nExperience with modern UI frameworks such as Angular or React for creating data visualizations.\\n\\nReward:\\n\\nWe offer a highly competitive rewards and benefits package including private healthcare, dental and life coverage. Mimecast is an entrepreneurial and high growth company which will provide the right candidate with a wealth of career development opportunities. All Mimecasters strive on being high performers, problem solvers, and team players with passion and integrity.\\n\\nAn Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Data Engineer (BI)</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Simply Business is more than our name. It's how we approach insurance: Make it clear. Make it simple. Make it affordable. By combining exceptional talent, technology, data, and knowledge, Simply Business is the go-to online insurance brokerage that protects small businesses and the entrepreneurs who work hard to build them.\\n\\nWe want team members who have the drive to challenge boundaries. If you’re smart and passionate about delivering brilliant customer experiences, we’d love to hear from you.\\n\\nThe Simply Business Data and Analytics team is looking for a Data Engineer. The Data Engineer will help design, build and maintain the systems that create and provide actionable information to help executives and managers make informed business decisions. This includes:\\nData warehouses and data lakesBusiness intelligence and analytics platformTools and solutions for ingesting, transforming and consuming data\\nAs a Data Engineer, you will:\\nBe hands-on in developing our products using best practices, appropriate tools and technologies.\\nHighlight areas for continuous improvement and drive their prioritisation.\\nBe proactive in collaborating and communicating with your colleagues near you and across the ocean.\\nChampion data thinking whilst creating amazingly useful systems in a collaborative way for our customers.\\nWhat we are looking for:\\nExperience building and maintaining data warehouses and developing ETL pipelines.\\nIntermediate or advanced fluency in SQL and experience with data modeling.\\nCoding experience is highly valued, especially for API integrations. In the Data and Analytics team we use Ruby, Scala and Python but you don’t need experience in all of them. If you're happy to learn them we will support you with that.\\nStrong interpersonal skills to work well in our very open and friendly environment.\\nWhat are the benefits?\\n\\nHere are some of the great benefits and perks that come from being a Simply Business employee:A salary that reflects your experience, our pay policy, and the market we’re in from your first dayGroup plan for medical, dental, and prescription drug coverageShort term disability, long term disability, and life insurance coverageParticipation in the Company’s bonus programParticipation in 401(k) plan with a 3% employer matchCommuter benefits to help cut down on commuting costs25 days of vacation time plus 10 company holidaysFlexible working hours and working from homeAnnual company trip, regular outings, and volunteer opportunitiesAn awesome WeWork office with cold brew coffee, beer on tap, local pop-up events, and more\\n\\nAs a company, we pride ourselves on inclusion in the workplace. Simply Business is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\\n\\nWant more info on working at Simply Business? Check out our careers page: https://www.simplybusiness.com/careers/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Senior Software Engineer - Data Engineering</td>\n",
       "      <td>Needham, MA 02494</td>\n",
       "      <td>Needham</td>\n",
       "      <td>MA</td>\n",
       "      <td>02494</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>TripAdvisor, the world’s largest travel site, operates at scale with over 760 million reviews, opinions, photos, and videos reaching over 490 million unique visitors each month. We are a data driven company, and we have lots and lots of data!\\n\\nThe CoreX Business unit is focused on the traveler’s experience and journey. We are using our data to help people have amazing and safe trips as they travel the world. The CoreX data engineering team are the experts in our data landscape at TripAdvisor, building out and maintaining truths to enable CoreX to empower travelers across the world finding the best hotels, the best restaurants, and the best experiences they can have!\\n\\nWe are looking for an experienced, hands-on data engineer to help us leverage the massive amount of data that we collect so that we can better understand how to guide each traveler to those experiences that are right for him/her. Our in-house cluster is over 10 PB and growing fast, in addition to a lot of data on AWS, Google, etc.\\n\\nWe need to build data marts. We need to build traffic models. We need to build business models. We need to analyze the way travelers use our site and apps. We need to leverage the significant internal resources that do data mining and machine learning to build recommenders. We need to analyze data and make sure it’s correct and clean. We need to get this data into the hands of the rest of the business.\\n\\nThis is a hands-on job for someone who wants to solve important business problems that depend on “big data” analysis. The job requires both serious technical chops and effective communication skills. Sometimes you will build the solution yourself, and sometimes you will be coordinating the efforts of others, but at all times you will be expected to think creatively about solving the business problem.\\n\\nWhat you will bring to the team:\\n\\nIn-depth technical experience with data technologies such as Hadoop (HDFS, Hive, Map/Reduce, EMR), Spark, Snowflake, Presto, Kafka / Samza, BigQuery, etc.\\nSolid RDBMS operational knowledge with outstanding SQL skills\\nAbility to transform raw, noisy log level data into useful business fact tables\\nSolid database design skills\\nETL expertise\\nComputer Science expertise – algorithms, data structures, software engineering\\nGeneral software and programming experience; a lot of our codebase is in Java , but we have lots of Python and Kotlin as well. The ability to navigate this codebase is critical.\\nAbility to understand and communicate business needs\\nAbility to come up to speed quickly in order to understand and coordinate the work of domain experts – in particular the ability to work effectively with product engineering and data analysts\\nStrong sense of responsibility: taking pride in your work, leveraging others, owning the problem\\n\\n\\nAnd you love, and we mean love, data!\\n\\n\\n#LI-RF1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Tolemi team is looking for Data Engineers to help our local government customers unlock the power of their data. You’ll play a pivotal role in bringing together disparate datasets from across government departments, then standardizing them into our map-based business intelligence application. You’ll work closely with our Customer Success &amp; Engineering teams to streamline implementation, to grow our database of municipal property information, and to adapt our data model to new use cases. If you’re looking to join a dynamic team with plenty of room to advance in a fast-growing Company, Tolemi is right for you.\\nWhat you’ll do:\\n\\nWrite &amp; optimize SQL queries to transform raw, disparate datasets into our standard\\n\\nProcess data from a range of formats, including CSV, JSON, XML, and Shapefile\\n\\nWrite basic scripts for data manipulation, data fetching, web scraping, and simple automation\\n\\nDrive continuous improvement of our processes and internal toolss\\n\\nWhat we’re looking for:\\n\\nWorking knowledge of SQL, including:\\nAbility to write and optimize complex queries\\nJoins, aggregation, indexes, and transactions\\nAbility to write and automate basic scripts for data manipulation and retrieval including CSV, JSON, XML data formats\\nGeneral understanding of APIs, command line, client server architecture, networks, and (S)FTP\\nExtra points for experience with GIS, including shapefiles, PostGIS, and spatial indexes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Senior Manager of Data Science – Cyber Security</td>\n",
       "      <td>Boston, MA 02298</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02298</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n7+ years real-world data science experience in a large organization or supporting a large organization\\n3+ years of experience managing analytic teams that deliver product\\nBackground experience with data engineering with an affinity for developing code from the command line\\nFoundational knowledge in information technology, including hardware, networking, architecture, protocols, file systems and operating systems\\n2+ years of experience implementing products within Big Data solutions including Hadoop, Spark, and Kafka\\n1-2 years utilizing Big Data resource and job managers including Oozie and YARN\\nRelevant bachelor’s degree, or equivalent experiences\\nFamiliarity with the cyber security domain</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Product ownership of several data science production-grade products</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Senior Manager of Data Science – Cyber Security role is critical to the Data Analytics and Security Innovation Team within the Optum Cyber Defense organization. The data scientist leads teams that build innovative solutions to facilitate UHG’s cybersecurity situational awareness, analytics, and investigations. Cyber security data is collected in structured, unstructured, and semi-structured formats, at varying velocities, and in a complex, dynamic security environment. This creates unique data science challenges in support analysts, and investigators. The ideal candidate has experience leading teams that deliver innovative product while navigating development, analytic, and engineering challenges within a large organization.\\n\\nPrimary Responsibilities:\\n Product ownership of several data science production-grade products\\n Working with internal Cyber Defense customers to create new innovative uses for data science within cyber security\\n Growing data scientist and data engineer team members into leaders\\n Developing data science models within an operational CICD pipeline\\n Developing solutions for technical users to access data from the Data Lake through a variety of means, including APIs, flat file extracts, or web applications\\n Remaining up to date and on the cutting edge of Big Data technology\\n Constantly learning and adapting the team to new techniques, languages, and technologies\\n Understanding emerging technology in the data space and security threats against it\\n\\nYou’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.\\n\\nRequired Qualifications:\\n7+ years real-world data science experience in a large organization or supporting a large organization\\n3+ years of experience managing analytic teams that deliver product\\nBackground experience with data engineering with an affinity for developing code from the command line\\nFoundational knowledge in information technology, including hardware, networking, architecture, protocols, file systems and operating systems\\n2+ years of experience implementing products within Big Data solutions including Hadoop, Spark, and Kafka\\n1-2 years utilizing Big Data resource and job managers including Oozie and YARN\\nRelevant bachelor’s degree, or equivalent experiences\\nFamiliarity with the cyber security domain\\nPreferred Qualifications:\\nPetabyte-scale experience with Big Data and data science\\nTechnology Careers with Optum. Information and technology have amazing power to transform the health care industry and improve people's lives. This is where it's happening. This is where you'll help solve the problems that have never been solved. We're freeing information so it can be used safely and securely wherever it's needed. We're creating the very best ideas that can most easily be put into action to help our clients improve the quality of care and lower costs for millions. This is where the best and the brightest work together to make positive change a reality. This is the place to do your life's best work.(sm)\\n\\nDiversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.\\n\\nUnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Who we are\\n\\nDrizly is the world’s largest alcohol marketplace and the best way to shop beer, wine and spirits. Our customers trust us to be part of their lives – their celebrations, parties, dinners and quiet nights at home. We are there when it matters - committed to life’s moments and the people who create them. We partner with the best retail stores in over 100 cities across North America to serve up the best buying experience. Drizly offers a huge selection and competitive pricing with a side of personalized content. That is what we do. Who we are is a different story.\\n\\nWe are more than just another tech company. There is an intellectual curiosity that occurs at Drizly. We have a desire to question, to understand, to figure it out. Bottom line, we solve it. We value not just the truth but the process to get to the truth, to deliberate, decide and then act. Most importantly, we care. We care about our customer. We care about our company. We care about our team. There will be long days and incredible challenges.\\n\\nWe are blazing a trail in an industry that hasn’t changed in nearly a century, and that doesn’t scare us (well, not all the time) -and even when it does, it doesn’t stop us, it energizes us.\\nDo you see yourself here? Read on.\\n\\n\\nWho you are\\n\\nYou get excited by the evolving landscape of big data solutions and setting up the infrastructure to enable other teams to harness them, and you have a track record of doing just that. Namely, you’ve stood up real-time systems at least a couple of times. When thinking of problems at-scale, you run full-speed toward challenges. You love working with teams like Data Science and Business Intelligence to see how fast you can transform data into something hugely impactful for business decisions.\\n\\nYou thrive in a hyper fast paced environment that focuses on real-world, customer centric problems with iterative technical solutions. Your toolkit includes having deep experience standing up infrastructure systems, highly advanced SQL use, Kafka, and/or Kinesis.\\n\\nWe’d be really impressed if you’ve had experience in eCommerce environments with massive amounts of clickstream and eventstream data, we’re talking close to a billion events.\\n\\n\\nWhat the role is\\n\\nThe Senior Data Engineer will accelerate Drizly’s data driven decision making through a period of rapid growth. From building a data processing framework to help support 1B+ events to integrating new data pipelines, Drizly’s most critical decisions will rely on a scalable, well-managed foundation of data.\\n\\nThe Senior Data Engineer is a critical member of the Engineering organization that works closely with Data Science and Business Intelligence, as well as product focused engineering teams. In short, the Senior Data Engineer will help define the building blocks for Drizly’s data capabilities.\\n\\nIn this role you will:\\nLead Drizly’s data solutions through our next phase of growth\\nBuild processes that unlock data potential for various parts of the business\\nDefine new strategies that will enable our Data Science and Business Intelligence teams to incorporate new data sources\\nServe as a subject matter expert when it comes to the way our data warehousing strategy\\nThe other stuff\\nCompetitive salary\\nOne-on-one professional coaching with external expert\\nHealth, Dental and Vision Insurance\\nFlexible vacation policy\\nCommuter benefits 401(K) Plan\\nAdded perks\\nYou do you.\\nDrizly is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Azure Data Architect</td>\n",
       "      <td>Boston, MA 02199</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02199</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of consulting or client service delivery experience on Azure\\n</td>\n",
       "      <td>DevOps on an Azure platform</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\n</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Azure Technical Architect is a highly performant Azure Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data solutions on cloud. Using Azure public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today's corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:Work with Sales and Bus Dev teams in providing Azure Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS &amp; NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nQualifications\\nBasic Qualifications\\nAt least 5 years of consulting or client service delivery experience on Azure\\nAt least 10 years of experience in big data, database and data warehouse architecture and delivery\\nMinimum of 5 years of professional experience in 2 of the following areas:\\n§ Solution/technical architecture in the cloud\\n§ Big Data/analytics/information analysis/database management in the cloud\\n§ IoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.\\n Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.\\n - Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nMCSA Cloud Platform (Azure) Training &amp; Certification\\nMCSE Cloud Platform &amp; Infratsructiure Training &amp; Certification\\nMCSD Azure Solutions Architect Training &amp; Certification\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nStrong in Power BI, Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience a plus - Azure, AWS, Google\\n\\nProfessional Skill Requirements\\n Proven ability to build, manage and foster a team-oriented environment\\n Proven ability to work creatively and analytically in a problem-solving environment\\n Desire to work in an information systems environment\\n Excellent communication (written and oral) and interpersonal skills\\n Excellent leadership and management skills\\n Excellent organizational, multi-tasking, and time-management skills\\n Proven ability to work independently\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Data Engineer – Personalization</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Spotify is looking for a Data Engineer to join us in our Personalization organisation. The Personalization team’s mission is to match listeners, music and audio in a personal and meaningful way to create great listening experiences. We are constantly working together using state of the art machine learning and applied research to create the best personalized soundtracks that connect people to music and audio.\\n\\nYou will clean and connect logs across a broad range of services – interaction logs, user behaviors, contextual streaming data, etc. to enable the creation of training data and analytics around the performance of our machine learning recommender systems. You will create and monitor pipelines generating metrics and useful datasets that power internal dashboards, providing insights into the Personalization mission.\\n\\nYou will build data driven solutions to bring music and digital media experiences to hundreds of millions of active users and millions of creators by matching fans with creators in a personal and relevant way. Above all, your work will impact the way the world experiences art.\\n\\nWhat you’ll do:\\n\\nWork with state-of-the-art data processing frameworks, technologies, and platforms\\nDesign, build and maintain data pipelines and ad-hoc solutions to gather relevant data across the sprawling data infrastructure of Spotify\\nWork from our office in Boston (USA)\\nImprove data quality through testing, tooling and continuously evaluating performance\\nWho you are:\\n\\nYou know the Scala and Python languages\\nYou have experience in developing/building data pipelines (batch or streaming)\\nYou are interested in being the glue between engineering and analysis, and have an eye for data accuracy and integrity\\nYou have experience with one or more higher-level JVM-based data processing frameworks such as Beam, Dataflow, Crunch, Scalding, Storm, Spark, or something we didn’t list- but not just Pig/Hive/BigQuery/other SQL-like abstractions\\nYou are knowledgeable about data modeling, data access, and data storage techniques\\nTools and infrastructure such as GCP (google cloud platform) is a plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Waltham, MA</td>\n",
       "      <td>Waltham</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Driven by Curiosity.\\n\\nAt Vistaprint, we believe that everything has a lasting impact on our customers and on each other. It begins and ends with a passion for helping our customers succeed. To give them our best, we empower our team with the autonomy they need to make smarter decisions and pursue higher value. We thrive on exploration, collaboration, and helping every customer grow their business beyond imagination. Fueled by technology and innovation, we are so much more than business cards.\\n\\nAbout Our Team:\\n\\nThe Data Engineering Team's mission is to enable our internal business partners with the information to make better decisions for our customers. All of our team members collaboratively work together to design, build, and sustain our Data Ecosystem, which includes technologies like our Data Warehouse, Data Lake, and proprietary predictive analytics tools. We build the right thing, the right way through positive relationships with Marketing, Analytics, and Finance.\\n\\nWhen you join our team, you will have a fantastic opportunity to advance your career by working in cutting-edge technology and growing your business expertise.\\n\\nWhat You Will Do:\\n\\nAs a Lead Data Engineer, you will help us scale our Data infrastructure to expand our proprietary predictive analytics and Big Data capabilities. You will work with our vast Data Ecosystem, including our Data Warehouse and Data Lake, to build out our platform and provide tooling to empower our internal customers. Your technical experience and business insight will directly drive real business results. Your ability to seek complex challenges and collaborate in a team environment will mean you have an immediate impact.\\n\\nEssential Functions Include:\\n\\nDesign, develop, and support our Data infrastructure utilizing various technologies to process terabytes of data, including SQL, Python, Microsoft Azure, and AWS.\\nCreate solutions to enable diagnostic and predictive analytics capabilities.\\nPartner with the Analytics, Marketing, and Finance organizations to get feedback and iterate upon the Data Ecosystem development.\\nDevelop components and distributed ETL systems for our suite of large data platforms\\nBe curious about trends and emerging technologies in the Data space, participate in user communities, and share what you learn with your teammates\\n\\nYou Have:\\n\\nFamiliarity with developing data processing solutions and data applications using technologies like Python, C#, Java, SQL, Spark, or No SQL DB\\nExperience working with all kinds of data- clean, dirty, unstructured, semi-structured and relational\\nProblem solving and multi-tasking in a fast-paced, globally distributed environment\\nStrong communication skills, good interpersonal skills\\nCollaborate with business partners to understand and refine requirements\\nExperience with developing end-to-end data pipelines in large cloud-compute infrastructure solutions such as Azure, AWS or Google is a plus\\nFive years working directly on Big Data technologies preferred\\n\\nWhat's in it for You:\\n\\nCompetitive salary + comprehensive benefits\\nFlexible working hours with paid time off\\nContinuous development opportunities such as onsite training and conferences\\nOnsite game room (foosball, ping pong, Xbox, pinball)\\nFree food, drinks, and fresh organic fruit\\nAnd more!\\n\\nEqual Opportunity Employer\\n\\nVistaprint, a Cimpress company, is an Equal Employment Opportunity Employer. All qualified candidates will receive consideration for employment without regard to race, color, sex, national or ethnic origin, nationality, age, religion, citizenship, disability, medical condition, sexual orientation, gender identity, gender presentation, legal or preferred name, marital status, pregnancy, family structure, veteran status or any other basis protected by human rights laws or regulations. This list is not exhaustive and, in fact, in many cases, we strive to do more than the law requires.\\n\\n#LI-ES1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Lead/Senior Oracle Data Engineer - Citizen/GC Preferred</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Asset Management Technology (AMT) is looking for an Oracle Data Engineer focused on our data tier to support as we design, build and advance our Managed Accounts capability. We are looking for individuals who continually strive to advance engineering excellence, agile delivery, and technology innovation in a highly collaborative environment.\\n\\nThe Expertise We’re Looking For\\n\\nExpertise with Oracle 12c, data movement technologies, and proficiency in designing solutions\\nExcellent Design &amp; Analysis skills with a demonstrated ability to align to long term strategies through interim states\\nExperience working on delivering solutions for globally distributed, large scale Agile software development teams\\nThe Purpose of Your Role\\n\\nYou will serve as an engineer and thought leader within our Engineering team. You will interface directly with business partners to understand problems &amp; opportunities and recommend solutions. You will be a key player as we scale our platform to support forecasted growth.\\n\\nThe Skills You Bring\\n\\nYou are a technical thought leader that leads and influences solutions\\nYou consistently drive strong collaboration, open communication, and reach across functional borders\\nYou are able to understand business problems and able to design scalable solutions\\nYou have a relentless commitment to quality and engineering excellence\\nYou have an ability to design and build performant solutions that scale and support stringent SLAs\\nYou are able to work successfully in an environment that encourages autonomy in the work that you do\\nYou are motivated, an excellent communicator, can take initiative to solve problems, and can make decisions based on the value of the solutions we build.\\nYou understand engineering best practices and have an aptitude to coach and mentor others on the team\\nThe Value You Deliver\\n\\nAs a data engineer, you will play a key role in shaping how our products are designed and developed\\nBring creativity and innovation, and experiment where needed, to provide solutions that help us deliver for the business.\\nPosition the organization for growing the Managed Accounts business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>System Test Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n2-3 years experience in a field related to data analysis, visualization or geographic information systems (GIS)\\nUndergraduate degree in a field related to analytics --- e.g., computer science, applied math, engineering\\nExperience working with databases, ETL frameworks, and creating dashboards\\nFluency in Python is required; experience in Plotly, Bokeh, Tableau, Jupyter, and/or D3 is desired\\nKnowledge of data mining and processing using SQL\\nExperience with Robotics or motion control systems is a plus\\nIdentify, analyze, and interpret trends or patterns in complex data sets using statistical techniques and regression methods\\nAbility to display complex quantitative data in a simple, intuitive format and to present findings in a clear and concise manner\\nStrong communication skills including effectively presenting results and recommendations both internally and externally, clearly communicating complex technical material, and writing high quality reports with all necessary backup material for external consumption with minimal supervision\\n</td>\n",
       "      <td>Description:\\nHumatics’ breakthrough microlocation system and analytics software comprise a Spatial Intelligence Platform™ that will revolutionize how people and machines locate, navigate and collaborate. A single Humatics system, using simple, inexpensive radio-frequency technology, can pinpoint multiple, moving targets with millimeter-scale precision, vastly outperforming existing systems at a fraction of the cost.\\n\\nHumatics is seeking a System Test Data Engineer to help drive the analysis of data generated by our Microlocation systems and the software tools required to analyze and visualize that data. You will join a team of experienced and highly motivated electrical, computer, RF and robotics engineers with decades of experience in navigation and autonomous robot operation to realize and apply new types of data to a wide variety of applications.\\n\\nAs a member of Humatics’ Quality Assurance System Test Team, you will work on Quality Assurance and product testing projects providing the data analysis and engineering required to demonstrate that our Microlocation solutions meet and exceed customers’ expectations. Included in this work is the development of data analysis tools including creating internal company dashboards, establishing and maintaining data analytics pipelines, and creating high-end visualizations. You will use this pipeline to analyze data from customer projects, interpret results, and prepare reports for internal and external use. This position will collaborate closely with Humatics’ Software, Quality Assurance and Manufacturing teams to ensure alignment with other company initiatives.\\n\\nResponsibilities\\n\\n\\nOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions\\nExpand an existing analysis library by integrating features prototyped by other engineers in Jupyter notebooks\\nRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation\\nDevelop data analysis tools including workflow definition from high-level requirements, hands-on coding of software tools, establishing and maintaining data analytics pipelines, and providing clear and compelling visualization of results\\nAnalyze manufacturing and quality data, synthesize with product test data\\nPrepare reports for presentation to customers and internal stakeholders\\nEnsure code aligns with company best practices and procedures.\\n\\nRequirements:\\n\\n2-3 years experience in a field related to data analysis, visualization or geographic information systems (GIS)\\nUndergraduate degree in a field related to analytics --- e.g., computer science, applied math, engineering\\nExperience working with databases, ETL frameworks, and creating dashboards\\nFluency in Python is required; experience in Plotly, Bokeh, Tableau, Jupyter, and/or D3 is desired\\nKnowledge of data mining and processing using SQL\\nExperience with Robotics or motion control systems is a plus\\nIdentify, analyze, and interpret trends or patterns in complex data sets using statistical techniques and regression methods\\nAbility to display complex quantitative data in a simple, intuitive format and to present findings in a clear and concise manner\\nStrong communication skills including effectively presenting results and recommendations both internally and externally, clearly communicating complex technical material, and writing high quality reports with all necessary backup material for external consumption with minimal supervision\\n\\nCompensation\\n\\n\\nCompetitive salary\\nMeaningful equity ownership in a well-funded early stage startup\\nExcellent medical, dental and vision benefits\\nPaid holidays and flexible vacation time\\n\\nHumatics is an equal employment opportunity employer. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, sex, sexual orientation, gender identity, religion, disability, age, genetic information, veteran status, ancestry, or national or ethnic origin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Data Engineer - (BOS01- Felton Street)</td>\n",
       "      <td>Waltham, MA</td>\n",
       "      <td>Waltham</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Schedule: M-F\\nDATA ENGINEER\\nLocation: Boston, MA\\n\\nWho You’ll Work For\\nREEF Technology is the ecosystem that connects the world to your block. Each REEF hub is a thriving, connected ecosystem of businesses, cities and people, that enables and provides the delivery of products and services to more people than ever before. Each location offers a variety of services including micro-fulfillment centers, bike and scooter rental stations, electric vehicle charging, rideshare and autonomous vehicle buffering areas, community spaces for pop-up businesses, and more.\\n\\n\\nREEF Technology has reimagined the role of a parking facility. We are the largest network of parking lots in North America, believing these locations can do a lot more than just store your car. They serve as buffers for high density, high activity areas and, as such, alleviate congestion and the ensuing pollution. But, with the explosive growth of the sharing and on-demand economy, it is expected that the need for parking to solely store cars will be outgrown by other needs.\\n\\n\\nWe are part of SoftBank, and its portfolio of leading companies transforming business and commerce at the cutting edge of technology in the world today\\n\\n\\nWhat You’ll Do\\n\\nDevelop, construct, integrate, and test large relational and non-relational databases to build new stable, scalable, rapid, and efficient databases\\nEnsure that the architecture will support the current and future business needs\\nHelp with data collection and cleaning\\nDevelop data processing pipelines for datasets that would be consumed by the data science team\\nWho Are You?\\n\\nDegree in a quantitative field (e.g., Computer Science, Engineering, Mathematics, Statistics, Operations Research or other related fields)\\nWork with a team of data scientists and engineers to build scalable, end-to-end data science solutions\\nExperience working with diverse large-scale structured and unstructured datasets\\nCapable of writing production-level code for implementing data and machine-learning pipelines that are robust, scalable, and performant\\nArchitect, design, and deploy data structures using best practices in data modeling, ETL and ELT processes\\nWork with technology teams to deploy and monitor data and machine-learning pipelines\\nWork with other teams to help them gather, model, and store data with best practices\\nModel data and metadata for ad-hoc and pre-built reporting\\nExperience with Big Data tools such as Spark and Hadoop\\nExperience with Greenplum, Postgres, and MySQL\\nWorking with public clouds (e.g., AWS and/or Google cloud)\\nDesired but not Required\\n\\nHands-on work with graph databases\\nWhat We’ll Provide\\nMedical\\nDental\\nVision\\n401K\\nPaid Time Off (PTO)\\nSpecial Instructions:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Senior Data Engineer – Personalization</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Personalization team makes deciding what to play next on Spotify easier and more enjoyable for every listener. We seek to understand the world of music and podcasts better than anyone else so that we can make great recommendations to every individual person and keep the world listening.\\n\\nEveryday, hundreds of millions of people all over the world use the products we build which include destinations like “Home” and “Search” as well as original playlists such as “Discover Weekly” and “Daily Mix.” We’re a team of technologists, product insight experts, designers, and product managers in Boston, New York, Stockholm, and London.\\n\\nSpotify is looking for a Data Engineer to join us. You will build data driven solutions to bring music and digital media experiences to hundreds of millions of active users and millions of creators by matching fans with creators in a personal and relevant way. You will take on complex data-related problems using some of the most diverse datasets available — user behaviors, acoustical analysis, revenue streams, cultural and contextual data, and other signals across our broad range of mobile and connected platforms. Above all, your work will impact the way the world experiences art.\\n\\nWhat you’ll do\\nBuild large-scale batch and real-time data pipelines with data processing frameworks like Scio, Storm or Spark and the Google Cloud Platform.\\nLeverage best practices in continuous integration and delivery.\\nHelp drive optimization, testing and tooling to improve data quality.\\nCollaborate with other engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.\\nWork in cross functional agile teams to continuously experiment, iterate and deliver on new product objectives.\\nWho you are\\nYou know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.\\nYou have experience with one or more higher-level JVM-based data processing frameworks such as Beam, Dataflow, Crunch, Scalding, Storm, Spark, or something we didn’t list- but not just Pig/Hive/BigQuery/other SQL-like abstractions\\nYou are knowledgeable about data modeling, data access, and data storage techniques.\\nYou care about agile software processes, data-driven development, reliability, and responsible experimentation.\\nYou understand the value of collaboration within teams.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Boston, MA 02210</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02210</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Are you adept at transforming and organizing varied, complex data? Do you have experience in data engineering with unstructured data? You might be the person we are looking for.\\n\\nOur enterprise-wide data science team is seeking a top-notch data engineer with strong technical knowledge and a real passion for addressing business needs through data analysis. As an individual contributor on this team, you will create tools and data pipelines that leverage the latest advances in data engineering to address high-impact research and business questions across research and development, clinical, commercial, and general and administrative areas of our business. You’ll work side-by-side with internal partners from across the organization to develop creative solutions for our highest priority business needs.\\n\\nThe ideal candidate will be a data engineer who is driven by building pipelines that feed data scientists with data and can work both independently as well as part of a highly collaborative team. We are seeking a candidate with the demonstrable ability to find solutions where others can’t, who has the drive and determination to pull the team forward and persevere. We’re looking for self-starters with a strong sense of urgency who thrive when operating in a fast-paced environment.\\n\\nPrimary Responsibilities:\\n\\nWrite clean, maintainable data pipelines that feed data scientists\\nCorrect, transform and enrich multiple sources of data\\nQuickly and efficiently load bulk and streaming data\\nWork closely with the data science team and internal business partners to identify the path to a successful product\\nQualification:\\n\\nBachelor’s degree or higher in Computer Science or related discipline\\n3+ years of experience using an ETL tool. Informatica or Talend is preferred\\n5+ years of experience with SQL database queries and programming\\nExperience programming in Java or Python\\nFamiliarity with data quality, cleaning and masking techniques\\nExperience handling unstructured data\\nExperience working across multiple compute environments to create workflows and pipelines (e.g. HPC, cloud, Linux systems)\\nAn ability to interact with a variety of large-scale data structures (e.g. HDFS, SQL, noSQL)Strong adherence to data privacy standards and ethics\\nDeep understanding of algorithms and performance optimization\\nStrong interpersonal and communication skills and a demonstrated ability to work and collaborate in a team environment\\nPreferred Qualifications:\\n\\nPrevious experience in healthcare, life sciences, or pharmaceutical industry is a plus\\nExperience with AWS cloud technologies and stack\\nKnowledge of distributed data processing and management systems\\nExperience with big data analytics platforms and/or workflow tools\\nDemonstrated ability to organize and incorporate complex systems requirements into product features and prioritize features effectively\\n#LI-MH1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Data Engineer / Senior Data Engineer</td>\n",
       "      <td>Boston, MA 02101</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02101</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor's degree in technical or business discipline or equivalent experience\\nGenerally 3-5 years of professional experience\\nPossess/develop in-depth understanding of data environment and leverages knowledge to build robust, scalable solutions.\\nPossess/develop domain expertise around Investments data and processes and applies to development of solutions.\\nExperienced with SQL and ideally the Microsoft SQL Server stack (SSIS, SSAS)\\nExperience with data management systems; both relational and NoSQL\\nExperience with business intelligence, analytics, and reporting tools (Power BI, Tableau, etc.)\\nExperience with Python or other programming languages\\nUnderstanding of data warehousing, modern Cloud / Hybrid data architecture concepts\\nFamiliarity with Agile development concepts – Test-Driven Design, MVP, iterative and incremental design and delivery\\nFamiliarity with ETL tools and concepts\\nFamiliarity with Data Modeling\\nFamiliarity with full stack application development\\nAbility to adhere to guidelines and standards while innovating and contributing to evolution of those standards.\\nAbility to balance support of legacy solutions with delivery of new business capabilities</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At Liberty Mutual Investments (LMI), we manage a high-quality investment portfolio utilizing a disciplined strategy. We have a large and varied customer base, supported by strategic business units that function as a complete investment firm within our Fortune 100 Company. Our Investment professionals are critical to our ability to keep our promises to policyholders, claimants and their families.\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\\nLMI Technology is actively searching for a Data Engineer within Data Platform Investments Technology who will be responsible for participating on a team to develop and enhance applications from general specifications. Our team is responsible for driving the LMI Data Strategy through the collection, maintenance, improvement, and manipulation of data within the operational and analytics databases. The candidate should be a creative thinker with a strong passion for building data products using the latest technology focused on data analytics.\\n\\nPlease note this role is posted as a range 14-16 grade level depending on experience.\\nJoin us!\\nParticipate in scrums as part of Agile development team\\nEstimate size and Implementation of backlog items, translate into engineering design and logical units of work (tasks)\\nEvaluation of technical feasibility\\nWrites and verifies code which adheres to the acceptance criteria\\nApplication of product development standard methodologies\\nDevelops domain expertise (Investment data and processes) and applies to development of solutions.\\nWork with investments and financial data in a variety of formats across structured and unstructured sources.\\nPerforms data research / profiling and applies results to solution design.\\nSupports the data environment by releasing new features, resolving issues for users, and working with other technology teams to define standards.\\nIdentifies and implements appropriate continuous improvement opportunities.\\nCollaborates with teams to integrate systems.\\nDevelops and updates technical documentation.\\nBuilds positive relationships with key investments business partners and users to understand, gather, and develop products to meet their needs.\\nPreferred Qualifications:\\nBachelor's degree in technical or business discipline or equivalent experience\\nGenerally 3-5 years of professional experience\\nPossess/develop in-depth understanding of data environment and leverages knowledge to build robust, scalable solutions.\\nPossess/develop domain expertise around Investments data and processes and applies to development of solutions.\\nExperienced with SQL and ideally the Microsoft SQL Server stack (SSIS, SSAS)\\nExperience with data management systems; both relational and NoSQL\\nExperience with business intelligence, analytics, and reporting tools (Power BI, Tableau, etc.)\\nExperience with Python or other programming languages\\nUnderstanding of data warehousing, modern Cloud / Hybrid data architecture concepts\\nFamiliarity with Agile development concepts – Test-Driven Design, MVP, iterative and incremental design and delivery\\nFamiliarity with ETL tools and concepts\\nFamiliarity with Data Modeling\\nFamiliarity with full stack application development\\nAbility to adhere to guidelines and standards while innovating and contributing to evolution of those standards.\\nAbility to balance support of legacy solutions with delivery of new business capabilities\\nBenefits: We value your hard work, integrity and commitment to positive change. In return for your service, it’s our privilege to offer you benefits and rewards that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits Overview: At Liberty Mutual, we give motivated, accomplished professionals the opportunity to help us redefine what insurance means; to work for a global leader with a deep sense of humanity and a focus on improving and protecting everyday lives. We create an inspired, collaborative environment, where people can take ownership of their work; push breakthrough ideas; and feel confident that their contributions will be valued and their growth championed. We’re dedicated to doing the right thing for our employees, because we know that their fulfillment and success leads us to great places. Life. Happiness. Innovation. Impact. Advancement. Whatever their pursuit, talented people find their path at Liberty Mutual.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BA/BS Degree required; advanced degree or certification preferred.\\n\\nCustom SQL and advanced ETL expertise required.\\n\\n2+ years of hands-on experience with data visualization, data warehouse, data preparation, and analytics tools.\\n\\nAmazon Web Services (AWS) is the leading cloud provider, offering virtualized infrastructure, storage, networking, messaging, analytics, and other web computing services to customers all over the world. AWS operates a globally distributed environment at massive levels of scale. Businesses, educational institutions and governments around the world depend on AWS for secure cloud services and solutions.\\n\\nThe Sales Systems and Data Team within AWS ensures that our Field Sales and Commercial Operations teams have the world-class data and analytics tools required to scale with our rapidly expanding business. We are seeking a Senior Data Engineer to support multiple projects and initiatives as we continue to build new capabilities around business analytics.\\n\\nThis individual will work closely with our Business Technology &amp; Solutions (BTS) group while engaging directly with business leaders, operations leaders, and field users. In addition to having deep technical skills and a passion for learning the latest cutting edge tools, this individual will have demonstrated experience working on cross-functional teams to develop creative data and analytics solutions.\\n\\nThe ideal candidate will have strong analytical skills to critically evaluate information gathered from multiple sources, reconcile conflicts, decompose high-level information into details, and abstract up from low-level information to a more general understanding.\\n\\nNecessary technical skills include a high level of expertise using SQL and Python to work with data warehouses like Redshift and familiarity with one or more cutting edge analytics tools like Tableau, Alteryx, Data Robot, and/or R.\\n\\nPractical experience with requirements gathering for BI Solutions (end-to-end), over multiple technologies, size, and scale of projects.\\n\\nSelf-starter who can operate independently.\\n\\nAble to influence decisions through effective verbal and written communication and logical reasoning.\\n\\nDemonstrated ability to manage multiple competing priorities simultaneously and drive projects to completion.\\n\\nSound business judgment, proven ability to influence others, and track record of taking ownership.\\n\\nDemonstrated business acumen and the ability to apply technology solutions to solve business problems.\\nExperience working within a high-growth, global technology company\\n\\nAdvanced degrees, certifications, or coursework in areas of business intelligence, computer science, or analytics\\n\\nExpertise with data storage and warehousing solutions such as S3 and Redshift.\\n\\nExpertise developing custom SQL for developing data pipelines\\n\\nExpertise in Tableau, Quicksight, or equivalent BI tool\\n\\nExperience developing Python scripts to support data and analytics automation\\n\\nFamiliarity with Salesforce.com\\n\\nFamiliarity with AWS Platform\\n\\nAmazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Boston, MA 02210</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02210</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\n\\nQlarion is looking for a Data Engineer who is an enthusiastic self-starter to join our analytics consulting team in Boston, MA. This role involves building data pipelines and analytic solutions to solve a variety of client problems. We are looking for someone who is willing to pivot between tools and subject areas, work independently when required, and eager to learn. Our ideal candidate would not only be comfortable working with ambiguous direction, but excited at the possibility of crafting novel and creative solutions. This candidate would step in when necessary to gather project requirements, analyze data, help create data pipelines, organize and document data and processes, automate tasks, and build data visualizations with a variety of tools.\\nThe desired candidates must meet the requirements below and will be compensated based on their qualifications.\\nBachelor’s Degree in Computer Science, Engineering or related field (Master’s degree a plus)\\n2-4 years of experience using Python in a data engineering, data analyses, or similar role\\nStrong proficiency in SQL\\nExperience with Python or similar language(s) for creating and managing data pipelines\\nExperience with data visualization and Business Intelligence tools (Tableau preferred)\\nUnderstanding of Data Warehouses and ETL methodologies\\nExperience with git and github is preferred\\nExperience with GIS tools, PostGIS and Docker a plus\\nStrong analytical skills and problem-solving skills\\nWell-rounded interpersonal skills and client management experience\\nMust possess excellent written and oral communication skills\\nResults-driven with the ability to take initiative, handle multiple tasks and shifting priorities to meet deadlines\\nMust have a strong ability and desire to assimilate and apply knowledge as well as to spread acquired knowledge and experience to other team members\\n\\nQualifications\\n\\nnull\\n\\nAdditional Information\\n\\nAll your information will be kept confidential according to EEO guidelines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Supply Chain Operational Excellence Analytics - Data Engineer</td>\n",
       "      <td>Cambridge, MA 02142</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>MA</td>\n",
       "      <td>02142</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Advanced experience using Data Preparation and visualization software is required (experience with Alteryx and Tableau is a plus)</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>With Biogen’s commitment in growing the Supply Chain Operational Excellence Analytics Team and creating a culture of data driven decision making, we are looking to identify a candidate to become a critical part of our Data Operations.\\nAs part of the Analytics Team within the Global Supply Chain/Operational Excellence organization, this individual will have the opportunity to collaborate with analytics team members, business stakeholders and IT experts to define and operationalize data and analytics initiatives. The Data Engineer will be responsible for building, managing and optimizing datasets from enterprise system data sources by creating and maintaining automated data preparation/management workflows and visual dashboards. This individual will also provide local analytics support to Business Stakeholders within PO&amp;T in the Cambridge office.\\n\\nThis role will report to the Senior Manager of the Operational Excellence Analytics Team.\\n\\nPrimary Responsibilities – Drive Automation through effective metadata management:\\nUsing innovative and modern tools, techniques and architectures to partially or completely automate the most-common, repeatable and tedious data preparation and integration tasks in order to minimize manual and error-prone processes and improve productivity\\nPrimary Responsibilities – Data Visualization:\\nPartner with Analysts and Business Partners to develop dashboards to provide business insights in TableauCollaborate with Analytics Analysts, IT and Business Partners to help drive a self-service model for Tableau and Power BI\\nPrimary Responsibilities – Analytics Innovation:\\nLead and/or contribute to analytics improvement projects working with cross-functional teams.Engage with key stakeholders to identify system needs and inefficiencies and provide targeted solutions\\nOther Responsibilities:\\nCreate, filter and maintain clean datasets that are not automated and/or housed in an enterprise systemProvide local analytics support to Business Stakeholders (e.g. Supply Chain, Quality and External Manufacturing) in the Cambridge Office.\\nLI-POT7\\nQualifications\\nApplicants will be highly detail oriented with excellent skills in data preparation and visualization. We are interested in hearing from candidates that meet the following requirements:\\n\\nAdvanced experience using Data Preparation and visualization software is required (experience with Alteryx and Tableau is a plus)\\nCandidate would be required to provide examples of building and deploying automated data preparation solutions in varying scope and complexity\\nAbility to recognize process and system problems and recommend solutions. Effectively identifying and escalating issues as necessary\\nBasic knowledge of supply chain and operations concepts and how they are applied in analytics is preferred\\nProven ability to prioritize and execute workload to meet requirements and deadlines is required\\nEducation\\nHigh School Diploma with a minimum of 6 years of relevant experience OR a Bachelor of Science degree with a minimum of 1 year of relevant coursework/experience Required\\n\\nDegree in Business Administration, Computer Science, Business Analytics, Supply Chain Management, and/or Applied Math preferred\\nEmployment CategoryFull-Time Regular\\nExperience Level\\nAssociate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Imaging Data Scientist &amp; Data Engineer</td>\n",
       "      <td>Boston, MA 02115</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02115</td>\n",
       "      <td>None Found</td>\n",
       "      <td>PhD or equivalent experience (evidence of impact in data science applied to real life problems in a research setting) ideally within a clinical research environment\\n2+ years of relevant experience, preferably post PhD\\nSignificant scientific background with prior experience in clinical research\\nImage analysis experience required (eg: Radiology, Pathology, DICOM, Tissue Microarray, H&amp;E, Microscopy images, etc.)\\nData engineering experience required\\nExcellent communication and effective problem-solving skills, track record in serving a variety of diverse customers and projects\\nAbility to work independently, prioritize, and manage students/interns if needed, within an interdisciplinary environment that includes physicians, scientists, engineers, and patient advocates.\\nGoogle Cloud Platform experience preferred\\nExperienced in data science methodologies and techniques, e.g. hypothesis testing, classification, regression, clustering, feature allocation, deep learning, time-series analysis, network modeling, feature selection/engineering.\\nVersion control for analyses and big unstructured data (such as images or free text) experience preferred\\nPrior experience supervising at least 1 person a plus</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nIntegrating and analyzing large collections of medical images (eg: Radiology studies, Pathology cases, etc.)\\nIntegrating large collections/datasets into cloud-enabled repositories that can be indexed, linked, and used for research purposes\\nDeveloping libraries and machine learning tools aimed at preprocessing / analyzing / summarizing such large and multimodal image collection\\nWork with our researchers and clinicians to develop functionalities for predicting outcome / biomarker / disease status / response to therapy, based on recent and historical imaging data.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview\\nLocated in Boston and the surrounding communities, Dana-Farber Cancer Institute brings together world renowned clinicians, innovative researchers and dedicated professionals, allies in the common mission of conquering cancer, HIV/AIDS and related diseases. Combining extremely talented people with the best technologies in a genuinely positive environment, we provide compassionate and comprehensive care to patients of all ages; we conduct research that advances treatment; we educate tomorrow's physician/researchers; we reach out to underserved members of our community; and we work with amazing partners, including other Harvard Medical School-affiliated hospitals\\n\\nThe Bioinformatics and Data Science group at the Dana Farber Cancer Institute is seeking an intelligent, hard-working and dynamic individual to join our expanding data science team. The group encompasses expertise in data science, medical imaging, machine learning, natural language processing, ml in production, and research. The successful candidate will have proven experience in leading and executing independently large and complex imaging data science projects, and some experience in the supervision of junior staff. The group is part of the Informatics &amp; Analytics department, that seeks to develop a highly interdisciplinary environment to support the overall mission of the Institute and contribute to maintaining superior cancer care to our pediatric and adult patient population.\\nResponsibilities\\nWe are seeking an energetic and motivated Senior Imaging Data Scientist and Data Engineer for our expanding data science team.\\nThe successful candidate will work on a project that will leverage multi-modal samples from over 100,000+ patients.\\nThe main objectives of the project include:\\nIntegrating and analyzing large collections of medical images (eg: Radiology studies, Pathology cases, etc.)\\nIntegrating large collections/datasets into cloud-enabled repositories that can be indexed, linked, and used for research purposes\\nDeveloping libraries and machine learning tools aimed at preprocessing / analyzing / summarizing such large and multimodal image collection\\nWork with our researchers and clinicians to develop functionalities for predicting outcome / biomarker / disease status / response to therapy, based on recent and historical imaging data.\\n\\nKey responsibilities of the role:\\nDesign and implement a variety of imaging-oriented data science and data engineering pipelines\\nMeeting and consulting with medical doctors and researchers as required to support plans and solutions\\nRegularly produce reports on project updates for project stakeholders and for other team members\\nContribute to the positive, inspiring, and result-oriented team environment\\nDeep data science and data engineering skills, at the interface of computer science and statistics\\nQualifications\\nPhD or equivalent experience (evidence of impact in data science applied to real life problems in a research setting) ideally within a clinical research environment\\n2+ years of relevant experience, preferably post PhD\\nSignificant scientific background with prior experience in clinical research\\nImage analysis experience required (eg: Radiology, Pathology, DICOM, Tissue Microarray, H&amp;E, Microscopy images, etc.)\\nData engineering experience required\\nExcellent communication and effective problem-solving skills, track record in serving a variety of diverse customers and projects\\nAbility to work independently, prioritize, and manage students/interns if needed, within an interdisciplinary environment that includes physicians, scientists, engineers, and patient advocates.\\nGoogle Cloud Platform experience preferred\\nExperienced in data science methodologies and techniques, e.g. hypothesis testing, classification, regression, clustering, feature allocation, deep learning, time-series analysis, network modeling, feature selection/engineering.\\nVersion control for analyses and big unstructured data (such as images or free text) experience preferred\\nPrior experience supervising at least 1 person a plus\\nDana-Farber Cancer Institute is an equal opportunity employer and affirms the right of every qualified applicant to receive consideration for employment without regard to race, color, religion, sex, gender identity or expression, national origin, sexual orientation, genetic information, disability, age, ancestry, military service, protected veteran status, or other groups as protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Delivered the full lifecycle of a solution using Hadoop, AWS S3, AWS EMR\\nDelivered at least one Big data solution using cloud services &amp; open source\\nExpert knowledge of programming languages such as Python, Java, or Scala\\nIngested data using Big Data ETL tools (Apache Spark)\\nSupport Data Science and ML tools like AWS Sagemaker, Cloudera CDSW, Google AI Platform\\nImplemented data security and privacy in a cloud environment\\nDelivered solutions using Agile methodology\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Design and develop ETL pipelines connected devices, web applications, and mobile applications that support the customer experiences\\nCollaborate with front-end and mobile app development teams on user-facing features and services\\nWork with platform architects on software and system optimizations, helping to identify and remove potential performance bottlenecks\\nFocus on innovating new and better ways to create solutions that add value and amaze the end user, with a penchant for simple elegant design in every aspect from data structures to code to UI and systems architecture\\nStay up to date on relevant technologies, plug into user groups, understand trends and opportunities that ensure we are using the best techniques and tools\\nWork with other software leads on developing continuous integration (CI) pipeline and unit test automation\\nDocument the work you do, especially APIs that you create\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Principal Duties and Responsibilities\\nAt Bose we strive to “Wow” the customer, and we are driven by curiosity and perseverance. We like to concentrate on the job at hand. We value passionate, down to earth, “can do” people who enjoy fine-tuning small details, without losing sight of the big picture. We are looking for the type of person who loves to challenge the status quo, who isn’t afraid to give honest feedback, and feels uncomfortable when a day goes by without achieving something impactful. We tend to get excited when a challenge demands a creative solution. Above all else, this role requires someone who takes great pride in their work and is inspired and motivated by their role in improving the way millions of people listen to music world wide.\\nAs a software engineer focusing on Big Data you will work with our IT team to develop data platforms that turn big data into big insights with tremendous value. Enable capabilities and provide business partners with the tools to make their decision making process more efficient and with greater speed. As part of an agile delivery team, you will design, develop, deploy and support the data ingestion pipeline and the data access solutions for our Big Data Analytics platform. Create new data sources for our dynamic customer experience strategies collaborating with touchpoint teams. This role requires knowledge and hands-on experience with big data technologies used throughout entire application stack include Spark, Cloudera Data Hub, and Python/Scala/R languages.\\nDesign and develop ETL pipelines connected devices, web applications, and mobile applications that support the customer experiences\\nCollaborate with front-end and mobile app development teams on user-facing features and services\\nWork with platform architects on software and system optimizations, helping to identify and remove potential performance bottlenecks\\nFocus on innovating new and better ways to create solutions that add value and amaze the end user, with a penchant for simple elegant design in every aspect from data structures to code to UI and systems architecture\\nStay up to date on relevant technologies, plug into user groups, understand trends and opportunities that ensure we are using the best techniques and tools\\nWork with other software leads on developing continuous integration (CI) pipeline and unit test automation\\nDocument the work you do, especially APIs that you create\\nQualifications (demonstrated competence):\\nDelivered the full lifecycle of a solution using Hadoop, AWS S3, AWS EMR\\nDelivered at least one Big data solution using cloud services &amp; open source\\nExpert knowledge of programming languages such as Python, Java, or Scala\\nIngested data using Big Data ETL tools (Apache Spark)\\nSupport Data Science and ML tools like AWS Sagemaker, Cloudera CDSW, Google AI Platform\\nImplemented data security and privacy in a cloud environment\\nDelivered solutions using Agile methodology\\nHighly desirable but not required skills include:\\nExperience with cloud computing (Amazon Web Services preferred)\\nExperience with cloud computing services (Amazon Web Services like EC2, Dynamo, S3, RDS preferred)\\nExperience\\n8 or more years working in software development\\n4+ years developing, deploying and maintaining high volume production big data solutions\\nBachelor's degree in Computer Science, or equivalent\\nBose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Cambridge, MA 02142</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>MA</td>\n",
       "      <td>02142</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Working at MIT offers opportunities, an environment, a culture – and benefits – that just aren’t found together anywhere else. If you’re curious, motivated, want to be part of a unique community, and help shape the future – then take a look at this opportunity.\\n\\nDATA ENGINEER, Office of the Vice President for Finance (VPF)-Controllership, to participate in the full life cycle of business intelligence platform development, including design, coding, testing, and production support. Will implement innovative data solutions and maintain MIT financial data used for reporting and analytics; support VPF staff in its strategic, financial, and operational analytical needs; and work as a team member/lead on projects spanning a broad range of data-focused applications.\\n\\nA full description is available at https://vpf.mit.edu/about-vpf/jobs-at-vpf.\\nJob Requirements\\n\\nREQUIRED: bachelor’s degree (master’s preferred); five years’ experience building databases, data marts, or data warehouses with structured querying language (SQL) across multiple platforms, preferably Microsoft, Oracle, PostgreSQL, and MySQL; three years’ experience in or supporting financial/accounting functions; experience with ETL software, preferably Talend, Pentaho, SQL Server Integration Services (SSIS), and Informatica; experience with online analytical processing (OLAP) and visualization and reporting software, preferably SQL SSAS and SSRS, Tableau, Cognos, and BusinessObjects; advanced-level experience with Excel including modeling and PowerPivot and Visual Basic for Applications; experience with SAP or equivalent enterprise resource planning system; experience with data profiling, data quality, and master data management; detail orientation; ability to work independently and as part of a team; excellent interpersonal, communication, and problem-identification and -solving skills; and ability to collaborate with a diverse group of colleagues from varying backgrounds and meet deadlines in a busy, changing environment. PREFERRED: Data Management Association and/or Data Warehousing Institute membership; SQL, Visual Basic, and Java programming experience; knowledge of statistical tools (e.g., R, Python, SAS, SPSS); and experience with Agile methodology data governance and data vault design, SAP or similar accounting and reporting systems, and higher education or nonprofit accounting. Job #18069-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Backend Engineer</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>-------------------------------------------------------\\nWork with a small elite team in a $15 trillion industry\\n-------------------------------------------------------\\n\\nAt Panjiva we've built the premier platform for working with global trade data. Our clients include manufacturers, retailers, logistics, legal, and financial companies hailing from over 100 countries. They use Panjiva to gather insights into global trade trends, find new suppliers and customers, monitor supply chain relationships, optimize logistics networks, and much more. Panjiva's award-winning and patented technology leverages machine learning to provide powerful search, analysis and visualization tools over more than a billion shipping records and 100 million indexed web pages. The Panjiva platform handles millions of requests a day from users all over the world.\\n\\nJoin our engineering team as we revolutionize a key and fascinating part of the world economy!\\n\\n----------------------------------------\\nSolve important and challenging problems\\n----------------------------------------\\n\\nAs a data engineer on our team, you will play a key role in developing our next- generation data mining and search platform for global trade data. You'll design and implement abstractions, data schemas, and APIs while working closely with our product engineers, product designers, and data scientists on new Panjiva products. You'll help scale our data infrastructure to seamlessly integrate an order-of-magnitude-more data sources by building systems to automate and optimize large scale distributed computing jobs. Your solutions will leverage distributed computing technologies to enable machine learning and NLP algorithms to be run on large scale data. As a data engineer at Panjiva, you will work with Panjiva's world-class data scientists, product designers, and engineers to create products that solve important real-world business problems in a collaborative, fast-paced, and fun startup environment.\\n\\nJoin us in building the next generation of products as we continue to deliver valuable and actionable insights to decision makers in the $15 trillion global trade industry.\\n\\nAt Panjiva, you'll work on:\\n\\nBuilding and managing highly reliable distributed data pipelines with high throughput\\nWorking with our data scientists to turn large scale messy and diverse unstructured data into structured, normalized data\\nMaintaining data integrity across various data sources\\nOptimizing slow running database queries and data pipelines\\nHelping enhance our search engine, capable of running sophisticated user queries quickly and efficiently\\nBuilding internal tools and backend services to enable our data scientists and product engineers to improve efficiency\\n\\n------------\\nRequirements\\n------------\\n\\n\\nBS, MS, or PhD in Computer Science or related field, or equivalent work experience\\n3+ years of experience in working with large scale data in a production environment\\nSignificant performance engineering experience (e.g., profiling slow code, understanding complicated query plans, etc.)\\nExperience with monitoring and profiling tools such as Nagios, perf, htop, etc.\\nAdvanced database skills\\nDeep knowledge of at least one dynamic programming language (Python, Ruby)\\nExperience with scripting languages (Bash, Python, Ruby)\\nExperience developing software on Linux-based OSes\\nExperience with distributed version control systems\\n\\n-------------\\nNice-to-Haves\\n-------------\\n\\n\\nStatistics knowledge\\nFamiliarity with column store databases\\nFamiliarity with database internals, especially PostgreSQL\\nFamiliarity with large-scale, distributed data processing systems, such as MapReduce pipelines, Mesos, Spark, etc.\\nContributions to open-source software\\nStartup experience\\n\\n------------------\\nPerks and Benefits\\n------------------\\n\\n\\nCompetitive salary\\nBuild or buy your dream computer\\nHarvard Square office 1 minute walk from the Red Line T stop\\n4 weeks vacation (rising to 5 weeks with longer tenure), 2 floating holidays / personal days, and 10-12 paid holidays annually, with unlimited sick leave\\nWeekly tech talks\\nCatered lunches\\nHealth, dental, and vision insurance as well as many other benefits\\n401(k) plan, with company matching\\nRegular company hackathons (3-to-4 times a year)\\nHacker Continuing Education Wednesdays i.e., craft beer tasting, ping pong lessons\\nWork with a talented, focused, driven team in a growing company with opportunity for advancement and career development\\n\\nAbout Panjiva\\n-------------\\n\\nPanjiva's technology creates insight from messy global trade data to provide powerful search, analysis, and visualizations of more than a billion shipping records from nearly every country in the world. More than 3,000 customers in over 100 countries, ranging from Fortune 500 companies and startups to government agencies and hedge funds, rely on our platform for supply chain intelligence. In global trade, better insight means better decision making and stronger connections between companies and governments across the globe.\\n\\nRecognizing Panjiva's cutting-edge technology, S&amp;P Global's Market Intelligence division acquired Panjiva in 2018. This acquisition has grown our resources, dramatically expanded our access to data, and accelerated our growth plans. Join our team as we revolutionize a key and fascinating part of the world economy!\\n\\nNOTE: Panjiva is an EOAA. Panjiva takes affirmative action to employ and advance in employment qualified individuals with disabilities. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity or physical or mental disabilities.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Big Data Engineer (Digital Health)</td>\n",
       "      <td>Somerville, MA 02145</td>\n",
       "      <td>Somerville</td>\n",
       "      <td>MA</td>\n",
       "      <td>02145</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s/Master’s degree in computer science or a healthcare related informatics field\\n7+ years of experience designing, developing and implementing highly scalable enterprise-wide data warehousing and integration initiativesExperience in a wide variety of traditional databases and Big Data technologies, developing strategies for data flow, data obfuscation, archiving and other aspects of data warehouses or data lakes5+ years architecting data warehouses and/or data lakes with traditional database enterprise-class RDBMS technologies, preferably MS SQL Server, and Big Data NoSQL like technologies, such as MongoDB, Couch DB, Hadoop and MapReduce, or Hbase. Experience includes creating data integration systems spanning a variety of modern architectural paradigms, including event based, streaming, and near real time design patterns3+ years of experience working in a variety of public and private cloud environments such as AWS, Azure, Dell EMC or VMWare, and experience deploying to data science machine learning/AI stacks such as Databricks, Cloudera/HortonWorks, HTFS, Kafka or SparkPreferred experience working with data/Big Data in HealthCare, including genomics, imaging and EHRSuccessful track record of delivering results on scope, on time, on budgetKnowledge of HIPPA, privacy regulations regarding patient data security and/or research related practices is a plus</td>\n",
       "      <td>Exceptional problem-solving skillsExcellent oral and written communication skillsAble to work in one or more cloud computing environmentsAble to work in a highly-collaborative team and provide valuable insightAble to document requirements and complex data architecturesAble to inform the project management process with thoughtful and accurate timeframe estimatesAble work efficiently under pressure and to manage to tight deadlines or shifting prioritiesEnjoy being challenged and solving complex problemsSelf-motivated, independent and possesses the ability to learn quicklyFamiliarity with medical terminology and healthcare concepts is a significant plus</td>\n",
       "      <td>Work with cross functional research leadership, technical and analytical teams to understand current and future enterprise-wide Big Data analytics goals spanning disparate platforms and datatypesParticipate hands-on in the projects to architect (research, recommend, design, develop and deploy) advanced systems for the collection, aggregation and analysis of those data in alignment with business objectivesProvide Big Data technology assessments, strategies, and roadmaps in several technical domains and act as a subject matter expert on Big DataCollaborate with data engineers to drive and build innovative solutions, defining best practices and methodologiesParticipate in configuring the architecture and advise data engineers on efficient performanceAssist data scientists, SMEs, Data Engineers and Big Data Cloud Architects in deploying and testing AI and machine learning algorithmsDevelop and optimize ETL processes, implement transformations and quality check resultsMay participate in cross-team code reviews among data engineering personnelStay informed on initiatives across the industry and the enterprise to help leadership effectively prioritize current and future Big Data needsExhibit willingness and participate actively in agile practices by collaborating with the Senior Manager and team membersProvide adequate documentation to effectively communicate architectural designs and data inputs, outputs and workflows for technical and nontechnical audiencesWorks closely with the RISC leadership, RISC Technical, other RISC managers and IT colleagues to support corporate/functional business and information needsHelp to establish processes to ensure HIPPA and institutional compliance in all aspects of the work</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Us:\\n\\nAs a not-for-profit organization, Partners HealthCare is committed to supporting patient care, research, teaching, and service to the community by leading innovation across our system. Founded by Brigham and Women’s Hospital and Massachusetts General Hospital, Partners HealthCare supports a complete continuum of care including community and specialty hospitals, a managed care organization, a physician network, community health centers, home care and other health-related entities. Several of our hospitals are teaching affiliates of Harvard Medical School, and our system is a national leader in biomedical research.\\n\\nWe’re focused on a people-first culture for our system’s patients and our professional family. That’s why we provide our employees with more ways to achieve their potential. Partners HealthCare is committed to aligning our employees’ personal aspirations with projects that match their capabilities and creating a culture that empowers our managers to become trusted mentors. We support each member of our team to own their personal development—and we recognize success at every step.\\n\\nOur employees use the Partners HealthCare values to govern decisions, actions and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability &amp; Service Commitment, Decisiveness, Innovation &amp; Thoughtful Risk; and how we treat each other: Diversity &amp; Inclusion, Integrity &amp; Respect, Learning, Continuous Improvement &amp; Personal Growth, Teamwork &amp; Collaboration.\\n\\n\\nGeneral Summary/Overview Statement:\\n\\nPartners HealthCare is embarking on a new Enterprise Data and Digital Health (EDDH) initiative focused on establishing best-in-class data analytics, digital competencies and tools to deliver superior care and experience for our patients. The organization is dedicated to creating a cutting-edge data science environment that supports patient discovery, cohort formation and patient disease stratification.\\n\\nThe Research Information Science &amp; Computing (RISC) group of Partners HealthCare plays a critical role in ensuring the success of this initiative. Our focus is on accelerating the digital transformation of healthcare by redefining how data is used to improve patient outcomes. To better enable researchers and clinicians, we are building tools, repositories, and new data workflows optimized for data science, machine learning and artificial intelligence.\\n\\nOur group is looking for a Big Data Engineer to be a key member of this fast-paced team. This position will be responsible for modifying, expanding and optimizing our data warehouse to include Big Data and Cloud technologies. You will have significant influence on our data strategy by helping define and build the next iterations of features for enterprise-wide data integration and data science. The ideal candidate should have prior experience in the implementation of a modern Big Data architecture and the design and implementation of analytical data platforms. S/he should have deep technical and analytical skills with a rich knowledge of accepted best practices around data flow, data transformation and data ingestion. To quickly get up to speed, a solid understanding of metadata catalogs, data governance, data science and data mining in Big Data technologies in Cloud, hybrid and/or on-premise environments is essential.\\n\\nThe ideal person must love a challenge and feel comfortable working on an evolving project in which the specific goals and deliverables are subject to change. Deep technical and analytical skills are a necessity as well as an ability to successfully work with new technologies, quickly grasp new concepts, and think creatively when solving complex problems. You may be asked to take on other duties the team may need as assigned and you will actively participate in all facets of the project.\\n\\nThis position will report to the RISC Senior Manager of Analytics with additional oversight from technical leads on each project. She/he will also be working closely with other developers, analysts and research-based teams around the enterprise.\\n\\nPrincipal Duties and Responsibilities:\\n\\nWork with cross functional research leadership, technical and analytical teams to understand current and future enterprise-wide Big Data analytics goals spanning disparate platforms and datatypesParticipate hands-on in the projects to architect (research, recommend, design, develop and deploy) advanced systems for the collection, aggregation and analysis of those data in alignment with business objectivesProvide Big Data technology assessments, strategies, and roadmaps in several technical domains and act as a subject matter expert on Big DataCollaborate with data engineers to drive and build innovative solutions, defining best practices and methodologiesParticipate in configuring the architecture and advise data engineers on efficient performanceAssist data scientists, SMEs, Data Engineers and Big Data Cloud Architects in deploying and testing AI and machine learning algorithmsDevelop and optimize ETL processes, implement transformations and quality check resultsMay participate in cross-team code reviews among data engineering personnelStay informed on initiatives across the industry and the enterprise to help leadership effectively prioritize current and future Big Data needsExhibit willingness and participate actively in agile practices by collaborating with the Senior Manager and team membersProvide adequate documentation to effectively communicate architectural designs and data inputs, outputs and workflows for technical and nontechnical audiencesWorks closely with the RISC leadership, RISC Technical, other RISC managers and IT colleagues to support corporate/functional business and information needsHelp to establish processes to ensure HIPPA and institutional compliance in all aspects of the work\\n\\n\\nQualifications\\nBachelor’s/Master’s degree in computer science or a healthcare related informatics field\\n7+ years of experience designing, developing and implementing highly scalable enterprise-wide data warehousing and integration initiativesExperience in a wide variety of traditional databases and Big Data technologies, developing strategies for data flow, data obfuscation, archiving and other aspects of data warehouses or data lakes5+ years architecting data warehouses and/or data lakes with traditional database enterprise-class RDBMS technologies, preferably MS SQL Server, and Big Data NoSQL like technologies, such as MongoDB, Couch DB, Hadoop and MapReduce, or Hbase. Experience includes creating data integration systems spanning a variety of modern architectural paradigms, including event based, streaming, and near real time design patterns3+ years of experience working in a variety of public and private cloud environments such as AWS, Azure, Dell EMC or VMWare, and experience deploying to data science machine learning/AI stacks such as Databricks, Cloudera/HortonWorks, HTFS, Kafka or SparkPreferred experience working with data/Big Data in HealthCare, including genomics, imaging and EHRSuccessful track record of delivering results on scope, on time, on budgetKnowledge of HIPPA, privacy regulations regarding patient data security and/or research related practices is a plus\\n\\nSkills/Abilities/Competencies Required:\\n\\nExceptional problem-solving skillsExcellent oral and written communication skillsAble to work in one or more cloud computing environmentsAble to work in a highly-collaborative team and provide valuable insightAble to document requirements and complex data architecturesAble to inform the project management process with thoughtful and accurate timeframe estimatesAble work efficiently under pressure and to manage to tight deadlines or shifting prioritiesEnjoy being challenged and solving complex problemsSelf-motivated, independent and possesses the ability to learn quicklyFamiliarity with medical terminology and healthcare concepts is a significant plus\\n\\nWorking Conditions:\\n\\nWorking with our team on site in Somerville as well as traveling to meet collaborators at multiple sites in the local Boston area. 5 – 10% travel outside of Boston may be required.\\n\\nEEO Statement\\n\\nPartners HealthCare is an Equal Opportunity Employer &amp; by embracing diverse skills, perspectives and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under law.\\n\\nPrimary Location: MA-Somerville-Assembly Row - PHS\\nWork Locations: Assembly Row - PHS 399 Revolution Drive Somerville 02145\\nJob: Business and Systems Analyst\\nOrganization: Partners HealthCare(PHS)\\nSchedule: Full-time\\nStandard Hours: 40\\nShift: Day Job\\nEmployee Status: Regular\\nRecruiting Department: PHS Enterprise Data &amp; Digital Health\\nJob Posting: Aug 20, 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Data Engineer, Data Intelligence</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5+ years experience with data engineering, data management, data operations or analytics OR equivalent combination of relevant masters/PHD + industry experience\\nBroad science or industry experience preferred (data science, tech, life sciences, etc.); experience working with scientific research data or engineering process development data\\nFluency in Python, SQL; experience in R is a plus\\nProfessional experience with github and AWS\\nExperience working with agile software development teams\\nExperience working in fast-paced, quickly pivoting environment\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nReview and understand all existing R&amp;D databases and data pipeline needs, and create prioritized map of needed pipelines to build and improve\\nCreate prioritized list of needed data dashboarding apps, in collaboration with head of data intelligence; plan implementation\\nHarden and update existing code into reusable pipelines and tools for data intelligence team\\nEnsure smooth operation of all R&amp;D data pipelines\\nSupport prototyping of new visuals, data dashboarding and upload tools &amp; apps, and data analytic pipelines\\nContribute to long-term Data Intelligence strategy development\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Indigo improves grower profitability, environmental sustainability, and consumer health through the use of natural microbiology and digital technologies. Utilizing beneficial plant microbes and agronomic insights, Indigo works with growers to sustainably produce high quality harvests. The company then connects growers and buyers directly to bring these harvests to market. Working across the supply chain, Indigo is forwarding its mission of harnessing nature to help farmers sustainably feed the planet. The company is headquartered in Boston, MA, with additional offices in Memphis, TN, Research Triangle Park, NC, Sydney, Australia, Buenos Aires, Argentina, and São Paulo, Brazil. http://www.indigoag.com/ ( http://www.indigoag.com/%20%20 )\\n\\nThe Data Intelligence team at Indigo captures, integrates, and visualizes complex scientific datasets to speed R+D and product development at Indigo. This requires data engineers to effectively enable support for R+D data processes. We are looking for someone with experience in handling, integrating and visualizing complex data sets. The candidate should also have experience in implementing quality control processes for experimental data including developing and training on tools for evaluating key performance indicators for data generating processes in the company. The ideal candidate will have experience with and strong interest in scientific data and empathy for customers working in R+D functions.\\n\\nResponsibilities:\\n\\nReview and understand all existing R&amp;D databases and data pipeline needs, and create prioritized map of needed pipelines to build and improve\\nCreate prioritized list of needed data dashboarding apps, in collaboration with head of data intelligence; plan implementation\\nHarden and update existing code into reusable pipelines and tools for data intelligence team\\nEnsure smooth operation of all R&amp;D data pipelines\\nSupport prototyping of new visuals, data dashboarding and upload tools &amp; apps, and data analytic pipelines\\nContribute to long-term Data Intelligence strategy development\\n\\nCompetencies:\\n\\nProficient in building and supporting ETL and data analytic pipelines\\nAble to build intuitive visualizations (preferably using dashboard/app development tools such as R Shiny, Dash, Flask)\\nSuperior communication skills; able to understand user needs and requirements\\nAble to prioritize conflicting demands and understand user needs\\nInteracts well with engineering and product management\\nWorks well in cross-functional environments with frequent context switching and can keep cross-functional projects moving\\nPassion for concise and thorough documentation\\nComfortable with ambiguity and challenging company goals\\nAble to work independently and drive to solutions with minimal direction\\n\\nQualifications:\\n\\n5+ years experience with data engineering, data management, data operations or analytics OR equivalent combination of relevant masters/PHD + industry experience\\nBroad science or industry experience preferred (data science, tech, life sciences, etc.); experience working with scientific research data or engineering process development data\\nFluency in Python, SQL; experience in R is a plus\\nProfessional experience with github and AWS\\nExperience working with agile software development teams\\nExperience working in fast-paced, quickly pivoting environment\\n\\nIndigo is committed to living our values, specifically \"creating a work environment where everyone feels respected, connected, and has opportunities to learn and grow.\" As part of living our values, we strive to create a diverse and inclusive work environment where everyone feels they can be themselves and has an equal opportunity of succeeding.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Digital Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor's degree in Information Systems, Information Technology or Computer Science.\\nMinimum 5 years of work experience in platform, data, or analytics development within the ecommerce/digital marketing/CRM/advertising space\\nExperience with cloud based data architecture and tool stacks; AWS stack experience a plus\\nStrong expertise in SQL; Experience with Python data science and engineering libraries; Proven track record dealing with structured and unstructured data\\nExperience working with marketing/digital/CRM data, including working with consumer data\\nProven track record of leading data integrity/QA, with an exceptional attention to detail\\nProven ability to speak both business and technology, and effectively liaison with both teams</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDevelopment of data extracts and feeds from the full spectrum of systems in the digital, marketing, CRM and adtech ecosystem; Building out data processing pipelines and ensuring data quality/availability and reliability standards are met\\nEngineering data foundations for a variety of digital/marketing/CRM use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases\\nSupports designing technical specifications &amp; data transformation models for junior developers\\nEnsures developments is on track and meets specifications as defined by product management and the business\\nResponsible for data integrity of current platform and QA of new releases\\nSupports the development and maintenance of backlog items and solution feature; participates in sprint planning activities from a development perspective.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>FocusKPI, Inc. is looking for a Digital Data Engineer to work for our client in Boston, MA. This is a full-time contract position.\\n\\nResponsibilities:\\nDevelopment of data extracts and feeds from the full spectrum of systems in the digital, marketing, CRM and adtech ecosystem; Building out data processing pipelines and ensuring data quality/availability and reliability standards are met\\nEngineering data foundations for a variety of digital/marketing/CRM use cases, ranging from reporting and data visualization to advanced analytics/machine learning use cases\\nSupports designing technical specifications &amp; data transformation models for junior developers\\nEnsures developments is on track and meets specifications as defined by product management and the business\\nResponsible for data integrity of current platform and QA of new releases\\nSupports the development and maintenance of backlog items and solution feature; participates in sprint planning activities from a development perspective.\\nQualifications\\nBachelor's degree in Information Systems, Information Technology or Computer Science.\\nMinimum 5 years of work experience in platform, data, or analytics development within the ecommerce/digital marketing/CRM/advertising space\\nExperience with cloud based data architecture and tool stacks; AWS stack experience a plus\\nStrong expertise in SQL; Experience with Python data science and engineering libraries; Proven track record dealing with structured and unstructured data\\nExperience working with marketing/digital/CRM data, including working with consumer data\\nProven track record of leading data integrity/QA, with an exceptional attention to detail\\nProven ability to speak both business and technology, and effectively liaison with both teams\\nThank you for applying!\\nFocusKPI Hiring Team\\nAbout FocusKPI:\\nFocusKPI Inc. is an analytics consulting business providing solutions such as CRM customization, Risk Management, Business Intelligence, Advanced/Operational/Web/Digital Analytics as well as Human Resources for our clients.\\n7xU6g3tPSB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Specialist, Data Engineer - Telecom, Media, Tech</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of working experience, both in hands-on and client-facing\\nCommercial experience leading on client-facing projects, including working in close-knit teams\\nExperience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)\\nExperience working on projects within the cloud ideally AWS or Azure\\nData security &amp; governance expertise\\nStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R\\nData Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models\\nExperience in at least one ETL tool (e.g. Informatica, Talend, Pentaho, DataStage)\\nAbility to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets\\nExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner\\nProven ability in clearly communicating complex solutions\\nExcel in team collaboration and working with others from diverse skill-sets and backgrounds</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Qualifications\\n5+ years of working experience, both in hands-on and client-facing\\nCommercial experience leading on client-facing projects, including working in close-knit teams\\nExperience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)\\nExperience working on projects within the cloud ideally AWS or Azure\\nData security &amp; governance expertise\\nStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R\\nData Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models\\nExperience in at least one ETL tool (e.g. Informatica, Talend, Pentaho, DataStage)\\nAbility to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets\\nExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner\\nProven ability in clearly communicating complex solutions\\nExcel in team collaboration and working with others from diverse skill-sets and backgrounds\\nWho You'll Work With\\nYou will join McKinsey’s Telecom, Media and Tech (TMT) practice which strives to help the largest companies in this exciting sector to find new opportunities and excel amid constant change. You will be based in one of our U.S. hubs, such as the Client Capability Center in Waltham, MA, which has a strong Data Science community with 300+ experts working across multiple industries and functions.\\nWhat You'll Do\\nYou will help transform Telecom, Media and Tech (TMT) clients by working closely with senior client stakeholders to understand their data, design the ingestion process to store the data locally and prepare it for Data Analytics.\\nAs someone passionate about data and the opportunity it provides to organizations, you'll play a critical role in database design and development, data integration and ingestion, as well as designing ETL architectures using a variety of ETL tools and techniques. You will also be a part of a well-established McKinsey Analytics community both within the TMT practice, and more broadly.\\nYou are someone with a drive to implement the best possible solutions for clients in close collaboration with a highly skilled Data Science team. As such, you will partner with clients to model their data landscape, obtain data extracts and define secure data exchange approaches. You'll also plan and execute secure, good practice data integration strategies and approaches. This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Waltham, MA</td>\n",
       "      <td>Waltham</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Why We Work at Dun &amp; Bradstreet\\nWe are at a transformational moment in our company journey - and we’re so excited about it. Each day, we are finding new ways to strengthen our award-winning culture, and to accelerate creativity, innovation and growth. Our purpose is to help customers improve business performance with Dun &amp; Bradstreet’s Data Cloud and Live Business Identity, and we’re wildly passionate and committed to this purpose. So, if you’re looking to make an immediate impact at a company that welcomes bold and diverse thinking, come join us!\\nA senior data engineer’s role involves the development of new applications for major projects to drive significant revenue and cost savings for Dun and Bradstreet. Senior data engineers are expected to delve into new, cutting-edge technologies to explore new markets and opportunities. Senior data engineers are expected to react quickly to high-tech marketplace needs, rapidly developing and deploying new products in an agile work environment to best meet and exceed the expectations of our clients and potential prospects. Senior data engineers are well respected by the engineering team, and grow the skills and confidence of those around them.\\nDevelop new applications in a variety of programming languages\\nTake ownership of existing applications for further development/improvements\\nWork closely with related groups to ensure business continuity\\nPerform analysis on large datasets to make and implement recommendations for maximizing customer experience\\nWork as a member of one or more agile teams, using lean principles and SCRUM methodology\\nBachelor’s degree (preferable in computer science, mathematics, data science, or a related field)\\nExperience with SQL and Linux (3-5 years)\\nExperience with application development (3-5 years), strong preference for candidates with Python experience.\\nStrong collaboration skills\\nExperience with hosted environments, AWS, Azure, or other cloud service providers preferred\\nDun &amp; Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law.\\n\\nWe are committed to Equal Employment Opportunity and providing reasonable accommodations to applicants with physical and/or mental disabilities. If you are interested in applying for employment with Dun &amp; Bradstreet and need special assistance or an accommodation to use our website or to apply for a position, please send an e-mail with your request to TalentAcquisitionTeam@dnb.com. Determination on requests for reasonable accommodation are made on a case-by-case basis.\\nPlease note that all Dun &amp; Bradstreet job postings can be found at https://dnb.wd1.myworkdayjobs.com/Careers and all communication from Dun &amp; Bradstreet will come from an email address ending in @dnb.com.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Junior Data Engineer</td>\n",
       "      <td>Boston, MA 02210</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02210</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n3+ years of working experience with relational databases, SQL, data warehousing and Business Intelligence tools.\\n3+ years of practical work experience\\nExperience with Microsoft SQL Server Integration Services (SSIS)\\nComputer science or a related technical BA/BS required, MBA a plus\\nImplementation experience of star schema data warehousing\\nDatabase experience with Oracle or MS SQL Server\\nProficient in the use of database access and connectivity tools (Native, ODBC, JDBC) for both general access and decision support purposes\\nETL tool experience with tools such as Microsoft SSIS, Informatica, and Oracle Warehouse Builder\\nReporting and Analytical experience with tools such as Microsoft Analytical and Reporting Services\\nStrong Microsoft Access and Excel skills a plus\\nNice to have experience with cloud technology such as AWS, Microsoft Azure.\\nKnowledge of database maintenance and administration techniques\\nDesire to work in a high paced start up environment\\nStrong problem-solving skills and the ability to work independently\\nStrong systems analysis, design and programming skills\\nStrong written and verbal communication skills\\nDesired skills include SQL, PL-SQL, T-SQL,\\nFamiliarity with Java, JCL, C, C++, VB Script, Unix Shell, XML</td>\n",
       "      <td>\\nPerform business requirements analysis necessary to design and construct the technical architecture to support the solution. Technical architecture solutions consist of an operational data store, data warehouse, data marts and associated delivery mechanisms.\\nDefine and implement the architectural components including RDBMS, operating system platform, hardware and tools.\\nDevelop data requirements, data maps, transformation rules, and aggregation plans. Define strategies for data extraction, hygiene, acquisition, and loading.\\nWork with the client and Data Architect to implement and continually refine data load best practices. Support overall architecture through operational processes. Recommend changes in development, maintenance and system standards based upon best practice experience.\\nEnsure quality implementation by development and execution of detailed plans, delivery of operational best practices.\\nWork with the client technical team on the definition and planning of system implementation and growth forecasting.\\nDefine and implement monitoring systems to perform load validation, to update and maintain metadata, and to ensure overall data integrity.\\nContinually improve the capabilities, working with the best of breed solutions to develop and enhance delivery capabilities based on new technology.\\nCollaborate with other data warehouse experts to maintain consistency of architecture and best practices technology.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Junior Data Engineer is responsible for the implementation of our client's data mart and customer analytics platform for our marketing services clients. You will be responsible for the technical definition of migrating customer data to the data mart environment, providing platform, database and infrastructure design and implementation. Working closely with lead Data Architect, you will ensure that the solution satisfies business requirements. This position requires an in-depth understanding of software/systems analysis, design and performance principles. You must have experience in conceptual analysis, problem solving, data analysis and programming of technical and business information systems. In addition to a software development background, the successful applicant will have proficient experience with ETL based tools and Star Schema data structures.\\n\\nKey Responsibilities\\nTECHNICAL SOLUTIONS ARCHITECTURE\\nPerform business requirements analysis necessary to design and construct the technical architecture to support the solution. Technical architecture solutions consist of an operational data store, data warehouse, data marts and associated delivery mechanisms.\\nDefine and implement the architectural components including RDBMS, operating system platform, hardware and tools.\\nDevelop data requirements, data maps, transformation rules, and aggregation plans. Define strategies for data extraction, hygiene, acquisition, and loading.\\nWork with the client and Data Architect to implement and continually refine data load best practices. Support overall architecture through operational processes. Recommend changes in development, maintenance and system standards based upon best practice experience.\\nEnsure quality implementation by development and execution of detailed plans, delivery of operational best practices.\\nWork with the client technical team on the definition and planning of system implementation and growth forecasting.\\nDefine and implement monitoring systems to perform load validation, to update and maintain metadata, and to ensure overall data integrity.\\nContinually improve the capabilities, working with the best of breed solutions to develop and enhance delivery capabilities based on new technology.\\nCollaborate with other data warehouse experts to maintain consistency of architecture and best practices technology.\\nDATA WAREHOUSING IMPLEMENTATION\\nExperience and Skills Required\\n3+ years of working experience with relational databases, SQL, data warehousing and Business Intelligence tools.\\n3+ years of practical work experience\\nExperience with Microsoft SQL Server Integration Services (SSIS)\\nComputer science or a related technical BA/BS required, MBA a plus\\nImplementation experience of star schema data warehousing\\nDatabase experience with Oracle or MS SQL Server\\nProficient in the use of database access and connectivity tools (Native, ODBC, JDBC) for both general access and decision support purposes\\nETL tool experience with tools such as Microsoft SSIS, Informatica, and Oracle Warehouse Builder\\nReporting and Analytical experience with tools such as Microsoft Analytical and Reporting Services\\nStrong Microsoft Access and Excel skills a plus\\nNice to have experience with cloud technology such as AWS, Microsoft Azure.\\nKnowledge of database maintenance and administration techniques\\nDesire to work in a high paced start up environment\\nStrong problem-solving skills and the ability to work independently\\nStrong systems analysis, design and programming skills\\nStrong written and verbal communication skills\\nDesired skills include SQL, PL-SQL, T-SQL,\\nFamiliarity with Java, JCL, C, C++, VB Script, Unix Shell, XML\\n\\nbombpYouol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Data Engineer II</td>\n",
       "      <td>Boston, MA 02116</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>02116</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At Rapid7, you'll join a world class team of thinkers and problem solvers who prioritize individual growth and team collaboration over ego and attitude, while conducting research and building products that solve the world's toughest security challenges.\\nYou will work with Data Scientists, Security Researchers, Threat Analysts and Software engineers to bring security research ideas from conception through to deployment in our products or publication in the community. You will participate in every aspect of research from design and planning, through implementation and testing, to deployment and support.\\nWhat you will do\\nYou will be instrumental in helping our research team move as quickly as possible to solve challenging security problems. You will help design, plan, and implement data analysis tools and systems with peers in small teams, and be part of a larger team working collaboratively to support our research goals: improving our products and supporting the security community by publishing interesting findings and presenting at conferences.\\nSuccessful candidates aren't just good at writing code or designing systems, but have a demonstrated focus on helping the rest of the team be as successful as possible. You should love being challenged by your work, with a desire to ship and iterate on tools, infrastructure and POCs, as well as an emphasis on collaboration, communication, and growth. Our entire team (Data Science, Security Research, Threat Intelligence, Engineering, Product Management) works together closely to ensure the success of our research projects as well as our products, and we're looking for someone who revels in exposure to the entire process of how research tools are planned, built, deployed, used, and supported.\\nWe value attitude and willingness to learn over ego and experience, and want someone who is looking to grow with the team as we continue to grow our product.\\nOur Tools\\nPython / R / Scala\\nJupyter / R notebooks\\nAWS:\\nEMR\\nAthena\\nGlue\\nSagemaker\\nSpark\\nTerraform\\nJenkins\\nYou need to be...\\nFocused on shipping solutions to problems, not just code\\nA strong communicator, able to explain your work at varying levels of detail depending on the audience\\nAble to read and write Python code\\nCommitted to developing high quality tools and systems within reasonable timeframes\\nExperienced with AWS data processing and transformation services\\nComfortable with foundational AWS services (S3, SQS, EC2)\\nAble to help distill research requirements into technical details, including to organize and plan epics, and help others understand the priorities and needs of that work\\nSomeone with a “grab a shovel” attitude, where you're excited to dig in and get your hands dirty working with your peers.\\nAble to initiate and drive projects to completion with minimal guidance\\nWilling to learn, and willing to teach. Everyone brings something new to the team, and we want to learn from you as much as we want to grow you as an engineer and a team member.\\nSomeone with around 4 or more years of experience that encompasses the above. We're flexible, and are looking for the right candidate - not just someone who hits a number.\\nIt would be great if you...\\nKnow how to use git and GitHub, and understand basic branching strategies and pull requests\\nAre familiar with interacting with REST endpoints\\nKnow your way around distributed processing coding tools (spark, pyspark) and concepts (MapReduce, message queueing)\\nHave used Terraform to build up and modify cloud deployments\\nHave experience with Numpy and Pandas\\nKnow how to read (or even write) R, Scala, Java, bash\\nWant to learn more about Data Science and Security Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Oracle / Sybase Data Engineer</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Details\\nJob Code\\nJPSC-7797\\nPosted Date\\n08/13/18\\nExperience\\n8 Years\\nPrimary Skills\\nSybase and Oracle internals,performance tuning of Sybase and Oracle,Sybase as a DBMS to Oracle\\nRequired Documents\\nResume\\nOverview\\nRole: Sybase / Oracle\\nWork Location: Boston, MA\\nDuration: 6 Months\\n\\nDevelops large project and business-wide logical data models. Creates physical database designs from logical data models. Maintains database dictionaries and integration of systems through database design. Having 6+ years’ experience.\\n\\nWhat are the top three must-haves you will be looking for in the resumes/interviews for this role?\\nIn-depth understanding of Sybase and Oracle internals\\nExperience with performance tuning of Sybase and Oracle\\nExperience with migration applications currently using Sybase as a DBMS to Oracle.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title          Location  \\\n",
       "0    Google Technical Architect                        Boston, MA 02199   \n",
       "1    Software Engineer/Data Engineer                   Boston, MA 02101   \n",
       "2    Senior Data Engineer                              Boston, MA         \n",
       "3    Senior Data Engineer, Delivery                    Boston, MA 02110   \n",
       "4    Data Engineer                                     Newton, MA         \n",
       "..             ...                                            ...         \n",
       "121  Specialist, Data Engineer - Telecom, Media, Tech  Boston, MA         \n",
       "122  Senior Data Engineer                              Waltham, MA        \n",
       "123  Junior Data Engineer                              Boston, MA 02210   \n",
       "124  Data Engineer II                                  Boston, MA 02116   \n",
       "125  Oracle / Sybase Data Engineer                     Boston, MA         \n",
       "\n",
       "        City State         Zip     Country  \\\n",
       "0    Boston   MA    02199       None Found   \n",
       "1    Boston   MA    02101       None Found   \n",
       "2    Boston   MA    None Found  None Found   \n",
       "3    Boston   MA    02110       None Found   \n",
       "4    Newton   MA    None Found  None Found   \n",
       "..      ...   ..           ...         ...   \n",
       "121  Boston   MA    None Found  None Found   \n",
       "122  Waltham  MA    None Found  None Found   \n",
       "123  Boston   MA    02210       None Found   \n",
       "124  Boston   MA    02116       None Found   \n",
       "125  Boston   MA    None Found  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Qualifications  \\\n",
       "0    Minimum 5 years of Consulting or client service delivery experience on Google GCP\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "1    \\nStrong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.\\nFamiliarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured (Postgres, MySQL) and unstructured data sources (Elastic, Kafka, and the Hadoop ecosystem) as implemented at Internet-scale.\\nExperience writing and optimizing streaming and batch analytics.\\nExperience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.\\nBS/BA in Computer Science, Engineering, or relevant field experience.                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2    3-5 years experience with data engineering or data warehousing, or an equivalent data-oriented software engineering background\\n2-3 years experience with ETL technologies (e.g. Talend, Informatica, Matillion)\\n2-3 years experience with key AWS technologies (Redshift, S3, IAM; also Kinesis, Lambda, EMR, Spark, Hive)\\n3-5 years of experience with SQL\\nExperience in optimizing queries and tuning database performance\\nFamiliarity with data analysis and data science tools (Looker, Python, Jupyter, R) is a plus\\nPython or Java experience is a plus                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "121  5+ years of working experience, both in hands-on and client-facing\\nCommercial experience leading on client-facing projects, including working in close-knit teams\\nExperience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)\\nExperience working on projects within the cloud ideally AWS or Azure\\nData security & governance expertise\\nStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R\\nData Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models\\nExperience in at least one ETL tool (e.g. Informatica, Talend, Pentaho, DataStage)\\nAbility to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets\\nExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner\\nProven ability in clearly communicating complex solutions\\nExcel in team collaboration and working with others from diverse skill-sets and backgrounds   \n",
       "122  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "123  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "124  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "125  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Skills  \\\n",
       "0    DevOps on an GCP platform. Multi-cloud experience a plus.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "121  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "122  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "123  \\n3+ years of working experience with relational databases, SQL, data warehousing and Business Intelligence tools.\\n3+ years of practical work experience\\nExperience with Microsoft SQL Server Integration Services (SSIS)\\nComputer science or a related technical BA/BS required, MBA a plus\\nImplementation experience of star schema data warehousing\\nDatabase experience with Oracle or MS SQL Server\\nProficient in the use of database access and connectivity tools (Native, ODBC, JDBC) for both general access and decision support purposes\\nETL tool experience with tools such as Microsoft SSIS, Informatica, and Oracle Warehouse Builder\\nReporting and Analytical experience with tools such as Microsoft Analytical and Reporting Services\\nStrong Microsoft Access and Excel skills a plus\\nNice to have experience with cloud technology such as AWS, Microsoft Azure.\\nKnowledge of database maintenance and administration techniques\\nDesire to work in a high paced start up environment\\nStrong problem-solving skills and the ability to work independently\\nStrong systems analysis, design and programming skills\\nStrong written and verbal communication skills\\nDesired skills include SQL, PL-SQL, T-SQL,\\nFamiliarity with Java, JCL, C, C++, VB Script, Unix Shell, XML   \n",
       "124  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "125  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Responsibilities  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "2    Implement data pipelines using agile, iterative processes to deliver value quickly\\nTreat data quality, reliability, and documentation as fundamental acceptance criteria for data pipelines\\nCollaborate with the team to make the best technology choices\\nDeliver solutions that ensure data accuracy, high availability, robust security, and that are built for rapid scaling\\nCreate and maintain data models, data catalogues, and data security.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "121  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "122  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "123  \\nPerform business requirements analysis necessary to design and construct the technical architecture to support the solution. Technical architecture solutions consist of an operational data store, data warehouse, data marts and associated delivery mechanisms.\\nDefine and implement the architectural components including RDBMS, operating system platform, hardware and tools.\\nDevelop data requirements, data maps, transformation rules, and aggregation plans. Define strategies for data extraction, hygiene, acquisition, and loading.\\nWork with the client and Data Architect to implement and continually refine data load best practices. Support overall architecture through operational processes. Recommend changes in development, maintenance and system standards based upon best practice experience.\\nEnsure quality implementation by development and execution of detailed plans, delivery of operational best practices.\\nWork with the client technical team on the definition and planning of system implementation and growth forecasting.\\nDefine and implement monitoring systems to perform load validation, to update and maintain metadata, and to ensure overall data integrity.\\nContinually improve the capabilities, working with the best of breed solutions to develop and enhance delivery capabilities based on new technology.\\nCollaborate with other data warehouse experts to maintain consistency of architecture and best practices technology.   \n",
       "124  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "125  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "\n",
       "      Education  \\\n",
       "0    None Found   \n",
       "1    None Found   \n",
       "2    None Found   \n",
       "3    None Found   \n",
       "4    None Found   \n",
       "..          ...   \n",
       "121  None Found   \n",
       "122  None Found   \n",
       "123  None Found   \n",
       "124  None Found   \n",
       "125  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                            Requirement  \\\n",
       "0    Proven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills   \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                           \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                           \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                           \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                           \n",
       "..          ...                                                                                                                                                                                                                                                                                                                           \n",
       "121  None Found                                                                                                                                                                                                                                                                                                                           \n",
       "122  None Found                                                                                                                                                                                                                                                                                                                           \n",
       "123  None Found                                                                                                                                                                                                                                                                                                                           \n",
       "124  None Found                                                                                                                                                                                                                                                                                                                           \n",
       "125  None Found                                                                                                                                                                                                                                                                                                                           \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                FullDescriptions  \n",
       "0    Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Google Cloud Platform (GCP) Technical Architect Delivery is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would also be responsible for developing and delivering Google GCP cloud solutions to meet todays high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Google GCP Technical Architect is a highly performant GCP Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data soltuions on cloud. Using Google GCP public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications.\\n\\nRole & Responsibilities:Work with Sales and Bus Dev teams in providing Data and GCP Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nBasic Qualifications\\nMinimum 5 years of Consulting or client service delivery experience on Google GCP\\nMinimum 10 years of experience in big data, database and data warehouse architecture and delivery\\nBachelors degree or 12 years previous professional experience\\nAble to travel 100% (M-TH)\\nMinimum of 5 years of professional experience in 2 of the following areas:\\nSolution/technical architecture in the cloud\\nBig Data/analytics/information analysis/database management in the cloud\\nIoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using GCP services etc.:\\nData Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core\\nStreaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam\\nData Warehousing & Data Lake : BigQuery, Cloud Storage\\nAdvanced Analytics : Cloud ML engine, Google Data Studio, Tensorflow & Sheets\\n\\nFamiliarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nCertified GCP Solutions Architect - Associate\\nCertified GCP Solutions Architect – Professional (Nice to have)\\nCertified GCP Big Data Specialty (Nice to have)\\nCertified GCP AI/ML Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an GCP platform. Multi-cloud experience a plus.\\nExperience developing and deploying ETL solutions on GCP\\nStrong in Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience beyond GCP a plus - AWS and Azure\\n\\nProfessional Skill Requirements\\nProven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.  \n",
       "1    Ideally, the successful candidate will be located near our NYC or College Park, MD office. However, there is the opportunity to work remotely based on role and level.\\nSoftware Engineer/Data Engineer\\nBlueVoyant is seeking a Software Engineer/Data Engineer to help us build a data analytics platform powerful enough to protect some of the world's biggest networks, and nimble enough to adapt to a quickly evolving product vision. We are solving interesting, exciting, and important problems with smart people.\\nQualifications for the Software Engineer/Data Engineer:\\nStrong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.\\nFamiliarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured (Postgres, MySQL) and unstructured data sources (Elastic, Kafka, and the Hadoop ecosystem) as implemented at Internet-scale.\\nExperience writing and optimizing streaming and batch analytics.\\nExperience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.\\nBS/BA in Computer Science, Engineering, or relevant field experience.\\nWhat you will do as a Software Engineer/Data Engineer:\\nWork closely with analysts to transform threat analytics into production-level code.\\nActively contribute to application architecture and product vision.\\nParticipate in requirements gathering and transformation from prototype to product design.\\nParticipate in daily development stand-up meetings and regular sprint planning and product demo meetings.\\nHelp us stay current on the latest data processing tools and trends.\\nIdeal candidates will:\\nThrive in our small, fast-paced, product-driven environment\\nCollaborate with teams from across the organization\\nDeliver features and fixes on tight schedules and under pressure\\nPresent ideas in business-friendly and user-friendly language\\nCreate systems that are maintainable, flexible and scalable\\nDefine and follow a disciplined development and engineering workflow\\nDemonstrate ownership of tasks with escalation as needed\\nBe a subject matter expert in one or more of the technologies employed\\nRelentlessly push for successful customer outcomes\\nPossess a strong interest or background in cyber security\\nGeneral responsibilities include:\\nParticipate in all stages of an agile software development lifecycle, including product ideation, requirements gathering, architecture, design, implementation, testing, documentation, and support\\nRefine our software development methodology based on agile/lean practices with continuous feedback and well-defined metrics to drive improvement\\nMaintain up-to-date knowledge of technology standards, industry trends, emerging technologies, and software development best practices\\nEnsure technical issues are quickly resolved and help implement strategies and solutions to reduce the likelihood of reoccurrence\\nIdentify competitive offerings and opportunities for innovation including assessments of risk/reward to the company.\\nAbout BlueVoyant\\nBlueVoyant is a global cybersecurity firm that provides Advanced Threat Intelligence, for large companies and a comprehensive Managed Security Service and Professional Services for small businesses, powered by one of the largest commercially available cyber threat databases in the world.\\nBy working with BlueVoyant, companies can gain unique and far-reaching visibility into malicious activity on their networks, in the dark web and across the internet, as well as real-time, automatable remediation services. Through our unique real-time external threat monitoring, predictive human and machine-sourced intelligence, and proactive managed security and incident response, BlueVoyant offers the private sector exceptional cyber defense capabilities.\\nCo-founded by CEO Jim Rosenthal, former Chief Operating Officer at Morgan Stanley, and Executive Chairman Tom Glocer, former Chief Executive Officer at Thomson Reuters, BlueVoyant has attracted a management team that comes from the world's preeminent intelligence, law enforcement, and private sector organizations. Other leaders include:\\nJim Penrose, COO, former EVP at Darktrace with 17 years at the NSA in key leadership roles.\\nGad Goldstein, Head of BlueVoyant Europe and Chairman of Israel, former division head (Major General equivalent) in the Israel Security Agency, Shin Bet.\\nRobert Hannigan, Chairman of BlueVoyant International, former Director of GCHQ.\\nAustin Berglas, Global Head of Professional Services, former head of the FBI's New York Cyber Branch.\\nDavid Etue, Global Head of MSS, former VP of Managed Services at Rapid7.\\nMilan Patel, Chief Client Officer, former CTO of the FBI Cyber Division.\\nRon Feler, Global Head of Threat Intelligence and Operations, former Deputy Commander of Unit 8200, the cybersecurity division of the Israel Defense Forces.\\nEldad Chai, CPO, former SVP of Products at Imperva.\\nJim Bieda, Senior Advisor, former NSA Deputy CTO.\\nBill Crumm, Senior Advisor, former NSA SIGINT Director and former Cybersecurity Head, Morgan Stanley.\\nDan Ennis, Senior Advisor, former Head of Threat Intelligence at the NSA\\nAll employees must be authorized to work in the United States or Israel. BlueVoyant provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, BlueVoyant complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.\\nCome work with us!\\nBlueVoyant is hiring software developers, infrastructure engineers, data science experts, and technologists of all types to build next generation predictive threat intelligence and advanced security monitoring solutions.\\nProjects currently in development include:\\nAn Internet-scale (multi-PB, > 500k TPS) repository made up of unstructured, structured, and semi-structured data sources used for real time alerting, threat analysis, and research and development internally.\\nA powerful, enterprise-scale suite of products used to provide managed security services and Security Operations Center (SOC) functionality to small and medium sized enterprises around the globe.\\nA unique internal platform to support the data research needs of our analysts and SOC so they can quickly and effectively identify new threat actors and techniques.\\nrOZMxAY0kC                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2    Senior Data Engineer\\nBoston, MA\\nU.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time. No Corp to Corp.\\n\\nApply direct to: creposa@syrinx.com\\nSummary:\\nThe Data team is responsible for leading efforts to ingest, structure, and model our data to support critical decision making. Our enterprise data warehouse is based on AWS Redshift, supporting Looker analytical dashboards, and is backed by an AWS data lake. In addition to analytics, the data warehouse also supports an active and growing data science practice.\\n\\nThe ideal candidate will have experience building and supporting data platforms that help end users make sound business decisions. This includes data pipeline creation, data quality management, system reliability, job orchestration and recovery aka “Data Ops”. In additional to technical competency, they should be truly motived to empower end users.\\n\\nResponsibilities\\nImplement data pipelines using agile, iterative processes to deliver value quickly\\nTreat data quality, reliability, and documentation as fundamental acceptance criteria for data pipelines\\nCollaborate with the team to make the best technology choices\\nDeliver solutions that ensure data accuracy, high availability, robust security, and that are built for rapid scaling\\nCreate and maintain data models, data catalogues, and data security.\\nQualifications\\n3-5 years experience with data engineering or data warehousing, or an equivalent data-oriented software engineering background\\n2-3 years experience with ETL technologies (e.g. Talend, Informatica, Matillion)\\n2-3 years experience with key AWS technologies (Redshift, S3, IAM; also Kinesis, Lambda, EMR, Spark, Hive)\\n3-5 years of experience with SQL\\nExperience in optimizing queries and tuning database performance\\nFamiliarity with data analysis and data science tools (Looker, Python, Jupyter, R) is a plus\\nPython or Java experience is a plus                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "3    We’re reimagining sports and technology.\\nDraftKings is bringing sports fans closer to the games they love and becoming an essential part of their experience in the process. An industry pioneer since our founding in 2012, we believe we can continue to define what it means to be a technology company in sports entertainment. We love what we do and we think you will too.\\n\\nLove data? We do too.\\nAs a Senior Data Engineer, Delivery you’ll be a creative contributor, leader and mentor to our data analysis and scalability processes, and you will use your experience to provide key insights that help us make data-driven decisions. Analytical thinking drives our business and when you join our team, you’ll not only solve new problems every day, you’ll see your data solutions immediately improve our users’ experience.\\nWho are we a good fit for?\\nWe love working with talented people but more than that, we seek out compassionate co-workers with a collaborative spirit. Our work moves quickly and we’re great at coming together to find creative solutions to some of tech’s most interesting problems. If that sounds good to you, join us.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "4    About the Data Engineer position\\nData fuels everything we do at Gravyty. It is part of our core value – to turn data into action for nonprofits to achieve their mission. We place high value in things like fast no-BS implementations, data quality/transparency, and building scalable data systems. As a Data Engineer at Gravyty you will be a core part of designing and implementing ETL (extract-transform-load) data flows for our customers. We are looking for a person who wants the fast-paced environment of a startup operating in the mission-driven world of nonprofits and wants to evolve into a senior data engineer with world-class abilities in ETL data workflows, database systems for processing, and big data applications.\\nData Engineer Responsiblities\\nScheduling jobs that recover from errors, such as hitting rate limits or data loading errors, and can do retries when failures occur\\nStoring data into various targets such as flat files, data stores, or outgoing REST APIs\\nDoing transformation tasks on data – such as cleansing, validation, filtering, partitioning, aggregation, calculating metrics, and reporting\\nUnderstand existing schemas and define the best possible new data functionality\\nHelping customers understand how processing is best done to achieve their non-technical objectives\\nUnderstanding the input/output formats of the data and constraints on data processing\\nData Engineer Requirements\\nAt least 1 year of full-time backend programming experience in a major programming language such as Python, Java, C#, Scala, PHP, or Ruby\\nGood practical knowledge of (ANSI) SQL, database design, and data modeling\\nExperience in programming ETL (extract-transform-load) workflows with a programming language such as Python or Scala\\nAs part of the job, be prepared to learn Python for writing ETL scripts and Django for integrating your work into web applications\\nKnowledge of programmatically fetching and sending data over REST APIs with some programming language such as Python, Ruby, Java, PHP, etc.\\nAt least a basic understanding and some experience in the topic of data quality and query performance\\nAdditional Information\\nOptional, but big pluses include doing prior work with nonprofit CRM systems such as Salesforce, Ellucian, Blackbaud, etc., and experience or interest in big data, including deployment of systems such as Hadoop.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "121  Qualifications\\n5+ years of working experience, both in hands-on and client-facing\\nCommercial experience leading on client-facing projects, including working in close-knit teams\\nExperience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)\\nExperience working on projects within the cloud ideally AWS or Azure\\nData security & governance expertise\\nStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R\\nData Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models\\nExperience in at least one ETL tool (e.g. Informatica, Talend, Pentaho, DataStage)\\nAbility to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets\\nExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner\\nProven ability in clearly communicating complex solutions\\nExcel in team collaboration and working with others from diverse skill-sets and backgrounds\\nWho You'll Work With\\nYou will join McKinsey’s Telecom, Media and Tech (TMT) practice which strives to help the largest companies in this exciting sector to find new opportunities and excel amid constant change. You will be based in one of our U.S. hubs, such as the Client Capability Center in Waltham, MA, which has a strong Data Science community with 300+ experts working across multiple industries and functions.\\nWhat You'll Do\\nYou will help transform Telecom, Media and Tech (TMT) clients by working closely with senior client stakeholders to understand their data, design the ingestion process to store the data locally and prepare it for Data Analytics.\\nAs someone passionate about data and the opportunity it provides to organizations, you'll play a critical role in database design and development, data integration and ingestion, as well as designing ETL architectures using a variety of ETL tools and techniques. You will also be a part of a well-established McKinsey Analytics community both within the TMT practice, and more broadly.\\nYou are someone with a drive to implement the best possible solutions for clients in close collaboration with a highly skilled Data Science team. As such, you will partner with clients to model their data landscape, obtain data extracts and define secure data exchange approaches. You'll also plan and execute secure, good practice data integration strategies and approaches. This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "122  Why We Work at Dun & Bradstreet\\nWe are at a transformational moment in our company journey - and we’re so excited about it. Each day, we are finding new ways to strengthen our award-winning culture, and to accelerate creativity, innovation and growth. Our purpose is to help customers improve business performance with Dun & Bradstreet’s Data Cloud and Live Business Identity, and we’re wildly passionate and committed to this purpose. So, if you’re looking to make an immediate impact at a company that welcomes bold and diverse thinking, come join us!\\nA senior data engineer’s role involves the development of new applications for major projects to drive significant revenue and cost savings for Dun and Bradstreet. Senior data engineers are expected to delve into new, cutting-edge technologies to explore new markets and opportunities. Senior data engineers are expected to react quickly to high-tech marketplace needs, rapidly developing and deploying new products in an agile work environment to best meet and exceed the expectations of our clients and potential prospects. Senior data engineers are well respected by the engineering team, and grow the skills and confidence of those around them.\\nDevelop new applications in a variety of programming languages\\nTake ownership of existing applications for further development/improvements\\nWork closely with related groups to ensure business continuity\\nPerform analysis on large datasets to make and implement recommendations for maximizing customer experience\\nWork as a member of one or more agile teams, using lean principles and SCRUM methodology\\nBachelor’s degree (preferable in computer science, mathematics, data science, or a related field)\\nExperience with SQL and Linux (3-5 years)\\nExperience with application development (3-5 years), strong preference for candidates with Python experience.\\nStrong collaboration skills\\nExperience with hosted environments, AWS, Azure, or other cloud service providers preferred\\nDun & Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law.\\n\\nWe are committed to Equal Employment Opportunity and providing reasonable accommodations to applicants with physical and/or mental disabilities. If you are interested in applying for employment with Dun & Bradstreet and need special assistance or an accommodation to use our website or to apply for a position, please send an e-mail with your request to TalentAcquisitionTeam@dnb.com. Determination on requests for reasonable accommodation are made on a case-by-case basis.\\nPlease note that all Dun & Bradstreet job postings can be found at https://dnb.wd1.myworkdayjobs.com/Careers and all communication from Dun & Bradstreet will come from an email address ending in @dnb.com.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "123  The Junior Data Engineer is responsible for the implementation of our client's data mart and customer analytics platform for our marketing services clients. You will be responsible for the technical definition of migrating customer data to the data mart environment, providing platform, database and infrastructure design and implementation. Working closely with lead Data Architect, you will ensure that the solution satisfies business requirements. This position requires an in-depth understanding of software/systems analysis, design and performance principles. You must have experience in conceptual analysis, problem solving, data analysis and programming of technical and business information systems. In addition to a software development background, the successful applicant will have proficient experience with ETL based tools and Star Schema data structures.\\n\\nKey Responsibilities\\nTECHNICAL SOLUTIONS ARCHITECTURE\\nPerform business requirements analysis necessary to design and construct the technical architecture to support the solution. Technical architecture solutions consist of an operational data store, data warehouse, data marts and associated delivery mechanisms.\\nDefine and implement the architectural components including RDBMS, operating system platform, hardware and tools.\\nDevelop data requirements, data maps, transformation rules, and aggregation plans. Define strategies for data extraction, hygiene, acquisition, and loading.\\nWork with the client and Data Architect to implement and continually refine data load best practices. Support overall architecture through operational processes. Recommend changes in development, maintenance and system standards based upon best practice experience.\\nEnsure quality implementation by development and execution of detailed plans, delivery of operational best practices.\\nWork with the client technical team on the definition and planning of system implementation and growth forecasting.\\nDefine and implement monitoring systems to perform load validation, to update and maintain metadata, and to ensure overall data integrity.\\nContinually improve the capabilities, working with the best of breed solutions to develop and enhance delivery capabilities based on new technology.\\nCollaborate with other data warehouse experts to maintain consistency of architecture and best practices technology.\\nDATA WAREHOUSING IMPLEMENTATION\\nExperience and Skills Required\\n3+ years of working experience with relational databases, SQL, data warehousing and Business Intelligence tools.\\n3+ years of practical work experience\\nExperience with Microsoft SQL Server Integration Services (SSIS)\\nComputer science or a related technical BA/BS required, MBA a plus\\nImplementation experience of star schema data warehousing\\nDatabase experience with Oracle or MS SQL Server\\nProficient in the use of database access and connectivity tools (Native, ODBC, JDBC) for both general access and decision support purposes\\nETL tool experience with tools such as Microsoft SSIS, Informatica, and Oracle Warehouse Builder\\nReporting and Analytical experience with tools such as Microsoft Analytical and Reporting Services\\nStrong Microsoft Access and Excel skills a plus\\nNice to have experience with cloud technology such as AWS, Microsoft Azure.\\nKnowledge of database maintenance and administration techniques\\nDesire to work in a high paced start up environment\\nStrong problem-solving skills and the ability to work independently\\nStrong systems analysis, design and programming skills\\nStrong written and verbal communication skills\\nDesired skills include SQL, PL-SQL, T-SQL,\\nFamiliarity with Java, JCL, C, C++, VB Script, Unix Shell, XML\\n\\nbombpYouol                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "124  At Rapid7, you'll join a world class team of thinkers and problem solvers who prioritize individual growth and team collaboration over ego and attitude, while conducting research and building products that solve the world's toughest security challenges.\\nYou will work with Data Scientists, Security Researchers, Threat Analysts and Software engineers to bring security research ideas from conception through to deployment in our products or publication in the community. You will participate in every aspect of research from design and planning, through implementation and testing, to deployment and support.\\nWhat you will do\\nYou will be instrumental in helping our research team move as quickly as possible to solve challenging security problems. You will help design, plan, and implement data analysis tools and systems with peers in small teams, and be part of a larger team working collaboratively to support our research goals: improving our products and supporting the security community by publishing interesting findings and presenting at conferences.\\nSuccessful candidates aren't just good at writing code or designing systems, but have a demonstrated focus on helping the rest of the team be as successful as possible. You should love being challenged by your work, with a desire to ship and iterate on tools, infrastructure and POCs, as well as an emphasis on collaboration, communication, and growth. Our entire team (Data Science, Security Research, Threat Intelligence, Engineering, Product Management) works together closely to ensure the success of our research projects as well as our products, and we're looking for someone who revels in exposure to the entire process of how research tools are planned, built, deployed, used, and supported.\\nWe value attitude and willingness to learn over ego and experience, and want someone who is looking to grow with the team as we continue to grow our product.\\nOur Tools\\nPython / R / Scala\\nJupyter / R notebooks\\nAWS:\\nEMR\\nAthena\\nGlue\\nSagemaker\\nSpark\\nTerraform\\nJenkins\\nYou need to be...\\nFocused on shipping solutions to problems, not just code\\nA strong communicator, able to explain your work at varying levels of detail depending on the audience\\nAble to read and write Python code\\nCommitted to developing high quality tools and systems within reasonable timeframes\\nExperienced with AWS data processing and transformation services\\nComfortable with foundational AWS services (S3, SQS, EC2)\\nAble to help distill research requirements into technical details, including to organize and plan epics, and help others understand the priorities and needs of that work\\nSomeone with a “grab a shovel” attitude, where you're excited to dig in and get your hands dirty working with your peers.\\nAble to initiate and drive projects to completion with minimal guidance\\nWilling to learn, and willing to teach. Everyone brings something new to the team, and we want to learn from you as much as we want to grow you as an engineer and a team member.\\nSomeone with around 4 or more years of experience that encompasses the above. We're flexible, and are looking for the right candidate - not just someone who hits a number.\\nIt would be great if you...\\nKnow how to use git and GitHub, and understand basic branching strategies and pull requests\\nAre familiar with interacting with REST endpoints\\nKnow your way around distributed processing coding tools (spark, pyspark) and concepts (MapReduce, message queueing)\\nHave used Terraform to build up and modify cloud deployments\\nHave experience with Numpy and Pandas\\nKnow how to read (or even write) R, Scala, Java, bash\\nWant to learn more about Data Science and Security Research                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "125  Job Details\\nJob Code\\nJPSC-7797\\nPosted Date\\n08/13/18\\nExperience\\n8 Years\\nPrimary Skills\\nSybase and Oracle internals,performance tuning of Sybase and Oracle,Sybase as a DBMS to Oracle\\nRequired Documents\\nResume\\nOverview\\nRole: Sybase / Oracle\\nWork Location: Boston, MA\\nDuration: 6 Months\\n\\nDevelops large project and business-wide logical data models. Creates physical database designs from logical data models. Maintains database dictionaries and integration of systems through database design. Having 6+ years’ experience.\\n\\nWhat are the top three must-haves you will be looking for in the resumes/interviews for this role?\\nIn-depth understanding of Sybase and Oracle internals\\nExperience with performance tuning of Sybase and Oracle\\nExperience with migration applications currently using Sybase as a DBMS to Oracle.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "\n",
       "[126 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_Boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
