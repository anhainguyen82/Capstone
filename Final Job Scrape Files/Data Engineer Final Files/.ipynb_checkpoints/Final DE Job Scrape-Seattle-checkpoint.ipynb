{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=Seattle%2C+WA&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Strategic Customer Engagements, Product Manager - Pricing Analytics</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree8+ years of work experienceAbility to drive project success in an ambiguous environmentStrong ability to build trust with stakeholdersAbility to communicate with both technical and non-technical partnersExperience building financial models\\n\\nAre you an entrepreneur that thrives in a fast-paced ambiguous environment? Do you want to be involved in strategic deals come to life?\\n\\nIf so, we are looking for a Sr. Product Manager to join the Strategic Customer Engagements team to build scalable tools to support complex deals. The Strategic Customer Engagements Deal Team works with our customers on commercial private opportunities to meet their desired business outcomes while ensuring alignment with AWS business objectives.\\n\\nIn this role, you be responsible for understanding and modeling customer usage patterns, the competitive environment, and the strategy for our largest AWS service. You will work with a data engineer to identify the data sources and metrics needed to support our largest customer facing deals. You will work cross-functionally with our competitive insights team and with technical partners to understand business tradeoffs and develop tools for our team to quickly analyze deals. You will also provide strategic support to model large deals and be a voice in the room during the deal process.\\n\\nThe ideal candidate will be able to work independently in an ambiguous environment. They will understand financial modelling and be able to communicate complex relationships to senior leaders.\\n\\nMBA5+ years of product management experienceExperience working with data engineersExperience working with AWS services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer with testing</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Details\\nJob Code\\nJPSC-7555\\nPosted Date\\n06/13/18\\nExperience\\n7 Years\\nPrimary Skills\\n• 3 + years’ experience with 1 or more Databases like Oracle,SQL Server ( basic SQL Concepts,writing simple to medium PL/SQL queries a must) • 2 + yrs Experience with Java server-side web application technology Spring,Hibernate and/or SpringBoot • 2+ yrs Experience with web services and REST architecture (using or building XML/JSON web/serverside APIs) • 1 + yrs of hands-on coding experience in an Agile based multi-tier application framework and environment.\\nRequired Documents\\nResume\\nOverview\\nRole: Data Engineer with testing\\nLocation: Seattle, WA\\nDuration: 6+ Months\\n\\nJob Description:\\n3 + years’ experience with 1 or more Databases like Oracle, SQL Server ( basic SQL Concepts , writing simple to medium PL/SQL queries a must)\\n2 + yrs Experience with Java server-side web application technology Spring, Hibernate and/or SpringBoot\\n2+ yrs Experience with web services and REST architecture (using or building XML/JSON web/serverside APIs)\\n1 + yrs of hands-on coding experience in an Agile based multi-tier application framework and environment.\\nGood understanding of software development frameworks, terminology\\nCompetent using version control systems such as GIT, SVN, VSS\\nKnowledge of continuous integration and development (CI/CD) methodologies\\nKnowledge/Understanding of RESTful APIs\\nKnowledge/Understanding of Cloud native Platforms like PCF (Pivotal Cloud Foundry)\\nKnowledge/Understanding of Cloud Datastores – In-Memory/Persistent, NOSQL / Relational\\nKnowledge/Understanding of Logging - using tools like SPLUNK.\\n\\nIn the COMMENTS include:\\nLegal name:\\nPhone #:\\nEmail address:\\nDaily Rate:\\nDOB (Date and Month) :\\nSkype ID :\\nLocation (City and State):\\nRelocate:\\nAvailability to start:\\nVisa type and expiration:\\nHiring Status: C2C/W2/1099\\nOpen for CTH:\\nTimeslots for phone conversation:\\nTimeslots for WebEx/video interview :\\n\\nSummary:\\n\\nThanks Regards,\\nSyed Raza\\nDesk Number: 585 - 532 - 7200 Extension 9002\\n687 Lee Road, Suite 250, Rochester, NY 14606\\nEmail : Syed.j@avanitechsolutions.com\\nEmail: syed_j@iic.com\\nGmail hangouts: syedraza199025@gmail.com\\nwww.iic.com\\nwww.avanitechsolutions.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Intelligence Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelors in engineering, science, math, statistics or computer science3+ years of work experience as a business intelligence engineer, data engineer or data scientist role3+ years of experience in SQL programming3+ years of experience in building data warehouses and dimensional modeling3+ years of experience with business intelligence and data visualization tools (e.g. Tableau)3+ years of experience with a modern programming language (e.g. Python, R, Scala etc.)Experience with AWS Suite\\n\\nAmazon Lab126 is an inventive research and development company that designs and engineers high-profile consumer electronics. Lab126 began in 2004 as a subsidiary of Amazon.com, Inc., originally creating the best-selling Kindle family of products. Since then, we have produced groundbreaking devices like Fire tablets, Fire TV, Amazon Echo and Amazon Show. The Amazon Devices group delivers delightfully unique Amazon experiences, giving customers instant access to everything, digital or physical.\\n\\nAre you interested in a fast-paced, high-growth environment with the opportunity to work on business-critical decisions? Amazon Device Accessories is looking for an outstanding Business Intelligence Engineer to join our Operational Excellence Team. We’re looking for someone who can provide insight on KPI’s, understand inferential statistics and advise business teams on how to optimize for profit.\\n\\nAs an engineer on the team, you'll leverage tools and services including Amazon Redshift, Tableau, AWS Glue, AWS Athena, Spark, EMR, Machine Learning and Time Series models to build solutions that deliver data-driven reports, dashboards, and recommendations to high level leadership.\\n\\nYou'll work directly with business leaders and stakeholders to understand different business problems and use cases. You'll work with Finance, Tech and Business teams to identify and consume data sources, transform the data, and build the reports and visualizations needed to meet the requirements. You’ll have the opportunity to get hands on experience with Machine Learning, Time Series Modelling and high impact business analysis.\\n\\nDeveloping this capability will provide insights that are used to lead decision making around product allocation, product effectiveness, productivity analysis and business impact. Consumers of these insights will include Directors, VP’s and SVP’s.\\n\\nOur tenets for analytics team members are as follows:\\nUtilizing the Scientific Method to make tangible business impactMetrics before Messes\\no Ensuring we’re measuring the right business metrics to guide the business\\nForecast or be Last\\no Developing state of the art predictive models for ensuring we’re moving in the right direction\\n\\nRoles and Responsibilities:\\nBuild data solutions using AWS services that deliver data-driven reports, dashboards, and tools.Develop and implement Time Series and Machine Learning Forecast ModelsManage marketing and sales data for the organizationManage ETL pipelines using AWS EMR and SparkDistill problem definitions, models, and constraints from informal business requirements.Provide innovative self-service tools to our customers to self-serve and scale dataFollow established engineering best practices and define new best practices where required.Identify critical metrics/reports that measure product performance, efficiency/effectiveness and create client facing dashboards to facilitate decision making.Collaborate on the design, development, maintenance, and delivery/presentation of forecasting models, metrics, reports, analyses, tools, and dashboardsPerform proactive diagnostic analysis on the various product measures and surface meaningful insights to the leadership team.Collaborate with Data Scientists, Data Engineers and Economists to develop Product Insights on Marketing and Sales data.\\n\\nMasters in engineering, science, math, statistics or computer scienceExperience using AWS services for data analytics (i.e., Athena, Glue, Redshift, EMR, etc.)Experience developing custom ETL solutions using Python and SQLExperience with Tableau Desktop and Tableau ServerStrong written and verbal communications skills. Having the ability to translate scientific findings into business recommendations and outputs.The ability to influence stakeholders through delivering results and earning trustBasic statistical tests (but not limited to) t-tests, chi-square and regressionExpert SQLProficiency in PythonExperience delivering the best Products to customers\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related field or 5+ years industry experience.\\n\\nAt Amazon we strive to be earth’s most customer centric company and data is a foundational component to making this happen. In this role you will be responsible for navigating this wealth of data and turning it into actionable and insightful information for business partners in order to deliver better products and services to our customers. The Devices &amp; Services HR Analytics group provides people metrics along the employee lifecycle for our Devices &amp; Services businesses. Our work is dedicated to empowering leaders and enabling action through data and science. In addition to standard reporting, we seek to leverage analytics to help our leaders focus their efforts in ways that will engage, retain and grow their team members to propel the business forward.\\n\\nWe are now recruiting for an exceptional Data Engineer, Devices &amp; Services HR.\\n\\nRoles and Responsibilities\\nIn this role, you will lead the design and maintenance of an end to end platform which will serve the historical reporting, real-time reporting and predictive analytics needs of the Devices &amp; Services HR org. You are the ideal candidate for this role if you are well-versed in multiple DW, reporting and visualization platforms. You will have hands on experience with fast-growing and evolving datasets of petabyte scale. You will understand the challenges of scaling analytical platforms globally, keeping the data security and regulations in mind. You build scalable systems and solutions. You are a team player, skilled at driving consensus through a data-driven approach and mentoring junior team members. You are eager to learn the latest technologies and know when to apply them.\\n\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.\\n\\n· Demonstrated industry leadership in the fields of Database and/or Data Warehousing, Big Data processing · Advanced knowledge of large data manipulation and data mining using SQL. ·\\nExcellent SQL query performance tuning skillsSolid written and verbal communication skillsAbility to work in a team environment that promotes collaboration.Proven ability to build and support large-scale data solutions serving a global customer baseBe self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty data.Ability to understand business requirements, convert them into technical solutions and deliver software incrementallyProven ability to drive adoption of data engineering best practices in a team of engineers, and ability to mentor junior team members\\nAdvanced knowledge and expertise with AWS technologies such as Redshift, S3, EC2, &amp; EMR.\\n\\nExperience with Big Data technologies, Spark, Hive\\nKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operationsAdvanced knowledge and expertise with Data modelling skills, Advanced SQL with Redshift, MySQL, and Columnar DatabasesAdvanced knowledge of a scripting language such as: R or Python.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BI Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BA/BS in Computer Science, Engineering, Statistics, Mathematics, Finance or related field.3+ years’ experience as a BIE, data scientist, data engineer or similar job function with a technology company.Demonstrated strength in SQL, data modeling, ETL development, and data warehousing.Advanced skills in Excel as well as any data visualization tools like Quicksight, Tableau or Cognos Solutions.Understanding of Finance concepts is a plus.Working knowledge of Python/Java or similar coding languages.Familiarity with AWS solutions such as Redshift, S3.Advanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management.Have ability to independently influence and drive outputs, meet deadlines, and set clear expectations and roadmaps.\\n\\nIf you enjoy creating solutions from scratch that revolve around BI and Analytics then this role is for you.\\n\\nAmazon Web Services seeks an experienced Business Intelligence Engineer (BIE) to join the AWS Finance BI(FinBI) team. The team is made of up Data Engineers, BIEs and tool developers. This team is building several platform solutions for all of AWS Finance to help invent and simplify on behalf of the customers. In this organization every day is Day 1 and no projects are the same. In this role you'll own solution designing, customer engagement and full end to end development on products. Some technologies used in the roll will be S3,Redshift, ETL, ETL automation, ad-hoc reporting with tools like QuickSight and IBM Cognos Analytics, and long term analytical projects that will affect the effectiveness of the FinBI team and the customer. You'll work with multiple AWS Finance Stakeholders and Functions, and will work with multiple sources on a wide range of data technologies developing the next generation of reporting solutions.\\n\\nThe successful candidate will be a self-starter comfortable with ambiguity, with strong attention to detail, and the ability to work in a fast-paced environment. You will consult, design, build and manage analytical projects with your customer in mind. You should have a strong expertise and proven success in the design, creation, management, and business use of extremely large datasets. You should be experienced at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications or reports. Above all you should be passionate about inventing on behalf of your customers while learning new solutions to answer business questions to drive tangible change.\\nDuties &amp; responsibilities for this role will include:\\nThe successful candidate will demonstrate good business acumen, experience in developing reports and conducting analysis, strong communication skills, an ability to work effectively with cross functional teams, and an ability to work in an ever-changing environment.Interfacing with business customers to gather data and metrics requirements, then driving analytic projects which will help solve complex challenges.Design, implement, and support key datasets that provide structured and timely access to actionable business information.Perform deep-dives to find the root causes behind variances of key parameters.Experience working in a very large data warehouse environment and multi data sources.Analyzing data and driving insights related to operation and compliance.Build data pipelines for the customers to self-serve very seamlessly.Investigate and implement new big data technologies to provide automatic resolutions to address stakeholder needs.\\n\\nMBA or Master’s in Computer Science, Engineering, Statistics, Mathematics, Finance or related field.Experience in projects involving complex data sets and high variability.A history of teamwork and willingness to roll up one’s sleeves to get the job done.Excellent communication (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teams.Experience handling confidential and sensitive data.Design and develop data infrastructure to support business growth.\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Business Intelligence Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a Business Intelligence Engineer (BIE) with strong analytical, communication and project management skills to join our team. Working closely with business stakeholders and senior leadership, you will help identify and solve complex language and currency problems and develop metrics and reports to measure our impact on our client’s business. In a typical day, you will work closely with the product management team, retail teams, machine-learning scientists, statisticians, software engineers, and various business groups.\\n\\nAbout you:\\nYou're looking for a career where you'll be able to build, to deliver, and to impress. You look at problems holistically and thrive on the intricate complexity of designing feedback loops and ecosystems. You want to work on projects where you are implementing solutions to real problems that require creative solutions and deep understanding of the problem space.\\n\\nYou challenge yourself and others to constantly come up with better solutions. This highly visible role requires frequent communication with senior leadership in order to help shape and deliver on the product roadmap and requires you to nimbly switch between strategic and tactical initiatives to achieve technical, business, and customer experience goals. You'll be given the unique opportunity to own and drive initiatives across the our Retail as a whole - from algorithmic innovation, all the way down to the datasets that the back-end services consume.\\n\\nAbout us together:\\nWe're going to change the way that we think about supporting our global customer. Along the way, we're going to face seemingly impossible problems. We're going to argue about how to solve them, and we'll work together to find a solution that is superior to each of the proposals we came in with. We'll make tough decisions, but we'll all understand why. We'll be the dream team.\\n\\nThe ideal candidate for this space will be highly quantitative, have great judgment, strong data mining and modeling skills and is comfortable facilitating ideation and working from concept through to execution. You will have demonstrated an ability to manage and develop medium to large-scale data tables, identify requirements and build financial reporting and planning models and tools that are statistically grounded but also explainable operationally, apply technical skills allowing the models to adapt to changing attributes, optimize forecast accuracy and to better understand and mitigate model variance drivers. In addition to building data tables, modeling and technical skills, you will possess strong written and verbal communication skills, strong focus on internal customers and professional demeanor and high intellectual curiosity with ability to learn new concepts and frameworks, algorithms and technology rapidly as changes arise.\\n\\nSome problem spaces we'll be working on:\\n\\nDATAMART - as we release new languages across marketplaces, our business teams will want to understand customer trends and interactions with these new marketplaces. Ideally, we want to enable our business teams to report on the various languages within a marketplace as if those languages were individual businesses. As such, we need to create a DataMart that enables all business metrics to be split by language and also enables business users to execute ad hoc queries to answer questions that we have not currently considered. As we create the DataMart, we will have to consider the scale of data that we will be handling (at the scale of our global retail business) and employ Bigdata techniques to aggregate and manipulate this data. We will need to design the platform to be robust and to seamlessly recover from disaster, should the need arise. Consistency and validation will be primary concerns as we understand that systems fail, specifically systems upon which we rely for signal and we need to protect our business teams from making decisions based upon incomplete information.\\n\\nCUSTOMER EXPERIENCE - as arbiters of the customer experience, we need to understand our customers' experience in their languages of preference. Similarly, given the scope of this initiative, it is clear that we will not be able to translate all content in a single release. As a result, it is critical that we can truly measure the customer experience as a function of our translations (both coverage and quality) throughout their journey within our marketplace. This is further complicated by the fact that our customers receive a unique experience based upon their browse history, so our method of measurement must be considerate of and support such a dynamic experience. Furthermore, in real time and with zero latency, we want to understand when the experience is broken so that we can take appropriate actions. This is going to be a challenge that may make use of the latest Bigdata streaming technologies to provide a real-time data and measurement pipeline.\\n\\nQuestions?\\nYou may already know if you're a fit, but perhaps you're worried about technology and experience requirements? Don't be - we're looking for smart, proven, engineers; if you're the right candidate, we're flexible.\\n\\nBASIC QUALIFICATIONSBS or MS in a quantitative field such as Mathematics, Statistics, Physics, Engineering, Computer Science or EconomicsIndustry experience as a Business Intelligence Engineer or related specialty (e.g. Software Engineer, Data Engineer, and Data Scientist) with extensive professional experience and a proven track record in a role focused on understanding, manipulating, processing and extracting value from large datasets.Experience with statistical analysis, regression modeling and forecasting, time series analysis, data mining, financial analysis, and demand modelingStrong analytical skills with passions on working with structural data setsProficiency with TABLEAU, Microsoft Excel to include making charts, data manipulation, pivot tables, creating macros, and visual basic knowledgeExperience processing, filtering, and presenting large quantities (100K to Millions of rows) of dataAble to write SQL scripts for analysis and reporting (Redshift, SQL, MySQL)Excellent communication skills and the ability to work well in a team.Effective analytical, troubleshooting and problem-solving skills.\\n\\nPREFERRED QUALIFICATIONSExperience in Statistical Software such as R, SAS, SPSS, MINITABAble to write SQL scripts for analysis and reporting (Redshift, SQL, MySQL)Experience using one or more Python, VBA, MATLAB, Java, C++ programming languages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sr Software Engineer</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>eBay is looking for a skilled and dynamic big data engineer to help drive the evolution of data acquisition and delivery in a high-performance, massive-scale analytics environment that powers a rapidly growing business where over 100 million active worldwide users.\\n\\nYou will assist in developing our suite of capabilities that enable rapid development on our platforms for data acquisition, transformation, integration, delivery, and presentation. All applications in this environment are characterized by extremely large data and processing volumes, as such, there is a premium placed on hyper-efficient design leveraging low-cost, highly- scalable infrastructure. You will focus on enhancing our existing, world-class analytics application to improve data availability, data quality, and speed of data. Experience supporting Machine Learning algorithms will also be beneficial.\\n\\nPeople in the team are friendly, highly motivated, and extremely bright. Our team tries to maintain a work climate of professionalism, innovation, career growth, and fun. We provide you with the best opportunity to work in a challenging, highly visible and fast paced environment.\\nJob Requirements\\nProficiency in big data technologies including Hadoop\\nExcellent programming skills - Java/Python/Scala\\nAdvanced SQL expertise with excellent understanding of SET operations\\nWorking knowledge of Hadoop Systems (Hive, MapR, Spark, HDFS)\\nExpertise with Linux File and directory permissions.\\nStrong knowledge of cloud technologies, distributed systems programming\\nOutstanding problem solving skills\\nExcellent communication skills\\nExperience with agile and waterfall project development methodologies\\nExperience with Spark, Storm, Kafka is a big plus\\nWorking knowledge of traditional RDMS systems like Teradata is a plus but not necessary\\nThis website uses cookies to enhance your experience. By continuing to browse the site, you agree to our use of cookies\\nView our privacy policy\\nView our accessibility info\\neBay Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talent@ebay.com. We will make every effort to respond to your request for disability assistance as soon as possible.\\nFor more information see:\\nEEO is the Law Poster\\nEEO is the Law Poster Supplement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Organizational Overview\\n\\nThe SEIU 775 Benefits Group is a family of employee benefit plans negotiated and sponsored by SEIU 775, the labor union for long-term care workers in Washington State &amp; Montana. We are leading the transformation of home care into a thriving career and helping to prevent the coming care crisis. With 10,000 people turning 65 every day in the U.S., there are not enough qualified, trained workers to care for the exponential rise in older adults needing care. Through groundbreaking initiatives, the SEIU 775 Benefits Group, together with its union and employer partners, are ensuring that Washington state home care workers have access to the skills and support they need to stay and grow in the field.\\n\\nThe Details:\\nLocation: Downtown Seattle (remote is not available for this role)\\nWork Schedule: Our typical business hours are 8:30-5:00 Monday-Friday\\nTravel Requirements: None\\nCategory: Full-Time, Exempt, Union\\n\\nThis position is approved through: June 30, 2021 and qualifies for our comprehensive benefits package\\nPhysical Requirements: Must be able to sit for long periods of the day\\n\\nWhat You'll Be Doing:\\n\\nReporting to the Enterprise Data Solutions Manager, the Data Engineer will help solve a variety of technical database challenges and work directly with the Business Analytics and Technology Solutions teams to support the development and maintenance of the organization’s data platform and reporting infrastructure. If you want to make a difference in the lives of home care workers and the seniors and people with disabilities they support, we want to hear from you!\\nPlan and coordinate data migrations between systems;\\nSupport the construction of the Data Warehouse - including the design of Data Marts;\\nWork with business users and analysts to gather business requirements and convert those requirements into high level and low-level designs;\\nDefine views and data marts, based upon business requirements;\\nImprove query performance and tuning of existing data marts;\\nDevelop and maintain a front-end application to allow access to back end tables and data;\\nAPI Gateway articulation for data exchanges internally and externally.\\nMonitor production jobs, maintain, and enhance legacy ETL processes on a regular basis.\\nDevelop and maintain automation of manual data extractions;\\nSupport ETL development and maintenance;\\nDevelop and update process and system documentation;\\nWork closely with the project team including Business Analysts, Architect and Data Developer’s, Project Managers &amp; QA to deliver data capabilities.\\nAttend design meetings and recommend security and data improvements – and will be empowered to work on those prioritized changes.\\nBe open to learning new technologies and new problem domains;.\\nParticipate in code reviews and documentation reviews;\\nBe a contributing member of a cross-functional team in charge of delivering new features and capabilities on an iterative basis;\\nOther duties as assigned by your manager.\\n\\nYou Will Need To Have:\\n5+ years of professional experience as an ETL developer in an Enterprise Data Warehousing environment.\\n3+ years’ Experience with connecting to and posting to API’s for data ingestion and publishing of data sets, including SOAP &amp; REST and related transport technologies (XML/JSON);\\nExperience in all aspects of data warehousing solutions (Database issues, Data modeling, Data mapping, ETL Development, Metadata Management, Data Cleansing, Data Profiling and Data Migration)\\nStrong understanding of Business Intelligence and Data Warehousing principles, approaches, technologies, and architectures - including the concepts, designs, and usage of data warehouses and data marts;\\nExperience designing and customizing data models for data warehouses, supporting data from multiple sources in real time;\\nExperience with Operational Data Stores and Data Governance Routines (Plus).\\nExperience with De-Identification or Encrypting the Sensitive Data.\\nExperience in normalizing and de-normalizing tables and maintaining referential integrity by using triggers and primary and foreign keys;\\nExperience building data solutions on Amazon Web Services, (such as S3, RDS, Redshift and Aurora), or on Microsoft Azure or Google Cloud;\\nExperience with connecting to and posting to API’s for data ingestion and publishing of data sets, including SOAP &amp; REST and related transport technologies (XML/JSON);\\nExperience with other object-oriented development tools such as C#/.Net ,Java, Python;\\nCritical Thinking – Thorough understanding of TSQL standards/techniques and knowing when to implement them to overcome an issue;\\nExperience with Salesforce integration is a plus;\\nExperience working in an Agile/Kanban environment;\\nThe ability to demonstrate strong verbal and written communication skills and interact with all levels of business and IT organizations;\\nA team-oriented mindset, ability to manage multiple priorities, follow a project plan, and meet project delivery dates;\\nStrong organizational ability with excellent decision making, analytical, problem solving, and presentation skills;\\nJudgment and Decision Making — Ability to deal with ambiguity and change;\\nTime Management – Highly self-motivated and delivery focused.\\nIt Will Be Great If You Have:\\nBachelor’s degree in Computer Science or related field\\nThe SEIU 775 Benefits Group provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.\\nRequisition # 1533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Engineer - Turn 10 Studios</td>\n",
       "      <td>Redmond, WA</td>\n",
       "      <td>Redmond</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nEnthusiasm for cars and/or gaming - Our priority is making amazing racing games. The ideal candidate must have enthusiasm for our products and empathy for our customers.\\nEnthusaism for cloud data technology - Our pipelines are fully supported by Azure leveraging things like Data Explorer (Kusto), Data Warehouse, Data Factory, Data Lake, SQL and Power BI. The ideal candidate has a passion for cloud technology and a minimum of 5 years' experience.\\nA drive to develop data insights - Collecting data is the easy part. Helping business leaders and game designers ask the right questions and answering these questions with a relentless attention to details (accuracy) is where the fun begins. The ideal candidate is a meticulous gatekeeper for data and code quality, passionate about generating insights from data, and a strong communicator and collaborator.\\nEnthusiasm for AI/ML or an interest to learn - We don't do science projects, but we have an aspiration to build AI/ML capabilities that generate customer value.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nEvolving our big data pipelines to streamline data collection (measure things) and democratize the consumption of data (generate information).\\nWorking with business leaders and game designers to answer the key questions that enable the team to drive franchise growth and create experiences that thrill customers.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Games. Xbox. Big data. AI/ML. Turn 10 Studios, makers of the award winning, billion-dollar Forza franchise, is searching for a senior engineer to help build our next generation data pipelines. You would be joining a small team of awesome people that move fast, innovate daily, and have fun (we make games!).\\n\\nAnalytics is a crucial part of the business at Turn 10. Understanding user motivations and behaviors enables the team to build fun, engaging racing experiences on PC, Xbox and mobile. The gaming industry is evolving to a GaaS model (Games as a Service) where the most successful studios will learn continuously from data and respond rapidly to customer's needs. This is our challenge. As a leader on the data engineering team, you will be front and center building the platform that will enable the entire studio to quickly learn, adapt and transform our games.\\nResponsibilities\\nYou'll focus on:\\n\\nEvolving our big data pipelines to streamline data collection (measure things) and democratize the consumption of data (generate information).\\nWorking with business leaders and game designers to answer the key questions that enable the team to drive franchise growth and create experiences that thrill customers.\\n\\nExpanding the studios capabilities in AI/ML, building an intelligent cloud for Forza.\\nQualifications\\nWe only have a few requirements:\\n\\nEnthusiasm for cars and/or gaming - Our priority is making amazing racing games. The ideal candidate must have enthusiasm for our products and empathy for our customers.\\nEnthusaism for cloud data technology - Our pipelines are fully supported by Azure leveraging things like Data Explorer (Kusto), Data Warehouse, Data Factory, Data Lake, SQL and Power BI. The ideal candidate has a passion for cloud technology and a minimum of 5 years' experience.\\nA drive to develop data insights - Collecting data is the easy part. Helping business leaders and game designers ask the right questions and answering these questions with a relentless attention to details (accuracy) is where the fun begins. The ideal candidate is a meticulous gatekeeper for data and code quality, passionate about generating insights from data, and a strong communicator and collaborator.\\nEnthusiasm for AI/ML or an interest to learn - We don't do science projects, but we have an aspiration to build AI/ML capabilities that generate customer value.\\n\\nMicrosoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.\\n\\nBenefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Redmond, WA</td>\n",
       "      <td>Redmond</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s degree in Computer Science, Engineering, or closely related field\\n8+ years of experience designing and developing software\\nStrong scripting skills to perform data/file manipulation\\n5+years of experience with data aggregation platforms based on technologies such as SQL, Azure Data Lake, Hadoop, Cosmos, CosmosDB, HDInsights etc.\\nSolid understanding and proven skills in raw and processed stream design, relational database design and dimensional models\\nSolid understanding of event processing including publish/subscribe mechanisms\\nDemonstrated ability to create and ship high quality code by using engineering best practices.\\nHands on experience in big data components.\\nExperience with data warehousing and datamart design and implementation\\nExperience with data security and compliance (PII, PHI, GDPR etc.)\\nStrong understanding and inner workings of metadata management, data lineage, and data governance\\nStrong experience in structured, unstructured, semi-structured data techniques and processes</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBe part of a small, agile team working with experienced engineers that behaves more like a start-up than an established team.\\nLeverage Microsoft BI Suites to provide actionable insights into customer acquisition, and other key business performance metrics\\nEngineer a modern data pipeline to collect, organize, and process data\\nProduce clean, reusable code that is unit tested, code reviewed, and adheres to code standards\\nLead and facilitate a close development partnership with our Multi-tenant Operations team\\nDrive the design and creation of solutions that allow MMD to offer a high level of service – reliable, available, and scalable\\nIdeate and propose new product innovations to meet the needs of our customers.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Microsoft Managed Desktop (MMD) is an important new MS offering. As an engineering organization we take a significant role in the deployment and management of the corporate desktop computing platforms for our customers. We leverage all of Microsoft 365, Enterprise cloud management and security offerings.\\n\\nIt has never been a more exciting time for us to have this offering in the market. MMD is leading with a cloud-first approach and using devices and integrated software offerings to provide great experience for our business customers. We’re working as a startup team in a dynamic engineering organization, building and leveraging key features into the broader Microsoft cloud technology stack like:\\nWindows 10 (Windows as a Service, Windows Defender Suite + Advanced Threat Protection, Autopilot, imageless deployment, etc.)\\nOffice 365\\nCloud Identity and management infrastructure (Azure Active Directory &amp; Intune)\\n\\nMMD is a new end-to-end offering in the market revolutionizing the experience of end users using Windows for their work. You will get an amazing opportunity to extend your knowledge of several important Microsoft products and technical areas listed above.\\nResponsibilities\\nBe part of a small, agile team working with experienced engineers that behaves more like a start-up than an established team.\\nLeverage Microsoft BI Suites to provide actionable insights into customer acquisition, and other key business performance metrics\\nEngineer a modern data pipeline to collect, organize, and process data\\nProduce clean, reusable code that is unit tested, code reviewed, and adheres to code standards\\nLead and facilitate a close development partnership with our Multi-tenant Operations team\\nDrive the design and creation of solutions that allow MMD to offer a high level of service – reliable, available, and scalable\\nIdeate and propose new product innovations to meet the needs of our customers.\\nQualifications\\nRequired Qualifications:\\nBachelor’s degree in Computer Science, Engineering, or closely related field\\n8+ years of experience designing and developing software\\nStrong scripting skills to perform data/file manipulation\\n5+years of experience with data aggregation platforms based on technologies such as SQL, Azure Data Lake, Hadoop, Cosmos, CosmosDB, HDInsights etc.\\nSolid understanding and proven skills in raw and processed stream design, relational database design and dimensional models\\nSolid understanding of event processing including publish/subscribe mechanisms\\nDemonstrated ability to create and ship high quality code by using engineering best practices.\\nHands on experience in big data components.\\nExperience with data warehousing and datamart design and implementation\\nExperience with data security and compliance (PII, PHI, GDPR etc.)\\nStrong understanding and inner workings of metadata management, data lineage, and data governance\\nStrong experience in structured, unstructured, semi-structured data techniques and processes\\n\\nPreferred Qualifications:\\nComfortable learning and growing in a fast paced, start-up environment.\\nProven track record of strong customer advocate and creative problem solver\\nAmbitious, self-motivated, proactive, and results-oriented\\nExperience using scalable data ingestion and transformation systems in batch and near real time environments (λ-lambda architecture)\\nExperience with SQL (T-SQL), relational modeling, and big data tools such as Hive, U-SQL (Scope) or Spark (experience with Spark is a plus)\\nExperience with ETL, data modeling, and working with Business Intelligence systems.\\nExperience with machine learning and predictive analytics is a plus.\\nAzure Data Factory or Integration Services Experiences is a plus.\\n\\nMicrosoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.\\n\\nBenefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Engineer, Sales &amp; Marketing</td>\n",
       "      <td>Seattle, WA 98104</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98104</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nTranslate requirements from various internal business teams to deliveroptimal data structures used to drive insightful, actionable reports and dashboards.\\nPlan and develop complex ETL processes extracting data from source systems and loading cleaned, transformed, and conformed data into our enterprise data warehouse\\nEnsure data integrity by validating against existing reports and source data\\nWork cross functionally with our Data Services and Data Science teams to architect and implement optimized data pipelines\\nCreate dimensional models that meet both immediate analytical and reporting needs and also conform to long-term data strategy\\nScope projects and provide estimates of work to be performed and timelines\\nDevelop ETL specifications, source to target mappings, and other documentation required for ETL development and maintenance</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Avalara is a fast-growing company providing a suite of sales tax SaaS solutions for thousands of businesses worldwide. Our Sales &amp; Marketing team is seeking a skilled and experienced data engineer to build and integrate scalable data pipelines and structures, enabling advanced analytics to accelerate growth and efficiency. You’ll take an active role in empowering marketers and leadership with information-rich data sources, using cutting-edge tools to consolidate data from across the organization and within our data warehouse.\\n\\nResponsibilities:\\nTranslate requirements from various internal business teams to deliveroptimal data structures used to drive insightful, actionable reports and dashboards.\\nPlan and develop complex ETL processes extracting data from source systems and loading cleaned, transformed, and conformed data into our enterprise data warehouse\\nEnsure data integrity by validating against existing reports and source data\\nWork cross functionally with our Data Services and Data Science teams to architect and implement optimized data pipelines\\nCreate dimensional models that meet both immediate analytical and reporting needs and also conform to long-term data strategy\\nScope projects and provide estimates of work to be performed and timelines\\nDevelop ETL specifications, source to target mappings, and other documentation required for ETL development and maintenance\\n\\nRequired:\\n5+ years experience in a data or software engineering role focused on data-heavy applications\\nStrong experience with ETL tools, preferably Talend, Wherescape and SSIS\\nTrack record of optimizing SQL scripts for performance in a data warehouse environment\\nProficiency in dimensional modeling that enables analysts to seamlessly generate insights from data structures\\nStrong communication skills, specifically the ability to translate non-technical stakeholder requirements into data architecture that meets and/or exceeds current needs and long-term analytics goals\\nExperience working cross-functionally to identify ideal source data, share tools, plan data strategy, and collaborate on ETL pipelines that can serve the needs of multiple teams\\nExperience in optimizing scripts and dimensional modeling on big data platforms (Snowflake preferred, or Hadoop or Spark)\\nExperience with a functional/object-oriented scripting language like Python, Java, Scala, etc. for automating jobs\\nAbility to self-direct to seek information and solve problems\\nRigorous attention to detail\\n\\nPreferred:\\nPassion for integrating data and the technology behind it (especially for Big Data technologies)\\nFamiliarity with CRM systems, specifically Salesforce\\nKnowledge of best practices for extracting data from systems such as Adobe Analytics and Eloqua\\nExperience with AWS cloud services such as EC2\\nExperience working in visual analysis tools such as Tableau\\nAbout Avalara\\n\\nAvalara helps businesses of all sizes achieve compliance with transaction taxes, including sales and use, VAT, excise, communications, and other tax types. The company delivers comprehensive, automated, cloud-based solutions designed to be fast, accurate, and easy to use. The Avalara Compliance Cloud® platform helps customers manage complicated and burdensome tax compliance obligations imposed by state, local, and other taxing authorities throughout the world.\\n\\nAvalara offers more than 600 pre-built connectors into leading accounting, ERP, ecommerce and other business applications, making the integration of tax and compliance solutions easy for customers. Each year, the company processes billions of indirect tax transactions for customers and users, files more than a million tax returns, and manages millions of tax exemption certificates and other compliance documents.\\n\\nHeadquartered in Seattle, Avalara has offices across the U.S. and overseas in the U.K., Belgium, Brazil, and India. More information at www.avalara.com\\n\\nAvalara is an Equal Opportunity Employer. All qualified candidates will receive consideration for employment without regard to race, color, creed, religion, age, gender, national orientation, disability, sexual orientation, US Veteran status, or any other factor protected by law.\\n\\n#LI-POST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Engineer Manager</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)3+ years of hands-on experience hiring and managing teams and5+ years as a hands-on Data Engineer or developer7+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics4+ years experience working with Open Source Big Data tools (Parquet, Spark, Hadoop, Presto)\\n\\nAre you inspired by innovation? Do you like to build products that are global? Do you like extending machine learning and NLP in the areas of data engineering and analytics? Answer yes to any of these and you’ll fit right in here at AWS-BTS Engineering. We are a team of doers who work passionately to apply cutting edge advances in technology and software to solve challenges that are unique to AWS and transform our customers’ experiences.\\n\\nAs an Amazon Data Engineering and Analytics leader, you will be working on and building large scale data environments that power our next generation products. We are passionate about building highly scalable real time data engines and seek a leader who can drive the vision, lead data engineers, BI engineers, ML and data scientists to deliver the solutions.\\n\\nThe Data Engineering &amp; Analytics team will build next generation data stores and real time insights by working with multiple data sources, warehouses and platforms. These solutions will power real time web applications and recommendation platforms. We collaborate with product managers, business stakeholders and data platform teams to make feature trade-offs, design, and power new applications. The solutions will include statistical modeling, machine learning, predictive analytics, and data visualization.\\n\\nA successful candidate should have a background in business analytics, data science, data visualization, and data engineering.\\n\\nMaster's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)Experience working with AWS Big Data Technologies (EMR, Redshift, S3)Proven track record of delivering a big data solution with ML and predictive use cases.Experience working with both Batch and Real Time data processing systemsProven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategyExperience providing technical leadership and mentoring other engineers for best practices on data engineeringKnowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operationsAbility to plan roadmap, prioritize tasks, manage dependencies and deliver on time.Excellent understanding of software development life cycle and/or agile development environment with emphasis on BI practices.Ability to create collaborative relationships with partners, stakeholders and customers while managing expectations, managing concerns and risks, and communicating progress.Proficiency in at least one modern programming language such as Java, Scala, or PythonKnowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and cloud computingStrong problem-solving skills and ability to prioritize conflicting requirements.Excellent written and verbal communication skills and ability to succinctly summarize key findings.Meets/exceeds Amazon’s leadership principles requirements for this roleMeets/exceeds Amazon’s functional/technical depth and complexity for this role\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Age/ Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Engineer, Runner Performance Lab</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nCreate and maintain optimal data pipeline architecture for the Run Research Lab\\n Lead and drive the re-structuring of the current data architecture, development and implementation of new data management projects &amp; capabilities, data applications and data cleansing.\\n Collaborate with appropriate data owners and key stakeholders including Research, Assessment, Run Sights and Product Creation to identify and map data from the source environment to the target data environment\\nClean, prepare and optimize data at scale for ingestion and consumption including interfaces between Brooks and third-party systems to enable real time data consumption and preparation for analysis\\n Identify data quality gaps and work with data owners to develop solutions and close gaps. Participate in on-going service delivery, including documentation and ownership of relevant change control requests (including evaluation, test, implementation, and verification).\\nWrite code or use specialized development tools to create product features, enhance and/or customize software components\\n Anticipate, identify and solve issues concerning data management in the Lab to improve data quality.\\nTroubleshoot data issues and perform root cause analysis to proactively resolve product and operational issues\\n Build continuous integration, test-driven development and production deployment frameworks. Drive collaborative reviews of design, code, test plans and data set implementation in support of maintaining data engineering standards. Test developed programs and integration of data from various sources.\\nLiaise with enterprise data teams to ensure that development adheres to organizational architecture guidelines.\\nParticipate in key architectural and technical decisions as they apply to the Run Research Lab\\nCoordinate and conduct application testing (new support packages, releases, functionality and customizing) in close cooperation with the technology team.\\nEngage system owners to filter, size and prioritize business requests and drive towards appropriate decision points.\\nEstablish consistent technical architecture &amp; contribute to development policies, standards and conventions\\nMaintain expert knowledge of development tools, technologies and related delivery methods.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s degree in computer science, statistics or applicable engineering fields with a focus in biomechanics and a research environment a plus.\\n3+ years’ experience with data management tools and industry standard relational database systems preferably in the lab based setting.\\n An expert in database technologies (SQL, Big Data frameworks (Hadoop, Spark), advanced data modelling, cloud platforms (AWS, Azure) as well as real-time (Kafka) and batch data integration frameworks\\nSignificant experience in writing programs to analyze biomechanical data is strongly preferred (matlab, visual 3D, Labview, ATL, Python, Jave, C/C++)\\nAdvanced knowledge and experience in use of biomechanics systems for analyzing running/walking gait (3D mocap systems (Motion Analysis, Vicon, Qualysis), Visual 3D, plantar pressure systems (Novel)\\nExperience in algorithms, especially in the field of AI and machine learning\\nExperienced in Agile/ Scrum methodologies and collaboration with cross functional teams\\nStrong project management and analytical skills\\nAbility to work cross functionally in a fast paced, dynamic environment\\nCurious and open minded; always open for a challenge, inventive, creative. Ability to challenge the status quo – always looking at improving our products and processes while also displaying a willingness to dive into the details.\\nUnwavering demonstration of Brooks’ corporate values: Serve People, Lead Thought, Compete as a Team, Have Integrity, Be Active, Have Fun!\\nA passion for the running enthusiast and active lifestyle\\nTravel 5% of the time</td>\n",
       "      <td>Who We Are:\\nBrooks is a team of passionate people united by a desire to do meaningful work, lead healthy lives and make a difference. We share a focused mission: to inspire everyone to run and be active. That’s it. No distractions—it’s all about the run. Through science, creativity, service, authenticity and connection, we obsess over delivering the best running gear on the planet. We do it our way, with our unique spirit, with a goal of being more relevant to runners than any other brand, day after day and mile after mile. We are determined to innovate, challenging ourselves to lead thought at every turn. Inside these walls and on the roads, tracks and trails, we live and breathe Run Happy, celebrating the positive impact running has on our lives and others. We inject it into all we do because it makes everything better, smarter, more fun and more memorable. Our company culture defines us, bonds us together and creates the conditions for success. It is lived daily as a behavioral expression of our collective set of brand values: Connect with People, Innovate for our Customer, Compete as a Team, Build Trust, Have Fun &amp; Bring Passion, and Be Active. If you’re on our team, it means you’re part of creating something extraordinary. You’re part of Brooks.\\n\\nWe are looking for a passionate Data Engineer on the Run Research team to help us build and create the future of the run. From optimizing performance to assessing injury risk to improving the experience on the run, you’ll help design &amp; build the components, frameworks and libraries to support and scale our analytics programs that will enable our teams to create amazing products and elevate experiences for our runners. In this role, you will work cross functionally, collaborating with the research teams, our product assessment teams and our product creation teams to develop data &amp; analytics capabilities that will allow us to leverage data to inform how we help runners achieve their path to a better self. You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection. The right candidate is excited and passionate about optimizing the Run Research Lab’s data architecture to support our next generation of products and data initiatives.\\n\\nJob Responsibilities\\nCreate and maintain optimal data pipeline architecture for the Run Research Lab\\n Lead and drive the re-structuring of the current data architecture, development and implementation of new data management projects &amp; capabilities, data applications and data cleansing.\\n Collaborate with appropriate data owners and key stakeholders including Research, Assessment, Run Sights and Product Creation to identify and map data from the source environment to the target data environment\\nClean, prepare and optimize data at scale for ingestion and consumption including interfaces between Brooks and third-party systems to enable real time data consumption and preparation for analysis\\n Identify data quality gaps and work with data owners to develop solutions and close gaps. Participate in on-going service delivery, including documentation and ownership of relevant change control requests (including evaluation, test, implementation, and verification).\\nWrite code or use specialized development tools to create product features, enhance and/or customize software components\\n Anticipate, identify and solve issues concerning data management in the Lab to improve data quality.\\nTroubleshoot data issues and perform root cause analysis to proactively resolve product and operational issues\\n Build continuous integration, test-driven development and production deployment frameworks. Drive collaborative reviews of design, code, test plans and data set implementation in support of maintaining data engineering standards. Test developed programs and integration of data from various sources.\\nLiaise with enterprise data teams to ensure that development adheres to organizational architecture guidelines.\\nParticipate in key architectural and technical decisions as they apply to the Run Research Lab\\nCoordinate and conduct application testing (new support packages, releases, functionality and customizing) in close cooperation with the technology team.\\nEngage system owners to filter, size and prioritize business requests and drive towards appropriate decision points.\\nEstablish consistent technical architecture &amp; contribute to development policies, standards and conventions\\nMaintain expert knowledge of development tools, technologies and related delivery methods.\\nRequirements\\nBachelor’s degree in computer science, statistics or applicable engineering fields with a focus in biomechanics and a research environment a plus.\\n3+ years’ experience with data management tools and industry standard relational database systems preferably in the lab based setting.\\n An expert in database technologies (SQL, Big Data frameworks (Hadoop, Spark), advanced data modelling, cloud platforms (AWS, Azure) as well as real-time (Kafka) and batch data integration frameworks\\nSignificant experience in writing programs to analyze biomechanical data is strongly preferred (matlab, visual 3D, Labview, ATL, Python, Jave, C/C++)\\nAdvanced knowledge and experience in use of biomechanics systems for analyzing running/walking gait (3D mocap systems (Motion Analysis, Vicon, Qualysis), Visual 3D, plantar pressure systems (Novel)\\nExperience in algorithms, especially in the field of AI and machine learning\\nExperienced in Agile/ Scrum methodologies and collaboration with cross functional teams\\nStrong project management and analytical skills\\nAbility to work cross functionally in a fast paced, dynamic environment\\nCurious and open minded; always open for a challenge, inventive, creative. Ability to challenge the status quo – always looking at improving our products and processes while also displaying a willingness to dive into the details.\\nUnwavering demonstration of Brooks’ corporate values: Serve People, Lead Thought, Compete as a Team, Have Integrity, Be Active, Have Fun!\\nA passion for the running enthusiast and active lifestyle\\nTravel 5% of the time\\n\\n\\nAt Brooks, we celebrate diversity &amp; equity. We are committed to creating an inclusive environment, and encourage people of all backgrounds, perspectives, experiences, and skills to apply. Brooks is proud to be an equal employment opportunity employer. All employment decisions are made without regard to race, religion, color, national origin, gender, gender identity, the presence of a sensory, physical or mental disability, medical condition, military status, marital status, pregnancy or child birth, sexual orientation, age, genetic information, status as a victim of domestic violence, sexual assault or stalking, political ideology, or any other non-merit based factors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA 98104</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98104</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years of industry experience\\nExperience with high scale, distributed, 24x7 systems and applications\\n5+ years of experience with SQL in both transactional and analytical applications\\nExperience with Linux platforms\\nExperience with AWS data technologies\\nExperience with Ruby, Python, or a similar programming language\\nStrong analytical and design skills\\nCapacity for learning quickly in a fast paced environment and handling multiple tasks simultaneously\\nStrong written and oral communication skills\\n</td>\n",
       "      <td>3+ years of industry experience\\nExperience with high scale, distributed, 24x7 systems and applications\\n5+ years of experience with SQL in both transactional and analytical applications\\nExperience with Linux platforms\\nExperience with AWS data technologies\\nExperience with Ruby, Python, or a similar programming language\\nStrong analytical and design skills\\nCapacity for learning quickly in a fast paced environment and handling multiple tasks simultaneously\\nStrong written and oral communication skills\\n</td>\n",
       "      <td>Define the architecture, technologies, and processes used to create a high scale data processing pipeline\\nWork with engineers to create the tools and infrastructure used to collect, transform, and store data to be used for analytics\\nIdentify and implement analytics tools used by internal stakeholders to engage with data\\nEnsure proper security and access control to analytics data\\nWork collaboratively with the team to deploy, support and operate your services and applications.\\n</td>\n",
       "      <td>BS/MS in Computer Science or Engineering</td>\n",
       "      <td>None Found</td>\n",
       "      <td>doxo is disrupting bill pay. We are founded on the idea to create a simple, secure way to pay all your bills from a single account. We’re a rapidly growing startup looking for a high lead energy Data Engineer with experience building scalable data collection, storage, and analysis systems.\\n\\nMajor Responsibilities\\nDefine the architecture, technologies, and processes used to create a high scale data processing pipeline\\nWork with engineers to create the tools and infrastructure used to collect, transform, and store data to be used for analytics\\nIdentify and implement analytics tools used by internal stakeholders to engage with data\\nEnsure proper security and access control to analytics data\\nWork collaboratively with the team to deploy, support and operate your services and applications.\\nSkills and Qualifications\\n3+ years of industry experience\\nExperience with high scale, distributed, 24x7 systems and applications\\n5+ years of experience with SQL in both transactional and analytical applications\\nExperience with Linux platforms\\nExperience with AWS data technologies\\nExperience with Ruby, Python, or a similar programming language\\nStrong analytical and design skills\\nCapacity for learning quickly in a fast paced environment and handling multiple tasks simultaneously\\nStrong written and oral communication skills\\nEducation\\nBS/MS in Computer Science or Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExperience writing software in one or more languages such as Python, Java, Scala, or Go\\nExperience building production-grade data solutions (relational and NoSQL)\\nExperience with systems monitoring/alerting, capacity planning and performance tuning\\nExperience in technical consulting or customer-facing role\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join SADA as a Sr. Data Engineer!\\n\\nYour Mission\\n\\nAs a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.\\n\\nYou will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.\\n\\nPathway to Success\\n\\n#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.\\n\\nYour success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.\\n\\nExpectations\\n\\nRequired Travel - 30% travel to customer sites, conferences, and other related events\\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.\\nTraining - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Data Engineer Certified\\n\\n[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment\\n\\nRequired Qualifications:\\n\\nMastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExperience writing software in one or more languages such as Python, Java, Scala, or Go\\nExperience building production-grade data solutions (relational and NoSQL)\\nExperience with systems monitoring/alerting, capacity planning and performance tuning\\nExperience in technical consulting or customer-facing role\\n\\nUseful Qualifications:\\n\\nExperience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)\\nExperience with IoT architectures and building real-time data streaming pipelines\\nExperience operationalizing machine learning models on large datasets\\nHihg\\nDemonstrated leadership and self-direction -- a willingness to teach others and learn new techniques\\nDemonstrated skills in selecting the right statistical tools given a data analysis problem\\n\\nAbout SADA\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Engineer - Alexa</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Computer Science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience.Relevant experience in analytics, data engineering, business intelligence, market research or related fieldExperience gathering business requirements, using industry standard business intelligence tool(s) to extract data, formulate metrics and build reportsExperience using SQL, ETL and databases with large-scale, complex datasets\\n\\nInterested in Amazon Alexa? Come work on it. We’re building the speech and language solutions behind Amazon Echo and other Amazon products and services. We’re working hard, having fun, and making history!\\n\\nWe are looking for candidates who want to help shape the future of human-computer interactions. Specifically, we are looking for an outstanding Data Engineer who is looking to work in a new space to help define how we use data to understand customer behavior and satisfaction. In this role, you will develop and support the analytic technologies that give our teams flexible and structured access to their data, including implementation of a BI platform, defining metrics and KPIs, and automating reporting and data visualization.\\n\\nThe successful candidate will be an expert with SQL, ETL (and general data wrangling) and have exemplary communication skills. The candidate will need to be a self-starter, comfortable with ambiguity in a fast-paced and ever-changing environment, and able to think big while paying careful attention to detail.\\n\\nResponsibilities\\n\\nYou know and love working with business intelligence tools, can model multidimensional datasets, and can partner with customers to answer key business questions. You will also have the opportunity to display your skills in the following areas:\\n\\n· · Design, implement, and support a platform providing ad hoc access to large datasets\\n· · Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL\\n· · Manage AWS Resources\\n· · Model data and metadata for ad hoc and pre-built reporting\\n· · Interface with business customers, gathering requirements and delivering complete reporting solutions\\n· · Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions\\n· · Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation\\n· · Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\\n\\nGraduate degree in computer science, business, mathematics, statistics, economics, or other quantitative fieldBoth technically deep and business savvy enough to interface with all levels and disciplines within the organizationDemonstrated ability to coordinate projects across functional teams, including engineering, IT, product management, marketing, finance, and operationsKnowledge of Advanced SQL and a programming languageExperience with data visualization using Tableau or similar toolsExperience with large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologiesProven track record of successful communication of analytical outcomes through written communication, including an ability to effectively communicate with both business and technical teams\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in computer science, mathematics, statistics, economics, or other quantitative field5+ years of relevant work experience in a role requiring application of data modeling and analytic skillsStrong experience with ETL development, data modeling, data warehousing, MySQL, Tableau, and databases in a business environment with large-scale, complex datasetsAdvanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management as requiredSelf-driven, with demonstrated ability to deliver on ambiguous projects as well as projects/requests where the underlying data is incompleteStrong verbal/written communication and presentation skillsExperience in gathering requirements and formulating business metrics for reportingExperienced working in a fast-paced, high-tech environment and comfortable navigating conflicting priorities and ambiguous problems\\n\\nAre you interested in defining the future operating model for Amazon’s NA Capacity Planning? Are you excited by high-visibility, strategic supply chain solutions and like to help drive Amazon's operations planning and forecasting? As a team player, you have an opportunity to work with some of the best Technical Engineers, Program Managers, 160+ FCs located across the network and Business Leaders to design the best fulfillment network on the planet. North America Supply Chain Operations team is looking for a Sr. Business Intelligence Engineer who will work on highly visible strategic projects that will influence business critical decisions.\\n\\nThe ideal candidate will have excellent statistical and analytical abilities, outstanding business acumen and judgment, intense curiosity, strong technical skills, and superior written and verbal communication skills. S/he will be a self-starter, comfortable with ambiguity, able to think big and be creative (while paying careful attention to detail), and enjoys working in a fast-paced dynamic environment. To be successful in this role, you should have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards and using visualization tools, while always applying analytical rigor to solve business problems. In addition to leading the design, development, and management of our analytical tools and reporting, we will also look to this person to provide thought leadership and business analysis support as needed. Your analytics will be used by the Capacity Planning, Capacity Execution, and Demand Planning teams. You will gain knowledge about Amazon’s operations in the capacity planning and forecasting space. You will work as a liaison with different stakeholders (Product Managers, Program Managers, Technical Engineers, Ops Engineering, Finance, and SCOT) in order to diagnose and solve complex business problems by analyzing data and providing recommendations. You will experience a wide range of problem solving situations, strategic to real-time, requiring extensive use of data collection and analysis.\\n\\nJob duties include:\\nDesign, develop and maintain scalable, automated, user-friendly systems, reports, dashboards, etc. that will support our analytical and business needsAnalyze key metrics to uncover trends and root causes of issuesSuggest and build new metrics and analysis that enable better perspective on businessCapture the right metrics to influence stakeholders and measure successDevelop domain expertise and apply to operational problems to find solutionIn-depth research of capacity-related issues, i.e., fullness, space utilization etcWork across teams with different stakeholders to prioritize and deliver data and reportingRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation\\n\\nMBA or Master’s degree in Computer Science, Engineering, Statistics, Mathematics or related fieldFamiliarity with supply chain management concepts including planning, forecasting and optimization gained through work experienceExpert in writing and tuning SQL scriptsExperience working in very large data warehouse environments3+ years of experience in a data engineer or BIE role with a technology companyAdvanced capabilities with productivity software such as Excel and Access</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Assurance\\nAt Assurance we are disrupting the antiquated and inefficient world of insurance and financial services. Our team of world class software engineers, data scientists, and business professionals are modernizing how people obtain and manage their financial life all through our powerful platform ecosystem. We are rapidly growing as we expand our product offerings and global footprint, and this growth continues to present new and exciting challenges as we push our industry into its future. We eliminate waste throughout the industry and calculate the complex into simple, valuable solutions to improve people's lives. We are humble, driven, and committed to improving the lives of millions.\\n\\nAbout the Position\\nAs we build the future of consumer insurance in a modern age, data is at the core of everything that we do. The role requires team members who are adept at building software tools to move and organize data with an approach that is rooted in improving the insights and efficiency of the business. Our team uses a variety of data mining and analysis methods, a variety of data tools, builds and implements models, develops algorithms, and creates simulations. Our Data Engineers design and build the backbone that makes this development possible with no support from engineering (we own our stack end to end). At Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise.\\nTo be successful in this role, you must possess the following:\\nExpertise in modeling data\\nExperience with Spark, Hadoop/EMR, SQL\\nAbility to optimize data access for speed/reliability/velocity as needed by the business\\nComfort with QA’ing your own data, to include ‘menial tasks’ like listening to calls or scrubbing excel files to ensure everything is correct\\nComfort with learning new technologies to help the team explore new solutions to existing problems\\nA drive to move fast and deliver business value\\nExcellent communication ability – you can explain your work in a way that anyone on the team can understand, and you can frame problems in a way that ensures the right question is being asked.\\nBusiness Acumen – you are always eager to understand how the business works, and more specifically, how your work impacts the business.\\nEnthusiastic yet humble – you are excited about the work you do, but you are also humble enough to embrace feedback – you don’t need to be the smartest person in the room.\\nThe following additional experience is desired:\\nCapable of modifying an existing job to add a new field and get it into production within a day.\\nCapable of creating a new data pipeline/job within 2-3 days.\\nYou have a proven ability to drive business results by building the right infrastructure that enables data-based insights. You are comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for enabling the discovery of solutions hidden in large data sets and working with stakeholders to improve business outcomes. We’re growing at a rapid pace, so it’s important that you embrace the opportunity to blaze your own trail. You thrive in a fast-paced environment where priorities can shift rapidly as we corner opportunity. You can work independently, with little oversight or guidance.\\n\\nAt Assurance, we hire experts in their field, and we give them the independence and trust to build based on their expertise. If this sounds like a good fit for you, give us a shout, we’d love to chat!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>----------------\\nRole Description\\n----------------\\n\\nThe Data Engineer is responsible for designing and developing robust, scalable solutions for collecting, analyzing large data sets, creating and maintaining data pipelines, data structures and reports to be used by the revenue organization at Dropbox.\\n\\n----------------\\nResponsibilities\\n----------------\\n\\n\\nUnderstand business processes, applications and how data is gathered; and tie application telemetry to transactional data model.\\nDesign, build and manage data marts to satisfy our growing data needs.\\nDevelop and manage data pipelines at enterprise scale\\nBuild data expertise and own data quality for various data flows\\nLaunch and support new data models that provide intuitive analytics to internal customers\\nDesign and develop new framework and automation tools to enable teams to consume and understand data faster\\nUse your expert coding skills across a number of languages like SQL, Python and Java to support analysts and data scientists\\nInterface with internal data consumers to understand data needs\\nCollaborate with multiple teams in high visibility roles and own the solution end-to-end\\n\\n------------\\nRequirements\\n------------\\n\\n\\n5+ years of SQL (Oracle, AWS Redshift, Hive, etc) experience required, No-SQL experience is a plus.\\n5+ years of Python or Java development experience.\\n5+ years of experience with schema design and dimensional data modeling.\\nHands-on experience working in data warehousing, data architecture and/or data engineering environments at enterprise scale.\\nAbility to analyze data to identify deliverables, gaps and inconsistencies.\\nExperience designing, building and maintaining data processing systems\\nExperience working with visualization tools like Tableau or MicroStrategy\\nCommunication skills including the ability to identify and communicate data driven insights\\nBS or MS degree in Computer Science or a related technical field\\n\\n------------------\\nBenefits and Perks\\n------------------\\n\\n\\n100% company paid individual medical, dental, &amp; vision insurance coverage\\n401k + company match\\nMarket competitive total compensation package\\nFree Dropbox space for your friends and family\\nWellness Reimbursement\\nGenerous vacation policy\\n10 company paid holidays\\nVolunteer time off\\nCompany sponsored tech talks (technology and other relevant professional topics)\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Engineer, Alexa Smart Home</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Computer Science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience.7+ years of professional experience3+ years of relevant work experience in analytics, data engineering, business intelligence, market research or related fieldExperience gathering business requirements, using industry standard business intelligence tool(s) to extract data, formulate metrics and build reportsExperience w/ Python/Shell scripting and Java/Scala codingExperience using SQL, ETL and databases in a business environment\\n\\nAlexa is the Amazon cloud service that powers Echo, the groundbreaking new Amazon device designed around your voice. We believe voice is the most natural user interface for interacting with the home and is fundamental to enabling the dream of the smart, connected home.\\n\\nWe are looking for candidates who want to help shape the future of human-computer interactions. Specifically, we are looking for an outstanding Data Engineer who is looking to work in a new space to help define how we use multi-modal data (voice, mobile, desktop) to understand customer behavior and satisfaction. In this role, you will develop and support the analytic technologies that give our teams flexible and structured access to their data, including implementation of a BI platform, defining metrics and KPIs, and automating reporting and data visualization.\\n\\nResponsibilities\\n\\nYou know and love working with business intelligence tools, can model multidimensional datasets, and can partner with customers to answer key business questions. You will also have the opportunity to display your skills in the following areas:\\n\\nDesign, implement, and support a platform providing ad hoc access to large datasetsInterface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQLModel data and metadata for ad hoc and pre-built reportingInterface with business customers, gathering requirements and delivering complete reporting solutionsOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisionsRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentationContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customersParticipate in strategic &amp; tactical planning discussions, including annual budget processes\\n\\nGraduate degree in computer science, business, mathematics, statistics, economics, or other quantitative fieldExperience in consumer-facing industryBoth technically deep and business savvy enough to interface with all levels and disciplines within the organizationDemonstrated ability to coordinate projects across functional teams, including engineering, IT, product management, marketing, finance, and operationsProven track record of successful communication of analytical outcomes through written communication, including an ability to effectively communicate with both business and technical teamsExpert with SQL, ETL (and general data wrangling)Experience with data visualization using Tableau or similar toolsExperience with AWS technologies including Redshift, RDS, S3, EMR, EMLSelf-starter that is comfortable with ambiguity in a fast-paced and ever-changing environmentAble to think big while paying careful attention to detail.Exemplary communication skills\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Cloud Solutions Architect</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExpertise in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes the full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio.\\nApplication Development: building custom web and mobile applications on top of the GCP stack.\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExcellent written and verbal communication skills with the ability to interface with and communicate complex technical concepts to a broad range of stakeholders.\\nHands-on experience with cloud computing, traditional on-premises and enterprise data-center technologies.\\nExperience working with engineering and sales teams.\\nExperience producing technical assets or writing technical documentation, including, but not limited to, architecture designs and documentation, statements of work, project plans, and working code samples.\\nTime management with the ability to manage multiple streams.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join SADA as a Cloud Solutions Architect!\\n\\nYour Mission\\n\\nAs a Cloud Solutions Architect at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You’re also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.\\n\\nYou will be an established contributor within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures successfully. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.\\n\\nPathway to Success\\n\\n#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.\\n\\nYou will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the solutions architecture or management growth tracks.\\n\\nExpectations\\n\\nRequired Travel - 30% travel to customer sites, conferences, and other related events.\\nCustomer Facing - This is very customer-facing role. You will usually interact with customers on a daily basis. You will participate on calls and onsite customer meetings to qualify consultative engagements with the engineering teams. You will present architecture proposals and code samples to build trust, confidence, and consensus both externally and internally.\\nTraining - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Cloud Architect Certified\\n\\n[https://cloud.google.com/certification/cloud-architect] and/or Google\\nProfessional Data Engineer Certified\\n[https://cloud.google.com/certification/data-engineer], or able to complete one of the above within the first 45 days of employment.\\n\\nRequired Qualifications:\\n\\nExpertise in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes the full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio.\\nApplication Development: building custom web and mobile applications on top of the GCP stack.\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExcellent written and verbal communication skills with the ability to interface with and communicate complex technical concepts to a broad range of stakeholders.\\nHands-on experience with cloud computing, traditional on-premises and enterprise data-center technologies.\\nExperience working with engineering and sales teams.\\nExperience producing technical assets or writing technical documentation, including, but not limited to, architecture designs and documentation, statements of work, project plans, and working code samples.\\nTime management with the ability to manage multiple streams.\\n\\nUseful Qualifications:\\n\\nDirect experience working with a variety of cloud technologies as well as designing and recommending elegant solutions that drive business outcomes.\\nUnderstanding of infrastructure automation, continuous integration/deployment, relational/NoSQL data stores, security, networking, and cloud-based delivery models.\\nAbility to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance and security.\\nThought leadership with the ability to recommend cloud-native approaches to solve customer business and technical challenges.\\nUnderstanding of best practices, design patterns and reference architectures with an uncanny ability to recommend these as needed.\\n\\nAbout SADA\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBS degree in CS or related engineering field\\nExcellent communication and collaboration skills\\nPassion for quality with strong customer empathy and focus\\nStrong intellectual curiosity and passion about learning new technologies</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Senior Software Engineer with total 5+ years’ and 1-2 yrs+ experience in Apache Spark with Scala. Candidate should be able to create data pipelines; handling big data across multiple data sources.\\nDesign, Development, Test, Deploy and maintain as big data systems for enterprise products\\nCoding, participating in Code Reviews, Enhancement discussion, maintenance of existing pipelines &amp; systems, testing and bug-fix activities carried out on an on-going basis\\nNice to have experience in Cloud environment, work experience on Azure is a plus. Knowledge in Azure Data Factory will be a plus\\nCandidate will deliver sprint work as defined by product backlog and prioritized by engineering leadership\\nExperience with Azure Data Services: Azure SQL, BLOB, ADF (Azure Data Factory) and Cosmos DB is desired but not necessary\\nQualifications and other skills:\\nBS degree in CS or related engineering field\\nExcellent communication and collaboration skills\\nPassion for quality with strong customer empathy and focus\\nStrong intellectual curiosity and passion about learning new technologies\\nPreferred skills\\nApache Kafka or Apache Spark\\nData manipulation of large amounts of unstructured data\\nLocation: Seattle, WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sr. Business Intelligence Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in computer science, mathematics, statistics, economics, or other quantitative field5+ years of relevant work experience in a role requiring application of data modeling and analytic skillsStrong experience with ETL development, data modeling, data warehousing, MySQL, Tableau, and databases in a business environment with large-scale, complex datasetsAdvanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management as requiredSelf-driven, with demonstrated ability to deliver on ambiguous projects as well as projects/requests where the underlying data is incompleteStrong verbal/written communication and presentation skillsExperience in gathering requirements and formulating business metrics for reportingExperienced working in a fast-paced, high-tech environment and comfortable navigating conflicting priorities and ambiguous problems\\n\\nAmazon is seeking an exceptional Sr. Business Intelligence Engineer (BIE) to join the Global Outsourcing (GO) Customer Service (CS) team. This is a unique, high visibility opportunity for someone with a passion to dive deep into disparate, large-scale data sets, surface data that provide unique insights to leaders of the GO organization and ultimately have an impact on the direction of our business. The GO CS network consists of numerous business process outsourcing (BPO) vendors operating a large variety of small to large CS sites across traditional and non-traditional BPO centers around the world. The network is supported by an internal group of Vendor Managers who among other responsibilities monitor numerous quality and performance metrics associated with these vendors.\\n\\nThe ideal candidate will have excellent analytical abilities, outstanding business acumen and judgment, intense curiosity, strong technical skills, and superior written and verbal communication skills. S/he will be a self-starter, comfortable with ambiguity, able to think big and be creative (while paying careful attention to detail), and enjoys working in a fast-paced dynamic environment. To be successful in this role, you should have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards and using visualization tools, while always applying analytical rigor to solve business problems. In addition to leading the design, development, and management of our analytical tools and reporting, we will also look to this person to provide thought leadership and business analysis support as needed.\\n\\nThe candidate will need to understand the complexities of a high growth, fast-paced, high performing organization and possess the ability to handle multiple, abstract projects while dealing with constant change. S/he should have a strong desire to achieve results, continuously raise the bar, and demonstrate an ability to effectively manage time, priorities and deliverables.\\n\\nThe primary responsibilities of this role include:\\nDesign, develop and maintain scalable, automated, user-friendly systems, reports, dashboards, etc. that will support our analytical and business needsAnalyze key metrics to uncover trends and root causes of issuesInvent and innovate - Tell the story of business trends, patterns, and outliers through rich visualizationsWork closely with various stakeholders, including GO leadership, to define the information needed and how best to present itSuggest and build new metrics and analysis that enable better perspective on businessWork with GO vendors to identify and resolve issues related to existing reportsDesign and develop weekly, monthly, and quarterly dashboards, scorecards and reporting for the team and present insights to stakeholdersRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age\\n\\nMBA or Master’s degree in Computer Science, Engineering, Statistics, Mathematics or related fieldExpert in writing and tuning SQL scriptsExperience working in very large data warehouse environments3+ years of experience in a data engineer or BIE role with a technology companyAdvanced capabilities with productivity software such as Excel and Access\\nAmazon.com is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Siri - Big Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: Oct 3, 2019\\nWeekly Hours: 40\\nRole Number: 200070238\\nPlay a part in the ongoing AI revolution that is redefining human-computer interactions. Solve some of the most challenging problems in Artificial Intelligence and NLP applied to international languages. Create cutting edge Deep Learning and NLP technologies and deploy them on a global scale. Shape the advancement of our multi-lingual, multi-cultural voice assistant used by millions around the world. Join Siri International at Apple.\\nAs a Data Engineer your responsibility is to support the Data Scientists and ML/NLP Engineers in the team to generate good quality data that is key to drive the development of our Machine Learning applications. Your responsibilities include, but are not limited to: extracting raw data from different sources, processing (cleaning, transformation, feature extraction) data for annotation, model training and evaluation, implementing data storage solutions, and monitoring data quality metrics.\\nKey Qualifications\\nProficiency implementing Big Data applications using MapReduce, Spark, Hadoop, Oozie and Pig.\\nStrong coding skills and experience with at least one high-level programming language (Python, Scala, Java or equivalent).\\nProven record of 3+ years of experience building Big Data pipelines.\\nProficiency with SQL and strong knowledge of relational and non-relational databases.\\nExperience with HBase, Cassandra, Druid, Kafka is a plus.\\nAbility to initiate and drive projects to completion with minimal guidance in a fast-paced dynamic environment.\\nAbility to work cross-functionally and to communicate ideas in a clear and effective manner with engineering and data science teams.\\nAttention to detail.\\nDescription\\nBe ready to make something great when you come here. Dynamic, inspiring people and innovative, industry-defining technologies are the norm at Apple. The people who work here have reinvented and defined entire industries with our products and services. The same passion for innovation also applies to our business practices - strengthening our commitment to leave the world better than we found it. You should join the Apple Siri International Team if you want to help deliver the next amazing Apple product.\\nEducation &amp; Experience\\nBS/MS in CS or related field\\nApple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sr. Big Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in computer science, engineering, mathematics, or a related technical discipline7+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets5+ years of experience in designing and developing data processing pipelines using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)5+ years of experience in designing and developing analytical systemsExperience building large-scale applications and services with big data technologiesExperience providing technical leadership and mentoring other engineers for best practices on data engineeringExpertise in SQL, DB and storage Internals, SQL tuning, and ETL developmentAbility to work and communicate effectively with developers and Business usersStrong organizational and multitasking skills with ability to balance competing prioritiesWorking knowledge of scripting languages such as Python, Perl, etc.\\n\\nAmazon’s Profit Intelligence is seeking a talented Senior Big Data Engineer to join the Profit Intelligence team. We develop software solutions that are revolutionizing Amazon business intelligence through advanced algorithms running on big data technologies. The ideal candidate thrives in a fast-paced environment and relishes working with petabytes of extremely complex and dynamic data. In this role you will be part of a team of high caliber data and software engineers to build data pipelines using big data technologies such as Apache Spark, Hive/Hadoop, and distributed query engines. You should be passionate about working with big data and have the aptitude to incorporate new technologies and evaluate them critically. You must possess excellent business and communication skills and be able to work with business owners to analyze requirements and build solutions. You are a self-starter, has a proven track record of dealing with ambiguity and working in a fast-paced, highly dynamic environment. Working experience of any one of the programming languages such as Java, C#, C++, Scala, etc. is a big plus.\\n\\nMajor Responsibilities:\\nInterface with PMs, business customers, and software developers to understand requirements and implement solutionsCollaborate with both Retail Finance and central FP&amp;A teams to understand the interdependencies and deliverablesDesign, develop, and operate highly-scalable, high-performance, low-cost, and accurate data pipelines in distributed data processing platforms with AWS technologiesRecognize and adopt best practices in data processing, reporting, and analysis: data integrity, test design, analysis, validation, and documentationKeep up to date with big data technologies, evaluate and make decisions around the use of new or existing software products to design the data architecture\\n\\n7+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in a company with large, complex data sources.Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data setsExperience with full software development life cycle, including coding standards, code reviews, source control management, build processes, and testingExperience with AWS technologies (EMR, Dynamo, RDS, Redshift, Athena, S3)Demonstrated strength in data modeling, ETL development, and data warehousingProven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Systems Data Engineer - New Glenn</td>\n",
       "      <td>Kent, WA</td>\n",
       "      <td>Kent</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum of a B.S. degree in Electrical Engineering, Systems Engineering, Computer Science, Computer Engineering, Physics, Mathematics, or other major requiring engineering core courses\\nMinimum of 5+ years of experience with aerospace/ control systems and software\\nExcellent written communication and presentation skills\\nAbility to collaborate across teams and balance priorities\\nThe ability to quickly absorb information in an unfamiliar domain\\nA self-driven nature with the ability to seek out requirements and propose solutions with minimal direction\\nTechnical expertise in data visualization tools (e.g. Tableau, PowerBI)\\nStrong analytic skill set and a high degree of proficiency in data mining\\nMust be a U.S. citizen or national, U.S. permanent resident (current Green Card holder), or lawfully admitted into the U.S. as a refugee or granted asylum.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Work with various subject matter experts to build reports and run analysis for various program and tactical related questions\\nGive voice to the data, talk with people, ensure they understand what it means and it meets their needs\\nAct as a change agent, help engineering teams adopt new more efficient methods of operating within large related data sets\\nDetermine and articulate balance of priorities to enable incremental delivery of needed functionality\\nPerform data mining for valued projects\\nCapture user feedback\\nWork as a flexible contributor in an agile team including building consensus on designs and participating in code reviews\\nParticipate in Strategic Development of technical performance / launch system health monitoring solutions\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\nAs part of a small, passionate, and accomplished team of experts, you will fill a flexible role within a fast pace development team. Your initial focus will be to interface with engineering teams and aid them in managing their data sets. You will be part of the software / process development team and will influence the features within the software tools. You will advocate for the customer in setting software development priorities and will primarily be focused on ensuring the underlying data sets are valid, current, and correct. You will work across the New Glenn program with teams of engineers in fluids, mechanical, electrical, and software subsystem teams to integrate and align data sets. Your work will ensure we have robust configuration control of data used to manufacture and operate New Glenn. We are expecting you to bring engineering analytical skills and experience working the interface between hardware and software. Once this foundation is established, your role will transition into support for building reliable health monitoring and prediction capabilities that will truly enable our vehicles to be reusable. This hands-on position requires a commitment to quality and attention to detail commensurate with safe human spaceflight. This is a rare opportunity to directly impact the future of human space exploration.\\nResponsibilities:\\nWork with various subject matter experts to build reports and run analysis for various program and tactical related questions\\nGive voice to the data, talk with people, ensure they understand what it means and it meets their needs\\nAct as a change agent, help engineering teams adopt new more efficient methods of operating within large related data sets\\nDetermine and articulate balance of priorities to enable incremental delivery of needed functionality\\nPerform data mining for valued projects\\nCapture user feedback\\nWork as a flexible contributor in an agile team including building consensus on designs and participating in code reviews\\nParticipate in Strategic Development of technical performance / launch system health monitoring solutions\\nQualifications:\\nMinimum of a B.S. degree in Electrical Engineering, Systems Engineering, Computer Science, Computer Engineering, Physics, Mathematics, or other major requiring engineering core courses\\nMinimum of 5+ years of experience with aerospace/ control systems and software\\nExcellent written communication and presentation skills\\nAbility to collaborate across teams and balance priorities\\nThe ability to quickly absorb information in an unfamiliar domain\\nA self-driven nature with the ability to seek out requirements and propose solutions with minimal direction\\nTechnical expertise in data visualization tools (e.g. Tableau, PowerBI)\\nStrong analytic skill set and a high degree of proficiency in data mining\\nMust be a U.S. citizen or national, U.S. permanent resident (current Green Card holder), or lawfully admitted into the U.S. as a refugee or granted asylum.\\nDesired:\\nExposure to product lifecycle / configuration management systems, specifically Windchill\\nExperience in multiple coding languages\\nProficiency in scripting languages (e.g. Python)\\nExperience with collaboration tools such as Confluence and JIRA\\nExperience with web based APIs (e.g. REST, SOAP)\\nExperience with micro-service architecture\\nProficiency in database interrogation of SQL and NOSQL databases (e.g. Oracle, MySQL, Neo4J, MongoDB)\\nBlue Origin offers a phenomenal work environment and awesome culture with competitive compensation, benefits, 401K, and relocation.\\n\\n\\nBlue Origin is an equal opportunity employer . In addition to EEO being the law, it is a policy that is fully consistent with Blue's principles. All qualified applicants will receive consideration for employment without regard to status as a protected veteran or a qualified individual with a disability, or other protected status such as race, religion, color, national origin, sex, sexual orientation, gender identity, genetic information, pregnancy or age. Blue Origin prohibits any form of workplace harassment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDevelop data processes for data modeling, mining, reporting and QA\\nEnsure architecture will support the business requirements\\nEmploy a variety of languages and tools (e.g. scripting languages) to integrate data from different systems.\\nRecommend ways to improve data reliability, efficiency and quality.\\nEmploy sophisticated analytics programs, machine learning and statistical methods to prepare data for use in predictive and prescriptive modeling.\\nExplore and examine data to find hidden patterns.\\nAnalyze potential data quality issues to determine the root cause and create effective solutions.\\nOptimize processes involving large data sets to improve performance.\\nWork with stakeholder to understand their business and make recommendations to help solve problems or improve processes.\\nDeliver high quality projects on time and budget in a fast-paced environment.\\nPreparing and presenting technical information to non-technical and highly technical audiences.\\nWorking on multiple projects simultaneously.\\nTraining and supporting others as needed.\\n</td>\n",
       "      <td>Description:\\nCampfire data engineers balance between strategy and execution to deliver best-in-class client service. We don't hire 'report monkeys' - we're looking for bright and curious minds who love to explore data, dig for insights, and mine opportunities for the clients we serve. Our Data Engineers are comfortable operating without oversight to solve problems and deliver value. Client-facing consulting experience is strongly preferred.\\n\\nRequirements:\\nResponsibilities\\n\\n\\nDevelop data processes for data modeling, mining, reporting and QA\\nEnsure architecture will support the business requirements\\nEmploy a variety of languages and tools (e.g. scripting languages) to integrate data from different systems.\\nRecommend ways to improve data reliability, efficiency and quality.\\nEmploy sophisticated analytics programs, machine learning and statistical methods to prepare data for use in predictive and prescriptive modeling.\\nExplore and examine data to find hidden patterns.\\nAnalyze potential data quality issues to determine the root cause and create effective solutions.\\nOptimize processes involving large data sets to improve performance.\\nWork with stakeholder to understand their business and make recommendations to help solve problems or improve processes.\\nDeliver high quality projects on time and budget in a fast-paced environment.\\nPreparing and presenting technical information to non-technical and highly technical audiences.\\nWorking on multiple projects simultaneously.\\nTraining and supporting others as needed.\\n\\nMinimum Qualifications\\n\\n\\nVery strong Problem solving / critical thinking skills\\nDemonstrated experience working directly with and creating data architectures.\\nAdvanced SQL Server programming (e.g. functions, views, stored procedures, parameters) and knowledge of data warehousing best practices.\\nCreating stored procedures, SSIS packages and using other methods to import/translate/manipulate data.\\nWorking experience with job automation (e.g. SQL server agent, Azure Data flow.. etc.).\\nAdvanced knowledge designing, developing, testing, and supporting SSAS technologies, and dimensional modeling.\\nAbility to manage projects, lead teams, document findings, and track action items to closure.\\nReport development experience (Tableau, PowerBI, Excel or other reporting solutions)\\nPerformance Tuning (e.g. indexing, partitioning, optimizing queries)\\n\\nPreferred Qualifications\\n\\n\\nExperience using statistical computer languages (R, Python, SLQ, etc.) to manipulate data and draw insights from large data sets.\\nExperience creating and using advanced machine learning algorithms and statistics and their real-world advantages/drawbacks: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, text mining, social network analysis etc.\\nExperience with distributed data/ cloud computing tools: Azure, AWS, Map/Reduce, MS Cosmos, Hadoop, Hive, Spark\\nExperience with DAX and MDX (eg: Ranking, date changing, data transformations, relating data)\\nMCSE certification preferred\\nKnowledge of best practices for database hardware &amp; software environments and configurations.\\nDegree in Computer, Science, Math, Engineering, MIS or related Fields (Or equivalent experience)\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Software Engineer, Data</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Foursquare:\\nFoursquare is the leading independent location technology platform, powering business solutions and consumer products through a deep understanding of location. Foursquare's business solutions include Pilgrim SDK, Places API, Analytics, Placed powered by Foursquare, and Pinpoint. Together, these products empower brands to analyze trends; measure foot traffic lift; optimize advertising campaigns; and drive deeper engagement via Foursquare's industry-leading developer tools, which have been selected by 150,000 developers including AccuWeather, Apple, Samsung, Microsoft, Snapchat, Tinder, TripAdvisor, Twitter and Uber. Our toolkit also includes our consumer apps Foursquare City Guide and Swarm. Over the past 10 years, we've counted more 13 billion verified signals from people around the world, helping us to keep our dynamic map and models fresh and up-to-date.\\n\\nAbout our Engineering Team:\\nAs a member of Foursquare's engineering team, we want you to bring experience building real products from the ground up. We're passionate about tackling tough challenges in the location space and look for others who like to dive deep into code and help solve hard problems. You should be comfortable running with your own ideas and eager to learn new skills on a bleeding edge platform. We use a variety of tools, technologies, and languages to build software (Scala, Thrift, MongoDB, Memcached, JS/jQuery, Kafka, Pants, Hadoop, MR, Spark) but experience with equivalent ones will do just fine.\\n\\nAs a data engineer, you will own critical pieces of the machine learning and analytics platforms. You will build data processing infrastructure to derive insights from billions of location data points every day. You will collaborate with Product, Engineering, and Data Science teams to create tools and processes to bring research and machine learning models to production.\\n\\nWhat you'll do:\\n\\nArchitect and implement scalable data processing and analytics infrastructure\\nWork with the Data Science team to bring machine learning models into production\\nBuild Hadoop MapReduce and Spark processing pipelines using Java, Python, and Ruby\\nBuild REST APIs for data access by systems across our infrastructure\\nFocus on performance, throughput, and latency, and drive these throughout our architecture\\nWrite test automation, conduct code reviews, and take end-to-end ownership of deployments to production\\n\\nWhat we're looking for:\\n\\nBS/BA in a technical field such as computer science or equivalent experience\\n3+ years of software development experience\\nProficiency in Python, Java, C#, and/or Ruby\\nExcellent communication skills, including the ability to identify and communicate data-driven insights\\nExperience with Hadoop MapReduce, Apache Spark, analytics systems (e.g. OLAP, BI tools), and semi-structured data (e.g. NoSQL, MongoDB, etc.) is preferred\\nExperience with AWS is preferred\\n\\nFoursquare is proud to foster an inclusive environment that is free from discrimination. We strongly believe in order to build the best products, we need a diversity of perspectives and backgrounds. This leads to a more delightful experience for our users and team members. We value listening to every voice and we encourage everyone to come be a part of building a company and products we love.\\n\\nFoursquare is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\n\\nOur Data Engineering team builds and maintains a secure, scalable, flexible and user-friendly analytics hub that allows us to make informed and data-driven decisions. They also construct and curate business-critical data sets that allow us to realize the value of all the data we collect.\\nA Data Engineer utilizes a multidisciplinary approach to providing ETL solutions for the business, combining technical, analytical, and domain knowledge. The perfect applicant for this role has strong development skills, experience transforming and profiling data to determine risks associated with proposed analytics solutions, a willingness to continually interface with analysts in order to determine an optimal approach, and an eagerness to explore data sources to understand the availability, utility, and integrity of our data.\\nWhat you'll own:\\nData pipeline / ETL development:\\nBuilding and enhancing data curation pipelines using tools like SQL, Python, Glue, Spark and other AWS technologies\\nFocus on data curation on top of datalake data to produce trusted datasets for analytics teams\\nData Curation:\\nProcessing and cleansing data from a variety of sources to transform collected data into an accessible and curated state for Analysts and Data Scientists\\nMigrating self-serve data pipeline to centrally managed ETL pipelines\\nAdvanced SQL development and performance tuning\\nSome exposure to Spark, Glue or other distributed processing frameworks helpful\\nWork with business data stewards &amp; analytics team to research and identify data quality issues to be resolved in the curation process\\nData Modeling:\\nDesign and build master dimensions to support analytic data requirements\\nReplacing legacy data structures with new datasets sourced from streaming data feeds from the core product and other operational systems\\nDesign, build and support pipelines to deliver business critical datasets\\nResolve complex data design issues &amp; provide optimal solutions that meet business requirements and benefit system performance\\nQuery Engine Expertise &amp; Performance Tuning:\\nAssist Analytics teams with tuning efforts\\nCurated dataset design for performance\\nOrchestration:\\nManagement of job scheduling\\nDependency management mapping and support\\nDocumentation of issue resolution procedures\\nData Access\\nDesign and management of data access controls mapped to curated datasets\\nLeveraging devops best practices, such as IAC and CI/CD to build upon a scalable and extensible data environment\\n\\nExperience you'll need:\\nStrong experience designing and building end-to-end data pipelines\\nExtensive SQL development experience\\nKnowledge of data management fundamentals and data storage principles\\nData modeling:\\nNormalization\\nDimensional/OLAP design and data warehousing\\nMaster data management patterns\\nModeling trade-offs impacting data management &amp; processing/query performance\\nKnowledge of distributed systems as it pertains to data storage, data processing and querying\\nExtensive experience in ETL and DB performance tuning\\nHands on experience with a scripting language (Python, bash, etc.)\\nSome experience with Hadoop, Spark, Kafka, Impala, or other big data technologies helpful\\n\\nFamiliarity with the technology stacks available for:\\nMetadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\nData management, data processing and curation:\\nPostgres, Hadoop, Hive, Impala, Presto, Spark, Glue, etc.\\nExperience in data modeling for batch processing and streaming data feeds; structured and unstructured data\\nExperience in data security / access management, data cataloging and overall data environment management\\n\\nExperience with cloud services such as AWS and APIs helpful\\nYou’d be a great fit if your current track record looks like this:\\n5+ years of progressive experience data engineering and data warehousing\\nExperience with a variety of data management platforms (e.g. RDBMS (Postgres), Hadoop (CDH, EMR))\\nExperience with high performance query engines (Hive, Impala, Presto, Athena, MPP engines like RedShift)\\nStrong capability to manipulate and analyze complex, high-volume data from a variety of sources\\nEffective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language\\nAbility to problem solve independently and prioritize work based on the anticipated business value\\n\\nQualifications\\n\\nnull\\n\\nAdditional Information\\n\\nAll your information will be kept confidential according to EEO guidelines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Business Intel Engineer III</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BA/BS in Computer Science, Engineering, Statistics, Mathematics, Finance or related field.5+ years’ experience as a BIE, data scientist, data engineer or similar job function with a technology company.Demonstrated strength in SQL, data modeling, ETL development, and data warehousing.Advanced skills in Excel as well as any data visualization tools like Quicksight, Tableau or similar BI tools.Experience of working in very large data warehouse environment and multi data sources.Familiarity with AWS solutions such as Redshift, S3.Advanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management.Have ability to influence and drive program from end to end.\\n\\nAmazon seeks an experienced Business Intelligence Engineer (BIE) to join Enterprise Risk Management and Compliance (ERMC) team. Our mission is to prevent denied parties from transacting with Amazon businesses, including AWS, customers, vendors, sellers, subsidiaries etc. We screen events in billion every day.\\n\\nYou efficiently and routinely deliver the right things. You are seasoned BI. You have broad knowledge on Amazon businesses, data, systems and tools. You have a department-wide view of the analytical solutions that you build, and you consistently think in terms of automating or expanding the results beyond our business. You are a key influencer in your team’s strategy and contribute significantly to team projects. You will drive reporting/analytics projects from end-to-end. You supervise the creation and implementation of BI projects, provide mentor and guidance to team members, help them to achieve their goals.\\n\\nThis role has opportunity to grow to a BI manager in the future.\\n\\nDuties &amp; responsibilities for this role will include:\\n\\nInterface with business customers to gather requirements, drive reporting or analytic projects to help solve complex challenges.\\nDesign, implement, and support datasets that provide structured data for reporting and analysis.\\nDevelop Tableau/QuickSight dashboards across various business teams to drive adoption and increase visibility into key measures of business performance.\\nAnalyze billions of rows of data to find the root causes behind variances and drive business insights.\\nInvestigate and implement new big data technologies to provide automatic resolutions to address business needs.\\nMentor junior team members and help them grow.\\nThe successful candidate will demonstrate good business acumen, experience in developing reports and conducting analysis, strong communication skills, an ability to work effectively with cross functional teams, and an ability to work in an ever-changing environment.\\n\\nMBA or Master’s in Computer Science, Engineering, Statistics, Mathematics, Finance or related field.Experience in projects involving complex data sets and high variability.Working knowledge of statistical methodologies.Experience to conduct complex data analysis and use ML models.A history of teamwork and willingness to roll up one’s sleeves to get the job done.Excellent communication (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teams.Experience to handle confidential and sensitive data.Design and develop data infrastructure to support business growth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>New York Hiring Conference - Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Economics, Finance, Mathematics, Statistics, Engineering).4+ years of relevant experience in one of the following areas: data science, data engineering, business intelligence or business analytics.Strong analytical and problem-solving skills.Expertise in the design, creation and management of large datasets/data models.Expert-level proficiency in writing complex, highly-optimized SQL queries across large datasets.Ability to work with business owners to define key business requirements and convert to technical specifications.Ability to manage priorities simultaneously and drive projects to completion.\\n\\nAmazon Global Finance &amp; Finance Tech teams are coming to New York!\\nWe are hosting an exclusive hiring event for lead engineers in the data space on October 10th &amp; 11th, 2019 – if you are passionate about Big Data, BI systems, Cloud/AWS &amp; ML, and always enjoy a good challenge of highly complex technical contexts, we have the opportunity for you!\\n\\nEven the best analysts’ and scientists’ impact is dependent on having access to high quality, reliable data at scale. We are looking for top data engineers to join various teams within the Finance &amp; Finance Tech space in our Seattle HQ, and the person will be responsible of partnering with our research team to understand data needs, establish/manage a data store, work with teams across multiple functions to identify normative data sources, and build data pipelines for production level systems. As a Data Engineer, you will be owning the technical architecture of BI and Data platforms, working with very large data sets in one of the world's largest and most complex data warehouse environments, and you will work closely with the business and technical teams in analyzing many unique business problems and use creative problem-solving to deliver results. You will work in a fast paced environment with some of the brightest engineers to innovate on behalf of the customer. You should be somebody that is passionate about solving customers’ problems and gets excited about owning infrastructure services that serve critical finance systems. You will also guide the team on software development best practices and set examples by using them in the solutions you build.\\n\\nIn summary, a typical Data Engineer in Amazon works on:\\nArchitecture design and implementation of next generation BI solutions, enabling stakeholders to manage the business and make effective decisions.Designing, planning, and building for secure, available, scalable, stable, and cost-effective data solutions in the various engineering subject areas as it relates to data storage and movement solutions: data warehousing, enterprise system data architecture, data design (e.g., Logical and Physical Modeling), data persistence technologies, data processing, data management, and data analysis.\\n\\nMasters in computer science, mathematics, statistics, economics, or other quantitative field.Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.Experience providing technical leadership and mentoring other engineers for best practices on data engineering.Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.Experience with AWS services including S3, Redshift, EMR and RDS.Knowledge with statistical and/or econometric modeling.Experience in BI/DW as a change leader providing strategic research, recommendations, and implementations.\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Sr Engineer, Data</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>One or more of the following: SQL, Hive, Pig, R, Matlab, SAS, Python, Java, Ruby, C++, Perl, MDX, DAX\\nSQL and SSAS Expert\\nExperience in BI/Analytics and Data Prep tools such as Tableau, PowerBI, CLIQ, Alteryx, SAP Data Hub Modeler, etc.\\nExtensive experience in ETL/ELT Deployment Environments such as on-prem (SSIS, Nifi, etc), cloud-based (Azure Data Factory, AWS Glue, etc) &amp; containerized (Kubernetes)\\nBroad knowledge of TCP/IP, Firewalls, SSH, SSL, SFTP, port forwarding, NTFS Security, ACLs, Least Privilege, Active Directory, LDAP\\nData Warehouse Virtualization Platforms (eg: Denodo)</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Technical System Expertise: Understands system protocols, how systems operate, and data flows. Aware of current technology benefits and trends. Understand the building blocks, interactions, dependencies and tools required to complete software and automation work. In-depth understanding of enterprise data warehouse patterns and technologies. Have experience with \"Big Data\" model paradigms such as MapReduce in order to build a scale out data processing solutions.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>This role is tasked with developing, constructing, testing and maintaining architectures and software in support of data transfer between systems. This is done by aligning architectural capabilities with requirements from the business to engineer data pipelines from data sourcing, ingestion, enrichment, and modeling. The Senior Data Engineer also optimizes existing ETL/ELT processes, builds APIs for data access, and analyzes source systems for optimization. They do this by partnering with Technical Product Managers, Data Scientists, Data Architects, and Business Functional Analysts, to build and deliver analytics solutions with a product mindset. Their experience and knowledge allows them to apply DevOps principals to data engineering processes to ensure reliable, accurate, and complete data pipelines.\\n\\nResponsibilities\\nTechnical System Expertise: Understands system protocols, how systems operate, and data flows. Aware of current technology benefits and trends. Understand the building blocks, interactions, dependencies and tools required to complete software and automation work. In-depth understanding of enterprise data warehouse patterns and technologies. Have experience with \"Big Data\" model paradigms such as MapReduce in order to build a scale out data processing solutions.\\nTechnical Engineering Services: Supports engineering projects by developing software solutions for data and analytics; conducting tests and inspections; preparing reports and calculations. Expected to supervise engineering teams on occasion.\\nInnovation: Presents new ideas which improve an existing system/process/service. Presents new ideas which utilize new frameworks to improve an existing system/process/services. Express new perspectives based on independent study of the industry. Review current company processes to highlight questions that may drive process refinement.\\nTechnical Writing: Maintains knowledge of existing technology documents. Writes basic documentation on how technology works. Contributes clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption at the VP level.\\nTechnical Leadership: Collaborates with technical teams and utilizes system experience to deliver technical solutions. Continuously learns new technologies.\\nTechnology Strategy: Understand current technology that supports business goals. Understand technology trends and how technical investments may be affected by changes within those trends. Identifies risks and mitigation strategies from a technical perspective as it relates to this field.\\nAlso responsible for other Duties/Projects as assigned by business management as needed.\\n\\nQualifications\\nOne or more of the following: SQL, Hive, Pig, R, Matlab, SAS, Python, Java, Ruby, C++, Perl, MDX, DAX\\nSQL and SSAS Expert\\nExperience in BI/Analytics and Data Prep tools such as Tableau, PowerBI, CLIQ, Alteryx, SAP Data Hub Modeler, etc.\\nExtensive experience in ETL/ELT Deployment Environments such as on-prem (SSIS, Nifi, etc), cloud-based (Azure Data Factory, AWS Glue, etc) &amp; containerized (Kubernetes)\\nBroad knowledge of TCP/IP, Firewalls, SSH, SSL, SFTP, port forwarding, NTFS Security, ACLs, Least Privilege, Active Directory, LDAP\\nData Warehouse Virtualization Platforms (eg: Denodo)\\n\\nMinimum Qualifications\\nBachelor's Degree in Computer Science or equivalent\\n\\n\\n\\nCompany Profile\\nAs America's Un-carrier, T-Mobile USA, Inc. (NASDAQ: \"TMUS\") is redefining the way consumers and businesses buy wireless services through leading product and service innovation. The company's advanced nationwide 4G and 4G LTE network delivers outstanding wireless experiences for customers who are unwilling to compromise on quality and value. Based in Bellevue, Washington, T-Mobile USA. Inc. provides services through its subsidiaries and operates its flagship brands, T-Mobile and Metro by T-Mobile. For more information, please visit http://www.t-mobile.com\\n\\nEOE Statement\\nWe Take Equal Opportunity Seriously - By Choice. T-Mobile USA, Inc. is an Equal Opportunity Employer. All decisions concerning the employment relationship will be made without regard to age, race, ethnicity, color, religion, creed, sex, sexual orientation, gender identity or expression, national origin, marital status, citizenship status, veteran status, the presence of any physical or mental disability, or any other status or characteristic protected by federal, state, or local law. Discrimination or harassment based upon any of these factors is wholly inconsistent with our Company values and will not be tolerated. Furthermore, such discrimination or harassment may violate federal, state, or local law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Business Intelligence Engineer (AWS)</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Background in computer engineering or information systems3+ years of relevant work experience as a business intelligence engineer or data engineer role3+ years of experience in SQL programming3+ years of experience in building data warehouses and dimensional modeling3+ years of experience with business intelligence and data visualization tools (e.g. Tableau)3+ years of experience with a modern programming language (e.g., Java, Ruby, Python, etc.)Experience with ETL tools and processes\\n\\nHave questions about this role? Start a chat with the recruiter today!\\n\\ncopy and paste this link into a new window: http://bit.ly/chatnowaws\\n\\nAre you interested in building the data foundation for helping scale the growth of the Solutions Architecture function with Amazon Web Services? In a fast-paced, high-growth environment, making data driven decisions is critical. The Amazon Web Services Solutions Architecture (SA) organization is looking for a Business Intelligence Engineer to join our Solution Architecture Tools Engineering team and better enable our decision making through the use of data analytics.\\n\\nAs an engineer on the team you'll leverage tools and services including Amazon Redshift, Amazon QuickSight, and AWS Glue to build solutions that deliver data-driven reports, dashboards, and tools to internal stakeholders. You'll work directly with customers and stakeholders to understand the different business problems and use cases. You'll work with the rest of the engineering team to identify and consume data sources, transform the data, and build the reports and visualizations needed to meet the requirements.\\n\\nDeveloping this capability will provide insights that are used to lead decision making around resource allocation, program effectiveness, productivity analysis and business impact. Consumers of these insights will include individual SAs, SA and Sales management, and AWS senior and executive leadership.\\n\\nRoles and Responsibilities:\\n\\nBuild solutions using AWS services that deliver data-driven reports, dashboards, and tools.Distill problem definitions, models, and constraints from informal business requirements.Dive deep into source data from various internal systems.Think strategically and analytically about the business, our products, and technical challenges.Follow established engineering best practices and define new best practices where required.\\n\\nExperience using AWS services for data analytics (i.e., Athena, Glue, Redshift, EMR, etc.)Experience developing custom ETL solutions using Python and SQLExperience with Tableau Desktop and Tableau ServerStrong written and verbal communications skills\\nFor more information on Amazon Web Services please visit: http://aws.amazon.com/\\n\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation / Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>This position is full time and based at our Seattle, U.S. office.\\n\\n\\nWHO YOU ARE\\nThe ideal candidate has extensive experience designing and building data solutions with the open-source technology stack including: Hadoop, Spark, Hive, Airflow, or any other. Experience developing and maintaining a commercial quality B2B SaaS platform is highly preferred as well as involvement on AWS (Amazon Web Services). You must enjoy collaborating with team members and acting as scrum master to successfully deliver projects using an agile methodology. Even if you do not possess skill in these technologies and architectures, but you are a knowledgeable, experienced data management profession, please do apply!\\n\\n\\nRESPONSIBILITIES\\nDefine, develop, and maintain the TenPoint7 Cloud platform including the technology components that provide: data ingestion, data integration, data modeling, data processing and data visualization\\nProvide data related consulting to clients for the creation of custom analytics apps or the implementation of packaged analytics apps\\nGather, analyze, and document project functional requirements\\nDefine epics, user stories, tasks, and subtasks for projects\\nDesign and develop new features for the SaaS platform\\nMaintain and enhance existing features of the SaaS platform\\nCreate unit tests, perform unit testing and fix bugs\\nAssist in project management responsibilities including scope, schedule, issues and risks\\nHelp validate that solutions meet requirements and service/quality level agreements\\nCommunicate project status to team members\\nMaintain quality standards of excellence and ensure compliance with TenPoint7 delivery standards and best practices\\n\\n\\nTECHNICAL REQUIREMENTS\\nPython, Java or JavaScript, SQL\\nSpark, Spark is nice to have\\nExperience with AWS, EMR or SageMaker is preferred\\n\\n\\nPERSONAL ATTRIBUTES\\nDrive – determined to work hard and get things done\\nIntegrity – always reliable and professional for our clients and our team members\\nTeam Oriented – Collaboratively create productive, cohesive, intercontinental teams\\nInnovative – solve complex problems in new and unique ways\\nAnalytical – Understand data and all its potential\\nSelf-Reliant and self-confident\\nPersistent and fearless\\nPowerfully passionate\\n\\n\\nQUALIFICATIONS\\nBachelor of Science degree in computer science\\nExperience in back-end web application development\\nExperience in managing projects, and providing clarity and transparency on project status to all stakeholders\\nVery good verbal and written English language with strong communication skills\\n\\n\\nABOUT TENPOINT7\\nTenPoint7 is an Analytics Software-as-a-Service company based in Seattle, WA with a global development office located in Ho Chi Minh City, Vietnam. We deliver high value analytics apps hosted in the cloud that are infused with Data Science based algorithms. We are driven by these 3 simple things: Data, People &amp; Value. If you find interest, please send your resume in English to careers@tenpoint7.com.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Senior Software Engineer - Data Streaming</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Epic is seeking a Senior Data Engineer to help build and manage our big data streaming technologies. You'll be deeply involved with optimizing our existing streaming services, evaluating new technologies, managing cloud infrastructure and more. Our ingestion pipelines peak at over 90GB per minute. If you’re interested in working with data at massive scale, let’s chat.\\nResponsibilities:\\nBuilding and supporting highly available streaming services, written in Java / Scala using Apache Flink\\nDeveloping and operating applications on distributed NoSQL key-value stores such as HBase, DynamoDB or FoundationDB\\nManaging cloud infrastructure launched in AWS via Terraform\\nParticipating in an on-call rotation for Epic Games streaming tech stack\\nEvaluating new technologies and components of the big data ecosystem for inclusion in our environment\\nQualifications:\\nExperience building high throughput Java / Scala streaming services using technologies such as Flink or Spark\\nStrong understanding of distributed message brokers such as Kafka and Kinesis\\nExperience with performance tuning of distributed computing technologies at massive scale\\nExperience working with AWS services such as Kinesis, EC2, EMR, RDS via infrastructure as code technologies\\nExperience working with high throughput, distributed NoSQL databases\\nCapable of independently performing root cause analysis on high throughput systems\\nThis is going to be Epic!\\n#LI2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Legal Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of relevant experience in a business intelligence role, including data warehousing and business intelligence tools, techniques and technology, as well as experience in diving deep on data analysis or technical issues to come up with effective solutions, experience in analytics, business analysis or comparable consumer analytics solutions.Bachelor’s degree in Computer Science, Engineering, Math, Finance, Statistics or related discipline or equivalent industry experience.Excellent knowledge and Expertise with SQL, OLAP, and Relational, NoSQL &amp; Multi-dimensional Databases.Experience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets.Knowledge and direct experience using business intelligence reporting tools like QuickSight or Tableau.Proven ability to look at solutions in unconventional ways and see opportunities to innovate.Excellent verbal and written communication and interpersonal skills to convey key insights from complex analysis in summarized business terms and an ability to effectively communicate with technical teams.\\n\\nAmazon Legal is looking for an outstanding, analytical, and technically skilled Data Engineer to join our Legal Technology team. This position will be responsible for building and supporting business analytics and reporting for the Amazon Legal department as a whole.\\n\\nThis role requires an individual with excellent statistical and analytical abilities, deep knowledge of business intelligence solution and, data engineering practices as well as outstanding business acumen and an ability to work with a variety of teams across Amazon Legal. The successful candidate will be a self-starter comfortable with ambiguity, have a strong attention to detail, an ability to work in a fast-paced environment, and be driven by a desire to innovate in this space. In this role, the right person will be able to revolutionize the department’s monthly reporting processes and develop insightful and meaningful metrics and reports that enable decision making at all levels.\\n\\nPrimary Responsibilities\\nSupport a platform providing secure access to departmental reporting across all areas of legal practice.Interface with business customers and delivering complete BI solutions.Model data and metadata to support ad-hoc and pre-built reporting.Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.Continually improve ongoing reporting and analysis processes, automating or creating self-service options.Develop analytical tools to provide transparency into spend.Create operational scorecards based on key metrics, leveraging multiple different data sources to build a cohesive story.Synthesize and translate complex findings into relevant and actionable insights.\\n\\nExperience with legal related data, such as litigation, legal billing, patents, IP, or related fields.Knowledge of AWS Infrastructure, Redshift, NoSQL databases &amp; associated technology.Candidates with a Ph.D. or MS degree in a relevant field are preferred.\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Data Engineer, Affiliate Marketing</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline3+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasetsExperience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)Knowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and computing\\n\\nDo you want a unique, exciting opportunity to help build something from the ground up?\\n\\nWe are looking for builders who are passionate about front end development to join the Amazon Associates team. Amazon Associates is the world’s largest affiliate marketing program and its success has been built on a rich selection of websites and blogs, all creating original content relevant to products sold on Amazon. We are passionate about building scalable, well-designed software services, and strive to constantly improve our technical foundation and user experience.\\n\\nWe are looking for a talented data engineer to help build/enhance the customer-facing systems. You will own many large datasets, implement new data pipelines that feed into or from critical data systems for Amazon Associates, and your insights will impact millions of Customers, Brands and Associates. You will develop new data engineering patterns that leverage new cloud architectures, and will extend or migrate existing data pipelines to the architectures as needed.\\n\\nYou are an ideal candidate if you are passionate about quality, consistency, maintainability, performance, security and all the other things that make great software great. Join us today to transform the world of advertising and affiliate marketing!\\n\\nMajor Responsibilities\\nBuild data pipelines to feed recommendation models for real-time and large-scale offline use cases.Modelling data and metadata to support ad-hoc and pre-built reporting. Interface with business stakeholders, gathering requirements and developing new datasets for reporting and analytics.Design and implement scalable ETL pipelines in AWS platform to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as service which will have an immediate influence on day-to-day decision making for new programs.\\n\\n4+ years of industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.Knowledge and direct experience using business intelligence reporting tools. (e.g. Tableau, QuickSight etc.)Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data setsExperience working with AWS big data technologies (Athena, EMR, Redshift, S3)Demonstrated strength in data modeling, ETL development, and data warehousingProven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategyExperience providing technical leadership and mentoring other engineers for best practices on data engineeringKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Business Intelligence Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in math, statistics, computer science, or finance or equivalent experience.5+ years of experience as a Data Engineer, BI Engineer, Business/Financial AnalystExperience in data mining (SQL, ETL, data warehouse, etc.) and using databases in a business environment with big data technologies and large-scale, complex datasets.Experience in complex data analysis (model creation, A/B testing, etc.)Verbal/written communication &amp; data presentation skills, including experience to effectively communicate with both business and technical teams.\\n\\nDo you want to change the world? Alexa and Echo are shaping the future of voice recognition and cloud-based content/services. Alexa is the name of the Amazon cloud-based voice service and the brain that powers Echo, the award-winning and groundbreaking Amazon device designed around your voice. Echo connects to Alexa, to provide information, answer questions, play music, read the news, check sports scores or the weather, and more—instantly. It's hands-free, and always ready. All you have to do is ask.\\n\\nTo achieve this, we blend of a variety of disciplines (such as NLP, data mining, machine learning, big data, semantic web, graph stores, cloud computing) in an effort to understand our customers and the things they're excited about. To complement our complex algorithms and extensive data analyses, we create elevated and inspirational mobile and web features across the entire communication experience. We use artificial intelligence, data mining and usability studies to develop new features, and we test them through hundreds of R &amp; D experiments a year. We are also incredibly intent on solving some of the most complex computing problems to be found in industry and academia, and we get to test our solutions in the real world every day. And most importantly, we relentlessly ask: \"What haven't we thought of yet?\"\\n\\nOur BIE duties &amp; Responsibilities will include:\\nDefine and deliver analyses and reports that supports the analytical needs of internal business leaders.Use analytical insights to help increase teams effectiveness against key strategic goals.Define process/methodology to help measure KPIs for the organization and provide an environment where key trends are recognized and acted-upon proactively.Identify and implement new capabilities and best practices to develop and improve automated data analysis processesEnsure clear communication and coordination across the product, engineering and central BI teams. Effectively collaborate with product managers to make critical product decisions and engineering to instrument key features.Quickly build a thorough understanding of Alexa customer base- feature preference, device preference, seasonality, global trends etc.\\n\\nData visualization and dashboarding experience (Tableau preferred)Expert understanding of best practices to handle extremely large volume of dataAbility to create extensible and scalable data schema that lay the foundation for downstream analysisA clear passion for learning new BI skills and techniques independently and continuouslyAbility to prioritize multiple concurrent projects while still delivering timely and accurate resultsExperience working in a lean, successful start-up or on a new product team where continuous innovation is desired and ambiguity is the normProficiency with scripting languages and Unix systems (Python, perl, bash, etc.)Experience in an internet-based company with large, complex data sources.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n4+ years of relevant technical experience, including 2+ years with noSQL databases (MongoDB preferred) as well as experience with SQL\\nStrong Python coding skills\\nExperience developing and implementing ETL architectures with large, complex data sets\\nUnderstanding of database architecture and data lakes\\nDistributing computing (parallel processing, multi-threading) – Hadoop, MapReduce, Spark\\nHands-on experience with web crawling/web scraping required (6+ months)\\nExperience developing APIs\\nExperience with Node.js and familiarity with Machine Learning are pluses\\nStrong quantitative data analysis skills\\nBeyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech\\nCuriosity about anomalies in the data and the ability to identify the business opportunities they represent.\\nStrong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels\\nAzure experience is a plus\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n4+ years of relevant technical experience, including 2+ years with noSQL databases (MongoDB preferred) as well as experience with SQL\\nStrong Python coding skills\\nExperience developing and implementing ETL architectures with large, complex data sets\\nUnderstanding of database architecture and data lakes\\nDistributing computing (parallel processing, multi-threading) – Hadoop, MapReduce, Spark\\nHands-on experience with web crawling/web scraping required (6+ months)\\nExperience developing APIs\\nExperience with Node.js and familiarity with Machine Learning are pluses\\nStrong quantitative data analysis skills\\nBeyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech\\nCuriosity about anomalies in the data and the ability to identify the business opportunities they represent.\\nStrong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels\\nAzure experience is a plus\\n</td>\n",
       "      <td>About us\\n\\nLaunched in October 2018, the Likewise app is the fun, social and incredibly useful way for people to discover, curate, and share recommendations on TV shows, movies, books, podcasts, restaurants, travel and more. Best of all, Likewise helps people quickly find recommendations from their friends, family, and other trusted sources.\\nImagined and backed by Bill Gates’ private office, Likewise is a rare early-stage startup that is thinking big, playing to win, and investing to continue its rapid growth trajectory. If you are passionate about what you do, and want to be a core part of creating a household consumer name, then come talk to us about getting in on the fun!\\nHere's a link to a Geekwire article about us: https://bit.ly/2RuxBlx. And Built In Seattle named us one of Seattle’s 50 Startups to Watch in 2019! https://bit.ly/2VXup3o\\nRole\\nWith the Likewise app launched, we have a lot of fun and creative work ahead of us in making Likewise’s AI into the end-all-be-all for determining the best recommendations to consumers across any category – movies, podcasts, books, restaurants, travel, and more! The work you will be doing is the foundation to making Likewise AI real, and it won’t happen without you. You’ll redefine how AI makes recommendations, and in doing so, change people’s lives for the better. We expect to grow the team as the company grows, and the right candidate will have the potential to lead that growth.\\nObjectives\\nCreate a process that handles our disparate internal and external data sources and automatically converts that unstructured data into structured data to be consumed by machine learning and our product, marketing, and executive teams\\nBuild data process pipelines for new and existing data sources\\nGlean insights and business opportunities from the data, and champion ideas for improvement based on those insights to the product team\\nLead the external data sources collection effort, and creatively identify new, relevant data sources that will positively impact our products and users\\nWork closely with the Data Science team to complete all data needs\\nFind the handful of outliers in massive data sets and define processes to handle them\\nRequirements\\nQualifications\\n4+ years of relevant technical experience, including 2+ years with noSQL databases (MongoDB preferred) as well as experience with SQL\\nStrong Python coding skills\\nExperience developing and implementing ETL architectures with large, complex data sets\\nUnderstanding of database architecture and data lakes\\nDistributing computing (parallel processing, multi-threading) – Hadoop, MapReduce, Spark\\nHands-on experience with web crawling/web scraping required (6+ months)\\nExperience developing APIs\\nExperience with Node.js and familiarity with Machine Learning are pluses\\nStrong quantitative data analysis skills\\nBeyond the technical, strong business thinking is required, including experience or interest in consumer apps/consumer tech\\nCuriosity about anomalies in the data and the ability to identify the business opportunities they represent.\\nStrong communication skills and excitement around championing your great ideas and insights to stakeholders at all levels\\nAzure experience is a plus\\nBenefits\\nWorking here\\nLocated in downtown Bellevue, close to restaurants, shopping, parks and transit, we are proud to offer a competitive benefits package including stock options, health care where we pay 100% of employee premiums, 401(k) plan, commuter benefits, flexible paid time off, and a culture that’s team-based, open, casual and fun. If you’re looking for a rare opportunity to be a part of an innovative, exciting company and become a key member on our team, join us!\\nWe support workplace diversity and do not discriminate on the basis of race, color, religion, gender identity or expression, national origin, age, military service eligibility, veteran status, sexual orientation, marital status, physical or mental disability, or any other protected class.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>DATA ENGINEER I, AWS DATA PLATFORM</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>This position requires a Bachelor's Degree in Computer Science or a related technical field, and 2+ years of meaningful employment experience.\\n\\n2+ years of work experience with ETL, Data Modeling, and Data Architecture.Expert-level skills in writing and optimizing SQL.Experience with Big Data technologies such as Hive/Spark.Proficiency in one of the scripting languages - python, ruby, java or similar.Experience operating very large data warehouses or data lakes.Proven interpersonal skills and standout colleague.A real passion for technology. We are looking for someone who is keen to demonstrate their existing skills while trying new approaches.\\n\\nAmazon Web Services is seeking an extraordinary Data Engineer to join the AWS Data Lake team.\\n\\nOur teams take on some of the hardest scalability, performance, and distributed computing challenges the world. We process trillions of events per month using stream processing techniques (Kinesis), process billions of line items via map reduce (EMR) and handle artifacts through the latest in database technologies (DynamoDB and Aurora). We process big data and provide tools for customers to interactively understand their bills. We also provide the analytics that let customers handle billions of dollars of IT usage and spending. Because we sit at the nexus of all AWS services and interact directly with end-customers, we also work closely across all AWS teams to ensure that we offer a great customer experience.\\n\\nThe AWS Data Lake team's vision is to help customers handle the full life cycle of data at all levels of granularity, simplify data collection, integration, and aggregation of AWS data assets, and provide services (compute, storage, security) to access datasets at scale. We collect and process billions of usage and billing transactions every single day into actionable information in the Data Lake and make it available to our internal service owners to analyze their business and service our external customers.\\n\\nWe are truly leading the way to disrupt the data warehouse industry. We are accomplishing this vision by bringing to bear Big Data technologies like Elastic Map Reduce (EMR) in addition to data warehouse technologies like Redshift to build a data platform capable of scaling with the ever-increasing volume of data produced by AWS services. You will have the ability to craft and build AWS' data lake platform and supporting systems for years to come.\\n\\nYou should have deep expertise in the design, creation, management, and business use of large datasets, across a variety of data platforms. You should have excellent business and interpersonal skills to be able to work with business owners to understand data requirements, and to build ETL to ingest the data into the data lake. You should be an authority at crafting, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data lake. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive growth.\\n\\nAuthoritative in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies.Experience with building data pipelines and applications to stream and process datasets at low latencies.Demonstrate efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.Sound knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures.Knowledge of Engineering and Operational Excellence using standard methodologies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for Data Engineers who has a passion for data and a passion for supplying their clients with that data. You know and love working with analytic tools, can write excellent SQL and Unix scripts and can use your technical skills and creative approaches to solve some unique problems in the BI space, can partner with customers to answer key business questions, and you are an advocate for your customers and you have a sense of ownership. You don’t quit. You should also have the following skills or experiences:\\nBachelors degree in CS or related technical field and 7+ years experience in Data WarehousingExcellent knowledge of SQL and UnixExcellent Knowledge of data warehousing concepts\\n\\nFrom Jeff Bezos’s, 2014 Amazon shareholder letter (http://tinyurl.com/q8zpobu): “Internally, Marketplace was known as SDP for Single Detail Page. The idea was to take our most valuable retail real estate – our product detail pages – and let third-party sellers compete against our own retail category managers.” Today, millions of third party sellers offer products alongside Amazon Retail though the Amazon Marketplace. The Featured Merchant Algorithm (FMA) team owns the system and algorithms that select the offers that are featured on the ‘Add to Cart’ button (aka the ‘Buy Box’) on Amazon’s most valuable retail real estate – the Product Detail Page referenced above. The FMA team’s mission is to ensure that featured offers provide the best possible customer value based on factors including price, availability, delivery options and customer service. Our team leverages sophisticated econometric, machine learning, and big data technologies to help customers to discover the right products at the right prices from millions of trusted sellers billions of times a day. If you are looking for a career-defining opportunity on one of the most customer centric and business impacting teams within Amazon, we’d love to hear from you.\\n\\nAs an Amazon.com Data Engineer you will be working in one of the world's largest and most complex data warehouse environments. You should have deep expertise in the design, creation, management, and business use of extremely large datasets. We are looking for an Data Engineer to help build the next generation of Buy Box algorithms. These new set of algorithms will incorporate the continually changing preferences of our customers and continue to scale with numerous new programs that Amazon is introducing for our customers. You should be expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications. You should be able to work with business customers in understanding the business requirements and implementing reporting solutions. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.You will provide guidance and support for other engineers with industry best practices and direction.\\n\\nDesign, implement and support a platform that can provide ad-hoc access to large datasetsModel data and metadata to support adhoc and pre-built reportingInterface with business customers, gathering requirements and delivering complete BI solutionsTune application and query performance using Unix profiling tools and SQL\\n\\nThis position requires a Bachelor's Degree in Computer Science or a related technical field, and 4+ years of relevant employment experience.Good work experience in using SQL and databases in a business environment.Oracle and Unix skills requiredTune application and query performance using Unix profiling tools and SQLSolid communication skills and team playerScripting languages like perl and Unix shell scriptsExperience must involve design and development of large-scale data structures for business intelligence analytics, using ETL/ELT processes, data modeling, SQL, and Oracle\\nAmazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Data Engineer, Internal Benchmarking</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree in Computer Science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience3+ years of relevant work experience in software development, analytics, data engineering, business intelligence or related IT fields, with 1+ years in data engineeringExperience in writing and optimizing SQL queries in a business environment with large-scale, complex datasetsDetailed knowledge of data warehouse technical architecture, infrastructure components, ETL and reporting/analytic tools and environmentsExperience in data visualization software (Tableau/Quicksight) or open-source projects\\n\\nWould you like to be a Data Engineering and Analytics SME who constantly learns new domains, tools and data sources to identify best practices from across Amazon and propagate them globally?\\n\\nAmazon’s Internal Benchmarking team is looking for a Data Engineer (DE) to join our Seattle-based team to design, manage, and continuously enhance our BEnchmarking Analytics Foundation (BEAF). BEAF is a shared foundation upon which multiple Benchmarking teams develop and deliver a wide variety of analytics applications including metrics generation, metrics correlation, econometric models, knowledge modeling, and many other use cases to help improve process effectiveness, customer experience, supplier experience, and employee and candidate experience. As the member of the BEAF team, you will manage the Redshift/Spectrum/EMR infrastructure and analytics tools, as well as build data pipelines, tools, and reports that enable product managers, analysts, BIEs, solution architects, and executives to design, deliver, and consume benchmarking services. You will work on three fronts: (1) collaborate with the 15+ specialized Benchmarking teams (4-5 of them are most active) to gather requirements for data collection, logging, storage, transforming, analysis, and reporting; (2) when appropriate, work directly with a benchmarking service team to design and develop data pipelines to process structured and unstructured data to support ML and other analytics applications; and (3) design and execute experiments to test new tools and new benchmarking ideas.\\n\\nKey Responsibilities:\\nManage Redshift/Spectrum/EMR infrastructure, and drive architectural plans and implementation for future data storage, reporting, and analytic solutionsDesign data schema and operate internal data warehouses and SQL/NoSQL database systemsDesign, implement, automate, and monitor data pipelinesDevelop Extract-Transform-Load (ETL) jobs and Redshift/Spectrum/EMR jobs to design and generate business metricsOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions\\n\\nGraduate degree in Computer Science, Mathematics, Statistics, Finance, or related technical field1+ years of experience in implementing big data processing technology (Hadoop, etc.)Strong ability to effectively communicate with both business and technical teamsDemonstrated experience delivering actionable insights for a consumer businessCoding proficiency in at least one modern programming language (Python, Ruby, Java, etc.)Experience with AWS technologies including Redshift, RDS, S3, EMR (or equivalent with other cloud-based technologies)Experience and interest in statistical analysis would be highly useful\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Backend Service Engineer</td>\n",
       "      <td>Redmond, WA</td>\n",
       "      <td>Redmond</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n2+ years of relevant software design and development in C# and Java\\nExperience with backend services and have developed backend APIs\\nKnowledge of Javascript and Typescript is preferred\\nKnowledge of the Azure services (Azure Table Storage and Azure Blob Storage) preferred\\nThe ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up\\nSelf-directed and comfortable supporting the data needs of multiple teams, systems and products\\nThe right candidate will be excited by the prospect of optimizing or even re-designing data architecture to support the next generation of products and data initiatives\\nAzure Data Factory and spark experience\\nExperience with Cosmos Database preferred as well as Azure table.\\nExperience with Micro-services and Kubernetes</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nResponsible for expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams\\nSupport software developers, database architects, data analysts and data scientists on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Beyondsoft Consulting, Inc., is a leading, technical solutions and consulting partner. We combine emerging technologies and proven methodologies to tailor elegant solutions that solve complex challenges and empower our customers to accelerate their business goals. Our services include end-to-end support for cloud, digital, data analytics, multi-language translation, and testing.\\n\\nWe are looking for a savvy Data Engineer to join our growing team of analytics experts. If you are passionate about programming and have experience with backend services this could be a perfect role for you. You will work with one of our major clients in the Greater Seattle area.\\nResponsibilities\\nResponsibilities:\\nResponsible for expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams\\nSupport software developers, database architects, data analysts and data scientists on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects\\nQualifications\\nQualifications:\\n2+ years of relevant software design and development in C# and Java\\nExperience with backend services and have developed backend APIs\\nKnowledge of Javascript and Typescript is preferred\\nKnowledge of the Azure services (Azure Table Storage and Azure Blob Storage) preferred\\nThe ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up\\nSelf-directed and comfortable supporting the data needs of multiple teams, systems and products\\nThe right candidate will be excited by the prospect of optimizing or even re-designing data architecture to support the next generation of products and data initiatives\\nAzure Data Factory and spark experience\\nExperience with Cosmos Database preferred as well as Azure table.\\nExperience with Micro-services and Kubernetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Principle Software Engineer - Data Storage, ETL and Analytics</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n8 or more years of professional software engineering experience.\\nAdvanced knowledge and experience with the Hadoop ecosystem: Hadoop, HBase, Spark\\nAdvanced knowledge and experience with Java\\nExperience in developing high performance and scalable systems\\nExperience with Elasticsearch or other Lucene-based datastores, a plus</td>\n",
       "      <td>Gigamon is an enterprise network security company focused on building capabilities that empower our customers to detect and track adversaries in real-time.\\nOur mission is to use the power of information to detect, track and dismantle hackers' means of attacking our customers.\\nOur team has seen all sides of the equation, as attackers and defenders, in addition to the complex engineering required to solve these problems at scale.\\nOur software helps security professionals get an unparalleled view into their networks, perform forensics on security incidents and build effective early warning systems.\\nWe are looking for Data Engineer to help us solve complex search and pattern matching problems at petabyte scale. Our data collection needs to operate in near real-time, our data stores need to scale linearly with our datasets, our search needs to perform sub-second matches, our classifiers and behavior analytics need to operate over streaming data sets, and our pattern matching needs to support a variety of operators, window functions, and custom intersection semantics. We work with a number of data storage and processing systems, including MySQL, Postgres, Hadoop, HBase/Phoenix, Spark, Elasticsearch, a variety of AWS data services as well as some proprietary systems.\\n\\nIf you are passionate about building robust, high-scale system, working with large data sets and protecting public and private organizations from today's ever increasing cyber threats, then Gigamon might be the place for you.\\nCore Job Role\\nDesign, build, test and deploy scalable systems to store, process and retrieve high-rate event streams.\\nBuild systems and processes for ETL, enrichment, alerting, and indexing high-rate event streams.\\nSupport the Operations team in capacity planning and performance tuning for large scale systems.\\nImplement processes for applying data-analysis algorithms to event-streams.\\nAbout us\\nFlexible work and vacation schedule\\nCompetitive pay and benefits\\nWe put great emphasis building well tested and stable solutions\\nJob Requirements\\n8 or more years of professional software engineering experience.\\nAdvanced knowledge and experience with the Hadoop ecosystem: Hadoop, HBase, Spark\\nAdvanced knowledge and experience with Java\\nExperience in developing high performance and scalable systems\\nExperience with Elasticsearch or other Lucene-based datastores, a plus\\n\\nEmployment at Gigamon is contingent on meeting eligibility requirements which may include additional security screening on being able to obtain a government sponsored security clearance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About New Engen\\n\\nNew Engen helps companies and marketing teams perform at their peak. Our powerful\\nmachine learning and AI technology solution fuels growth at companies across\\nindustries, geographies and levels of maturity.\\n\\nWe are focused on helping our partners achieve massive scale of new customer value\\nacross all digital marketing platforms. We are results driven, committed, iterative and\\ntransparent. Our teams of expert Marketers, Application Developers and Data Scientists\\nhelp tackle some of the most complex digital marketing challenges.\\n\\nAt New Engen we don't just accept difference—we celebrate it, we support it, and we\\nembrace it for the benefit of our employees, our partners, and our community. We are\\ncommitted to equal employment opportunity and diversity.\\n\\nAbout the Role\\n\\nWe are seeking a Data Engineer to execute on our vision for building scalable and\\nreliable data infrastructure for a high-growth, data reliant business. You will create\\nsomething new, using new technology which sets us up for future growth and success.\\n\\n-----------------------------\\nWe'd love to hear from you if\\n-----------------------------\\n\\n\\nYou want a chance to create and optimize data pipelines for large amounts of data\\nYou're committed to choosing the best technology and strategy for data gathering, storage and retrieval\\nYou love building reusable data stores for a rapidly growing company\\nYou are comfortable collaborating with people inside and outside our organization\\nYou have dedication to your customers and want to make a difference\\nYou do not have an ego and enjoy working with others\\n\\n--------------\\nQualifications\\n--------------\\n\\n\\n2+ years of experience implementing data solutions including data modeling, data warehousing, ETL, and analytics\\n3+ year of experience in SQL\\n2+ years programming experience coding with Python\\nSome experience with Java or other Object-Oriented Languages\\nSome experience with AWS technologies\\nDesire to build complex data structures that serve varying needs\\nA real passion for keeping up with new technology and scalability\\n\\n---------------------------------\\nExtras that Will Give you an Edge\\n---------------------------------\\n\\n\\nYou've worked in a fast-paced agile environment\\nYou have experience with distributed data processing (Apache Hadoop, Spark, Hive, Presto)\\nExperience with RESTful web services\\nKnowledge of streaming technologies (Kinesis, Kafka, Spark Stream)\\nYou have worked with NoSQL solutions (Cassandra, HBase, DynamoDB, Bigtable)\\nYou enjoy challenges and jump at the chance to create something new\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Data Engineer, Amazon Devices</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar.5+ years of experience with data warehouse technical architectures, ETL/ ELT, reporting/analytic tools and, scripting.5+ years of demonstrated quantitative and qualitative data experience with data modeling, ETL developmentKnowledge of data modeling and experience SQL with Redshift, Oracle, MySQL, and Columnar DatabasesExperience managing competing priorities simultaneously and driving projects to completion\\n\\nThe Amazon Devices team designs and engineers high-profile consumer electronics, including the best-selling Kindle family of products. We have also produced groundbreaking devices like Fire tablets, Fire TV, Amazon Dash, and Amazon Echo.\\n\\nWhat will you help us create?\\n\\nThe Team: How often have you had an opportunity to be a founding member of a team that is solving a significant problem through innovative technology? Would you like to know more about how we are envisioning the use of data analytics, machine learning, AI and linear programming to solve these problems? If this sounds intriguing, then we’d like to talk to you about a role on a new Amazon team that's tackling a set of problems requiring significant innovation and scaling.\\n\\nWe are seeking a Data Engineer with strong analytical, communication and project management skills to join our team. This role will be a key member of a Science and Data technology team based in Seattle, WA. Working closely with business stakeholders, software development engineers and scientist colleagues, you will design, evangelize, and implement state-of-the-art solutions for never-before-solved problems, helping Amazon Device to provide customer great products and keep the data secure. You will work with the most complicated data environment, employ right architecture to handle big data and support various analytics use cases, including business reporting, production data pipeline, machine learning, optimization models, statistical models, simulation, etc. Your work will have a direct impact on the day-to-day decision making in the Amazon Devices Sales &amp; Operations Technology, and end customers.\\n\\nYou are an individual with outstanding analytical abilities, excellent communication skills, good business understanding, and technically savvy. The successful candidate will be an analytical problem solver who enjoys diving into data, is excited about solving ambiguity problems, can multi-task, and can credibly interface between technical teams and business stakeholders.\\n\\nDesire to create and maintain data warehouse systemsExperience with big data technologiesMaster's degree in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, or EngineeringExperience with AWS services including S3, Lambda, EMR, RDS, Data-pipeline and other big data technologiesExperience with scripting (Python experience is a strong plus).Proficient in the composition of Advanced SQL (analytical functions) and query performance tuning skillsAbility to interact, communicate, present and influence with both business and technical teamsA self-starter who loves data and who enjoys spotting the trends in it!\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age\\n\\n#d2ctech tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Sr. Data Engineer, Ads Partner Network</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering).5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.5+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.5+ years of experience in scripting languages like Python etc.\\n\\nAmazon Advertising is dedicated to driving measurable outcomes for brand advertisers, agencies, authors, and entrepreneurs. Our ad solutions—including sponsored, display, video, and custom ads—leverage Amazon’s innovations and insights to find, attract, and engage intended audiences throughout their daily journeys. With a range of flexible pricing and buying models, including self-service, managed service, and programmatic ad buying, these solutions help businesses build brand awareness, increase product sales, and more.\\n\\nAs the Senior Data Engineer on our team, you will lead the development of our data platform and put together the Ads, content, partner, seller and customer engagement data into one scalable design to enable the best possible customer experiences. You will also communicate effectively with engineers, partners, leaders and a broad set of stakeholders.\\n\\nA successful engineer in this role is:\\nHighly analytical: You solve problems in ways that can be backed up with verifiable data. You focus on driving processes, tools, and statistical methods which support rational decision-making.Technically fearless: You aren't satisfied by performing 'as expected' and push the limits past conventional boundaries. Your dial goes to '11'.Team obsessed: You help grow your team members to achieve outstanding results. You foster the creative atmosphere to let engineers innovate, while holding them accountable for making smart decisions and delivering results.Humbitious: You’re ambitious, yet humble. You recognize that there’s always opportunity for improvement and use introspection and feedback from teammates and peers to raise the bar for your team.Engaged by ambiguity: You're able to explore new problem spaces with unique constraints and thus non-obvious solutions; you’re quick to identify any gaps in the team and the right person to fill them to best deliver value to customers.\\nYour responsibilities in this role will include:\\nLeading architecture design and implementation of next generation BI solutionsManaging AWS resources including EC2, RDS, Redshift, Kinesis, EMR, Lambda etc.Mentoring and developing other DE's and BIE's.Building and deliver high quality data architecture and pipelines to support business analyst, data scientists, and reporting needs.Interfacing with other technology teams to extract, transform, and load data from a wide variety of data sources.Continually improving ongoing reporting and analysis processes, automating or simplifying self-service support for customers.Designing and building systems from scratch.\\n\\nDemonstrated strength in data modeling, ETL development, and Data warehousing. Data WarehousingExperience with Redshift,Experience with AWS services including S3, Redshift, EMR, Kinesis and RDS.Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.).Experience in working and delivering end-to-end projects independently.Knowledge of distributed systems as it pertains to data storage and computing.Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.Experience providing technical leadership and mentoring other engineers for best practices on data engineering.Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.Masters in computer science, mathematics, statistics, economics, or other quantitative field\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Data Engineering Manager</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Degree in Computer Science, Engineering, Mathematics, or a related field and 7+ years industry experience2+ years of hands-on experience hiring and managing teams of Data Engineers and 5+ as a hands-on Data EngineerKnowledge of professional software engineering practices &amp; best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operationsProficiency with at least one Object Oriented language (e.g. Java, Python, Ruby)Highly proficient in SQL and knowledgeable about data warehousing concepts.Working knowledge of software development methodologies like AgileStrong customer focus, ownership, urgency and drive.Excellent communication skills and the ability to work well in a team.Effective analytical, troubleshooting and problem-solving skills.\\n\\nWhat is the team?\\nThe Workforce Staffing Research &amp; Business Intelligence team is your opportunity to make an impact across multiple layers of the company and the hourly workforce as a whole. This team will lead and influence programs that will enable Amazon to scale more efficiently, while also providing a unique voice for the hourly labor population within the United States and Canada. Amazon’s mission is to be the most customer centric company in the world. The Workforce Staffing (WFS) Organization is on the front line of that mission by hiring hundreds of thousands of hourly associates across multiple types of roles and businesses. To drive the continued scale of Amazon’s labor needs within a constrained employment environment, and to improve the candidate/employee experience within Amazon’s operation, Amazon is investing in its Workforce Staffing Research &amp; Business Intelligence programs.\\n\\nWhat is the role?\\nAs a Data Engineering Manager you will be leading a team of Data Engineers and SDEs and will play a thought leadership role in our team – the team will look to you for advice on data and business issues facing them. You work very efficiently and routinely deliver the right things. You will have a company-wide view of the Data Engineering solutions that you build, and you will consistently think in terms of automating or expanding the results company-wide. This high impact role will have an opportunity to lead a team to help design and build our data infrastructure and work with emerging technologies such as Redshift and associated AWS cloud services while driving business intelligence solutions end-to-end: business requirements, workflow instrumentation, data modeling and ETL. He/she should be an expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data lake and into end-user facing applications. The role requires someone who loves data, understands enterprise information systems, has a strong business sense, and can lead a team to put these skills into action.\\n\\nGraduate degree in Computer Science, Engineering or related technical fieldExperience building data products incrementally and integrating and managing datasets from multiple sourcesExperience with AWS Tools and Technologies (Redshift, S3, EC2)Experience providing technical leadership and mentor other engineers for data engineering best practicesAdvanced knowledge and expertise with Data modelling skills, Advanced SQL with Oracle, MySQL, and Columnar DatabasesDemonstrated industry leadership in the fields of Database and/or Data Warehousing, Data Sciences and Big Data processing\\nAmazon.com is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nAt least 5-years of experience as a software development or data engineer\\nAt least 3-years of experience with SQL, Python and/or other data collection tools &amp; reporting\\nExperience with Pyspark or Scala is necessity (Databricks or Spark).\\nAdvanced knowledge and skills with Azure, or similar cloud platforms.\\nExcellent collaboration skills to work on a team as well as independently (be self-reliant and resourceful)\\nExcellent organization skills and able to multi-task and detailed oriented\\nExcellent verbal and written communication skills (must be able to write clear and concise emails for any audience, etc.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Who is Blueprint?\\n\\nBlueprint Technologies is a group of solution minded thinkers changing the face of Technology in Bellevue, WA. We follow a Mission, Vision, and Core Values that allow us to function as a collaborative unit.\\n\\nWhat are our Solutions?\\n\\nBlueprint is a technology solutions firm that connects strategy, product and delivery. We help companies digitally transform. We have a special focus in cloud and infrastructure, data platform and engineering, data science and analytics, organizational modernization and customer experience optimization.\\n\\nWhy you want to be a part of Blueprint?\\n\\nWe are innovators. Motivators. Thought provokers. And coffee drinkers. Our collective backgrounds bring diverse perspectives that enable us to consistently think differently. Our people are our solutions. We want you to bring your biggest and best ideas to help positively impact our culture, clients and the community around us. We believe in the importance of a healthy and happy team, which is why our benefits include full medical, dental and vision coverage, as well as paid time off, 401k, paid volunteer hours and tuition reimbursement.\\n\\nBlueprint is looking for Data Engineer to join us as we build cutting-edge technology solutions!\\n\\nQualifications:\\n\\nAt least 5-years of experience as a software development or data engineer\\nAt least 3-years of experience with SQL, Python and/or other data collection tools &amp; reporting\\nExperience with Pyspark or Scala is necessity (Databricks or Spark).\\nAdvanced knowledge and skills with Azure, or similar cloud platforms.\\nExcellent collaboration skills to work on a team as well as independently (be self-reliant and resourceful)\\nExcellent organization skills and able to multi-task and detailed oriented\\nExcellent verbal and written communication skills (must be able to write clear and concise emails for any audience, etc.\\n\\nNice to have\\n\\n\\nExperience working with Business Intelligence BI tools such as PowerBI\\nBachelor's Degree\\n\\n**We are not able to sponsor Visa's at this time or do Corp-to-Corp arrangements. Must be able work on a W2 basis please!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Renton, WA</td>\n",
       "      <td>Renton</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5-years in SQL, SQL Server, Oracle, JDBC\\n5-years in Hadoop, HDFS, MapReduce, YARN\\n5-years in Sqoop, Oozie, Parquet, Hive, Impala, Spark, HBase, HUE\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Details\\nJob Code\\nJPSC-6693\\nPosted Date\\n01/12/18\\nExperience\\n8 Years\\nPrimary Skills\\nOracle,FIORI,sql server,Hive,MapReduce,HDFS,Oozie,• 5-years in SQL,JDBC • 5-years in Hadoop,YARN • 5-years in Sqoop,Parquet\\nRequired Documents\\nResume\\nOverview\\nRole: Big Data Engineer\\nLocation: Renton, Washington\\nDuration: 6 Months\\nTop Three Skills:\\n5-years in SQL, SQL Server, Oracle, JDBC\\n5-years in Hadoop, HDFS, MapReduce, YARN\\n5-years in Sqoop, Oozie, Parquet, Hive, Impala, Spark, HBase, HUE\\n\\nJob Description:\\nClient is looking for a Big Data, Data Engineer.\\nThis is deployed on the Microsoft Azure platform using core Cloudera technologies such as Cloudera Director 2.1, CDH 5.7, and Cloudera Manager along with Apache Hive, Yarn, HBase, and Spark.\\nCloudera based data lake\\nCDH 5.7.3 on Azure cloud platform\\nMoving data from large SQL Server based tables into lake is biggest challenge\\nConcerns around Incremental data load strategy\\nPerformance and Scalability issues in order to meet SLA with business teams\\nLimited internal bandwidth and skill set on No SQL database (Hbase)\\nData Source - Epic (Clarity DB)\\nMostly all structure data sets to ingest to lake\\nLarge volume of data in some tables\\n Please Fill up following details and send me back ASAP if you’re interested in this Position.\\n\\nFull Name:\\nEmail id:\\nContact Information:\\nCurrent Location:\\nVisa status (Need Visa copy):\\nVisa Validity:\\nAvailability:\\nPreferred interview timings:\\nAre you ready for F2F Interview:\\nWillingness to relocate across US:\\nReason for looking new project :\\nYear of Graduation &amp; Degree &amp; university Name:\\nDate of Birth:\\nSkype ID:\\nLast 4 Digits of SSN:\\nReferences:\\nDetails\\nReference-1\\nReference-2\\n\\nFull Name\\n\\nCompany\\n\\nDesignation\\n\\nContact/Email\\n\\nSyed Raza\\n585 532 7200 Ext 9002\\nSyed.j@avanitechsolutions.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Renewable Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline3+ years of relevant experience data engineering or business intelligence roles2+ years of experience in scripting languages such as Python, Ruby, Perl, Bash2+ years of database experience in database schema design, writing advanced SQL queries, data warehousing in Redshift, Oracle, MySQL, or other relational database systems2+ years of experience with ETL and report automation1+ years of experience creating visual data representations, such as Tableau, QuickSight, or other BI platforms\\n\\nDo you love transforming raw data to business critical decisions? Are you excited to be a part of a more renewable, sustainable future?\\n\\nThe AWS Renewable Energy Team is looking for a passionate, results-oriented Renewable Energy Data Engineer to build our renewable energy data platform. The candidate will help AWS march toward its goal of powering our global data center infrastructure with 100% renewable energy. This role will work with data on a scale unique to our business. Collaborating with our data scientists, the Data Engineer will turn this data into valuable business information. This information will drive solutions to problems regarding the optimization of data center power procurement and renewable energy projections.\\n\\nA successful candidate will streamline data collection process, build automation tools, enhance the quality of the data, and develop the visual dash boarding to monitor our renewable energy portfolio performance and identify new financial opportunities. We obtain this information from multiple sources via APIs, SCADA systems, utility online portals, real estate and construction databases and documents, internal Redshift databases, accounting systems, invoices, and energy trading platforms. The ideal candidate will know how to design logical schemas that organize data in a meaningful, efficient way and understand how to build scalable and maintainable data pipelines. This candidate is an expert at data modeling, ETL design and business intelligence tools, and has hands-on knowledge of databases such as Redshift. The Data Engineer will partner closely with internal customers and research scientists to invent new technical solutions to highly complex energy data analytics problems.\\n\\nMasters or MBA in related field1+ years of experience with AWS services such as Redshift, DynamoDB, S3, EC2, Lambda, EMR, RDS, Aurora1+ years with statistical software or programming languages such as R, SAS, Stata, JMP, Minitab1+ years of experience with big data technologies such as Hadoop, Hive, Hbase, Pig, SparkPrevious experience building risk-based financial models and advanced proficiency in ExcelProven track record partnering with business owners to understand requirements and developing analysis to solve their business problemsExcellent speaking-listening-writing skills and attention to detailsBe a passionate self-starterMeets/exceeds Amazon’s functional/technical depth and complexity for this roleMeets/exceeds Amazon’s leadership principles requirements for this role\\nAmazon.com is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Data Engineer In Test</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's in Computer Science or related degree\\n</td>\n",
       "      <td>Bachelor's in Computer Science or related degree\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.\\nOne thing that's stayed the same since our founding: our commitment to our customers, partners and employees.\\nJoin us on our journey as we continue to grow into a powerful contender in the field of insurance.\\nThis position contributes to developing, implementing, and sustaining manual &amp; automation testing including performance testing processes, practices, and controls in support of application and system requirement throughout the software development and sustainment lifecycles. Provides direction on the development and implementation of test automation and performance testing processes, methods and tools.\\n\\nThe position requires understanding &amp; experience in AWS Data Platform\\n\\nExperience is required in creating test plans/test cases, executing tests for applications &amp; validating data using Tableau.\\nSkills Required:\\nHands-on Engineer with experience in development/testing software with big data components in AWS Cloud infrastructure\\nExperience in testing AWS data pipelines using S3, AWS GLUE, Athena, PySpark, AWS Code Pipeline, Jupyter Notebooks, XML/JSON, Redshift, Tableau, etc.\\nSkills in SQL, Python, pytest, Git, Code deployment &amp; CI/CD practices\\nExperience scripting, running ETL jobs, troubleshooting errors, analyzing data and performance testing.\\nExperience working with Agile SDLC frameworks i.e. SCRUM, Kanban, DevOps.\\nExperience developing or working with commercial or open source automation tools and frameworks\\nDemonstrate knowledge using version control and defect tracking methods, including an understanding of associated tools\\nDemonstrated collaboration working with diverse teams including project managers, business analysts, and Engineers related to quality assurance roles and responsibilities\\nQualifications:\\nBachelor's in Computer Science or related degree\\n3-5 years of experience in Data Engineering in Test\\n3+ years of experience with SQL, Python &amp; Tableau\\nUnderstanding of key QA metrics and defect management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Sr Data Engineer</td>\n",
       "      <td>Bellevue, WA 98004</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>98004</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Deep experience with RDMS databases (SQL Server) and Data Warehouse (OLAP, Redshift) managing connection-pools, performance tuning and optimizations.\\n</td>\n",
       "      <td>Deep experience with RDMS databases (SQL Server) and Data Warehouse (OLAP, Redshift) managing connection-pools, performance tuning and optimizations.\\n</td>\n",
       "      <td>\\nDeliver on large and complex solution data needs by leading requirements and technical specifications that drive resulting data designs. Participate in strategic and innovative data design, requirements, walkthroughs, and reviews. Design and document system integration/configuration as required.\\nImplement and advance data models and configurations in support of integrating data from source systems and environments to promote Continuous Integration/Continuous Deployment (CI/CD) and CQRS pattern caching.\\nResearch, design, and implement next generation analytics and machine learning platforms.\\nBuild tools, frameworks, APIs, and dashboards to support telemetry and advanced analytics focusing on ways to improve data security, accessibility, reliability, scalability, efficiency and quality.\\nLead data governance and guide all data schema/configuration changes.</td>\n",
       "      <td>Deep experience with RDMS databases (SQL Server) and Data Warehouse (OLAP, Redshift) managing connection-pools, performance tuning and optimizations.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>A Sr Data Engineer will build, manage, integrate, and optimize reservoirs for data in support of delivering relevant information for business consumption promoting advances in predictive analytics and machine learning. This individual develops, constructs, tests and maintains designs for databases and large-scale data processing systems in support of underlying business, solution, and enterprise architectures. They support and maintain pipelines delivering relevant data sets for business consumption and analysis. This individual works closely with architects to determine which data management systems are appropriate and with the business to determine what data is needed for analysis. This individual works with architects to guide/align data management systems and closely with the business to determine what data is needed for analysis. This individual will wrestle with problems associated with database integration and messy, unstructured data sets toward the ultimate goal of providing clean, usable data to whomever may require it.\\n\\nResponsibilities\\n\\nDeliver on large and complex solution data needs by leading requirements and technical specifications that drive resulting data designs. Participate in strategic and innovative data design, requirements, walkthroughs, and reviews. Design and document system integration/configuration as required.\\nImplement and advance data models and configurations in support of integrating data from source systems and environments to promote Continuous Integration/Continuous Deployment (CI/CD) and CQRS pattern caching.\\nResearch, design, and implement next generation analytics and machine learning platforms.\\nBuild tools, frameworks, APIs, and dashboards to support telemetry and advanced analytics focusing on ways to improve data security, accessibility, reliability, scalability, efficiency and quality.\\nLead data governance and guide all data schema/configuration changes.\\nRequired/Preferred Qualifications\\n\\nEducation Required:\\nB.S. in Computer Science, Mathematics, Software/Computer Engineering, Information Systems or science related field. A Data Professional Certification (e.g., ICCP Certified Data Professional, BI Professional, Big Data Professional, Data Governance Professional, or vendor equivalent) is encouraged.\\n\\nMinimum Years of Related Work Experience Required:\\nMinimum of 7-10 years of data design &amp; development experience in relevant technologies/systems required including technical experience implementing and delivering from system architecture, design, integration, implementation, security, and capability roadmap for a data environment.\\n\\nSkills and Abilities Required:\\n Deep experience with RDMS databases (SQL Server) and Data Warehouse (OLAP, Redshift) managing connection-pools, performance tuning and optimizations.\\n Awareness/exposure to NoSQL technologies (Key/Value, Columnar, Document, Graph)\\n Creative Problem-Solving: Approaches data organization challenges leveraging experience with multiple, diverse technical configurations, technologies and processing environments.\\n Effective Collaboration: Carefully listens to business partners, data scientists and architects to ascertain their needs partnering to establishing optimal outcomes.\\n Intellectual Curiosity: Awareness and exposure with Data Visualization (e.g., Power BI, Microstrategy, Tableau), big data systems including MapReduce technologies (e.g., Hadoop, Spark), NoSQL technologies (Key/Value, Columnar, Document, Graph), and Monitoring platforms (e.g., Splunk, the Elastic Stack, CloudTrail, CloudWatch).\\n Awareness/exposure development and modeling programming languages (e.g., Java, C#, R, Python).\\n\\nSymetra is a dynamic and growing financial services company with 60 years of experience and customers nationwide. In our daily work delivering retirement, employee benefits, and life insurance products, we're guided by the principles of VALUE, TRANSPARENCY AND SUSTAINABILITY. That means we provide products and services people need at a competitive price, we communicate clearly and honestly so people understand what they're getting, and we build products that stand the test of time. We work hard and do what's right for our customers, communities and employees. Join our team and share in our success as we work toward becoming the next national player in our industry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Data Engineer - AWS Product BI</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Basic Qualifications\\nBachelor's Degree in Computer Science or a related technical field, and solid years of relevant experience.A strong grasp of SQL and at least one scripting or programming language.4+ years of experience with and detailed knowledge of data warehouse technical architectures, data modeling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding.3+ years of large IT project delivery for BI oriented projects.3+ years of working with very large data warehousing environment\\n\\nAmazon is looking for an outstanding Data Engineer to join the AWS Product BI team. This is your opportunity to be a core part of the team that has direct impact on the day-to-day decision making in the many AWS Product teams like EC2, S3 and IoT.\\n\\nSince early 2006, AWS has provided companies of all sizes with an infrastructure platform in the cloud. AWS is a high-growth, fast-moving division within Amazon with a start-up mentality where new and diverse challenges arise every day. On the AWS Product BI team you will be surrounded by people that are exceptionally talented, bright, and driven, and believe that world class BI is critical to our success. To help build this growing team, you must be highly analytical and possess a strong passion for analytics and accountability, set high standards with a focus on superior business success. We take working hard, having fun, and making history seriously. AWS sets the standard for functionality, cost, and performance for many cloud based services, but it’s still early days for cloud computing, and there are boundless opportunities to continue to redefine the world of cloud computing - come help us make history!\\n\\nAs a Data engineer on this team, you will be a technical leader in our team, and own the technical architecture of our BI and Data platforms. You will get the exciting opportunity to work on very large data sets in one of the world's largest and most complex data warehouse environments. You will work closely with the business and technical teams in analysis on many non-standard and unique business problems and use creative-problem solving to deliver actionable output.\\nOur team is serious about great design and redefining best practices with a cloud-based approach to scalability and automation. A successful candidate will be a self-starter, comfortable with ambiguity, with strong attention to detail, an ability to work in a fast-paced and ever-changing environment, and an ability to work effectively with cross-functional teams.\\n\\nKey responsibilities include\\nDesigning, developing, troubleshooting, evaluating, deploying, and documenting data management and business intelligence systems, enabling stakeholders to manage the business and make effective decisions.Building secure, available, scalable, stable, and cost-effective data solutions using data storage technologies, distributed file system, data processing, and business intelligence best practices.Working with business customers in understanding the business requirements and implementing solutions to support analytical and reporting needs.Designing and planning for solutions in the various engineering subject areas as it relates to data storage and movement solutions: data warehousing, enterprise system data architecture, data design (e.g., Logical and Physical Modeling), data persistence technologies, data processing, data management, and data analysis.Ensuring completeness and compatibility of the technical infrastructure to support system performance, availability and architecture requirementsReviewing and participating in testing of the data design, tool design, data extracts/transforms, networks and hardware selections\\n\\nPreferred Qualifications\\nExperience in designing and delivering cross functional custom reporting solutions.\\n- Experience with Massively Parallel Processing (MPP) databases - Redshift, Teradata etc\\nExperience with distributed systems and NoSQL databasesExperience with Big Data technologies e.g. Hadoop, Hive, Oozie, Presto, Hue, Spark, Scala and more!Excellent oral and written communication skills including the ability to communicate effectively with both technical and non-technical stakeholders.Proven ability to meet tight deadlines, multi-task, and prioritize workloadA work ethic based on a strong desire to exceed expectations.Strong analytical skills\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Sr. BI Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BA/BS in Analytics, MIS, Computer Science, Engineering, Statistics, Mathematics or related field.5+ years’ experience as a BIE, data scientist, data engineer or similar job function with a technology company.A history of teamwork and willingness to roll up one’s sleeves to get the job done.Demonstrated strength in SQL, data modeling, ETL development, and data warehousing.Advanced skills in data visualization tools like Quicksight, Tableau or Cognos Solutions.Understanding of Finance concepts is a plus.Working knowledge of Python/Java or similar coding languages is a plus.Familiarity with AWS solutions such as Redshift, S3, EC2, Quicksight is a plus but not required.Ability to draw insights from data and clearly communicate them to the stakeholders and senior management.Have ability to independently influence and drive outputs, meet deadlines, and set clear expectations and road-maps.\\n\\nAmazon Web Services seeks an experienced Sr. Business Intelligence Engineer (BIE) to join the AWS Finance BI(FinBI) team. The team is made of up Data Engineers, BIEs and tool developers. This team is building several platform solutions for all of AWS Finance to help invent and simplify on behalf of the customers. In this organization every day is Day 1 and no projects are the same. In this role you'll own solution designing, customer engagement and full end to end development on products. Some technologies used in the roll will be S3,Redshift, ETL, ETL automation, ad-hoc reporting with tools like QuickSight and IBM Cognos Analytics, and long term analytical projects that will affect the effectiveness of the FinBI team and the customer. You'll work with multiple AWS Finance Stakeholders and Functions, and will work with multiple sources on a wide range of data technologies developing the next generation of reporting solutions.\\n\\nThe successful candidate will be a self-starter comfortable with ambiguity, with strong attention to detail, and the ability to work in a fast-paced environment. You will consult, design, build and manage analytical projects with your customer in mind. You should have a strong expertise and proven success in the design, creation, management, and business use of extremely large datasets. You should be experienced at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications or reports. Above all you should be passionate about inventing on behalf of your customers while learning new solutions to answer business questions to drive tangible change.\\n\\nDuties &amp; responsibilities for this role will include:\\nThe successful candidate will demonstrate good business acumen, experience in developing reports and conducting analysis, strong communication skills, an ability to work effectively with cross functional teams, and an ability to work in an ever-changing environment.Interfacing with business customers to gather data and metrics requirements, then driving analytic projects which will help solve complex challenges.Design, implement, and support key datasets that provide structured and timely access to actionable business information.Perform deep-dives to find the root causes behind variances of key parameters.Experience working in a very large data warehouse environment and multi data sources.Analyzing data and driving insights related to operation and compliance.Build data pipelines for the customers to self-serve very seamlessly.Investigate and implement new big data technologies to provide automatic resolutions to address stakeholder needs.\\n\\nAdvanced Degrees in Analytical, MIS, Computer Science, Engineering, Statistics, Mathematics or 8+ years in related experience.Experience in end to end projects involving complex data sets and high variability.Excellent communication (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teams.Experience handling confidential and sensitive data.Design and develop data infrastructure to support business growth.Ability to coach and grow team-members is a plus.\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Seattle, WA 98127</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98127</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Cloud experience is must (AWS-S3, Snowflake, Redshift, Big Query etc.)\\nExperience with open source such as Hadoop, Spark, Kafka, Druid, Pilosa and Yarn/Kubernetes.\\nExperience in SQL, ETL Tools is required\\nAre passionate about data, technology, &amp; creative innovation.\\nExperience in working with Data Scientists to operationalize machine learning models.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for strong Big Data Engineers and Data Analysts. This person will be handling petabytes of consumer data for analytics. Excellent salary and benefits.\\n\\n\\nLocations – Connecticut, Los Angeles, Seattle and New York.\\n\\n\\nBasic Qualifications:\\n\\nHave 5+ years of experience developing with a mix of languages (Scala, Python, SQL, etc.) and frameworks to implement data ingest, processing, and serving technologies. Experience with real-time and very large scalable online systems are preferred.\\n\\nCloud experience is must (AWS-S3, Snowflake, Redshift, Big Query etc.)\\nExperience with open source such as Hadoop, Spark, Kafka, Druid, Pilosa and Yarn/Kubernetes.\\nExperience in SQL, ETL Tools is required\\nAre passionate about data, technology, &amp; creative innovation.\\nExperience in working with Data Scientists to operationalize machine learning models.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Data Engineer, Prime Video</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's Degree in Computer Science or a related technical discipline5+ years of experience in the field of data engineeringExpertise in the design, creation and management of very large datasetsExpertise in writing high quality, maintainable, and robust code, often in SQL and PythonExpertise in data modeling - understanding of 3NF, Star schema, etc.Expertise in designing systems and workflows (ETL) for handling Big data volumesExperience working with business owners to convert key business requirements into technical specifications\\n\\nPrime Video is changing the way people watch movies and TV shows, with hundreds of thousands of titles available to stream and download on all your favorite devices - Amazon FireTV, iOS devices, Roku, game consoles and Fire Tablets. The mission of Prime Video- Product Analytics team is to drive product decisions that improve customer experience by building decision-making analytical products and deep customer insights. We are seeking a competent, curious, resourceful, and experienced Data Engineer to develop a scalable data warehouse architecture and performant data models that powers our analytical ecosystem and day-to-day decision making. The team presents an exciting opportunity to work on very large data sets in one of the world's largest data warehouse environments. Our data warehouse is built on AWS cloud technologies such as Spectrum and Redshift for performing ETL processing of TBs of relational data.\\n\\nAs a data engineer in this team, you will solve big data warehousing problems on a massive scale. You will apply cloud-based AWS services to solve challenging problems around: big data processing, data warehouse design, and enabling self-service. You will focus on automation and optimization for all areas of DW/ETL maintenance and deployment. You will work closely with the business intelligence engineers, product managers and the software development teams on many non-standard and unique business problems and use creative problem solving to deliver actionable output. The role of data engineer in Amazon requires excellent technical skills in order to develop systems and tools to process data as well as, but not limited to, the ability to analyze data. Your work will have a direct impact on the day-to-day decision making in the Prime Video team.\\n\\nExperience with Amazon RedshiftExperience in scripting languagesExperience handling Big data volumes and performance tuningExposure/Experience in Big data Technologies (hadoop, spark, presto, etc.)Experience working in a UNIX/LINUX environmentStrong analytical and problem solving skillsExcellent verbal and written communication skills\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Seattle, WA 98108</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98108</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Basic qualifications\\nBachelors or Master’s degree in computer science, engineering, statistics, information science or a related fieldAt least 5 years of working experience with data pipelines, ETL processes and SQL in a business environment with large-scale, complex datasets3+ years of experience as a software developer or related technical field with fluency in at least one of the programming languages such as Python, Ruby, Java or ScalaExperience with AWS Big Data technologies (e.g. RedShift, S3, Data Pipeline, etc.) and AWS cloud infrastructureExperience working with disparate data sources to develop complex database architecture and data modelsKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes and testingKnowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and computingExcellent verbal and written communication skills and technical writing skills\\n\\nAt Amazon, we are working to be the most customer-centric company on earth. To get there, we need exceptionally talented, bright and driven people. Are you excited to help us learn more about how our customers shop and how satisfied they are when using our services? We are looking for a talented Data Engineer to join our Global Cross Channel and Cross Category (XCM) Insights team to focus on building advanced data pipelines and innovative data solutions.\\n\\nThis role requires an individual with excellent data modeling, database architecture, and software development skills. A successful candidate will have the ability to work with technology, research, marketing, finance and business teams. They will have passion for data and analytics, be a self-starter comfortable with ambiguity, with strong attention to detail, ability to work in a fast-paced and entrepreneurial environment and driven by a desire to innovate on behalf of our customers.\\n\\nKey Responsibilities:\\nArchitect and develop end to end scalable data applications and data pipelinesHelp build a data lake of disparate data sourcesEstablish scalable, efficient, automated processes for data analyses, model development, model validation and model implementationDevelop strong collaborative relationships with key partners in data engineering, software development, research, modeling and marketing teamsDevelop complex SQL queries and scripts for business logic implementationParticipate in data strategy and road map exercises, data warehouse design and implementation\\nBasic qualifications\\nBachelors or Master’s degree in computer science, engineering, statistics, information science or a related fieldAt least 5 years of working experience with data pipelines, ETL processes and SQL in a business environment with large-scale, complex datasets3+ years of experience as a software developer or related technical field with fluency in at least one of the programming languages such as Python, Ruby, Java or ScalaExperience with AWS Big Data technologies (e.g. RedShift, S3, Data Pipeline, etc.) and AWS cloud infrastructureExperience working with disparate data sources to develop complex database architecture and data modelsKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes and testingKnowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and computingExcellent verbal and written communication skills and technical writing skills\\nPreferred qualifications\\nMasters in computer science, statistics, engineering or other quantitative fieldsKnowledge of data security and data privacy regulation processes, operations, principles, architectural requirements, engineering and vulnerabilitiesExperience with AWS solutions such as Amazon Simple Notification Service (SNS), Amazon SQS queues, AWS Lambda functions is a plusExperience developing cloud software services and an understanding of design for scalability, performance, privacy, security and reliabilityExperience with implementing supervised and unsupervised machine learning models for marketing use cases is a plusAbility to provide data-driven decision support and business intelligence that is timely, accurate, and actionableExperience gathering requirements and formulating business metrics for reportingExperience working with teams across different geographies\\n\\nPreferred qualifications\\nMasters in computer science, statistics, engineering or other quantitative fieldsKnowledge of data security and data privacy regulation processes, operations, principles, architectural requirements, engineering and vulnerabilitiesExperience with AWS solutions such as Amazon Simple Notification Service (SNS), Amazon SQS queues, AWS Lambda functions is a plusExperience developing cloud software services and an understanding of design for scalability, performance, privacy, security and reliabilityExperience with implementing supervised and unsupervised machine learning models for marketing use cases is a plusAbility to provide data-driven decision support and business intelligence that is timely, accurate, and actionableExperience gathering requirements and formulating business metrics for reportingExperience working with teams across different geographies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Manager III, Data Engineer</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Experience mentoring and managing other Data Engineers or Software Development Engineers, ensuring data engineering best practices are being followedA desire to work in a collaborative, intellectually curious environment.Bachelors degree in Computer Science, Mathematics, Physical Sciences or a related STEM field5+ Years of Data Warehouse Experience with Oracle, Redshift, PostgreSQL, etc.Experience in maintaining data warehouse systems and working on large scale data transformation using EMR, Hadoop, Hive, or other Big Data technologiesDemonstrated strength in SQL, data modeling, ETL development, and data warehousingExperience with administering and supporting multiple relational and non relational DBs managing data at peta byte scaleExperience with hardware provisioning, forecasting hardware usage, and managing to a budget\\n\\nThe S3C Compliance team owns the end-to-end compliance experience for over two million active Selling Partners. We own innovation in food and product safety, compliance for global trade, and accuracy for dangerous goods classifications. We support worldwide product, program, data science, and analytics teams. We provide scalable technology to improve safety and compliance in throughout the supply chain.\\n\\nWe are looking for an experienced Data Engineering Manager to lead the Risk Data Technologies team, manage existing data resources, implement new technologies and tooling to further enable science and analytics, as well as help drive scalable data sharing practices. In this role you will split your time between hands on development and managerial activities. You will own data environments, integrate with new technologies, and oversee the development of new processes that support teams across the global compliance organization. You will gather requirements through direct interaction with business, science, as well as software development teams. You will track the performance of our resources and related capabilities, constantly evolving our offering in order to scale our capability set with the growth of the business and needs of our customers.\\n\\nThe ideal candidate will have outstanding communication skills, proven data infrastructure design and implementation capabilities, strong business acumen, and an innate drive to deliver results. He/she will be a self-starter, comfortable with ambiguity and will enjoy working in a fast-paced dynamic environment.\\n\\nExtensive experience working with AWS with a strong understanding of Redshift, EMR, Athena, Aurora, DynamoDB, Kinesis, Lambda, S3, EC2, etc.Extensive Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)Strong interpersonal skills and the ability to communicate complex technology solutions to senior leadership, gain alignment, and drive progress\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Healthcare Data Engineer</td>\n",
       "      <td>Seattle, WA 98104</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>98104</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Company Overview\\n\\nNavigating Cancer is the pioneer in patient relationship management software and services for cancer care. The company has developed the first patient-centered oncology platform that improves the care delivery model for patients, providers, pharmaceutical manufacturers and payers. By putting patients at the center of their care, providers become economically viable in value-based payment models, payers and at-risk providers reduce the total cost of cancer care, and pharmaceutical manufacturers improve medication adherence for their life-saving treatments. Most importantly, patients receive a holistic service offering to support their individual goals and preferences.\\n\\nThe company is backed by industry-leading patient satisfaction ratings and currently supports over a million patients, thousands of care providers, dozens of pharmaceutical manufacturers, and several payer models nationwide to lower costs, improve patient satisfaction, and drive better outcomes.\\n\\nJob Summary\\n\\nAs our Healthcare Data Engineer, you will be responsible for architecting &amp; implementing complex big data projects with a focus on collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into insights using multiple platforms. You will be a key contributor in influencing infrastructure and big data platform decisions. Collaborating closely with peers in the Product, Engineering &amp; Data Sciences team, you will also be at the forefront of developing prototypes and proof of concepts for various lines of businesses. Your work will impact the way cancer patients experience Navigating Care.\\n\\nThe Successful Candidate\\n\\n\\nExtremely strong skills in at-least one programming and scripting language (Java, Python, Scala, Julia, Stata, Ruby, R).\\nHas built large-scale batch and real-time data pipelines using technology such as Airflow, Spark, AWS Big Data stack, Cloudera, HortonWorks,H20.\\nDeveloping with high volume heterogeneous data, Hadoop based technologies like MapReduce, Hive, Cassandra, Hbase and Impala.\\nExperience with AWS Services (Redshift, Redshift Spectrum, S3, Glacier, DynamoDB), Parquet/Avro/ORC.\\nStrong documentation skills.\\nShows a preference for standard, well supported tools over in house, custom solutions.\\nExperience with one or more data analytics and visualization packages.\\nExpert knowledge of scaling and tuning SQL and NoSQL systems.\\nFamiliarity with DevOps tools (e.g. Docker, Ansible, Hashicorp, Jenkins, etc.)\\nDeep understanding of how algorithms work and have experience building high-performance algorithms\\nAbility to multi-task, prioritize assignments and work well under deadlines in a changing environment.\\nStrong quantitative, analytical, process development, facilitation and organizational skills required.\\nBachelor's degree in Information Systems, MIS, Statistics, related field or equivalent work experience required.\\nWork in cross functional agile teams to continuously experiment, iterate and deliver on new product objectives.\\n5+ years of experience in building and sustaining similar solutions, preferably in the healthcare industry.\\n\\nNice to have:\\n\\nExperience building and querying from a variety of electronic health records systems.\\nPrior experience in regulated industries with high Data Quality and Governance standards.\\nBasic knowledge of the medical terminologies, messaging standards, healthcare coding systems.\\n\\nWhy work here?\\n\\nWe support thousands of healthcare professionals and tens of thousands of cancer patients every day. We help cancer patients get better care—and consider this a valuable and meaningful reason to come to work everyday. You get to work with leading technology, wear many hats, and contribute every day to our business success! You get to help us grow and establish ourselves as a world-class business. We are lean, agile, and lightweight. We believe in each other, believe in our work, and have fun along the way.\\n\\nThis position is a full time, on-site, with the Navigating Cancer team, in our downtown Seattle office. We offer competitive compensation, benefits and a fantastic work environment. Come join our team and make a difference!\\n\\nOur Values\\n\\nWork with Purpose . Work together to improve the lives of cancer patients. Collaborate with team members, have passion, enthusiasm and mission for the work that we do.\\n\\nAct with Integrity . Communicate openly, be honest, follow through.\\n\\nBe Agile . Discover creative solutions, welcome change. Adapt and pursue continuous improvement.\\n\\nSeek Simplicity . Reduce complexity for our customers and patients. Simplify our products, our processes and our messages.\\n\\nStrive for Growth . Constantly strive to achieve personal, professional, and company goals. Continuous reflection, learning and achievement.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   Title  \\\n",
       "0    Strategic Customer Engagements, Product Manager - Pricing Analytics   \n",
       "1    Data Engineer with testing                                            \n",
       "2    Business Intelligence Engineer                                        \n",
       "3    Data Engineer                                                         \n",
       "4    BI Engineer                                                           \n",
       "..           ...                                                           \n",
       "167  Big Data Engineer                                                     \n",
       "168  Data Engineer, Prime Video                                            \n",
       "169  Data Engineer                                                         \n",
       "170  Manager III, Data Engineer                                            \n",
       "171  Healthcare Data Engineer                                              \n",
       "\n",
       "              Location     City State         Zip     Country  \\\n",
       "0    Seattle, WA        Seattle  WA    None Found  None Found   \n",
       "1    Seattle, WA        Seattle  WA    None Found  None Found   \n",
       "2    Seattle, WA        Seattle  WA    None Found  None Found   \n",
       "3    Seattle, WA        Seattle  WA    None Found  None Found   \n",
       "4    Seattle, WA        Seattle  WA    None Found  None Found   \n",
       "..           ...            ...  ..           ...         ...   \n",
       "167  Seattle, WA 98127  Seattle  WA    98127       None Found   \n",
       "168  Seattle, WA        Seattle  WA    None Found  None Found   \n",
       "169  Seattle, WA 98108  Seattle  WA    98108       None Found   \n",
       "170  Seattle, WA        Seattle  WA    None Found  None Found   \n",
       "171  Seattle, WA 98104  Seattle  WA    98104       None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                           Qualifications  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                             \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                             \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                             \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                             \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                             \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                             \n",
       "167  Cloud experience is must (AWS-S3, Snowflake, Redshift, Big Query etc.)\\nExperience with open source such as Hadoop, Spark, Kafka, Druid, Pilosa and Yarn/Kubernetes.\\nExperience in SQL, ETL Tools is required\\nAre passionate about data, technology, & creative innovation.\\nExperience in working with Data Scientists to operationalize machine learning models.   \n",
       "168  None Found                                                                                                                                                                                                                                                                                                                                                             \n",
       "169  None Found                                                                                                                                                                                                                                                                                                                                                             \n",
       "170  None Found                                                                                                                                                                                                                                                                                                                                                             \n",
       "171  None Found                                                                                                                                                                                                                                                                                                                                                             \n",
       "\n",
       "         Skills Responsibilities   Education Requirement  \\\n",
       "0    None Found  None Found       None Found  None Found   \n",
       "1    None Found  None Found       None Found  None Found   \n",
       "2    None Found  None Found       None Found  None Found   \n",
       "3    None Found  None Found       None Found  None Found   \n",
       "4    None Found  None Found       None Found  None Found   \n",
       "..          ...         ...              ...         ...   \n",
       "167  None Found  None Found       None Found  None Found   \n",
       "168  None Found  None Found       None Found  None Found   \n",
       "169  None Found  None Found       None Found  None Found   \n",
       "170  None Found  None Found       None Found  None Found   \n",
       "171  None Found  None Found       None Found  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  FullDescriptions  \n",
       "0    Bachelor's degree8+ years of work experienceAbility to drive project success in an ambiguous environmentStrong ability to build trust with stakeholdersAbility to communicate with both technical and non-technical partnersExperience building financial models\\n\\nAre you an entrepreneur that thrives in a fast-paced ambiguous environment? Do you want to be involved in strategic deals come to life?\\n\\nIf so, we are looking for a Sr. Product Manager to join the Strategic Customer Engagements team to build scalable tools to support complex deals. The Strategic Customer Engagements Deal Team works with our customers on commercial private opportunities to meet their desired business outcomes while ensuring alignment with AWS business objectives.\\n\\nIn this role, you be responsible for understanding and modeling customer usage patterns, the competitive environment, and the strategy for our largest AWS service. You will work with a data engineer to identify the data sources and metrics needed to support our largest customer facing deals. You will work cross-functionally with our competitive insights team and with technical partners to understand business tradeoffs and develop tools for our team to quickly analyze deals. You will also provide strategic support to model large deals and be a voice in the room during the deal process.\\n\\nThe ideal candidate will be able to work independently in an ambiguous environment. They will understand financial modelling and be able to communicate complex relationships to senior leaders.\\n\\nMBA5+ years of product management experienceExperience working with data engineersExperience working with AWS services                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "1    Job Details\\nJob Code\\nJPSC-7555\\nPosted Date\\n06/13/18\\nExperience\\n7 Years\\nPrimary Skills\\n• 3 + years’ experience with 1 or more Databases like Oracle,SQL Server ( basic SQL Concepts,writing simple to medium PL/SQL queries a must) • 2 + yrs Experience with Java server-side web application technology Spring,Hibernate and/or SpringBoot • 2+ yrs Experience with web services and REST architecture (using or building XML/JSON web/serverside APIs) • 1 + yrs of hands-on coding experience in an Agile based multi-tier application framework and environment.\\nRequired Documents\\nResume\\nOverview\\nRole: Data Engineer with testing\\nLocation: Seattle, WA\\nDuration: 6+ Months\\n\\nJob Description:\\n3 + years’ experience with 1 or more Databases like Oracle, SQL Server ( basic SQL Concepts , writing simple to medium PL/SQL queries a must)\\n2 + yrs Experience with Java server-side web application technology Spring, Hibernate and/or SpringBoot\\n2+ yrs Experience with web services and REST architecture (using or building XML/JSON web/serverside APIs)\\n1 + yrs of hands-on coding experience in an Agile based multi-tier application framework and environment.\\nGood understanding of software development frameworks, terminology\\nCompetent using version control systems such as GIT, SVN, VSS\\nKnowledge of continuous integration and development (CI/CD) methodologies\\nKnowledge/Understanding of RESTful APIs\\nKnowledge/Understanding of Cloud native Platforms like PCF (Pivotal Cloud Foundry)\\nKnowledge/Understanding of Cloud Datastores – In-Memory/Persistent, NOSQL / Relational\\nKnowledge/Understanding of Logging - using tools like SPLUNK.\\n\\nIn the COMMENTS include:\\nLegal name:\\nPhone #:\\nEmail address:\\nDaily Rate:\\nDOB (Date and Month) :\\nSkype ID :\\nLocation (City and State):\\nRelocate:\\nAvailability to start:\\nVisa type and expiration:\\nHiring Status: C2C/W2/1099\\nOpen for CTH:\\nTimeslots for phone conversation:\\nTimeslots for WebEx/video interview :\\n\\nSummary:\\n\\nThanks Regards,\\nSyed Raza\\nDesk Number: 585 - 532 - 7200 Extension 9002\\n687 Lee Road, Suite 250, Rochester, NY 14606\\nEmail : Syed.j@avanitechsolutions.com\\nEmail: syed_j@iic.com\\nGmail hangouts: syedraza199025@gmail.com\\nwww.iic.com\\nwww.avanitechsolutions.com                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "2    Bachelors in engineering, science, math, statistics or computer science3+ years of work experience as a business intelligence engineer, data engineer or data scientist role3+ years of experience in SQL programming3+ years of experience in building data warehouses and dimensional modeling3+ years of experience with business intelligence and data visualization tools (e.g. Tableau)3+ years of experience with a modern programming language (e.g. Python, R, Scala etc.)Experience with AWS Suite\\n\\nAmazon Lab126 is an inventive research and development company that designs and engineers high-profile consumer electronics. Lab126 began in 2004 as a subsidiary of Amazon.com, Inc., originally creating the best-selling Kindle family of products. Since then, we have produced groundbreaking devices like Fire tablets, Fire TV, Amazon Echo and Amazon Show. The Amazon Devices group delivers delightfully unique Amazon experiences, giving customers instant access to everything, digital or physical.\\n\\nAre you interested in a fast-paced, high-growth environment with the opportunity to work on business-critical decisions? Amazon Device Accessories is looking for an outstanding Business Intelligence Engineer to join our Operational Excellence Team. We’re looking for someone who can provide insight on KPI’s, understand inferential statistics and advise business teams on how to optimize for profit.\\n\\nAs an engineer on the team, you'll leverage tools and services including Amazon Redshift, Tableau, AWS Glue, AWS Athena, Spark, EMR, Machine Learning and Time Series models to build solutions that deliver data-driven reports, dashboards, and recommendations to high level leadership.\\n\\nYou'll work directly with business leaders and stakeholders to understand different business problems and use cases. You'll work with Finance, Tech and Business teams to identify and consume data sources, transform the data, and build the reports and visualizations needed to meet the requirements. You’ll have the opportunity to get hands on experience with Machine Learning, Time Series Modelling and high impact business analysis.\\n\\nDeveloping this capability will provide insights that are used to lead decision making around product allocation, product effectiveness, productivity analysis and business impact. Consumers of these insights will include Directors, VP’s and SVP’s.\\n\\nOur tenets for analytics team members are as follows:\\nUtilizing the Scientific Method to make tangible business impactMetrics before Messes\\no Ensuring we’re measuring the right business metrics to guide the business\\nForecast or be Last\\no Developing state of the art predictive models for ensuring we’re moving in the right direction\\n\\nRoles and Responsibilities:\\nBuild data solutions using AWS services that deliver data-driven reports, dashboards, and tools.Develop and implement Time Series and Machine Learning Forecast ModelsManage marketing and sales data for the organizationManage ETL pipelines using AWS EMR and SparkDistill problem definitions, models, and constraints from informal business requirements.Provide innovative self-service tools to our customers to self-serve and scale dataFollow established engineering best practices and define new best practices where required.Identify critical metrics/reports that measure product performance, efficiency/effectiveness and create client facing dashboards to facilitate decision making.Collaborate on the design, development, maintenance, and delivery/presentation of forecasting models, metrics, reports, analyses, tools, and dashboardsPerform proactive diagnostic analysis on the various product measures and surface meaningful insights to the leadership team.Collaborate with Data Scientists, Data Engineers and Economists to develop Product Insights on Marketing and Sales data.\\n\\nMasters in engineering, science, math, statistics or computer scienceExperience using AWS services for data analytics (i.e., Athena, Glue, Redshift, EMR, etc.)Experience developing custom ETL solutions using Python and SQLExperience with Tableau Desktop and Tableau ServerStrong written and verbal communications skills. Having the ability to translate scientific findings into business recommendations and outputs.The ability to influence stakeholders through delivering results and earning trustBasic statistical tests (but not limited to) t-tests, chi-square and regressionExpert SQLProficiency in PythonExperience delivering the best Products to customers\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "3    Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related field or 5+ years industry experience.\\n\\nAt Amazon we strive to be earth’s most customer centric company and data is a foundational component to making this happen. In this role you will be responsible for navigating this wealth of data and turning it into actionable and insightful information for business partners in order to deliver better products and services to our customers. The Devices & Services HR Analytics group provides people metrics along the employee lifecycle for our Devices & Services businesses. Our work is dedicated to empowering leaders and enabling action through data and science. In addition to standard reporting, we seek to leverage analytics to help our leaders focus their efforts in ways that will engage, retain and grow their team members to propel the business forward.\\n\\nWe are now recruiting for an exceptional Data Engineer, Devices & Services HR.\\n\\nRoles and Responsibilities\\nIn this role, you will lead the design and maintenance of an end to end platform which will serve the historical reporting, real-time reporting and predictive analytics needs of the Devices & Services HR org. You are the ideal candidate for this role if you are well-versed in multiple DW, reporting and visualization platforms. You will have hands on experience with fast-growing and evolving datasets of petabyte scale. You will understand the challenges of scaling analytical platforms globally, keeping the data security and regulations in mind. You build scalable systems and solutions. You are a team player, skilled at driving consensus through a data-driven approach and mentoring junior team members. You are eager to learn the latest technologies and know when to apply them.\\n\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.\\n\\n· Demonstrated industry leadership in the fields of Database and/or Data Warehousing, Big Data processing · Advanced knowledge of large data manipulation and data mining using SQL. ·\\nExcellent SQL query performance tuning skillsSolid written and verbal communication skillsAbility to work in a team environment that promotes collaboration.Proven ability to build and support large-scale data solutions serving a global customer baseBe self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty data.Ability to understand business requirements, convert them into technical solutions and deliver software incrementallyProven ability to drive adoption of data engineering best practices in a team of engineers, and ability to mentor junior team members\\nAdvanced knowledge and expertise with AWS technologies such as Redshift, S3, EC2, & EMR.\\n\\nExperience with Big Data technologies, Spark, Hive\\nKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operationsAdvanced knowledge and expertise with Data modelling skills, Advanced SQL with Redshift, MySQL, and Columnar DatabasesAdvanced knowledge of a scripting language such as: R or Python.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4    BA/BS in Computer Science, Engineering, Statistics, Mathematics, Finance or related field.3+ years’ experience as a BIE, data scientist, data engineer or similar job function with a technology company.Demonstrated strength in SQL, data modeling, ETL development, and data warehousing.Advanced skills in Excel as well as any data visualization tools like Quicksight, Tableau or Cognos Solutions.Understanding of Finance concepts is a plus.Working knowledge of Python/Java or similar coding languages.Familiarity with AWS solutions such as Redshift, S3.Advanced ability to draw insights from data and clearly communicate them to the stakeholders and senior management.Have ability to independently influence and drive outputs, meet deadlines, and set clear expectations and roadmaps.\\n\\nIf you enjoy creating solutions from scratch that revolve around BI and Analytics then this role is for you.\\n\\nAmazon Web Services seeks an experienced Business Intelligence Engineer (BIE) to join the AWS Finance BI(FinBI) team. The team is made of up Data Engineers, BIEs and tool developers. This team is building several platform solutions for all of AWS Finance to help invent and simplify on behalf of the customers. In this organization every day is Day 1 and no projects are the same. In this role you'll own solution designing, customer engagement and full end to end development on products. Some technologies used in the roll will be S3,Redshift, ETL, ETL automation, ad-hoc reporting with tools like QuickSight and IBM Cognos Analytics, and long term analytical projects that will affect the effectiveness of the FinBI team and the customer. You'll work with multiple AWS Finance Stakeholders and Functions, and will work with multiple sources on a wide range of data technologies developing the next generation of reporting solutions.\\n\\nThe successful candidate will be a self-starter comfortable with ambiguity, with strong attention to detail, and the ability to work in a fast-paced environment. You will consult, design, build and manage analytical projects with your customer in mind. You should have a strong expertise and proven success in the design, creation, management, and business use of extremely large datasets. You should be experienced at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing applications or reports. Above all you should be passionate about inventing on behalf of your customers while learning new solutions to answer business questions to drive tangible change.\\nDuties & responsibilities for this role will include:\\nThe successful candidate will demonstrate good business acumen, experience in developing reports and conducting analysis, strong communication skills, an ability to work effectively with cross functional teams, and an ability to work in an ever-changing environment.Interfacing with business customers to gather data and metrics requirements, then driving analytic projects which will help solve complex challenges.Design, implement, and support key datasets that provide structured and timely access to actionable business information.Perform deep-dives to find the root causes behind variances of key parameters.Experience working in a very large data warehouse environment and multi data sources.Analyzing data and driving insights related to operation and compliance.Build data pipelines for the customers to self-serve very seamlessly.Investigate and implement new big data technologies to provide automatic resolutions to address stakeholder needs.\\n\\nMBA or Master’s in Computer Science, Engineering, Statistics, Mathematics, Finance or related field.Experience in projects involving complex data sets and high variability.A history of teamwork and willingness to roll up one’s sleeves to get the job done.Excellent communication (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teams.Experience handling confidential and sensitive data.Design and develop data infrastructure to support business growth.\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "167  We are looking for strong Big Data Engineers and Data Analysts. This person will be handling petabytes of consumer data for analytics. Excellent salary and benefits.\\n\\n\\nLocations – Connecticut, Los Angeles, Seattle and New York.\\n\\n\\nBasic Qualifications:\\n\\nHave 5+ years of experience developing with a mix of languages (Scala, Python, SQL, etc.) and frameworks to implement data ingest, processing, and serving technologies. Experience with real-time and very large scalable online systems are preferred.\\n\\nCloud experience is must (AWS-S3, Snowflake, Redshift, Big Query etc.)\\nExperience with open source such as Hadoop, Spark, Kafka, Druid, Pilosa and Yarn/Kubernetes.\\nExperience in SQL, ETL Tools is required\\nAre passionate about data, technology, & creative innovation.\\nExperience in working with Data Scientists to operationalize machine learning models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "168  Bachelor's Degree in Computer Science or a related technical discipline5+ years of experience in the field of data engineeringExpertise in the design, creation and management of very large datasetsExpertise in writing high quality, maintainable, and robust code, often in SQL and PythonExpertise in data modeling - understanding of 3NF, Star schema, etc.Expertise in designing systems and workflows (ETL) for handling Big data volumesExperience working with business owners to convert key business requirements into technical specifications\\n\\nPrime Video is changing the way people watch movies and TV shows, with hundreds of thousands of titles available to stream and download on all your favorite devices - Amazon FireTV, iOS devices, Roku, game consoles and Fire Tablets. The mission of Prime Video- Product Analytics team is to drive product decisions that improve customer experience by building decision-making analytical products and deep customer insights. We are seeking a competent, curious, resourceful, and experienced Data Engineer to develop a scalable data warehouse architecture and performant data models that powers our analytical ecosystem and day-to-day decision making. The team presents an exciting opportunity to work on very large data sets in one of the world's largest data warehouse environments. Our data warehouse is built on AWS cloud technologies such as Spectrum and Redshift for performing ETL processing of TBs of relational data.\\n\\nAs a data engineer in this team, you will solve big data warehousing problems on a massive scale. You will apply cloud-based AWS services to solve challenging problems around: big data processing, data warehouse design, and enabling self-service. You will focus on automation and optimization for all areas of DW/ETL maintenance and deployment. You will work closely with the business intelligence engineers, product managers and the software development teams on many non-standard and unique business problems and use creative problem solving to deliver actionable output. The role of data engineer in Amazon requires excellent technical skills in order to develop systems and tools to process data as well as, but not limited to, the ability to analyze data. Your work will have a direct impact on the day-to-day decision making in the Prime Video team.\\n\\nExperience with Amazon RedshiftExperience in scripting languagesExperience handling Big data volumes and performance tuningExposure/Experience in Big data Technologies (hadoop, spark, presto, etc.)Experience working in a UNIX/LINUX environmentStrong analytical and problem solving skillsExcellent verbal and written communication skills\\nAmazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "169  Basic qualifications\\nBachelors or Master’s degree in computer science, engineering, statistics, information science or a related fieldAt least 5 years of working experience with data pipelines, ETL processes and SQL in a business environment with large-scale, complex datasets3+ years of experience as a software developer or related technical field with fluency in at least one of the programming languages such as Python, Ruby, Java or ScalaExperience with AWS Big Data technologies (e.g. RedShift, S3, Data Pipeline, etc.) and AWS cloud infrastructureExperience working with disparate data sources to develop complex database architecture and data modelsKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes and testingKnowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and computingExcellent verbal and written communication skills and technical writing skills\\n\\nAt Amazon, we are working to be the most customer-centric company on earth. To get there, we need exceptionally talented, bright and driven people. Are you excited to help us learn more about how our customers shop and how satisfied they are when using our services? We are looking for a talented Data Engineer to join our Global Cross Channel and Cross Category (XCM) Insights team to focus on building advanced data pipelines and innovative data solutions.\\n\\nThis role requires an individual with excellent data modeling, database architecture, and software development skills. A successful candidate will have the ability to work with technology, research, marketing, finance and business teams. They will have passion for data and analytics, be a self-starter comfortable with ambiguity, with strong attention to detail, ability to work in a fast-paced and entrepreneurial environment and driven by a desire to innovate on behalf of our customers.\\n\\nKey Responsibilities:\\nArchitect and develop end to end scalable data applications and data pipelinesHelp build a data lake of disparate data sourcesEstablish scalable, efficient, automated processes for data analyses, model development, model validation and model implementationDevelop strong collaborative relationships with key partners in data engineering, software development, research, modeling and marketing teamsDevelop complex SQL queries and scripts for business logic implementationParticipate in data strategy and road map exercises, data warehouse design and implementation\\nBasic qualifications\\nBachelors or Master’s degree in computer science, engineering, statistics, information science or a related fieldAt least 5 years of working experience with data pipelines, ETL processes and SQL in a business environment with large-scale, complex datasets3+ years of experience as a software developer or related technical field with fluency in at least one of the programming languages such as Python, Ruby, Java or ScalaExperience with AWS Big Data technologies (e.g. RedShift, S3, Data Pipeline, etc.) and AWS cloud infrastructureExperience working with disparate data sources to develop complex database architecture and data modelsKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes and testingKnowledge of data management fundamentals and data storage principlesKnowledge of distributed systems as it pertains to data storage and computingExcellent verbal and written communication skills and technical writing skills\\nPreferred qualifications\\nMasters in computer science, statistics, engineering or other quantitative fieldsKnowledge of data security and data privacy regulation processes, operations, principles, architectural requirements, engineering and vulnerabilitiesExperience with AWS solutions such as Amazon Simple Notification Service (SNS), Amazon SQS queues, AWS Lambda functions is a plusExperience developing cloud software services and an understanding of design for scalability, performance, privacy, security and reliabilityExperience with implementing supervised and unsupervised machine learning models for marketing use cases is a plusAbility to provide data-driven decision support and business intelligence that is timely, accurate, and actionableExperience gathering requirements and formulating business metrics for reportingExperience working with teams across different geographies\\n\\nPreferred qualifications\\nMasters in computer science, statistics, engineering or other quantitative fieldsKnowledge of data security and data privacy regulation processes, operations, principles, architectural requirements, engineering and vulnerabilitiesExperience with AWS solutions such as Amazon Simple Notification Service (SNS), Amazon SQS queues, AWS Lambda functions is a plusExperience developing cloud software services and an understanding of design for scalability, performance, privacy, security and reliabilityExperience with implementing supervised and unsupervised machine learning models for marketing use cases is a plusAbility to provide data-driven decision support and business intelligence that is timely, accurate, and actionableExperience gathering requirements and formulating business metrics for reportingExperience working with teams across different geographies  \n",
       "170  Experience mentoring and managing other Data Engineers or Software Development Engineers, ensuring data engineering best practices are being followedA desire to work in a collaborative, intellectually curious environment.Bachelors degree in Computer Science, Mathematics, Physical Sciences or a related STEM field5+ Years of Data Warehouse Experience with Oracle, Redshift, PostgreSQL, etc.Experience in maintaining data warehouse systems and working on large scale data transformation using EMR, Hadoop, Hive, or other Big Data technologiesDemonstrated strength in SQL, data modeling, ETL development, and data warehousingExperience with administering and supporting multiple relational and non relational DBs managing data at peta byte scaleExperience with hardware provisioning, forecasting hardware usage, and managing to a budget\\n\\nThe S3C Compliance team owns the end-to-end compliance experience for over two million active Selling Partners. We own innovation in food and product safety, compliance for global trade, and accuracy for dangerous goods classifications. We support worldwide product, program, data science, and analytics teams. We provide scalable technology to improve safety and compliance in throughout the supply chain.\\n\\nWe are looking for an experienced Data Engineering Manager to lead the Risk Data Technologies team, manage existing data resources, implement new technologies and tooling to further enable science and analytics, as well as help drive scalable data sharing practices. In this role you will split your time between hands on development and managerial activities. You will own data environments, integrate with new technologies, and oversee the development of new processes that support teams across the global compliance organization. You will gather requirements through direct interaction with business, science, as well as software development teams. You will track the performance of our resources and related capabilities, constantly evolving our offering in order to scale our capability set with the growth of the business and needs of our customers.\\n\\nThe ideal candidate will have outstanding communication skills, proven data infrastructure design and implementation capabilities, strong business acumen, and an innate drive to deliver results. He/she will be a self-starter, comfortable with ambiguity and will enjoy working in a fast-paced dynamic environment.\\n\\nExtensive experience working with AWS with a strong understanding of Redshift, EMR, Athena, Aurora, DynamoDB, Kinesis, Lambda, S3, EC2, etc.Extensive Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)Strong interpersonal skills and the ability to communicate complex technology solutions to senior leadership, gain alignment, and drive progress\\nAmazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "171  Company Overview\\n\\nNavigating Cancer is the pioneer in patient relationship management software and services for cancer care. The company has developed the first patient-centered oncology platform that improves the care delivery model for patients, providers, pharmaceutical manufacturers and payers. By putting patients at the center of their care, providers become economically viable in value-based payment models, payers and at-risk providers reduce the total cost of cancer care, and pharmaceutical manufacturers improve medication adherence for their life-saving treatments. Most importantly, patients receive a holistic service offering to support their individual goals and preferences.\\n\\nThe company is backed by industry-leading patient satisfaction ratings and currently supports over a million patients, thousands of care providers, dozens of pharmaceutical manufacturers, and several payer models nationwide to lower costs, improve patient satisfaction, and drive better outcomes.\\n\\nJob Summary\\n\\nAs our Healthcare Data Engineer, you will be responsible for architecting & implementing complex big data projects with a focus on collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into insights using multiple platforms. You will be a key contributor in influencing infrastructure and big data platform decisions. Collaborating closely with peers in the Product, Engineering & Data Sciences team, you will also be at the forefront of developing prototypes and proof of concepts for various lines of businesses. Your work will impact the way cancer patients experience Navigating Care.\\n\\nThe Successful Candidate\\n\\n\\nExtremely strong skills in at-least one programming and scripting language (Java, Python, Scala, Julia, Stata, Ruby, R).\\nHas built large-scale batch and real-time data pipelines using technology such as Airflow, Spark, AWS Big Data stack, Cloudera, HortonWorks,H20.\\nDeveloping with high volume heterogeneous data, Hadoop based technologies like MapReduce, Hive, Cassandra, Hbase and Impala.\\nExperience with AWS Services (Redshift, Redshift Spectrum, S3, Glacier, DynamoDB), Parquet/Avro/ORC.\\nStrong documentation skills.\\nShows a preference for standard, well supported tools over in house, custom solutions.\\nExperience with one or more data analytics and visualization packages.\\nExpert knowledge of scaling and tuning SQL and NoSQL systems.\\nFamiliarity with DevOps tools (e.g. Docker, Ansible, Hashicorp, Jenkins, etc.)\\nDeep understanding of how algorithms work and have experience building high-performance algorithms\\nAbility to multi-task, prioritize assignments and work well under deadlines in a changing environment.\\nStrong quantitative, analytical, process development, facilitation and organizational skills required.\\nBachelor's degree in Information Systems, MIS, Statistics, related field or equivalent work experience required.\\nWork in cross functional agile teams to continuously experiment, iterate and deliver on new product objectives.\\n5+ years of experience in building and sustaining similar solutions, preferably in the healthcare industry.\\n\\nNice to have:\\n\\nExperience building and querying from a variety of electronic health records systems.\\nPrior experience in regulated industries with high Data Quality and Governance standards.\\nBasic knowledge of the medical terminologies, messaging standards, healthcare coding systems.\\n\\nWhy work here?\\n\\nWe support thousands of healthcare professionals and tens of thousands of cancer patients every day. We help cancer patients get better care—and consider this a valuable and meaningful reason to come to work everyday. You get to work with leading technology, wear many hats, and contribute every day to our business success! You get to help us grow and establish ourselves as a world-class business. We are lean, agile, and lightweight. We believe in each other, believe in our work, and have fun along the way.\\n\\nThis position is a full time, on-site, with the Navigating Cancer team, in our downtown Seattle office. We offer competitive compensation, benefits and a fantastic work environment. Come join our team and make a difference!\\n\\nOur Values\\n\\nWork with Purpose . Work together to improve the lives of cancer patients. Collaborate with team members, have passion, enthusiasm and mission for the work that we do.\\n\\nAct with Integrity . Communicate openly, be honest, follow through.\\n\\nBe Agile . Discover creative solutions, welcome change. Adapt and pursue continuous improvement.\\n\\nSeek Simplicity . Reduce complexity for our customers and patients. Simplify our products, our processes and our messages.\\n\\nStrive for Growth . Constantly strive to achieve personal, professional, and company goals. Continuous reflection, learning and achievement.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "\n",
       "[172 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_Seattle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
