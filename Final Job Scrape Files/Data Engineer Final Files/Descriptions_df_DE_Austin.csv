,Title,Location,City,State,Zip,Country,Qualifications,Skills,Responsibilities,Education,Requirement,FullDescriptions
0,Senior Data Engineer – Elastic Engineer,"Austin, TX 73344",Austin,TX,73344,None Found,None Found,None Found,None Found,None Found,None Found,"Introduction
At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities
We are looking for a Data engineer to deploy complex big data and analytics solutions using Elastic stack. You should have strong experience in deploying and managing Elastic cluster on Kubernetes in multi-site, multi cluster environment in both on-Premise as well as Cloud platforms. You should have applied or expert knowledge in big data platforms. The main use case for such a platform for us would be Real-Time Anomaly Detection and Time Series models on IT operations data like logs, metrics, events, wired data, transaction flow, ITIL process related data, knowledge repositories, etc
Business Unit/ Team Overview
Global Technology Services (GTS) at IBM manages the IT infrastructure for some of the world’s leading corporations and with that comes the responsibility of managing enormous amounts of IT data and the opportunity for making better decisions using that data. In GTS analytics team at IBM, Data Scientists, Data Engineers, and BigData IT Architects are developing novel models, cutting edge algorithms, and custom analytics solutions to tackle BigData challenges in the IT Infrastructure space.
ITOA / AIops provides real time machine-data (log, events, performance, capacity, ITIL data, wire data, etc) analytics solutions that helps customers manage Business Services and manage the quality of the end-user experience
It can tell a client in real time ‘What happened’, ‘Why did it happen’, ‘Will it happen again’ and ‘What to do if it happens again? etc
Keeps everyone on the same page by looking at the same Business Transaction data and metrics.
Keeps the focus on operational data that translate to the business value the application delivers; dive in deeper when appropriate.
Identify resolution criteria, assign ownership
Take lessons learned to improve development, test, deployment, and production processes
Education & Experience
Minimum 4 years of relevant experience working on the Elastic based products & distributions specifically used in Real Time ITOA or AIops use-cases processing logs, metrics, events, etc
At least 4 years of experience in development & implementation of logging and metrics solutions in with TB+ / day ingestion per day
At least 5 years of hands-on experience in IT support (Infrastructure / Application) and IT monitoring tools
Overall 7+ years of core Big data / Analytics experience in various domains
Degree / Master’s degree in computers or equivalent
Certifications:
Elastic certified engineer
Certifications showing proficiency in the Usage, design & deployment of ITOA / AIOps solutions like Elastic or Splunk
Specialized certifications on specific technologies like Hadoop, Cloudera, Spark, Kafka, etc
Job Responsibilities
Deploy Elastic stack cluster on native kubernetes or Kubernetes services and maintain the clusters efficiently
Leading end to end deployment of ITOA / AIOps solutions for enterprise customers
Provide engineering inputs to Architects and Data scientists on various stages of solution design
Perform Integration and deployment of ITOA solution as per design provided by Architects
Participate & be an active member of internal capability building projects
Train & support junior resources as needed
Provide resolution to customer queries and issues
Skills Required:
Excellent knowledge on log analytics, time series data anomaly detection and correlation of events
Hands-on experience with IT operational data like logs, metrics, events, RDBMS tables, etc and ingesting them into Elastic stack
Expert Knowledge on GO/grok/REGEX/Logstash/Fluentd to perform Extract, Transform and Load for IT operational data into big data repositories like Elasticsearch, Cassandra, Hadoop, etc
Expert level experience in managing large Elastic cluster and in-depth knowledge in Elastic features
Alerting
Security
Curator
Reporting
Monitoring
Backup and resiliency
Kubernetes cluster management
Python/R/Scala languages/Scripting Languages in context of Anomaly Detection & Time Series modelling
Working experience with ITIL Framework
Working knowledge on Apache Hadoop, Spark, Airflow, Cassandra and Kafka ecosystem
Prior experience in deploying Elastic solutions in production environments processing operational data in terms of at least 500 GB / day
Experience with SQL based tools & expertise on any one traditional RDBMS – mySQl; MSSQL;Oracle;DB2 etc
Prior experience with DevOps projects, Github, Jira, Travis, etc
Working knowledge on Windows, Linux and AIX platforms
Working knowledge of Top commercial distributions of the above stack – MapR; Cloudera; Hortonworks etc
Knowledge on shell scripting
Good knowledge on other Top level Apache Big Data technologies like Cassandra, NIFI, Fluentd, Drill, Sentry etc
Excellent understanding on HDFS & other similar Map/Reduce paradigms
Knowledge on 1-2 NoSQL databases – Redis; MongoDB;Cassandra;Neo4j; VoltDB etc
Preferred:
Experience with Elastic ML and real-time operations analytics using Apache suit of products like Spark using Python or Scala
Experience with Kibana plug-in development and other UI development
Experience working with large data sets leveraging distributed systems e.g. Spark/Hadoop.
Tools & Methods (Experience in at least one in each category or similar if not listed below)
Log Analytics – Elastic Search, Apache Solr
Data Pipelines: Logstash, Fluentd, Kafka, Nifi
Languages: Python, PySpark, Spark, Scala, R, Java, Java Script
Visualization : Kibana, Tableau, Cognos
Machine learning – Elastic ML, Python, Spark, Tensorflow, H2O
Streaming: Spark; Storm;
Relational Database technologies: Oracle, Db2, SQL, MySQL,
NoSQl DB’s: MongoDB, Cassandra, Neo4J,Redis, VoltDB, CouchDB
Apache Hadoop Distribution – Apache, Hortonworks, Cloudera, MapR
ETL technologies: Datastage, Informatica, Pentaho DI, SAS DI, SSIS or R, Python based Data munging
Cloud technologies: AWS, Azure, IBM Softlayer
Soft Skills
Excellent Written & Verbal Communication
Excellent Analytical & Virtual troubleshooting skills
Skills to work in team and collaborative environment
Customer/Vendor interaction & co-ordinations

Required Technical and Professional Expertise
7+ years of professional hands on experience in IT operations
Excellent knowledge on log analytics, time series data anomaly detection and correlation of events
Hands-on experience with IT operational data like logs, metrics, events, RDBMS tables, etc and ingesting them into Elastic stack
Expert Knowledge on GO/grok/REGEX/Logstash/Fluentd to perform Extract, Transform and Load for IT operational data into big data repositories like Elasticsearch, Cassandra, Hadoop, etc
Expert level experience in managing large Elastic cluster and in-depth knowledge in Elastic features
Python/R/Scala languages/Scripting Languages in context of Anomaly Detection & Time Series modelling
Working experience with ITIL Framework
Working knowledge on Apache Hadoop, Spark, Airflow, Cassandra and Kafka ecosystem
Prior experience in deploying Elastic solutions in production environments processing operational data in terms of at least 500 GB / day


Preferred Technical and Professional Expertise
Experience with SQL based tools & expertise on any one traditional RDBMS – mySQl; MSSQL;Oracle;DB2 etc
Prior experience with DevOps projects, Github, Jira, Travis, etc
Working knowledge on Windows, Linux and AIX platforms
Working knowledge of Top commercial distributions of the above stack – MapR; Cloudera; Hortonworks etc
Knowledge on shell scripting
Good knowledge on other Top level Apache Big Data technologies like Cassandra, NIFI, Fluentd, Drill, Sentry etc
Excellent understanding on HDFS & other similar Map/Reduce paradigms
Knowledge on 1-2 NoSQL databases – Redis; MongoDB;Cassandra;Neo4j; VoltDB etc



About Business Unit
At Global Technology Services (GTS), we help our clients envision the future by offering end-to-end IT and technology support services, supported by an unmatched global delivery network. It's a unique blend of bold new ideas and client-first thinking. If you can restlessly reinvent yourself and solve problems in new ways, work on both technology and business projects, and ask, ""What else is possible?"" GTS is the place for you!

Your Life @ IBM
What matters to you when you’re looking for your next career challenge?

Maybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities – where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust – where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.

Impact. Inclusion. Infinite Experiences. Do your best work ever.

About IBM
IBM’s greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.

Location Statement
For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBM
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
1,Data Engineer,"Austin, TX 78746",Austin,TX,78746,None Found,None Found,"Strong knowledge of statistics, including hands-on experience with SAS, R, Matlab, Machine Learning, AI.
Experience working with large datasets (1B+ Records)
Knowledge of Big Data or Cloud technologies
Experience with version control (e.g. TFS, SVN or Git) and build tools.
Tableau/BI Tools
","Develop logical data models and processes to transform, cleanse, and normalize raw data into high-quality datasets aligned with our analytical requirements.
Develop and maintain comprehensive controls to ensure data quality and completeness.
Manage data movement through our infrastructure. Streamline existing data workflows to create a flexible, reliable, and faster process.
Develop real-time data transformations and validations.
Identify and onboard new data sources. Collaborate with data vendors and internal stakeholders to define requirements and build interfaces. Troubleshoot and resolve issues with data feeds.
Work with our team of researchers to identify and analyze investment opportunities in real estate and fixed income securities markets.
",None Found,"3-5 years of experience in data analysis and/or management in an enterprise environment within finance, operations or analytics.
Passion for data organization, quality, and reliability.
MS SQL Server, Oracle, Postgres, Hive, Presto, etc. preferred. Experience developing complex efficient queries, designing and building logical data models, and working with large datasets on a relational database system
Experience with at least one language (e.g. Python, C#, Scala, Java).
Strong problem-solving skills.
Must be an intellectually curious self-starter and motivated to continually learn.
Proactive, hardworking team player with excellent communication skills
","Data Engineer – Austin, TX
Amherst is revolutionizing the way U.S. real estate is priced, managed and financed in order to unlock opportunities for all market participants. Driven by data, analytics, and technology, Amherst has a 20-year history of anticipating where the next risks and opportunities are likely to emerge and designing actionable strategies for investors to capitalize on opportunities across residential real estate, commercial real estate and public securities. Amherst, along with its affiliates and subsidiaries, has more than 900 employees, $5 billion under management and approximately $15 billion under advisement and oversight. www.amherst.com.
We are hiring for a Data Engineer to utilize their industry knowledge, technical skills and passion for data to work closely with executive stakeholders and our financial engineering team developing solutions that support and optimize business operations. We’re solving a variety of Big Data challenges and modernizing legacy data loaders as well as exploring the benefits and tradeoffs of other cutting edge tools. In this role, you will be responsible for a variety of duties including; understanding our data, application design and development, SQL query optimization and ensuring accuracy and consistency of data.
Ideal Candidate:
Analytical mindset with the ability to structure and process qualitative data and draw insightful conclusions.
Problem solver able to take a complex business request and transform it into a clean, simple data solution.
Quick learner open to new ideas and technologies, and willing to offer creative solutions.
Ownership and prideful in work and brings new ways and ideas to the table.
Responsibilities:
Develop logical data models and processes to transform, cleanse, and normalize raw data into high-quality datasets aligned with our analytical requirements.
Develop and maintain comprehensive controls to ensure data quality and completeness.
Manage data movement through our infrastructure. Streamline existing data workflows to create a flexible, reliable, and faster process.
Develop real-time data transformations and validations.
Identify and onboard new data sources. Collaborate with data vendors and internal stakeholders to define requirements and build interfaces. Troubleshoot and resolve issues with data feeds.
Work with our team of researchers to identify and analyze investment opportunities in real estate and fixed income securities markets.
Requirements:
3-5 years of experience in data analysis and/or management in an enterprise environment within finance, operations or analytics.
Passion for data organization, quality, and reliability.
MS SQL Server, Oracle, Postgres, Hive, Presto, etc. preferred. Experience developing complex efficient queries, designing and building logical data models, and working with large datasets on a relational database system
Experience with at least one language (e.g. Python, C#, Scala, Java).
Strong problem-solving skills.
Must be an intellectually curious self-starter and motivated to continually learn.
Proactive, hardworking team player with excellent communication skills
Bonus Skills:
Strong knowledge of statistics, including hands-on experience with SAS, R, Matlab, Machine Learning, AI.
Experience working with large datasets (1B+ Records)
Knowledge of Big Data or Cloud technologies
Experience with version control (e.g. TFS, SVN or Git) and build tools.
Tableau/BI Tools
What We Offer:
Competitive salaries
Choice of Mac or Windows hardware
Flexible vacation days, paid holidays, and work from home options
Medical, Dental, Vision, LTD, Life, EAP, and 401K with matching benefits
Stellar colleagues with proven track records
Free sodas, kombucha, cold brew, beer and healthy and unhealthy snacks at our Barton Springs WeWork location
Free lunch every day and breakfast tacos on Thursdays!
Amherst is proud to be an Equal Opportunity Employer and committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, color, religion, national origin, gender, pregnancy, sexual orientation, gender identity, age, physical or mental disability, genetic information or veteran status, and encourage all applicants to apply."
2,Data Engineer/Analytic Manager,"Austin, TX 78746",Austin,TX,78746,None Found,None Found,None Found,None Found,None Found,None Found,"ABOUT THE TEAM:
Our team oversees all aspects of eBay’s finance, analytics, and information technology functions – including controllership, procurement, financial planning and analysis, tax, treasury, audit, mergers and acquisitions, and investor relations. At eBay, we love data, so finance plays a critical role in establishing strategic focus, enabling growth, ensuring execution and driving efficiencies across the organization.
JOB REQUIREMENTS
Global Collections analytics team is responsible for delivering business insights and performance insights for Collections and Treasury.
We are currently seeking a Data Engineer/Analytic Manager to lead the team and support our department. In this role you will support internal collections teams, own development and implementation of analysis and help prioritize key bad debt reduction initiatives through decisions based on data without impact on top line revenue.
CHARACTERISTICS:
IN THIS ROLE THE MANAGER WILL:Deliver insights into customers, processes and products to drive improvements in collections and reducing bad debt for eBay
Conduct post-mortem loss analysis on strategic growth initiatives and share updates with the collections leadership team
Perform exploratory data analysis to develop hypotheses for causes of bad debt
Develop SQL queries to extract data for both analysis and model construction
Design eye-catching visualizations in Tableau and build supporting calculations
Build database tables and schemas to support the assembly of data for analytics
Develop SQL code for ETL projects
Support critical Collections metrics and build out self-serve reporting that will strengthen decision quality through meaningful data
Demonstrate the ability to work in ambiguity using experience and proven theory knowledge
Partner with other cross functional teams within eBay to use standard methodology, share data analysis and establish business cases
Provide cross-functional, data-driven recommendations for lowering bad debt while remaining aligned with other team goals
Assess events on customer invoice timeline to drive better business outcomes
BASIC QUALIFICATIONS
If you work well in a demanding environment, you learn quickly and possess extensive collection/risk management experience, this is a role you should consider. Being able to demonstrate the ability to breakdown processes, look for immediate opportunities and have a vision for future needs is what we need. In this role you will have the opportunity to scope and size things globally, recommend solutions that are scalable and balanced while considering future growth expectations for the company.
Analytics experience in Tech or Financial Industries
Proficient in SQL
Experience developing Tableau dashboards with a deep understanding of associated data architecture trade-offs
+5 years’ analytics/database experience
+5 years Software development, software testing, or operational improvement experience preferred
Experience working with groups remotely and multiple cultures
Proficient in MS Office applications, Tableau, SQL, Teradata, and Unix commands
Ability to think from the perspective multiple user and employee personas
Familiarity with traditional collection metrics (roll rates, liquidation and charge off rates)
Proficient in Microsoft Excel, Word and PowerPoint
Collections and Loss Financial Data Analysis Experience (preferred)
Fluency in English is a requirement with excellent communication skills
Excellent collaboration skills, team-work, and influencing business outcomes across a matrixed environment
BA/BS degree required
We love creating opportunities for others by connecting people from widely diverse backgrounds, perspectives, and geographies. So, being diverse and inclusive isn’t just something we strive for, it is who we are, and part of what we do each and every single day. We want to ensure that as an employee, you feel eBay is a place where, no matter who you are, you feel safe, included, and that you have the opportunity to bring your unique self to work.. To learn about our Diversity & Inclusion click here: https://www.ebayinc.com/our-company/diversity-inclusion/.
This website uses cookies to enhance your experience. By continuing to browse the site, you agree to our use of cookies
View our privacy policy
View our accessibility info
eBay Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talent@ebay.com. We will make every effort to respond to your request for disability assistance as soon as possible.
For more information see:
EEO is the Law Poster
EEO is the Law Poster Supplement"
3,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description

Our Data Engineering team builds and maintains a secure, scalable, flexible and user-friendly analytics hub that allows us to make informed and data-driven decisions. They also construct and curate business-critical data sets that allow us to realize the value of all the data we collect.
A Data Engineer utilizes a multidisciplinary approach to providing ETL solutions for the business, combining technical, analytical, and domain knowledge. The perfect applicant for this role has strong development skills, experience transforming and profiling data to determine risks associated with proposed analytics solutions, a willingness to continually interface with analysts in order to determine an optimal approach, and an eagerness to explore data sources to understand the availability, utility, and integrity of our data.
What you'll own:
Data pipeline / ETL development:
Building and enhancing data curation pipelines using tools like SQL, Python, Glue, Spark and other AWS technologies
Focus on data curation on top of datalake data to produce trusted datasets for analytics teams
Data Curation:
Processing and cleansing data from a variety of sources to transform collected data into an accessible and curated state for Analysts and Data Scientists
Migrating self-serve data pipeline to centrally managed ETL pipelines
Advanced SQL development and performance tuning
Some exposure to Spark, Glue or other distributed processing frameworks helpful
Work with business data stewards & analytics team to research and identify data quality issues to be resolved in the curation process
Data Modeling:
Design and build master dimensions to support analytic data requirements
Replacing legacy data structures with new datasets sourced from streaming data feeds from the core product and other operational systems
Design, build and support pipelines to deliver business critical datasets
Resolve complex data design issues & provide optimal solutions that meet business requirements and benefit system performance
Query Engine Expertise & Performance Tuning:
Assist Analytics teams with tuning efforts
Curated dataset design for performance
Orchestration:
Management of job scheduling
Dependency management mapping and support
Documentation of issue resolution procedures
Data Access
Design and management of data access controls mapped to curated datasets
Leveraging devops best practices, such as IAC and CI/CD to build upon a scalable and extensible data environment

Experience you'll need:
Strong experience designing and building end-to-end data pipelines
Extensive SQL development experience
Knowledge of data management fundamentals and data storage principles
Data modeling:
Normalization
Dimensional/OLAP design and data warehousing
Master data management patterns
Modeling trade-offs impacting data management & processing/query performance
Knowledge of distributed systems as it pertains to data storage, data processing and querying
Extensive experience in ETL and DB performance tuning
Hands on experience with a scripting language (Python, bash, etc.)
Some experience with Hadoop, Spark, Kafka, Impala, or other big data technologies helpful

Familiarity with the technology stacks available for:
Metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Data management, data processing and curation:
Postgres, Hadoop, Hive, Impala, Presto, Spark, Glue, etc.
Experience in data modeling for batch processing and streaming data feeds; structured and unstructured data
Experience in data security / access management, data cataloging and overall data environment management

Experience with cloud services such as AWS and APIs helpful
You’d be a great fit if your current track record looks like this:
5+ years of progressive experience data engineering and data warehousing
Experience with a variety of data management platforms (e.g. RDBMS (Postgres), Hadoop (CDH, EMR))
Experience with high performance query engines (Hive, Impala, Presto, Athena, MPP engines like RedShift)
Strong capability to manipulate and analyze complex, high-volume data from a variety of sources
Effective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language
Ability to problem solve independently and prioritize work based on the anticipated business value

Qualifications

null

Additional Information

All your information will be kept confidential according to EEO guidelines."
4,Data Engineering Manager,"Austin, TX",Austin,TX,None Found,None Found,"
Experience leading, managing and hiring a team of talented engineers
Expertise in at least one of the following engineering domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Expertise in at least one of the following data domains: * Predictive analytics (e.g., recommendation systems, predictive maintenance)
Natural language processing (e.g., conversational chatbots)
Document understanding
Image classification
Marketing analytics
IoT systems
Experience writing software in one or more languages such as Python or Java/Scala
Experience in technical consulting or customer-facing role
Excellent critical thinking, problem-solving and analytical skills
",None Found,None Found,None Found,None Found,"Join SADA as a Data Engineering Manager!

Your Mission

As a Data Engineering Manager at SADA, you will build and lead a growing Data Engineering team as we deliver robust data solutions for our clients on Google Cloud Platform (GCP). You will be responsible for managing a blended team of data engineers and data scientists, so a broad background in Big Data, data warehouse modernization, analytics, disaster recovery, data science, and machine learning is highly advantageous.

The diversity of customers that SADA works with ensures a steady flow of challenging data work. Be prepared to tackle real-world data problems that our customers find too difficult or time-consuming to solve themselves. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of data domain areas. Management here at SADA also means developing people and being a leader.

In this role, you will:

Be comfortable working with customer executives to align business outcomes with technical vision and goals.
Guide the day-to-day activities of a geographically distributed team, including hiring world-class talent, reviewing work and setting goals.
Provide technical and professional leadership and mentorship on a diverse range of subject matter areas, such as Big Data pipelines and data warehouses to statistics and machine learning.
Develop and codify best practices for your team that can be replicated across multiple customer engagements.
Partner with your team to develop services and offerings that scale and are repeatable.
Participate in key technical and design discussions with technical leads as a hands-on manager.
Partner with other practice leads, architects, project managers, executives and sales personnel to develop statements of work, and then oversee execution by your team with high levels of agility and quality.

Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our employees know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing data practice area with vision and passion. You will be measured by your team’s performance on customer engagements, how well your team achieves internal organizational goals, how well you collaborate with and support your team and peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the management growth track.

Expectations


Required Travel - 15-25% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Experience leading, managing and hiring a team of talented engineers
Expertise in at least one of the following engineering domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Expertise in at least one of the following data domains: * Predictive analytics (e.g., recommendation systems, predictive maintenance)
Natural language processing (e.g., conversational chatbots)
Document understanding
Image classification
Marketing analytics
IoT systems
Experience writing software in one or more languages such as Python or Java/Scala
Experience in technical consulting or customer-facing role
Excellent critical thinking, problem-solving and analytical skills

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience in a large scale, high-volume data warehouse environment
Experience operationalizing machine learning models on large datasets
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
5,Sr. Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"ABOUT THIS ROLE
As a member of our engineering team, you will work on the design, implementation and delivery of data platform frameworks, pipelines, microservices, and other features that build on our high-value data assets. Your customers will include data analytics, marketing, and leadership teams as well as our Care Partners (hospitals and physicians) and external data partners, in concert with our Enterprise Data Warehouse delivery teams.
RESPONSIBILITIES
Evaluate and build proofs of concept for Cloud PaaS and IaaS offerings for data liquidity, data management and storage, data pipelines built on both traditional ETL as well as streaming platforms, master data management, data stewardship, record-linkage, NLP services, and more.
Write traditional code and server-less functions using the right language for the task, which may be SQL, Python, C#, Java, PowerShell, SSIS/BIML, and others.
Participate in build-buy-open source decisions for parsing and managing industry standard formats such as FHIR/NDJSON, pipe-and-hat HL7, and x12 EDI
Evaluate, select, and apply Cloud and OO design and resiliency patterns
Build APIs and data microservices to share our data with internal and external partners, and write interfaces to public data sets to enrich our analytics data stores
Provide subject matter expertise on performance tuning and query optimization to full-stack peers, data analysts, and EDW developers
Participate in building and owning a DevOps culture
Continuously document your code, framework standards, and team processes
EDUCATION, TRAINING, AND PROFESSIONAL EXPERIENCE
5+ years of experience in an enterprise or commercial software development environment
Extensive experience developing data-intensive solutions against an RDBMS, such as SQL Server, Postgres or Oracle.
Highly skilled writing SQL queries, DML and DDL, CDC/change tracking patterns, indexes and performance tuning.
Enterprise development experience coding in at least one, but preferably more than one, procedural/OO language, e.g., C#, Java, JavaScript, Python, C++, PowerShell
Proficiency in using OOTB components, as well as implementing custom components or frameworks, on at least one traditional ETL platform, preferably SSIS, Informatica, or Talend
Team player who is not afraid to ask questions, take risks, share in owning team victories as well as team failures
Good communicator – both written and verbal – with high emotional intelligence
Ability to focus on MVP and shipping software while remaining cognizant of the long-term costs of technical debt
Healthcare data background a must
MUST HAVE THE RIGHT TO WORK IN THE US WITHOUT VISA SPONSORSHIP
Ideal candidates come to the table with one or more additional competencies, such as:
Exposure to Enterprise Data Warehouse , Data Lake, Big Data, unstructured data, in-memory data stores
Familiarity with NoSQL database systems such as MongoDB, Cassandra, CosmosDB, neo4j etc.
Familiarity with Kimball-like star and snowflake data models and columnstore Indexing
Experience building metadata-driven data pipeline frameworks for quickly mapping, onboarding, and ingesting data from a wide variety of partner sources
Enterprise experience with data movement and management in the Cloud utilizing some combination of Azure and/or AWS features such as Data Factory, Blob Storage, Service Bus, Kafka, Redis, S3 Buckets, Azure Automation, Machine Learning, elastic search, Glue etc.
Data Science training or experience to better understand and collaborate with one of our key data consumers (notably, this is still an engineering role and not a data science role)
CRM experience, such as MS Dynamics or SalesForce
ABOUT US

At Bright Health, we brought together the brightest minds from the health care industry and consumer technology and together we created Bright Health: a new, brighter approach to healthcare, built for individuals. Our plans are easy to manage, personalized and more affordable, giving people the quality care they deserve. Through our exclusive care partnerships with leading health systems in local communities we are reshaping how people and physicians achieve better health together.

Bright Health is tripling its footprint in 2019 to offer a variety of health insurance plans to more individuals. Bright Health operates health insurance offerings across Individual and Family Plan segments and the Medicare Advantage space in Alabama, Arizona, Colorado, Ohio, New York and Tennessee.

We’re Making Healthcare Right. Together.

We've won some fun awards like: Great Places to Work, Modern Healthcare, Forbes, etc. But more than anything, we're a group of people who are really dedicated to our mission in healthcare. Come join our growing team!

As an Equal Opportunity Employer, we welcome and employ a diverse employee group committed to meeting the needs of Bright Health, our consumers, and the communities we serve. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law."
6,Software Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"So what’s our story? We believe our value is in helping our clients do something they never dreamed possible. Giving them that certain moment when clarity becomes confidence. Finding a whole new customer segment. Reaching everyone who needs to be reached. Understanding those tiny market nuances. There’s more, of course, but these are the kinds of things that make the difference to our clients – the things that help them sleep at night.


Are you self-motivated with a proven track record of delivering results? Do you have a demonstrated ability to think strategically in complex situations, motivate and mobilize resources, and deliver over the top results? This Big Data Engineer role will be provide the opportunity to do all of the above while driving architecture design, data modeling, and implementation of Big Data platform and analytic applications.
What you will do:
Fulfill requests from the product team such as building upon our Acxiom Unified Data Layer(UDL) product base, optimizing data processing via Spark/Scala, researching new innovation ideas.
Optionally solve additional challenges in our service tier in NodeJS
Write spark jobs and components in Scala programming language
Work in an agile teaming environment
Consistently provides proven, formal mentorship
Regularly lead self and others and/or established as Product SME and/or established as specialist
Understands how whole picture aligns to overall Acxiom strategy
Works with product manager to maximize the components ROI
Expected more thought leadership
Have a broad understanding of the external events that may impact applications or systems (networking, operations, etc.)
What you will need:
3+ years experience in Big Data Hadoop, Hive and Spark with hands on expertise in design and implementation of high data volume solution
Good knowledge of configuring Spark and working on multi node clusters and distributed data processing framework.
Hands on knowledge of Big Data Analytics and Predictive Analytics
Extremely agile with good knowledge of relational DWs as well as NoSQL DBs like, MongoDB etc.,
Strong in Spark Scala pipelines (both ETL & Streaming)
Proficient in Spark architecture
Cloud developer experience in leading cloud providers (AWS, Azure, GCP)
Strong development skills in Node.js for REST apis
Experience with real-time data ingestion using streaming technologies such as Kafka and Kinesis.
#GD17
Primary Location City/State:
Austin, Texas"
7,Data Engineer,"Austin, TX 78758",Austin,TX,78758,None Found,None Found,None Found,None Found,None Found,None Found,"Summary
Posted: Oct 8, 2019
Weekly Hours: 40
Role Number: 200041718
At Apple, excellent ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis? If so, Apple's Global Business Intelligence (GBI) team is seeking an expert Data Engineer to build high quality, scalable and resilient distributed systems that power apple's analytics platform and data pipelines. Apple's Enterprise Data warehouse system cater to a wide variety of real-time, near real-time and batch analytical solutions. These solutions are integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet services enabling business drivers to make critical decisions. We use a diverse technology stack such as Teradata, HANA, Vertica, Hadoop, Kafka, Spark, and Cassandra and beyond. Designing, Developing and scaling these big data technologies are a core part of our daily job. The team member will be able think outside of the box and should have passion for building analytics solutions to enable business in making time sensitive and critical decisions.
Key Qualifications
We would like for you to have In-depth understanding of data structures and algorithms
We are looking for experience in designing and building dimensional data models to improve accessibility, efficiency, and quality of data
Database development experience with Relational or MPP/distributed systems such as Oracle/Teradata/Vertica/Hadoop
We are seeking programming experience in building high quality software in Java, Python or Scala preferred
Experience in designing and developing ETL data pipelines. Should be proficient in writing Advanced SQLs, Expertise in performance tuning of SQLs
You will demonstrate excellent understanding of development processes and agile methodologies
Strong analytical and interpersonal skills
Self-driven, highly motivated and ability to learn quick
Experience with or advance courses on data science and machine learning is ideal
Work/project experience with big data and advanced programming languages is a plus
Experience developing Big Data/Hadoop applications using java, Spark, Hive, Oozie, Kafka, and Map Reduce is a huge plus
Description
You will build and design data structures on MPP platform like Teradata, Hadoop to provide efficient reporting and analytics capability. Design and build highly scalable data pipelines using new generation tools and technologies like Spark, Kafka to induct data from various systems.
Translate complex business requirements into scalable technical solutions meeting data warehousing design standards. Strong understanding of analytics needs and proactive-ness to build generic solutions to improve the efficiency Build dashboards using Self-Service tools like Tableau and perform data analysis to support business
Collaborate with multiple cross functional teams and work on solutions which has larger impact on Apple business
We seek a self starter, visionary person with strong leadership capabilities
Ability to communicate effectively, both written and verbal, with technical and non-technical multi-functional teams
You will interact with many other group’s internal team to lead and deliver elite products in an exciting rapidly changing environment
Dynamic, smart people and inspiring, innovative technologies are the norm here. Will you join us in crafting solutions that do not yet exist?
Education & Experience
Bachelor’s Degree"
8,Junior Data Engineer,"Austin, TX 78705",Austin,TX,78705,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for a Junior Data Engineer to join our Data Engineering team in Austin. This team member will be responsible for working with the team to expand and optimize our cloud-native big data pipeline architecture, as well as optimize data flow and collection for the teams we support. The ideal candidate is excited to work closely with a team of engineers and analysts and to learn new tools and technologies in the data and data engineering space and already has an understanding of SQL, database structures, and software development processes. Curiosity, a great attitude, and an aptitude for data and software are highly valued.

About Us
We are the leading technology provider for the $38 billion self-storage industry. Our solutions, which includes more than 15,000 facilities, offers consumers the ability to find, compare and book self-storage, manage the operations of their storage business, and a host of other capabilities that make them better business managers . We provide a suite of industry-leading web marketing tools for self-storage operators and have been chosen as a preferred partner by more major online brands than any other self-storage company.

In 2018, SpareFoot acquired SiteLink and storEDGE, global leaders in self-storage management software and in-house payment processing. The deal allows the combined business to accelerate investment, drive innovation and generate value for both consumers and facility operators.

We are now relaunching the combined company under one name and consolidating the way that all of the merged and acquired entities go to market.

For three consecutive years, SpareFoot has been recognized by Entrepreneur Magazine and the Austin American-Statesman for an exceptional company culture.

What you’ll do everyday:
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet both functional and non-functional requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologies
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needs
Create data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our product
Work with data and analytics experts to strive for greater functionality in our data systems
What you need to bring to the table:
Computer science degree or equivalent experience
2+ years experience in software development, data engineering, BI development, and / or data architecture
Experience with Python, SQL, AWS, RESTful APIs, and Tableau or other data visualization tools"
9,Healthcare Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"What You'll Do
Work closely with the various areas of the business to design and implement requirements and develop processes necessary to provide visibility into the data via the data warehouse
Ensure data that is brought into the data warehouse is clean, accurate, available and complete
Architect, develop, implement and test algorithms that consist of value-add routes and build data warehouse infrastructure for automated interpretation of healthcare data
Identify ways to improve data reliability, efficiency and quality. Produce actionable recommendations that address known problems and then implement automated solutions
Assist the development teams with business knowledge of various healthcare data that is ingested from various sources
Notches in your belt:
5+ years of professional SQL programming experience
In depth experience with SQL and NOSQL databases
5+ years working with health plan data
Strong knowledge of healthcare data, specifically health plan (i.e. medical and Rx claims, eligibility, provider data)
Deep understanding with healthcare coding terminologies (CPT, diagnosis, DRG, etc)
Experience in designing and building efficient and scalable solutions for big data
Experience with query development and optimization
Experience with common data warehouse and data lake architecture concepts and best practices
Proven work experience with algorithm design and implementation
Strong problem solving skills
Ability to independently manage all phases of development including requirement documentation, building, configuration, bug tracking, testing, and validation
Strong experience with cloud-based infrastructure
Strong communication skills between business and technical resources

Great to have:
Experience providing database endpoints and/or the creation of RESTful API’s
Previous experience in the Python/Django framework
Previous knowledge of data integration processes and tools (specifically AWS based)
Experience with healthcare data formats (processing 834, 837, X12 file formats)
Experience with health system EHR data
Experience leveraging Redshift and other AWS data pipelines / tools
Advanced degree in Computer Science
An active GitHub profile or other public code portfolio

How to Apply

To apply, email careers@icanbwell.com with your resume, your GitHub account, and why you’d love to help us change the face of healthcare. Please use the subject line: “Apply - Healthcare Data Engineer”
You will also enjoy a fun, collaborative, and exciting culture that is fully immersed in Austin & Baltimore’s health and tech communities. We work hard for our mission and are compensated accordingly. We offer competitive salaries, 401(k), and medical, dental, and vision insurance."
10,Senior Systems Administrator (Sharepoint)- Senior Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Overview
By Light is hiring a Senior Systems Administrator (Sharepoint)- Senior Data Engineer to join our team supporting the Department of Veterans Affairs.
Responsibilities
Experience in planning and supporting IT business processes
Working knowledge of SharePoint
 Knowledge of all phases of software development with emphasis on the planning, analysis, modeling, simulation, testing, integration, documentation and presentation phases
Document ITS business processes including both process descriptions (or operating procedures) and process maps
Analyze process gaps and assist in process improvement and/or re-engineering activities
Coach management to explore and advise SharePoint technology possibilities
Maintain the existing custom SharePoint workflow applications, and managing authentication and authorization in line with VA and FSC policies and processes
Work with functional stakeholders to diagnose problems of custom SharePoint workflow applications to troubleshoot for changes and/or updates
Solicit/elaborate Functional user requirements in the effort of developing workflows to automate business processes
Lead the effort in custom workflow solution design on SharePoint to automate business collaboration processes
Lead the effort in measuring business process performance by designing and maintaining measurement dashboards
Determine how each functional operation works and maintain an awareness of change management as it applies to implementing and/or updating custom SharePoint workflow applications
Required Experience/Qualifications
Bachelor's Degree
A minimum of 5 years’ progressive experience
Preferred Experience/Qualifications
Practical knowledge in business process improvement and re-engineering disciplines
Being familiar with Lean Six Sigma, ITIL, and/or CMMI practices
Experience in leading business process improvement efforts in IT service organizations
Ability to standardize business processes and operating procedures
Ability to elaborate requirements from business needs
Experience in business process mapping and documentation, business process/workflow automation and measurement using proven technologies
Experience working with SharePoint as a super user, i.e. ability to design zero-code custom applications and automated workflows on SharePoint
Sharp skills in PowerApps, MS Flows, Excel VBA, and Power BI
Excellent customer service, communication and organizational skills are required
Capable to work under pressure, handle multiple tasks simultaneously
Experience providing services to the federal government and/or the Dept. of Veterans' Affairs is preferred
Special Requirements/Security Clearance
Candidates must successfully complete a background investigation"
11,Senior Data Engineer,"Austin, TX 78705",Austin,TX,78705,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for a Data Engineer to join our Data Architecture team in Austin. This team member will be responsible for expanding and optimizing our cloud-native big data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing distributed systems as well as building them from the ground up.

The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

About Us:
We are the leading technology provider for the $38 billion self-storage industry. Our solutions, which includes more than 15,000 facilities, offers consumers the ability to find, compare and book self-storage, manage the operations of their storage business, and a host of other capabilities that make them better business managers . We provide a suite of industry-leading web marketing tools for self-storage operators and have been chosen as a preferred partner by more major online brands than any other self-storage company.

In 2018, SpareFoot acquired SiteLink and storEDGE, global leaders in self-storage management software and in-house payment processing. The deal allows the combined business to accelerate investment, drive innovation and generate value for both consumers and facility operators.

We are now relaunching the combined company under one name and consolidating the way that all of the merged and acquired entities go to market.

For three consecutive years, SpareFoot has been recognized by Entrepreneur Magazine and the Austin American-Statesman for an exceptional company culture.

What you’ll do everyday:
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet both functional and non-functional requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologies
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Work closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needs
Create data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our product
Work with data and analytics experts to strive for greater functionality in our data systems
What you need to bring to the table:
Computer science degree or equivalent experience
7+ years experience in software development, data engineering, BI development, and / or data architecture
Experience with Python, SQL, AWS, RESTful APIs, and Tableau or other data visualization tools
Consistent track record of leading successful delivery for a large-scale project or being a key contributor on multiple projects
Consistent track record of positively influencing project direction and contributions to cross-functional and/or cross-organizational collaborations"
12,Senior Application Support Analyst (Temp. 6 months),"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Named as one of Fortunes’ 100 Fastest Growing Companies for 2019, EPAM is committed to providing our global team of 30,100+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential.

Description

You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Application Support Analyst. Scroll down to learn more about the position’s responsibilities and requirements.

EPAM Systems is seeking a Cloud Platform/Big Data Support Specialist to provide enterprise-level support to customers of a major cloud service provider. As a cloud support specialist, you will work in a team of experienced support engineers to resolve customer's concerns and issues for using cloud platform and big data products.

You would use your technical expertise and communication skills to understand customer's problem, provide technical assistance, then guide them to resolution. You would also participate in discussion with product engineers to share your insights on customer needs and issues to help product improvements. You will be fully exposed to the cutting edge technologies of a prominent cloud service provider, and play a key role in the growth of their cloud computing products.

#LI-DNI
What You’ll Do
Provide technical assistance and support over e-mail, chat and phone as part of a global 24x7-support organization
Provide initial response to customer's inquiry, troubleshoot, provide updates, identify root case, and resolve the issue to the satisfaction of customer
Handle escalation from customer and lead to satisfactory resolution
Co-work with engineers across technical and product domains to resolve complex cross-domain issues
Consult with senior engineers and subject matter experts (SME) to accelerate problem resolution
Hand-off or take-over cases to/from other geographical region to provide around-the-clock issue resolution for premium customers
Follow communication guidelines and security policies when communicating with customer
Categorize support requests for support and service analytics
Produce support documents, perform knowledge sharing and training
Keep technical skills up to date with latest cloud technologies
What You Have
A degree in an associated field and/or other advanced certification along with significant experience
Strong analytical / troubleshooting / problem solving skills
Strong verbal and written communication skills
Excellent customer service skills
Ability to perform job functions under stress and pressure
Commitment to continuous self-learning
Regular, reliable attendance
3+ years of experience as developer or a combination developer + big data engineer
Proficient in at least one of the following development languages: Java, Python, .NET, Ruby, PHP, Go or Javascript (NodeJS)
Hands on experience with RESTful APIs
Experience with relational databases (e.g. MySQL, PostgreSQL, etc.)
Experience with Big Data architectures and technologies and BI solutions
Experience in CI/CD, DevOps and related automation tools (e.g. Jenkins, Chef, Puppet, etc.)
Ability to read and understand code and able to write code samples to reproduce customer issues
Ability to read and understand logs and stack traces to troubleshoot issues
Good oral and written business communication skills in English (CEF Level C1 or above)
Must be able to work on the following shifts:
Early week shift from 7:00 AM to 6:00 PM, Sunday to Wednesday
Late week shift from 7:00 AM to 6:00 PM, Wednesday to Saturday
Yes, you will work 4 days and take 3 days off
May need to work on public holidays. If worked on a public holiday, you will be provided with a day-off in lieu
Nice to have
BA/BS degree preferred
2+ years of customer support experience preferably in Enterprise software support
Experience with PaaS and IaaS technologies
Experience with distributed computing frameworks (e.g. Hadoop, Spark, Flink, Storm, Samza, Beam, Airflow, Google Big Query, etc.)
Experience with distributed data stores (HBase, Cassandra, Riak, Google Bigtable, Amazon Dynamo DB, etc.) and/or distributed message brokers (Kafka, RabbitMQ, ActiveMQ, Google Pub/Sub, Amazon Kinesis, etc.)
Experience with ETL processes and tools (e.g. AWS Glue, Google Dataprep and/or Datafusion, MS SSIS, ODI, IPC, etc.)
Experience with any ML library (scikit-learn, XGBoost, pytorch, tensorflow, Spark mllib) or basic understanding of ML concepts
What We Offer
Medical, Dental and Vision Insurance (Subsidized)
Health Savings Account
Flexible Spending Accounts (Healthcare, Dependent Care, Commuter)
Short-Term and Long-Term Disability (Company Provided)
Life and AD&D Insurance (Company Provided)
Employee Assistance Program
Unlimited access to LinkedIn learning solutions
Matched 401(k) Retirement Savings Plan
Paid Time Off
Legal Plan and Identity Theft Protection
Accident Insurance
Employee Discounts
Pet Insurance"
13,Reporting and Data Engineer-Signify Community,"Austin, TX 78730",Austin,TX,78730,None Found,"Bachelor’s degree in Engineering, Mathematics, Computer Science or related field
Clinical and healthcare relevance; a Clinical degree isn’t required, but a deep understanding of the healthcare market is.
","Solid knowledge of SQL queries and relational databases
Good understanding of math and statistics
Deep experience with Excel and/or other data manipulation tools
Familiarity with data visualization tools (e.g. Looker, Tableau)
Programming experience with R/Python for data analysis preferred
","Assist in the design, preparation and distribution of reports and dashboards for end-users, management, and key stakeholders.
Use statistical methods to ensure metrics are well defined and match to the customer’s goals and desired outcomes documented during the discovery process.
Support the standardization of reporting for end-users.
Serve as internal expert user of the reporting system for input to the ongoing improvement and development of reporting tools.
Communicate reporting enhancements to internal and external users through presentations and documentation.
Seek and analyze trends and patterns across communities, populations, and contacts to assist in product management and development.
","Bachelor’s degree in Engineering, Mathematics, Computer Science or related field
Clinical and healthcare relevance; a Clinical degree isn’t required, but a deep understanding of the healthcare market is.
","Bachelor’s degree in Engineering, Mathematics, Computer Science or related field
Clinical and healthcare relevance; a Clinical degree isn’t required, but a deep understanding of the healthcare market is.
","Position Overview:
Data and human connection come together in our mission to bring communities together to collaborate and solve Social Determinants of Health – a career at Signify Community (a subsidiary of Signify Health) is a career with purpose. Our employees are empathic, passionate, confident, innovative change agents who aren’t afraid to take risks and also have fun.
Reporting to the Director of Data Intelligence, the Reporting and Data Engineer is responsible for building the tools and reports that enable us to discover and share insights about our customers, as well as identifying system and workflow enhancements to Signify Community’s platform. The Reporting and Data Engineer will uncover novel ways of connecting, comparing and using information from across communities, populations and contacts to provide key input for product development and management.
The ideal candidate thrives on the opportunity to collect and assimilate data and has a passion to collaborate closely with other analysts, engineers, and customer facing teams to drive decision-making and build innovative products that help people live better lives.
If this sounds like an exciting opportunity, and you are interested in realizing our mission to bring communities together to collaborate, address unmet social needs, and improve outcomes, contact us to learn more!
Qualifications:
Education/Licensing Requirements:
Bachelor’s degree in Engineering, Mathematics, Computer Science or related field
Clinical and healthcare relevance; a Clinical degree isn’t required, but a deep understanding of the healthcare market is.

Experience Requirements:
2+ years of experience with healthcare data.
2+ years of data analysis

Essential Skills/Experience:
Solid knowledge of SQL queries and relational databases
Good understanding of math and statistics
Deep experience with Excel and/or other data manipulation tools
Familiarity with data visualization tools (e.g. Looker, Tableau)
Programming experience with R/Python for data analysis preferred

Essential Characteristics:
Accuracy/Attention to Detail - Diligently attends to details and pursues quality in accomplishing tasks.
Analysis/Reasoning - Examines data to grasp issues, draw conclusions, and solve complex business problems.
Critical Listening - Makes a concerted effort to listen to what people are saying and what they are trying to say; thinks carefully before responding and avoids jumping to conclusions; asks thoughtful questions to elicit more information; maintains positive and supportive demeanor to allow meaningful conversation.
Adaptability - Maintains job performance and focus under pressure; adapts to changing needs; able to respond appropriately to new information and schedule changes; handles stress in an appropriate manner; uses appropriate skills, tools, and techniques to manage time when accomplishing specific tasks.
Organization/Time Management - Establishes course of action for self and others to ensure that the work is completed efficiently, without jeopardizing excellent service; prioritizes tasks appropriately; adjusts priorities when appropriate; able to stay focused on the task at hand; allocates appropriate amount of time for completing own work; quickly able to determine roadblocks to accomplishing a goal.
Essential Job Responsibilities:

To perform this job successfully, an individual must be able to perform each essential function satisfactorily, with or without reasonable accommodation.
Assist in the design, preparation and distribution of reports and dashboards for end-users, management, and key stakeholders.
Use statistical methods to ensure metrics are well defined and match to the customer’s goals and desired outcomes documented during the discovery process.
Support the standardization of reporting for end-users.
Serve as internal expert user of the reporting system for input to the ongoing improvement and development of reporting tools.
Communicate reporting enhancements to internal and external users through presentations and documentation.
Seek and analyze trends and patterns across communities, populations, and contacts to assist in product management and development.

Working Conditions:
Ability to work well in a fast-paced environment.
Enjoys working with people to address their needs, and being a partner in achieving good health.
Ability to work at a desk and to use a phone and computer.
Ability to use office equipment and machinery effectively.
Ability to work effectively with frequent interruptions."
14,Principal Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,"
Lead efforts to design, build, scale, and maintain multiple data pipelines
Architect highly scalable data solutions
Be a technical thought leader within the org
Work closely with business owners and external stakeholders to provide actionable data
Ensure data accuracy and reliability
",None Found,"
Experience building large scale streaming and batch data pipelines
Experience using Big Data technologies (Spark, EMR, hadoop, data lakes, etc.)
Mastery of multiple databases (e.g. MongoDB, MySQL, etc.)
Understanding of data security best practices
","Crowdskout is looking for a Lead Data Engineer that can both architect and lead efforts around our expanding data pipeline infrastructure. Crowdskout's product has most recently been centered in the CRM space, but we are looking to expand far beyond that. Currently, we process millions of data points through multiple data pipelines to feed a suite of datastores and applications. We are preparing for +10x growth both in the volume of data processed and the speed in which that data can be available and actionable. To accomplish this we are looking for someone who can architect and lead the effort to build out highly scalable data solutions.

If you are highly motivated, super passionate about democracy, and want to join a close-knit team that is looking to build great things together, Crowdskout may be for you. This is a full-time position in Austin, TX.

Responsibilities:

Lead efforts to design, build, scale, and maintain multiple data pipelines
Architect highly scalable data solutions
Be a technical thought leader within the org
Work closely with business owners and external stakeholders to provide actionable data
Ensure data accuracy and reliability

Requirements:

Experience building large scale streaming and batch data pipelines
Experience using Big Data technologies (Spark, EMR, hadoop, data lakes, etc.)
Mastery of multiple databases (e.g. MongoDB, MySQL, etc.)
Understanding of data security best practices

Extras:

AWS data technologies (e.g. Kinesis, Glue, RDS, Athena, Redshift, etc.)
Experience building out data warehouse infrastructure
DevOps or System Admin experience
Data Science exploration and modeling

Crowdskout is an equal opportunity employer that encourages diversity across all spectrums in its hiring, without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, or any other protected factor. With that being said, we wouldn't be able to accommodate candidates in need of work sponsorship at this time since we are a small company. If you find this role interesting and you hit on the elements above, please apply!"
15,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"ABOUT THIS ROLE
As a member of our engineering team, you will work on the design, implementation and delivery of data platform frameworks, pipelines, microservices, and other features that build on our high-value data assets. Your customers will include data analytics, marketing, and leadership teams as well as our Care Partners (hospitals and physicians) and external data partners, in concert with our Enterprise Data Warehouse delivery teams.
RESPONSIBILITIES
Evaluate and build proofs of concept for Cloud PaaS and IaaS offerings for data liquidity, data management and storage, data pipelines built on both traditional ETL as well as streaming platforms, master data management, data stewardship, record-linkage, NLP services, and more.
Write traditional code and server-less functions using the right language for the task, which may be SQL, Python, C#, Java, PowerShell, SSIS/BIML, and others.
Participate in build-buy-open source decisions for parsing and managing industry standard formats such as FHIR/NDJSON, pipe-and-hat HL7, and x12 EDI
Evaluate, select, and apply Cloud and OO design and resiliency patterns
Build APIs and data microservices to share our data with internal and external partners, and write interfaces to public data sets to enrich our analytics data stores
Provide subject matter expertise on performance tuning and query optimization to full-stack peers, data analysts, and EDW developers
Participate in building and owning a DevOps culture
Continuously document your code, framework standards, and team processes
EDUCATION, TRAINING, AND PROFESSIONAL EXPERIENCE
1 - 5 years of experience in an enterprise or commercial software development environment
Extensive experience developing data-intensive solutions against an RDBMS, such as SQL Server, Postgres or Oracle.
Highly skilled writing SQL queries, DML and DDL, CDC/change tracking patterns, indexes and performance tuning.
Enterprise development experience coding in at least one, but preferably more than one, procedural/OO language, e.g., C#, Java, JavaScript, Python, C++, PowerShell
Proficiency in using OOTB components, as well as implementing custom components or frameworks, on at least one traditional ETL platform, preferably SSIS, Informatica, or Talend
Team player who is not afraid to ask questions, take risks, share in owning team victories as well as team failures
Good communicator – both written and verbal – with high emotional intelligence
Ability to focus on MVP and shipping software while remaining cognizant of the long-term costs of technical debt
Healthcare data background a must
MUST HAVE THE RIGHT TO WORK IN THE US WITHOUT VISA SPONSORSHIP
Ideal candidates come to the table with one or more additional competencies, such as:
Exposure to Enterprise Data Warehouse , Data Lake, Big Data, unstructured data, in-memory data stores
Familiarity with NoSQL database systems such as MongoDB, Cassandra, CosmosDB, neo4j etc.
Familiarity with Kimball-like star and snowflake data models and columnstore Indexing
Experience building metadata-driven data pipeline frameworks for quickly mapping, onboarding, and ingesting data from a wide variety of partner sources
Enterprise experience with data movement and management in the Cloud utilizing some combination of Azure and/or AWS features such as Data Factory, Blob Storage, Service Bus, Kafka, Redis, S3 Buckets, Azure Automation, Machine Learning, elastic search, Glue etc.
Data Science training or experience to better understand and collaborate with one of our key data consumers (notably, this is still an engineering role and not a data science role)
CRM experience, such as MS Dynamics or SalesForce
ABOUT US

At Bright Health, we brought together the brightest minds from the health care industry and consumer technology and together we created Bright Health: a new, brighter approach to healthcare, built for individuals. Our plans are easy to manage, personalized and more affordable, giving people the quality care they deserve. Through our exclusive care partnerships with leading health systems in local communities we are reshaping how people and physicians achieve better health together.

Bright Health is tripling its footprint in 2019 to offer a variety of health insurance plans to more individuals. Bright Health operates health insurance offerings across Individual and Family Plan segments and the Medicare Advantage space in Alabama, Arizona, Colorado, Ohio, New York and Tennessee.

We’re Making Healthcare Right. Together.

We've won some fun awards like: Great Places to Work, Modern Healthcare, Forbes, etc. But more than anything, we're a group of people who are really dedicated to our mission in healthcare. Come join our growing team!

As an Equal Opportunity Employer, we welcome and employ a diverse employee group committed to meeting the needs of Bright Health, our consumers, and the communities we serve. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law."
16,"Training Solutions Advisor, Google Cloud","Austin, TX 78731",Austin,TX,78731,None Found,None Found,None Found,"
Understand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.
Connect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.
Partner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.
Support escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.
Partner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.
",None Found,None Found,"Note: By applying to this position your application is automatically submitted to the following locations: Sunnyvale, CA, USA; Austin, TX, USA; New York, NY, USA; Reston, VA, USA
Minimum qualifications:

Bachelor's degree in Computer Science or a related technical field, or equivalent practical experience.
5 years of experience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.
Experience working in a customer facing environment in a technology company and helping customers identify solutions that best fit their unique needs.
Ability to travel to support Customer Engagements up to 30% of the time.

Preferred qualifications:

Google Cloud Certified e.g. Cloud Architect, Data Engineer or Associate Cloud Engineer or other comparable Cloud Certification.
Experience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.
Ability to take customers’ technical requirements and architect a proposal that maps to technical skills required.
Ability to quickly learn and understand new training offerings.
Ability to work well cross functionally and understand when to pull in specialist knowledge into Customer conversations.
Excellent communication and presentation skills.
About the job
The Google Cloud team helps customers transform and evolve their business through the use of Google’s global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you'll help shape the future of businesses of all sizes and enable them to better use technology to drive innovation.This role will enable you to make a huge impact across Google Cloud’s most strategic accounts and ensure they have Learning Plans that effectively help them develop the knowledge and skills they need to adopt Google Cloud. The role is also an exciting mix of elements as you will be working in Technical Customer Facing activities (usually in partnership with the CE or PSO/TAM Account owner), working in partnership with Cloud Learning GTM leads on Learning Plan development, and working with the Curriculum Tech leads to provide curriculum feedback and validate proposals as required.

Google Cloud helps millions of employees and organizations empower their employees, serve their customers, and build what’s next for their business — all with technology built in the cloud. Our products are engineered for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. And our teams are dedicated to helping our customers and developers see the benefits of our technology come to life.
Responsibilities
Understand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.
Connect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.
Partner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.
Support escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.
Partner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.
At Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form."
17,Senior Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,"
Mastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role
",None Found,None Found,None Found,None Found,"Join SADA as a Sr. Data Engineer!

Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Mastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Hihg
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
18,Principal Data Engineer,"Austin, TX 78723",Austin,TX,78723,None Found,"Develop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server
Play a lead role in migrating data from legacy systems to cloud platforms
Create custom data sets for use by business analysts and data scientists
Rapidly prototype new data sets for exploratory analysis
Use SQL skills to manage data
Monitor database performance and tuning to improve query performance
Design and automate data pipelines to integrate different data sources using SSIS
Collaborate with IT partners to move data prototypes into production
Develop real-time data integrations in MS SQL Server
Establish techniques to monitor data quality and implement remediation procedures
Working directly with non-technical users to identify complex needs/requirements and translate into technical solutions
Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler
Create data training programs
Train analyst community on data sources and best practices
Lead other staff on SSIS, performance tuning, and database administration
",None Found,"Develop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server
Play a lead role in migrating data from legacy systems to cloud platforms
Create custom data sets for use by business analysts and data scientists
Rapidly prototype new data sets for exploratory analysis
Use SQL skills to manage data
Monitor database performance and tuning to improve query performance
Design and automate data pipelines to integrate different data sources using SSIS
Collaborate with IT partners to move data prototypes into production
Develop real-time data integrations in MS SQL Server
Establish techniques to monitor data quality and implement remediation procedures
Working directly with non-technical users to identify complex needs/requirements and translate into technical solutions
Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler
Create data training programs
Train analyst community on data sources and best practices
Lead other staff on SSIS, performance tuning, and database administration
",None Found,None Found,"At Texas Mutual, we're creating a stronger, safer Texas. Here, you will explore and create new data sets and work with business and IT partners to support our corporate data strategy. Join one of the best companies to work for in Texas, reporting into the Chief Data Office. Located in the Mueller development, we offer modern sit/stand workstations, free on-site fitness center, and free garage parking.
Responsibilities & Qualifications
In this Role:
Develop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server
Play a lead role in migrating data from legacy systems to cloud platforms
Create custom data sets for use by business analysts and data scientists
Rapidly prototype new data sets for exploratory analysis
Use SQL skills to manage data
Monitor database performance and tuning to improve query performance
Design and automate data pipelines to integrate different data sources using SSIS
Collaborate with IT partners to move data prototypes into production
Develop real-time data integrations in MS SQL Server
Establish techniques to monitor data quality and implement remediation procedures
Working directly with non-technical users to identify complex needs/requirements and translate into technical solutions
Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler
Create data training programs
Train analyst community on data sources and best practices
Lead other staff on SSIS, performance tuning, and database administration
Required Qualifications:
At least 10 years of related data experience
Bachelor's degree in a related field
Experience in the design, development, implementation and support of SQL Server
Experience with SSIS, Alteryx or similar ETL tools ﻿﻿ ﻿﻿
Our Benefits:
Day one health, dental, and vision insurance
Performance bonus
401k plan with 4% basic employer contribution and 100% employer match contribution up to 6%
Vacation, sick, holiday and volunteer time off
Life and disability insurance
Flexible spending account
Free on-site gym and fitness classes
Professional development
Tuition reimbursement
Pet insurance
Free identity theft protection
Company-sponsored social and philanthropy events
Texas Mutual Insurance Company is an Equal Employment Opportunity employer."
19,Data Engineer – Elastic Engineer,"Austin, TX 73344",Austin,TX,73344,None Found,None Found,None Found,None Found,None Found,None Found,"Introduction
At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities
We are looking for a Data engineer to deploy complex big data and analytics solutions using Elastic stack. You should have strong experience in deploying and managing Elastic cluster on Kubernetes in multi-site, multi cluster environment in both on-Premise as well as Cloud platforms. You should have applied or expert knowledge in big data platforms. The main use case for such a platform for us would be Real-Time Anomaly Detection and Time Series models on IT operations data like logs, metrics, events, wired data, transaction flow, ITIL process related data, knowledge repositories, etc
Business Unit/ Team Overview
Global Technology Services (GTS) at IBM manages the IT infrastructure for some of the world’s leading corporations and with that comes the responsibility of managing enormous amounts of IT data and the opportunity for making better decisions using that data. In GTS analytics team at IBM, Data Scientists, Data Engineers, and BigData IT Architects are developing novel models, cutting edge algorithms, and custom analytics solutions to tackle BigData challenges in the IT Infrastructure space.
ITOA / AIops provides real time machine-data (log, events, performance, capacity, ITIL data, wire data, etc) analytics solutions that helps customers manage Business Services and manage the quality of the end-user experience
It can tell a client in real time ‘What happened’, ‘Why did it happen’, ‘Will it happen again’ and ‘What to do if it happens again? etc
Keeps everyone on the same page by looking at the same Business Transaction data and metrics.
Keeps the focus on operational data that translate to the business value the application delivers; dive in deeper when appropriate.
Identify resolution criteria, assign ownership
Take lessons learned to improve development, test, deployment, and production processes
Education & Experience
Minimum 4 years of relevant experience working on the Elastic based products & distributions specifically used in Real Time ITOA or AIops use-cases processing logs, metrics, events, etc
At least 4 years of experience in development & implementation of logging and metrics solutions in with TB+ / day ingestion per day
At least 5 years of hands-on experience in IT support (Infrastructure / Application) and IT monitoring tools
Overall 7+ years of core Big data / Analytics experience in various domains
Degree / Master’s degree in computers or equivalent
Certifications:
Elastic certified engineer
Certifications showing proficiency in the Usage, design & deployment of ITOA / AIOps solutions like Elastic or Splunk
Specialized certifications on specific technologies like Hadoop, Cloudera, Spark, Kafka, etc
Job Responsibilities
Deploy Elastic stack cluster on native kubernetes or Kubernetes services and maintain the clusters efficiently
Leading end to end deployment of ITOA / AIOps solutions for enterprise customers
Provide engineering inputs to Architects and Data scientists on various stages of solution design
Perform Integration and deployment of ITOA solution as per design provided by Architects
Participate & be an active member of internal capability building projects
Train & support junior resources as needed
Provide resolution to customer queries and issues
Skills Required:
Excellent knowledge on log analytics, time series data anomaly detection and correlation of events
Hands-on experience with IT operational data like logs, metrics, events, RDBMS tables, etc and ingesting them into Elastic stack
Expert Knowledge on GO/grok/REGEX/Logstash/Fluentd to perform Extract, Transform and Load for IT operational data into big data repositories like Elasticsearch, Cassandra, Hadoop, etc
Expert level experience in managing large Elastic cluster and in-depth knowledge in Elastic features
Alerting
Security
Curator
Reporting
Monitoring
Backup and resiliency
Kubernetes cluster management
Python/R/Scala languages/Scripting Languages in context of Anomaly Detection & Time Series modelling
Working experience with ITIL Framework
Working knowledge on Apache Hadoop, Spark, Airflow, Cassandra and Kafka ecosystem
Prior experience in deploying Elastic solutions in production environments processing operational data in terms of at least 500 GB / day
Experience with SQL based tools & expertise on any one traditional RDBMS – mySQl; MSSQL;Oracle;DB2 etc
Prior experience with DevOps projects, Github, Jira, Travis, etc
Working knowledge on Windows, Linux and AIX platforms
Working knowledge of Top commercial distributions of the above stack – MapR; Cloudera; Hortonworks etc
Knowledge on shell scripting
Good knowledge on other Top level Apache Big Data technologies like Cassandra, NIFI, Fluentd, Drill, Sentry etc
Excellent understanding on HDFS & other similar Map/Reduce paradigms
Knowledge on 1-2 NoSQL databases – Redis; MongoDB;Cassandra;Neo4j; VoltDB etc
Preferred:
Experience with Elastic ML and real-time operations analytics using Apache suit of products like Spark using Python or Scala
Experience with Kibana plug-in development and other UI development
Experience working with large data sets leveraging distributed systems e.g. Spark/Hadoop.
Tools & Methods (Experience in at least one in each category or similar if not listed below)
Log Analytics – Elastic Search, Apache Solr
Data Pipelines: Logstash, Fluentd, Kafka, Nifi
Languages: Python, PySpark, Spark, Scala, R, Java, Java Script
Visualization : Kibana, Tableau, Cognos
Machine learning – Elastic ML, Python, Spark, Tensorflow, H2O
Streaming: Spark; Storm;
Relational Database technologies: Oracle, Db2, SQL, MySQL,
NoSQl DB’s: MongoDB, Cassandra, Neo4J,Redis, VoltDB, CouchDB
Apache Hadoop Distribution – Apache, Hortonworks, Cloudera, MapR
ETL technologies: Datastage, Informatica, Pentaho DI, SAS DI, SSIS or R, Python based Data munging
Cloud technologies: AWS, Azure, IBM Softlayer
Soft Skills
Excellent Written & Verbal Communication
Excellent Analytical & Virtual troubleshooting skills
Skills to work in team and collaborative environment
Customer/Vendor interaction & co-ordinations

Required Technical and Professional Expertise
7+ years of professional hands on experience in IT operations
Excellent knowledge on log analytics, time series data anomaly detection and correlation of events
Hands-on experience with IT operational data like logs, metrics, events, RDBMS tables, etc and ingesting them into Elastic stack
Expert Knowledge on GO/grok/REGEX/Logstash/Fluentd to perform Extract, Transform and Load for IT operational data into big data repositories like Elasticsearch, Cassandra, Hadoop, etc
Expert level experience in managing large Elastic cluster and in-depth knowledge in Elastic features
Python/R/Scala languages/Scripting Languages in context of Anomaly Detection & Time Series modelling
Working experience with ITIL Framework
Working knowledge on Apache Hadoop, Spark, Airflow, Cassandra and Kafka ecosystem
Prior experience in deploying Elastic solutions in production environments processing operational data in terms of at least 500 GB / day


Preferred Technical and Professional Expertise
Experience with SQL based tools & expertise on any one traditional RDBMS – mySQl; MSSQL;Oracle;DB2 etc
Prior experience with DevOps projects, Github, Jira, Travis, etc
Working knowledge on Windows, Linux and AIX platforms
Working knowledge of Top commercial distributions of the above stack – MapR; Cloudera; Hortonworks etc
Knowledge on shell scripting
Good knowledge on other Top level Apache Big Data technologies like Cassandra, NIFI, Fluentd, Drill, Sentry etc
Excellent understanding on HDFS & other similar Map/Reduce paradigms
Knowledge on 1-2 NoSQL databases – Redis; MongoDB;Cassandra;Neo4j; VoltDB etc



About Business Unit
At Global Technology Services (GTS), we help our clients envision the future by offering end-to-end IT and technology support services, supported by an unmatched global delivery network. It's a unique blend of bold new ideas and client-first thinking. If you can restlessly reinvent yourself and solve problems in new ways, work on both technology and business projects, and ask, ""What else is possible?"" GTS is the place for you!

Your Life @ IBM
What matters to you when you’re looking for your next career challenge?

Maybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities – where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust – where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.

Impact. Inclusion. Infinite Experiences. Do your best work ever.

About IBM
IBM’s greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.

Location Statement
For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBM
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
20,Big Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,"
Build and Support scalable and reliable data solutions that can enable self-service reporting and advanced analytics at Cloudflare using modern data lake and EDW technologies (Hadoop, Spark, Cloud, NoSQL etc.) in a agile manner.
Strong understanding of business and product data needs.
Close partnership with internal stakeholders and partners from Engineering, product, and business(Finance, Sales, Customer Experience, Marketing etc.).
Active role in hiring and growing the team in Austin with data Engineers, analysts, and data scientists.
",None Found,"
Bachelor's or Master's Degree in Computer Science or Engineering or related experience required.
3+ years of development experience in Big data space working with Petabytes of data and building large scale data solutions.
Solid understanding of Google Cloud Platform, Hadoop, Python, Spark, Hive, and Kafka.
Experience in all aspects of data systems(both Big data and traditional) including data schema design, ETL, aggregation strategy, and performance optimization.
Capable of working closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality.
Experience in hiring data Engineers preferred.
Experience in Internet security industry preferred.
","About Us

At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world's largest networks that powers trillions of requests per month. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare have all web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was recognized by the World Economic Forum as a Technology Pioneer and named to Entrepreneur Magazine's Top Company Cultures list.

We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!

About the department

Cloudflare is looking to build and grow our Business Intelligence team responsible for building large-scale enterprise data lake and EDW from different sources and enabling various product and business teams such as Marketing, Customer Support, Sales, Finance with key business dashboards/reporting, insights and recommendation models.

About the role

As part of this initiative, we are looking for a Big Data(EDW/Analytics) Engineers to come join Cloudflare and help us build a scalable petabyte scale data lake and EDW using modern tech stack from the ground up. Success in this role comes from marrying a strong data engineering background with product and business acumen to deliver scalable data pipeline and BI solutions that can enable self-service analytics at Cloudflare in a simple and standard manner. This person will also play a crucial role in hiring and growing the business intelligence team in Austin in a rapid manner.

What we look for: Agile Delivery & Execution, Engineering Excellence, Tech Savvy, Business & Product acumen, Creative Problem solver

Responsibilities:

Build and Support scalable and reliable data solutions that can enable self-service reporting and advanced analytics at Cloudflare using modern data lake and EDW technologies (Hadoop, Spark, Cloud, NoSQL etc.) in a agile manner.
Strong understanding of business and product data needs.
Close partnership with internal stakeholders and partners from Engineering, product, and business(Finance, Sales, Customer Experience, Marketing etc.).
Active role in hiring and growing the team in Austin with data Engineers, analysts, and data scientists.

Requirements:

Bachelor's or Master's Degree in Computer Science or Engineering or related experience required.
3+ years of development experience in Big data space working with Petabytes of data and building large scale data solutions.
Solid understanding of Google Cloud Platform, Hadoop, Python, Spark, Hive, and Kafka.
Experience in all aspects of data systems(both Big data and traditional) including data schema design, ETL, aggregation strategy, and performance optimization.
Capable of working closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality.
Experience in hiring data Engineers preferred.
Experience in Internet security industry preferred.

What Makes Us Special

We're not just a highly ambitious, large-scale technology company. We're a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.

Project Galileo ( https://blog.cloudflare.com/protecting-free-expression-online/ ): We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare's enterprise customers--at no cost.

Project Athenian ( https://www.cloudflare.com/athenian/ ): We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.

Path Forward Partnership ( https://blog.cloudflare.com/tag/path-forward/ ): Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.

1.1.1.1 ( https://1.1.1.1/ ): We released 1.1.1.1 ( https://1.1.1.1/ ) to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here's the deal - we don't store client IP addresses never, ever. We will continue to abide by our privacy policy ( https://developers.cloudflare.com/1.1.1.1/commitment-to-privacy/privacy-policy/privacy-policy/ ) and ensure that no user data is sold to advertisers or used to target consumers.

Sound like something you'd like to be a part of? We'd love to hear from you!

Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.

Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107."
21,Sr. Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,"6+ years of professional experience as a data engineer or in a similar role
4+ years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL
Hands-on experience writing SQL, Perform SQL optimization and tuning
Strong work experience with Python scripting or equivalent
Working knowledge with AWS RDS, Data Lakes and data analytics
Must be able to work in a diverse team environment
Must possess problem-solving skills and ability to multitask
Outstanding analytical skills and problem-solving abilities, drive for results, attention to quality and detail, and a collaborative attitude
Bachelor’s degree in Computer Science or Engineering, or equivalent experience
",None Found,"Overall responsibility for day-to-day data pipeline operations and manage database acquisition and data delivery methods effectively
Serve as a lead data engineer to manage the development of web services for data posting and delivery
Provide technical leadership and expertise on data integration and data delivery
Perform data quality procedures to ensure data consistency and data integrity
Ensure proper documentation of data posting and related API objects
Work directly with product and engineering team to understand data needs and provide end-to-end data solutions that address customer requirements
Perform other duties as assigned
",None Found,"6+ years of professional experience as a data engineer or in a similar role
4+ years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL
Hands-on experience writing SQL, Perform SQL optimization and tuning
Strong work experience with Python scripting or equivalent
Working knowledge with AWS RDS, Data Lakes and data analytics
Must be able to work in a diverse team environment
Must possess problem-solving skills and ability to multitask
Outstanding analytical skills and problem-solving abilities, drive for results, attention to quality and detail, and a collaborative attitude
Bachelor’s degree in Computer Science or Engineering, or equivalent experience
","About the Role:
As the Data Engineer, you will join our engineering team and build world-class data solutions for RealMassive. You will have oversight of the commercial real estate data pipeline process and develop best practices and recommendations for the data acquisition and data delivery method for the company. You will be the expert on the entire data solution from data collection, transformation, ingestion, and data delivery.
This role reports directly to the VP of Engineering and is located in Austin, TX.

Responsibilities:
Overall responsibility for day-to-day data pipeline operations and manage database acquisition and data delivery methods effectively
Serve as a lead data engineer to manage the development of web services for data posting and delivery
Provide technical leadership and expertise on data integration and data delivery
Perform data quality procedures to ensure data consistency and data integrity
Ensure proper documentation of data posting and related API objects
Work directly with product and engineering team to understand data needs and provide end-to-end data solutions that address customer requirements
Perform other duties as assigned
Requirements
Required Qualifications:
6+ years of professional experience as a data engineer or in a similar role
4+ years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL
Hands-on experience writing SQL, Perform SQL optimization and tuning
Strong work experience with Python scripting or equivalent
Working knowledge with AWS RDS, Data Lakes and data analytics
Must be able to work in a diverse team environment
Must possess problem-solving skills and ability to multitask
Outstanding analytical skills and problem-solving abilities, drive for results, attention to quality and detail, and a collaborative attitude
Bachelor’s degree in Computer Science or Engineering, or equivalent experience
Preferred Qualifications:
Knowledge of advanced analytics tools and Agile development methodologies
AWS RDS PostgreSQL skills and experience is a plus
Experience working with AWS Glue, EMR, and Kinesis
Experience with data science and machine learning
Working knowledge with big data and advanced programming languages
Experience in working with data visualization tools such as Tableau
Benefits
Best-in-class benefits: medical, dental, and vision
Company sponsored long-term disability
Unlimited vacation
Maternity/paternity leave
Flexible working options
Suite of optional benefits include HSA, FSA, and short-term disability
Enjoy catered lunch several times per week + snacks + fancy coffees
Why join us:
At RealMassive, our strength is our knowledgeable, passionate, and creative people. We are commercial real estate’s only real-time, big data solution and property marketplace dedicated to providing innovative technology solutions to modernize a $15 Trillion industry. Our products are designed to break down the barriers that have traditionally isolated data and insights from those who benefit the most from an open data environment. We strive to make the lives of our customers easier through information and access, which in turn helps local communities around the country grow and thrive.
Whether you work in engineering, data science, product, sales or operations, at RealMassive, we are all focused on the same mission and vision – to be the world’s source of commercial real estate data and insights. We are focused, dedicated and committed to improving the lives of our customers by staying experimental, agile and innovative.
Our culture is built on inclusion, integrity and ideation. We value the voices, thoughts and passions of every employee and believe that success is collaborative, not individual. When you join RealMassive, you’ll instantly help our customers make better decisions, create opportunities, move faster and improve the commercial real estate transaction experience.
About RealMassive:
RealMassive, commercial real estate’s real-time data provider and marketplace, is the industry’s source for searching, finding and evaluating commercial property and markets around the country. The marketplace makes it easy for business owners, entrepreneurs, brokers, developers and investors to find the perfect property to start and grow their business in small rural communities or major metropolitan areas.
With innovative data solutions, RealMassive is the industry’s largest data source of commercial property data that is updated and verified weekly. The data collected, standardized and enriched by RealMassive provides our customers with faster, more effective information to make decisions relating to leasing, acquiring and dispositioning commercial properties.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
How to apply:
If your career preference is to work in a fast-paced entrepreneurial environment, where your success is measured by your contribution to the team—we should talk.
Please complete the online application as well as the behavioral assessment at the following link: https://assess.predictiveindex.com/Gvjan"
22,Sr. Data Engineer - Payments Systems Risk,"Austin, TX",Austin,TX,None Found,None Found,"
Minimum of 2-3 years’ experience in production ETL pipelines, utilizing big data engineering techniques that enable statistical solutions to solve business problems
Post graduate degree in Computer Science/ Engineering, Information Science or a related discipline with strong technical experiences highly desired
Previous exposure to financial services, credit cards or merchant analytics is a plus, but not required
Extensive experience with SQL and big data technologies (Hadoop, Python , Java, Spark, Hive etc.) tools for large scale data processing, data transformation and machine learning pipelines
Experience with data visualization and business intelligence tools like Tableau, Microstrategy, or other programs highly desired
Experience with SAS as a statistical package is preferred
Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred",None Found,None Found,None Found,None Found,"Job Description

To ensure that Visa’s payment technology is truly available to everyone, everywhere requires the success of our key bank or merchant partners and internal business units. The Global Data Science group supports these partners by using our extraordinarily rich data set that spans more than 3 billion cards globally and captures more than 100 billion transactions in a single year.
Are you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.
As a Senior Data Engineer, you will be responsible for helping to blueprint and deliver modelled attributes, data assets, and self-serve workflows that solve clients' business objectives. You will get the chance to leverage your business acumen and technical knowledge of big data and data mining techniques. Based on deep understanding and knowledge of big-data engineering techniques, you will develop and maintain data and tools to enable data scientists to draw fact based insights and build models This function is critical in building market-relevant client solutions and intellectual property for Visa.
Essential Functions
Work with manager and clients to fully understand business requirements and desired business outcomes
Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions
Build and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists
Perform other tasks on R&D, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis
Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users
Execute data engineering projects ranging from small to large either individually or as part of a project team
Ensure project delivery within timelines and budget requirements
Provide coaching and mentoring to junior team members

Qualifications

Basic Qualifications:
2 years of work experience with a Bachelor’s Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD, or PhD)
Preferred Qualifications:
Minimum of 2-3 years’ experience in production ETL pipelines, utilizing big data engineering techniques that enable statistical solutions to solve business problems
Post graduate degree in Computer Science/ Engineering, Information Science or a related discipline with strong technical experiences highly desired
Previous exposure to financial services, credit cards or merchant analytics is a plus, but not required
Extensive experience with SQL and big data technologies (Hadoop, Python , Java, Spark, Hive etc.) tools for large scale data processing, data transformation and machine learning pipelines
Experience with data visualization and business intelligence tools like Tableau, Microstrategy, or other programs highly desired
Experience with SAS as a statistical package is preferred
Familiarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred
Additional Information

Work Hours
The incumbent must make themselves available during core business hours.
Travel Requirements
The position requires the incumbent to travel for work 5% of the time.
Physical Requirements
This position will be performed in an office setting. The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers, reach with hands and arms, and bend or lift up to 25 pounds.
Visa will consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law."
23,Cloud Solutions Architect,"Austin, TX",Austin,TX,None Found,None Found,"
Expertise in at least one of the following domain areas:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes the full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio.
Application Development: building custom web and mobile applications on top of the GCP stack.
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Excellent written and verbal communication skills with the ability to interface with and communicate complex technical concepts to a broad range of stakeholders.
Hands-on experience with cloud computing, traditional on-premises and enterprise data-center technologies.
Experience working with engineering and sales teams.
Experience producing technical assets or writing technical documentation, including, but not limited to, architecture designs and documentation, statements of work, project plans, and working code samples.
Time management with the ability to manage multiple streams.
",None Found,None Found,None Found,None Found,"Join SADA as a Cloud Solutions Architect!

Your Mission

As a Cloud Solutions Architect at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You’re also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.

You will be an established contributor within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures successfully. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.

Pathway to Success

#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.

You will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.

As you continue to execute successfully, we will build a customized development plan together that leads you through the solutions architecture or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events.
Customer Facing - This is very customer-facing role. You will usually interact with customers on a daily basis. You will participate on calls and onsite customer meetings to qualify consultative engagements with the engineering teams. You will present architecture proposals and code samples to build trust, confidence, and consensus both externally and internally.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Cloud Architect Certified

[https://cloud.google.com/certification/cloud-architect] and/or Google
Professional Data Engineer Certified
[https://cloud.google.com/certification/data-engineer], or able to complete one of the above within the first 45 days of employment.

Required Qualifications:

Expertise in at least one of the following domain areas:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes the full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio.
Application Development: building custom web and mobile applications on top of the GCP stack.
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Excellent written and verbal communication skills with the ability to interface with and communicate complex technical concepts to a broad range of stakeholders.
Hands-on experience with cloud computing, traditional on-premises and enterprise data-center technologies.
Experience working with engineering and sales teams.
Experience producing technical assets or writing technical documentation, including, but not limited to, architecture designs and documentation, statements of work, project plans, and working code samples.
Time management with the ability to manage multiple streams.

Useful Qualifications:

Direct experience working with a variety of cloud technologies as well as designing and recommending elegant solutions that drive business outcomes.
Understanding of infrastructure automation, continuous integration/deployment, relational/NoSQL data stores, security, networking, and cloud-based delivery models.
Ability to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance and security.
Thought leadership with the ability to recommend cloud-native approaches to solve customer business and technical challenges.
Understanding of best practices, design patterns and reference architectures with an uncanny ability to recommend these as needed.

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
24,Senior Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,"
B.S. in Computer Science/Engineering and 5 years of professional software development experience or equivalent.
3+ years experience with Python using Django or Flask
Experience with Python Celery or other task/job management frameworks
5+ years of experience in a software development environment
Experience with AWS services
Experience with data modeling techniques
3+ years experience with PostgreSQL or other SQL server
","NarrativeDx is expanding our software development team and we are looking for smart, talented engineers that have a history of getting products complete and in the hands of customers. For this position we are searching for a data engineer with experience architecting data processing pipelines and working alongside data scientists to adapt data models into production systems. The position requires experience developing web applications in python and experience deploying applications on AWS. This position will be involved with a wide variety of development tasks on the engineering team and will be the primary link between our research team and our application development team. Candidates must have experience with: python server application development, web application development with exposure to frontend visualizations of datasets, SQL databases and query optimization, and scaling data processing.
Requirements
B.S. in Computer Science/Engineering and 5 years of professional software development experience or equivalent.
3+ years experience with Python using Django or Flask
Experience with Python Celery or other task/job management frameworks
5+ years of experience in a software development environment
Experience with AWS services
Experience with data modeling techniques
3+ years experience with PostgreSQL or other SQL server
Nice to haves:
Experience with natural language processing
Experience with sentiment classification
Experience with computational linguistics
Benefits
NarrativeDx is a 6 year old company with VC backing. We have a great office in downtown Austin with good parking, a great healthcare package, 401K Match and flexible schedules. We are a small close-knit team where we have fun and work smart and we keep our team small by hiring smart experienced people!"
25,SQL Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,"Bachelor's Degree in Computer Science, Engineering, Math or related technical field (8 years of additional experience can be substituted for education)
5+ years' relevant experience
Experience in Data Platform Languages such as SSIS and TSQL
Experience in Data Platform Tools such as SSIS, SSDT, SSMS and/or Visual Studio
Experience working in an environment using Agile methodology
Experience in Data Engineering concepts such as ETL, ELT or performance tuning
Hands on experience writing SQL scripts
Strong Communication, Presentation and Facilitation Skills. Must be able to explain data quality issues and impacts to a non-technical audience",None Found,"Develop, implement and maintain a scalable data management architecture to support the storage and querying of large datasets
Create and maintain data pipelines to automate the processing of large data sets
Help design and maintain efficient data collection workflows with other groups within the company
Manage and perform data analysis to identify data quality issues
Propose new technologies that could improve the way data is handled
Manage data security and provide efficient access to engineering teams
Communicate technical data and approaches to both technical and non-technical audiences
Perform Database maintenance
Building and analyzing dashboards and reports
Evaluating and defining metrics and perform exploratory analysis
Monitoring key product metrics and understanding root causes of changes in metrics
Empower and assist operation and product teams through building key data sets and data-based recommendations
Automating analyses and authoring pipelines via SQL/python based ETL framework",None Found,None Found,"Overview
ProSphere is seeking an experienced SQL Data Engineer to provide highly specialized applications and operational analysis. The Engineer assists with planning and supporting network and computing infrastructure and has knowledge of networking technologies. The Engineer is cognizant of all phases of software development with emphasis on the planning, analysis, modeling, simulation, testing, integration, documentation, and presentation phases.

This is full-time position located in Austin, TX. Veterans are encouraged to apply.
Responsibilities
Develop, implement and maintain a scalable data management architecture to support the storage and querying of large datasets
Create and maintain data pipelines to automate the processing of large data sets
Help design and maintain efficient data collection workflows with other groups within the company
Manage and perform data analysis to identify data quality issues
Propose new technologies that could improve the way data is handled
Manage data security and provide efficient access to engineering teams
Communicate technical data and approaches to both technical and non-technical audiences
Perform Database maintenance
Building and analyzing dashboards and reports
Evaluating and defining metrics and perform exploratory analysis
Monitoring key product metrics and understanding root causes of changes in metrics
Empower and assist operation and product teams through building key data sets and data-based recommendations
Automating analyses and authoring pipelines via SQL/python based ETL framework
Qualifications
Bachelor's Degree in Computer Science, Engineering, Math or related technical field (8 years of additional experience can be substituted for education)
5+ years' relevant experience
Experience in Data Platform Languages such as SSIS and TSQL
Experience in Data Platform Tools such as SSIS, SSDT, SSMS and/or Visual Studio
Experience working in an environment using Agile methodology
Experience in Data Engineering concepts such as ETL, ELT or performance tuning
Hands on experience writing SQL scripts
Strong Communication, Presentation and Facilitation Skills. Must be able to explain data quality issues and impacts to a non-technical audience
Physical Demands
Typical office environment. Ability to sit and stand for extended periods of time
Ability to lift 5-20 lbs.

ProSphere offers full-time employees a comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.

It is ProSphere’s policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability or any other characteristic protected by applicable federal, state or local law."
26,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,7+ Years Data Engineering5+ Years RDBMS Management2+ Years of NoSQLAWS Ecosystem knowledgeSolid Data Modeling/Design Experience,"Come join our team at Cerity! We are a cutting edge insure tech company working with some really cool technologies. We have an opening for a data engineer on our growing team. We are looking for highly motivated, startup minded engineers to join our team of stellar engineers. Our data engineers work with latest big data tech including NoSQL (DynamoDB), AWS Aurora, AWS Data pipeline, Kinesis and Snowflake DB. Come help us revolutionize insurance technology.

Responsibilities will include:Designing and Implementing RDBMS Data ModelsDesigning and Implementing NoSQL Data ModelsImplementing ETL solutionsWorking with DevOps to establish infrastructure as codeFixing any issues that arise with data functionality
Requirements7+ Years Data Engineering5+ Years RDBMS Management2+ Years of NoSQLAWS Ecosystem knowledgeSolid Data Modeling/Design Experience
BenefitsCompetitive SalariesAnnual Bonus ProgramGreat Health and other BenefitsUnlimited PTO"
27,Data Analyst,"Austin, TX 78716",Austin,TX,78716,None Found,None Found,None Found,None Found,None Found,None Found,"Who are we? Raybeam Inc. is a software engineering consulting company focused on strategic consulting, business intelligence, and online/database marketing for the past twenty years. We have offices near Boston and San Francisco and support a strong list of clients including Google, Facebook, Microsoft, eBay, One Kings Lane, Disney and Hilton Worldwide. We are in the process of opening a new office in Austin, TX.

What do we do? We provide technology solutions by architecting and developing enterprise systems using a variety of programming languages, tools and platforms. This can range from building data warehouses, to web applications to implementing reporting platforms. We work in small teams, own the projects that we work on, and have direct input into the business decisions of our clients.

What are we looking for? We are looking for a technically savvy database-oriented Analyst or Data-Engineer with good people skills and the ability to pick up new business concepts and technologies.

The ideal candidate will possess:

A strong to very strong working knowledge of SQL and python.
An ability to write and troubleshoot complex SQL procedures.
The desire to understand business events through data.• An understanding of Data Warehousing and ETL techniques.
High level understanding in at least one scripting language such as Ruby, Shell, Python.
An interest in learning large data set processing with MapReduce/Hadoop/Pig/etc.
A minimum of 8 years experience.
Linux skills are a plus.
Good client relations skills strongly preferred.
If you are interested in applying for the position please click on the link below to take a 10 minute quiz.

http://careerseval.raybeam.com/sign_in

Please note that Raybeam, Inc. is not E verified and is unable to provide sponsorship. We will only consider local candidates. Recent grads are encouraged to apply, and an MBA is desirable. This is a full time contract role. Thank You"
28,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,"Bachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with a federal government customer.
Experience with big data tools: Hadoop, Spark, Kafka
Experience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB
Experience with data governance tools: Collibra, Immuta
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala
Must possess strong written and verbal communication skills.
Secret or Top Secret clearance is preferred.",None Found,"Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.",None Found,None Found,"LMI is currently seeking a data engineer within LMI’s Advanced Analytics service line to support the design and implementation of business critical data management & engineering solutions.

This position is located in Austin, TX
Responsibilities
The ideal candidate will have direct, applied experience with one or more of the following areas:
Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.
Qualifications
Bachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with a federal government customer.
Experience with big data tools: Hadoop, Spark, Kafka
Experience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB
Experience with data governance tools: Collibra, Immuta
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala
Must possess strong written and verbal communication skills.
Secret or Top Secret clearance is preferred.
#LI-SH1"
29,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Engineer role will require partnering with key internal (intra-team and cross-disciplinary) stakeholders to implement automated reporting deliverables and work with our broader team to contribute to and evolve our data automation, visualization, warehousing and reporting services. The ideal candidate will be able to help support the team in development work by unifying multiple data sources and evolving our process to create and maintain reporting and analytical tools. Additionally, they will be able to act as subject matter expert and be a partner to both our internal and client teams.
They will work with the broader Analytics team to map data sources, create a data lake, develop SQL reporting views to consolidate dimensions and metrics across tables, link the reporting view to visualization tools (Datorama), and customize the reporting templates to meet the client teams’ needs. Additionally, this role will require the Data Engineer to maintain and update existing reports based on requests and changing technologies.
What you’ll be responsible for:
Ability to develop, build, and maintain data architecture, including data warehouse/data lake, that leverages existing automation
Work closely with internal insights, reporting & data visualization teams to drive data accuracy, reporting advancement and data warehousing efforts
Own key client data visualization builds and overall maintenance
Take lead of data and technology-centric projects and contribute to the evolution of processes, working with the team as well as independently with the goal of driving efficiency
Drive implementation, validation and ongoing support of data visualization tools and recurring reports
Ability to build custom data models for data mining and reporting
Serve the client teams as an expert on internal data visualization best practices
Grow credibility with internal teams by exceeding expectations and delivering valuable tools
Develop and refine documentation around our tech platforms and data processes
Contribute to the company's knowledge base by being the forward-thinking data technology expert with a mind for strategic growth
Manage, mentor and coach broader analytics team on technical skills and platform work
Drive advancements in our data visualization platforms including accuracy, efficiency and aesthetics - The Data Engineer will assist in onboarding new client reporting by following internal processes, maintain and update existing reports based on requests and changing technologies.
You’ll need to have:
A Bachelor's degree or equivalent years’ experience required (Computer Science, Computer Engineering, Data Analysis/Marketing, or related field); Master’s degree preferred
Expertise with data visualization tools such as Datorama, Tableau, Excel, Google Sheets, Google Data Studio required
Advanced/Intermediate experience developing custom SQL
Experience in developing data warehouses/data lakes from scratch"
30,Sr Big Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Your Opportunity
Our Global Data Technologies organization is currently seeking an experienced Big Data professional to showcase your technical expertise and passion. You will be working with a brilliant team of Big Data technologists who bring energy, focus and fresh ideas that support our mission to provide value by seeing the world “Through Clients' Eyes”.

As a Big Data Engineer you will positively impact the design and development of our Big Data solutions, while aligning with the needs of our business partners. You will use your Hadoop technology experience to implement and support new solutions within the MapR environment. Join this bright team of Big Data minds and share in our success.
What you’re good at
Designing, Developing, Configuring, Deploying, Optimizing, Documenting, and Maintaining:
Custom ingest jobs
Enhance and allocate data in a MapR ecosystem
File, source and record level data integrity and quality checks
Logging and Error Handling
Job & workflow scheduling, including ESP jobs
Python scripting, Bash Scripting
Hive Tables and Views
Role-Based security across all data storage locations
MapR Folder, Processing and Storage Components
Data comparison across data storage locations: from files, to Hive, to traditional relational databases
Working as an engineering team member in an agile team environment, including supporting story creation and refinement, creation of technical documentation, maintain Jira and support scrum and project meetings and status reporting
Leveraging existing frameworks and standards, while contributing ideas and resolving issues with current framework owners.
Professionally influencing and negotiating with other technical leaders to define and implement the optimal solution with consideration for technical and project constraints
Interacting professionally with business partners and key contacts
Creating solutions with a production end state in mind
Reviewing code and provide peer feedback relative to best practices
Working closely with architects to deliver appropriate technical solutions
Working with data scientists and solution analysts to understand the business problems, pro-actively design intuitive data structures and create the best-suited data structures for modeling and analysis
Translating advanced analytics results into production and maintain updates/refresh.
Integrating additional and new sources of data that can enhance analytic capability, continually identifying and fixing issues that enforce a robust analytic process
Providing customer service analytic support and behavioral insight and engagement
What you have
BS or MS in computer science
2+ years of hands on experience with Hadoop
5+ years software engineering experience
5+ years expertise with UNIX, JAVA, Python, BASH
2+ years hands on experience with Big data technologies ( SPARK, HBASE, FLUME, HIVE, SQOOP, MAPR, DRILL,KAFKA, PIG, STORM MAPR Streaming technologies, machine learning libraries)
2+ years hands on experience with query optimization and tuning on Hive/Drill technology for heterogeneous large data sets.
Experience with SPLUNK
MapR experience desired
Solid grasp of computer science fundamentals including data structures and algorithms.
Proven ability to evaluate and apply new technologies in a short time.
Proven understanding of the full software development lifecycle including testing, continuous integration, and deployment.
Data and Analytics tools and building data structures to support advanced analytic and research functions.
Expertise with both structured and unstructured data in a Big Data ecosystem
Experience on database development, management and ETL in a big data ecosystem
Experience with analytic scripting R, Python and strong with SQL.
Nice to have :
Cloud experience with either AWS or GCP on Data processing and storage solutions.
Experience communicating with senior level business leaders and stakeholders
Ability to interact and communicate successfully with business partners and technology teams.
Personality that engages peers and promotes collaborative teamwork
Extremely strong problem-solving skills
Knowledge of various data science techniques and experience implementing models developed with these techniques into a production environment
Experience in building data science or data analysis tools
Machine Learning background"
31,Tech Consulting Senior - Big Data Engineer,"Austin, TX 78701",Austin,TX,78701,None Found,None Found,"Designing, Architecting, and Developing solutions leveraging big data technology (Open Source, AWS, or Microsoft) to ingest, process and analyze large, disparate data sets to exceed business requirements
Unifying, enriching, and analyzing customer data to derive insights and opportunities
Leveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements
Clearly communicating findings, recommendations, and opportunities to improve data systems and solutions
Demonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches
Seeking out information to learn about emerging methodologies and technologies
Clarifying problems by driving to understand the true issue
Looking for opportunities for improving methods and outcomes
Applying data driven approach (KPIs) in tying technology solutions to specific business outcomes
Collaborating, influencing and building consensus through constructive relationships and effective listening
Solving problems by incorporating data into decision making
",None Found,None Found,None Found,"EY delivers unparalleled service in big data, business intelligence, and digital analytics built on a blend of custom-developed methods related to customer analytics, data visualization, and optimization. We leverage best practices and a high degree of business acumen that has been compiled over years of experience to ensure the highest level of execution and satisfaction for our clients. At EY, our methods are not tied to any specific platforms but rather arrived at by analyzing business needs and making sure that the solutions delivered meet all client goals.
The opportunity
You will help our clients navigate the complex world of modern data analytics. We’ll look to you to provide our clients with a unique business perspective on how Big Data analytics can transform and improve their entire organization - starting with key business issues they face. This is a high growth, high visibility area with plenty of opportunities to enhance your skillset and build your career.
Your key responsibilities
You’ll spend most of your time working with a wide variety of clients to deliver the latest big data technologies and practices to design, build and maintain scalable and robust solutions that unify, enrich and analyse data from multiple sources.
Skills and attributes for success
Designing, Architecting, and Developing solutions leveraging big data technology (Open Source, AWS, or Microsoft) to ingest, process and analyze large, disparate data sets to exceed business requirements
Unifying, enriching, and analyzing customer data to derive insights and opportunities
Leveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements
Clearly communicating findings, recommendations, and opportunities to improve data systems and solutions
Demonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches
Seeking out information to learn about emerging methodologies and technologies
Clarifying problems by driving to understand the true issue
Looking for opportunities for improving methods and outcomes
Applying data driven approach (KPIs) in tying technology solutions to specific business outcomes
Collaborating, influencing and building consensus through constructive relationships and effective listening
Solving problems by incorporating data into decision making
To qualify for the role you must have
A bachelor's degree and approximately three years of related work experience; or a master's degree and approximately two years of related work experience
At least five years hands-on experience with various Big Data technologies in one or more ecosystems: Open Source, Microsoft, or AWS:
Hadoop, Spark, NoSQL, Streaming, Atlas, Sqoop, HIVE
AWS, EMR, Hortonworks, Cassandra, Mongo, Redshift, Kafka
Azure, HDInsight, Azure DocumentDB, SQL Server
Proficiency coding in Java, C#, C++, or Scala
Experienced organizing, aggregating, querying, and analyzing large data sets
Communication is essential, must be able to listen and understand the question and develop and deliver clear insights.
Outstanding team player.
Independent and able to manage and prioritize workload.
Ability to quickly and positively adapt to change.
A valid driver’s license in the US; willingness and ability to travel to meet client needs.
Ideally, you’ll also have
Bachelor’s Degree or above in mathematics, information systems, statistics, computer science, or related disciplines
What we look for
We’re interested in passionate leaders with strong vision and a desire to stay on top of trends in the Big Data industry. If you have a genuine passion for helping businesses achieve the full potential of their data, this role is for you.
What working at EY offers
We offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, both pension and 401(k) plans, a minimum of 15 days of vacation plus ten observed holidays and three paid personal days, and a range of programs and benefits designed to support your physical, financial and social well-being. Plus, we offer:
Opportunities to develop new skills and progress your career
A collaborative environment where everyone works together to create a better working world
Excellent training and development prospects, both through established programs and on-the-job training
About EY
As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.
Join us in building a better working world. Apply now.

EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status."
32,Data engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Details
Position : Data engineer
Primary Job Duties:
Build and maintain Bioinformatics data processing environments.
Create DDL scripts for new schemas and support for database updates, restructuring, etc.
Work with enormous amounts of data – loading, migration, etc. and work on optimizing
CRUD operations
Build and maintain SQL scripts and PL/SQL procedures.
Build and maintain Java based web services.
Analyze and troubleshoot data and web service issues and implement smart improvements
and solutions.
Minimum Qualifications:
A Bachelors’ Degree in Computer Science, Engineering, Biology, or equivalent experience.
Highly proficient in Oracle SQL and PL/SQL.
Very good working knowledge of Oracle and MySQL databases.
Very good knowledge of Core Java.
Good understanding of Java EE (Servlets, JSP).
Good working knowledge of scripting languages like PERL.
Good working knowledge of UNIX / Linux systems.
Basic knowledge of Bio Sciences and Genetics is highly preferred.
Familiarity with Bioinformatics tools is a plus.
Preferably has experience with LSF - Load Sharing Facility.
Experience with Agile Software Development process a plus.
Excellent communication / documentation skills.
Must be detail oriented and a self-starter.
Multi-tasking with good follow through skills, good communication skills.
Ability to work well in a team environment.
Strong problem solving, debugging and troubleshooting skills using latest tools and
technology.
Ability to work alone and accomplish tasks without supervision."
33,Data Engineer Python Programmer,"Austin, TX 78716",Austin,TX,78716,None Found,"Degree in computer science or related field
5+ years current programming experience operating as individual contributor/hands on developer with programming projects as primary part of job
3+ Python experience (years can include advanced degree python projects)
3+ years building and maintaining data pipelines and data assets
2+ years working with distributed data processing frameworks such as Spark, Hive, and MapReduce
Demonstrated knowledge of data management best practices
Strong prioritization skills; ability to manage ad-hoc requests in parallel with ongoing projects
Attention to detail, intellectual curiosity, collaborative attitude and strong communication skills
Willingness to pick up new platforms and technologies and strong curiosity about new technologies",None Found,None Found,None Found,None Found,"Do you want to do something meaningful with Big Data? At Optum, we?re leading the fastest moving industry in the world. Health care is transforming and we?re innovating to help make the health care system work better for everyone. Our technology teams are working on projects with national and international visibility and impact. As a Principal Data Engineer, you will join a company and a team that is finding solutions to transform terabytes of healthcare information into actionable data. Join us. Let innovation and performance fuel your life?s best work.(sm)

Optum Analytics solutions create a longitudinal view of both individual patients and patient populations. We gather, normalize, and analyze data from disparate sources that, uniquely, span the continuum of care-including EHRs, Practice Management Systems and claims. Our EHR data alone accounts for over 80M patient lives across the U.S.

We are looking for a Principal Data Engineer who is eager to tackle the challenges of processing vast amounts of EHR data originating from multiple sources. You will be the driving force, and thought-leader behind helping us build services based on NLP products.

Your primary responsibilities will be to design and maintain data pipelines and services using best practices for data management and governance, and deploy machine learning and NLP applications in production. You will be working with EHR data and working across teams with ETL, NLP engineers and data scientists, researchers and clinicians to provide data services with high data quality control standards. You will need to develop a deep understanding of the data and drive efforts to maintain and improve data quality and usability. You should understand the importance and value of writing maintainable, documented, and well-tested code throughout the entire product lifecycle. Above all, you should be curious about what is possible in healthcare with the right tools and infrastructure.

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.


Required Qualifications:
Degree in computer science or related field
5+ years current programming experience operating as individual contributor/hands on developer with programming projects as primary part of job
3+ Python experience (years can include advanced degree python projects)
3+ years building and maintaining data pipelines and data assets
2+ years working with distributed data processing frameworks such as Spark, Hive, and MapReduce
Demonstrated knowledge of data management best practices
Strong prioritization skills; ability to manage ad-hoc requests in parallel with ongoing projects
Attention to detail, intellectual curiosity, collaborative attitude and strong communication skills
Willingness to pick up new platforms and technologies and strong curiosity about new technologies
Preferred Qualifications:
Experience running machine learning or NLP applications at scale
Experience with data pipeline frameworks such as Airflow, Luigi or Oozie
Experience with search engines (Elasticsearch, Solr)
Experience with cloud-based computing (AWS, Azure)
Experience with Scala, in particular with Spark Scala API
Familiarity with EHR data and standards (HL7, FHIR)
Experience with HBase or other non-relational data bases
Experience with explaining, educating, presenting and/or training non-engineers on engineering concepts and processes
Experience with ETL
Experience with continuous integration and delivery

Careers with Optum. Here's the idea. We built an entire organization around one giant objective; make health care work better for everyone. So when it comes to how we use the world?s large accumulation of health-related information, or guide health and lifestyle choices or manage pharmacy benefits for millions, our first goal is to leap beyond the status quo and uncover new ways to serve. Optum, part of the UnitedHealth Group family of businesses, brings together some of the greatest minds and most advanced ideas on where health care has to go in order to reach its fullest potential. For you, that means working on high performance teams against sophisticated challenges that matter. Optum, incredible ideas in one incredible company and a singular opportunity to do your life's best work.(sm)

Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.

UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment."
34,Senior Data Engineer - BlackLocus,"Austin, TX 73301",Austin,TX,73301,None Found,None Found,None Found,None Found,None Found,None Found,"POSITION PURPOSE
The Sr Data Engineer will expand and optimize data, data flow, data collection for cross functional teams, and data pipeline architecture. The Sr Data Engineer will support and collaborate with the software engineering team, data analysts, and data scientist to ensure data delivery architecture is consistent throughout ongoing projects. Continuously improve or re-design data architecture to support the next generation of products and initiatives.
MAJOR TASKS, RESPONSIBILITES AND KEY ACCOUNTABILITIES
40% - Data Validation, ETL, Infrastructure Development: Coding validation and ETL to ensure successful data integration
30% - Data Infrastructure Maintenance: Backup and optimization activities to maintain performance; code, configure, test, etc data to ensure integrity
20% - Data Architecture Design and Analysis: Create and maintain optimal data pipeline architecture; Develop data architecture to meet business requirements
10% - Planning/Requirements Analysis: Collaborate with team leads and cross functional partners to assess business requirements and communicate opportunities
NATURE AND SCOPE
This position reports to the Technology Leader.
This position has 0 direct reports.
ENVIRONMENTAL JOB REQUIREMENTS
Environment:
Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable.
Travel:
Typically requires overnight travel less than 10% of the time.
Additional Environmental Job Requirements:
MINIMUM QUALIFICATIONS
Must be eighteen years of age or older.
Must be legally permitted to work in the United States.
Additional Minimum Qualifications:
Experience message queuing, stream processing, and scalable data stores
Experience managing projects and organizing data
Experience supporting and working with cross-functional teams
Experience with SQL, NoSQL databases, relational databases, and query authoring
Experience building and optimizing data pipelines, architectures, data sets and workflow management tools such as Azkaban, Luigi, Airflow, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C#, Scala, GoLang, etc.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Education Required:
The knowledge, skills and abilities typically acquired through the completion of a bachelor's degree program or equivalent degree in a field of study related to the job.
Years of Relevant Work Experience: 5 years
Physical Requirements:
Most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. On rare occasions there may be a need to move or lift light articles.
Additional Qualifications:

Preferred Qualifications:
8+ years of previous related work experience
Ability to convey complex or technical ideas and processes in easy-to-understand terms to diverse audiences
Ability to negotiate, handle complaints, settle disputes, and resolve grievances with both internal and external customers
Excellent written and verbal communication skills
Knowledge, Skills, Abilities and Competencies:Collaborates - Building partnerships and working collaboratively with others to meet shared objectives
Communicates Effectively - Developing and delivering multi-mode communications that convey a clear understanding of the unique needs of different audiences
Cultivates Innovation - Creating new and better ways for the organization to be successful
Drives Engagement - Creating a climate where people are motivated to do their best to help the organization achieve its objectives
Instills Trust - Gaining the confidence and trust of others through honesty, integrity, and authenticity
Nimble Learning - Actively learning through experimentation when tackling new problems, using both successes and failures as learning fodder
Optimizes Work Processes - Knowing the most effective and efficient processes to get things done with a focus on continuous improvement
Plans and Aligns - Planning and prioritizing work to meet commitments aligned with organizational goals
Tech Savvy - Anticipating and adopting innovations in business-building digital and technology applications."
35,Senior Data Engineer,"Austin, TX 78746",Austin,TX,78746,None Found,"
Bachelor’s degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.
6 or more years of experience as a data engineer on enterprise-level data solutions.
Experience in SQL and scripting for automation with Python, Perl or Ruby.
Experience working with relational and unstructured databases and enterprise data warehouses, including MySQL, PostgreSQL, MongoDB, SQL Server or Oracle.
",None Found,"
Participate in data architecture discussions to understand target data structures, required data transformations and inform architectural approach based on best practices for data processing.
Lead detailed exploration of new internal and external source data to advise strategic initiatives led by the Product, Artificial Intelligence and Business Intelligence teams.
Influence technical and business strategy by making insightful contributions to team priorities and overall data processing approach.
Work in close collaboration with data-minded colleagues focused on back-end (microservice) development, business intelligence reporting, machine learning and artificial intelligence models.
Participate in the hiring and mentoring of other data engineers.
",None Found,None Found,"What you’ll be called: Senior Data Engineer

Where you’ll work: KWRI Headquarters—Austin, TX

Named a Happiest Company to Work for in 2019; one of the Best Places to Work in Austin, TX; and featured on the Training Magazine Training 125 list seven times, Keller Williams Realty International (KWRI) thrives within a creative and collaborative culture where transforming the real estate industry through technology is our primary goal.

KW Technology is the foremost provider of real estate solutions, offering the most comprehensive end-to-end portfolio of products, services and training in the industry. Our Data Engineering team converts agent and consumer challenges into intuitive, insight-enhanced technology and consumer experiences using tools such as Python, Hadoop, Spark, MySQL, MongoDB and Snaplogic.

What you’ll do:

Design, develop and implement data infrastructure and pipelines that collect, connect, centralize and curate data from various internal and external data sources. You will ensure that architectures support the needs of the business, and recommend ways to improve data reliability, efficiency and quality.

Essential Duties and Responsibilities:

Participate in data architecture discussions to understand target data structures, required data transformations and inform architectural approach based on best practices for data processing.
Lead detailed exploration of new internal and external source data to advise strategic initiatives led by the Product, Artificial Intelligence and Business Intelligence teams.
Influence technical and business strategy by making insightful contributions to team priorities and overall data processing approach.
Work in close collaboration with data-minded colleagues focused on back-end (microservice) development, business intelligence reporting, machine learning and artificial intelligence models.
Participate in the hiring and mentoring of other data engineers.
Minimum Qualifications:

Bachelor’s degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.
6 or more years of experience as a data engineer on enterprise-level data solutions.
Experience in SQL and scripting for automation with Python, Perl or Ruby.
Experience working with relational and unstructured databases and enterprise data warehouses, including MySQL, PostgreSQL, MongoDB, SQL Server or Oracle.
Preferred Qualifications:

Master’s degree in Information Management, Data Science, Analytics or related field.
Expert in SQL and Python for scripting automation.
Experience building open source data pipeline systems such as AirFlow, Hadoop or Kafka.
Experience with Spark, Presto, Hive or other map/reduce ""big data"" systems and services.
Who are we?

Austin, Texas-based Keller Williams, the world's largest real estate franchise by agent count, has more than 1,000 offices and 180,000 associates. The franchise is also No. 1 in units and sales volume in the United States. In 2015, Keller Williams began its evolution into a technology company, now building the real estate platform that agents' buyers and sellers prefer. Since 1983, the company has cultivated an agent-centric, technology-driven and education-based culture that rewards agents as stakeholders."
36,Senior Software Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"So what’s our story? We believe our value is in helping our clients do something they never dreamed possible. Giving them that certain moment when clarity becomes confidence. Finding a whole new customer segment. Reaching everyone who needs to be reached. Understanding those tiny market nuances. There’s more, of course, but these are the kinds of things that make the difference to our clients – the things that help them sleep at night.


Are you self-motivated with a proven track record of delivering results? Do you have a demonstrated ability to think strategically in complex situations, motivate and mobilize resources, and deliver over the top results? This Big Data Engineer role will be provide the opportunity to do all of the above while driving architecture design, data modeling, and implementation of Big Data platform and analytic applications.
What you will do:
Fulfill requests from the product team such as building upon our Acxiom Unified Data Layer(UDL) product base, optimizing data processing via Spark/Scala, researching new innovation ideas.
Optionally solve additional challenges in our service tier in NodeJS
Write spark jobs and components in Scala programming language
Work in an agile teaming environment
Consistently provides proven, formal mentorship
Regularly lead self and others and/or established as Product SME and/or established as specialist
Understands how whole picture aligns to overall Acxiom strategy
Works with product manager to maximize the components ROI
Expected more thought leadership
Have a broad understanding of the external events that may impact applications or systems (networking, operations, etc.)
What you will need:
5+ years experience in Big Data Hadoop, Hive and Spark with hands on expertise in design and implementation of high data volume solution
Good knowledge of configuring Spark and working on multi node clusters and distributed data processing framework.
Hands on knowledge of Big Data Analytics and Predictive Analytics
Extremely agile with good knowledge of relational DWs as well as NoSQL DBs like, MongoDB etc.,
Strong in Spark Scala pipelines (both ETL & Streaming)
Proficient in Spark architecture
Cloud developer experience in leading cloud providers (AWS, Azure, GCP)
Strong development skills in Node.js for REST apis
Experience with real-time data ingestion using streaming technologies such as Kafka and Kinesis.
#GD17
Primary Location City/State:
Austin, Texas"
37,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,"
Collaborate directly with external customers to understand their student success goals, specify and design data solutions and commission products into production.
Develop ETL transformations to map client data systems into our canonical data model.
Collaborate with teams across Civitas to drive innovation and best practices in our data and data science platform.
Up to 20% Travel to visit external clients for technical discovery and/or UAT/QA of data mappings.
Design, Implement and Maintain Database Models.
Build and operationalize data science models on AWS.
",None Found,None Found,"Our mission at Civitas Learning is to partner with forward-thinking colleges and universities, harnessing the power of insight and action analytics to help students learn well and finish strong. Data and predictive models are at our core to achieve this mission. Are you amazing at SQL and PHP? Curious about data transformation for data science? We are looking for a smart and dedicated data integration specialists to join our data engineering services team.

The Data Engineering role at Civitas Learning is critical to onboarding colleges and universities onto our cloud platform and making the process smoother, more flexible, and faster. We work with people who are passionate about our mission, enjoy working with customers, and are eager to use their technical skills. We work hard, but also like to have fun! If this sounds like you, keep reading.

Primary Responsibilities:

Collaborate directly with external customers to understand their student success goals, specify and design data solutions and commission products into production.
Develop ETL transformations to map client data systems into our canonical data model.
Collaborate with teams across Civitas to drive innovation and best practices in our data and data science platform.
Up to 20% Travel to visit external clients for technical discovery and/or UAT/QA of data mappings.
Design, Implement and Maintain Database Models.
Build and operationalize data science models on AWS.

Minimum Qualifications

Bachelor's degree plus 3 years experience designing, developing, testing, and implementing complex ETL solutions using enterprise ETL tools
Expertise in writing complex database SQL queries with a focus on Postgres and Redshift.
Strong understanding of ETL best practices.
Proficient manipulating and processing text files with command line build tools and/or writing scripts to manipulate text files
Experience with database design and function
Expertise in working with technical and business teams to extract and document data integration/exchange requirements
Ability to handle multiple projects and deadlines with minimal supervision.
Ability to work independently and learn on the job as well as in a cross-functional team environment, collaborating with others and sharing tools, skills, and knowledge
Strong organizational skills and ability to meet deadlines, prioritize workload, and manage time effectively
Solid problem-solving and analysis skills that demonstrate resourcefulness and attention to detail
Strong customer service focus and a comfort with engaging with a customer via email, phone, and in person
Ability to express complex technical concepts effectively, both verbally and in writing.

Must have experience in some of the following:

Writing SQL and using command line tools (preferably in a linux or linux like environment)
Experience with at least one major RDBMS (preferably postGres, RedShift, or MySQL)
Python/PHP or other programming language

Preferred experience in the following:

Version Control System (preferably Github)
Experience with Higher Education Student Information Systems or Learning Management Systems (Ellucian Banner, Peoplesoft, Blackboard LMS, Canvas, etc.)
JIRA

About Civitas Learning:
Civitas Learning partners with universities and colleges dedicated to helping more students learn well and finish strong. We provide tools and services for educators that bring together and make the most of their diverse and disconnected data streams; personalize information and support for their students; and deepen understanding of the impact of their student-success initiatives. Through our work together, our partners are empowering leaders, advisors, faculty, & students—and measurably improving enrollment, persistence, and graduation outcomes.

Today, Civitas Learning has more than 350 colleges and universities as customers, serving nearly 8 million students. Together with our growing community of customers, Civitas Learning is making the most of the world’s learning data to graduate a million more students per year by 2025.

Civitas Learning is located in the heart of downtown Austin, directly across from Lady Bird Lake and the Austin Hike and Bike Trail. Civitas offers a comprehensive benefits package including medical, dental, vision, disability insurance, onsite paid parking, 401-K Program and a flexible paid time off policy. Civitas Learning is an equal opportunity employer and values diversity. We do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability."
38,Data Engineer – Warehouse and B.I.,"Austin, TX",Austin,TX,None Found,None Found," Graduation from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information systems or information science or a related field. High School diploma or equivalent and additional directly related experience may substitute for the required education on a year-for-year basis."," Emerging data and analytics technologies (i.e. Hadoop, Spark, MongoDB, Azure Data Lake, etc.) Cloud platforms and development patterns (i.e. AWS, Azure, MapReduce, etc.) Machine-learning, statistical analysis, artificial intelligence, predictive analytics. Relational and non-relational data structures, theories, principles, and practices. Metadata management and associated processes. Web services (REST, SOAP, XML, WSDL, JSON). Data encryption and secure transmission practices (SSL, SSH, SFTP, Certificates, PKI, OAUTH2)."," Works closely with internal customers regarding their specific data needs to develop the requirements for data subject areas needed in the Data Warehouse. Captures the inventory of data sources and dashboards to prepare and manage integrated data. Acts as the IT knowledge leader on data warehousing and data analytics to the business. Ensures data warehouse implementations meet business expectations and coordinates customer acceptance testing and training. Ensures the customer can exploit the data warehouse solutions and helps identify additional possible uses of information; anticipates future needs and opportunities. Assists in the identification and integration of potential new data sources. Designs and develops ETL pipelines that extract data from various sources and load into the data warehouse or other systems. Ensures that controls to verify the accuracy and consistency of data are implemented and monitored. Provides ongoing operational support of the enterprise data warehouse, continued development and enhancement of the data warehouse, automation of daily data extracts and external system feeds, and development and enhancement of current dashboards."," Graduation from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information systems or information science or a related field. High School diploma or equivalent and additional directly related experience may substitute for the required education on a year-for-year basis."," normal cognitive abilities including the ability to learn, recall, and apply certain practices and policies; marginal or corrected visual and auditory requirements; constant use of personal computers, copiers, printers, and telephones; the ability to move about the office to access file cabinets and office machinery; frequent sitting and/or remaining in a stationary position; and the ability to work under deadlines, as a team member, and in direct contact with others.","WHO WE ARE:


The Teacher Retirement System of Texas is the largest public retirement system in Texas, serving more than 1.5 million people. Innovation, technology, and collaboration make the difference as we strive to continue earning your trust every day. TRS improves the retirement security of Texas public education employees through our 'best in class' investment management and delivery of pension and health care benefits.


Our Mission: Improving the retirement security of TRS members by prudently investing and managing the Trust assets and delivering benefits that make a positive difference in their lives.


Teacher Retirement System of Texas (TRS) is developing groundbreaking solutions to manage retirement & healthcare services as well as maximizing investment returns for the state’s public education employees. Succeeding in life is about commitment and hard work—maybe a favorite school teacher imparted that idea to you along the way. At TRS, we take that to heart and believe life can also be a balance between giving back while excelling professionally.
The Data Engineer – Warehouse and Business Intelligence, performs advanced (senior-level) data analysis and architectural work for TRS’ Financial department. Work involves leading initiatives to develop and implement data analytics-driven solutions to meet critical business objectives. This position serves as liaison and technical expert between IT technical teams and the internal customers (business intelligence and data analytics team) of the data warehouse. May supervise the work of others. Works under minimal supervision with extensive latitude for the use of initiative and independent judgement.

State Classification
Data Analyst VI: 0655/B28

Duties and Responsibilities

Data Engineer, Warehouse and B.I Works closely with internal customers regarding their specific data needs to develop the requirements for data subject areas needed in the Data Warehouse. Captures the inventory of data sources and dashboards to prepare and manage integrated data. Acts as the IT knowledge leader on data warehousing and data analytics to the business. Ensures data warehouse implementations meet business expectations and coordinates customer acceptance testing and training. Ensures the customer can exploit the data warehouse solutions and helps identify additional possible uses of information; anticipates future needs and opportunities. Assists in the identification and integration of potential new data sources. Designs and develops ETL pipelines that extract data from various sources and load into the data warehouse or other systems. Ensures that controls to verify the accuracy and consistency of data are implemented and monitored. Provides ongoing operational support of the enterprise data warehouse, continued development and enhancement of the data warehouse, automation of daily data extracts and external system feeds, and development and enhancement of current dashboards.
Performs related work as assigned

Minimum Required Qualifications
Education: Graduation from an accredited four-year college or university with major coursework in computer information systems, computer science, data management, information systems or information science or a related field. High School diploma or equivalent and additional directly related experience may substitute for the required education on a year-for-year basis.

Experience: Three (3) years of experience in financial/accounting data warehouse, including dimensional modeling, ETL pipeline design & development, data management, data analysis, process measurement, and metrics management. Three (3) years of experience working closely with business analytics teams and data analysts. Two (2) years of experience with PowerBI or equivalent modern data visualization package. One (1) Experience with Talend Studio and Data Integration or equivalent modern ETL package. Experience with metadata-driven ETL templates. Successful track record of Business Intelligence/Data Warehouse solution implementations (design, implementation, data visualization and ongoing support/maintenance) with extensive interaction with business users. Demonstrated record of accomplishment working in a cross-functional team environment; including project management skills utilizing business and technical resources. Ability to apply data warehouse architecture theories and concepts to create business solutions. Thorough understanding of ETL (extract/transform/load) processing methods, underlying data warehouse data models, security, and BI products.

Registration, Certification, or Licensure: None.

Preferred QualificationsExperienced in Agile methodologies & DevOps approach to maintaining pipelines and databases. Recent experience with PowerBI (both cloud and on-premises). Recent experience with Talend. SQL and T-SQL knowledge.

Knowledge, Skills and Abilities
Knowledge of: Emerging data and analytics technologies (i.e. Hadoop, Spark, MongoDB, Azure Data Lake, etc.) Cloud platforms and development patterns (i.e. AWS, Azure, MapReduce, etc.) Machine-learning, statistical analysis, artificial intelligence, predictive analytics. Relational and non-relational data structures, theories, principles, and practices. Metadata management and associated processes. Web services (REST, SOAP, XML, WSDL, JSON). Data encryption and secure transmission practices (SSL, SSH, SFTP, Certificates, PKI, OAUTH2).

Skill in: Highly complex problem solving and critical thinking, and operating computers and applicable computer software. Planning, organizing, and coordinating work assignments to effectively meet frequent and/or multiple deadlines, handling multiple tasks simultaneously, and managing conflicting priorities and demands. Project management and system development life cycle concepts. Client/user interaction to determine system requirements. Strong written and oral communication skills. Strong presentation and interpersonal skills. Ability to present ideas in user-friendly language. Strong technical zeal with a passion for solving complex problems.

Ability to: Establish and maintain harmonious working relationships with co-workers, agency staff, and external contacts. Work effectively in a professional team environment. Work in an Agile development environment.

Physical Requirements and/or Working Conditions
Work is performed in a standard office environment and requires: normal cognitive abilities including the ability to learn, recall, and apply certain practices and policies; marginal or corrected visual and auditory requirements; constant use of personal computers, copiers, printers, and telephones; the ability to move about the office to access file cabinets and office machinery; frequent sitting and/or remaining in a stationary position; and the ability to work under deadlines, as a team member, and in direct contact with others.

Workforce Expectations
Must be able to: regularly, reliably, and punctually attend work; work extended hours as necessary; travel occasionally for work assignments and trainings; show flexibility and adaptability toward changes in assignments and work schedules; adhere to the agency’s internal management policies and procedures; and exhibit work behaviors consistent with agency core values.



Military Occupational Specialty (MOS) Codes:

Veterans, Reservists or Guardsmen with experience in the Military Occupational Specialty ( http://www.hr.sao.texas.gov/Compensation/MilitaryCrosswalk/MOSC_AdministrativeSupport.pdf ) along with the minimum qualifications listed above may meet the minimum requirements and are highly encouraged to apply. Please contact Talent Acquisition at careers@trs.texas.gov with questions or for additional information.


To view all job vacancies, visit www.trs.texas.gov/careers or www.trs.csod.com/careersite.


For more information, visit www.trs.texas.gov."
39,Sr. Data Engineer (REF21652H) - Visa Digital Developer Platform (VDDP),"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description

The Digital & Developer Platform (DDP) has exciting opportunities in the field of Data engineering delivering analytics and insights on products & platforms that enable the next generation of payments. Bring passion and dedication to your job and there's no telling what you could accomplish. Are you ready to apply your educational experience to real-world problems? Are you passionate about applying your data skills in a real-world tech environment? If you’re interested in being a part of a team that’s constantly learning and problem-solving, we’d love to talk with you.
As a Sr.Data Engineer in DDP Data Engineering and Analytics team you will play a critical role in providing data insights for DDP products like Visa Checkout, Visa Direct, Visa Developer Platform, Visa Token Services, and Visa Digital Commerce Apps to technology stakeholders , business and executives. You will build data pipelines for new services and utilize data technology to integrate and process data in a large scale. It is expected that you have high attention to detail and quality. You will share the ownership of technical vision and direction for advanced analytics and insights. You will be a part of a team of top notch technical professionals developing data pipelines and solutions at scale and with a focus on sustained operational excellence.
Responsibilities :
Work across multiple teams in high visibility roles and own the solution end-to-end.
Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) in/out of our Data Lake.
Assist in scoping and designing analytic data assets.
Build and maintain a robust data engineering process to develop and implement self-serve data .
Find opportunities to create, automate and scale repeatable analyses or build self-service tools for business users.

Qualifications

Can you take on the responsibilities described above? Then please apply!
Basic
2 years of work experience with a Bachelor’s Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD, or PhD)
Preferred
Minimum of 2-3 years’ experience in production ETL pipelines, utilizing big data engineering techniques that enable solutions to solve business problems.
Extensive experience with SQL and big data technologies (Hadoop, Python , Java, Spark, Hive etc.) tools for large scale data processing, data transformation and machine learning pipelines.
Knowledge of Kafka, Airflow, Spark Streaming for Building data pipelines is nice to have.
Experience with data visualization and business intelligence tools like Tableau, PowerBI , Microstrategy, or other programs highly desired.
Familiarity or experience with statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is very helpful.
Strategic thinker and good business acumen to orient data engineering to the business needs of internal clients.
Demonstrated intellectual and analytical rigor, team oriented, energetic and collaborative.
Previous exposure to financial services or merchant analytics is a plus, but not required.
Additional Information

Work Hours This position requires the incumbent to be available during core business hours.
Travel Requirements This position requires the incumbent to travel for work less than 10% of the time.
Mental/Physical Requirements This position will be performed in an office setting. The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers, and reach with hands and arms.
EEO Statement Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law."
40,Google Technical Architect,"Austin, TX 78727",Austin,TX,78727,None Found,"Minimum 5 years of Consulting or client service delivery experience on Google GCP
",DevOps on an GCP platform. Multi-cloud experience a plus.,None Found,None Found,"Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google Cloud Platform (GCP) Technical Architect Delivery is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would also be responsible for developing and delivering Google GCP cloud solutions to meet todays high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Google GCP Technical Architect is a highly performant GCP Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data soltuions on cloud. Using Google GCP public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications.

Role & Responsibilities:Work with Sales and Bus Dev teams in providing Data and GCP Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
Minimum 5 years of Consulting or client service delivery experience on Google GCP
Minimum 10 years of experience in big data, database and data warehouse architecture and delivery
Bachelors degree or 12 years previous professional experience
Able to travel 100% (M-TH)
Minimum of 5 years of professional experience in 2 of the following areas:
Solution/technical architecture in the cloud
Big Data/analytics/information analysis/database management in the cloud
IoT/event-driven/microservices in the cloud
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc.:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Tensorflow & Sheets

Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Certified GCP Solutions Architect - Associate
Certified GCP Solutions Architect – Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)
Certified GCP AI/ML Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP
Strong in Java, C##, Spark, PySpark, Unix shell/Perl scripting
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
- Multi-cloud experience beyond GCP a plus - AWS and Azure

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
41,Azure Data Engineer,"Austin, TX 78727",Austin,TX,78727,None Found,"At least 5 years of consulting or client service delivery experience on Azure
",DevOps on an Azure platform,None Found,None Found," Proven ability to build, manage and foster a team-oriented environment
","Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
42,Data Platform Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Sysco LABS
Sysco LABS is a technology-focused division within Sysco, dedicated to reimagining foodservice through innovation. An extension of Sysco’s commitment to deliver exceptional products and services to the foodservice industry, Sysco LABS uses customer and market intelligence, data-driven insights and agile technology development to rethink the entire foodservice ecosystem.
Our innovation will improve everything from the ordering process, inventory, pricing and automation to the in-restaurant customer experience. Operating with the mindset of a startup and backed by the authoritative expertise of an industry leader, Sysco LABS’ mission is to improve the Sysco customer experience and consistently deliver cost savings and new innovations through technology.
About the Role
As a Data Engineer at SyscoLABS you will build scalable data and analytics models and architecture from event formation to ingestion to reporting from scratch. You will work on all aspects of the system from stream configuration, to ETL, to aggregate tables/cubes for reporting needs. You will make an impact in creating robust systems that are used by every team. You will help shape the vision and architecture of the end-to-end data pipeline while following industry best practices. You will work with all facets of the business from dev ops, to development, to consumers of the data. You will work with data stakeholders to understand their needs and translate those into executable plans. You will be part of an experienced data team and work with passionate leaders.

What you’ll bring to the table
5+ years of experience in data engineering
Experience working closely with analysts and business stakeholders
Proficient in SQL and Python
Proficient in Redshift and Postgres
Proficient in AWS platform and technologies including Kinesis, Firehose, and Glue
Cloud storage such as S3
Familiar with Data Lake technologies like EMR, Athena, hive, presto, parquet
Familiar with Event streaming and processing.
Experienced in orchestrating data ingestion and validation.
Experienced in ETL for reporting/business users (marketing, finance, product owners)
Experience with Tableau and Tableau Server a plus
Experience working with Virtual Container Environment (eg Docker, ECS) a plus.

Perks at Sysco LABS:
Competitive base & bonus packages
Fun, casual, fast-paced work environment filled with talented colleagues
Brand-new high-tech office space located in the heart of East 6th St, (walking distance to the MetroRail)
Generous PTO policy
Flexible work environment that encourages work/life balance
Fully stocked break room & weekly catered lunch
Excellent medical, dental and vision benefits and company 401(k) program

At Sysco LABS you will be part of a team that values trust, learning, and working together to solve problems. Your efforts will help to build groundbreaking experiences for our customers. Come join us!
We embrace diversity and equal opportunity in a serious way. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. The more inclusive we are, the better our work will be."
43,Application Support Engineer (English+Japanese),"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Named as one of Fortunes’ 100 Fastest Growing Companies for 2019, EPAM is committed to providing our global team of 30,100+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential.

Description

You are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as an Application Support Engineer. Scroll down to learn more about the position’s responsibilities and requirements.

#LI-DNI
What You’ll Do
Provide support to customers using Cloud Platform products, solutions and APIs, including Big Data and related services
Provide technical assistance and support as part of a global 24x7-support organization
Work closely with engineers and product managers to improve the product and make our customers successful
Incident Management, Problem Management and Change Management: change requests and incident ticket handling for the remaining Infrastructure Teams
Provide technical and developer support to customers using Cloud Platform products, solutions and APIs
Follow notification and escalation procedures
Follow standards for communications with business involving operational issues
Identify and document product bugs and feature requests and work with internal support teams as well as customers to implement effective solutions
Work closely with internal support team s to improve cloud products at a senior level
What You Have
A degree in an associated field and/or other advanced certification along with significant experience
Native or full professional proficiency level in Japanese and English languages
3+ years’ experience fully as developer or a combination developer + big data engineer
Ability to read and understand code, able to write code to reproduce customer problems
Strong research, analytical and problem-solving skills required to work with petabyte or even exabytes of data
Familiar with web protocols (HTTP, TLS/SSL, etc)
Firm understanding of programming (Java, C++, C#, Scala, Python, etc.) and scripting languages (Python/PHP/R)
Background in SQL
Experience with BigData architectures and technologies (more than 1TB of data) and BI solutions
Experience with distributed computing frameworks (e.g. Hadoop, Spark, Flink, Storm, Samza, Beam, Google Big Query, etc.)
Experience with distributed data stores (HBase, Cassandra, Riak, Google Bigtable, Amazon Dynamo DB, etc.) and/or distributed message brokers (Kafka, RabbitMQ, ActiveMQ, Google Pub/Sub, Amazon Kinesis, etc.)
Nice to have
Experience in technical support: familiarity with case prioritization, SLA compliance, and quality
Experience with the popular technologies in the machine learning/big data ecosystem
Experience with any ML library (scikit¬learn, pytorch, tensorflow, Spark mllib) or basic understanding of ML concepts
Experience with PaaS and IaaS technologies
Java or Objective C
What We Offer
Medical, Dental and Vision Insurance (Subsidized)
Health Savings Account
Flexible Spending Accounts (Healthcare, Dependent Care, Commuter)
Short-Term and Long-Term Disability (Company Provided)
Life and AD&D Insurance (Company Provided)
Employee Assistance Program
Unlimited access to LinkedIn learning solutions
Matched 401(k) Retirement Savings Plan
Paid Time Off
Legal Plan and Identity Theft Protection
Accident Insurance
Employee Discounts
Pet Insurance"
44,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,"
Design, build, scale, and maintain multiple data pipelines
Work closely with business owners and external stakeholders to provide actionable data
Ensure data accuracy and reliability
",None Found,"
Experience building large scale streaming and batch data pipelines
Experience using Big Data technologies (Spark, EMR, hadoop, data lakes, etc.)
Mastery of multiple databases (e.g. MongoDB, MySQL, etc.)
Understanding of data security best practices
","Crowdskout is looking for a Data Engineer that can help us expand our data pipeline infrastructure. Crowdskout's product has most recently been centered in the CRM space, but we are looking to change that. Currently, we process millions of data points through multiple data pipelines to feed into a suite of databases. We are preparing for 10x growth both in the volume of data processed and the speed in which that data can be available and actionable. To accomplish this we are looking for someone who can build out highly scalable data solutions.

If you are highly motivated, super passionate about democracy, and want to join a close-knit team that is looking to build great things together, Crowdskout may be for you. This is a full-time position in Washington, DC; Sacramento, CA; Austin, TX; Raleigh-Durham, NC; Salt Lake City, UT; or Chicago, IL.

Responsibilities:

Design, build, scale, and maintain multiple data pipelines
Work closely with business owners and external stakeholders to provide actionable data
Ensure data accuracy and reliability

Requirements:

Experience building large scale streaming and batch data pipelines
Experience using Big Data technologies (Spark, EMR, hadoop, data lakes, etc.)
Mastery of multiple databases (e.g. MongoDB, MySQL, etc.)
Understanding of data security best practices

Extras:

AWS data technologies (e.g. Kenesis, Glue, RDS, Athena, etc.)
Experience building out data warehouse infrastructure
Software development using PHP
DevOps or System Admin experience
Data Science exploration and modeling

Crowdskout is an equal opportunity employer that encourages diversity across all spectrums in its hiring, without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, or any other protected factor. With that being said, we wouldn't be able to accommodate candidates in need of work sponsorship at this time since we are a small company. If you find this role interesting and you hit on the elements above, please apply!"
45,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,"2+ years of experience with building end-to-end scalable production-grade data pipelines
Knowledge of data warehousing
Understanding of modern data architecture, data modeling, and data management principles
Experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)
Good foundation in data structures and algorithms
Understanding with Relational and NoSQL databases to help teams best organize their data for analysis
Experienced in OOP design and development, preferably in Python
Knowledge in writing, understanding and tuning PL/SQL and/or T-SQL
BS/BA degree in Computer Science, Engineering or related fields or equivalent experience
","2+ years of experience with building end-to-end scalable production-grade data pipelines
Knowledge of data warehousing
Understanding of modern data architecture, data modeling, and data management principles
Experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)
Good foundation in data structures and algorithms
Understanding with Relational and NoSQL databases to help teams best organize their data for analysis
Experienced in OOP design and development, preferably in Python
Knowledge in writing, understanding and tuning PL/SQL and/or T-SQL
BS/BA degree in Computer Science, Engineering or related fields or equivalent experience
","General Information
Ref #: 27449
Functional Area: Technology
Employee Type: Full Time
Location: Austin
Experienced Required: Please See Below
Education Required: Bachelors Degree
Job Posting Shift: 1st
Date published: 26-Jun-2019
About Us:
We are PIMCO, a leading global asset management firm. We manage investments and develop solutions across the full spectrum of asset classes, strategies and vehicles: fixed income, equities, commodities, asset allocation, ETFs, hedge funds and private equity. PIMCO is one of the largest investment managers, actively managing more than $1.84 trillion in assets for clients around the world. PIMCO has over 2,700 employees in 17 offices globally. PIMCO is recognized as an innovator, industry thought leader and trusted advisor to our clients.

PIMCO is one of the world’s premier fixed income investment managers with thousands of professionals around the world united in a single purpose: creating opportunities for our clients in every environment. Since 1971, we have brought innovation and expertise to our partnership with the institutions, financial advisors and millions of individual investors who entrust us with their assets. We aspire to cultivate performance and leadership through empowering our people, diversity of thought, and a commitment to an inclusive culture that engages in our global communities.
Position Description:
As a data engineer on our team, we will be working on a number of high-profile projects that will require you to collaborate with key partners and develop data solutions that enable insights into our clients through an evolving data architecture and drive our next generation data platform. In this role you will meet with relevant partners, understand and model data assets, and constantly find opportunities to optimize and evolve the underlining data platform. You will work with software developers, data architects, data analysts, and data scientists to solve complex business problems. Demonstrating lifelong learning and collaboration to fill gaps in knowledge is essential.
Position Requirements:

2+ years of experience with building end-to-end scalable production-grade data pipelines
Knowledge of data warehousing
Understanding of modern data architecture, data modeling, and data management principles
Experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)
Good foundation in data structures and algorithms
Understanding with Relational and NoSQL databases to help teams best organize their data for analysis
Experienced in OOP design and development, preferably in Python
Knowledge in writing, understanding and tuning PL/SQL and/or T-SQL
BS/BA degree in Computer Science, Engineering or related fields or equivalent experience
PREFERRED QUALIFICATIONS
Domain knowledge of Financial Services
Hands-on experience with working with Cloud technologies, including AWS
Experience with CI/CD methodologies
Experience with *nix environments, including shell script development

We are PIMCO, a global investment management firm with a singular focus on preserving and enhancing investors’ assets. We manage investments for institutions, financial advisors and individuals, helping millions of people around the world meet their financial goals.

Our technology powers the firm’s global trading platform. We employ sophisticated and cutting edge technology tools that support PIMCO’s core investment management strategy.

Why PIMCO? At PIMCO you will join a dynamic, constantly evolving global firm that pushes you to grow, lead and innovate. You will be client- focused and work on technologies that will be put to immediate use. You will be part of a team whose members are encouraged to speak up with an idea or challenge existing views, regardless of title or tenure. You will have the opportunity to receive competitive compensation and other attractive benefits (Please see below).

PIMCO’s Technology Team is organized in small, focused, agile groups, that either work closely with business units to deliver value or develop core technologies that lever the product teams. Our environment fosters innovation and promotes entrepreneurial spirit, and we use top of the line tools. PIMCO recognizes the paramount role of tech now and in the future and invests in technology accordingly. Technology careers are available in Newport Beach, Austin, New York, London, Munich, Singapore and Tokyo.
Benefits:
PIMCO is committed to offering a comprehensive portfolio of employee benefits designed to support the health and wellbeing of you and your family. Benefits vary by location but may include:
Medical, dental, and vision coverage
Life insurance and travel coverage
401(k) (defined contribution) retirement savings, retirement plan, pension contribution from your first day of employment
Work/life programs such as flexible work arrangements, parental leave and support, employee assistance plan, commuter benefits, health club discounts, and educational/CFA certification reimbursement programs
Community involvement opportunities with The PIMCO Foundation in each PIMCO office"
46,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,"
At least one (1) year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models
At least two (2) years of experience in Scala, Python, Apache Spark and SQL
Experience working directly with relational database structures and flat files
Ability to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices
Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations.
Good verbal and written communication skills, with both technical and non-technical stakeholders",None Found,"
Build pipelines to ingest and maintain complex data sets into Cerebri AI’s proprietary data stores for use in machine learning modeling
Develop and maintain data ontologies for key market segments
Collaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data
Collaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data
Write quality documentation on the discovery process and software projects
Work equally well in a team environment and on your own.
Communicate complex ideas clearly with both team members and clients
Travel up to 25%",None Found,None Found,"Design, develop and build out data pipelines to ingest data into our proprietary data structures, and be a key collaborator in the data discovery and exploratory analysis process during our client engagements.
Responsibilities
Build pipelines to ingest and maintain complex data sets into Cerebri AI’s proprietary data stores for use in machine learning modeling
Develop and maintain data ontologies for key market segments
Collaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data
Collaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data
Write quality documentation on the discovery process and software projects
Work equally well in a team environment and on your own.
Communicate complex ideas clearly with both team members and clients
Travel up to 25%
Qualifications
At least one (1) year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models
At least two (2) years of experience in Scala, Python, Apache Spark and SQL
Experience working directly with relational database structures and flat files
Ability to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices
Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations.
Good verbal and written communication skills, with both technical and non-technical stakeholders
Nice to Haves
Experience in Java and/or Scala
Experience with data management processing tools such as Kafka, Elasticsearch and Logstash
Experience with NoSQL distributed databases such as Cassandra.
Experience in business intelligence visualization tools such as Grafana, Superset, Redash or Tableau.
Experience with Microsoft Azure or similar cloud computing solutions
Master’s degree or higher in a relevant quantitative subject"
47,Senior Data Engineer,"Austin, TX 78701",Austin,TX,78701,None Found,None Found,None Found,None Found,None Found,None Found,"Position Overview: From software hacking to hardware hacking, we help secure everything from cryptocurrency exchanges and space telescopes to autonomous vehicles and the electric grid. Today, Praetorian is making significant investments in terms of financial and engineering resources to develop a radically new customer experience we call “Security-as-a-Service” to provide customers with a unified, efficient, and data-driven security platform. We are looking to add the right individual to our growing team supporting the next wave of cybersecurity products and solutions.

As part of that investment, Praetorian is seeking a seasoned Data Engineer with a successful track record in data engineering in a hyper growth company setting. You will have the opportunity to work with some of the best security engineers in the world who hail from organizations such as Amazon, CIA, Facebook, Google, Microsoft, NSA, Redhat, Sun Microsystems, and US Air Force. As an Inc. Best Places to Work, Inc. 500 | 5000, Cybersecurity 500, and Austin Fast 50 Award recipient, we are seeking an individual that understands the professional and personal growth attached to this opportunity and who has the corresponding internal drive to maximize it.

To learn more about Praetorian, visit: https://www.praetorian.com/careers
[https://www.praetorian.com/careers]

Career opportunity:

Join an industry with massive socio, economic, and political importance in the 21st century
Work alongside some of the best and the brightest minds in the security industry
Leave an indelible mark on a company where individual input has real impact
Be recognized, internally and publicly, for your contributions in a high profile position
Align your career trajectory with a hyper growth company that is on the move

Core responsibilities:

Create pipelines to ingest and maintain complex data sets into Praetorian's data stores for use in machine learning models
Create tools to scour the internet to find important security information and ingest it into Praetorian's infrastructure
Work with Data Scientist to create and maintain data ontologies for seurity
Create the roadmap of how to continually evolve the data engineering infrastructure and techniques to improve Praetorian's ability to find security information
Mentor junior data engineers and teach them how to use data engineering techniques to solve real world problems
Communication of complex concepts to team members

Accountable for:

Creation of data engineering pipeline to find and ingest security vulnerabilities
Creation of data engineering tools to help label and validate data

Required qualifications:

At least 8 years experience designing and building data processing/ETL pipelines
At least 8 years experience in Python and Spark or similar technologies
At least 8 years experience with SQL and relational databases
At least 8 years experience parsing flat files
8+ years development experience
Prior track record in a hyper-growth, high-tech company
Bachelor's degree or equivalent practical experience

Desired qualifications:

Experience working with Google Tensorflow
Experience with modern technology stacks
Experience with micro-services architectures
Experience with cloud platforms and SaaS solutions
Experience with agile/scrum development practices
Experience with test driven development, continuous integration, continuous deployment
Experience with Git, JIRA, Confluence
Experience with Google Compute, Firebase, and GKE
Experience with Docker

Desired behaviors:

Relentless restlessness to turn theory into practice and develop production worthy code that solves real-world customer problems
Determination to always learn and get better and never rest on ones laurels
Personable individual who enjoys working in a team-oriented environment
Comfort dealing with ambiguity in an environment where we build the plane as we fly it
Ability to work within constraints and to challenge the status quo
Ability to self-direct work and truly own the position in a hyper-growth environment

Compensation package:

Competitive compensation
Ownership opportunity through employee stock option plan
Health, dental, and vision insurance
4% company 401K matching vested immediately

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

We are committed to an inclusive and diverse Praetorian. We are an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, disability, veteran status, genetic information, marital status, or any other legally protected status.

We ask that you please include a few paragraphs about yourself and what you are passionate about in your application."
48,Lead Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"You are curious, persistent, logical and clever a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Lead Data Enigneer. Scroll down to learn more about the position’s responsibilities and requirements.
What You’ll Do
Architecture design of holistic Cloud data ecosystem with a focus on Google Cloud Platform capabilities and features
Architecture design of Production, Staging/QA, and Development infrastructures running is 24/7 environments
Robust and consistent Cloud Strategy design aligned with business objectives
Provide guidelines for data migration approaches and techniques including ingest, store, process, analyze and explore/visualize data
Assistance with data migration and transformation
Evangelize Cloud computing expertise internally and externally to drive Cloud Adoption
What You Have
A degree in an associated field and/or other advanced certification along with significant experience
In-depth cloud professional, competent of quickly establishing connections and credibility in how to address the business needs via design and operate cloud-based solutions
Experience in Agile or PMI methodology managed projects
Experience in enterprise applications, and big data solutions
Experience in platform and cloud migrations, including migration factory
In-depth experience with databases and tools analysis
In-depth experience with ETL tools
Processes design and development for the data modeling, mining, and analysis
Extensive experience in methodologies and processes for large-scale databases management on-premises and cloud environment
In-depth understanding and knowledge of distributed version control systems like Git
Strong understanding of concepts and experience with StackDriver and other cloud-based monitoring tools including application level and logging
Nice to have
Google Cloud Certified Professional Data Engineer
Experience Creating automated tooling for cloud platforms
Experience with architecting and handling large datasets, structured and semi-structured data formats
Experience with streaming processing
Experience with messaging platforms
Experience with performance testing and tuning
Experience with GCP based security hardening including IAM, ACL, firewall rules, data traffic encryption
What We Offer
Medical, Dental and Vision Insurance (Subsidized)
Health Savings Account
Flexible Spending Accounts (Healthcare, Dependent Care, Commuter)
Short-Term and Long-Term Disability (Company Provided)
Life and AD&D Insurance (Company Provided)
Employee Assistance Program
Unlimited access to LinkedIn learning solutions
Matched 401(k) Retirement Savings Plan
Paid Time Off
Legal Plan and Identity Theft Protection
Accident Insurance
Employee Discounts
Pet Insurance"
49,Data Engineer,"Austin, TX 78723",Austin,TX,78723,None Found,"Use advanced SQL skills to manage data
Monitor database performance and tuning to improve query performance
Design and automate data pipelines and integrate different data sources using SSIS
Apply advanced skills to develop real-time data integrations in MS SQL Server
Establish techniques to monitor data quality and implement remediation procedures
Partner with non-technical users to identify needs/requirements and then translating the requirements into technical solutions
Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler
Provide advanced technical expertise to staff and end-users
",None Found,"Use advanced SQL skills to manage data
Monitor database performance and tuning to improve query performance
Design and automate data pipelines and integrate different data sources using SSIS
Apply advanced skills to develop real-time data integrations in MS SQL Server
Establish techniques to monitor data quality and implement remediation procedures
Partner with non-technical users to identify needs/requirements and then translating the requirements into technical solutions
Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler
Provide advanced technical expertise to staff and end-users
",None Found,None Found,"We’re excited you’re considering joining a great place to work! At Texas Mutual, we value our employees. Our service-inspired culture, great compensation and benefits package, award-winning wellness program and excellent career opportunities make Texas Mutual a great place to work. In the Data Engineer role, you will use advanced technical skills and knowledge to develop data models, automated ETL processes, stored procedures, and views in MS SQL Server. You will provide technical expertise to staff and end-users.
Responsibilities & Qualifications
Essential Functions:
Use advanced SQL skills to manage data
Monitor database performance and tuning to improve query performance
Design and automate data pipelines and integrate different data sources using SSIS
Apply advanced skills to develop real-time data integrations in MS SQL Server
Establish techniques to monitor data quality and implement remediation procedures
Partner with non-technical users to identify needs/requirements and then translating the requirements into technical solutions
Demonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler
Provide advanced technical expertise to staff and end-users
Requir ed Qualifications:
Bachelor’s degree in a related field.
At least five years of professional data experience for a Data Engineer; at least seven years of professional data experience for a Senior Data Engineer or any equivalent combination of education, training , and experience that provides the skills necessary to perform the essential function of the job.
Preferred Qualifications:
Experience managing cloud data assets
Data preparation using Python or R
Experience building data pipelines for machine learning
NoSQL database experience
Our Benefits:
Day one health, dental, and vision insurance
Performance bonus
401k plan with 4% basic employer contribution and 100% employer match contribution up to 6%
Vacation, sick, holiday and volunteer time off
Life and disability insurance
Flexible spending account
Free on-site gym and fitness classes
Professional development
Tuition reimbursement
Pet insurance
Free identity theft protection
Company-sponsored social and philanthropy events
Texas Mutual Insurance Company is an Equal Employment Opportunity employer."
50,Data Engineer Lead,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Key Role:
Lead a team of engineers to design, implement, and manage databases and data delivery systems and transform them into beautiful insights, analysis, and reporting. Comprehend database design and implementation tools, including entity-relationship data modelling and SQL, distributed computing architectures, operating systems, storage technologies, memory management, and networking to create structure and value out of complex and ambiguous technical challenges with little guidance. Leverage experience with structured and unstructured data, streaming and batch data processing, ETL, data wrangling, data ingest, and data access.
Basic Qualifications:7+ years of experience with data engineering3+ years of experience with leading a teamExperience with custom or structured ETL design, implementation, and maintenanceExperience with normalized and dimensional data modelsExperience with Microsoft SQL, SSIS, and SSASExperience with Microsoft Azure, including SQL PaaS, SQL DW, and Azure Data Factory (ADF)Experience with Spark and big data engineeringAbility to learn technical concepts quickly and communicate with multiple functional groupsAbility to obtain a security clearanceBA or BS degree
Additional Qualifications:Experience with GitHub, Confluence, and JenkinsPossession of excellent analytical and problem-solving skillsExperience with working in a DevOps environmentMA or MS degree
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
We’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change.
#LI-AH1, APC1, CJ1"
51,Senior Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,"5+ years of experience with building end-to-end scalable production-grade data pipelines
In-depth knowledge of data warehousing and master data management
Expertise with modern data architecture, data modeling, and data management principles
Hands-on experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)
Solid foundation in data structures, algorithms, and software design
Expertise with Relational and NoSQL databases to help teams best organize their data for analysis
Skilled in OOP design and development, preferably in Python
Advanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL
BS/BA degree in Computer Science, Engineering or related fields or equivalent experience","5+ years of experience with building end-to-end scalable production-grade data pipelines
In-depth knowledge of data warehousing and master data management
Expertise with modern data architecture, data modeling, and data management principles
Hands-on experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)
Solid foundation in data structures, algorithms, and software design
Expertise with Relational and NoSQL databases to help teams best organize their data for analysis
Skilled in OOP design and development, preferably in Python
Advanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL
BS/BA degree in Computer Science, Engineering or related fields or equivalent experience","General Information
Ref #: 27451
Functional Area: Technology
Employee Type: Full Time
Location: Austin
Experienced Required: Please See Below
Education Required: Bachelors Degree
Job Posting Shift: 1st
Date published: 28-Jun-2019
About Us:
We are PIMCO, a leading global asset management firm. We manage investments and develop solutions across the full spectrum of asset classes, strategies and vehicles: fixed income, equities, commodities, asset allocation, ETFs, hedge funds and private equity. PIMCO is one of the largest investment managers, actively managing more than $1.84 trillion in assets for clients around the world. PIMCO has over 2,700 employees in 17 offices globally. PIMCO is recognized as an innovator, industry thought leader and trusted advisor to our clients.

PIMCO is one of the world’s premier fixed income investment managers with thousands of professionals around the world united in a single purpose: creating opportunities for our clients in every environment. Since 1971, we have brought innovation and expertise to our partnership with the institutions, financial advisors and millions of individual investors who entrust us with their assets. We aspire to cultivate performance and leadership through empowering our people, diversity of thought, and a commitment to an inclusive culture that engages in our global communities.
Position Description:
As a Sr. Data Engineer on our team we will be working on a number of high-profile projects that will require you to collaborate with key partners and develop data solutions that enable insights into our clients through an evolving data architecture and drive our next generation data platform. A data engineer will meet with relevant partners, understand and model data assets, and constantly find opportunities to optimize and evolve the underlining data platform. You will work with software developers, data architects, data analysts, and data scientists to solve complex business problems. Demonstrating lifelong learning and collaboration to fill gaps in knowledge is essential.
Position Requirements:
5+ years of experience with building end-to-end scalable production-grade data pipelines
In-depth knowledge of data warehousing and master data management
Expertise with modern data architecture, data modeling, and data management principles
Hands-on experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)
Solid foundation in data structures, algorithms, and software design
Expertise with Relational and NoSQL databases to help teams best organize their data for analysis
Skilled in OOP design and development, preferably in Python
Advanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL
BS/BA degree in Computer Science, Engineering or related fields or equivalent experience

PREFERRED QUALIFICATIONS
Domain knowledge of Financial Services
Hands-on experience with working with Cloud technologies, including AWS
Skilled at designing and implementing ETL frameworks
Experience with CI/CD methodologies
Experience with *nix environments, including shell script development

We are PIMCO, a global investment management firm with a singular focus on preserving and enhancing investors’ assets. We manage investments for institutions, financial advisors and individuals, helping millions of people around the world meet their financial goals.

Our technology powers the firm’s global trading platform. We employ sophisticated and cutting edge technology tools that support PIMCO’s core investment management strategy.

Why PIMCO? At PIMCO you will join a dynamic, constantly evolving global firm that pushes you to grow, lead and innovate. You will be client- focused and work on technologies that will be put to immediate use. You will be part of a team whose members are encouraged to speak up with an idea or challenge existing views, regardless of title or tenure. You will have the opportunity to receive competitive compensation and other attractive benefits (Please see below).

PIMCO’s Technology Team is organized in small, focused, agile groups, that either work closely with business units to deliver value or develop core technologies that lever the product teams. Our environment fosters innovation and promotes entrepreneurial spirit, and we use top of the line tools. PIMCO recognizes the paramount role of tech now and in the future and invests in technology accordingly. Technology careers are available in Newport Beach, Austin, New York, London, Munich, Singapore and Tokyo.
Benefits:
PIMCO is committed to offering a comprehensive portfolio of employee benefits designed to support the health and wellbeing of you and your family. Benefits vary by location but may include:
Medical, dental, and vision coverage
Life insurance and travel coverage
401(k) (defined contribution) retirement savings, retirement plan, pension contribution from your first day of employment
Work/life programs such as flexible work arrangements, parental leave and support, employee assistance plan, commuter benefits, health club discounts, and educational/CFA certification reimbursement programs
Community involvement opportunities with The PIMCO Foundation in each PIMCO office"
52,Data Engineer,"Austin, TX 78728",Austin,TX,78728,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description
Job Title: Data Engineer
Location: San Francisco, CA, Austin, TX, San Jose, CA
Terms: Full-time
About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.
What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.
As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do
Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:
Cloud
Analytics
Digitization
Infrastructure
Security
Job Description
Overview
Data is the way our clients make decisions. It is the core to their business, helping create an experience for customers and providing insights into the effectiveness of our product launch & features.

As a Data Engineer , you will be a part of an early stage team that builds the data pipelines, collection, and storage, and exposes services that make data a first-class citizen. We are looking for a Data Engineer to build a scalable data platform. You'll have ownership of core data pipelines that powers top line metrics; You will also use data expertise to help evolve data models in several components of the data stack; You will help architect, building, and launching scalable data pipelines to support growing data processing and analytics needs. Your efforts will allow access to business and user behavior insights, using huge amounts of data to fuel several teams such as Analytics, Data Science, Marketplace and many others.

Responsibilities

Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth
Evolve data model and data schema based on business and engineering needs
Implement systems tracking data quality and consistency
Develop tools supporting self-service data pipeline management (ETL)
SQL and MapReduce job tuning to improve data processing performance

Experience

3+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct advanced performance tuning
Strong skills in scripting language (Python, Ruby, Bash)
1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)
Comfortable working directly with data analytics to bridge Lyft's business goals with data engineering

We are Growing Rapidly: 2019 Highlights
Trianz is growing above the average of the professional services industry. Here are some highlights.
Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.
Won the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.
Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.
Featured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.
Achieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.
Talk to us, Join us & Develop into Leaders
Come join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.
We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
Equal Opportunity Employer
Trianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law)."
53,Senior Cloud Solutions Architect,"Austin, TX",Austin,TX,None Found,None Found,"
Mastery in at least one of the following domain areas:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio
Application Development: building custom web and mobile applications on top of the GCP stack
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Experience providing oversight and direction of cloud projects
Experience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states
Experience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP
Experience across multiple cloud platforms: GCP, AWS, Azure
Experience with container engines: Kubernetes, Docker, AWS Elastic Container Service
Experience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation
Experience working with engineering and sales teams to elicit customer requirements
Ability to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders
Time management skills with the ability to manage multiple streams and lead less experienced architects
Experience as a technical consultant or another customer-facing technical role
",None Found,None Found,None Found,None Found,"Join SADA as a Sr. Cloud Solutions Architect!

Your Mission

As a Sr. Cloud Solutions Architect at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You’re also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.

You will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures that other engineers and architects demur to you for lack of expertise. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.

Pathway to Success

#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.

You will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.

As you continue to execute successfully, we will build a customized development plan together that leads you through the solution architecture or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Cloud Architect Certified

[https://cloud.google.com/certification/cloud-architect] and/or Google
Professional Data Engineer Certified
[https://cloud.google.com/certification/data-engineer], or able to complete one of the above within the first 45 days of employment.

Required Qualifications:

Mastery in at least one of the following domain areas:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio
Application Development: building custom web and mobile applications on top of the GCP stack
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Experience providing oversight and direction of cloud projects
Experience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states
Experience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP
Experience across multiple cloud platforms: GCP, AWS, Azure
Experience with container engines: Kubernetes, Docker, AWS Elastic Container Service
Experience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation
Experience working with engineering and sales teams to elicit customer requirements
Ability to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders
Time management skills with the ability to manage multiple streams and lead less experienced architects
Experience as a technical consultant or another customer-facing technical role

Useful Qualifications:

Hands-on experience designing and recommending elegant solutions that drive business outcomes
Experience building, designing and migrating complex cloud architectures
Strong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team
Ability to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security
Deep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed
Knowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes
Highly self-motivated and able to work independently as well as in a team environment

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
54,Data Engineer,"Austin, TX 78746",Austin,TX,78746,None Found,"
Bachelor’s degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.
3 or more years of experience as a data engineer on enterprise-level data solutions, specifically as a Data Engineer or ETL Developer.
2 or more years of experience working with relational and unstructured databases and enterprise data warehouses, such as work with MySQL, PostgreSQL, MongoDB, SQL Server, or Oracle.
Experience with Spark, Presto, Hive and/or other map/reduce ""big data"" systems and services.
Experience in SQL and Python for scripting automation.
",None Found,"
Design, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources
Participate in data architecture discussions to understand target data structures, required data transformations and deliver data pipelines/ETL loading processes that meet requirements.
Perform detailed exploration of new internal and external source data to perform source-to-target mapping to inform the development of new data pipelines/flows.
Work in close collaboration with your data-minded colleagues focused on back-end (microservice) development, business intelligence reporting, machine learning and artificial intelligence models.
Investigate the root cause of data-related issues and implement viable, sustainable solutions to correct issues.
Perform database administration activities such as refreshes, updates, migrations, etc. in support of data pipeline maintenance.
",None Found,None Found,"What you’ll be called: Data Engineer

Where you’ll work: KWRI Headquarters—Austin, TX

Named a Happiest Company to Work for in 2019; one of the Best Places to Work in Austin, TX; and featured on the Training Magazine Training 125 list seven times, Keller Williams Realty International (KWRI) thrives within a creative and collaborative culture where transforming the real estate industry through technology is our primary goal.

KW Technology is the foremost provider of real estate solutions, offering the most comprehensive end-to-end portfolio of products, services and training in the industry. Our Data Engineering team converts agent and consumer challenges into intuitive, insight-enhanced technology and consumer experiences using tools such as Python, Hadoop, Spark, MySQL, MongoDB and Snaplogic.

What you’ll do:

Design, develop and implement data infrastructure and best-in-class pipelines that collect, connect, centralize and curate data from various internal and external data sources. You will ensure that architectures support the needs of the business, and recommend ways to improve data reliability, efficiency.

Essential Duties and Responsibilities:

Design, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources
Participate in data architecture discussions to understand target data structures, required data transformations and deliver data pipelines/ETL loading processes that meet requirements.
Perform detailed exploration of new internal and external source data to perform source-to-target mapping to inform the development of new data pipelines/flows.
Work in close collaboration with your data-minded colleagues focused on back-end (microservice) development, business intelligence reporting, machine learning and artificial intelligence models.
Investigate the root cause of data-related issues and implement viable, sustainable solutions to correct issues.
Perform database administration activities such as refreshes, updates, migrations, etc. in support of data pipeline maintenance.
Minimum Qualifications:

Bachelor’s degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.
3 or more years of experience as a data engineer on enterprise-level data solutions, specifically as a Data Engineer or ETL Developer.
2 or more years of experience working with relational and unstructured databases and enterprise data warehouses, such as work with MySQL, PostgreSQL, MongoDB, SQL Server, or Oracle.
Experience with Spark, Presto, Hive and/or other map/reduce ""big data"" systems and services.
Experience in SQL and Python for scripting automation.
Preferred Qualifications:

Master’s degree in Information Management, Data Science, Analytics or related field.
Experience building open source data pipeline systems such as AirFlow, Hadoop or Kafka.
Familiar working in a Cloud environment (AWS or GCP) with a subset of the following tools or their equivalent - Redshift, RDS, S3, EC2, Lambda, Kinesis, Elasticsearch, EMR, BigQuery, GCS.
Who are we?

Keller Williams Realty Inc. is the largest real estate company by agent count across the globe and is number one in units and volume in the United States. Founded in 1983, we pride ourselves on an agent-centric, technology-driven and education-based culture that rewards agents as stakeholders. Keller Williams Realty International (KWRI), is the company’s corporate headquarters located in Austin, TX. Here, through a focus on cutting edge technology, education, and products and services, we support our agents and associates to create careers worth having, businesses worth owning, lives worth living, experiences worth giving and legacies worth leaving."
55,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Are you looking for a high energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform?
Are you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley?
Your next adventure at VMware is only a click away!
VMware's Data Analytics Team is looking for a Data Engineer to help build on Next generation Near Realtime BI Platform based on SAP HANA and Hadoop. You will be responsible for building and enhancing the solutions on the existing platform based on the business needs in partnering with fellow Developers and Business groups.
Responsibilities:Understand the business capability/requirements and transform them into robust design solutionsPerform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as neededPerform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms.Perform hands on work using SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform.Integrate data sets from difference sources using Informatica, Python, SAP SDI/SLTProtect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data.Help data consumers to correctly understand and use the data.Building reports based on the business need.
Qualifications:5+ years of experience in as a BI/Data Engineer handling large volumes of data.Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools.Expertise in writing advanced SQL queries.Experience working with Informatica, SAP SDI/SLTExpertise in SAP HANA, Hive/Hadoop/HawqWorking knowledge of BI Reporting tools like BOBJ and TableauExperience in Python ScriptingFamiliarity with Amazon Web Services (AWS), Redshift is a plusStrong analytical and troubleshooting skillsExcellent verbal and written communication skillsBachelor’s degree in Computer science, Statistics, Mathematics, Engineering or relevant field.
This position is eligible for the IT Apps Hiring FY20 referral campaign
VMware (NYSE: VMW) is the global leader in virtualization and cloud infrastructure, two areas that consistently rank as top priorities among CIOs. VMware delivers award-winning, customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Our solutions help organizations of all sizes, lower costs, increase business agility and ensure freedom of choice. We are searching for people who are ready to accelerate, innovate and lead to join our team of more than 20,000 employees in 40+ locations worldwide working to develop innovative solutions that deliver the future of IT through cloud computing. Having the audacity to challenge constraints and problem-solve for tomorrow starts today, and it starts with you. Learn more at www.vmware.com/careers

VMware Company Overview: VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com.

Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law."
56,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,"
Expertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
",None Found,None Found,None Found,None Found,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Expertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
57,"Data Engineer, Research","Austin, TX",Austin,TX,None Found,None Found,"Bachelor’s or Master’s in Computer Science, Computer Engineering, Electrical Engineering, Computer Information Systems, MIS, or relevant technology degree from a top-tier school.
Minimum of 5+ years SQL Server Development experience preferred.
Demonstrate knowledge and ability of database development skills including physical structure, overall architecture, and database analysis.
Solid software development skills in an object-oriented programming language (C#, C++, Java, etc.).
Programming skills with statistical software including Python, R, SAS or Matlab a plus.
Proven database design and implementation experience with the ability to provide end-to-end database solutions and resolve complex database issues.
Experience in database query optimization, performance tuning, and monitoring.
Expert knowledge of best practices in database design.
Strong time management skills with the ability to participate in multiple projects/work streams simultaneously.
Detail-oriented, organized, highly motivated and able to work independently and in a team environment.
Excellent verbal and written communications skills.
Self-starter who is capable of managing multiple projects and meeting deadlines.
Experience with research in securities and financial markets preferred.
Knowledge of finance/asset pricing is preferred.
",None Found,"Design, compile and manage efficiently large financial databases from a variety of financial data vendors.
Develop sophisticated code in object-oriented programming languages such as C#, C++ or Python for historical portfolio simulations and tools for investment and performance analysis.
Conduct data analysis for projects related to research on equities and fixed income markets, retirement research, and other investment research.
",None Found,None Found,"The Research group at Dimensional is essential both in the successful daily functioning of the firm and helping develop Dimensional’s long-term strategy. The team produces high-quality, expert research on investments and financial markets that is of interest to and helps educate clients. Research is also involved in the design of the firm’s investment approach and the application of that approach through portfolio management and trading.
This position is responsible for the management and development of our proprietary global security-level database. This database is used by the Research team to enhance our understanding of long-term, short-term and intra-day drivers of expected returns and to help develop new investment strategies to meet the needs, goals and preferences of our clients. It will be primarily project-based work, extending existing code infrastructure and creating new ones. This role will be based in our Austin, TX headquarters and reports to a senior member of our Research team.
Responsibilities:
Design, compile and manage efficiently large financial databases from a variety of financial data vendors.
Develop sophisticated code in object-oriented programming languages such as C#, C++ or Python for historical portfolio simulations and tools for investment and performance analysis.
Conduct data analysis for projects related to research on equities and fixed income markets, retirement research, and other investment research.
Qualifications:
Bachelor’s or Master’s in Computer Science, Computer Engineering, Electrical Engineering, Computer Information Systems, MIS, or relevant technology degree from a top-tier school.
Minimum of 5+ years SQL Server Development experience preferred.
Demonstrate knowledge and ability of database development skills including physical structure, overall architecture, and database analysis.
Solid software development skills in an object-oriented programming language (C#, C++, Java, etc.).
Programming skills with statistical software including Python, R, SAS or Matlab a plus.
Proven database design and implementation experience with the ability to provide end-to-end database solutions and resolve complex database issues.
Experience in database query optimization, performance tuning, and monitoring.
Expert knowledge of best practices in database design.
Strong time management skills with the ability to participate in multiple projects/work streams simultaneously.
Detail-oriented, organized, highly motivated and able to work independently and in a team environment.
Excellent verbal and written communications skills.
Self-starter who is capable of managing multiple projects and meeting deadlines.
Experience with research in securities and financial markets preferred.
Knowledge of finance/asset pricing is preferred.
It is the policy of the Company to provide equal employment opportunity for all applicants and employees. The Company does not unlawfully discriminate on the basis of race, color, religion, creed, sex, gender, gender identity, gender expression, national origin, age, disability, genetic information, ancestry, medical condition, marital status, covered veteran status, citizenship status, sexual orientation, or any other protected status. This policy applies to all areas of employment including recruitment, hiring, training, job assignment, promotion, compensation, benefits, transfer, discipline, termination, and social and recreational programs."
58,Tech Consulting Manager - Big Data Engineer,"Austin, TX 78701",Austin,TX,78701,None Found,None Found,"Designing, Architecting, and Developing solutions leveraging big data technology (Open Source, AWS, or Microsoft) to ingest, process and analyze large, disparate data sets to exceed business requirements
Unifying, enriching, and analyzing customer data to derive insights and opportunities
Leveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements
Clearly communicating findings, recommendations, and opportunities to improve data systems and solutions
Demonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches
Seeking out information to learn about emerging methodologies and technologies
Clarifying problems by driving to understand the true issue
Looking for opportunities for improving methods and outcomes
Applying data driven approach (KPIs) in tying technology solutions to specific business outcomes
Collaborating, influencing and building consensus through constructive relationships and effective listening
Solving problems by incorporating data into decision making
",None Found,None Found,None Found,"EY delivers unparalleled service in big data, business intelligence, and digital analytics built on a blend of custom-developed methods related to customer analytics, data visualization, and optimization. We leverage best practices and a high degree of business acumen that has been compiled over years of experience to ensure the highest level of execution and satisfaction for our clients. At EY, our methods are not tied to any specific platforms but rather arrived at by analyzing business needs and making sure that the solutions delivered meet all client goals.
The opportunity
You will help our clients navigate the complex world of modern data analytics. We’ll look to you to provide our clients with a unique business perspective on how Big Data analytics can transform and improve their entire organization - starting with key business issues they face. This is a high growth, high visibility area with plenty of opportunities to enhance your skillset and build your career.
Your key responsibilities
You’ll spend most of your time working with a wide variety of clients to deliver the latest big data technologies and practices to design, build and maintain scalable and robust solutions that unify, enrich and analyse data from multiple sources.
Skills and attributes for success
Designing, Architecting, and Developing solutions leveraging big data technology (Open Source, AWS, or Microsoft) to ingest, process and analyze large, disparate data sets to exceed business requirements
Unifying, enriching, and analyzing customer data to derive insights and opportunities
Leveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements
Clearly communicating findings, recommendations, and opportunities to improve data systems and solutions
Demonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches
Seeking out information to learn about emerging methodologies and technologies
Clarifying problems by driving to understand the true issue
Looking for opportunities for improving methods and outcomes
Applying data driven approach (KPIs) in tying technology solutions to specific business outcomes
Collaborating, influencing and building consensus through constructive relationships and effective listening
Solving problems by incorporating data into decision making
To qualify for the role you must have
A bachelor's degree and approximately six years of related work experience; or a master's degree and approximately five years of related work experience
At least five years hands-on experience with various Big Data technologies in one or more ecosystems: Open Source, Microsoft, or AWS:
Hadoop, Spark, NoSQL, Streaming, Atlas, Sqoop, HIVE
AWS, EMR, Hortonworks, Cassandra, Mongo, Redshift, Kafka
Azure, HDInsight, Azure DocumentDB, SQL Server
Proficiency coding in Java, C#, C++, or Scala
Experienced organizing, aggregating, querying, and analyzing large data sets
Communication is essential, must be able to listen and understand the question and develop and deliver clear insights.
Outstanding team player.
Independent and able to manage and prioritize workload.
Ability to quickly and positively adapt to change.
A valid driver’s license in the US; willingness and ability to travel to meet client needs.
Ideally, you’ll also have
Bachelor’s Degree or above in mathematics, information systems, statistics, computer science, or related disciplines
What we look for
We’re interested in passionate leaders with strong vision and a desire to stay on top of trends in the Big Data industry. If you have a genuine passion for helping businesses achieve the full potential of their data, this role is for you.
What working at EY offers
We offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, both pension and 401(k) plans, a minimum of 15 days of vacation plus ten observed holidays and three paid personal days, and a range of programs and benefits designed to support your physical, financial and social well-being. Plus, we offer:
Opportunities to develop new skills and progress your career
A collaborative environment where everyone works together to create a better working world
Excellent training and development prospects, both through established programs and on-the-job training
About EY
As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.
Join us in building a better working world. Apply now.

EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status."
59,Senior Data Engineer,"Austin, TX 78701",Austin,TX,78701,None Found,None Found,None Found,None Found,None Found,None Found,"At RetailMeNot, we believe that gaining valuable insights using our data is core to our future success. The Data team at RetailMeNot is responsible for developing core datasets and for exposing data services consumed by product, data science and business teams. Daily, we collect approximately a terabyte of analytics events and process hundreds of terabytes of data. Our team works efficiently to deliver new features for real-time and batch processing services. We use primarily AWS cloud services and Kubernetes to build and deploy services quickly, at scale and with no downtime.

This team is integral to the RetailMeNot business, so we need engineers who can deliver results while understanding the structure of a large system. We provide cross-team leadership that ensures that RetailMeNot code meets a consistent standard while building the platform of the future. With this team, your daily activities will involve oversight, mentoring, delivering key pieces of functionality, and collaborating with technology leadership to plan the technical roadmap for RMN.

We are constantly evolving both the software and the teams that deliver it. If you’re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, then we want you to be a part of the team.
Who You Are
You have 4+ years work experience
You are highly skilled using Scala, Python (Spark), Linux, Docker, Git, and Amazon Web Services (or have translatable experience with similar toolsets)
You have extensive SQL experience on a variety of RDBMS, and enjoy optimizing queries as well as designing efficient data models
You have developed scalable solutions using both SQL and NoSQL (Hadoop) databases. Working with data sets comprised of millions or billions of records is comfortable
You are familiar with one or more cluster-computing frameworks (Spark)
You strive to identify simple solutions to complex problems, can identify a minimal viable product and enjoy iterative development
You are able to accurately estimate tasks, identify dependencies and dedicatedly solve problems to ensure commitments are met
You recognize that your success depends upon enabling your fellow team members to succeed; taking time to help others energizes you
You enjoy gathering requirements from non-technical coworkers and delivering solutions that meet their needs and exceed their expectations
You derive satisfaction from enabling the business to succeed and delighting coworkers, not building technology for its own sake
You have a work ethic that inspires your fellow team members to give their best
What You'll Do
Implement data system for both real-time and warehouse applications
Develop ETL processes that ensure data is accurate and available within SLAs
Enhance data models by developing integrations with business partners
Seek opportunities for performance improvement and implement optimizations
Create dashboards that provide insight into the health of data integrations, ETL processes and data sets
Mentor junior data engineers on standard methodologies and provide code review when needed
Who We Are
We hire intelligent people and give them the autonomy to be creative, have an impact, and share standard methodologies.
We have a generous leave policy for new parents and 401k matching.
We have a thriving Diversity and Inclusion program that gives back to the community and supports multiple Austin events and organizations with like-minded goals throughout the year.
We serve breakfast Mondays and Fridays, lunch four days a week, provide all the snacks you could dream of, and have our own coffee bar run by trained baristas.
We have a Friday board game happy hour for co-workers to mingle or just relax.
We provide reimbursements for cell phones and gym memberships.
We have an outstanding open vacation policy.

Rewards*
We offer an opportunity to be an integral part of a company that eagerly pursues disruption in its space to continue to drive innovation and lead the competition. Benefits of being an employee of RetailMeNot, Inc. include, but are not limited to the following:
Competitive base & bonus packages; salary negotiableLong Term Incentive PlanPerformance based rewards & recognition for your hard work and serviceVery competitive benefits packages, including best-in-class parental leaveOpen & flexible PTOCell phone & gym membership reimbursementsFully stocked break room & onsite catered breakfasts & lunches multiple days/week
Some rewards do not apply to contract workers or interns.

About Us
RetailMeNot, Inc. is a leading savings destination bringing people and the things they love together through savings with retailers, brands, restaurants and pharmacies. RetailMeNot makes everyday life more affordable through online and in-store coupon codes, cash back offers, discount gift cards, and the RetailMeNot Genie browser extension. Savings are also provided in consumers’ mailboxes through the RetailMeNot Everyday™ direct mail package, and at the pharmacy with RxSaver by RetailMeNot.

RetailMeNot is a wholly owned subsidiary of Harland Clarke Holdings. http://www.retailmenot.com/corp or follow @RetailMeNot on social media.

U.S. Equal Employment Opportunity/Affirmative Action Information
Individuals seeking employment at RetailMeNot, Inc. are considered without regards to race, color, creed, religion, gender, gender identity, national origin, citizenship, age, sex, marital status, ancestry, physical or mental disability, veteran status, sexual orientation, or any other protected classification. You are being given the opportunity to provide the following information in order to help us align with federal and state Equal Employment Opportunity/Affirmative Action record keeping, reporting, and other legal requirements."
60,AWS Data Engineer,"Austin, TX 78727",Austin,TX,78727,None Found,"At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.","DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud",None Found,None Found," Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
§ Certified AWS Developer - Associate
§ Certified AWS DevOps – Professional (Nice to have)
§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
61,Big Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"At SpringML, we are all about empowering the 'doers' in companies to make smarter decisions with their data. Our predictive analytics products and solutions apply machine learning to today's most pressing business problems so customers get insights they can trust to drive business growth. We are a tight knit, friendly team of passionate and driven people who are dedicated to learning, get excited to solve tough problems and like seeing results, fast.
Your primary role will be to design and build data pipelines. You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive. In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies. If you believe you have these skills please email your resume to info@springml.com.


Required Skills:
4-7 years Python and Java programming
3-5 years knowledge of Java/J2EE
3-5 years Hadoop, Big Data ecosystem experience
3-5 years of Unix experience
Bachelors in Computer Science (or equivalent)
Duties and Responsibilities:
Design and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.
Read, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.
Migrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.
Should have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Additional Skills that are a plus:
C, Perl, Javascript or other programming skills and experience a plus
Production support/troubleshooting experience
Data cleaning/wrangling
Data visualization and reporting
Devops, Kubernetes, Docker containers
dpFhuZNHdz"
62,Sr. Data Engineer,"Austin, TX 78728",Austin,TX,78728,None Found,None Found,None Found,"
Ingestion of data from multiple, unstructured sources using multiple analytics tools
Implementing ETL process
Monitoring performance and advising any necessary infrastructure changes
Defining data retention policies",None Found,"
Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.","Job Title: Data Engineer

Location: San Francisco, Chicago, San Jose, Palo Alto, Austin, TX

Terms: Full-time, Contract, Contract-2-Hire

About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.

What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.

As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do

Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:

Cloud
Analytics
Digitization
Infrastructure
Security

Sr. Data Engineer
Job Description
Responsibilities
Ingestion of data from multiple, unstructured sources using multiple analytics tools
Implementing ETL process
Monitoring performance and advising any necessary infrastructure changes
Defining data retention policies

Requirements
3+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct query performance tuning
Strong skills in one of the scripting language (Python, Ruby, Bash)
1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)

We are Growing Rapidly: 2019 Highlights

Trianz is growing rapidly. Here are some highlights.

Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.

Won the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.

Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.

Featured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.

Achieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.

Talk to us, Join us & Develop into Leaders
Come join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.
 We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
 Equal Opportunity Employer
Trianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law)."
63,Staff Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,"Collaborate with peers on requirements, designs, code reviews, and testing
Produce designs and rough estimates, and implement features based on product requirements
Deliver efficient, maintainable, robust Java/Scala based microservices
Produce unit and end-to-end tests to improve code quality and maximize code coverage for new and existing features
Productize and operationalize machine learning algorithms
Actively engage in technology discovery that can be applied to the product
",None Found,"7-10 years of professional software development experience
2+ years of data engineering or related experience
Strong Java and/or Scala experience
Experience with Agile development practices and continuous delivery
Proficient understanding of distributed computing principles. microservice architectures and patterns
Experience with integration of data from multiple data sources
Experience writing unit and integration tests
Great communication skills
BS in Computer Science or a related experience
","At SailPoint, we do things differently. We understand that a fun-loving work environment can be highly motivating and productive. When smart people work on intriguing problems, and they enjoy coming to work each day, they accomplish great things together. With that philosophy, we’ve assembled the best identity team in the world that is passionate about the power of identity.
As the fastest-growing, independent identity and access management (IAM) provider, SailPoint helps hundreds of global organizations securely and effectively deliver and manage user access from any device to data and applications residing in the data center, on mobile devices, and in the cloud. The company’s innovative product portfolio offers customers an integrated set of core services including identity governance, provisioning, and access management delivered on-premises or from the cloud (IAM-as-a-service).
SailPoint is seeking a Sr/Staff Data Software Engineer to help build a new cloud-based identity analytics product incorporating real-time data pipelines, machine learning algorithms and multi-tenancy support. We are looking for well-rounded backend or full stack engineers who are passionate about building and delivering reliable, scalable microservices and infrastructure for SaaS products.
Responsibilities
Collaborate with peers on requirements, designs, code reviews, and testing
Produce designs and rough estimates, and implement features based on product requirements
Deliver efficient, maintainable, robust Java/Scala based microservices
Produce unit and end-to-end tests to improve code quality and maximize code coverage for new and existing features
Productize and operationalize machine learning algorithms
Actively engage in technology discovery that can be applied to the product
Requirements
7-10 years of professional software development experience
2+ years of data engineering or related experience
Strong Java and/or Scala experience
Experience with Agile development practices and continuous delivery
Proficient understanding of distributed computing principles. microservice architectures and patterns
Experience with integration of data from multiple data sources
Experience writing unit and integration tests
Great communication skills
BS in Computer Science or a related experience
Preferred
Experience with Cloud computing architectures (AWS, Google Cloud)
Experience with Kafka, Flink/Spark, Elasticsearch technologies or related
Experience integrating data pipelines for machine learning
Experience with container technologies (Docker, Kubernetes, etc.)
Experience with NoSQL databases, such as Redshift, Cassandra, DynamoDB
Experience instrumenting code for gathering production performance metrics
Compensation and benefits
Experience a Small-company Atmosphere with Big-company Benefits
Competitive pay, 401(k) and comprehensive medical, dental and vision plans
Recharge your batteries with a flexible vacation policy and paid holidays
Grow with us with both technical and career growth opportunities
Enjoy a healthy work-life balance with flexible hours, family-friendly company events and charitable work
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status."
64,Data Engineer - Associate,"Austin, TX",Austin,TX,None Found,None Found,None Found,"
Experience with RDBMS applications (SQL Server preferred)
Good communication skills and experience working with cross-functional teams
Exposure to the concepts of data warehouse design
SQL programming familiarity in large RDBMS systems (T-SQL preferred)","
Troubleshoot and resolve issues as they arise related to all BI Tools
Manage iteration and release cycles and deployments
Assist in data modeling and design sessions
Proactively maintain documentation and training materials",None Found,None Found,"Data Engineer (early career)
Austin or Chicago
(Visa sponsorship not currently offered)

Mattersight is a leader in enterprise analytics focused on customer and employee interactions and behaviors. Mattersight's Behavioral Analytics service captures and analyzes customer and employee interactions, employee desktop data, and other contextual information to improve operational performance and predict future customer and employee outcomes. Mattersight’s analytics are based on millions of proprietary algorithms and the application of unique behavioral models. The company's SaaS+ delivery model combines analytics in the cloud with deep customer partnerships to drive significant business value. Mattersight's applications are used by leading companies in Healthcare, Insurance, Financial Services, Telecommunications, Cable, Utilities and Government. See What Matters™ by visiting www.Mattersight.com.

Data Engineer Role & Responsibilities:
The Data Engineer will be part of the Routing Analytics R&D team, which provides the data sources and analytical tools used to help our clients derive maximum value from our Behavioral Routing solution. This position will support the day to day reporting needs of the Routing clients, including Business Monitoring, Insights, Product Development, Analysis & Testing, and others. Responsibilities include troubleshooting issues that occur with existing report deliverables as well as developing new reports and integrating them into the overall service catalog. This role will require development, testing, and configuration management of all BI deliverables in coordination with Data Engineers, Software Engineers, and Testers within the organization. Additionally, the Data Engineer will be responsible for maintaining any documentation and training materials required to support the various business units the group serves.

This individual will also support the Data Warehouse Specialist to troubleshoot issues relating to the warehouse, especially as they impact the reporting environment.

To summarize, the Data Engineer will be responsible for, but not limited to, the following tasks:

Troubleshoot and resolve issues as they arise related to all BI Tools
Manage iteration and release cycles and deployments
Assist in data modeling and design sessions
Proactively maintain documentation and training materials

Because of the data-centric culture and rapid growth of NICE Mattersight, a rich career path exists for the Data Engineer within Mattersight.

Preferred Skills/Attributes
Experience with RDBMS applications (SQL Server preferred)
Good communication skills and experience working with cross-functional teams
Exposure to the concepts of data warehouse design
SQL programming familiarity in large RDBMS systems (T-SQL preferred)
Exposure to ETL and data integration processes

Required Knowledge, Skills & Abilities

Previous Experience
For this role, Mattersight is not requiring previous work experience in a technical role though some experience in a Business Intelligence environment working with BI visualization tools, relational databases, and/or data warehousing systems is helpful. Experience with ETL applications and data modeling/UML software is also helpful. Required is an interest in data pipelines, data aggregations, and creatively solving data-related problems as well as the motivation to dive deeper into the data engineering world. We’d also like to see a candidate who displays evidence of strong communication skills and the ability to work under pressure.
Ideal CandidateAnalyticalDetail OrientedStrong CommunicatorEntrepreneurialResults OrientedTask AgilityOperations-MindedAdept Time ManagerProblem SolverTeam PlayerSeeker of ExcellenceHigh Knowledge Bandwidth

Company Culture & Facts
Corporate Culture
Mattersight values diversity amongst its employees. Employees from all levels of experience and backgrounds are mingled together and are encouraged to learn about projects others are working on. Mattersight fosters teamwork as well as self motivation.
Eligibility & Location
Mattersight seeks candidates authorized to work in the United States. The Routing Analytics Data Engineer role will be based out of Mattersight’s Austin location; candidates should anticipate little to no travel.
Compensation
Mattersight is prepared to offer a highly competitive benefits and compensation package for the ideal candidate.
For more information about Mattersight, visit http://www.mattersight.com/. Mattersight is committed to equal opportunity and affirmative action in all employment matters: Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, ancestry, marital or domestic partner status, national origin, disability or medical condition, pregnancy, veteran or military status, sexual orientation, gender identity, or on account of membership or affiliation with anyone in any of the foregoing categories, or any other protected category under federal, state or local law. Although particular legal provisions may differ in various locations in which we do business, our principles are the same worldwide."
65,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Are you passionate about data ecosystems and the ability to use data to drive actionable change? If so this role with our team at Atlassian is for you. As a Data Engineer on the Customers Success and Support team, you'll be driving our business to scale via building and improving our data infrastructures and data pipeline.
Some examples of what you will be doing are:
Architect, build, launch, and manage data models to enable analytics
Design, build, and manage data warehouse
Design, build, improve, and manage data pipelines and ETL jobs
Create ETL scripts via SQL/HiveQL/SparkSQL
Automate data pipeline and reporting processes
Build data expertise and own data quality for the awesome pipelines you build
Some of the tools/languages/systems you'll use are:
Hive, Spark, Postgres, Presto
Amazon EC2, EMR, S3
Python
Docker
Tableau
SourceTree and Bitbucket
Linux Shell
You'll work together with other data engineers, analysts, project managers, and subject matter experts to deliver impactful outcomes to the organization. You'll participate in multiple concurrent high-visibility projects along with occasional ad-hoc questions from your internal customers.

We continually require modifications to the data pipeline for improvements in quality, speed, and features. In this role you'll focus on building out the future state of our data pipeline. You'll help design event collection infrastructure, build data models, and ETL processes to collect, extract, and clean the data for subsequent reporting and analysis. The target is making our data model more scalable, reliable, maintainable, and better integrated with other parts of our data ecosystem.

More about you

You've been in a data engineering role for 5+ years and have a BS degree in Engineering, Computer Science, or other technical discipline. You value high-quality work with attention to detail and have a track record of delivering both. You combine curiosity with critical thinking and good judgment, and like asking ""why"" to unravel a seemingly complex problem and get to the root cause. You know how to gather, document, and interpret business requirements.

You're a wizard with SQL; better than anyone else you know. You've got practical experience working with large structured or unstructured datasets. You enjoy thinking about improvements to the ways in which data is consumed and then figuring out how to make it happen via reporting platforms and visualizations. You have a nearly insatiable desire to learn new concepts and technologies and apply them to your work.

When you encounter a problem you come up with multiple solutions, weigh the tradeoffs and efforts, identify the best path forward, and exercise good judgment to drive ahead. You're comfortable interacting with people across all levels of an organization and can field questions during a presentation like a pro. You're self-driven and find ways to be impactful.

You're quick on your feet and take on challenges with ease. You can take an ambiguous assignment and derive valuable insight. You use multiple tools and methods to find solutions, and couple that with intuition and quick tests to prioritize how to unravel complicated problems.


More about our team

You'll be joining a growing analytics and project delivery team located in multiple regions across the globe. We challenge each other constantly to improve our work and ask hard questions. We're direct, focused, and demand excellence, but there's laughter in every meeting because we thoroughly enjoy the work we do and the impact it has. We're constantly growing, learning, adapting, and trying new things. BBQ, tacos, and coffee are a few of our favorite things.

More about our benefits

Whether you work in an office or a distributed team, Atlassian is highly collaborative and yes, fun! To support you at work (and play) we offer some fantastic perks: ample time off to relax and recharge, flexible working options, five paid volunteer days a year for your favourite cause, an annual allowance to support your learning & growth, unique ShipIt days, a company paid trip after five years and lots more.

More about Atlassian

Creating software that empowers everyone from small startups to the who’s who of tech is why we’re here. We build tools like Jira, Confluence, Bitbucket, and Trello to help teams across the world become more nimble, creative, and aligned—collaboration is the heart of every product we dream of at Atlassian. From Amsterdam and Austin, to Sydney and San Francisco, we’re looking for people who want to write the future and who believe that we can accomplish so much more together than apart. At Atlassian, we’re committed to an environment where everyone has the autonomy and freedom to thrive, as well as the support of like-minded colleagues who are motivated by a common goal to: Unleash the potential of every team.

Additional Information

We believe that the unique contributions of all Atlassians is the driver of our success. To make sure that our products and culture continue to incorporate everyone's perspectives and experience we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status.

All your information will be kept confidential according to EEO guidelines."
66,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Square Root is built on understanding our customers' data more deeply than they do, and our data engineers are instrumental in that. In this role, you'll take heterogeneous, unstructured data, mine it for insights, and apply it to business problems in innovative ways. While you don't need tons of experience, you'll need to be sharp and possess a genuine interest in using data to solve business problems. You'll be working side by side with experienced cloud architects and data engineers. We want someone that will grow with us and is comfortable with the idea of programming, algorithmic thinking, and automated testing.

Sound like your kind of challenge? Dig in to learn more!
The Gig
You'll design, develop, and improve our ETL Engine to allow for rapid customer implementations, minimal customer investment, data quality and consistency, flexibility, durability, and availability.
You'll design scalable systems that work in concurrency and are fault-tolerant.
As part of our implementation process, you'll work directly with customers to integrate their business data. You'll work closely with different teams at Square Root to deliver enhancements, ensure operational stability, and define + refine product features.
You'll help expand the scalability of our system and maintain our customer relationships as we adapt to more clients and larger data volumes.
We're all about driving action from data, and that includes putting our enterprise data at the fingertips of all Radicals to empower our team.
You'll formulate metrics for business users using math, forecast models, and statistical packages with Python to compute them.
About You
A Master's degree in Computer Science, Computer Engineering, Information Technology, Information Sciences, or a related technical field.
2+ years' experience in a professional environment. You've practically applied engineering principles to the design and development of a data warehouse and scalable ETL pipeline.
2+ years' experience in SQL and Python. You must be able to write joins and use aggregate functions in SQL as well as understand data structures in Python.
1+ years' experience working with visualization technology such as Tableau or Looker.
1+ years' experience in cloud computing, including Amazon S3 and AWS EMR as well as familiarity with Microsoft Azure or Google Cloud Platform.
1+ years' experience with Big Data technologies such as Hadoop, Spark, Hive, and Kafka.
1+ years' experience with Linux and Jenkins or similar technology.
You're familiar with a data lake pattern and understand the process of a slowly changing dimension.
You enjoy digging into a problem and investigating potential causes and solutions. You think creatively and use your reasoning skills to discover connections and potential strategies.
You're a thoughtful communicator able to articulate data insights to non-technical customers.
You're an active listener. Once you understand internal and external customer problems, you're excited to propose and debate solutions which elevate the team or business.
You're calm in a crisis and have examples of navigating difficult situations and relationships.
You keep up with the latest and greatest in technology and bring insights back to the team.
You've demonstrated an eagerness to learn, ability to adapt and perfect your work, and willingness to seek out help and put it to good use.
Our Radical Culture
Our culture is at the core of everything we do. As we grow, we're not only looking to hire the best and brightest, but we're also looking for people that share our values. This is the code we live by:
Think big. Do bigger. Big ideas are meant to be pursued. We have a bias for action, iteration, and impact.
Be Customer Inspired. Our customers' toughest challenges inspire us to build innovative software. We delight them by deeply understanding their business and driving results.
Partner. We're approachable, dependable, and collaborative. We go above and beyond to help our customers, our partners, and one another succeed.
Thrive. We revere personal and professional growth. We recognize individuality, embrace authenticity, and celebrate each other's success.

The Good Stuff
Founded in Austin, Texas, we've been bootstrapped and profitable since our start in 2006. Along the way, we've added a slew of perks inspired by our team of Radicals. Here are some fan favorites:
Flexibility: no dress code, unlimited PTO, and paid parental leave
Competitive benefits and compensation
A cozy campus of 1920's craftsman homes in downtown
Team lunches, stocked kitchen + bar, coffee-snob compliant coffee machines
Radicals get $3,000 a year to learn anything (seriously!). PEOPLE Magazine recognized our Learn Anything program in their 2019 list of ""50 Companies that Care,"" located here (https://square-root.com/2019/07/blog-companies-that-care/).
We're always looking for people to add to and evolve our culture + company. We want you to bring your unique experience and perspective, so don't worry about checking all the boxes. If our values and mission resonate with you, please apply! Additionally, Square Root is an Equal Opportunity Employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender and sexual identity, national origin, disability status, protected veteran status or any other characteristic protected by law."
67,Senior Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,"Five or more years of professional experience as data engineer. Bachelor’s degree in Computer Science or equivalent experience.
Demonstrated experience in data warehousing and ETL development.
Experience building complex data pipelines using large, disparate data sources.
Demonstrated expert knowledge in SQL.
Demonstrated experience working with relational databases such as Oracle, Postgres and other modern database technologies.
Proficiency in modern programming languages such as Python, R, Java.
Thorough understanding of data movement and transformation tools, such as Informatica, Datastage or equivalent.
Demonstrated experience in selecting tools, methods, techniques, and evaluation criteria for designing optimal data engineering solutions.
Demonstrated experience in leading complex technical projects, including assigning tasks and selecting team members.
Ability to make technical presentations to teams, focus groups, management, and governance committees.
Excellent customer service, communication and collaboration skills.
",None Found,"Design, develop, and automate scalable data engineering solutions by leveraging cloud infrastructure. Extend or migrate existing data pipelines to new cloud environment.
Lead technical projects involving design and development of data pipelines for complex datasets. Document project plans, outline tasks and milestones, provide estimation of effort.
Work closely with business partners to devise and manage data pipelines, load frequency, data delivery mechanisms, and performance tuning.
Identify and implement best practices for data engineering and software development to ensure quality delivery of enterprise solutions.
Help enable team alignment by participating in code reviews, change management and team meetings.
Develop and maintain detailed technical documentation of data engineering solutions.
Collaborate with key stakeholders, both internal and external, including enterprise data architect, data modelers, and subject matter experts (SMEs).
",None Found,None Found,"Job Posting Title:
Senior Data Engineer
-
Hiring Department:
IQ - Information Quest
-
Position Open To:
All Applicants
-
Weekly Scheduled Hours:
40
-
FLSA Status:
Exempt
-
Earliest Start Date:
Immediately
-
Position Duration:
Expected to Continue
-
Location:
UT MAIN CAMPUS
-
Job Description:
You will be part of a team building the next generation data warehouse platform and will design, develop, and maintain complex extract, transform, and load (ETL) data pipelines using large heterogeneous datasets. You will also build data engineering solutions for complex data models that express academic and administrative business processes. Your expertise with leading technologies and tools such as Oracle, Postgres, Python, etc. will result in a valuable modern data warehouse that supports critical business decisions and data analysis processes. Your collaboration and communication skills will help to establish stakeholder relationships and ensure that your work products are in alignment with project goals. Most importantly, you will be passionate about working with data and will be a significant contributor to our university mission: To transform lives for the benefit of society.
-
Job Details:
General Notes
Senior level technical role that builds and supports cloud-based Enterprise Data Warehouse ecosystem.
Responsibilities
Design, develop, and automate scalable data engineering solutions by leveraging cloud infrastructure. Extend or migrate existing data pipelines to new cloud environment.
Lead technical projects involving design and development of data pipelines for complex datasets. Document project plans, outline tasks and milestones, provide estimation of effort.
Work closely with business partners to devise and manage data pipelines, load frequency, data delivery mechanisms, and performance tuning.
Identify and implement best practices for data engineering and software development to ensure quality delivery of enterprise solutions.
Help enable team alignment by participating in code reviews, change management and team meetings.
Develop and maintain detailed technical documentation of data engineering solutions.
Collaborate with key stakeholders, both internal and external, including enterprise data architect, data modelers, and subject matter experts (SMEs).
Required Qualifications
Five or more years of professional experience as data engineer. Bachelor’s degree in Computer Science or equivalent experience.
Demonstrated experience in data warehousing and ETL development.
Experience building complex data pipelines using large, disparate data sources.
Demonstrated expert knowledge in SQL.
Demonstrated experience working with relational databases such as Oracle, Postgres and other modern database technologies.
Proficiency in modern programming languages such as Python, R, Java.
Thorough understanding of data movement and transformation tools, such as Informatica, Datastage or equivalent.
Demonstrated experience in selecting tools, methods, techniques, and evaluation criteria for designing optimal data engineering solutions.
Demonstrated experience in leading complex technical projects, including assigning tasks and selecting team members.
Ability to make technical presentations to teams, focus groups, management, and governance committees.
Excellent customer service, communication and collaboration skills.
Preferred Qualifications
Five years or more experience as data engineer designing and implementing complex data pipelines.
Master’s degree in Computer Science, Information Technology or related field.
Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.).
Experience with AWS technologies.
Salary Range
$105,000 + depending on qualifications
Working Conditions
May work around standard office conditions.
Repetitive use of a keyboard at a workstation.
May require occasional off-hours work.
Required Materials
Resume/CV
3 work references with their contact information; at least one reference should be from a supervisor
Letter of interest
Important for applicants who are NOT current university employees or contingent workers: You will be prompted to submit your resume in the first step of the online job application process. Then, any additional Required Materials will be uploaded in the My Experience section; you can multi-select the additional files or click the Upload button for each file. Before submitting your online job application, ensure that ALL Required Materials have been uploaded. Once your job application has been submitted, you cannot make changes.
Important for Current university employees and contingent workers: As a current university employee or contingent worker, you MUST apply within Workday by searching for Find Jobs. Before you apply though, log-in to Workday, navigate to your Worker Profile, click the Career link in the left hand navigation menu and then update the sections in your Professional Profile. This information will be pulled in to your application. The application is one page and you will need to click the Upload button multiple times in order to attach your Resume, References and any additional Required Materials noted above.
-
Employment Eligibility:
Regular staff who have been employed in their current position for the last six continuous months are eligible for openings being recruited for through University-Wide or Open Recruiting, to include both promotional opportunities and lateral transfers. Staff who are promotion/transfer eligible may apply for positions without supervisor approval.
-
Retirement Plan Eligibility:
The retirement plan for this position is Teacher Retirement System of Texas (TRS), subject to the position being at least 20 hours per week and at least 135 days in length.
-
Background Checks:
A criminal history background check will be required for finalist(s) under consideration for this position.
-
Equal Opportunity Employer:
The University of Texas at Austin, as an equal opportunity/affirmative action employer , complies with all applicable federal and state laws regarding nondiscrimination and affirmative action. The University is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, national origin, age, marital status, sex, sexual orientation, gender identity, gender expression, disability, religion, or veteran status in employment, educational programs and activities, and admissions.
-
Pay Transparency:
The University of Texas at Austin will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information.
-
Employment Eligibility Verification:
If hired, you will be required to complete the federal Employment Eligibility Verification I-9 form. You will be required to present acceptable and original documents to prove your identity and authorization to work in the United States. Documents need to be presented no later than the third day of employment. Failure to do so will result in loss of employment at the university.
-
E-Verify:
The University of Texas at Austin use E-Verify to check the work authorization of all new hires effective May 2015. The university’s company ID number for purposes of E-Verify is 854197."
68,Azure Data Architect,"Austin, TX 78727",Austin,TX,78727,None Found,"At least 5 years of consulting or client service delivery experience on Azure
",DevOps on an Azure platform,None Found,None Found," Proven ability to build, manage and foster a team-oriented environment
","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Azure Technical Architect is a highly performant Azure Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data solutions on cloud. Using Azure public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today's corporate and emerging digital applications.

Role & Responsibilities:Work with Sales and Bus Dev teams in providing Azure Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Qualifications
Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 10 years of experience in big data, database and data warehouse architecture and delivery
Minimum of 5 years of professional experience in 2 of the following areas:
§ Solution/technical architecture in the cloud
§ Big Data/analytics/information analysis/database management in the cloud
§ IoT/event-driven/microservices in the cloud
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
 - Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
Strong in Power BI, Java, C##, Spark, PySpark, Unix shell/Perl scripting
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
69,"Senior Software Engineer, Data Science","Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Machine Learning team at RetailMeNot work on initiatives that enable us to provide users meaningful content at the right place, right time, and right price. We build, test and iterate on Machine Learning models, learning from and leveraging datasets with millions of daily visits. We are looking for software engineers who are not afraid of learning new technologies, have an itch or have been playing around with Machine Learning, and are ready to apply it to solve real and meaningful business problems.

As part of the Machine Learning team you will work together with software engineers, data scientist and data engineers leveraging the latest technology and bringing new ideas to the table and building the world’s largest savings destination. Our teams challenge each other to have fun while connecting shoppers to the brands they love. Some of the technologies that we use today include AWS (EMR), Spark, Docker, Luigi, and Tensorflow.

This team is integral to the RetailMeNot business, so we need engineers who can deliver results while understanding the structure of a large system. We provide cross-team leadership that ensures that RetailMeNot code meets a consistently high standard while building the platform to support the future of the company. Your daily activities will involve oversight, mentoring, delivering key pieces of functionality, and collaborating with technology leadership to plan the technical roadmap for RMN.

We are constantly evolving both the software and the teams that deliver it. If you’re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, then we want you to be a part of the team.
Who You Are
You have a Bachelor's degree in computer science or equivalent STEM field, or equivalent work experience
You have an ownership mentality and track record of successful high-quality results. You identify any ambiguous requirements and provide clarity when needed.
You bring a proven understanding and application of computer science fundamentals: data structures, algorithms and design patterns.
You’re proficient with technologies such as Java, AWS.
You have an understanding of systems architecture technologies including Linux, Amazon Web Services, Kubernetes and Docker.
You have high level understanding and interest in Machine Learning, and want to work together with scientist and data engineers to apply it to real life.
You have experience with common software engineering tools such as Git, JIRA, TeamCity, Confluence or similar.
You have 5+ years of application development experience.
What You'll Do
You will work together with data scientist and data engineer to deliver improved algorithms and experiences using Machine Learning.
You will understand and constantly improve the cloud infrastructure that runs our high performance, consumer-scale site.
You will help support the build and deployment pipeline and when necessary both diagnose and solve production support issues.
You will contribute to the overall system design, architecture, security, scalability, reliability, and performance of applications.
You’ll consistently improve maintainability and stability of the codebase.
You’ll coach and mentor junior engineers on software engineering techniques, process, and new technologies.
You will also have the opportunity to stay ahead of new technologies with an eye to evaluating and potentially incorporating them into your team's architecture.
Who We Are
We hire smart people and give them the autonomy to be creative in how they impact the business.
We embrace Diversity and Inclusion as core values and have a thriving program to support it throughout the company. We give back to the community and support multiple Austin events and organizations with like-minded goals.
We have an extraordinary open vacation policy.
We offer a generous leave policy for new parents as well as 401k matching.
We provide lunch four days a week, breakfast twice a week, all the snacks you could dream of, and have our own coffee bar run by trained baristas.
We reimburse expenses for cell phone service and gym memberships.

Rewards*
We offer an opportunity to be an integral part of a company that eagerly pursues disruption in its space to continue to drive innovation and lead the competition. Benefits of being an employee of RetailMeNot, Inc. include, but are not limited to the following:
Competitive base & bonus packages; salary negotiableLong Term Incentive PlanPerformance based rewards & recognition for your hard work and serviceVery competitive benefits packages, including best-in-class parental leaveOpen & flexible PTOCell phone & gym membership reimbursementsFully stocked break room & onsite catered breakfasts & lunches multiple days/week
Some rewards do not apply to contract workers or interns.

About Us
RetailMeNot, Inc. is a leading savings destination bringing people and the things they love together through savings with retailers, brands, restaurants and pharmacies. RetailMeNot makes everyday life more affordable through online and in-store coupon codes, cash back offers, discount gift cards, and the RetailMeNot Genie browser extension. Savings are also provided in consumers’ mailboxes through the RetailMeNot Everyday™ direct mail package, and at the pharmacy with RxSaver by RetailMeNot.

RetailMeNot is a wholly owned subsidiary of Harland Clarke Holdings. http://www.retailmenot.com/corp or follow @RetailMeNot on social media.

U.S. Equal Employment Opportunity/Affirmative Action Information
Individuals seeking employment at RetailMeNot, Inc. are considered without regards to race, color, creed, religion, gender, gender identity, national origin, citizenship, age, sex, marital status, ancestry, physical or mental disability, veteran status, sexual orientation, or any other protected classification. You are being given the opportunity to provide the following information in order to help us align with federal and state Equal Employment Opportunity/Affirmative Action record keeping, reporting, and other legal requirements."
70,Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Engineer will be part of the Capture & Analytics Data Services team, which provides the data sources and analytical tools used to help our clients derive maximum value from our Behavioral Analytics and Routing solutions. This position will support the day to day reporting needs of numerous clients, including Business Monitoring, Insights, Product Development, Analysis & Testing, Telephony Integration and others. Responsibilities include troubleshooting issues that occur with existing report deliverables as well as developing new reports and integrating them into the overall service catalog. This role will require development, testing, and configuration management of deliverables in coordination with Data Engineers and Testers within the organization, as well as external vendor and customer resources. Additionally, the Data Engineer will be responsible for maintaining any documentation and training materials required to support the various business units the group serves.
This individual will also support the Data Warehouse to troubleshoot issues relating to the warehouse, especially as they impact the reporting environment.

To summarize, the Data Engineer will be responsible for, but not limited to, the following tasks:

1. Troubleshoot and resolve issues as they arise related to all Data Services
2. Help to improve automation within the data warehouse and customer environments
2. Collaborate with vendor and customer resources to build complex data feeds for reportings
3. Manage iteration and release cycles and deployments
4. Assist in data modeling and design sessions
5. Proactively maintain documentation and training materials

Because of the data-centric culture and rapid growth of NICE-Mattersight, a rich career path exists for the Data Engineer within Mattersight.

What you definitely have:
2–5 years report development experience, specifically with Business Intelligence tools (Tableau preferred)Solid experience with RDBMS applications (SQL Server preferred)Good communication skills and experience working with cross-functional teamsStrong understanding of data warehouse design and implementation best practicesAbility to explain principles of data visualizationSQL programming familiarity in large RDBMS systems (T-SQL preferred)Exposure to ETL and data integration processes
What we’d also love to see:
Project experience preparing use cases and user requirementsDesign and development of micro-services-oriented ArchitectureExposure to reporting interfaces that integrate with popular telephony platforms
Be You

We are all different and that is powerful. Variability fuels our business and unites our work. It teaches us that strength lies in differences. To see what matters is our culture, and our culture starts with you.

Your different perspectives inspire us to be better. Your diversity fosters creativity and accelerates our innovation. Your unique skills and abilities make us stand out. Your background and experiences help us reach our full potential.

We are committed to a workplace that is increasingly diverse and inclusive, so be your best you.

NICE Systems is an Equal Opportunity/Affirmative Action Employer, M/F/D/V."
71,Database Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer.
Teradata, Hadoop, SQL and Data Analysis, Python R Spark Scala,
Debugging, Azure Cloud

Qualifications

B.Tech

Primary Location: US-TX-Austin
Organization: UST USA
Employment Type: Regular Employee
Job Type: Full-time
 Day Job
Job Posting: Sep 27, 2019, 11:01:07 AM


 UST-Global is an Equal Opportunity Employer"
72,Senior Data Engineer,"Austin, TX",Austin,TX,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Halfaker and Associates, LLC, an award winning high growth small business, creates innovative and customer-centric technology solutions in the areas of Cyber Security, Data Analytics, Software Engineering and IT Infrastructure to improve the health, security and well-being of all Americans. Our commitment to excellence and our vision to “Continue to Serve” has resulted in steady growth and an expanding client base across government agencies in the health, defense, security and intelligence sectors. Headquartered in Arlington, VA, we have employees nationwide and were recently named a 2018 Top Work Place by the Washington Post. Please take a moment to browse through our website and learn more about what it means to serve with Halfaker.


Halfaker has an opening for a Senior Data Engineer to join our talented, dynamic team. The key responsibilities for this position are:

Participate in technical research and development to enable continuous innovation within the infrastructure
Resolve a wide range of hardware and network technology issues in a fast-paced environment
First escalation resource for Tier-3 troubleshooting for all assigned service desk tickets from creation to resolution
Ensure that system hardware, operating systems, software systems, and related procedures adhere to organizational SLAs
Troubleshooting ranges from basic issues to email related issues for Executive level users
Provisioning, installation/configuration, operation, and maintenance of systems hardware and network infrastructure
Maintain standard image for Windows platforms
Other tasks and projects as assigned


Required Skills
Hands on Experience with the Informatica PowerCenter v10x Software Installation and Configuration
Knowledge and/or Experience with administration of Informatica Enterprise Data Quality,
Experience with administration of Informatica DQ and PC modules.
Experience with administration of MDM tool
Those without Informatica experience but with experience in Jenkins, Puppet, Ansible or Terraform will also be considered.
PowerShell scripting is highly desirable
Excellent customer service, communication and organizational skills are required
Excellent troubleshooting and problem resolution skills for Desktops and Laptops
Ability to work independently with minimal direction providing technical and non-technical support to multiple users
Capable of working under pressure, while handling multiple tasks simultaneously
Experience providing services to the federal government is preferred
Travel may be required
Ability to work overtime and weekends required on occasion
Ability to sit in an office environment for long periods of time
Remote and on-call responsibilities may be required



Required Experience
Bachelor's degree in computer science, systems engineering, or related technical discipline (8 years of additional relevant experience may be substituted for education)
5+ years of relevant experience
Experience providing IT support to multiple work sites
MS Active Directory administration (Users, Groups, GPO, Security, etc.)


Halfaker and Associates, LLC, is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/ Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. U.S. Citizenship is required for most positions."
