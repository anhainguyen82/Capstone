{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=Dallas%2C+TX&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Microsoft Power BI Developer</td>\n",
       "      <td>Dallas, TX 75202</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75202</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description:\\nApplies specialized knowledge in Microsoft Power BI, Flow and Azure Analysis Services to conceptualize, design, develop, unit-test, configure, and implement portions of new or enhanced (upgrades or conversions) business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies.\\nResponsibilities:\\nCore team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives\\nDevelop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams\\nDocument requirements and translate into proper system requirements specifications using high-maturity methods, processes and tools.\\nDevelop visualization, user experience and configuration elements of solution design.\\nExecute and coordinate requirements management and change management processes. Participates as a member of and leads development teams.\\nDesigns units for others.\\nCompletes development to implement complex components.\\nDesigns solutions for others to develop.\\nParticipates in cross-functional teams.\\nLeads design activities and provides mentoring and guidance to developers.\\nDesigns, prepares and executes unit tests.\\nRepresents team to clients.\\nDemonstrates technical leadership and exerts influence outside of immediate team.\\nDevelops innovative team solutions to complex problems.\\nContributes to strategic direction for teams.\\nApplies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g. Power BI and Power App development).\\nIntegrates technical expertise and business understanding to create superior solutions for clients.\\nConsults with team members and other organizations, clients and vendors on complex issues.\\nEducation and Experience Required:\\nTypically, a technical bachelor’s degree or equivalent experience and a minimum of 8 years of related experience or a master’s degree and a minimum of 6 years of experience.\\nKnowledge and Skills:\\n8 or more years’ experience writing code using languages such as (and not limited to) Power BI, Tableau, QlikView, Java, C, C++, C#, VB.Net.\\nSignificant hands on experience in creating and deploying KPI, analytic, and dashboard solutions including design, development, deployment, data engineering, data warehousing, and data management projects and practices.\\nAdvanced understanding of RDBMS databases such SQL Server and Oracle.\\nAdvanced understanding of modern software design and development methodologies.\\nExperience on multiple full release project life cycles.\\nAdvanced understanding of modern SCM (software configuration management).\\nAdvanced understanding of testing tools and unit test and integration test scripting, and testing methodologies\\nAdvanced experience using an Integrated Development Environment (e.g., Eclipse, Visual Studio) and development of tool add-ins.\\nStrong understanding of basic Database Administration. Able to define quality and security standards.\\nGood verbal and written communication and negotiation skills.\\nGeneral project management/team leader skills.\\nAbility to work effectively in a globally dispersed team and with clients and vendors.\\nDemonstrated technical leadership skills.\\nBusiness skills - 6+ years of technology services industry and business operations experience in a technology services organization\\nData engineering, analytics and systems subject matter expert on financial, workforce and operational systems for technology services business desired.\\nExperience in multiple solution development methodologies and participation in a fast paced, Dev/Ops environment\\nDrives the construction of highly innovative statistical and financial models to analyze new aspects of business performance.\\nEstablishes the metrics required to measure business performance and recommends the go-forward strategy to address performance gaps.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Strategy Specialist - Business &amp; Data Analysis, Cloud, AWS, Azure, Big Data</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe North America Data Strategy &amp; Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.\\n\\n Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model\\n\\nRequired Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:\\no Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling\\no Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.\\no Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel\\n Able to travel up to 100% (Mon-Thu)\\n\\nOptional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model\\n Experience in compiling business cases and roadmaps for data, analytics and technology investments\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Allen, TX</td>\n",
       "      <td>Allen</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5 years of working with following technology stack: Core languages are Java and C#; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, and AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, and SQS.\\nGrowing track record of success or the groundwork to be an impactful member of the team. We’re looking for candidates that exhibit many of the following skills/attributes:\\nStrong Educational Background\\nHands-on Engineering experience in\\nProblem solving and debugging skillsWriting and deploying code the Linux, Windows, or cloud environmentsFamiliarity with algorithms and performance analysisWillingness to contribute to the operational responsibility of the team’s applications\\nSome experience with one or more of the following:\\nRelational Databases &amp; SQL NoSQL databases (Cassandra, Redis, DynamoDB, MongoDB)Big Data tools such as Hadoop, Hive, EMR, Storm, Spark, DynamoDB, HBase</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The engineering team consists of talented, team-oriented individuals who are empowered to take advantage of the latest cloud and distributed technologies to deliver reliable, high-throughput applications.\\n\\nAs a Data Engineer, you’ll employ your skills on a daily basis to design and build data processing and storage applications to handle millions of transactions per day. You will analyze business requirements and consult with the broader team to ensure successful processing, storage and reporting of our Big Data. You’ll have a wide variety of languages and technologies at your disposal that you can use to solve problems. Your work will directly shape and create our data architecture to ultimately deliver systems that stand up to unpredictable environments at massive scale.\\n\\nTechnical Skills Needed:\\n5 years of working with following technology stack: Core languages are Java and C#; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, and AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, and SQS.\\nGrowing track record of success or the groundwork to be an impactful member of the team. We’re looking for candidates that exhibit many of the following skills/attributes:\\nStrong Educational Background\\nHands-on Engineering experience in\\nProblem solving and debugging skillsWriting and deploying code the Linux, Windows, or cloud environmentsFamiliarity with algorithms and performance analysisWillingness to contribute to the operational responsibility of the team’s applications\\nSome experience with one or more of the following:\\nRelational Databases &amp; SQL NoSQL databases (Cassandra, Redis, DynamoDB, MongoDB)Big Data tools such as Hadoop, Hive, EMR, Storm, Spark, DynamoDB, HBase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer-Senior Advisor</td>\n",
       "      <td>Richardson, TX</td>\n",
       "      <td>Richardson</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.7+ years of SQL development skills within a multi-tier environment are required.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Data Analyst, Senior Advisor will work with data stewards, data owners, master data management analysts, operations teams and IT partners across the organization to drive the enterprise data conformance program. Data research and analysis, cross-functional requirements gathering and documentation and solution development are key aspects of the role. The Data Analyst, Senior Advisor supports reactive data quality by researching and fixing known data issues and proactive data quality by defining the requirements for data controls and enhancements to data capture, monitoring and maintenance processes, procedures and standards.\\n\\nPrimary responsibilities include performing data operations such as conversion, address hygiene, and postal presort, as well as variable document composition and file creation.\\n\\nEssential Functions:\\n\\nData Analysis\\nIdependently design, develop and implement data integration solutions that support our platforms resiliency, stability, and supportability using a variety of ETL and database technologies.Rapidly develop and refine data integration solutions using Infosphere Datastage, SQL, FastTrack, or other technologies.Experience integrating large structured and unstructured data in multiple format, character sets and delivery methodsWorks with business sponsors, SMES and application teams; to understand the business requirements; analyze and assess availability, quality, and lineage of source system data.Design, map data from source to target and develop data integration solutions that meet business needs.Develop and socialize data integration standards.Partners with other engineers through design reviews, providing feedback on feasibility, scalability, performance and adherence to standards.Partners with business, analysts, BI team, application teams and other stakeholders to design, map data from source to target, develop, test, and implement production data integration solutions that are fully integrated into the Enterprise Data Warehouse.Ensuring model design solves the end users need.Contribute information to the data governance software to improve knowledge downstream.\\nData Conformance\\nPerforms analysis on known data quality issues and develops and/or recommends operational or technical solutions for remediation, including development and implementation of automated data quality controls that proactively trigger notifications to process owners when data is out of range. Keeps stakeholders informed of progress and solutions in a timely manner.Autonomously, and proactively performs data profiling to explore data, identify issues and summarize findings.Defines data quality metrics to assess completeness, accuracy, consistency, and conformance to business rules. Designs dashboards to support continuous monitoring and measurement of data quality.Partners with IT to cleanse data to achieve the desired level of data qualityPartners with data stakeholders, process and product owners across the organization to define data standards and communicate changes to data capture procedures, processes, standards, and controls.Cleanses and prepares datasets to be consumed by data scientists and other analystsCollaborates with external teams working on data integration and engineeringAdvocates data governance and hygiene best practicesAssists with scoping and integrating orphan datasets\\nYou will gain hands on experience implementing through embedding standardized data elements in the database or system, employing standardized data elements in an exchange mechanism (usually XML schema), or mapping the application data elements to the standardized elements for purposes of exchange.\\n\\nBig Data tools:\\n Big Data: Hadoop, PIG, Sqoop, Hive and Hcatalog &amp; NoSQL (HBase, Cassandra) , SQL.\\n Programming: Scala, Java, Python, Spark\\n\\nRequired Qualifications\\n7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.7+ years of SQL development skills within a multi-tier environment are required.\\n\\nPreferred Qualifications\\nIn depth understanding of data integration best practices, leading industry applications and features such as master data management, entity resolution, data quality assessment, metadata management, etc.Expertise in flat file formats, XML within PL/SQL and file format conversion.Exposure to application security technologies and approaches is preferred.Experience processing and parsing CSV, JSON and XML file formatsDatastage, SQL, FastTrack, or other technologiesStrong analytical, debugging and testing skillsSoftware development experience using scripting languages such as JavaScript, Python or RubyProficient using Infosphere/DataStage or equivalent ETL software.Proficient with relational databases and using SQL to query, create tables, views, indexes, joins.Proficient using Unix and applicable scripting/scheduling tools.Experience with Python for data analysis\\n• Knowledge of clinical and financial Healthcare data • • Knowledge of all data formats (HL7, EDI, CSV, XML, etc)\\n\\nEducation\\nBachelor's Degree in Computer Science, Computer Engineering, Computer Information Systems, Information Systems, Management Information Systems or related engineering discipline.\\n\\nEquivalent work experience will be considered in lieu of degree.\\n\\nBusiness Overview\\nIt’s a new day in health care.\\n\\nCombining CVS Health and Aetna was a transformative moment for our company and our industry, establishing CVS Health as the nation’s premier health innovation company. Through our health services, insurance plans and community pharmacists, we’re pioneering a bold new approach to total health. As a CVS Health colleague, you’ll be at the center of it all.\\n\\nWe offer a diverse work experience that empowers colleagues for career success. In addition to skill and experience, we also seek to attract and retain colleagues whose beliefs and behaviors are in alignment with our core values of collaboration, innovation, caring, integrity and accountability.\\n\\nCVS Health is an equal opportunity/affirmative action employer. Gender/Ethnicity/Disability/Protected Veteran – we highly value and are committed to all forms of diversity in the workplace. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities. We comply with the laws and regulations set forth in the following EEO is the Law Poster: EEO IS THE LAW and EEO IS THE LAW SUPPLEMENT. We provide reasonable accommodations to qualified individuals with disabilities. If you require assistance to apply for this job, please contact our Advice and Counsel Reasonable Accommodations team. Please note that we only accept applications for employment via this site.\\n\\nIf technical issues are preventing you from applying to a position, contact Kenexa Helpdesk at 1-855-338-5609 or cvshealthsupport@us.ibm.com. For technical issues with the Virtual Job Tryout assessment, contact the Shaker Help Desk at 1-877-987-5352.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Microsoft SQL Database Developer</td>\n",
       "      <td>Dallas, TX 75202</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75202</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description:\\nApplies specialized knowledge in Microsoft SQL and Azure technologies to conceptualize, design, develop, unit-test, configure, and implement business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies. Analyze customer information requirements and product specifications to define technical content strategy and plan. Designs and develops written and/or visual product-related information – hard copy, web - (e.g., user/configuration/troubleshooting guides), and online information (interactive demos, help systems) integrated into product, for a variety of audiences (end user, system administrators, internal support engineers, product developers, training developers). Codes, builds, compiles, and tests online information and/or sets-up, loads and tests systems hardware to create information deliverables and provide feedback on ease of use and user interfaces to product development. As customer advocate, helps define/refine product requirements. Develops standards and style documents and templates, scripts, style sheets, and script and graphic libraries to ensure common look and feel. Interfaces with cross-functional areas as a member of the product development team, such as marketing, test, support, and manufacturing.\\nCore team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives. Design, develop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams. Works with peers outside immediate organization to define and characterize complex technology or process problems and/or develops new solutions, yet works independently to drive technical problems to a solution. Provides advanced technical consulting and advice to proposal efforts, solution design.\\nProduces strategies which assist company in becoming No. 1 in the market place. Actively participates in company professions program and Practice Improvement activities. Role models knowledge sharing and re-use within practice or profession. Proactively encourages Leads technically significant work on enterprise scale projects. Is recognized by peers as an expert in a particular area of technology. Sustain Architect custom solutions of project and program or operational scope. Architect reusable solutions of project or operational scope. Build custom reusable solutions of project and program or operational scope. Capture and share architectural IP at the project and program level. Assess business impact of specific technologies/strategies Identify and address technical or operational risks. Provide review/input on project activities for medium to large business unit level projects Collaborates with the project manager to develop detailed project plans and work breakdown structures for medium to large business unit level projects.\\nEducation and Experience Required:\\nTypically, a technical bachelor’s degree in information systems, computer science, or related field and a minimum of 8 years of related experience or a master’s degree and a minimum of 6 years of experience.\\nKnowledge and Skills:\\nExpert in Microsoft SQL BI solutions, architecture, design, development, data engineering, data warehousing, and data management. Extensive experience in Kimball method of data warehousing, data models, dimensional maintenance, and TSQL development in high-performance environments required. Experience in SSIS – Particularly using C# scripting object, C#, .Net with MVC, ClickOnce, and TFS or SVN experience\\nExpert in Microsoft SQL reporting solutions and development experience in T-SQL, stored procedures, complex reporting query development, query optimization, and DevOps\\n6+ years of technology services and business operations experience in a technology services organization\\nData engineering, analytics and systems subject matter expert on financial, workforce and operational systems for technology services business models\\nExperience in multiple solution development methodologies and participation in a fast paced, Dev/Ops environment\\nExperience in scorecard, dashboard and business analytics solutions for executive information reporting, analysis and decision support\\nExcellent analytical thinking, technical analysis, and data manipulation skills.\\nAbility to leverage new analytical techniques to develop creative approaches and insights.\\nCan validate/evaluate if an information systems or operational architecture meets business needs.\\nBe able to evaluate the effect of external factors on designed solution.\\nData, analytics and systems subject matter expert on leading financial, workforce and services solutions\\nAdvanced understanding of modern software design and development methodologies. Experience on multiple major full release project life cycles. Advanced understanding of modern SCM (software configuration management). Advanced understanding of testing tools and unit test and integration test scripting, and testing methodologies. Advanced experience using an Integrated Development Environment (e.g., Eclipse, Visual Studio) and development of tool add-ins.\\nExpert in defining quality and security standards.\\nAbility to work effectively in a globally dispersed team. High level of technical leadership skills.\\nStrong verbal and written communication and negotiation skills. Strong project management/team leader skills. Excellent oral and written communication skills for interacting with stakeholders. Strong presentation skills.\\nExpert analytical skills and the ability to synthesize change quickly using advanced subject and process knowledge. Excellent time management skills and ability to prioritize\\nAbility to generate original ideas and to bring about their implementation.\\nKnowledge of and ability to utilize a variety of tools and technologies to support multiple technologies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Azure Data Architect</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of consulting or client service delivery experience on Azure\\n</td>\n",
       "      <td>DevOps on an Azure platform</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\n</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Azure Technical Architect is a highly performant Azure Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data solutions on cloud. Using Azure public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today's corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:Work with Sales and Bus Dev teams in providing Azure Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS &amp; NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nQualifications\\nBasic Qualifications\\nAt least 5 years of consulting or client service delivery experience on Azure\\nAt least 10 years of experience in big data, database and data warehouse architecture and delivery\\nMinimum of 5 years of professional experience in 2 of the following areas:\\n§ Solution/technical architecture in the cloud\\n§ Big Data/analytics/information analysis/database management in the cloud\\n§ IoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.\\n Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.\\n - Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nMCSA Cloud Platform (Azure) Training &amp; Certification\\nMCSE Cloud Platform &amp; Infratsructiure Training &amp; Certification\\nMCSD Azure Solutions Architect Training &amp; Certification\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nStrong in Power BI, Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience a plus - Azure, AWS, Google\\n\\nProfessional Skill Requirements\\n Proven ability to build, manage and foster a team-oriented environment\\n Proven ability to work creatively and analytically in a problem-solving environment\\n Desire to work in an information systems environment\\n Excellent communication (written and oral) and interpersonal skills\\n Excellent leadership and management skills\\n Excellent organizational, multi-tasking, and time-management skills\\n Proven ability to work independently\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5-8 years of Python or Java/J2EE development experience\\n3+ years of demonstrated technical proficiency with Hadoop and big data projects\\n5-8 years of demonstrated experience and success in data modeling\\nFluent in writing shell scripts [bash, korn]\\nWriting high-performance, reliable and maintainable code\\nAbility to write MapReduce jobs\\nKnowledge of database structures, theories, principles, and practices\\nUnderstand how to develop code in an environment secured using a local KDC and OpenLDAP\\nFamiliarity with and implementation knowledge of loading data using Sqoop\\nKnowledge and ability to implement workflow/schedulers within Oozie\\nExperience working with AWS components [EC2, S3, SNS, SQS]\\nAnalytical and problem-solving skills, applied to Big Data domain\\nProven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark\\nAptitude in multi-threading and concurrency concepts\\nM.S. in Computer Science or Engineering</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nTranslate complex functional and technical requirements into detailed design\\nHadoop technical development and implementation\\nLoading from disparate data sets by leveraging various big data technology e.g. Kafka\\nPre-processing using Hive, Impala, Spark, and Pig\\nDesign and implement data modeling\\nMaintain security and data privacy in an environment secured using Kerberos and LDAP\\nHigh-speed querying using in-memory technologies such as Spark\\nFollowing and contributing best engineering practice for source control, release management, deployment etc\\nProduction support, job scheduling/monitoring, ETL data quality, data freshness reporting</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Beyondsoft Consulting, Inc., is a leading, technical solutions and consulting partner. We combine emerging technologies and proven methodologies to tailor elegant solutions that solve complex challenges and empower our customers to accelerate their business goals. Our services include end-to-end support for cloud, digital, data analytics, multi-language translation, and testing.\\n\\nOur client is growing their Data Engineering team within a demanding and well recognized enterprise and information technology company. This role will be the core solution of the Strategic Analytics organization, ensuring both the reliability and applicability of the team’s data products to the organization. This individual will have extensive experience with ETL design, coding, and testing patterns as well as engineering software platforms and large-scale data infrastructures. The Data Engineers will have the capability to architect highly scalable end-to-end pipeline using different open source tools, including building and operationalizing high-performance algorithms. Proven experience with technologies to solve big data problems with expert knowledge in programming languages like Java, Python, Linux, PHP, Hive, Impala, and Spark.\\nResponsibilities\\nResponsibilities:\\n\\nTranslate complex functional and technical requirements into detailed design\\nHadoop technical development and implementation\\nLoading from disparate data sets by leveraging various big data technology e.g. Kafka\\nPre-processing using Hive, Impala, Spark, and Pig\\nDesign and implement data modeling\\nMaintain security and data privacy in an environment secured using Kerberos and LDAP\\nHigh-speed querying using in-memory technologies such as Spark\\nFollowing and contributing best engineering practice for source control, release management, deployment etc\\nProduction support, job scheduling/monitoring, ETL data quality, data freshness reporting\\nQualifications\\nQualifications:\\n\\n5-8 years of Python or Java/J2EE development experience\\n3+ years of demonstrated technical proficiency with Hadoop and big data projects\\n5-8 years of demonstrated experience and success in data modeling\\nFluent in writing shell scripts [bash, korn]\\nWriting high-performance, reliable and maintainable code\\nAbility to write MapReduce jobs\\nKnowledge of database structures, theories, principles, and practices\\nUnderstand how to develop code in an environment secured using a local KDC and OpenLDAP\\nFamiliarity with and implementation knowledge of loading data using Sqoop\\nKnowledge and ability to implement workflow/schedulers within Oozie\\nExperience working with AWS components [EC2, S3, SNS, SQS]\\nAnalytical and problem-solving skills, applied to Big Data domain\\nProven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark\\nAptitude in multi-threading and concurrency concepts\\nM.S. in Computer Science or Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Big Data Engineer (Kafka)</td>\n",
       "      <td>Allen, TX 75013</td>\n",
       "      <td>Allen</td>\n",
       "      <td>TX</td>\n",
       "      <td>75013</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Experian is seeking a Big Data Engineer to join our CIS Product Delivery team. This is a great opportunity for someone who specializes in Kafka Architecture to enable cloud-based financial services platform to access timely, accurate and relevant data. An ideal candidate will have built real time software services platforms where large volume messaging is core to the solution set.\\n\\nAbout Experian\\n\\nExperian is the world’s leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses and society. For five years in a row, we have been named in the Top 100 “World’s Most Innovative Companies” by Forbes Magazine. With a focus on our employees, we were rated the #1 Top Workplace by the Orange County Register. Experian Consumer Information Services is redefining the way our clients do business within all aspects of the customer credit lifecycle. Fueled by best-in-class data and innovative technology we help businesses make smarter decisions, identify consumers, make decisions on loans, market to prospects and collect.\\n\\nAbout this role\\n\\nAs a Big Data Engineer, you must be a messaging expert with extensive, well-rounded background in a diverse set of messaging middleware solutions (commercial, open source, in-house) with in-depth understanding of architectures of such solutions such Kafka. You’ll need an established track record with Kafka technology (administration, configuration, and troubleshooting), with hands-on production experience and a deep understanding of the Kafka architecture and internals of how it works, along with interplay of architectural components such as Kafka Connect, Kafka Streams. Additional responsibilities for this position include:\\nDesign, develop and implement the Kafka ecosystem by creating a framework for leveraging technologies such as Kafka Connect, KStreams/KSQL, Attunity, Schema Registry, and other streaming-oriented technology\\nAssist in building out the DevOps strategy for hosting and managing our SDP microservice and connector infrastructure in AWS cloud\\nStrong track record of design/implementing big data technologies around Apache Hadoop, Kafka streaming, No SQL, Java/J2EE and distributed computing platforms in large enterprises where scale and complexity have been tackled.\\nProven experience participating in agile development projects for enterprise-level systems component design and implementation\\nDeep understanding and application of enterprise software design for implementation of data services and middleware.\\n\\nWhat your background looks like\\n\\n5+ years of experience in relevant Streaming/Queueing implementation roles\\nBachelor degree in Technical discipline; Masters preferred\\nExperience in monitoring the health of Kafka cluster (data loss and data lagging) and strategy for short TTD (time to detect) of broker failure and fast TTR (time to recover)\\nStrong coder who can implement Kafka producers and consumers in various programming languages following the common patterns and best practices\\nExperience in various integration with Kakfa such as Elastic Search, Databases (RDBMS or NoSQL)\\nExperience in Spark stream processing is a plus\\nExperience in RDBMS change log streaming is a plus\\nSystems integration experience, including design and development of APIs, Adapters, and Connectors and Integration with Hadoop/HDFS, Real-Time Systems, Data Warehouses, and Analytics solutions.\\nFinancial Industry experience preferred\\n\\nPerks\\n\\nYou’ll be working in a big data analytic incubator with talented, passionate individuals who are focused on architecting world class cutting edge analytics solutions\\nFour weeks of vacation to start, five sick days and two volunteer days (plus eleven paid holidays)\\nThis is a bonus eligible position with a 15% bonus target\\nEmployee stock purchase program and 401K matching\\nWellness initiatives, online discounts, employee discounts, pet insurance and more\\n\\nEOE including disability/veterans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tax Services Senior – National Tax – Tax Technology and Transformation (TTT) – Data Scientist – Advanced Technologies - Dallas</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Strong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.\\nExperience with common data science programming languages, such as Python R\\nStrong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential\\nExperience with data visualization tools, such as PowerBI, D3.js, etc.\\nExperience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB\\nStrong scripting and programming skills\\nOwnership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.\\nAbility to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies\\nAbility to consolidate tax data to make analysis and planning more efficient\\nFocus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities\\nWillingness to support project team members in any way needed to help ensure timely completion of deliverables</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Tax Technology and Transformation offers services to companies in response to the impact of existing and emerging technology, including the growing data burden that many businesses face, driving efficiencies to create a cost-effective tax function and the need to understand how to make data an asset. The underlying objective of the combined offerings is to help businesses navigate the digital age of tax transparency alongside new trends in tax compliance and tax audit methods, as well as helping to solve the most pressing challenges that businesses face. Tax Technology and Transformation is composed of the following services:\\nDigital tax transformation\\nTax applications-as-a-service\\nTax data and improvement\\nTax analytics and reporting enhancement\\nEmerging tax technology, including robotic process automation (RPA), artificial intelligence (AI), blockchain, cloud solutions, data lake development and business intelligence innovation\\nTax technology program mobilization\\nCustom tax technology application development and deployment\\nTax technology strategy and road mapping\\nDirect and indirect tax systems implementation and configuration\\nPost-transaction (M&amp;A) tax function operational services\\nTax operating model transformation, including process improvement, risk and controls\\nTax function assessments\\nThe opportunity\\n\\nTax Technology and Transformation is an area that has seen significant growth and investment recently, and you will see that reflected in your experience. It is no exaggeration to say that you will be working on highly publicized projects. The field of taxation is constantly changing as new laws, regulations, and technologies are created, and this is your opportunity to be part of that development.\\n\\nKey responsibilities\\n\\nWe are looking for an ambitious, self-motivated data scientist or data engineer who will help us discover the information hidden in vast amounts of data, and help us deliver even better products to our clients. Your primary focus will be in applying data mining techniques, doing statistical analysis and building high quantity prediction systems integrated with our products. You will be expected to team on a national and even global scale, so strong communication skills, attention to detail, and ability to effectively drive results are essential.\\nSelecting features and, building and optimizing classifiers using machine learning techniques\\nData mining using state-of-the-art methods\\nEnhancing data collection procedures to include information that is relevant for building analytic systems\\nProcessing, cleansing and verifying the integrity of data used for analysis\\nDoing ad-hoc analysis and presenting results in a clear manner\\n\\nDepending on your unique skills and ambitions, you could be supporting various client projects, ranging from assisting in the production of leading edge machine models, to designing and implementing robust data pipelines that can handle data at a multinational, enterprise scale. Whatever you find yourself doing, you will contribute and help toward developing a highly trained team, all the while handling activities with a focus on quality and commercial value. This is a highly regulated industry, so it is all about maintaining our reputation as trusted advisors by taking on bold initiatives and owning new challenges.\\n\\nSkills and attributes for success\\nStrong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.\\nExperience with common data science programming languages, such as Python R\\nStrong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential\\nExperience with data visualization tools, such as PowerBI, D3.js, etc.\\nExperience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB\\nStrong scripting and programming skills\\nOwnership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.\\nAbility to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies\\nAbility to consolidate tax data to make analysis and planning more efficient\\nFocus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities\\nWillingness to support project team members in any way needed to help ensure timely completion of deliverables\\nTo qualify for the role, you must have\\nA bachelor’s degree in information system, tax technology, management information systems or computer science or related field and a minimum of two years of related work experience\\nA passionate interest in data science and its role in the organization\\nExcellent communication and business writing skills\\nA natural flair for problem solving and an entrepreneurial approach to work\\nStrong organizational and time management skills, with exceptional client-serving consulting skills\\nDemonstrated ability to capture and synthesize business requirements\\nDesire and demonstrated ability to provide leadership within a team\\nIdeally, you’ll also have\\nExperience with Apache Spark\\nExperience with Hadoop and/or distributed database systems\\nExperience working in the Microsoft Azure Cloud environment\\nExperience developing ETL solutions using SSIS or other tools\\nERP experience, including SAP and/or Oracle-preferred but not required\\nPractical experience or strong theoretical understanding of neural networks\\nWhat we look for\\n\\nWe are looking for knowledgeable data science professionals with a passion for turning data into actionable insight. You will need strong business acumen and a firm strategic vision, so if you are ready to use those skills to develop your team, this role is for you.\\n\\nWhat working at EY offers\\n\\nWe offer a competitive compensation package where you will be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, a minimum of three weeks of vacation plus ten observed holidays and three paid personal days; and a range of programs and benefits designed to support your physical, financial and social well-being. We also offer:\\nSupport and coaching from some of the most engaging colleagues in the industry\\nOpportunities to develop new skills and progress your career\\nThe freedom and flexibility to handle your role in a way that is right for you\\nAbout EY\\n\\nAs a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.\\nEY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\\n\\nIf you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible.\\n\\nMake your mark. Apply today.\\n\\n.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Reliability Engineer</td>\n",
       "      <td>Dallas, TX 75240</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75240</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Hotels.com\\nJob Description\\nAre you a data engineer who wants to work for a strong and creative online travel technology company? If you enjoy working in a flexible, Agile environment where your efforts to ensure data reliability and availability affect all the teams in the data technology pillar, then… Expedia is looking for YOU!\\nThe team at Expedia Group are looking for data and software development enthusiasts who are highly motivated and focused, to join their awesome Hotels.com Marketing Technology team. This role will collaborate with teams across our Dallas and London offices.\\nWhat you'll do...\\nWriting queries – lots of them. Much of our code is in Spark SQL and Python – scheduled using Apache Airflow - but we also use whatever other tools or languages are needed. We welcome programmers of all backgrounds as long as you are keen to work with data and deliver good-quality code!\\nMaking sure code deploys smoothly and correctly into our cloud environment (AWS).\\nGetting a good understanding of our data sets and the business processes and teams that they support.\\nWorking in an Agile team and looking for ways to continuously improve through asking for and providing feedback.\\nSharing your technical knowledge with the team, reviewing work and providing guidance where you have seniority or experience with a specific technology.\\nActing as a point of contact for technical issues for specific workstreams within a project.\\nImplementing product features, working with Product and Data Engineering teams. Using good practice like of checking requirements, unit testing, and code reviews to ensure these are delivered correctly.\\nFeeding into and helping implement our Roadmap, which includes investigating new data sets, innovating with new technologies and approaches and helping to build up and broaden the technical skills of yourself and the rest of your team.\\nWho you are...\\nWe don’t like skill matching against a list of buzzwords. We look for clever people with good general programming and operational skills because we believe that extraordinary people can learn new technologies quickly and well. However, it wouldn't hurt to have experience with some of the following (or a passion to learn them):\\nCoding in Python (including use of Airflow), Java or another object-oriented language\\n“Big Data”/ distributed systems - Hadoop, Hive, and Amazon EMR\\nExperience of using cloud services (e.g. Amazon Web Services), particularly with an “infrastructure-as-code” approach (we use Terraform and Ansible)\\nKnowledge of visualisation technologies such as Tableau, Power BI, Kibana\\nScalable relational database systems such as Redshift, Teradata\\nPassionate about learning, be that about languages, tools, data sets or how they are used by the business\\nThe tech stack: a widely varying mix of: SQL/HQL , Griffin, Quibble, Hadoop stack (including mainly Hive and HDFS), Python, AWS stack ( including EMR, EC2, S3), Airflow, Azkaban, Spark, Tableau, Power BI, Kibana, Graphite and Grafana, Bash, Maven, Jenkins, Qubole, GitHub, Stash, GCP, JIRA, Confluence, Agile\\nWhy join us:\\nExpedia Group recognizes our success is dependent on the success of our people. We are the world's travel platform, made up of the most knowledgeable, passionate, and creative people in our business. Our brands recognize the power of travel to break down barriers and make people's lives better – that responsibility inspires us to be the place where exceptional people want to do their best work, and to provide them the tools to do so.\\nWhether you're applying to work in engineering or customer support, marketing or lodging supply, at Expedia Group we act as one team, working towards a common goal; to bring the world within reach. We relentlessly strive for better, but not at the cost of the customer. We act with humility and optimism, respecting ideas big and small. We value diversity and voices of all volumes. We are a global organization but keep our feet on the ground so we can act fast and stay simple. Our teams also have the chance to give back on a local level and make a difference through our corporate social responsibility program, Expedia Cares.\\nIf you have a hunger to make a difference with one of the most loved consumer brands in the world and to work in the dynamic travel industry, this is the job for you.\\nOur family of travel brands includes: Brand Expedia®, Hotels.com®, Expedia® Partner Solutions, Egencia®, trivago®, HomeAway®, Orbitz®, Travelocity®, Wotif®, lastminute.com.au®, ebookers®, CheapTickets®, Hotwire®, Classic Vacations®, Expedia® Media Solutions, CarRentals.com™, Expedia Local Expert®, Expedia® CruiseShipCenters®, SilverRail Technologies, Inc., ALICE and Traveldoo®.\\nAbout Hotels.com®\\nHotels.com is the most rewarding way to book a place to stay. We really love travel and we know you do too. That’s why we make it really easy to book with us. With hundreds of thousands of places to stay around the world and 90 local websites in 41 languages, Hotels.com has it all. So, whether you’re looking for value in Vegas, treehouses in Thailand or villas with views, it’s all just a click away. And with our “Reward-winning” loyalty program you earn free* nights while you sleep…what could be better? Booking just got smarter too. With over 25 million real guest reviews and an app so easy to use that it’s been downloaded 70 million times, you can be sure to find the perfect place for you.\\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. This employer participates in E-Verify. The employer will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience on client-facing projects, including working in close-knit teams\\nExperience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)\\nExperience or familiarity with real-time ingestion and streaming frameworks is a plus\\nExperience and desire to work with open source and branded open source frameworks\\nExperience working on projects within the cloud ideally AWS or Azure\\nExperience with NLP, Machine Learning, etc. is a plus\\nExperience working on lively projects and a consulting setting, often working on different and multiple projects at the same time\\nStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R\\nData Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models\\nExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.\\nA deep personal motivation to always produce outstanding work for your clients and colleagues\\nExcel in team collaboration and working with others from diverse skill-sets and backgrounds</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary:\\n\\nYou have experience with client projects and in handling vast amounts of data – working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Analytics team. Play a key role on projects from a data engineering perspective, working with our Architects and clients to model the data landscape, obtain data extracts and define secure data exchange approaches.\\nPlan and execute secure, good practice data integration strategies and approaches\\nAcquire, ingest, and process data from multiple sources and systems into Big Data platforms\\nCreate and manage data environments in the Cloud\\nCollaborate with our business analysts and data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models\\nHave a strong understanding of Information Security principles to ensure compliant handling and management of client data\\nThis is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science\\nQualifications:\\nExperience on client-facing projects, including working in close-knit teams\\nExperience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)\\nExperience or familiarity with real-time ingestion and streaming frameworks is a plus\\nExperience and desire to work with open source and branded open source frameworks\\nExperience working on projects within the cloud ideally AWS or Azure\\nExperience with NLP, Machine Learning, etc. is a plus\\nExperience working on lively projects and a consulting setting, often working on different and multiple projects at the same time\\nStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R\\nData Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models\\nExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.\\nA deep personal motivation to always produce outstanding work for your clients and colleagues\\nExcel in team collaboration and working with others from diverse skill-sets and backgrounds\\nCervello is a dynamic technology company that is focused on business analytics and planning. We take an innovative approach to making complex solutions simple so our clients can focus on running their businesses. Our services and applications enable our clients to gain the benefits of a world-class analytics and planning capability without the headaches.\\n\\nhG1oHsQXvK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Microsoft Power BI Developer</td>\n",
       "      <td>Dallas, TX 75202</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75202</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description:\\nJob Description\\nApplies specialized knowledge of Microsoft Power BI and Azure Analysis services to conceptualize, design, develop, unit-test, configure, and implement portions of new or enhanced (upgrades or conversions) business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate techniques and tools to analyze, identify, and deliver high quality reporting tools. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies.\\nResponsibilities:\\nCore team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives\\nDevelop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams\\nDocument requirements and translate into proper system requirements specifications using high-maturity methods, processes and tools.\\nDevelop visualization, user experience and configuration elements of solution design.\\nExecute and coordinate requirements management and change management processes. Participates as a member of and leads development teams.\\nDesigns, prepares and executes unit tests.\\nCompletes development to implement complex components.\\nDesigns solutions for others to develop.\\nParticipates in cross-functional teams.\\nLeads design activities and provides mentoring and guidance to developers.\\nDesigns, prepares and executes unit tests.\\nRepresents team to clients.\\nDemonstrates technical leadership and exerts influence outside of immediate team.\\nDevelops innovative team solutions to complex problems.\\nContributes to strategic direction for teams.\\nApplies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g. Power BI and Power App development).\\nIntegrates technical expertise and business understanding to create superior solutions for clients.\\nConsults with team members and other organizations, clients and vendors on complex issues.\\nEducation and Experience Required:\\nTypically, a technical bachelor's degree or equivalent experience and a minimum of 6 years of related experience or a master's degree and a minimum of 4 years of experience.\\nKnowledge and Skills:\\n4 or more years’ experience writing code using languages such as (and not limited to) Power BI, Tableau, QlikView, Java, C, C++, C#, VB.Net.\\nSignificant hands on experience in creating and deploying KPI, analytic, and dashboard solutions including design, development, deployment, data engineering, data warehousing, and data management projects and practices.\\nAdvanced understanding of RDBMS databases such SQL Server and Oracle.\\nAdvanced understanding of modern software design and development methodologies.\\nExperience on multiple full release project life cycles.\\nAdvanced understanding of modern SCM (software configuration management).\\nAdvanced understanding of testing tools and unit test and integration test scripting, and testing methodologies\\nAdvanced experience using an Integrated Development Environment (e.g., Eclipse, Visual Studio) and development of tool add-ins.\\nStrong understanding of basic Database Administration. Able to define quality and security standards.\\nGood verbal and written communication and negotiation skills.\\nGeneral project management/team leader skills.\\nAbility to work effectively in a globally dispersed team and with clients and vendors.\\nDemonstrated technical leadership skills.\\nBusiness skills - 4+ years of technology services industry and business operations experience in a technology services organization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Network Data Engineer -- Data Modelling</td>\n",
       "      <td>Richardson, TX 75081</td>\n",
       "      <td>Richardson</td>\n",
       "      <td>TX</td>\n",
       "      <td>75081</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>What you’ll be doing...\\nSDN Planning group under Verizon Network Technology and Planning organization is looking for self-motivated and innovative engineer in support of Verizon SDN architecture and technology development.As a senior level member of technical staff, you will be interacting with software engineers, network/system operations staff, network/system architects and vendor partners to provide SDN technology evolution strategies and solutions to keep our service relevance in the market place in the fast moving and quickly evolving networking industry. Proactive technology research, industry trend analysis, and developing next generation network architecture using modern networking technology (e.g., Software Defined Networking) and providing production deployable solutions are key functions of the team.\\nResponsibilities include:\\nNew technology validation and prototyping.\\nSoftware Defined Networking (SDN) ecosystem development and technical vendor management.\\nSDN/NFV platform architecture and development.\\nDesign and develop machine learning and deep learning models for real world, large scale problems in computer networks.\\nDevelop data collections, labeling pipelines, and evaluation pipelines.\\nOptimize models for on-device and multi-modal intelligence.\\nDesign and implement software applications using Machine Learning and Artificial Intelligence for data verification, transformation, and analytics.\\nOpen API development and verification in support of Verizon SDN platform infrastructure.\\nLinux application deployment and linux networking lab infrastructure support.\\nLinux scripting, networking and administration knowledge.\\nParticipate in open-source and open standard industry collaboration activities.\\nWhat we’re looking for...\\nYou’ll need to have:\\nBachelor’s degree or four or more years of work experience.\\nSix or more years of relevant work experience.\\nExperience developing applications using Machine Learning algorithms.\\nEven better if you have:\\nMaster’s degree or PhD degree in Computer Science or Electrical Engineering with modern data communication technology discipline.\\nExperience on machine learning algorithms, from supervised and unsupervised to reinforcement learning\\nJava, C/C++, and/or Python programming skills.\\nExperience with one or more of the following: artificial neural networks, classification, pattern recognition, recommendation systems, targeting systems, ranking systems or similar.\\nExperience in analyzing sophisticated and dynamic patterns.\\nFive or more years of data communication industry experience.\\nNetwork (WAN/LAN) Engineering (both Layer 2 and 3) experience in Service Provider Network or Enterprise Network environment.\\nSDN knowledge and development experience.\\nExperience on with automation, Machine Learning, and Deep Learning tools and applications (TensorFlow, RPA, etc.).\\nExperience in understanding the complexity of the algorithms as well as in optimizing algorithms.\\nKnowledge of Packet, Optical, and Wireless network technologies.\\nIndustry technology leadership skill.\\nMulti-vendor system integration and technical vendor management experience.\\nOpen source tools development, implementation, and collaboration experience (Robot Framework, Jenkins, Phabricator, Kafka, Screwdriver, etc.).\\nOpen networking, open API, and open platform development experience.\\nWhen you join Verizon...\\nYou’ll be doing work that matters alongside other talented people, transforming the way people, businesses and things connect with each other. Beyond powering America’s fastest and most reliable network, we’re leading the way in broadband, cloud and security solutions, Internet of Things and innovating in areas such as, video entertainment. Of course, we will offer you great pay and benefits, but we’re about more than that. Verizon is a place where you can craft your own path to greatness. Whether you think in code, words, pictures or numbers, find your future at Verizon.\\nEqual Employment Opportunity\\nWe're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AI Model Development Lead for Virtual Channels (Analytic Manager 5)</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview:\\nWells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.\\n\\nSUCCESS PROFILE\\nCheck out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.\\n\\nAnalytical\\nDetail-oriented\\nInsightful\\nInventive\\nProblem Solver\\nCurious\\nBenefits\\nWells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:\\n\\nMedical, Dental and Vision\\nEmployer Matching 401(k)\\nTuition Reimbursment\\nMaternity and Paternity Leave\\nPaid Time Off\\nResponsibilties\\nJob Description\\nAt Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.\\nHelp us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.\\nEnterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.\\nAs part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, Virtual Channels, and virtual channels is looking for an experienced AI leader to manage the development of AI models for Virtual Channels.\\n\\nThis leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on Virtual Channels’ AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and Virtual Channels executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.\\n\\nKEY RESPONSIBILITIES INCLUDE:\\nBuild and grow a team of data scientists responsible for AI model development in support of Virtual Channels\\nDesign, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions\\nPartner with Virtual Channels executives to frame the problem, manage the model development process, and business relationship\\nManage a portfolio of the data science projects including the following responsibilities:\\nHelp finalize project scope working with business partners\\nOn-going touch-base with business partners and governance stakeholders\\nDefine priorities in partnership with the business partners during on-going development\\nWork with AI technology and production teams to operationalize models\\nWill be called upon to review vendor models and solutions and/or models developed outside of EADS\\nDivisional Information:\\nData Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.\\nAs a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:\\nLead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.\\nAccomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.\\n\\nRequired Qualifications\\n\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques\\n\\n\\nDesired Qualifications\\n\\nStrong analytical skills with high attention to detail and accuracy\\nAbility to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members\\nMeeting facilitation experience in leading discussions that result in consensus and commitment\\n\\n\\nOther Desired Qualifications\\n4+ years managing or directing data scientist/ statistician/ data engineer teams\\nHands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet\\nDetail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable\\nExperience with agile project management methodologies for data science\\nExperience with Big Data or Hadoop tools such as Spark, Hive, Kafka and Map\\n\\nStreet Address\\nNC-Charlotte: 401 S Tryon St - Charlotte, NC\\nMN-Minneapolis: 600 S 4th St - Minneapolis, MN\\nNC-Charlotte: 11625 N Community House Road - Charlotte, NC\\nSC-Fort Mill: 3480 State View Blvd - Fort Mill, SC\\nTX-Addison: 5080 Spectrum Dr - Addison, TX\\nTX-DAL-Downtown Dallas: 1445 Ross Ave - Dallas, TX\\nTX-Irving: 5000 Riverside Drive - Irving, TX\\nAZ-Tempe: 1150 W Washington St - Tempe, AZ\\nIA-Des Moines: 6200 Park Ave - Des Moines, IA\\nIA-Des Moines: 800 Walnut St - Des Moines, IA\\nGA-Atlanta: 3579 Atlanta Ave - Atlanta, GA\\n\\n\\nDisclaimer\\n\\nAll offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.\\n\\nRelevant military experience is considered for veterans and transitioning service men and women.\\n\\nWells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.\\n\\nENT FINANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tax Staff – National Tax – Tax Technology and Transformation (TTT) – Data Scientist – Advanced Technologies - Dallas</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nStrong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.\\nExperience with common data science programming languages, such as Python R\\nStrong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential\\nExperience with data visualization tools, such as PowerBI, D3.js, etc.\\nExperience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB\\nStrong scripting and programming skills\\nOwnership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.\\nAbility to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies\\nAbility to consolidate tax data to make analysis and planning more efficient\\nFocus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities\\nWillingness to support project team members in any way needed to help ensure timely completion of deliverables</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Tax Technology and Transformation offers services to companies in response to the impact of existing and emerging technology, including the growing data burden that many businesses face, driving efficiencies to create a cost-effective tax function and the need to understand how to make data an asset. The underlying objective of the combined offerings is to help businesses navigate the digital age of tax transparency alongside new trends in tax compliance and tax audit methods, as well as helping to solve the most pressing challenges that businesses face. Tax Technology and Transformation is composed of the following services:\\nDigital tax transformation\\nTax applications-as-a-service\\nTax data and improvement\\nTax analytics and reporting enhancement\\nEmerging tax technology, including robotic process automation (RPA), artificial intelligence (AI), blockchain, cloud solutions, data lake development and business intelligence innovation\\nTax technology program mobilization\\nCustom tax technology application development and deployment\\nTax technology strategy and road mapping\\nDirect and indirect tax systems implementation and configuration\\nPost-transaction (M&amp;A) tax function operational services\\nTax operating model transformation, including process improvement, risk and controls\\nTax function assessments\\nThe opportunity\\nTax Technology and Transformation is an area that has seen significant growth and investment, and you will see that reflected in your experience. It is no exaggeration to say that you will be working on highly publicized projects. The field of taxation is constantly changing as new laws, regulations, and technologies are created, and this is your opportunity to be part of that development.\\n\\nKey responsibilities\\nWe are looking for an ambitious, self-motivated data scientist or data engineer who will help us discover the information hidden in vast amounts of data, and help us deliver even better products to our clients. Your primary focus will be in applying data mining techniques, doing statistical analysis and building high quantity prediction systems integrated with our products. You will be expected to team on a national and even global scale, so strong communication skills, attention to detail, and ability to effectively drive results are essential.\\nSelecting features and, building and optimizing classifiers using machine learning techniques\\nData mining using state-of-the-art methods\\nEnhancing data collection procedures to include information that is relevant for building analytic systems\\nProcessing, cleansing and verifying the integrity of data used for analysis\\nDoing ad-hoc analysis and presenting results in a clear manner\\nDepending on your unique skills and ambitions, you could be supporting various client projects, ranging from assisting in the production of leading edge machine models, to designing and implementing robust data pipelines that can handle data at a multinational, enterprise scale. Whatever you find yourself doing, you will contribute and help toward developing a highly trained team, all the while handling activities with a focus on quality and commercial value. This is a highly regulated industry, so it is all about maintaining our reputation as trusted advisors by taking on bold initiatives and owning new challenges.\\n\\nSkills and attributes for success\\nStrong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.\\nExperience with common data science programming languages, such as Python R\\nStrong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential\\nExperience with data visualization tools, such as PowerBI, D3.js, etc.\\nExperience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB\\nStrong scripting and programming skills\\nOwnership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.\\nAbility to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies\\nAbility to consolidate tax data to make analysis and planning more efficient\\nFocus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities\\nWillingness to support project team members in any way needed to help ensure timely completion of deliverables\\nTo qualify for the role, you must have\\nA bachelor’s degree in information system, tax technology, management information systems or computer science or related field and a minimum of one year of related work experience\\nA passionate interest in data science and its role in the organization\\nExcellent communication and business writing skills\\nA natural flair for problem solving and an entrepreneurial approach to work\\nStrong organizational and time management skills, with exceptional client-serving consulting skills\\nDemonstrated ability to capture and synthesize business requirements\\nDesire and demonstrated ability to provide leadership within a team\\nIdeally, you’ll also have\\nExperience with Apache Spark\\nExperience with Hadoop and/or distributed database systems\\nExperience working in the Microsoft Azure Cloud environment\\nExperience developing ETL solutions using SSIS or other tools\\nERP experience, including SAP and/or Oracle-preferred but not required\\nPractical experience or strong theoretical understanding of neural networks\\nWhat we look for\\nWe are looking for knowledgeable data science professionals with a passion for turning data into actionable insight. You will need strong business acumen and a firm strategic vision, so if you are ready to use those skills to develop your team, this role is for you.\\n\\nWhat working at EY offers\\nWe offer a competitive compensation package where you will be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, a minimum of three weeks of vacation plus ten observed holidays and three paid personal days; and a range of programs and benefits designed to support your physical, financial and social well-being. We also offer:\\nSupport and coaching from some of the most engaging colleagues in the industry\\nOpportunities to develop new skills and progress your career\\nThe freedom and flexibility to handle your role in a way that is right for you\\nAbout EY\\nAs a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.\\n\\nEY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\\n\\nIf you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible.\\n\\nMake your mark. Apply today.\\n\\n.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Engineer - ETL Developer</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Advanced career position responsible for leading development of Pioneer's enterprise data solutions. Major Responsibilities: Works independently designing, developing, documenting, and supporting full lifecycle data, reporting, and analytic solutions. These solutions should include large data ingestion, persistence, transformation, and retrieval. Will develop, maintain, and deploy code to include data acquisition, aggregation, and data warehousing. Addresses problems of systems integration and compatibility. Works on complex, less clearly-defined projects. Requires advanced knowledge of Pioneer's enterprise solutions. Applies a full understanding of established systems software development principles and best practices to analyze complex issues and update existing solutions. Responsible for reviewing work product of other developers. Acts as team leader on projects and provides guidance and training to less-experienced developers. Work closely with enterprise architects to ensure that solutions are developed using best practice and following standard software development lifecycle (SDLC) methodologies. You will work with independently and with team members on projects that include incident analysis, break-fix enhancements, ETL/ELT framework enhancements, data quality enhancements, usage reporting and general design, coding and release/build activities. Minimum Qualifications: Bachelor’s degree in Computer Science, Information Systems, Engineering, Science, or related field is required; consideration given for related technical experience. Minimum of 6 years related experience is required; consideration given for related technical aptitude. Experience in agile development methodologies 4 or more years of ETL development in tools like ODI, SSIS, DataStage or Informatica 4 or more years of experience in technologies like Oracle Database (v11+), Microsoft SQL Server, SQL &amp; PL/SQL, T-SQL development, stored procedures, triggers, and ANSI SQL standards. 4 or more years of Data modeling, Datawarehouse and Data mart design in tools like Oracle Business Analytics Warehouse, and/or Microsoft SQL Server. Preferred Qualifications: Advanced knowledge of agile development methodologies. Java or Python programming experience. Experience building streaming data pipelines Experience working with Azure cloud technologies like Azure Datawarehouse, ADF and ADF2 Experience working with cloud-based data platforms Experience working with data virtualization technologies like Tibco Data Virtualization Advanced with Linux or Unix systems / scripting Experience predictive analytics implementations Experience with NoSQL data platforms Must have aptitude for problem-solving. Must have a “can do” attitude. Must demonstrate desire to learn. Exceptionally detail-oriented Must be able to perform in a collaborative manner by coordinating changes while maintaining a positive relationship with team members, external consultants and vendors, business partners, interfacing application system contacts and numerous technology services groups. Must be able to communicate effectively at all levels in the organization and with personnel located at headquarters and in field locations. Curious and excited by new ideas Energized by a fast-paced environment Able to understand and translate business needs into leading-edge technology Comfortable working as part of a connected team, but self-motivated Salary competitive and commensurate with qualifications and experience Pioneer Natural Resources is an EEO/M/F/disability/veteran Employer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hadoop Developer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Title: Hadoop Developer\\n\\nLocation: Dallas, TX\\n\\nType: Contract\\n\\nDuration: Long term\\n\\nJD:\\n\\n· We need Big Data (Hadoop) HortonworksData Platform (HDP ) Data Engineer who is well versed with Hortonworks Data Flow ( HDF) and apache Nifi /miNifi and Kafka tools .\\n\\n· The candidate should have strong experience on Scala &amp; Spart.\\n\\n· The resource should be 6+ yrs exp with good in depth knowledge &amp; experience in Hadoop around all the Hadoop ecosystem- HDP, HDF, Nifi , M/R, Hive, pig, Spark/scala, kafka, Hbase and having ETL Background.\\n\\n· The resource should be able Develop the framework of Data Ingestion into Data lake with Utilities around this Platform.\\n\\n· 6+ years total experience in development mainly around Java and all related technologies in the Java stack (e.g. Spring)\\n\\n· 6+ year in depth knowledge &amp; experience in Hadoop around all the Hadoop ecosystem (HDP, HDF, M/R, Hive, pig, Spark/scala, kafka, Hbase, Elastic search and log stash a plus)\\n\\n· 4+ years of experience working in Linux/Unix\\n\\n· Good understanding &amp; experience with Performance and Performance tuning for complex S/W projects mainly around large scale and low latency.\\n\\n· Experience with leading Design &amp; Architecture\\n\\n· Hadoop/Java certifications is a plus\\n\\n· Excellent communication skills.\\n\\n· Ability to work in a fast-paced, team oriented environment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Big Data Developer- Senior Advisor</td>\n",
       "      <td>Richardson, TX</td>\n",
       "      <td>Richardson</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.4+ years of SQL development skills within a multi-tier environment are required.experience in Big Data tools: Hadoop, PIG, Sqoop, Hive and Hcatalog &amp; NoSQL (HBase, Cassandra) , SQL.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Big Data Developer will work with data stewards, data owners, master data management analysts, operations teams and IT partners across the organization to drive the enterprise data conformance program. Data research and analysis, cross-functional requirements gathering and documentation and solution development are key aspects of the role. The Data Engineer supports reactive data quality by researching and fixing known data issues and proactive data quality by defining the requirements for data controls and enhancements to data capture, monitoring and maintenance processes, procedures and standards.\\n\\nPrimary responsibilities include performing data operations such as conversion, address hygiene, and postal presort, as well as variable document composition and file creation.\\n\\nEssential Functions:\\n\\nData Analysis\\nIdependently design, develop and implement data integration solutions that support our platforms resiliency, stability, and supportability using a variety of ETL and database technologies.\\nRapidly develop and refine data integration solutions using Infosphere Datastage, SQL, FastTrack, or other technologies.\\nExperience integrating large structured and unstructured data in multiple format, character sets and delivery methods\\nWorks with business sponsors, SMES and application teams; to understand the business requirements; analyze and assess availability, quality, and lineage of source system data.\\nDesign, map data from source to target and develop data integration solutions that meet business needs.\\nDevelop and socialize data integration standards.\\nPartners with other engineers through design reviews, providing feedback on feasibility, scalability, performance and adherence to standards.\\nPartners with business, analysts, BI team, application teams and other stakeholders to design, map data from source to target, develop, test, and implement production data integration solutions that are fully integrated into the Enterprise Data Warehouse.\\nEnsuring model design solves the end users need.\\nContribute information to the data governance software to improve knowledge downstream.\\n\\nData Conformance\\nPerforms analysis on known data quality issues and develops and/or recommends operational or technical solutions for remediation, including development and implementation of automated data quality controls that proactively trigger notifications to process owners when data is out of range. Keeps stakeholders informed of progress and solutions in a timely manner.\\nAutonomously, and proactively performs data profiling to explore data, identify issues and summarize findings.\\nDefines data quality metrics to assess completeness, accuracy, consistency, and conformance to business rules. Designs dashboards to support continuous monitoring and measurement of data quality.\\nPartners with IT to cleanse data to achieve the desired level of data quality\\nPartners with data stakeholders, process and product owners across the organization to define data standards and communicate changes to data capture procedures, processes, standards, and controls.\\nCleanses and prepares datasets to be consumed by data scientists and other analysts\\nCollaborates with external teams working on data integration and engineering\\nAdvocates data governance and hygiene best practices\\nAssists with scoping and integrating orphan datasets\\n\\nYou will gain hands on experience implementing through embedding standardized data elements in the database or system, employing standardized data elements in an exchange mechanism (usually XML schema), or mapping the application data elements to the standardized elements for purposes of exchange.\\n\\nRequired Qualifications\\n7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.4+ years of SQL development skills within a multi-tier environment are required.experience in Big Data tools: Hadoop, PIG, Sqoop, Hive and Hcatalog &amp; NoSQL (HBase, Cassandra) , SQL.\\nProgramming: Scala, Java, Python, Spark\\n\\nPreferred Qualifications\\nExperience in programming languages like Scala, Java, Python, Spark\\nIn depth understanding of data integration best practices, leading industry applications and features such as master data management, entity resolution, data quality assessment, metadata management, etc.Expertise in flat file formats, XML within PL/SQL and file format conversion.Exposure to application security technologies and approaches is preferred.Experience processing and parsing CSV, JSON and XML file formatsDatastage, SQL, FastTrack, or other technologies\\nStrong analytical, debugging and testing skills\\nSoftware development experience using scripting languages such as JavaScript, Python or RubyProficient using Infosphere/DataStage or equivalent ETL software.Proficient with relational databases and using SQL to query, create tables, views, indexes, joins.Proficient using Unix and applicable scripting/scheduling tools.\\n-Knowledge of clinical and financial Healthcare data -Knowledge of all data formats (HL7, EDI, CSV, XML, etc)\\n\\nEducation\\nBachelors Degree in Computer Science or related field required\\n\\nBusiness Overview\\nIt’s a new day in health care.\\n\\nCombining CVS Health and Aetna was a transformative moment for our company and our industry, establishing CVS Health as the nation’s premier health innovation company. Through our health services, insurance plans and community pharmacists, we’re pioneering a bold new approach to total health. As a CVS Health colleague, you’ll be at the center of it all.\\n\\nWe offer a diverse work experience that empowers colleagues for career success. In addition to skill and experience, we also seek to attract and retain colleagues whose beliefs and behaviors are in alignment with our core values of collaboration, innovation, caring, integrity and accountability.\\n\\nCVS Health is an equal opportunity/affirmative action employer. Gender/Ethnicity/Disability/Protected Veteran – we highly value and are committed to all forms of diversity in the workplace. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities. We comply with the laws and regulations set forth in the following EEO is the Law Poster: EEO IS THE LAW and EEO IS THE LAW SUPPLEMENT. We provide reasonable accommodations to qualified individuals with disabilities. If you require assistance to apply for this job, please contact our Advice and Counsel Reasonable Accommodations team. Please note that we only accept applications for employment via this site.\\n\\nIf technical issues are preventing you from applying to a position, contact Kenexa Helpdesk at 1-855-338-5609 or cvshealthsupport@us.ibm.com. For technical issues with the Virtual Job Tryout assessment, contact the Shaker Help Desk at 1-877-987-5352.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Technical Advisor - Data Engineer</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree; STEM background preferred5+ years’ experience developing scalable big data solutions in cloud and on premise environmentsExperience with big data technologies (Spark, Hadoop); data lakesExperience developing data processing/ETL/ELT solutions using e.g. Scala, Python, Java, SQLExperience deploying statistical/machine learning models to productionExperience building cloud-based data solutions; Azure experience preferred</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Develop, construct, test and maintain architectures for scalable, timely and efficient big data processingBuild data pipelines in lambda architectures for structured/unstructured, streaming/batch dataDevelop solutions for data modeling, mining and quality monitoringDevelop solutions for deploying models to productionIdentify and make recommendations on ways to improve data quality, reliability and efficiencyDiscover opportunities for data acquisitionMaintain awareness of ongoing developments in big data and data engineering technologies both in the industry in general and within the enterprise</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Company: FedEx Services\\nJob Title: Technical Advisor - Data Engineer\\nJob Requisition Number: RC196455\\nCategory: Information Technology\\nLocations:\\nPlano, Texas 75024\\nUnited States\\n\\nJob Summary:\\nThe Technical Advisor - Data Engineer will play a critical role in modernizing the architectures and processes enabling operationalization of big data acquisition, processing and quality monitoring for our retail domain (Omni-channel, manufacturing network, back office). You will work closely with technical solution architects, data scientists and business stakeholders to develop capabilities for enabling timely connections to high-quality data sourced from many disparate systems. While this role will primarily be technical in nature, we are looking for an individual who can effectively collaborate with our business partners and other IT members to identify and define scalable architectures and capabilities. This is an exciting opportunity to work with a team of passionate practitioners using current data technologies, and with significant potential for growth (machine learning, etc.).\\n\\nKey Responsibilities\\nDevelop, construct, test and maintain architectures for scalable, timely and efficient big data processingBuild data pipelines in lambda architectures for structured/unstructured, streaming/batch dataDevelop solutions for data modeling, mining and quality monitoringDevelop solutions for deploying models to productionIdentify and make recommendations on ways to improve data quality, reliability and efficiencyDiscover opportunities for data acquisitionMaintain awareness of ongoing developments in big data and data engineering technologies both in the industry in general and within the enterprise\\n\\nBasic Qualifications\\nBachelor’s degree; STEM background preferred5+ years’ experience developing scalable big data solutions in cloud and on premise environmentsExperience with big data technologies (Spark, Hadoop); data lakesExperience developing data processing/ETL/ELT solutions using e.g. Scala, Python, Java, SQLExperience deploying statistical/machine learning models to productionExperience building cloud-based data solutions; Azure experience preferred\\n\\nPreferred Qualifications\\nExperience with notebook-based development (Databricks, Jupyter)Experience with data governance/master data managementExperience developing statistical and/or machine learning modelsExperience developing serverless solutionsExperience with graph databasesIoT knowledge; edge computingRetail domain knowledgeSAFe/agile experience\\nDomicile: Plano, TX\\nRelocation assistance may be available for this position, but is based on business decision.\\n\\nMinimum Qualifications:\\nBachelor's Degree, in computer science, engineering, information system or related field and/or equivalent formal training or work experience. Requires five (5) or more years qualifying work experience in information technology or engineering environment. A related advanced degree may offset the related experience requirements.\\n\\nTo apply for this position, please upload a current copy of your resume and complete the required screening questionnaire by close of business (5:00 PM CST) on _10/14/19, to be considered.\\n\\n\\nWant a career where you are empowered to make a difference? Want to work for a company that is environmentally responsible? Want to grow and develop on the job? If so, FedEx is the place for you! Every day FedEx delivers for its customers with transportation and business solutions. FedEx serves more than 220 countries and territories around the globe. We can serve this global network due to our outstanding team of FedEx employees. FedEx has over 400,000 talented employees who are tasked with making every FedEx experience outstanding. FedEx has been recognized on many different lists both for business success and for being a great employer.\\nHere are some of the recognitions FedEx has received from the past couple of years:\\nFortune \"World’s Most Admired Companies\" – 2019\\nForbes \"Best Employers for Diversity\" - 2019\\nReputation Institute \"World\\’s Most Reputable Companies\" – 2019\\nNational Business Inclusion Consortium \"Best-of-the-Best Corporations for Inclusion\" - 2019\\nWomen\\’s Business Enterprise National Council \"America’s Top Corporations for Women’s Business Enterprises\" - 2018\\nCorporate Responsibility Magazine \"100 Best Corporate Citizens\" – 2018\\nBlack Enterprise \"50 Best Companies For Diversity\" – 2018\\nWhen 400,000 employees around the globe are all working together it is amazing what we can achieve! FedEx connects people and ideas. If you would like to make a difference on a global scale while receiving top notch benefits, competitive pay, and plenty of opportunities to develop, click ‘Apply’ and tell us more about yourself.\\nEEO Statement - FedEx is an equal opportunity/affirmative action employer (minorities/females/disability/veterans) that is committed to diversifying its workforce.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management, and product design, combined with digital engineering excellence.\\n\\nAs a Senior Data Engineer, here's what we'll be looking for you to bring:\\n\\n\\nHands-on Engineering Leadership\\nProven track record of Innovation and expertise in Data Engineering\\nTenure in coding, architecting and delivering complex projects\\nDeep understanding and application of modern data processing technology stacks. For example Spark, Kafka, Hadoop, ecosystem technologies, and others\\nDeep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies\\nDeep understanding of relational database technologies and database development techniques\\nUnderstanding of how to architect solutions for data science and analytics\\nData management for reporting and BI experience is a plus\\nUnderstanding of \"Agility\", including core values, guiding principles, and key agile practices\\nUnderstanding of the theory and application of Continuous Integration/Delivery\\nPassion for software craftmanship\\nA rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..\\nStrong stakeholder management and interaction experience at different levels\\nAny experience building and leading an offshore/outsourcing function would be highly beneficial.\\n\\nThere's no typical day or engagement for our Senior Engineers. Here's what you'll do:\\n\\n\\nBe the SME. Develop Big Data architectural approach to meet key business objectives and provide end to end development solution\\nYou might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that Big Data has to solve their most pressing problems.\\nOn other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.\\nIt could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.\\nWhatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.\\nYou have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.\\nYou recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.\\n\\nRegardless of what you do at ThoughtWorks, you'll always have the opportunity to:\\n\\n\\nThink through hard problems, and work with a team to make them reality.\\nLearn something new every day.\\nWork in a dynamic, collaborative, transparent, non-hierarchal, and ego-free culture where your talent is valued over a role title\\nTravel the world.\\nSpeak at conferences.\\nWrite blogs and books.\\nDevelop your career outside of the confinements of a traditional career path by focusing on what you're passionate about rather than a predetermined one-size-fits-all plan\\nBe part of a company with Social and Economic Justice at the heart of its mission.\\n\\nA few important things to know:\\n-------------------------------\\n\\nProjects are almost exclusively on customer site, so candidates should be flexible and open to travel.\\n\\nCandidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.\\n\\nNot quite ready to apply? Or maybe this isn't the right role for you? That's OK, you can stay in touch with AccessThoughtWorks ( https://www.thoughtworks.com/careers/access?utm_source=apply-jobs&amp;utm_medium=jd&amp;utm_campaign=access-thoughtworks ), our learning community (click \"contact me about recruitment opportunities\" to hear about jobs in the future).\\n\\nIt is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.\\n\\n#LI-NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Engineer Cloud</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>4 year College degree in Computer Science, Information Technology or equivalent demonstrated experience. Masters degree preferred.\\nStrong SQL development skills using databases like oracle and Vertica.\\nExperience in cloud databases like Snowflake or Redshift is a plus\\nExperience with AWS technologies such as EC2, S3 and other basic AWS technologies\\nCertification –preferably AWS Certified Big Data or any other cloud data platforms, big data platforms\\n4+ years of experience in the data and analytics space\\nSolid Programing experience in Python - needs to be an expert in this 4/5 level.\\nExperience with workload automation tools such as Airflow, Autosys.\\n4+ years experience developing and implementing enterprise-level data solutions utilizing Python , Java, Spark, and Scala, Airflow , Hive\\n3+ years in key aspects of software engineering such as parallel data processing, data flows, REST APIs, JSON, XML, and micro service architectures.\\n6+ years of RDBMS concepts with Strong Data analysis and SQL experience\\n3+ years of Linux OS command line tools and bash scripting proficiency\\nCloud data warehouse experience - Snowflake is a plus\\nMust be an EXPERT with ETL Development on Unix Servers.\\nMust have demonstratable working knowledge of modern information and delivery practices for on-premises and cloud environments.\\nMust have demonstratable experience delivering robust information delivery and management solutions as part of a fast paced data platform program.\\nMUST BE AN EXPERT applying business rules/logic using SQL scripts.\\nMust have working knowledge of various data modeling techniques (3NF, denormalized, STAR Schema).\\nPosition requires a self-starter, capable of quickly turning around vaguely defined projects with minimal supervision or assistance.\\nAbility to conduct analysis of source data sets to achieve target data set objectives.\\nSTRONG VERBAL/WRITTEN COMMUNICATION is a MUST. Interacting with business community/users is a core requirement of the role.\\nMust be able to provide specialized support for our Legacy platforms, as well as the new Cloud Based Data Platform.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>COMPANY OVERVIEW\\nFor over a century, Neiman Marcus Group has served the unique needs of our discerning customers by staying true to the principles of our founders: to be the premier omni-channel retailer of luxury and fashion merchandise dedicated to providing superior service and a distinctive shopping experience in our stores and on our websites. Neiman Marcus Group is comprised of the Specialty Retail Stores division, which includes Neiman Marcus and Bergdorf Goodman, and our international brand, mytheresa.com. Our portfolio of brands offers the finest luxury and fashion apparel, accessories, jewelry, beauty, and home décor. The Company operates more than 40 Neiman Marcus full-line stores in the most affluent markets across the United States, including U.S. gateway cities that draw an international clientele. In addition, we operate 2 Bergdorf Goodman stores in landmark locations on Fifth Avenue in New York City. We also operate more than 40 Last Call by Neiman Marcus off-price stores that cater to a value oriented, yet fashion minded customer. Our upscale eCommerce and direct-to-consumer division includes NeimanMarcus.com, BergdorfGoodman.com Horchow.com, LastCall.com, and CUSP.com. Every day each of our 15,000 NMG associates works towards the goal of enabling our customer to shop any of our brands \"anytime, anywhere, and on any device.\" Whether the merchandise we sell, the customer service we offer, or our investments in technology, everything we do is to enhance the customer experience across all channels and brands.\\nDESCRIPTION\\nNeiman Marcus Group has an immediate opening for a Lead Data Platform Engineer.\\nThe Senior Data Platform Engineer will have the unique combination of business acumen needed to interface directly with key stakeholders to understand business challenges, along with the skills and vision required to translate the need into a world-class technical solution using the latest technologies.\\nThis person will be in a hands-on role as part of a development team responsible for building data engineering solutions for the NMG Enterprise using cloud based data platforms. They will work closely with solution architects and support teams and take a lead on day-to-day development and support for data engineering workloads. In this role, you need to be equally skilled with the whiteboard and the keyboard.\\nSUMMARY\\nPrimary focus of this position is to provide design and development support to the Data Platform team. Day to day activities will include, but not be limited to…ingesting various data sources into the data platform, designing and deploying consolidation and summary tables, design and deployment of data marts, data/process modeling, systematic auditing of load processes, creating batch process automation, and analyzing source data from various applications for purposes of reporting, business intelligence and data science initiatives.\\nThis position is a hands-on, intense role of working side by side with our end user partners to drive better analytics, reporting and most importantly…competitive advantage. This person will be an integral part of the Customer Data platform team, working with architects, developers and data platform engineers to support the overall Neiman Marcus enterprise.\\n\\nESSENTIAL DUTIES AND RESPONSIBILITIES (include the following, other duties not listed may be assigned)…\\nWork primarily with architects and at times with business partners and data science teams to understand business context and craft best-in-class solutions to their toughest problems\\nProvide data modeling, process modeling, and data mart design support.\\nCreate Python Scripts/SQL scripts in support of data platform load and batch processes.\\nDesign and deploy consolidation and summary tables as required within the data warehousing environment.\\nPerform periodic performance assessments of the automated load processes.\\nBe proactive in identifying and working with issues.\\nProvide specialized support for our Legacy platforms, as well as the new EDW.\\nDocumentation of deliverables.\\nStandardization of deliverables.\\nCreate robust and automated pipelines to ingest and process structured and unstructured data from source systems into analytical platforms using batch and streaming mechanisms leveraging a cloud native toolset\\nImplements automation to optimize data platform compute and storage resources\\nDevelops and enhances end to end monitoring capability of cloud Data platforms\\nResponsible for implementing custom data applications as required for delivering actionable insights\\nProvides regular status updates to all relevant stakeholders\\nParticipates in daily scrum calls and provides clear visibility to work products\\nParticipates in developing projects plan, timelines and providing estimates\\nProvide hands-on technical assistance in all aspects of data engineering design and implementations including data ingestion, data models, data structures, data storage, data processing, and data monitoring at scale\\nDevelop data engineering best practices with considerations for high data availability, fault tolerance, computational efficiency, cost, and quality\\n\\nQualifications\\n\\nREQUIREMENTS\\n4 year College degree in Computer Science, Information Technology or equivalent demonstrated experience. Masters degree preferred.\\nStrong SQL development skills using databases like oracle and Vertica.\\nExperience in cloud databases like Snowflake or Redshift is a plus\\nExperience with AWS technologies such as EC2, S3 and other basic AWS technologies\\nCertification –preferably AWS Certified Big Data or any other cloud data platforms, big data platforms\\n4+ years of experience in the data and analytics space\\nSolid Programing experience in Python - needs to be an expert in this 4/5 level.\\nExperience with workload automation tools such as Airflow, Autosys.\\n4+ years experience developing and implementing enterprise-level data solutions utilizing Python , Java, Spark, and Scala, Airflow , Hive\\n3+ years in key aspects of software engineering such as parallel data processing, data flows, REST APIs, JSON, XML, and micro service architectures.\\n6+ years of RDBMS concepts with Strong Data analysis and SQL experience\\n3+ years of Linux OS command line tools and bash scripting proficiency\\nCloud data warehouse experience - Snowflake is a plus\\nMust be an EXPERT with ETL Development on Unix Servers.\\nMust have demonstratable working knowledge of modern information and delivery practices for on-premises and cloud environments.\\nMust have demonstratable experience delivering robust information delivery and management solutions as part of a fast paced data platform program.\\nMUST BE AN EXPERT applying business rules/logic using SQL scripts.\\nMust have working knowledge of various data modeling techniques (3NF, denormalized, STAR Schema).\\nPosition requires a self-starter, capable of quickly turning around vaguely defined projects with minimal supervision or assistance.\\nAbility to conduct analysis of source data sets to achieve target data set objectives.\\nSTRONG VERBAL/WRITTEN COMMUNICATION is a MUST. Interacting with business community/users is a core requirement of the role.\\nMust be able to provide specialized support for our Legacy platforms, as well as the new Cloud Based Data Platform.\\nADDITIONAL REQUIREMENTS\\nCandidate MUST possess a STRONG INITIATIVE.\\nCandidate MUST be able to run with a project on their own, with as little as a few sentences to begin the project with. Candidates requiring design specs will not be successful.\\nRetail experience is a plus.\\nExperience in the migration of data from an on premise database to a cloud based data warehouse platform is a strong plus.\\nExperience with Qlik, Business Objects, or Tableau is a plus.\\nA candidate with experience working with terabyte sized data warehouses and complex ETL mappings that process 50+ million records per day is strongly preferred.\\nNICE TO HAVE\\nKubernetes and Docker experience a plus\\nPrior working experience on data science work bench\\nCloud data warehouse experience - Snowflake is a plus\\nData Modeling experience a plus\\nLANGUAGE/WRITING SKILLS\\nStrong, concise and grammatical oral and written communication skills.\\n\\nMATHEMATICAL SKILLS\\n\\nBasic math skills\\nREASONING/ ANALYTICAL ABILITY\\n\\nComplex problem solving skills\\nExtensive data analytical skills\\nInitiative to develop efficiencies and process improvements\\nDocumentation\\n\\nPrimary Location: United States of America-Texas-DALLAS-Irving-Information Services\\nWork Locations: Information Services Neiman Marcus 111 Customer Way Irving 75039\\nJob: Information Technology\\nOrganization: Corporate\\nSchedule: Full-time\\nShift: Day\\nEmployee Status: Regular\\nJob Type: Standard\\nJob Level: Individual Contributor\\nTravel: No\\nJob Posting: Jun 3, 2019, 1:35:50 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Locations: TX - Plano, United States of America, Plano, Texas\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nSr. Data Engineer\\n\\nCandidates should possess strong knowledge and interest across big data technologies and have a background in data engineering. In this role you will be part of a collaborative, agile team you will:\\n\\nBuild data pipeline frameworks to automate high-volume and real-time data delivery for our Spark and streaming data hub\\n\\nTransform complex analytical models in scalable, production-ready solutions\\n\\nProvide support and enhancements for an advanced anomaly detection machine learning platform\\n\\nContinuously integrate and ship code into our cloud production environments\\n\\nDevelop cloud based applications from the ground up using a modern technology stack\\n\\nWork directly with Product Owners and customers to deliver data products in a collaborative and agile environment\\n\\nYour responsibilities will include:\\nDeveloping sustainable data driven solutions with current new generation data technologies to drive our business and technology strategies\\n\\nBuilding data APIs and data delivery services to support critical operational and analytical applications\\n\\nContributing to the design of robust systems with an eye on the long-term maintenance and support of the application\\n\\nLeveraging reusable code modules to solve problems across the team and organization\\n\\nHandling multiple functions and roles for the projects and Agile teams\\n\\nDefining, executing and continuously improving our internal software architecture processes\\n\\nBeing a technology thought leader and strategist\\n\\nBasic Qualifications:\\nBachelor’s Degree\\n\\nAt least 2 years of experience on designing and developing Data Pipelines for Data Ingestion or Transformation using Java or Scala or Python\\n\\nAt least 2 years experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC), Resource Management, Distributed Processing and RDBMS\\n\\nAt least 2 years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps\\n\\nAt least 2 years of experience with SQL and Shell Scripting experience\\n\\nPreferred Qualifications:\\nMaster's Degree\\n\\n1+ years’ experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service\\n\\n2+ years of experience working with Streaming using Spark or Flink or Kafka or NoSQL\\n\\n2+ years of experience working with Dimensional Data Model and pipelines in relation with the same\\n\\nIntermediate level experience/knowledge in at least one scripting language (Python, Perl, JavaScript)\\n\\nHands on design experience with data pipelines, joining data between structured and unstructured data\\n\\nBasic understanding of ETL standards and best practices\\n\\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBased on understanding of system upstream &amp; downstream, provide feedback and inputs on gaps in requirements and technical feasibility of requirements.\\n</td>\n",
       "      <td>Technical Lead\\n\\nQualification: Bachelors in science , engineering or equivalent\\n\\nResponsibility:Project Planning and Setup:\\n\\nUnderstand the project scope, identify activities/ tasks, task level estimates, schedule, dependencies, risks and provide inputs to Module Lead for review.\\nProvide inputs to testing strategy, configuration, deployment, hardware/software requirement etc.\\nReview plan and provide feedback on gaps, timeline and execution feasibility etc as required in the project.\\nParticipate in KT sessions conducted by customer/ other business teams and provide feedback on requirements.\\n\\nRequirement Understanding and Analysis: • Analyze functional/non functional requirements and seek clarifications for better understanding of requirements.\\n\\nBased on understanding of system upstream &amp; downstream, provide feedback and inputs on gaps in requirements and technical feasibility of requirements.\\n\\nDesign: • Prepare the LLD/ detailed design documents based on HLD and briefing from Module Lead.\\n\\nSeek inputs from the developers on specific modules as applicable.\\nConsolidate all modules and provide to Module Lead/ Architects/ Designers for review.\\nSuggest changes in design on technical grounds.\\nDevelop components inventory for the code to be developed tying it to the nonfunctional requirements.\\nPerform sampling of data to understand the character/ quality of the data (project dependent in the absence of data analyst or designer).\\nIdentify tools and technologies to be used in the project as well as reusable objects that could be customized for the project.\\n\\nCoding: • Follow coding standards and best practices to develop code and check code quality.\\n\\nShare developed code with supervisor for review.\\nRework on the code based on inputs if required.\\nPerform complex integration.\\nGuide the developers in identifying, preparing and conducting unit test cases and fixing defects based on results.\\nConsolidate the test results and share with supervisor.\\nProvide periodic status update to supervisor and highlight / recommend any changes in design based on challenges faced.\\nAnticipate unreported defects and raise the same to supervisor.\\nConduct technical troubleshooting.\\nConduct reviews for codes created by team.\\n\\nTesting Management: • Develop unit test case for each module.\\n\\nConduct/ guide conducting of unit and integration testing and fix defects.\\nReview/ approve code to be moved to testing environment.\\nProvide support to the QA team and coordinate for various phases of testing.\\nAddress queries raised by QA within defined timelines.\\nInvestigate critical defects and establish need for fixing.\\nRaise issues to leads/QA.\\nReport defect status as per project standard process within agreed timelines.\\nShare revised code with supervisor for review.\\nAssist team lead and project manager on estimates around defect fixes.\\n\\nConfiguration Management: • Maintain versions of the code or consolidate version maintained by the Developers.\\n\\nProvide support as required to the Administrators during configuration, code backups, deployment etc.\\n\\nDeployment: • Assess and create deployment/ roll back plan.\\n\\nValidate if all the components have been migrated and the right version is checked in.\\nMaintain deployment tracker.\\nPerform sanity checks post deployment to ensure smooth production.\\nShare activity status with supervisor and highlight concerns if any.\\n\\nProject Execution Monitoring &amp; closure (Support to Project Management activities): • Monitor work of the developers and share work achieved with them.\\n\\nProvide guidance through SDLC.\\nProvide status of progress to leads.\\nIn case of change requests, provide inputs on the plan.\\n\\nService Support and Maintenance:\\nSpecific to production and maintenance support: • Provide support for 1 week and hand over to production team.\\n\\nIdentify if the incoming request is a service request/ defect during the warranty or an incident.\\nIf it is code defect in the warranty, highlight to Lead and initiate defect fix process.\\nPost warranty, support in transition to maintenance team.\\n\\nKnowledge Management: • Post release participate in project review call and discuss points on what went well and what didn't.\\n\\nCreate and update knowledge articles (case studies, lessons learnt) in the knowledge management repository.\\nGuide developers in creating such documents.\\nPublish white papers/ blogs/ articles (if required).\\n\\nPeople Management: • Conduct training through academy or internally within the team.\\n\\nConduct technical, face to face interviews for internal transfer or external hiring.\\nProvide feedback on Developers form technical /domain standpoint to the module lead.\\n\\nTechnical Skills SNo Primary Skill Proficiency Level * Rqrd./Dsrd. 1 Snowflake PL1 Desired 2 Apache Spark PL3 Required 3 Amazon Web Services PL1 Required 4 Python PL1 Required 5 Core Java PL1 Desired\\n\\n\\nProficiency Legends Proficiency Level Generic Reference PL1 The associate has basic awareness and comprehension of the skill and is in the process of acquiring this skill through various channels. PL2 The associate possesses working knowledge of the skill, and can actively and independently apply this skill in engagements and projects. PL3 The associate has comprehensive, in-depth and specialized knowledge of the skill. She / he has extensively demonstrated successful application of the skill in engagements or projects. PL4 The associate can function as a subject matter expert for this skill. The associate is capable of analyzing, evaluating and synthesizing solutions using the skill.\\n\\nEmployee Status : Full Time Employee\\nShift : Day Job\\nTravel : No\\nJob Posting : Aug 28 2019\\n\\nAbout Cognizant Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 193 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @Cognizant.\\nCognizant is recognized as a Military Friendly Employer and is a coalition member of the Veteran Jobs Mission. Our Cognizant Veterans Network assists Veterans in building and growing a career at Cognizant that allows them to leverage the leadership, loyalty, integrity, and commitment to excellence instilled in them through participation in military service.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Senior Big Data Engineer / ML Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nParticipate in the design and development of a big data analytics application\\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\\nWrite complex ETL processes and frameworks for analytics and data management\\nDeveloping new processes and models\\nImplement large-scale near real-time streaming data processing pipelines\\nWork with a team of industry experts on cutting-edge big data technologies to develop solutions for deployment at massive scale</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5+ years of experience in Hadoop ecosystem\\n3+ years of hands-on experience in architecting, designing, and implementing data ingestion pipes for batch, real-time, and streams.\\n3+ years of hands-on experience with a proven track record in building data lakes on Public Clouds (preferably GCP and Azure (HDInsight))\\n3+ years of hands-on experience in Bigdata tools such as Sqoop, Hive, Spark, Scala, HBase, MapReduce etc.\\n2+ years of experience in Python and Scala.\\nReady to work onsite I the USA up to 3 months.\\nExperience in data wrangling, advanced analytic modeling, and AI/ML capabilities is preferred.\\nBA/BS required; preferably in Computer Science, Data Analytics, Data Science or Operations Research.\\nHighly analytical, motivated, decisive thought leader with solid critical thinking able to quickly connect technical and business ‘dots’.\\nHas strong communication and organizational skills and has the ability to deal with ambiguity while juggling multiple priorities and projects at the same time.\\nAble to understand statistical solutions and execute similar activities.</td>\n",
       "      <td>Responsibilities:\\nParticipate in the design and development of a big data analytics application\\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\\nWrite complex ETL processes and frameworks for analytics and data management\\nDeveloping new processes and models\\nImplement large-scale near real-time streaming data processing pipelines\\nWork with a team of industry experts on cutting-edge big data technologies to develop solutions for deployment at massive scale\\nRequirements:\\n5+ years of experience in Hadoop ecosystem\\n3+ years of hands-on experience in architecting, designing, and implementing data ingestion pipes for batch, real-time, and streams.\\n3+ years of hands-on experience with a proven track record in building data lakes on Public Clouds (preferably GCP and Azure (HDInsight))\\n3+ years of hands-on experience in Bigdata tools such as Sqoop, Hive, Spark, Scala, HBase, MapReduce etc.\\n2+ years of experience in Python and Scala.\\nReady to work onsite I the USA up to 3 months.\\nExperience in data wrangling, advanced analytic modeling, and AI/ML capabilities is preferred.\\nBA/BS required; preferably in Computer Science, Data Analytics, Data Science or Operations Research.\\nHighly analytical, motivated, decisive thought leader with solid critical thinking able to quickly connect technical and business ‘dots’.\\nHas strong communication and organizational skills and has the ability to deal with ambiguity while juggling multiple priorities and projects at the same time.\\nAble to understand statistical solutions and execute similar activities.\\nWill be a plus:\\nHaving hands-on experience in using Infoworks / Nifi will be an added advantage.\\nHaving experience in leading, guiding, and coaching data engineers is a plus.\\nHaving exposure to R and ML technologies is a plus.\\nWe offer:\\nOpportunity to work on bleeding-edge projects\\nWork with a highly motivated and dedicated team\\nCompetitive salary\\nFlexible schedule\\nBenefits program\\nSocial package - medical insurance, sports\\nCorporate social events\\nProfessional development opportunities\\nAbout us:\\nGrid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, scalable omnichannel services, DevOps, and cloud enablement.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Big Data Engineer II</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming\\nExperience in Big Data technologies (Hadoop, Spark, NiFi, Kafka)\\nAbility to demonstrate experience in distributed UNIX environments.\\nExperience in writing shell scripts\\nAbility to demonstrate proficiency in Microsoft Access, Excel, Word, PowerPoint and Visio.\\nAbility to multi-task and work under pressure.\\nAbility to be careful and thorough with detail.\\nAbility to work both independently and in a collaborative environment.\\nAbility to analyze information and use logic to address work related issues and problems.\\nExperience in the Healthcare Industry is a plus.</td>\n",
       "      <td>Command-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming\\nExperience in Big Data technologies (Hadoop, Spark, NiFi, Kafka)\\nAbility to demonstrate experience in distributed UNIX environments.\\nExperience in writing shell scripts\\nAbility to demonstrate proficiency in Microsoft Access, Excel, Word, PowerPoint and Visio.\\nAbility to multi-task and work under pressure.\\nAbility to be careful and thorough with detail.\\nAbility to work both independently and in a collaborative environment.\\nAbility to analyze information and use logic to address work related issues and problems.\\nExperience in the Healthcare Industry is a plus.</td>\n",
       "      <td>Design, implement, and test major subsystems of AWS or AZURE cloud platform and core service offerings using the Scrum agile framework\\nDevelop and follow best practices relative to design, implementation, and testing\\nPrototype new ideas or technologies to prove efficacy and usefulness in production\\nBuild a service structure on AWS or AZURE capable of being deployed and scaled to run a variety of platform components dynamically\\nBuild a next-generation tools platform for creating, managing and deploying multi-channel outreach campaigns in the AWS or AZURE cloud\\nConstruct a state-of-the-art data lake using Hadoop or Cassandra, Apache Spark, NiFi, and Kafka\\nDesign &amp; develop data pipelines for batch &amp; streaming data sets using ETL and Data Integration tools, open source, AWS &amp; Azure tech stack\\nMentor junior team members</td>\n",
       "      <td>The knowledge typically acquired during the course of attaining a Bachelor’s degree in Computer Science, Mathematics, or related discipline is required. A combination of education and experience may be used in lieu of a diploma.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are seeking a passionate and intellectually curious Big Data Engineer II for our Data Engineering team. Data Engineering team is responsible for creating data pipelines in big data space including data lake and data warehouse in AWS (Amazon Web Services), Azure cloud and on premise environments.\\n\\nEssential Responsibilities:\\nDesign, implement, and test major subsystems of AWS or AZURE cloud platform and core service offerings using the Scrum agile framework\\nDevelop and follow best practices relative to design, implementation, and testing\\nPrototype new ideas or technologies to prove efficacy and usefulness in production\\nBuild a service structure on AWS or AZURE capable of being deployed and scaled to run a variety of platform components dynamically\\nBuild a next-generation tools platform for creating, managing and deploying multi-channel outreach campaigns in the AWS or AZURE cloud\\nConstruct a state-of-the-art data lake using Hadoop or Cassandra, Apache Spark, NiFi, and Kafka\\nDesign &amp; develop data pipelines for batch &amp; streaming data sets using ETL and Data Integration tools, open source, AWS &amp; Azure tech stack\\nMentor junior team members\\nNon-Essential Responsibilities:\\nOther duties as assigned\\nQualifications : Knowledge, Skills and Abilities:\\nCommand-level knowledge of Java and Python programming, and the fundamentals of computer science, data structures and programming\\nExperience in Big Data technologies (Hadoop, Spark, NiFi, Kafka)\\nAbility to demonstrate experience in distributed UNIX environments.\\nExperience in writing shell scripts\\nAbility to demonstrate proficiency in Microsoft Access, Excel, Word, PowerPoint and Visio.\\nAbility to multi-task and work under pressure.\\nAbility to be careful and thorough with detail.\\nAbility to work both independently and in a collaborative environment.\\nAbility to analyze information and use logic to address work related issues and problems.\\nExperience in the Healthcare Industry is a plus.\\nWork Conditions and Physical Demands:\\nPrimarily sedentary work in a general office environment\\nAbility to communicate and exchange information\\nAbility to comprehend and interpret documents and data\\nRequires occasional standing, walking, lifting, and moving objects (up to 10 lbs.)\\nRequires manual dexterity to use computer, telephone and peripherals\\nMay be required to work extended hours for special business needs\\nMay be required to travel at least 10% of time based on business needs\\nMinimum Education:\\nThe knowledge typically acquired during the course of attaining a Bachelor’s degree in Computer Science, Mathematics, or related discipline is required. A combination of education and experience may be used in lieu of a diploma.\\nMinimum Related Work Experience:\\n2-4 years’ experience designing and delivering production software\\n2 years’ experience designing and implementing big data high performance operational systems\\nProven experience using the Microsoft development tools and stack, e.g., TFS, Github, Eclipse, JVM, etc\\nNothing in this job description restricts management’s right to assign or reassign duties and responsibilities to this job at any time.\\n\\nEOE including disability/veteran.\\nJob Posting : Aug 13, 2019, 4:09:21 PM\\nWork Locations : USA-Texas-Irving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AI Model Development Lead for Virtual Channels (Analytic Manager 5)</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview:\\nWells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.\\n\\nSUCCESS PROFILE\\nCheck out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.\\n\\nAnalytical\\nDetail-oriented\\nInsightful\\nInventive\\nProblem Solver\\nCurious\\nBenefits\\nWells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:\\n\\nMedical, Dental and Vision\\nEmployer Matching 401(k)\\nTuition Reimbursment\\nMaternity and Paternity Leave\\nPaid Time Off\\nResponsibilties\\nJob Description\\nAt Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.\\nHelp us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.\\nEnterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.\\nAs part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, Virtual Channels, and virtual channels is looking for an experienced AI leader to manage the development of AI models for Virtual Channels.\\n\\nThis leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on Virtual Channels’ AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and Virtual Channels executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.\\n\\nKEY RESPONSIBILITIES INCLUDE:\\nBuild and grow a team of data scientists responsible for AI model development in support of Virtual Channels\\nDesign, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions\\nPartner with Virtual Channels executives to frame the problem, manage the model development process, and business relationship\\nManage a portfolio of the data science projects including the following responsibilities:\\nHelp finalize project scope working with business partners\\nOn-going touch-base with business partners and governance stakeholders\\nDefine priorities in partnership with the business partners during on-going development\\nWork with AI technology and production teams to operationalize models\\nWill be called upon to review vendor models and solutions and/or models developed outside of EADS\\nDivisional Information:\\nData Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.\\nAs a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:\\nLead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.\\nAccomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.\\n\\nRequired Qualifications\\n\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques\\n\\n\\nDesired Qualifications\\n\\nStrong analytical skills with high attention to detail and accuracy\\nAbility to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members\\nMeeting facilitation experience in leading discussions that result in consensus and commitment\\n\\n\\nOther Desired Qualifications\\n4+ years managing or directing data scientist/ statistician/ data engineer teams\\nHands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet\\nDetail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable\\nExperience with agile project management methodologies for data science\\nExperience with Big Data or Hadoop tools such as Spark, Hive, Kafka and Map\\n\\nStreet Address\\nNC-Charlotte: 401 S Tryon St - Charlotte, NC\\nMN-Minneapolis: 600 S 4th St - Minneapolis, MN\\nNC-Charlotte: 11625 N Community House Road - Charlotte, NC\\nSC-Fort Mill: 3480 State View Blvd - Fort Mill, SC\\nTX-Addison: 5080 Spectrum Dr - Addison, TX\\nTX-DAL-Downtown Dallas: 1445 Ross Ave - Dallas, TX\\nTX-Irving: 5000 Riverside Drive - Irving, TX\\nAZ-Tempe: 1150 W Washington St - Tempe, AZ\\nIA-Des Moines: 6200 Park Ave - Des Moines, IA\\nIA-Des Moines: 800 Walnut St - Des Moines, IA\\nGA-Atlanta: 3579 Atlanta Ave - Atlanta, GA\\n\\n\\nDisclaimer\\n\\nAll offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.\\n\\nRelevant military experience is considered for veterans and transitioning service men and women.\\n\\nWells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.\\n\\nENT FINANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AI Model Development Lead For Marketing (Analytics Manager 5)</td>\n",
       "      <td>Addison, TX 75001</td>\n",
       "      <td>Addison</td>\n",
       "      <td>TX</td>\n",
       "      <td>75001</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques\\n4+ years of experience working with digital data science including analyzing cookie-level data</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview:\\nWells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.\\n\\nSUCCESS PROFILE\\nCheck out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.\\n\\nAnalytical\\nDetail-oriented\\nInsightful\\nInventive\\nProblem Solver\\nCurious\\nBenefits\\nWells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:\\n\\nMedical, Dental and Vision\\nEmployer Matching 401(k)\\nTuition Reimbursment\\nMaternity and Paternity Leave\\nPaid Time Off\\nResponsibilties\\nJob Description\\nAt Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.\\nHelp us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.\\nEnterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.\\nAs part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, marketing, and virtual channels is looking for an experienced AI leader to manage the development of AI models for marketing.\\n\\nThis leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on marketing’s AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and marketing executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.\\nKEY RESPONSIBILITIES INCLUDE:\\nBuild and grow a team of data scientists responsible for AI model development in support of marketing\\nDesign, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions\\nPartner with marketing executives to frame the problem, manage the model development process, and business relationship\\nManage a portfolio of the data science projects including the following responsibilities:\\nHelp finalize project scope working with business partners\\nOn-going touch-base with business partners and governance stakeholders\\nDefine priorities in partnership with the business partners during on-going development\\nWork with AI technology and production teams to operationalize models\\nMay be called upon to review vendor models and solutions and/or models developed outside of EADS\\nDivisional Information:\\nData Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.\\nAs a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:\\nLead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.\\nAccomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.\\n\\nRequired Qualifications\\n\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques\\n4+ years of experience working with digital data science including analyzing cookie-level data\\n\\n\\nDesired Qualifications\\n\\nStrong analytical skills with high attention to detail and accuracy\\nAbility to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members\\nMeeting facilitation experience in leading discussions that result in consensus and commitment\\n\\n\\nOther Desired Qualifications\\nData science experience in the ad tech space (DMPs, DSPs, Google and/or Adobe Cloud, digital attribution)\\n4+ years managing or directing data scientist/ statistician/ data engineer teams\\nHands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet\\nDetail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable\\nExperience with agile project management methodologies for data science\\n\\nDisclaimer\\n\\nAll offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.\\n\\nRelevant military experience is considered for veterans and transitioning service men and women.\\n\\nWells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.\\n\\nENT FINANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sr Security Data Engineer - Security Incident Response Team</td>\n",
       "      <td>Dallas, TX 75201</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75201</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Financial Services industry experiencePrevious work experience in Cyber Security field is a plus.Experience with the Hadoop eco-system (HDFS, Spark)Experience with cloud based big data platforms such as AWS or Google a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>MORE ABOUT THIS JOB\\nGoldman Sachs Technology Risk is leading threat, risk analysis and data science initiatives\\nthat are helping to protect the firm and our clients from information and cyber security risks. Our team equips the firm with the knowledge and tools to measure risk, identify and mitigate threats and protect against unauthorized disclosure of confidential information for our clients, internal business functions, and our extended supply chain.\\nSECURITY INCIDENT RESPONSE TEAM (SIRT) supports and enables a comprehensive technical Cyber Defense program for the firm while increasing awareness of current and potential Cyber Threats. Works across the organization to operate efficiently, provide technical\\ninvestigative support and mitigate threats to the firm.\\nDo you enjoy solving challenging puzzles? Protecting critical networks from cyber-attacks? Designing and integrating state-of-the-art technical solutions? A position as a Security Data Engineer on Goldman Sachs’ Threat Management Center lets you do all this and more:\\nRESPONSIBILITIES AND QUALIFICATIONS\\nHOW YOU WILL FULFILL YOUR POTENTIAL\\nDesign and develop data ingest and transform processesEngineer streaming data processing pipelinesDrive adoption of Cloud technology for data processing and warehousingEngage with data consumers and producers in order to design appropriate models to suit all needsApply latest technologies in machine learning, data mining, and predictive analytics to correlate the big datasets and events, and derive dynamic cybersecurity rules.Collaborate with a global team to continually operate and improve a world-class cyber program by driving the uplift of sensory tools, detection tuning, and access to data sources to increase detection effectiveness by applying data analytics.Participate in a 24x7 coverage model to prevent and remediate security threats against Goldman Sachs’ global business network.\\n\\nSKILLS AND EXPERIENCE WE ARE LOOKING FOR\\n5+ years of relevant work experience in a team-focused environmentBachelor’s degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)Working knowledge one or more programming languages (Python, Java, C++, C#, etc.)Extensive knowledge and proven experience applying domain driven design to build complex business applicationsDeep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processesIn-depth knowledge of relational and columnar SQL databases, including database designExcellent communications skills and the ability to work with subject matter experts to extract critical conceptsAbility to multi-task and prioritize work effectivelyIndependent thinker, willing to engage, challenge or learnHighly motivated self-starter who can provide thought leadership in big data analyticsStrong work ethic, a sense of ownership and urgencyStrong analytical and problem solving skillsResponsive to challenging tasking.Ability to document and explain technical details in a concise and understandable manner.Strong sense of ownership and driven to manage tasks to completion.\\n\\nPreferred Qualifications\\nFinancial Services industry experiencePrevious work experience in Cyber Security field is a plus.Experience with the Hadoop eco-system (HDFS, Spark)Experience with cloud based big data platforms such as AWS or Google a plus.\\nABOUT GOLDMAN SACHS\\nThe Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.\\n\\nÂ© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Senior Big Data Engineer</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nUndergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred\\n5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function\\nAbility to work with broad parameters in complex situations\\nExperience in developing, managing, and manipulating large, complex datasets\\nExpert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nResponsible for design, prototyping and delivery of software solutions within the big data eco-system\\nLeading projects and/or serving as analytics SME to provide new or enhanced data to the business\\nImproving data governance and quality increasing the reliability of our data\\nInfluencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Where good people build rewarding careers.\\nThink that working in the insurance field can’t be exciting, rewarding and challenging? Think again. You’ll help us reinvent protection and retirement to improve customers’ lives. We’ll help you make an impact with our training and mentoring offerings. Here, you’ll have the opportunity to expand and apply your skills in ways you never thought possible. And you’ll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.\\nAbout our team\\n360 Finance Advanced Analytics data engineering team works with multiple internal and external data sources to deliver data that is readily available, easily accessible, accurate and complete. They are responsible for building a centralized data lake/hub using the Hadoop ecosystem that will be used by Reporting &amp; Operational Analytics teams and the Machine learning teams.\\nJob Description\\nThis Lead Consultant is an experienced professional who is responsible for leveraging data and analytics to help automate and optimize Claims Analytics Data processes enabling our Claims employees to focus on serving our customers and delivering the most advanced claims experience on the planet. They will be responsible for the strategy around how we bring together complex data into clean and useful data structures making our valuable data more approachable.\\nKey Responsibilities\\nResponsible for design, prototyping and delivery of software solutions within the big data eco-system\\nLeading projects and/or serving as analytics SME to provide new or enhanced data to the business\\nImproving data governance and quality increasing the reliability of our data\\nInfluencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise\\nKey Responsibilities (Cont'd)\\nResponsible for designing and building new Big Data systems for turning data into actionable insights\\nTrain and mentor junior team members on Big Data/Hadoop tools and technologies\\nIdentifies opportunities for improvement and presents recommendations to management\\nSeeks out and evaluates emerging big data technologies and open-source packages\\nParticipate in strategic planning discussions with technical and non-technical partners\\nUses, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).\\nUses, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e. Java, Python, SQL, R)\\nJob Qualifications\\nUndergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred\\n5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function\\nAbility to work with broad parameters in complex situations\\nExperience in developing, managing, and manipulating large, complex datasets\\nExpert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.\\nJob Qualifications (Cont'd)\\nSome understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.\\nExperience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required\\n4-5+ years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required\\nExperience with Agile development methodologies and tools to iterate quickly on product changes, developing user stories and working through backlog (Continuous Integration and JIRA a plus)\\nExperience with Airflow a plus\\nUndergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred\\n5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function\\nExpert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.\\nThe candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.\\nGood Work. Good Life. Good Hands®.\\nAs a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.\\nLearn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.\\nAllstate generally does not sponsor individuals for employment-based visas for this position.\\nEffective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.\\nFor jobs in San Francisco, please click “here” for information regarding the San Francisco Fair Chance Ordinance.\\nFor jobs in Los Angeles, please click “here” for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.\\nTo view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs\\nTo view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.\\nIt is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Google Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum of 3 years previous Consulting or client service delivery experience on Google GCP\\n</td>\n",
       "      <td>DevOps on an GCP platform. Multi-cloud experience a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet today’s high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.\\n\\nBasic Qualifications\\nMinimum of 3 years previous Consulting or client service delivery experience on Google GCP\\nMinimum of 3 years of RDBMS experience\\nMinimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using GCP services etc:\\nData Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core\\nData Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore\\nStreaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam\\nData Warehousing &amp; Data Lake : BigQuery, Cloud Storage\\nAdvanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow &amp; Sheets\\nBachelors or higher degree in Computer Science or a related discipline.\\nAble to trval 100% M-TH\\n\\nCandidate Must Have Completed The Following Certifications\\nCertified GCP Developer - Associate\\nCertified GCP DevOps – Professional (Nice to have)\\nCertified GCP Big Data Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an GCP platform. Multi-cloud experience a plus.\\nExperience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion\\nIoT, event-driven, microservices, containers/Kubernetes in the cloud\\n\\nProfessional Skill Requirements\\nProven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience in developing, deploying and operating on large scale distributed systems on a commercial scale\\nExperience working in Cloud-based Big Data Infrastructure eg: Azure\\nGood working experience on Cloud, Delta Lake, ETL processing.\\nExperience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc.\\nWorking knowledge on Python and PySpark Programming.\\nWorking with a wide range of data sources like (DB2, SAP HANA etc) and intermediate expertise in SQL and PL/SQL(optional)\\nAbility to work with a global team, playing a key role in communicating problem context to the remote teams, stake holders and product owners.\\nWork in a highly agile environment\\nExcellent communication and teamwork skills.\\nKnowledge on Data Governance &amp; Security Principles\\n</td>\n",
       "      <td>Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.\\nThis role will be responsible for Architecture, Designing and implementing Advanced Analytics capabilities . These capabilities include Batch and Streaming Analytics, Machine learning models, Natural Language processing and Natural language generation and other emerging technologies in the field of Advanced Analytics.\\n\\nRequirements\\nExperience in developing, deploying and operating on large scale distributed systems on a commercial scale\\nExperience working in Cloud-based Big Data Infrastructure eg: Azure\\nGood working experience on Cloud, Delta Lake, ETL processing.\\nExperience in Big Data technologies like HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Spark, etc.\\nWorking knowledge on Python and PySpark Programming.\\nWorking with a wide range of data sources like (DB2, SAP HANA etc) and intermediate expertise in SQL and PL/SQL(optional)\\nAbility to work with a global team, playing a key role in communicating problem context to the remote teams, stake holders and product owners.\\nWork in a highly agile environment\\nExcellent communication and teamwork skills.\\nKnowledge on Data Governance &amp; Security Principles\\nBenefits\\nSignificant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>AWS Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.</td>\n",
       "      <td>DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:\\nProvide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.\\n- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)\\n\\nBasic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\n§ Certified AWS Developer - Associate\\n§ Certified AWS DevOps – Professional (Nice to have)\\n§ Certified AWS Big Data Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud\\nExperience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus\\n\\nProfessional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nCommercial experience leading on client-facing projects, including working in close-knit teams\\n5+ years of experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)\\n5+ years of experience working on projects within the cloud ideally AWS or Azure\\n5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent\\nExperience with open source tools like Apache Airflow and Griffin\\nExperience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform\\nData Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift\\nExperience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models\\nExperience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma\\nExperience building automated data quality and testing into data pipelines\\nExperience with AI, NLP, Machine Learning, etc. is a plus\\nStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R\\nExperience working on lively projects and a consulting setting, often working on different and multiple projects at the same time\\nExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.\\nA deep personal motivation to always produce outstanding work for your clients and colleagues\\nExcel in team collaboration and working with others from diverse skill-sets and backgrounds</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary:\\n\\nYou have experience with client projects and in handling vast amounts of data – working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Data Science team. Lead on projects from a data engineering perspective, working with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches\\nPlan and execute secure, good practice data integration strategies and approaches\\nAcquire, ingest, and process data from multiple sources and systems into Big Data platforms\\nCreate and manage data environments in the Cloud\\nCollaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models\\nHave a strong understanding of Information Security principles to ensure compliant handling and management of client data\\nThis is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science\\nQualifications:\\nCommercial experience leading on client-facing projects, including working in close-knit teams\\n5+ years of experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)\\n5+ years of experience working on projects within the cloud ideally AWS or Azure\\n5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent\\nExperience with open source tools like Apache Airflow and Griffin\\nExperience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform\\nData Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift\\nExperience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models\\nExperience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma\\nExperience building automated data quality and testing into data pipelines\\nExperience with AI, NLP, Machine Learning, etc. is a plus\\nStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R\\nExperience working on lively projects and a consulting setting, often working on different and multiple projects at the same time\\nExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.\\nA deep personal motivation to always produce outstanding work for your clients and colleagues\\nExcel in team collaboration and working with others from diverse skill-sets and backgrounds\\nCervello is a dynamic technology company that is focused on business analytics and planning. We take an innovative approach to making complex solutions simple so our clients can focus on running their businesses. Our services and applications enable our clients to gain the benefits of a world-class analytics and planning capability without the headaches.\\n\\n7E9OZvBK9m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Irving, TX 75038</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>75038</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>What you’ll be doing...\\nVerizon’s Data Analytics, Insights and Enablement team works with some of the largest data sets collected from the largest, more reliable network. We are responsible for providing fast, clean, and relevant data for Verizon’s network and field organizations as well as key measurements and insights about performance to front line employees serving our customers. You will work side by side with Verizon team members and other partners to develop next generation technologies to ensure our networks are available when our customers need them through complex, scalable data platforms, with ever growing and interesting challenges.\\nDesigning, architecting and developing data analytics systems for various use cases including but not limited to network performance and field operations.\\nParticipating and contributing in engineering lifecycle including writing production code, building end-to-end machine learning solutions, conduct code reviews and working closely with infrastructure teams\\nBuilding scalable systems to process petabytes of wireline and wireless data to provide real-time insights into the health of Verizon networks.\\nBuilding batch and real-time data pipelines to ingest and transform data for model training/ testing; and building and deploy model inference pipeline.\\nWhat we’re looking for...\\nYou will need to have:\\nBachelor's degree or four or more years of work experience.\\nSix or more years of relevant work experience.\\nEven better if you have:\\nA degree in Computer Science, Engineering or related discipline.\\nSix or more years of architectural design experience, preferably focused on Hadoop, Open Source and Big Data.\\nSolid Software Development skills with proficiency in Java and Python.\\nComputer science fundamentals in object-oriented design, data structures and algorithms and complexity analysis.\\nExperience working in distributed computing with Apache Hadoop, Apache Spark, Apache Druid and data ingestion frameworks like Apache Gobblin, Logstash, open source data connectors, and real-time data streaming services like Apache Kafka, Apache Flink and or Apache Storm/Pulsar.\\nExperience with Machine learning tools like TensorFlow, scikit-learn, Pandas, Kera’s, etc.\\nStrong experience with Big data processing tools like Hadoop/Hive/HBase/Oozie/HDFS/Yarn.\\nExperience implementing and monitoring big data pipelines and working in a large, complex devops and CICD environment.\\nExperience in production scale software development with ML/AI use cases.\\nStrong background in basic machine learning techniques including Anomaly Detection, Time-series, Deep Learning, supervised and unsupervised learning.\\nExperience in building scalable solutions using advanced analytics and machine learning techniques.\\nWorking in an Agile-team environment, with other data engineers, data scientist, ML engineers, etc.\\nExperience in feature engineering for both ML/DL models, A/B testing, data management and model governance.\\nExperience in productionizing ML/DL models with containers using Docker and Kubernetes.\\nNetwork domain knowledge.\\nWillingness to travel.\\nWhen you join Verizon...\\nYou’ll have the power to go beyond – doing the work that’s transforming how people, businesses and things connect with each other. Not only do we provide the fastest and most reliable network for our customers, but we were first to 5G - a quantum leap in connectivity. Our connected solutions are making communities stronger and enabling energy efficiency. Here, you’ll have the ability to make an impact and create positive change. Whether you think in code, words, pictures or numbers, join our team of the best and brightest. We offer great pay, amazing benefits and opportunity to learn and grow in every role. Together we’ll go far.\\nEqual Employment Opportunity\\nWe're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Senior Microsoft Azure Analysis Services Developer</td>\n",
       "      <td>Dallas, TX 75202</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75202</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description:\\nApplies specialized knowledge to conceptualize, design, develop, unit-test, configure, and implement portions of new or enhanced (upgrades or conversions) business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies.\\nResponsibilities:\\nCore team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives\\nDevelop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams\\nDocument requirements and translate into proper system requirements specifications using high-maturity methods, processes and tools.\\nDevelop visualization, user experience and configuration elements of solution design.\\nExecute and coordinate requirements management and change management processes.\\nParticipates as a member of and leads development teams.\\nDesigns, prepares and executes unit tests.\\nCompletes development to implement complex components.\\nDesigns solutions for others to develop.\\nParticipates in cross-functional teams.\\nLeads design activities and provides mentoring and guidance to developers.\\nDesigns, prepares and executes unit tests.\\nRepresents team to clients.\\nDemonstrates technical leadership and exerts influence outside of immediate team.\\nDevelops innovative team solutions to complex problems.\\nContributes to strategic direction for teams.\\nApplies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g. Azure Analysis Services, SQL Server, Power BI, and Power App development).\\nIntegrates technical expertise and business understanding to create superior solutions for clients.\\nConsults with team members and other organizations, clients and vendors on complex issues.\\nEducation and Experience Required:\\nTypically, a technical bachelor’s degree or equivalent experience and a minimum of 8 years of related experience or a master’s degree and a minimum of 6 years of experience.\\nKnowledge and Skills:\\nBuilding business intelligence solutions in a SQL Server environment\\nTabular and Multidimensional SSAS design and development\\nSenior to Expert level DAX/MDX, SQL\\nBuilding load specifications for cubes\\nPower BI on-Prem and Cloud experience\\nBuilding and modifying stored procedures, functions, creating tables and views\\nSQL Server Analysis Services (SSAS) and SQL Server Reporting Services (SSRS)\\nExperience in data movement analysis, validation and cube performance tuning\\nPassion for technology specifically related to data, dashboards, reports and analytics\\nWorking knowledge of star and snowflake schema(s)\\nStrong analytical skills and the ability to learn new technologies\\nAbility to meet tight deadlines\\nMust be a team player who loves to code and can thrive on a team that is mostly remote\\nPredictive analytics experience is a plus strong understanding of basic Database Administration.\\nAble to define quality and security standards.\\nGood verbal and written communication and negotiation skills.\\nGeneral project management/team leader skills.\\nAbility to work effectively in a globally dispersed team and with clients and vendors.\\nDemonstrated technical leadership skills.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Data Engineer 1</td>\n",
       "      <td>Dallas, TX 75201</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75201</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nStrong Database Development skills across multiple platforms\\nSuperior Interpersonal Skills: Ability to interface with a wide range of personalities and levels within Cvent and client organizations; Professional communication style\\nData Collection and Analysis: Proactive listening; resourceful in collecting sufficient data; Analysis of data to develop and implement best solution\\nInitiative: Self-starter with strong sense of ownership; Tenacity in problem solving with positive outcomes; Motivated to increase capacity and responsibility\\nDetailed Oriented: Detailed administrative skills for tracking and reporting</td>\n",
       "      <td>\\nDesign and implement database structures for OLAP and OLTP systems\\nDesign and implement ETL and ELT process to consolidate data across multiple systems\\nDesign and implement APIs, services, data transfers to internal and external systems\\nIdentify and resolve performance and security issues relating to data access and maintenance\\nDefine and enforce data design, security and performance standards\\nUnderstand and contribute to a corporate data model and overall data governance\\nCommunicate with application, back-office and external customer teams regarding data requirements, standards, performance and access\\nDefine and follow best practices for a full software development lifecycle involving data and database code.\\nDefine and perform unit testing of database code\\nContribute to the analysis and remediation of system behavior using tools like New Relic to understand application and process bottlenecks\\nPerform code reviews and audits of application team’s database code to ensure compliance with established best practices.\\nContribute to new technology evaluations and recommended usage\\nProvide on call support for database related issues affecting system or process availability.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelors degree in Computer Science, CIS or related field\\n4+ years’ experience with multiple databases (SQL Server, Oracle, Postgres, NoSQL)\\nExperience with ETL, ELT, Replication (SSIS, Informatica, GoldenGate)\\nExperience with Data Marts, Data Warehouses, Data Lakes\\nExperience with one or more reporting tools (Birst, Cognos, Business Objects)\\nExperience with Amazon, RedShift, Cloud\\n</td>\n",
       "      <td>Cvent is an exciting, fast-growing tech company that provides industry-leading software to event professionals around the world. Our suite of services – online event registration, venue selection, mobile apps, email marketing, web surveys, and targeted hotel advertising opportunities – have positioned us a major player in the estimated $565 billion global meetings and events industry.\\n\\nEssential Duties and Responsibilities\\n\\nDesign and implement database structures for OLAP and OLTP systems\\nDesign and implement ETL and ELT process to consolidate data across multiple systems\\nDesign and implement APIs, services, data transfers to internal and external systems\\nIdentify and resolve performance and security issues relating to data access and maintenance\\nDefine and enforce data design, security and performance standards\\nUnderstand and contribute to a corporate data model and overall data governance\\nCommunicate with application, back-office and external customer teams regarding data requirements, standards, performance and access\\nDefine and follow best practices for a full software development lifecycle involving data and database code.\\nDefine and perform unit testing of database code\\nContribute to the analysis and remediation of system behavior using tools like New Relic to understand application and process bottlenecks\\nPerform code reviews and audits of application team’s database code to ensure compliance with established best practices.\\nContribute to new technology evaluations and recommended usage\\nProvide on call support for database related issues affecting system or process availability.\\n\\n\\nJob Requirements\\n\\nBachelors degree in Computer Science, CIS or related field\\n4+ years’ experience with multiple databases (SQL Server, Oracle, Postgres, NoSQL)\\nExperience with ETL, ELT, Replication (SSIS, Informatica, GoldenGate)\\nExperience with Data Marts, Data Warehouses, Data Lakes\\nExperience with one or more reporting tools (Birst, Cognos, Business Objects)\\nExperience with Amazon, RedShift, Cloud\\n\\n\\nKey Skills &amp; Competencies\\n\\nStrong Database Development skills across multiple platforms\\nSuperior Interpersonal Skills: Ability to interface with a wide range of personalities and levels within Cvent and client organizations; Professional communication style\\nData Collection and Analysis: Proactive listening; resourceful in collecting sufficient data; Analysis of data to develop and implement best solution\\nInitiative: Self-starter with strong sense of ownership; Tenacity in problem solving with positive outcomes; Motivated to increase capacity and responsibility\\nDetailed Oriented: Detailed administrative skills for tracking and reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Data Engineer - Senior Consultant (Spark) - National</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.\\n\\nWe are...\\n\\n • The Industry-recognized data and analytics leaders\\n • Passionate problem solvers across a broad spectrum of technologies and industries\\n • Value seekers for measurable business outcomes\\n • Continuous learners through training and education\\n • Focused on a work-life balance with an unlimited paid time off policy\\n\\nData engineers are challenged with building the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally. Our engagements typically target a variety of use cases across data engineering, data science, data governance, and visualization.\\nData engineers deliver value through...\\nHands-on, self-directed design and development of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools\\n Demonstration of technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven insights\\nCollaboration with team members, business stakeholders and data SMEs to elicit requirements and to develop business metrics and analytical insights\\nInternal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO\\nA data engineer’s skills include, but are not limited to...\\nBachelors Degree and 5+ years of work experience\\n5+ years of professional IT work experience\\nSQL, SQL, SQL!\\n2+ years of Spark\\nProgramming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)\\nLinux / Windows (Command line)\\nBig Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop)\\nCloud Platforms (AWS, Azure, Google Cloud Platform)\\nData Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)\\nData Integration Tools (Talend, DataStage, Informatica, SSIS)\\nDatabases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)\\nMarkup Languages (JSON, XML, YAML)\\nCode Management Tools (Git/GitHub, SVN, TFS)\\nDevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)\\nTesting / Data Quality (TDD, unit, regression, automation)\\nSolving complex data and technology problems\\nLeading technical teams of 2+ consultants\\nAbility to design components of a larger implementation\\nExcellent communication to narrate data driven insights and technical approach\\n\\nIf this sounds like you, let’s talk!\\n\\nCandidates must be comfortable with a national travel model to client locations weekly (M-TH is typical).\\n\\nClarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.\\n\\nGLDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor in Computer Science, Data Science, Informatics, Engineering, or a related field.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Pieces Tech is currently in need of a Data Engineer to be responsible for building and scaling the next generation of the Pieces data engineering solutions to support our fast-growing artificial intelligent (AI) and analytics business in healthcare.\\n\\nMore specifically, this role will leverage the state-of-the-art data engineering technologies to develop and evolve the Pieces AI engine that delivers machine learning, natural language processing (NLP), and other AI and analytics based solutions in a highly scalable fashion. It involves the development of the following infrastructure and systems:\\n\\nAI data engineering infrastructure to efficiently support predictive modeling and analytics tasks on healthcare big data\\nAnalytics and Clinician-in-the-loop systems for AI-assisted chart review, rapid model development, explanatory model development, and model quality assurance (QA)\\nThis role is also responsible for exploratory work, improving and innovating, while focusing on delivering the required applications of relevance to Pieces Tech’s goals.\\n\\nAdditional roles and responsibilities include the following:\\n\\nDevelop and maintain data solution architecture (e.g. data ingestion modules, AI pipelines, ETL, data models, etc) in and around the Pieces AI engine (including NLP engine)\\nDevelop and maintain distributed computing infrastructures within and around Pieces AI engine that are horizontally scalable (e.g. Docker and Kubernetes)\\nDevelop and maintain relational databases (e.g. MySQL), distributed databases (e.g. Elasticsearch), and their data models that support both analytics and production pipelines\\nDevelop and maintain an efficient and reliable deployment process across all clients for both analytical model deployment and functional module deployment\\nImplement monitoring and QA layers for all existing and new components within and around Pieces AI engine\\nCollaborate with front-end developers, back-end developers, AI architect, and data scientists to build end-to-end AI systems\\nSupport general data analytics to answer business questions for product ideation, product development, and client engagement\\n Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\\n\\n\\nSome of the benefits that the successful candidate can expect are below:\\n\\n\\nOpportunity to work remotely\\nA knowledgeable, high-achieving, experienced and fun team\\nThe chance to be part of a rapidly growing startup and next success story\\nA competitive base salary\\nStructured career development\\n\\n\\n\\nRequired Skills\\nAbility to self-manage, work independently and meet deadlines\\nCritical thinking skills\\nWillingness to learn\\nTeamwork mentality and skills\\nStrong communication, interpersonal and analytical skills\\nTeam-based communication and project management tools (Google Doc, SmartSheet, JIRA, Confluence, Bitbuckets, etc)\\nProficient with Linux and MacOS operating system\\nProficient with programming languages including Python, Java, and SQL\\n\\n\\nRequired Experience\\nRequired Education\\n\\nBachelor in Computer Science, Data Science, Informatics, Engineering, or a related field.\\n\\n\\nPreferred Education\\n\\nMaster, or PhD degree in Computer Science, Data Science, Informatics, Engineering or equivalent.\\n\\n\\nRequired Experiences\\n\\n\\nAt least 3-year experience in full-stack software engineering\\nProven experience in data modeling in relational databases (e.g. MySQL, PostgreSQL, Snowflake, etc) and/or NoSQL databases (e.g. Elasticsearch, MongoDB, Columnar databases, etc)\\nProven experience with web service development using Spring (Spring Boot, Spring Data, Spring security, etc), REST, and JSON.\\nProven experience in developing container-based system (e.g. Docker and Kubernetes)\\n\\n\\nPreferred Experiences\\n\\nExperience in building distributed systems (e.g. MapReduce, Hadoop, or Spark)\\nExperience in front-end development (e.g. AngularJS, JavaScript, HTML5, etc)\\nExperience in developing systems on healthcare data (e.g. Epic, Meditech, Cerner, Allscripts, etc)\\nExperience in machine learning, deep learning, active learning, and/or transfer learning to solve practical AI problems in a primary or supporting role\\nExperience in developing NLP system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Sr. Data Engineer (Houston)</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Darby Consulting is looking for an experienced Sr. Data Engineer with Azure experience to support our client in the oil and gas industry. The Sr. Data Engineer will have a minimum of four years working with big data processing utilizing Azure Dev Ops, Azure Data Factory and Azure Data Warehouse in an Agile development environment. This position is offered to candidates based in the Houston-area on a 1099 Independent Contractor or W2-Hourly Employee basis. Excellent pay, relocation assistance, top-tier client, excellent work environment, long-term assignment and flexible work schedule are just a few of the many perks for this opportunity.\\n\\nWORK HOURS AND LOCATION\\nServices to be provided in Houston, TX during normal business hours (typically Monday through Friday from 8:00AM to 5:00PM, excluding holidays when the client office is closed).\\n\\nABOUT DARBY CONSULTING\\n\\nIf you've worked for a few consulting firms by now, you know there's a lot of \"great\" places to work. We've worked with many of them too. But, let's be honest: if everyone claims to be great, then great really is just the new average.\\n\\nOur goal is to build an amazing place to work. That means designing and building a company that goes beyond average. We want amazing. For us, Amazing is the opportunity to join a growing company of highly talented and experienced professionals. It's about learning and working alongside great people who care as much about you and your success as they do about their clients. Amazing is about flexibility, work-life balance and opportunities to deliver solutions for respected companies who value your knowledge, skills and experience. If you want great, there's a lot of companies out there. But if you want amazing, welcome to Darby Consulting!\\n\\nWHAT WE DO\\n\\nDarby Consulting is a full-service IT consulting firm specializing in IT project management, systems design and deployment of software and hardware-related projects. Supporting clients in the energy, government and education sectors, Darby helps growing organizations to maximize the value from their IT projects by integrating experienced and specialized IT project professionals, success-based methodology and cloud-based project management tools at affordable rates for growing organizations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Data Integration Engineer</td>\n",
       "      <td>Arlington, TX 76016</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>TX</td>\n",
       "      <td>76016</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nYou can consume and utilize new languages, design patterns, APIs and toolsets.\\nYou can work in a fast-paced and collaborative environment.\\nEffective communication skills and a willingness to learn are a must.\\nExperience with Test-Driven Development and writing unit and integration tests.\\nExperience using a Behavior-Driven Development suite like Cucumber.\\nCompetent writing software with JavaScript ecosystems like React.\\nComfortable working in a cloud environment like AWS.\\nMust have experience basic Linux/Unix CLI and using Git and GitHub for source code control.\\nKnowledge of Continuous Integration/Continuous Delivery systems like Jenkins.\\nKnowledge of Docker and Kubernetes is a plus.\\nExhibits enthusiasm and well-rounded knowledge of backend systems and software architecture.\\nApplies best practices including design patterns and linting to all software development.\\nApproaches engineering requests from a user's vantage point to form architectural and technical requirements.\\nStays well-informed of emerging technologies and software trends.\\nCapable of debugging problems related to HTTP, XHR, JSON, CORS, SSL, S3, etc.\\nAble to investigate performance and memory issues.\\nAble to reduce complex requirements and user interaction flows into long-term API designs.\\nGood understanding of architectural messaging patterns and pitfalls using Kafka, Rabbit MQ, etc.\\nEndeavors to establish positive relationships, both inter-departmentally, and cross-functionally.</td>\n",
       "      <td>\\nProficient with interviewing and gathering business requirements, definition and design of data source and data flow, data quality analysis, and working with the data architect on the development of logical data models. Proficient using Infosphere/DataStage or equivalent ETL software.\\nProficient with relational databases and using SQL to query, create tables, views, indexes, joins\\nProficient using Unix and applicable scripting/scheduling tools\\nExperience with the SDLC, ITSM and privacy and security concepts.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology required upon hire\\nMaster's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology preferred</td>\n",
       "      <td>\\n3 Years Working as a Data Integration Engineer or Data Integration Production Support Engineer with a Bachelor's degree is required.\\nExperience working in a healthcare or related field\\nTeradata Database Experience\\nExperience upgrading IBM Infosphere Tools</td>\n",
       "      <td>Currently seeking Data Integration Engineers located in or near Arlington, TX. As a data engineer, you'll be handling the design and construction of scalable systems, ensure that all data systems meet company requirements, and also research new uses for data acquisition. You should also know the ins and outs of the industry such as data mining practices, algorithms, and how data can be used.\\n\\nRequired Skills\\nProficient with interviewing and gathering business requirements, definition and design of data source and data flow, data quality analysis, and working with the data architect on the development of logical data models. Proficient using Infosphere/DataStage or equivalent ETL software.\\nProficient with relational databases and using SQL to query, create tables, views, indexes, joins\\nProficient using Unix and applicable scripting/scheduling tools\\nExperience with the SDLC, ITSM and privacy and security concepts.\\n\\nQualifications\\nYou can consume and utilize new languages, design patterns, APIs and toolsets.\\nYou can work in a fast-paced and collaborative environment.\\nEffective communication skills and a willingness to learn are a must.\\nExperience with Test-Driven Development and writing unit and integration tests.\\nExperience using a Behavior-Driven Development suite like Cucumber.\\nCompetent writing software with JavaScript ecosystems like React.\\nComfortable working in a cloud environment like AWS.\\nMust have experience basic Linux/Unix CLI and using Git and GitHub for source code control.\\nKnowledge of Continuous Integration/Continuous Delivery systems like Jenkins.\\nKnowledge of Docker and Kubernetes is a plus.\\nExhibits enthusiasm and well-rounded knowledge of backend systems and software architecture.\\nApplies best practices including design patterns and linting to all software development.\\nApproaches engineering requests from a user's vantage point to form architectural and technical requirements.\\nStays well-informed of emerging technologies and software trends.\\nCapable of debugging problems related to HTTP, XHR, JSON, CORS, SSL, S3, etc.\\nAble to investigate performance and memory issues.\\nAble to reduce complex requirements and user interaction flows into long-term API designs.\\nGood understanding of architectural messaging patterns and pitfalls using Kafka, Rabbit MQ, etc.\\nEndeavors to establish positive relationships, both inter-departmentally, and cross-functionally.\\n\\nRequirements\\n3 Years Working as a Data Integration Engineer or Data Integration Production Support Engineer with a Bachelor's degree is required.\\nExperience working in a healthcare or related field\\nTeradata Database Experience\\nExperience upgrading IBM Infosphere Tools\\n\\nEducation\\nBachelor's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology required upon hire\\nMaster's Degree in Business Administration, Information Science, Computer Science, Computer Engineering, or Information Technology preferred\\n3xW99DwBQZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Datalitical Inc is one largest consulting firms in U.S and with lots of engagements. We are a diverse community of people, all working together to bring high-end solution to any business problem and provide the best service to our clients. We’re looking for talented individuals who want to work in an energetic, respectful, collaborative environment. With a wide array of jobs, internships, training and more, there are countless opportunities for you to grow your career with us.\\n\\nResponsibilities:\\nOwner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth at Datalitical Consistently evolve data model &amp; data schema based on business and engineering needs Implement systems tracking data quality and consistency Develop tools supporting self-service data pipeline management (ETL) SQL and MapReduce job tuning to improve data processing performance\\nExperience &amp; Skills:\\nExtensive experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet) Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle) Good understanding of SQL Engine and able to conduct advanced performance tuning Strong skills in scripting language (Python, Ruby, Perl, Bash) Experience with workflow management tools (Airflow, Oozie, Azkaban, UC4) Comfortable working directly with data analytics to bridge business requirements with data engineering\\nBonus:\\nMPP database experience (Redshift, Vertica, Teradata) Experience with building tools to support self-service pipeline Experience with one of the messaging system (Kafka, SQS, Kinesis) and different data serialization (json, protobuf, avro)\\n\\nOur corporate office supports and offers a competitive benefits package including medical/dental/vision, term life insurance, paid vacation/holidays, 401(k) savings plan with company match.\\n\\nApply: hr@datalitical.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Senior Data Engineer, Products</td>\n",
       "      <td>Farmers Branch, TX 75244</td>\n",
       "      <td>Farmers Branch</td>\n",
       "      <td>TX</td>\n",
       "      <td>75244</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Lead a technical team to rapidly architect, design, prototype, and implement and optimize architectures to tackle the Big Data and Data Science problems.Design, maintain and oversee the operational process to develop modular code base to solve “real” world problems.Conduct regular peer code reviews to ensure code quality and compliance following best practices in the industry.Work in cross-disciplinary teams to understand client needs and ingest rich data sources.Research, experiment, and utilize leading Big Data methodologies (Hadoop, Spark, Kafka, Netezza, Snowflake, and AWS) with cloud/on premise/hybrid hosting solutions.Oversee a technical team to provide proficient documentation and operating guidance for users of all levels.Lead a technical team to architect, implement, and test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings (AWS, Azure, client technology stacks, and on-prem clusters)Translate advanced business analytics problems into technical approaches that yield actionable recommendations across multiple, diverse domains; Communicate results and educate others through design and build of insightful visualizations, reports, and presentations.Develop skills in business requirement capture and translation, hypothesis-driven consulting, work stream and project management and client relationship developmentHelp drive the process for pursuing innovations, target solutions, and extendable platforms for Merkle’s products.Participate in developing and presenting thought leadership, and assist in ensuring that Merkle’s “data source” technology stack incorporates and is optimized for using specific technologies.Promote the Merkle brand in the broader “data source” community.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\n\\nWe are seeking a highly motivated, and technically proficient Data Engineer to work on architecting, designing, building and managing data products on \"big data\" infrastructure.\\nYou will expand upon your current skill set through cross-disciplinary collaboration with some of the smartest and nicest people in the industry, while learning the inner workings of a fast-paced global performance marketing agency. You will work closely with a team of engineers in an agile development environment to expand our proprietary marketing products.\\nAdditionally, as a Data Engineer, you’ll play an integral role in the growth of our team. You will assist in reviewing complicated and mission-critical designs, code, and tests; and documenting ways to improve our current codebase and system processes using the latest technologies.\\nKey ResponsibilitiesLead a technical team to rapidly architect, design, prototype, and implement and optimize architectures to tackle the Big Data and Data Science problems.Design, maintain and oversee the operational process to develop modular code base to solve “real” world problems.Conduct regular peer code reviews to ensure code quality and compliance following best practices in the industry.Work in cross-disciplinary teams to understand client needs and ingest rich data sources.Research, experiment, and utilize leading Big Data methodologies (Hadoop, Spark, Kafka, Netezza, Snowflake, and AWS) with cloud/on premise/hybrid hosting solutions.Oversee a technical team to provide proficient documentation and operating guidance for users of all levels.Lead a technical team to architect, implement, and test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings (AWS, Azure, client technology stacks, and on-prem clusters)Translate advanced business analytics problems into technical approaches that yield actionable recommendations across multiple, diverse domains; Communicate results and educate others through design and build of insightful visualizations, reports, and presentations.Develop skills in business requirement capture and translation, hypothesis-driven consulting, work stream and project management and client relationship developmentHelp drive the process for pursuing innovations, target solutions, and extendable platforms for Merkle’s products.Participate in developing and presenting thought leadership, and assist in ensuring that Merkle’s “data source” technology stack incorporates and is optimized for using specific technologies.Promote the Merkle brand in the broader “data source” community.\\n\\nQualifications\\n\\nQualified individuals possess the Merkle attributes of being smart, curious, committed to vision, passionate, fun/pleasant, an achiever and having a sense of urgencyMinimum of ten years of big data experience with multiple programming languages and technologies, three years as a lead and three years at a management level with minimum five years of big data experience.Bachelor's degree or Master's degree from an accredited college/university in Computer Science, Computer Engineering, or related field(i.e. math and physics);Ability to manage complex engagements and interface with senior level management internally as well as with clients.Ability to communicate complex technical concepts succinctly to non-technical colleagues, understand &amp; manage interdependencies between all facets of a project.Ability to lead client presentations; Must have demonstrated advanced proficiency in complex, mature and sophisticated Design &amp; Analysis technologies and solutions.Ability to mentor others and publish whitepapers or articles on complex D&amp;A technologies or solutions.Market-leading proficiency with multiple large scale and/or distributed processing methodologies (Hadoop, Storm, Spark).Skilled ability to rapidly ingest, transform, engineer, and visualize data, both for ad hoc and product-level (e.g., automated) data &amp; analytics solutions.Market-leading fluency in several programming languages (Python, Scala, or Java), with the ability to pick up new languages and technologies quickly.Understanding of cloud and distributed systems principles (such as load balancing, networks, scaling, in-memory vs. disk).Experience with large-scale, big data methods (MapReduce, Hadoop, Spark, Hive, Impala, or Storm, SnowFlake) and AWS solutions (EC2, S3, RDS, EMR, Kinesis, DynamoDB, Redshift).Experience storing, managing, and processing massive data sets using tools such as Hadoop, Apache Spark, AWS EMR, Hive, SnowFlake or the like.Ability to work efficiently under Unix/Linux environment and .NET, having experience with source code management systems like GIT and SVN.Strong knowledge with programming methodologies (version control, testing, QA) and development methodologies (Waterfall and Agile).Experience with object-oriented design, coding, and testing patterns as well as experience in engineering (commercial or open source) software platforms and large-scale data infrastructures.Familiarity with different architecture patterns of development such as Event Driven, SOA, micro services, functional programming, Lambda.Capability to architect highly scalable distributed systems, using different open source tools.Knowledge of traditional and digital data-driven marketing.\\nAdditional Information\\n\\nAll your information will be kept confidential according to EEO guidelines. At Merkle, we believe that a diverse environment improves us as a community and as a business. We want to foster an environment of growth, where all ideas and contributions are encouraged. We need this culture of courage to continue to thrive in our fast-paced industry. We embrace differences of opinion. We value diversity of experience and thought, which help us to challenge and define industry-leading solutions, and support our goal of being a great place to work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Multiple Bigdata Job Positions across Dallas, TX and Tampa, FL\\nPosition 1: Big Data Engineer\\nLocation: Dallas TX\\nExperience Required - 6+ years\\nJob Description\\nHadoop/HDFS.\\nSpark is a must. Scala preferred but Java is ok too.\\nExperience Spark core, Spark SQL, Spark Streaming ( these 3 are a must) and Spark ML is good to have.\\nPosition 2: Sr. Big Data Engineer\\nExperience Required - 8-15 years\\nLocation: Tampa, FL\\nJob Description\\nThe client is looking for a senior engineer who can drive things rather than being managed by the client.\\nHadoop/Hive and Kafka.\\nSpark streaming using Java is a must.\\nPosition 3: Lead Big Data Engineer\\nLocation: Dallas TX\\nExperience Required - 8-15 years\\nJob Description\\nPrimary requirement - Spark, Scala developer with knowledge of Kafka.\\nGood to have exposure to other NoSQL databases like Casandra.\\nAWS experience will be a definite plus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Dallas, TX 75244</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75244</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions.\\nAt least 2 years of experience with Big Data technologies.\\nAt least 2 years of coding experience in data environments.\\n3+ years design &amp; implementation experience with distributed applications.\\n2+ years of experience in database architectures and data pipeline development.\\nStrong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.\\nExperience in manipulating multiple, complex and large data sources.\\nExperienced in Data Science, statistical models as a plus.\\nExperience in Designing, implementing and maintaining SQL Server databases.\\nExperience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.\\nExperience in SQL query tuning and optimization.\\nExperience working in SaaS, IaaS, and PaaS.\\n\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions.\\nAt least 2 years of experience with Big Data technologies.\\nAt least 2 years of coding experience in data environments.\\n3+ years design &amp; implementation experience with distributed applications.\\n2+ years of experience in database architectures and data pipeline development.\\nStrong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.\\nExperience in manipulating multiple, complex and large data sources.\\nExperienced in Data Science, statistical models as a plus.\\nExperience in Designing, implementing and maintaining SQL Server databases.\\nExperience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.\\nExperience in SQL query tuning and optimization.\\nExperience working in SaaS, IaaS, and PaaS.\\n\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Basic Function:\\n\\nKomen is seeking a Data Engineer who lives and breathes data, sweats the details, deeply cares about data quality, data flows, integration, ETL, storage &amp; performance. The Data Engineer will be creating data pipelines and process data sets that are available within Komen covering constituents, campaigns, research and patient domains.\\n\\nPrimary Responsibilities:\\n\\nBuilding and maintaining data processing workflows feeding our analytics, CRM and various other internal applications\\n\\nâ€¢ Develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organization\\nâ€¢ Responsible for design, development and implementation of optimal solutions to integrate, store, process and analyze huge data sets\\nâ€¢ Recommend and implement strategies for bi-directional synchronization between sourcing data repositories and the central normalized data repository\\nâ€¢ Build data pipeline frameworks to automate high-volume and real-time data delivery\\nâ€¢ Build data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners\\nâ€¢ Work on multiple projects/tasks simultaneously to meet project deadlines for self and others as required.\\nâ€¢ Establish and maintain positive working relationships with other employees\\nâ€¢ All other duties as assigned.\\n\\nPosition Qualifications:\\n\\nThe ideal candidate will have a Bachelor's Degree in Computer Science or Math and 7-10 years of directly related experience that includes:\\n3+ years of experience in SQL and developing SQL server objects e.g., store procedures, tables, triggers, views and functions.\\nAt least 2 years of experience with Big Data technologies.\\nAt least 2 years of coding experience in data environments.\\n3+ years design &amp; implementation experience with distributed applications.\\n2+ years of experience in database architectures and data pipeline development.\\nStrong working and conceptual knowledge of reporting and visualization tools such as SSRS, PowerBI, Tableau, or other business intelligence tools.\\nExperience in manipulating multiple, complex and large data sources.\\nExperienced in Data Science, statistical models as a plus.\\nExperience in Designing, implementing and maintaining SQL Server databases.\\nExperience in Designing, implementing and maintaining ETL processes using SQL Server SSIS.\\nExperience in SQL query tuning and optimization.\\nExperience working in SaaS, IaaS, and PaaS.\\n\\nIn addition to the minimum qualifications above, the successful candidate should have:\\nStrong working and conceptual knowledge of building and maintaining physical and logical data models.\\nStrong project management, business writing, communication and presentation skills.\\nAbility to work cross-functionally within the organization.\\nFamiliarity with or experience working in Agile Scrum software development teams.\\nAbility to multi-task and maintain flexibility and creativity in a variety of situations.\\nAbility to analyze and resolve problems.\\nAbility to set and meet goals and consistently meet deadlines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a Lead Data Engineer, here's what we'll be looking for you to bring:\\n\\nHands-on Engineering Leadership\\nProven track record of Innovation and expertise in Data Engineering\\nTenure in coding, architecting and delivering complex projects\\nDeep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others\\nDeep understanding of streaming data architectures and technologies for real-time and low-latency data processing\\nDeep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies\\nUnderstanding of how to architect solutions for data science and analytics such as productionizing machine learning models and collaborating with data scientists\\nUnderstanding of agile development methods including: core values, guiding principles, and key agile practices\\nUnderstanding of the theory and application of Continuous Integration/Delivery\\nPassion for software craftsmanship\\nA rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..\\nStrong stakeholder management and interaction experience at different levels\\n\\n\\nThere's no typical day or engagement for our Senior Data Engineers. Here’s what you’ll do:\\n\\nBe the SME. Develop modern data architectural approaches to meet key business objectives and provide end to end data solutions\\nYou might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems.\\nOn other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.\\nIt could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.\\nWhatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.\\nYou have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.\\nYou recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.\\nA few important things to know:\\nProjects are almost exclusively on customer site, so candidates should be flexible and open to extensive travel.\\n\\nCandidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.\\n\\nNot quite ready to apply? Or maybe this isn’t the right role for you? That’s OK, you can stay in touch with AccessThoughtWorks, our learning community (click \"contact me about recruitment opportunities\" to hear about jobs in the future).\\n\\nIt is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Azure Data Engineer</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of consulting or client service delivery experience on Azure\\n</td>\n",
       "      <td>DevOps on an Azure platform</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\n</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n People in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications\\n\\n Role &amp; Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers successfully delivering work efforts\\n\\n (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nBasic Qualifications\\nAt least 5 years of consulting or client service delivery experience on Azure\\nAt least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions\\nExtensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.\\nExtensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.\\n Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.\\n5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.\\nMinimum of 5 years of RDBMS experience\\nExperience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nMCSA Cloud Platform (Azure) Training &amp; Certification\\nMCSE Cloud Platform &amp; Infratsructiure Training &amp; Certification\\nMCSD Azure Solutions Architect Training &amp; Certification\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nIoT, event-driven, microservices, containers/Kubernetes in the cloud\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\nFamiliarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\n- Multi-cloud experience a plus - Azure, AWS, Google\\n\\nProfessional Skill Requirements\\n Proven ability to build, manage and foster a team-oriented environment\\n Proven ability to work creatively and analytically in a problem-solving environment\\n Desire to work in an information systems environment\\n Excellent communication (written and oral) and interpersonal skills\\n Excellent leadership and management skills\\n Excellent organizational, multi-tasking, and time-management skills\\n Proven ability to work independently\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Dallas-Fort Worth, TX</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>SUMMARY:\\nThe Senior Data Engineer is responsible for designing, developing, testing, tuning, and deploying Extract, Transformation, and Load (ETL) processes and API's for a moderate number of projects related to below:\\n\\nDatasets within the Oncology reporting environment, and/or Data sources within the core data model of the Enterprise Data Warehouse (EDW), and Subject area data marts within a defined CMS defined Clinical Program.\\n\\nRESPONSIBILITIES:\\nAcquire and interpret business requirements, create technical artifacts and determine most efficient design solution.\\nExperience working in large data warehouse. Working knowledge of Master Data Management, Metadata management, Data Architecture, Data Governance and Data Quality.\\nCombined experience of advanced SQL and PL/SQL (in Oracle, SQL Server Database Platforms) with hands-on experience of designing solutions.\\nPrepares clear and complete complex specifications, designs, test plans, and operations documentation from which ETL processes are developed.\\nParticipates in teams to define design and development standards and processes\\nMonitors production processes using enterprise scheduler tools and troubleshoots incidents surrounding supported solutions, including after-hours escalations of major incidents.\\nConfiguration of Staging, Base Objects Tables, Look-ups, Cleanse Lists, Cleanse functions, Mappings, Audit trail/ Delta detection and Develop ETL Mappings to move the data loaded from stage to downstream applications.\\nParticipates in work group sessions with users and other project stakeholders and actively contributes to the definition of requirements, data analysis to define reporting objectives and/or metric benchmarks, and review of in progress solution development.\\nUnderstands and applies HIPAA standards and regulations to all areas of work.\\nDevelops expertise with current ETL technologies, trends, and best practices.\\nAdheres to standards, methodologies, and processes. Identifies possible improvements and champions revisions with appropriate standards owner.\\nExperience in designing, implementing and operationalizing data solutions in cloud production environments Azure using one or more relevant technologies such as Azure Data Lake is big plus\\nExperience working in SAFE Agile or Scrum based environment and Lead development efforts using Agile/Scrum based methodologies\\nQUALIFICATIONS:\\nBachelor's degree.\\nDemonstrated experience in creating medium to large-scale logical data models\\nExperience building data marts and business intelligence solutions\\nWorked on one or more of the following databases: Oracle, SQL Server, MySQL\\nStrong experience using at least one of the top data modeling tools, such as ERwin, ER/Studio or Oracle data modeler\\nFamiliarity with open source data pipeline and transformation technologies\\nStrong leadership and ability to work and influence cross-functional teams\\nExperience authoring technical documents and working with teams to drive reference implementation to production scale\\nFor Internal Use: Career Level P3\\nMcKesson is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, national origin, disability, or protected Veteran status.Qualified applicants will not be disqualified from consideration for employment based upon criminal history.McKesson is committed to being an Equal Employment Opportunity Employer and offers opportunities to all job seekers including job seekers with disabilities. If you need a reasonable accommodation to assist with your job search or application for employment, please contact us by sending an email to Disability_Accommodation@McKesson.com. Resumes or CVs submitted to this email box will not be accepted.Current employees must apply through internal career site.Join us at McKesson!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Senior Big Data Data Engineer</td>\n",
       "      <td>Irving, TX 75038</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>75038</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>What you’ll be doing...\\nLooking for a Senior Data Engineer to work with a small team responsible for building, deploying, and supporting a Big Data solution that will enable operations for a large enterprise environment. Must be able to design, build and maintain Enterprise Level Data Pipe-Lines utilizing the tools available within Big Data Eco-System. As a Senior Big Data Engineer - You will work on Advanced Analytics using Big Data technologies such as Hadoop and Data Warehousing.\\nBuild analytical solutions to enable Data Scientist by manipulating large data sets and integrating diverse data sources.\\nPerform ad-hoc analysis and develop reproducible analytical approaches to meet business requirements.\\nPerform exploratory and targeted data analyses using descriptive statistics and other methods.\\nUse complex algorithms to develop systems &amp; applications that deliver business functions or architectural components.\\nTypical duties will include the following:\\nWork closely with the data scientists, and database and systems administrators to create data solutions.\\nFollow best practices on design and implementation to aid in company-wide data governance.\\nImprove existing data pipelines by simplifying and increasing performance.\\nDesign, build, and deploy new data pipelines within Big Data Eco-Systems.\\nDocuments new/existing pipelines, Data Sets and Data Sets.\\nAbides by department development standards and SOP's.\\nAttends all department meetings.\\nAll other duties as assigned.\\nKeeps updated on latest technologies relevant to position’s duties.\\nHas great knowledge of commonly used software concepts and design.\\nGreat knowledge of the development lifecycle.\\nKeeps management updated on projects and assigned work.\\nWhat we’re looking for...\\nYou will need to have:\\nBachelor’s degree or four or more years of work experience.\\nSix or more years of relevant work experience.\\nEven Better If You Have:\\nA Degree.\\nExperience of designing, building, and deploying production-level data pipelines using tools from Hadoop stack (HDFS, Hive, Spark, HBase, Kafka, NiFi, Oozie, Splunk etc).\\nExperience with various noSQL databases (Hive, MongoDB, Couchbase, Cassandra, and Neo4j).\\nExperience with analytic or feature engineer programming (python or scala or java).\\nExperience implementing open source frameworks &amp; exposure to various open source &amp; package software architectures (AngularJS, ReactJS, Node, Elasticsearch, Spark, Scala, Splunk, Apigee, and Jenkins etc.).\\nExperience troubleshooting JVM-related issues.\\nExperience with SQL databases and Change Data Capture.\\nExperience and strategies to deal with mutable data in Hadoop.\\nExperience with Stream sets.\\nExperience of Agile and DevOps methodologies.\\nExperiencewithjudgment to plan and accomplish goals.\\nWorks under general supervision.\\nExperience in full development life cycle and significant experience in delivering applications and architecture services.\\nExperience in data visualization tools like Kibana, Grafana, Tableau and associated architectures.\\nExperience evaluating and implementing cutting-edge digital technologies.\\nExperience with Cloud technologies (AWS, GCP, PCF, Docker, Kubernetes and application migration.\\nWhen you join Verizon...\\nYou’ll have the power to go beyond – doing the work that’s transforming how people, businesses and things connect with each other. Not only do we provide the fastest and most reliable network for our customers, but we were first to 5G - a quantum leap in connectivity. Our connected solutions are making communities stronger and enabling energy efficiency. Here, you’ll have the ability to make an impact and create positive change. Whether you think in code, words, pictures or numbers, join our team of the best and brightest. We offer great pay, amazing benefits and opportunity to learn and grow in every role. Together we’ll go far.\\nEqual Employment Opportunity\\nWe're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Product Designer/Data Architect</td>\n",
       "      <td>Plano, TX 75023</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>75023</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Who we are\\n\\nCollaborative.\\nRespectful. A place to dream and do. These are just a few words that describe\\nwhat life is like at Toyota. As one of the world’s most admired brands, Toyota\\nis growing and leading the future of mobility through innovative, high-quality\\nsolutions designed to enhance lives and delight those we serve. We’re looking\\nfor diverse, talented team members who want to Dream. Do. Grow. with us.\\nAn important part of the Toyota family is Toyota Financial\\nServices (TFS), the finance and insurance brand for Toyota and Lexus in North\\nAmerica. While TFS is a separate business entity, it is an essential part of\\nthis world-changing company – delivering on Toyota’s vision to move people\\nbeyond what’s possible. At TFS, you will help create best-in-class customer\\nexperiences in an innovative, collaborative environment.\\n\\nWho we’re looking for\\n\\nThe\\nTFS IDS Department is looking for a passionate and highly-motivated Product Designer. The role\\nis\\nprimarily responsible for the TFS enterprise data warehouse architecture and\\ndevelopment lifecycle process that translates business requirements into data\\nsolutions aligned to a Factory or and DIO’s Application Portfolio or specific\\nportion of the application portfolio.\\n\\n\\n\\n\\nWhat you’ll be doing\\nCreate and manage conceptual, logical and physical data models using iDERA\\nEmbarcadero ER/Studio suite\\n Implement physical data models in Snowflake, SQL Server, Postgres, AWS\\nRedshift, and NoSQL platforms (MongoDB)\\nParticipate as an active member of the team to shape Data Architecture vision,\\nstandards and guidelines\\n Design and Implement data devops processes for code deployment and\\nsynchronization across environments\\nConduct code reviews and provide constructive feedback on database changes\\nfrom product developers and engineers\\nProvide technical leadership and mentoring on database technologies and\\nissues to product designers and engineers\\nWhat you bring\\n\\nMultiple years of experience working with Financial Services organization with emphasis on Captive Auto Finance where the potential candidate has working knowledge of\\nNeed to have substantial experience BigData/ cloud ecosystem (e.g., Hadoop, Data Lake)\\nSkills with iDERA - Embarcadero ER/Studio suite, Erwin, or other data modeling tool for OLTP systems\\nExpert in data modeling for relational and document-based data stores, including conceptual, logical, and physical data models (ability to build from scratch)\\nExperience with various data integration patterns across a variety of sources\\nExperience working with terabyte data sets, billion+ row tables, and high transaction volumes in an OLTP\\nExperience with distributed systems and cloud data platforms\\nAs a Big Data Engineer You have experience designing, developing and maintaining systems at scaleAs a Big Data Engineer you will be responsible for the continued development of the data pipeline\\n\\nWhat we’ll bring\\n\\n\\nDuring your interview process, our team can fill you in on all the details of our industry-leading benefits and career development opportunities. A few highlights include:\\n\\n\\nA work environment built on teamwork, flexibility and respect\\nProfessional growth and development programs to help advance your career, as well as tuition reimbursement\\nVehicle purchase &amp; lease programs\\nComprehensive health care and wellness plans for your entire family\\nFlexible work options based on business needs\\nToyota 401(k) Savings Plan featuring a company match, as well as an annual retirement contribution from Toyota regardless of whether you contribute\\nPaid holidays and paid time off\\nReferral services related to prenatal services, adoption, child care, schools and more\\nFlexible spending accounts\\nRelocation assistance\\nOnsite amenities such as fitness center, restaurants, etc.\\nWhat you should know\\n\\nOur success begins and ends with our people. We embrace diverse perspectives and value unique human experiences. We are proud to be an equal opportunity employer that celebrates the diversity of the communities where we live and do business. Applicants for our positions are considered without regard to race, ethnicity, national origin, sex, sexual orientation, gender identity or expression, age, disability, religion, military or veteran status, or any other characteristics protected by law.\\n\\nHave a question or need assistance\\nwith your application? Check out the How to Apply section of our\\ncareers page on Toyota.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Cloud Engineer</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s Degree in Computer Science or related area, or equivalent work experience.\\nBuilding out a scalable multi-tenant AWS private and public cloud environment that meets the architectural requirements of the cloud platform\\nFamiliar with software development to drive data &amp; analytic efforts using Java and either Scala or Python as the programming languages.\\nDeveloper experience building applications/solutions using public/private cloud infrastructure\\nAWS Knowledge and experience - Lambda, API Gateway, EC2, S3 etc.\\nDatabase knowledge – Oracle or Any No SQL DB like Couchbase, MongoDB, DocumentDB\\nExperience with CICD (continuous integration &amp; delivery) pipelines\\nAutomation, Configuration Management (e.g. Ansible, Puppet), Dev-ops practices\\nFamiliarity with Linux Containers / Docker / Kubernetes.\\n</td>\n",
       "      <td>Design and implement a centralized unstructured data management ecosystem to collect, analyze, store, protect and govern unstructured data for the enterprise.\\nWork closely with the team to design, plan, build and maintain a scalable Cloud platform for the evolving needs to support Allstate’s Unstructured Data Strategy.\\nProvide recommendations on implementation of organization wide processes and procedures for the management of data risk especially if the solution is based on a public cloud services.\\nCoordinate with Network and Infrastructure teams to design systems that can scale to the operational needs of handling large volumes of Unstructured data\\nPartners with other internal teams and peers in the department to ensure holistic solutions meet the needs of various stakeholders.\\nLeverages and uses Big Data best practices to develop technical solutions used for handling of Unstructured data.\\nSupports Innovation; regularly provides new ideas to help people, process, and technology that interact with data ecosystem.\\nDevelop and builds frameworks/prototypes that integrate Unstructured data and advanced analytics to make better business decisions.\\nSupports a clear communication strategy that keeps all relevant stakeholders informed and provides an opportunity to influence the direction of the work\\nTrains and develops other engineers.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Where good people build rewarding careers.\\nThink that working in the insurance field can’t be exciting, rewarding and challenging? Think again. You’ll help us reinvent protection and retirement to improve customers’ lives. We’ll help you make an impact with our training and mentoring offerings. Here, you’ll have the opportunity to expand and apply your skills in ways you never thought possible. And you’ll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.\\nJob Summary\\nThis role is responsible for a DevOps approach to development of new systems for analyzing/processing/enriching Unstructured content; the coding &amp; development of advanced analytics solutions to make/optimize business decisions and processes; integrating new tools to improve analytics; and address new technical challenges using existing and emerging technology solutions.\\nResponsibilities\\nDesign and implement a centralized unstructured data management ecosystem to collect, analyze, store, protect and govern unstructured data for the enterprise.\\nWork closely with the team to design, plan, build and maintain a scalable Cloud platform for the evolving needs to support Allstate’s Unstructured Data Strategy.\\nProvide recommendations on implementation of organization wide processes and procedures for the management of data risk especially if the solution is based on a public cloud services.\\nCoordinate with Network and Infrastructure teams to design systems that can scale to the operational needs of handling large volumes of Unstructured data\\nPartners with other internal teams and peers in the department to ensure holistic solutions meet the needs of various stakeholders.\\nLeverages and uses Big Data best practices to develop technical solutions used for handling of Unstructured data.\\nSupports Innovation; regularly provides new ideas to help people, process, and technology that interact with data ecosystem.\\nDevelop and builds frameworks/prototypes that integrate Unstructured data and advanced analytics to make better business decisions.\\nSupports a clear communication strategy that keeps all relevant stakeholders informed and provides an opportunity to influence the direction of the work\\nTrains and develops other engineers.\\nKnowledge/Skills/Abilities/Experience\\nBachelor’s Degree in Computer Science or related area, or equivalent work experience.\\nBuilding out a scalable multi-tenant AWS private and public cloud environment that meets the architectural requirements of the cloud platform\\nFamiliar with software development to drive data &amp; analytic efforts using Java and either Scala or Python as the programming languages.\\nDeveloper experience building applications/solutions using public/private cloud infrastructure\\nAWS Knowledge and experience - Lambda, API Gateway, EC2, S3 etc.\\nDatabase knowledge – Oracle or Any No SQL DB like Couchbase, MongoDB, DocumentDB\\nExperience with CICD (continuous integration &amp; delivery) pipelines\\nAutomation, Configuration Management (e.g. Ansible, Puppet), Dev-ops practices\\nFamiliarity with Linux Containers / Docker / Kubernetes.\\nDesired Experience\\n2+ Experience as a Big Data Engineer or System Engineer\\nGood understanding of data flow, data governance principles\\nFamiliar with Development frameworks: Hibernate, Micro Service, Spring boot, Spring Cloud, Spring Integration\\nFamiliar with Messaging/Streaming: JMS, Kafka/Kinesis\\nThe candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.\\n\\n\\nGood Work. Good Life. Good Hands®.\\n\\nAs a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.\\n\\nLearn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.\\n\\nAllstate generally does not sponsor individuals for employment-based visas for this position.\\nEffective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.\\n\\nFor jobs in San Francisco, please click \"here\" for information regarding the San Francisco Fair Chance Ordinance.\\nFor jobs in Los Angeles, please click \"here\" for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.\\n\\nIt is the policy of Allstate to employ the best qualified individuals available for all jobs without regard to race, color, religion, sex, age, national origin, sexual orientation, gender identity/gender expression, disability, and citizenship status as a veteran with a disability or veteran of the Vietnam Era.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Google Technical Architect</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum 5 years of Consulting or client service delivery experience on Google GCP\\n</td>\n",
       "      <td>DevOps on an GCP platform. Multi-cloud experience a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Google Cloud Platform (GCP) Technical Architect Delivery is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would also be responsible for developing and delivering Google GCP cloud solutions to meet todays high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Google GCP Technical Architect is a highly performant GCP Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data soltuions on cloud. Using Google GCP public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:Work with Sales and Bus Dev teams in providing Data and GCP Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS &amp; NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nBasic Qualifications\\nMinimum 5 years of Consulting or client service delivery experience on Google GCP\\nMinimum 10 years of experience in big data, database and data warehouse architecture and delivery\\nBachelors degree or 12 years previous professional experience\\nAble to travel 100% (M-TH)\\nMinimum of 5 years of professional experience in 2 of the following areas:\\nSolution/technical architecture in the cloud\\nBig Data/analytics/information analysis/database management in the cloud\\nIoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using GCP services etc.:\\nData Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core\\nStreaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam\\nData Warehousing &amp; Data Lake : BigQuery, Cloud Storage\\nAdvanced Analytics : Cloud ML engine, Google Data Studio, Tensorflow &amp; Sheets\\n\\nFamiliarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nCertified GCP Solutions Architect - Associate\\nCertified GCP Solutions Architect – Professional (Nice to have)\\nCertified GCP Big Data Specialty (Nice to have)\\nCertified GCP AI/ML Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an GCP platform. Multi-cloud experience a plus.\\nExperience developing and deploying ETL solutions on GCP\\nStrong in Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience beyond GCP a plus - AWS and Azure\\n\\nProfessional Skill Requirements\\nProven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Sr Professional, Data Engineer</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BS Degree or equivalent work experience\\nFormal training and /or certifications in programming languages a plus\\nTypically have 4-8 years of directly related experience.\\nExperience with data used in CoreLogic products and processes (public record, tax, census, appraisal, etc.).\\nFluent in multiple program languages at an expert level\\nStrong experience with various computer platforms and application environments\\nExpertise with developing multiple tiers of multi-tiered software applications\\nExpertise designing programs and data systems\\nConstantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team\\nProject management skills\\nExperience with ETL methodologies\\nStrong understanding of data structures and design\\nExperience with data flow, data enrichment, transformations\\nAbility to optimize database queries and performance\\nDemonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation\\nExperience in developing data service processes and system infrastructure to be used Enterprise Wide\\nStrong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.\\n</td>\n",
       "      <td>BS Degree or equivalent work experience\\nFormal training and /or certifications in programming languages a plus\\nTypically have 4-8 years of directly related experience.\\nExperience with data used in CoreLogic products and processes (public record, tax, census, appraisal, etc.).\\nFluent in multiple program languages at an expert level\\nStrong experience with various computer platforms and application environments\\nExpertise with developing multiple tiers of multi-tiered software applications\\nExpertise designing programs and data systems\\nConstantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team\\nProject management skills\\nExperience with ETL methodologies\\nStrong understanding of data structures and design\\nExperience with data flow, data enrichment, transformations\\nAbility to optimize database queries and performance\\nDemonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation\\nExperience in developing data service processes and system infrastructure to be used Enterprise Wide\\nStrong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BS Degree or equivalent work experience\\nFormal training and /or certifications in programming languages a plus\\nTypically have 4-8 years of directly related experience.\\nExperience with data used in CoreLogic products and processes (public record, tax, census, appraisal, etc.).\\nFluent in multiple program languages at an expert level\\nStrong experience with various computer platforms and application environments\\nExpertise with developing multiple tiers of multi-tiered software applications\\nExpertise designing programs and data systems\\nConstantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team\\nProject management skills\\nExperience with ETL methodologies\\nStrong understanding of data structures and design\\nExperience with data flow, data enrichment, transformations\\nAbility to optimize database queries and performance\\nDemonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation\\nExperience in developing data service processes and system infrastructure to be used Enterprise Wide\\nStrong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Summary\\nJoin the team that powers the global real estate economy - CoreLogic is an innovative, future focused company whose vision is to deliver unique property-level insights that power the global real estate economy.\\nWe are a $1.95 billion in sales company with more than 6,000 employees globally serving the financial services and insurance industries. We are evolving at a rapid pace and the clients we serve are challenged from every direction, which means we are growing and innovating to help drive their success. Working together, and differentiated by our superior data, analytics and data-enabled solutions, we empower our clients to make smarter business decisions through data-driven insights. We take initiative, are fully accountable, build respect and trust, make transparency a must—and engage, include and collaborate at every turn.\\nWe take pride in our work and believe in cultivating a work environment that supports and values our greatest asset: our talented employees.\\nJob Description:\\nJob Duties\\nDevises/modifies procedures to solve problems considering computer equipment capacity and limitations, operating time, and desired results for a specific data process. Designs, codes, tests, debugs, and documents those programs.\\nPrepare detailed specifications from which programs will be written, designed, coded, tested and debugged.\\nConsult with users and develop business relationships and integrate activities with other departments to ensure successful implementation.\\nMay lead small projects or regularly coach other team members to ensure data processes and systems are developed in a way that complies with data architecture, standards and internally established methodologies and practices applicable to Data Operations.\\nMonitor and report to management on the status of project efforts, anticipating/identifying issues that inhibit the attainment of project goals and implementing corrective actions.\\nDevelop business relationships and integrate activities with other departments to ensure successful implementation and support project efforts. Foster and maintain good relationships with customers and colleagues to meet expected customer service levels.\\nJob Qualifications:\\nEducation, Experience, Knowledge and Skills\\nBS Degree or equivalent work experience\\nFormal training and /or certifications in programming languages a plus\\nTypically have 4-8 years of directly related experience.\\nExperience with data used in CoreLogic products and processes (public record, tax, census, appraisal, etc.).\\nFluent in multiple program languages at an expert level\\nStrong experience with various computer platforms and application environments\\nExpertise with developing multiple tiers of multi-tiered software applications\\nExpertise designing programs and data systems\\nConstantly updating personal technical and business knowledge and skills and mentoring others to increase the knowledge and skills of the team\\nProject management skills\\nExperience with ETL methodologies\\nStrong understanding of data structures and design\\nExperience with data flow, data enrichment, transformations\\nAbility to optimize database queries and performance\\nDemonstrates expertise in a variety of the concepts, practices, and procedures in database design and implementation\\nExperience in developing data service processes and system infrastructure to be used Enterprise Wide\\nStrong communication skills in order to participate in business meetings and provide clear written documentation on a variety of complex technical issues to a wide audience.\\nCoreLogic offers an empowered work environment that encourages creativity, initiative and professional growth and provides a competitive salary and benefits package. CoreLogic is an Equal Opportunity/Affirmative Action employer committed to attracting and retaining the best-qualified people available, without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, disability or status as a veteran of the Armed Forces, or any other basis protected by federal, state or local law. CoreLogic maintains a Drug-Free Workplace. We are fully committed to employing a diverse workforce and creating an inclusive work environment that embraces everyone’s unique contributions, experiences and values. Please apply on our website for consideration.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Data Engineer - Wealth Management</td>\n",
       "      <td>Plano, TX 75024</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>75024</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Computer Science or any other relevant field.\\nStrong Database/SQL skills\\n5+ years hands on ETL skills – Informatica, Pentaho\\nUnix/Perl scripting\\nStrong knowledge of Data modeling/Data Warehousing concepts\\nExperience of a scheduling tool - CA/Autosys\\nExperience working within an Agile environment\\nDemonstrated analytical and problem solving skills.\\n</td>\n",
       "      <td>Bachelor’s degree in Computer Science or any other relevant field.\\nStrong Database/SQL skills\\n5+ years hands on ETL skills – Informatica, Pentaho\\nUnix/Perl scripting\\nStrong knowledge of Data modeling/Data Warehousing concepts\\nExperience of a scheduling tool - CA/Autosys\\nExperience working within an Agile environment\\nDemonstrated analytical and problem solving skills.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation &amp; engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You’ll work in a collaborative, trusting, thought-provoking environment—one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.\\n\\nThis role requires a wide variety of strengths and capabilities, including:\\nStrong Data Engineer with strong command over PL/SQL, data integration/transformation tools like Pentaho, Informatica and exposure to Big Data technology stack (Hadoop, Spark, Hive, Impala, etc.).\\nUtilize Agile methodology and adhere to coding standards, procedures and techniques while contributing to the technical code documentation.\\nProvide high quality technology solutions that address business needs developing applications within mature technology environments.\\nConverting data to stories using advanced analytical and visualization techniques to help with data-driven decision making and management reporting.\\nDevops model - Design, develop, code, test, debug, document and support.\\nCollaborate with team and come up with solutions for any identified problem by team.\\nDeployment of newly build modules in QA and Production environment.\\nManage code quality for total build effort.\\nCoordinate with end users during User Acceptance Testing.\\nBS/BA degree or equivalent experience\\nAdvanced knowledge of application, data and infrastructure architecture disciplines\\nQualifications/Skills Required:\\nBachelor’s degree in Computer Science or any other relevant field.\\nStrong Database/SQL skills\\n5+ years hands on ETL skills – Informatica, Pentaho\\nUnix/Perl scripting\\nStrong knowledge of Data modeling/Data Warehousing concepts\\nExperience of a scheduling tool - CA/Autosys\\nExperience working within an Agile environment\\nDemonstrated analytical and problem solving skills.\\nDesirable Skills\\nHadoop / Spark / Hive, big data exposure is a big plus\\nExperience with any Reporting/Business Intelligence tools (Qlikview/Qliksense/Tableau/OBIEE)\\nCoding skills in Java/JavaScript/Python\\nFinancial industry experience\\nScrum/Agile knowledge\\nStrong verbal communication skills\\nOur Asset and Wealth Management division is driven by innovators like you who are driven to create technology solutions that make us work more efficiently and help our businesses grow. It’s our mission to efficiently take care of our clients’ wealth, helping them get, and remain properly invested. Across 27 cities, our team of 4,600 agile technologists thrive in a cloud-native environment that values continuous learning using a data-centric approach in developing innovative technology solutions.\\n\\nWhen you work at JPMorgan Chase &amp; Co., you’re not just working at a global financial institution. You’re an integral part of one of the world’s biggest tech companies. In 20 technology centers worldwide, our team of 50,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $10B+ annual investment in technology enables us to hire people to create innovative solutions that will are transforming the financial services industry.\\n\\n\\nAt JPMorgan Chase &amp; Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you’re looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Data Engineer 3 (TX)</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description:\\n\\nResponsible for completing our transition into fully automated operational reports across different functions within Care (including repair operations, contact center, digital support, product quality and finance) and for bringing our Care Big Data capabilities to the next level by designing and implementing a new analytics governance model, with emphasis on architecting consistent root cause analysis procedures resulting in enhanced operational and customer engagement results.\\n\\nSummary:\\nThe main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization�s data assets.\\n\\nJob Responsibilities:\\n� Design, construct, install, test and maintain highly scalable data management systems.\\n� Ensure systems meet business requirements and industry practices.\\n� Design, implement, automate and maintain large scale enterprise data ETL processes.\\n� Build high-performance algorithms, prototypes, predictive models and proof of concepts.\\n\\nSkills:\\n� Ability to work as part of a team, as well as work independently or with minimal direction.\\n� Excellent written, presentation, and verbal communication skills.\\n� Collaborate with data architects, modelers and IT team members on project goals.\\n� Strong PC skills including knowledge of Microsoft SharePoint.\\n\\nEducation/Experience:\\n� Bachelor's degree in a technical field such as computer science, computer engineering or related field required.\\n� 5-7 years of experience required.\\n� Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI.\\n***Please submit candidates with the following must haves***\\n\\nAnalytical and problem solving skills, applied to Big Data domain\\nProven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark\\n5-8 years of Python or Java/J2EE development experience\\n3+ years of demonstrated technical proficiency with Hadoop and big data projects\\n\\nDay to Day duties:\\nData collection – gather information and required data fields.\\nData manipulation – Join data from multiple data sources and build ETLs to be sent to Tableau for reporting purpose\\n- Measure &amp; Improve - Implement success indicators to continuously measure and improve, while providing relevant insight and reporting to leadership and teams.\\n\\nInterview process:\\n4 interviewers – Ashok Padmaraju and Jyotsna Pagadrai to evaluate from the technical perspective. Kayla and Guilherme Koga to evaluate from the business perspective.\\n\\nAnalytical and problem solving skills, applied to Big Data domain\\nProven understanding and hands on experience with Hadoop, Hive, Pig, Impala, and Spark\\n5-8 years of Python or Java/J2EE development experience\\n3+ years of demonstrated technical proficiency with Hadoop and big data projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Enterprise Data Architect Consultant</td>\n",
       "      <td>Plano, TX 75023</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>75023</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Who we are\\n\\nCollaborative.\\nRespectful. A place to dream and do. These are just a few words that\\ndescribe what life is like at Toyota. As one of the world’s most admired\\nbrands, Toyota\\nis growing and leading the future of mobility through innovative, high-quality\\nsolutions designed to enhance lives and delight those we serve. We’re looking\\nfor diverse, talented team members who want to grow and challenge what's possible.\\nAn important part of the Toyota family is Toyota Financial\\nServices (TFS), the finance and insurance brand for Toyota and Lexus in North America.\\nWhile TFS is a separate business entity, it is an essential part of this\\nworld-changing company – delivering on Toyota’s vision to move people beyond\\nwhat’s possible. At TFS, you will help create best-in-class customer\\nexperiences in an innovative, collaborative environment.\\n\\nWho we’re looking for\\n\\nThe TFS Information\\nand Digital Systems (IDS) Department is looking for a passionate and highly-motivated\\nData Architect Consultant responsible for the entire TFS enterprise data\\narchitecture and development lifecycle process that translates business\\nrequirements into functioning software systems, services and solutions aligned\\nto a Factory or and DIO’s Application Portfolio or specific portion of the\\napplication portfolio. This role will\\nrequire a strong knowledge of data life cycle and application development,\\ndatabase design and systems implementation as this role is a \"enterprise\\ndata architect\" crafting technical solutions to complex business needs.\\nThe\\nEnterprise Data Architect must synthesize requirements information from the\\nbusiness and develop comprehensive technical architecture design that\\nconstitutes the blueprint for a the solution.\\nData architect play a significant role in ensuring the solution aligns\\nand fits with enterprise systems and configurations. This roll (often\\ntimes working with vendor resources) will develop technical products that will\\nbe validated, reviewed and tested. The\\nbest candidates will be highly technical professionals experienced in data\\narchitecture/database programming and web application development.\\n Solid knowledge and implementation experience\\nof Cloud Computing\\n Domain Knowledge per area\\n Multiple years of experience working with\\nFinancial organization with emphasis on Captive Auto Finance where the\\npotential candidate has working knowledge of\\n Need to have strong experience with programming\\nusing Java or Scala or Python\\nNeed to have working experience in MongoDB\\n Need to have substantial experience BigData\\necosystem (e.g., Hadoop, Data Lake)\\n Skills with iDERA - Embarcadero ER/Studio\\nsuite, Erwin, or other data modeling tool for OLTP systems\\n Expert in data modeling for relational and\\ndocument-based data stores, including conceptual, logical, and physical data\\nmodels\\n Guides teams through the complete software\\ndevelopment life cycle-from concept to delivery\\n Experience working with terabyte data sets,\\nbillion+ row tables, and high transaction volumes in an OLTP\\n Demonstrated leadership in root cause analysis\\nand risk mitigation\\n Expertise in system performance analysis and\\ntuning\\n Experience with service-oriented architectures\\nand microservices\\n Experience with distributed systems and cloud\\ncomputing\\n Experience with Jenkins or similar continuous\\nintegration systems\\nDemonstrated leadership in agile software\\ndevelopment environments\\n Self-starter with exceptional communication\\nskills\\n5 + years of experience in a leadership role of\\nestablishing, driving and implementing a multi-year technology strategy(s) and\\nTechnology Roadmap\\n At least 7 years of experience in technology\\nconsulting, enterprise and solutions architecture and architectural frameworks\\n At least 7+ years of experience in project\\nexecution\\nSolid knowledge and implementation experience\\nwith Integration techniques and enabling technology working knowledge including\\nstreaming services/processes (e.g., Kafka, Kinesis,\\nSpark etc.)\\nProvide a critical role in a highly agile scrum environment working\\nagainst tight requirements\\n Drive standards around and propose system\\nsolutions to meet business requirements.\\nIntegrate\\nthe data and technical system changes across the entire application portfolio\\n Oversee\\ndevelopment of data and technical changes and validate and implement the data\\nand technical changes provided by the vendor.\\nAct\\nas liaison with the vendors, factories and product owners and end users to\\ncreate and finalize end-to-end technical solutions.\\n Supervise\\nand provide guidance to factories product designers and other domain Architects\\non the team\\nReview/approve\\nsystems documentation as changes/enhancements that are planned to deploy to\\nensure accuracy need to communicate and coordinate with key business\\nstakeholders on business process changes, requirements definition (e.g., BRD,\\nFRD), testing, end user acceptance criteria, operational readiness, and change\\nmanagement are crucial responsibilities of this position.\\n Success\\nin this role requires in-depth knowledge and hands-on technical skills, with\\nstrong leadership, oral, written communication skills; an ability to present an\\ndiscuss technical, functional and managerial information clearly and concisely\\nto executive management. The ability to\\nwork effectively within IDS, various vendors (service integrators as well as\\ntechnology providers) and business teams to develop solutions that meet\\ntechnology standards and business needs is required.\\n Solid\\nknowledge and implementation experience of Cloud Computing\\nSolid\\nknowledge and implementation experience with Integration techniques and\\nenabling technology working knowledge including streaming services/processes\\n(e.g., Kafka, Kinesis, Spark etc.)\\n Provide a critical role in a highly agile scrum environment\\nworking against tight requirements\\nWhat you bring\\n8 – 12 years’ industry experience with having auto finance experience will be a plus\\n4 years’ experience as a Solution Architect\\n4 years’ experience as a Software Engineer\\n3 years’ experience using the .Net framework\\n3 years’ experience using J2EE framework\\n1 years’ experience developing software for mobile devices using emerging technologies\\n4 years’ experience developing software for internet web applications and portals\\n1 years’ experience with performance architecture, tuning and debugging distributed systems.\\nMastery of multi-tenancy, multi-branding, multi-device and multi country and language architecture and enterprise services implementation\\nMastery of database design for applications and familiar with Information Management / Data Management\\nPossesses excellent verbal and written communication and interpersonal skills and the ability to interface with leadership and all levels of associates.\\nExperience with Open Source Development\\nDemonstrated success as Solution Architect on both large and small projects\\nAs a Big Data Engineer You will be accountable for data storage and ETL development and improvements\\nDemonstrated continued knowledge acquisition of emerging technologies\\nAbility to work independently, with strong organizational and flexibility skills in a team-oriented environment\\n\\nWhat we’ll bring\\n\\n\\nDuring your interview process, our team can fill you in on all the details of our industry-leading benefits and career development opportunities. A few highlights include:\\n\\n\\nA work environment built on teamwork, flexibility and respect\\nProfessional growth and development programs to help advance your career, as well as tuition reimbursement\\nVehicle purchase &amp; lease programs\\nComprehensive health care and wellness plans for your entire family\\nFlexible work options based on business needs\\nToyota 401(k) Savings Plan featuring a company match, as well as an annual retirement contribution from Toyota regardless of whether you contribute\\nPaid holidays and paid time off\\nReferral services related to prenatal services, adoption, child care, schools and more\\nFlexible spending accounts\\nRelocation assistance\\nOnsite amenities such as fitness center, restaurants, etc\\nWhat you should know\\n\\nOur success begins and ends with our people. We embrace diverse perspectives and value unique human experiences. We are proud to be an equal opportunity employer that celebrates the diversity of the communities where we live and do business. Applicants for our positions are considered without regard to race, ethnicity, national origin, sex, sexual orientation, gender identity or expression, age, disability, religion, military or veteran status, or any other characteristics protected by law.\\n\\nHave a question or need assistance with your application? Check out the How to Apply section of our careers page on Toyota.com!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>AI Model Development Lead For Marketing (Analytics Manager 5)</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques\\n4+ years of experience working with digital data science including analyzing cookie-level data</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview:\\nWells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.\\n\\nSUCCESS PROFILE\\nCheck out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.\\n\\nAnalytical\\nDetail-oriented\\nInsightful\\nInventive\\nProblem Solver\\nCurious\\nBenefits\\nWells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:\\n\\nMedical, Dental and Vision\\nEmployer Matching 401(k)\\nTuition Reimbursment\\nMaternity and Paternity Leave\\nPaid Time Off\\nResponsibilties\\nJob Description\\nAt Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.\\nHelp us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.\\nEnterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.\\nAs part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, marketing, and virtual channels is looking for an experienced AI leader to manage the development of AI models for marketing.\\n\\nThis leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on marketing’s AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and marketing executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.\\nKEY RESPONSIBILITIES INCLUDE:\\nBuild and grow a team of data scientists responsible for AI model development in support of marketing\\nDesign, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions\\nPartner with marketing executives to frame the problem, manage the model development process, and business relationship\\nManage a portfolio of the data science projects including the following responsibilities:\\nHelp finalize project scope working with business partners\\nOn-going touch-base with business partners and governance stakeholders\\nDefine priorities in partnership with the business partners during on-going development\\nWork with AI technology and production teams to operationalize models\\nMay be called upon to review vendor models and solutions and/or models developed outside of EADS\\nDivisional Information:\\nData Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.\\nAs a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:\\nLead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.\\nAccomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.\\n\\nRequired Qualifications\\n\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques\\n4+ years of experience working with digital data science including analyzing cookie-level data\\n\\n\\nDesired Qualifications\\n\\nStrong analytical skills with high attention to detail and accuracy\\nAbility to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members\\nMeeting facilitation experience in leading discussions that result in consensus and commitment\\n\\n\\nOther Desired Qualifications\\nData science experience in the ad tech space (DMPs, DSPs, Google and/or Adobe Cloud, digital attribution)\\n4+ years managing or directing data scientist/ statistician/ data engineer teams\\nHands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet\\nDetail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable\\nExperience with agile project management methodologies for data science\\n\\nDisclaimer\\n\\nAll offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.\\n\\nRelevant military experience is considered for veterans and transitioning service men and women.\\n\\nWells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.\\n\\nENT FINANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Lean Big Data Engineer</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nUndergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred\\n5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function\\nAbility to work with broad parameters in complex situations\\nExperience in developing, managing, and manipulating large, complex datasets</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nResponsible for design, prototyping and delivery of software solutions within the big data eco-system\\nLeading projects and/or serving as analytics SME to provide new or enhanced data to the business\\nImproving data governance and quality increasing the reliability of our data\\nInfluencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Where good people build rewarding careers.\\nThink that working in the insurance field can’t be exciting, rewarding and challenging? Think again. You’ll help us reinvent protection and retirement to improve customers’ lives. We’ll help you make an impact with our training and mentoring offerings. Here, you’ll have the opportunity to expand and apply your skills in ways you never thought possible. And you’ll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.\\nAbout our team\\n360 Finance Advanced Analytics data engineering team works with multiple internal and external data sources to deliver data that is readily available, easily accessible, accurate and complete. They are responsible for building a centralized data lake/hub using the Hadoop ecosystem that will be used by Reporting &amp; Operational Analytics teams and the Machine learning teams.\\nJob Description\\nThis Lead Consultant is an experienced professional who is responsible for leveraging data and analytics to help automate and optimize Claims Analytics Data processes enabling our Claims employees to focus on serving our customers and delivering the most advanced claims experience on the planet. They will be responsible for the strategy around how we bring together complex data into clean and useful data structures making our valuable data more approachable.\\nKey Responsibilities\\nResponsible for design, prototyping and delivery of software solutions within the big data eco-system\\nLeading projects and/or serving as analytics SME to provide new or enhanced data to the business\\nImproving data governance and quality increasing the reliability of our data\\nInfluencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise\\nKey Responsibilities (Cont'd)\\nResponsible for designing and building new Big Data systems for turning data into actionable insights\\nTrain and mentor junior team members on Big Data/Hadoop tools and technologies\\nIdentifies opportunities for improvement and presents recommendations to management\\nDevelop solutions and iterates quickly to continuously improve\\nSeeks out and evaluates emerging big data technologies and open-source packages\\nParticipate in strategic planning discussions with technical and non-technical partners\\nUses, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).\\nUses, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e. Java, Python, SQL, R)\\nJob Qualifications\\nUndergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred\\n5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function\\nAbility to work with broad parameters in complex situations\\nExperience in developing, managing, and manipulating large, complex datasets\\nRoles and Responsibilities\\nExpert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.\\nSome understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.\\nExperience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required\\n5-6+ years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required\\nExperience with Agile development methodologies and tools to iterate quickly on product changes,\\nRoles and Responsibilities (Cont'd)\\ndeveloping user stories and working through backlog (Continuous Integration and JIRA a plus)\\nExperience with Airflow a plus\\nWorking knowledge of Tableau – a plus\\nAdvanced oral and written communication skills\\nStrong quantitative and analytical abilities\\nGood organizational and time management skills\\nAbility to manage and coach others\\nDecision making capabilities including problem solving approaches, decision frameworks; ability to design and lead complex analysis\\nStrong interpersonal skills\\nThe candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.\\nGood Work. Good Life. Good Hands®.\\nAs a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.\\nLearn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.\\nAllstate generally does not sponsor individuals for employment-based visas for this position.\\nEffective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.\\nFor jobs in San Francisco, please click “here” for information regarding the San Francisco Fair Chance Ordinance.\\nFor jobs in Los Angeles, please click “here” for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.\\nTo view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs\\nTo view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.\\nIt is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nSkilled in Python and preferably in one or more programming languages like C++, Java, Go, etc\\nExperience with Docker and Kubernetes\\nExperience working with SQL and NoSQL based database solutions\\nPublic cloud technology experience in production (Azure, AWS, or Equivalent)\\n3+ years of collective experience in data engineering and data analysis\\n2+ years of experience architecting, building and administering large-scale distributed applications\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nSkilled in Python and preferably in one or more programming languages like C++, Java, Go, etc\\nExperience with Docker and Kubernetes\\nExperience working with SQL and NoSQL based database solutions\\nPublic cloud technology experience in production (Azure, AWS, or Equivalent)\\n3+ years of collective experience in data engineering and data analysis\\n2+ years of experience architecting, building and administering large-scale distributed applications\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview:\\nOur client is looking for a Data Engineer to join the R&amp;D Team. You’re joining a multidisciplinary team of product directors, product managers, and digital analysts to create product listings across the top ecommerce marketplaces.\\n\\nWe are preferred supplier to 7-Eleven. This position may be available for conversion to full time employment upon successful evaluation of performance. You’ll be working in a fast-paced, eclectic environment of talented professionals who are leading the industry in digital capabilities.\\n\\nResponsibilities:\\nThe ideal candidate will be driven and passionate in creating the next generation of data products and capabilities. You will work directly with Product Owners and customers to deliver data products in a collaborative and agile environment. The ideal candidate will build data pipeline frameworks to automate high-volume and real-time data delivery and streaming data hub.\\n\\nBasic Qualifications:\\n\\nSkilled in Python and preferably in one or more programming languages like C++, Java, Go, etc\\nExperience with Docker and Kubernetes\\nExperience working with SQL and NoSQL based database solutions\\nPublic cloud technology experience in production (Azure, AWS, or Equivalent)\\n3+ years of collective experience in data engineering and data analysis\\n2+ years of experience architecting, building and administering large-scale distributed applications\\n\\nPreferred Qualifications:\\n\\nExperience in engineering data pipelines using Big Data technologies (Hadoop, Spark, Storm, Kafka, etc) on large scale unstructured data sets is a plus\\nFamiliarity with distributed data stores like Elasticsearch is a plus\\nFamiliarity with Machine Learning concepts is a plus\\n\\nGOIN Technology is a technology consulting and managed services business headquartered in Dallas/Ft. Worth. The company is focused on CIO/CTO Advisory Consulting, Information Security, Digital Transformation, Software/Data Engineering, Innovation as a Service and Managed Services.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Data Engineer - Wealth Management</td>\n",
       "      <td>Plano, TX 75024</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>75024</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Computer Science or any other relevant field.\\nStrong Database/SQL skills\\n5+ years hands on ETL skills – Informatica, Pentaho\\nUnix/Perl scripting\\nStrong knowledge of Data modeling/Data Warehousing concepts\\nExperience of a scheduling tool - CA/Autosys\\nExperience working within an Agile environment\\nDemonstrated analytical and problem solving skills.\\n</td>\n",
       "      <td>Bachelor’s degree in Computer Science or any other relevant field.\\nStrong Database/SQL skills\\n5+ years hands on ETL skills – Informatica, Pentaho\\nUnix/Perl scripting\\nStrong knowledge of Data modeling/Data Warehousing concepts\\nExperience of a scheduling tool - CA/Autosys\\nExperience working within an Agile environment\\nDemonstrated analytical and problem solving skills.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation &amp; engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You’ll work in a collaborative, trusting, thought-provoking environment—one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.\\n\\nThis role requires a wide variety of strengths and capabilities, including:\\nStrong Data Engineer with strong command over PL/SQL, data integration/transformation tools like Pentaho, Informatica and exposure to Big Data technology stack (Hadoop, Spark, Hive, Impala, etc.).\\nUtilize Agile methodology and adhere to coding standards, procedures and techniques while contributing to the technical code documentation.\\nProvide high quality technology solutions that address business needs developing applications within mature technology environments.\\nConverting data to stories using advanced analytical and visualization techniques to help with data-driven decision making and management reporting.\\nDevops model - Design, develop, code, test, debug, document and support.\\nCollaborate with team and come up with solutions for any identified problem by team.\\nDeployment of newly build modules in QA and Production environment.\\nManage code quality for total build effort.\\nCoordinate with end users during User Acceptance Testing.\\nBS/BA degree or equivalent experience\\nAdvanced knowledge of application, data and infrastructure architecture disciplines\\nQualifications/Skills Required:\\nBachelor’s degree in Computer Science or any other relevant field.\\nStrong Database/SQL skills\\n5+ years hands on ETL skills – Informatica, Pentaho\\nUnix/Perl scripting\\nStrong knowledge of Data modeling/Data Warehousing concepts\\nExperience of a scheduling tool - CA/Autosys\\nExperience working within an Agile environment\\nDemonstrated analytical and problem solving skills.\\nDesirable Skills\\nHadoop / Spark / Hive, big data exposure is a big plus\\nExperience with any Reporting/Business Intelligence tools (Qlikview/Qliksense/Tableau/OBIEE)\\nCoding skills in Java/JavaScript/Python\\nFinancial industry experience\\nScrum/Agile knowledge\\nStrong verbal communication skills\\nOur Asset and Wealth Management division is driven by innovators like you who are driven to create technology solutions that make us work more efficiently and help our businesses grow. It’s our mission to efficiently take care of our clients’ wealth, helping them get, and remain properly invested. Across 27 cities, our team of 4,600 agile technologists thrive in a cloud-native environment that values continuous learning using a data-centric approach in developing innovative technology solutions.\\n\\nWhen you work at JPMorgan Chase &amp; Co., you’re not just working at a global financial institution. You’re an integral part of one of the world’s biggest tech companies. In 20 technology centers worldwide, our team of 50,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $10B+ annual investment in technology enables us to hire people to create innovative solutions that will are transforming the financial services industry.\\n\\n\\nAt JPMorgan Chase &amp; Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you’re looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>AI Model Development Lead for Virtual Channels (Analytic Manager 5)</td>\n",
       "      <td>Addison, TX 75001</td>\n",
       "      <td>Addison</td>\n",
       "      <td>TX</td>\n",
       "      <td>75001</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview:\\nWells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.\\n\\nSUCCESS PROFILE\\nCheck out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.\\n\\nAnalytical\\nDetail-oriented\\nInsightful\\nInventive\\nProblem Solver\\nCurious\\nBenefits\\nWells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:\\n\\nMedical, Dental and Vision\\nEmployer Matching 401(k)\\nTuition Reimbursment\\nMaternity and Paternity Leave\\nPaid Time Off\\nResponsibilties\\nJob Description\\nAt Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.\\nHelp us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.\\nEnterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.\\nAs part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, Virtual Channels, and virtual channels is looking for an experienced AI leader to manage the development of AI models for Virtual Channels.\\n\\nThis leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on Virtual Channels’ AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and Virtual Channels executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.\\n\\nKEY RESPONSIBILITIES INCLUDE:\\nBuild and grow a team of data scientists responsible for AI model development in support of Virtual Channels\\nDesign, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions\\nPartner with Virtual Channels executives to frame the problem, manage the model development process, and business relationship\\nManage a portfolio of the data science projects including the following responsibilities:\\nHelp finalize project scope working with business partners\\nOn-going touch-base with business partners and governance stakeholders\\nDefine priorities in partnership with the business partners during on-going development\\nWork with AI technology and production teams to operationalize models\\nWill be called upon to review vendor models and solutions and/or models developed outside of EADS\\nDivisional Information:\\nData Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.\\nAs a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:\\nLead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.\\nAccomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.\\n\\nRequired Qualifications\\n\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques\\n\\n\\nDesired Qualifications\\n\\nStrong analytical skills with high attention to detail and accuracy\\nAbility to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members\\nMeeting facilitation experience in leading discussions that result in consensus and commitment\\n\\n\\nOther Desired Qualifications\\n4+ years managing or directing data scientist/ statistician/ data engineer teams\\nHands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet\\nDetail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable\\nExperience with agile project management methodologies for data science\\nExperience with Big Data or Hadoop tools such as Spark, Hive, Kafka and Map\\n\\nStreet Address\\nNC-Charlotte: 401 S Tryon St - Charlotte, NC\\nMN-Minneapolis: 600 S 4th St - Minneapolis, MN\\nNC-Charlotte: 11625 N Community House Road - Charlotte, NC\\nSC-Fort Mill: 3480 State View Blvd - Fort Mill, SC\\nTX-Addison: 5080 Spectrum Dr - Addison, TX\\nTX-DAL-Downtown Dallas: 1445 Ross Ave - Dallas, TX\\nTX-Irving: 5000 Riverside Drive - Irving, TX\\nAZ-Tempe: 1150 W Washington St - Tempe, AZ\\nIA-Des Moines: 6200 Park Ave - Des Moines, IA\\nIA-Des Moines: 800 Walnut St - Des Moines, IA\\nGA-Atlanta: 3579 Atlanta Ave - Atlanta, GA\\n\\n\\nDisclaimer\\n\\nAll offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.\\n\\nRelevant military experience is considered for veterans and transitioning service men and women.\\n\\nWells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.\\n\\nENT FINANCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               Title  \\\n",
       "0   Senior Microsoft Power BI Developer                                                \n",
       "1   Data Strategy Specialist - Business & Data Analysis, Cloud, AWS, Azure, Big Data   \n",
       "2   Data Engineer                                                                      \n",
       "3   Data Engineer-Senior Advisor                                                       \n",
       "4   Senior Microsoft SQL Database Developer                                            \n",
       "..                                      ...                                            \n",
       "67  AI Model Development Lead For Marketing (Analytics Manager 5)                      \n",
       "68  Lean Big Data Engineer                                                             \n",
       "69  Data Engineer                                                                      \n",
       "70  Data Engineer - Wealth Management                                                  \n",
       "71  AI Model Development Lead for Virtual Channels (Analytic Manager 5)                \n",
       "\n",
       "             Location        City State         Zip     Country  \\\n",
       "0   Dallas, TX 75202   Dallas      TX    75202       None Found   \n",
       "1   Dallas, TX         Dallas      TX    None Found  None Found   \n",
       "2   Allen, TX          Allen       TX    None Found  None Found   \n",
       "3   Richardson, TX     Richardson  TX    None Found  None Found   \n",
       "4   Dallas, TX 75202   Dallas      TX    75202       None Found   \n",
       "..               ...      ...      ..      ...              ...   \n",
       "67  Dallas, TX         Dallas      TX    None Found  None Found   \n",
       "68  Irving, TX         Irving      TX    None Found  None Found   \n",
       "69  Irving, TX         Irving      TX    None Found  None Found   \n",
       "70  Plano, TX 75024    Plano       TX    75024       None Found   \n",
       "71  Addison, TX 75001  Addison     TX    75001       None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                           Qualifications  \\\n",
       "0   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "2   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "3   7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.7+ years of SQL development skills within a multi-tier environment are required.                                                                                                                     \n",
       "4   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "..         ...                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "67  \\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques\\n4+ years of experience working with digital data science including analyzing cookie-level data                                                                  \n",
       "68  \\nUndergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred\\n5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function\\nAbility to work with broad parameters in complex situations\\nExperience in developing, managing, and manipulating large, complex datasets                                                                             \n",
       "69  \\nSkilled in Python and preferably in one or more programming languages like C++, Java, Go, etc\\nExperience with Docker and Kubernetes\\nExperience working with SQL and NoSQL based database solutions\\nPublic cloud technology experience in production (Azure, AWS, or Equivalent)\\n3+ years of collective experience in data engineering and data analysis\\n2+ years of experience architecting, building and administering large-scale distributed applications\\n   \n",
       "70  Bachelor’s degree in Computer Science or any other relevant field.\\nStrong Database/SQL skills\\n5+ years hands on ETL skills – Informatica, Pentaho\\nUnix/Perl scripting\\nStrong knowledge of Data modeling/Data Warehousing concepts\\nExperience of a scheduling tool - CA/Autosys\\nExperience working within an Agile environment\\nDemonstrated analytical and problem solving skills.\\n                                                                              \n",
       "71  \\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques                                                                                                                                                                  \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Skills  \\\n",
       "0   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "1    3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "2   5 years of working with following technology stack: Core languages are Java and C#; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, and AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, and SQS.\\nGrowing track record of success or the groundwork to be an impactful member of the team. We’re looking for candidates that exhibit many of the following skills/attributes:\\nStrong Educational Background\\nHands-on Engineering experience in\\nProblem solving and debugging skillsWriting and deploying code the Linux, Windows, or cloud environmentsFamiliarity with algorithms and performance analysisWillingness to contribute to the operational responsibility of the team’s applications\\nSome experience with one or more of the following:\\nRelational Databases & SQL NoSQL databases (Cassandra, Redis, DynamoDB, MongoDB)Big Data tools such as Hadoop, Hive, EMR, Storm, Spark, DynamoDB, HBase   \n",
       "3   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "4   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "..         ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "67  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "68  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "69  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "70  Bachelor’s degree in Computer Science or any other relevant field.\\nStrong Database/SQL skills\\n5+ years hands on ETL skills – Informatica, Pentaho\\nUnix/Perl scripting\\nStrong knowledge of Data modeling/Data Warehousing concepts\\nExperience of a scheduling tool - CA/Autosys\\nExperience working within an Agile environment\\nDemonstrated analytical and problem solving skills.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "71  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                         Responsibilities  \\\n",
       "0   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "2   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "3   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "4   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "..         ...                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "67  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "68  \\nResponsible for design, prototyping and delivery of software solutions within the big data eco-system\\nLeading projects and/or serving as analytics SME to provide new or enhanced data to the business\\nImproving data governance and quality increasing the reliability of our data\\nInfluencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise\\n                                                   \n",
       "69  \\nSkilled in Python and preferably in one or more programming languages like C++, Java, Go, etc\\nExperience with Docker and Kubernetes\\nExperience working with SQL and NoSQL based database solutions\\nPublic cloud technology experience in production (Azure, AWS, or Equivalent)\\n3+ years of collective experience in data engineering and data analysis\\n2+ years of experience architecting, building and administering large-scale distributed applications\\n   \n",
       "70  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "71  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "     Education Requirement  \\\n",
       "0   None Found  None Found   \n",
       "1   None Found  None Found   \n",
       "2   None Found  None Found   \n",
       "3   None Found  None Found   \n",
       "4   None Found  None Found   \n",
       "..         ...         ...   \n",
       "67  None Found  None Found   \n",
       "68  None Found  None Found   \n",
       "69  None Found  None Found   \n",
       "70  None Found  None Found   \n",
       "71  None Found  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         FullDescriptions  \n",
       "0   Job Description:\\nApplies specialized knowledge in Microsoft Power BI, Flow and Azure Analysis Services to conceptualize, design, develop, unit-test, configure, and implement portions of new or enhanced (upgrades or conversions) business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies.\\nResponsibilities:\\nCore team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives\\nDevelop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams\\nDocument requirements and translate into proper system requirements specifications using high-maturity methods, processes and tools.\\nDevelop visualization, user experience and configuration elements of solution design.\\nExecute and coordinate requirements management and change management processes. Participates as a member of and leads development teams.\\nDesigns units for others.\\nCompletes development to implement complex components.\\nDesigns solutions for others to develop.\\nParticipates in cross-functional teams.\\nLeads design activities and provides mentoring and guidance to developers.\\nDesigns, prepares and executes unit tests.\\nRepresents team to clients.\\nDemonstrates technical leadership and exerts influence outside of immediate team.\\nDevelops innovative team solutions to complex problems.\\nContributes to strategic direction for teams.\\nApplies in-depth or broad technical knowledge to provide maintenance solutions across one or more technology areas (e.g. Power BI and Power App development).\\nIntegrates technical expertise and business understanding to create superior solutions for clients.\\nConsults with team members and other organizations, clients and vendors on complex issues.\\nEducation and Experience Required:\\nTypically, a technical bachelor’s degree or equivalent experience and a minimum of 8 years of related experience or a master’s degree and a minimum of 6 years of experience.\\nKnowledge and Skills:\\n8 or more years’ experience writing code using languages such as (and not limited to) Power BI, Tableau, QlikView, Java, C, C++, C#, VB.Net.\\nSignificant hands on experience in creating and deploying KPI, analytic, and dashboard solutions including design, development, deployment, data engineering, data warehousing, and data management projects and practices.\\nAdvanced understanding of RDBMS databases such SQL Server and Oracle.\\nAdvanced understanding of modern software design and development methodologies.\\nExperience on multiple full release project life cycles.\\nAdvanced understanding of modern SCM (software configuration management).\\nAdvanced understanding of testing tools and unit test and integration test scripting, and testing methodologies\\nAdvanced experience using an Integrated Development Environment (e.g., Eclipse, Visual Studio) and development of tool add-ins.\\nStrong understanding of basic Database Administration. Able to define quality and security standards.\\nGood verbal and written communication and negotiation skills.\\nGeneral project management/team leader skills.\\nAbility to work effectively in a globally dispersed team and with clients and vendors.\\nDemonstrated technical leadership skills.\\nBusiness skills - 6+ years of technology services industry and business operations experience in a technology services organization\\nData engineering, analytics and systems subject matter expert on financial, workforce and operational systems for technology services business desired.\\nExperience in multiple solution development methodologies and participation in a fast paced, Dev/Ops environment\\nDrives the construction of highly innovative statistical and financial models to analyze new aspects of business performance.\\nEstablishes the metrics required to measure business performance and recommends the go-forward strategy to address performance gaps.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1   Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe North America Data Strategy & Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.\\n\\n Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model\\n\\nRequired Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:\\no Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling\\no Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.\\no Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel\\n Able to travel up to 100% (Mon-Thu)\\n\\nOptional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model\\n Experience in compiling business cases and roadmaps for data, analytics and technology investments\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "2   The engineering team consists of talented, team-oriented individuals who are empowered to take advantage of the latest cloud and distributed technologies to deliver reliable, high-throughput applications.\\n\\nAs a Data Engineer, you’ll employ your skills on a daily basis to design and build data processing and storage applications to handle millions of transactions per day. You will analyze business requirements and consult with the broader team to ensure successful processing, storage and reporting of our Big Data. You’ll have a wide variety of languages and technologies at your disposal that you can use to solve problems. Your work will directly shape and create our data architecture to ultimately deliver systems that stand up to unpredictable environments at massive scale.\\n\\nTechnical Skills Needed:\\n5 years of working with following technology stack: Core languages are Java and C#; RESTful services, jQuery, SQL Server, Hadoop, Hive, HBase, Storm, Spark, and AWS Services such as Kinesis, DynamoDB, Redshift, Lamda, and SQS.\\nGrowing track record of success or the groundwork to be an impactful member of the team. We’re looking for candidates that exhibit many of the following skills/attributes:\\nStrong Educational Background\\nHands-on Engineering experience in\\nProblem solving and debugging skillsWriting and deploying code the Linux, Windows, or cloud environmentsFamiliarity with algorithms and performance analysisWillingness to contribute to the operational responsibility of the team’s applications\\nSome experience with one or more of the following:\\nRelational Databases & SQL NoSQL databases (Cassandra, Redis, DynamoDB, MongoDB)Big Data tools such as Hadoop, Hive, EMR, Storm, Spark, DynamoDB, HBase                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "3   The Data Analyst, Senior Advisor will work with data stewards, data owners, master data management analysts, operations teams and IT partners across the organization to drive the enterprise data conformance program. Data research and analysis, cross-functional requirements gathering and documentation and solution development are key aspects of the role. The Data Analyst, Senior Advisor supports reactive data quality by researching and fixing known data issues and proactive data quality by defining the requirements for data controls and enhancements to data capture, monitoring and maintenance processes, procedures and standards.\\n\\nPrimary responsibilities include performing data operations such as conversion, address hygiene, and postal presort, as well as variable document composition and file creation.\\n\\nEssential Functions:\\n\\nData Analysis\\nIdependently design, develop and implement data integration solutions that support our platforms resiliency, stability, and supportability using a variety of ETL and database technologies.Rapidly develop and refine data integration solutions using Infosphere Datastage, SQL, FastTrack, or other technologies.Experience integrating large structured and unstructured data in multiple format, character sets and delivery methodsWorks with business sponsors, SMES and application teams; to understand the business requirements; analyze and assess availability, quality, and lineage of source system data.Design, map data from source to target and develop data integration solutions that meet business needs.Develop and socialize data integration standards.Partners with other engineers through design reviews, providing feedback on feasibility, scalability, performance and adherence to standards.Partners with business, analysts, BI team, application teams and other stakeholders to design, map data from source to target, develop, test, and implement production data integration solutions that are fully integrated into the Enterprise Data Warehouse.Ensuring model design solves the end users need.Contribute information to the data governance software to improve knowledge downstream.\\nData Conformance\\nPerforms analysis on known data quality issues and develops and/or recommends operational or technical solutions for remediation, including development and implementation of automated data quality controls that proactively trigger notifications to process owners when data is out of range. Keeps stakeholders informed of progress and solutions in a timely manner.Autonomously, and proactively performs data profiling to explore data, identify issues and summarize findings.Defines data quality metrics to assess completeness, accuracy, consistency, and conformance to business rules. Designs dashboards to support continuous monitoring and measurement of data quality.Partners with IT to cleanse data to achieve the desired level of data qualityPartners with data stakeholders, process and product owners across the organization to define data standards and communicate changes to data capture procedures, processes, standards, and controls.Cleanses and prepares datasets to be consumed by data scientists and other analystsCollaborates with external teams working on data integration and engineeringAdvocates data governance and hygiene best practicesAssists with scoping and integrating orphan datasets\\nYou will gain hands on experience implementing through embedding standardized data elements in the database or system, employing standardized data elements in an exchange mechanism (usually XML schema), or mapping the application data elements to the standardized elements for purposes of exchange.\\n\\nBig Data tools:\\n Big Data: Hadoop, PIG, Sqoop, Hive and Hcatalog & NoSQL (HBase, Cassandra) , SQL.\\n Programming: Scala, Java, Python, Spark\\n\\nRequired Qualifications\\n7+ years of experience in data analysis.7+ years of experience integrating large data in multiple formats7+ years of experience working with high volume data exchange and transaction processing systems. Preferably in a custom software development environment.7+ years of SQL development skills within a multi-tier environment are required.\\n\\nPreferred Qualifications\\nIn depth understanding of data integration best practices, leading industry applications and features such as master data management, entity resolution, data quality assessment, metadata management, etc.Expertise in flat file formats, XML within PL/SQL and file format conversion.Exposure to application security technologies and approaches is preferred.Experience processing and parsing CSV, JSON and XML file formatsDatastage, SQL, FastTrack, or other technologiesStrong analytical, debugging and testing skillsSoftware development experience using scripting languages such as JavaScript, Python or RubyProficient using Infosphere/DataStage or equivalent ETL software.Proficient with relational databases and using SQL to query, create tables, views, indexes, joins.Proficient using Unix and applicable scripting/scheduling tools.Experience with Python for data analysis\\n• Knowledge of clinical and financial Healthcare data • • Knowledge of all data formats (HL7, EDI, CSV, XML, etc)\\n\\nEducation\\nBachelor's Degree in Computer Science, Computer Engineering, Computer Information Systems, Information Systems, Management Information Systems or related engineering discipline.\\n\\nEquivalent work experience will be considered in lieu of degree.\\n\\nBusiness Overview\\nIt’s a new day in health care.\\n\\nCombining CVS Health and Aetna was a transformative moment for our company and our industry, establishing CVS Health as the nation’s premier health innovation company. Through our health services, insurance plans and community pharmacists, we’re pioneering a bold new approach to total health. As a CVS Health colleague, you’ll be at the center of it all.\\n\\nWe offer a diverse work experience that empowers colleagues for career success. In addition to skill and experience, we also seek to attract and retain colleagues whose beliefs and behaviors are in alignment with our core values of collaboration, innovation, caring, integrity and accountability.\\n\\nCVS Health is an equal opportunity/affirmative action employer. Gender/Ethnicity/Disability/Protected Veteran – we highly value and are committed to all forms of diversity in the workplace. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities. We comply with the laws and regulations set forth in the following EEO is the Law Poster: EEO IS THE LAW and EEO IS THE LAW SUPPLEMENT. We provide reasonable accommodations to qualified individuals with disabilities. If you require assistance to apply for this job, please contact our Advice and Counsel Reasonable Accommodations team. Please note that we only accept applications for employment via this site.\\n\\nIf technical issues are preventing you from applying to a position, contact Kenexa Helpdesk at 1-855-338-5609 or cvshealthsupport@us.ibm.com. For technical issues with the Virtual Job Tryout assessment, contact the Shaker Help Desk at 1-877-987-5352.                                                                                                                                                                                                                                                                                                          \n",
       "4   Job Description:\\nApplies specialized knowledge in Microsoft SQL and Azure technologies to conceptualize, design, develop, unit-test, configure, and implement business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies. Analyze customer information requirements and product specifications to define technical content strategy and plan. Designs and develops written and/or visual product-related information – hard copy, web - (e.g., user/configuration/troubleshooting guides), and online information (interactive demos, help systems) integrated into product, for a variety of audiences (end user, system administrators, internal support engineers, product developers, training developers). Codes, builds, compiles, and tests online information and/or sets-up, loads and tests systems hardware to create information deliverables and provide feedback on ease of use and user interfaces to product development. As customer advocate, helps define/refine product requirements. Develops standards and style documents and templates, scripts, style sheets, and script and graphic libraries to ensure common look and feel. Interfaces with cross-functional areas as a member of the product development team, such as marketing, test, support, and manufacturing.\\nCore team member of a high-performance business analytics and executive performance management team that translates business information into business value to achieve corporate business goals and objectives. Design, develop, deploy, manage, and support advanced analytic and business performance management solutions for executive leadership teams. Works with peers outside immediate organization to define and characterize complex technology or process problems and/or develops new solutions, yet works independently to drive technical problems to a solution. Provides advanced technical consulting and advice to proposal efforts, solution design.\\nProduces strategies which assist company in becoming No. 1 in the market place. Actively participates in company professions program and Practice Improvement activities. Role models knowledge sharing and re-use within practice or profession. Proactively encourages Leads technically significant work on enterprise scale projects. Is recognized by peers as an expert in a particular area of technology. Sustain Architect custom solutions of project and program or operational scope. Architect reusable solutions of project or operational scope. Build custom reusable solutions of project and program or operational scope. Capture and share architectural IP at the project and program level. Assess business impact of specific technologies/strategies Identify and address technical or operational risks. Provide review/input on project activities for medium to large business unit level projects Collaborates with the project manager to develop detailed project plans and work breakdown structures for medium to large business unit level projects.\\nEducation and Experience Required:\\nTypically, a technical bachelor’s degree in information systems, computer science, or related field and a minimum of 8 years of related experience or a master’s degree and a minimum of 6 years of experience.\\nKnowledge and Skills:\\nExpert in Microsoft SQL BI solutions, architecture, design, development, data engineering, data warehousing, and data management. Extensive experience in Kimball method of data warehousing, data models, dimensional maintenance, and TSQL development in high-performance environments required. Experience in SSIS – Particularly using C# scripting object, C#, .Net with MVC, ClickOnce, and TFS or SVN experience\\nExpert in Microsoft SQL reporting solutions and development experience in T-SQL, stored procedures, complex reporting query development, query optimization, and DevOps\\n6+ years of technology services and business operations experience in a technology services organization\\nData engineering, analytics and systems subject matter expert on financial, workforce and operational systems for technology services business models\\nExperience in multiple solution development methodologies and participation in a fast paced, Dev/Ops environment\\nExperience in scorecard, dashboard and business analytics solutions for executive information reporting, analysis and decision support\\nExcellent analytical thinking, technical analysis, and data manipulation skills.\\nAbility to leverage new analytical techniques to develop creative approaches and insights.\\nCan validate/evaluate if an information systems or operational architecture meets business needs.\\nBe able to evaluate the effect of external factors on designed solution.\\nData, analytics and systems subject matter expert on leading financial, workforce and services solutions\\nAdvanced understanding of modern software design and development methodologies. Experience on multiple major full release project life cycles. Advanced understanding of modern SCM (software configuration management). Advanced understanding of testing tools and unit test and integration test scripting, and testing methodologies. Advanced experience using an Integrated Development Environment (e.g., Eclipse, Visual Studio) and development of tool add-ins.\\nExpert in defining quality and security standards.\\nAbility to work effectively in a globally dispersed team. High level of technical leadership skills.\\nStrong verbal and written communication and negotiation skills. Strong project management/team leader skills. Excellent oral and written communication skills for interacting with stakeholders. Strong presentation skills.\\nExpert analytical skills and the ability to synthesize change quickly using advanced subject and process knowledge. Excellent time management skills and ability to prioritize\\nAbility to generate original ideas and to bring about their implementation.\\nKnowledge of and ability to utilize a variety of tools and technologies to support multiple technologies.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "67  Overview:\\nWells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.\\n\\nSUCCESS PROFILE\\nCheck out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.\\n\\nAnalytical\\nDetail-oriented\\nInsightful\\nInventive\\nProblem Solver\\nCurious\\nBenefits\\nWells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:\\n\\nMedical, Dental and Vision\\nEmployer Matching 401(k)\\nTuition Reimbursment\\nMaternity and Paternity Leave\\nPaid Time Off\\nResponsibilties\\nJob Description\\nAt Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.\\nHelp us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.\\nEnterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.\\nAs part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, marketing, and virtual channels is looking for an experienced AI leader to manage the development of AI models for marketing.\\n\\nThis leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on marketing’s AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and marketing executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.\\nKEY RESPONSIBILITIES INCLUDE:\\nBuild and grow a team of data scientists responsible for AI model development in support of marketing\\nDesign, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions\\nPartner with marketing executives to frame the problem, manage the model development process, and business relationship\\nManage a portfolio of the data science projects including the following responsibilities:\\nHelp finalize project scope working with business partners\\nOn-going touch-base with business partners and governance stakeholders\\nDefine priorities in partnership with the business partners during on-going development\\nWork with AI technology and production teams to operationalize models\\nMay be called upon to review vendor models and solutions and/or models developed outside of EADS\\nDivisional Information:\\nData Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.\\nAs a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:\\nLead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.\\nAccomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.\\n\\nRequired Qualifications\\n\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques\\n4+ years of experience working with digital data science including analyzing cookie-level data\\n\\n\\nDesired Qualifications\\n\\nStrong analytical skills with high attention to detail and accuracy\\nAbility to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members\\nMeeting facilitation experience in leading discussions that result in consensus and commitment\\n\\n\\nOther Desired Qualifications\\nData science experience in the ad tech space (DMPs, DSPs, Google and/or Adobe Cloud, digital attribution)\\n4+ years managing or directing data scientist/ statistician/ data engineer teams\\nHands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet\\nDetail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable\\nExperience with agile project management methodologies for data science\\n\\nDisclaimer\\n\\nAll offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.\\n\\nRelevant military experience is considered for veterans and transitioning service men and women.\\n\\nWells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.\\n\\nENT FINANCE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "68  Where good people build rewarding careers.\\nThink that working in the insurance field can’t be exciting, rewarding and challenging? Think again. You’ll help us reinvent protection and retirement to improve customers’ lives. We’ll help you make an impact with our training and mentoring offerings. Here, you’ll have the opportunity to expand and apply your skills in ways you never thought possible. And you’ll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.\\nAbout our team\\n360 Finance Advanced Analytics data engineering team works with multiple internal and external data sources to deliver data that is readily available, easily accessible, accurate and complete. They are responsible for building a centralized data lake/hub using the Hadoop ecosystem that will be used by Reporting & Operational Analytics teams and the Machine learning teams.\\nJob Description\\nThis Lead Consultant is an experienced professional who is responsible for leveraging data and analytics to help automate and optimize Claims Analytics Data processes enabling our Claims employees to focus on serving our customers and delivering the most advanced claims experience on the planet. They will be responsible for the strategy around how we bring together complex data into clean and useful data structures making our valuable data more approachable.\\nKey Responsibilities\\nResponsible for design, prototyping and delivery of software solutions within the big data eco-system\\nLeading projects and/or serving as analytics SME to provide new or enhanced data to the business\\nImproving data governance and quality increasing the reliability of our data\\nInfluencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise\\nKey Responsibilities (Cont'd)\\nResponsible for designing and building new Big Data systems for turning data into actionable insights\\nTrain and mentor junior team members on Big Data/Hadoop tools and technologies\\nIdentifies opportunities for improvement and presents recommendations to management\\nDevelop solutions and iterates quickly to continuously improve\\nSeeks out and evaluates emerging big data technologies and open-source packages\\nParticipate in strategic planning discussions with technical and non-technical partners\\nUses, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).\\nUses, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e. Java, Python, SQL, R)\\nJob Qualifications\\nUndergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred\\n5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function\\nAbility to work with broad parameters in complex situations\\nExperience in developing, managing, and manipulating large, complex datasets\\nRoles and Responsibilities\\nExpert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.\\nSome understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.\\nExperience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required\\n5-6+ years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required\\nExperience with Agile development methodologies and tools to iterate quickly on product changes,\\nRoles and Responsibilities (Cont'd)\\ndeveloping user stories and working through backlog (Continuous Integration and JIRA a plus)\\nExperience with Airflow a plus\\nWorking knowledge of Tableau – a plus\\nAdvanced oral and written communication skills\\nStrong quantitative and analytical abilities\\nGood organizational and time management skills\\nAbility to manage and coach others\\nDecision making capabilities including problem solving approaches, decision frameworks; ability to design and lead complex analysis\\nStrong interpersonal skills\\nThe candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.\\nGood Work. Good Life. Good Hands®.\\nAs a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.\\nLearn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.\\nAllstate generally does not sponsor individuals for employment-based visas for this position.\\nEffective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.\\nFor jobs in San Francisco, please click “here” for information regarding the San Francisco Fair Chance Ordinance.\\nFor jobs in Los Angeles, please click “here” for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.\\nTo view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs\\nTo view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.\\nIt is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "69  Overview:\\nOur client is looking for a Data Engineer to join the R&D Team. You’re joining a multidisciplinary team of product directors, product managers, and digital analysts to create product listings across the top ecommerce marketplaces.\\n\\nWe are preferred supplier to 7-Eleven. This position may be available for conversion to full time employment upon successful evaluation of performance. You’ll be working in a fast-paced, eclectic environment of talented professionals who are leading the industry in digital capabilities.\\n\\nResponsibilities:\\nThe ideal candidate will be driven and passionate in creating the next generation of data products and capabilities. You will work directly with Product Owners and customers to deliver data products in a collaborative and agile environment. The ideal candidate will build data pipeline frameworks to automate high-volume and real-time data delivery and streaming data hub.\\n\\nBasic Qualifications:\\n\\nSkilled in Python and preferably in one or more programming languages like C++, Java, Go, etc\\nExperience with Docker and Kubernetes\\nExperience working with SQL and NoSQL based database solutions\\nPublic cloud technology experience in production (Azure, AWS, or Equivalent)\\n3+ years of collective experience in data engineering and data analysis\\n2+ years of experience architecting, building and administering large-scale distributed applications\\n\\nPreferred Qualifications:\\n\\nExperience in engineering data pipelines using Big Data technologies (Hadoop, Spark, Storm, Kafka, etc) on large scale unstructured data sets is a plus\\nFamiliarity with distributed data stores like Elasticsearch is a plus\\nFamiliarity with Machine Learning concepts is a plus\\n\\nGOIN Technology is a technology consulting and managed services business headquartered in Dallas/Ft. Worth. The company is focused on CIO/CTO Advisory Consulting, Information Security, Digital Transformation, Software/Data Engineering, Innovation as a Service and Managed Services.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "70  As a member of our Software Engineering Group we look first and foremost for people who are passionate around solving business problems through innovation & engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You’ll work in a collaborative, trusting, thought-provoking environment—one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.\\n\\nThis role requires a wide variety of strengths and capabilities, including:\\nStrong Data Engineer with strong command over PL/SQL, data integration/transformation tools like Pentaho, Informatica and exposure to Big Data technology stack (Hadoop, Spark, Hive, Impala, etc.).\\nUtilize Agile methodology and adhere to coding standards, procedures and techniques while contributing to the technical code documentation.\\nProvide high quality technology solutions that address business needs developing applications within mature technology environments.\\nConverting data to stories using advanced analytical and visualization techniques to help with data-driven decision making and management reporting.\\nDevops model - Design, develop, code, test, debug, document and support.\\nCollaborate with team and come up with solutions for any identified problem by team.\\nDeployment of newly build modules in QA and Production environment.\\nManage code quality for total build effort.\\nCoordinate with end users during User Acceptance Testing.\\nBS/BA degree or equivalent experience\\nAdvanced knowledge of application, data and infrastructure architecture disciplines\\nQualifications/Skills Required:\\nBachelor’s degree in Computer Science or any other relevant field.\\nStrong Database/SQL skills\\n5+ years hands on ETL skills – Informatica, Pentaho\\nUnix/Perl scripting\\nStrong knowledge of Data modeling/Data Warehousing concepts\\nExperience of a scheduling tool - CA/Autosys\\nExperience working within an Agile environment\\nDemonstrated analytical and problem solving skills.\\nDesirable Skills\\nHadoop / Spark / Hive, big data exposure is a big plus\\nExperience with any Reporting/Business Intelligence tools (Qlikview/Qliksense/Tableau/OBIEE)\\nCoding skills in Java/JavaScript/Python\\nFinancial industry experience\\nScrum/Agile knowledge\\nStrong verbal communication skills\\nOur Asset and Wealth Management division is driven by innovators like you who are driven to create technology solutions that make us work more efficiently and help our businesses grow. It’s our mission to efficiently take care of our clients’ wealth, helping them get, and remain properly invested. Across 27 cities, our team of 4,600 agile technologists thrive in a cloud-native environment that values continuous learning using a data-centric approach in developing innovative technology solutions.\\n\\nWhen you work at JPMorgan Chase & Co., you’re not just working at a global financial institution. You’re an integral part of one of the world’s biggest tech companies. In 20 technology centers worldwide, our team of 50,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $10B+ annual investment in technology enables us to hire people to create innovative solutions that will are transforming the financial services industry.\\n\\n\\nAt JPMorgan Chase & Co. we value the unique skills of every employee, and we’re building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you’re looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "71  Overview:\\nWells Fargo technology teams drive innovation to create a more powerful and fulfilling financial experience for our customers and team members. You will join more than 24,000 team members supporting 95 billion transactions annually in 10 countries. Our career opportunities span the technology spectrum: advanced analytics, big data, information security, application development, cloud enablement, project management and more.\\n\\nSUCCESS PROFILE\\nCheck out the top traits we're looking for and see if you have the right mix. Additional related traits listed below.\\n\\nAnalytical\\nDetail-oriented\\nInsightful\\nInventive\\nProblem Solver\\nCurious\\nBenefits\\nWells Fargo wants to help you get more out of life and take care of things outside the office to make life a little easier. We provide:\\n\\nMedical, Dental and Vision\\nEmployer Matching 401(k)\\nTuition Reimbursment\\nMaternity and Paternity Leave\\nPaid Time Off\\nResponsibilties\\nJob Description\\nAt Wells Fargo, we want to satisfy our customers’ financial needs and help them succeed financially. We’re looking for talented people who will put our customers at the center of everything we do. Join our diverse and inclusive team where you’ll feel valued and inspired to contribute your unique skills and experience.\\nHelp us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.\\nEnterprise Finance drives financial management for the company and maintains and enhances risk and financial controls. Key functions within Enterprise Finance include finance and accounting; Treasury; corporate development, mergers, and acquisitions; Data Management and Insights, the Customer Remediation Center of Excellence, Enterprise Shared Services, Business Process Management, and Corporate Strategy. Enterprise Finance informs shareholders, regulators, taxing authorities, team members, and leaders of the company’s financial performance through earnings releases, investor meetings and conferences, and meetings with regulators and credit rating agencies, following appropriate reporting guidelines. They also maintain and enhance risk and financial controls and lead many of the company’s shared services functions including corporate properties, security, and global services.\\nAs part of the newly formed AI Model Development COE in Enterprise Analytics and Data Science (EADS), which focuses on building, implementing, and monitoring AI models for the enterprise, the AI model development team for personalization, Virtual Channels, and virtual channels is looking for an experienced AI leader to manage the development of AI models for Virtual Channels.\\n\\nThis leader will be responsible for building and managing a team of data scientists to design, develop, and implement AI models focused on Virtual Channels’ AI priorities. Partnering with Wells Fargo AI technology team, Wells Fargo AI business solution team, and Virtual Channels executives, you and your team will deliver and deploy AI models on the Well Fargo AI open source platform to scale these solutions and embed them in our operational processes.\\n\\nKEY RESPONSIBILITIES INCLUDE:\\nBuild and grow a team of data scientists responsible for AI model development in support of Virtual Channels\\nDesign, develop, and deploy AI models using state of the art AI techniques available in the open stack and/or vendor solutions\\nPartner with Virtual Channels executives to frame the problem, manage the model development process, and business relationship\\nManage a portfolio of the data science projects including the following responsibilities:\\nHelp finalize project scope working with business partners\\nOn-going touch-base with business partners and governance stakeholders\\nDefine priorities in partnership with the business partners during on-going development\\nWork with AI technology and production teams to operationalize models\\nWill be called upon to review vendor models and solutions and/or models developed outside of EADS\\nDivisional Information:\\nData Management and Insights (DMI) is transforming the way that Wells Fargo uses and manages data. Our work enables Wells Fargo to empower and inform our team members, deliver exceptional experiences for our customers, and meet the elevated expectations of our regulators. The team is responsible for designing the future data environment, defining data governance and oversight, and partnering with technology to operate the data infrastructure for the company. This team also provides next generation analytic insights to drive business strategies and help meet our commitment to satisfy our customers’ financial needs.\\nAs a Team Member Manager, you are expected to achieve success by leading yourself, your team, and the business. Specifically you will:\\nLead your team with integrity and create an environment where your team members feel included, valued, and supported to do work that energizes them.\\nAccomplish management responsibilities which include sourcing and hiring talented team members, providing ongoing coaching and feedback, recognizing and developing team members, identifying and managing risks, and completing daily management tasks.\\n\\nRequired Qualifications\\n\\n8+ years of experience in analytics, modeling, or a combination of both\\n6+ years of management experience; or 6+ years of leadership experience in an advanced quantitative analytics function\\nA Master's degree or higher\\n5 + years of experience using quantitative machine learning techniques\\n\\n\\nDesired Qualifications\\n\\nStrong analytical skills with high attention to detail and accuracy\\nAbility to work and influence successfully within a matrix environment and build effective business partnerships with all levels of team members\\nMeeting facilitation experience in leading discussions that result in consensus and commitment\\n\\n\\nOther Desired Qualifications\\n4+ years managing or directing data scientist/ statistician/ data engineer teams\\nHands on experience with deep learning toolkits such as Tensorflow, Keras, PyTorch, Dynet\\nDetail oriented. Experience with model governance requirements. Able to de-mystify AI models to make them transparent and explainable\\nExperience with agile project management methodologies for data science\\nExperience with Big Data or Hadoop tools such as Spark, Hive, Kafka and Map\\n\\nStreet Address\\nNC-Charlotte: 401 S Tryon St - Charlotte, NC\\nMN-Minneapolis: 600 S 4th St - Minneapolis, MN\\nNC-Charlotte: 11625 N Community House Road - Charlotte, NC\\nSC-Fort Mill: 3480 State View Blvd - Fort Mill, SC\\nTX-Addison: 5080 Spectrum Dr - Addison, TX\\nTX-DAL-Downtown Dallas: 1445 Ross Ave - Dallas, TX\\nTX-Irving: 5000 Riverside Drive - Irving, TX\\nAZ-Tempe: 1150 W Washington St - Tempe, AZ\\nIA-Des Moines: 6200 Park Ave - Des Moines, IA\\nIA-Des Moines: 800 Walnut St - Des Moines, IA\\nGA-Atlanta: 3579 Atlanta Ave - Atlanta, GA\\n\\n\\nDisclaimer\\n\\nAll offers for employment with Wells Fargo are contingent upon the candidate having successfully completed a criminal background check. Wells Fargo will consider qualified candidates with criminal histories in a manner consistent with the requirements of applicable local, state and Federal law, including Section 19 of the Federal Deposit Insurance Act.\\n\\nRelevant military experience is considered for veterans and transitioning service men and women.\\n\\nWells Fargo is an Affirmative Action and Equal Opportunity Employer, Minority/Female/Disabled/Veteran/Gender Identity/Sexual Orientation.\\n\\nENT FINANCE  \n",
       "\n",
       "[72 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_Dallas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
