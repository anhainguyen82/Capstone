,Title,Location,City,State,Zip,Country,Qualifications,Skills,Responsibilities,Education,Requirement,FullDescriptions
0,Big Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions for the advertising industry and we're looking for a Data Engineer to join our Data Engineering team. If you are excited by technology that has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day; and evaluate thousands of data-points in real-time all while responding in just a few milliseconds, then IAS is the place for you!

As a Data Engineer you will build and expand upon the testing framework and testing infrastructure of IAS' core ad verification, analytics and anti ad fraud software products.

The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment. You should be able to not take yourself too seriously as well.

What you'll do:

Working on Big Data technologies such as Hadoop, MapReduce, Kafka, and/or Spark in columnar databases
Architect, design, code and maintain components for aggregating tens of billions of daily transactions
Lead the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming and batch ETL's and RESTful API's
Mentor junior team members

Who you are and what you have:

Up to 5 years of recent hands-on experience with object oriented languages (Java, Scala, Python)
Strong knowledge of collections, multi-threading, JVM memory model, etc.
Great understanding of designing for performance, scalability, and reliability
Superb understanding of algorithms, data structures, scalability and various tradeoffs in a Big Data setting
In-depth understanding of object oriented programming concepts
Excellent interpersonal and communication skills
Understanding of full software development life cycle, agile development and continuous integration
Good knowledge of Linux command line tools
Experience with Hadoop MapReduce, Spark, Pig
Good understanding of database fundamentals, good knowledge of SQL

What puts you over the top:

Exposure to messaging frameworks like Kafka or RabbitMQ
Some exposure to functional programming languages like Scala
Experience with Spark

About Integral Ad Science

Integral Ad Science (IAS) is the global market leader in digital ad verification, offering technologies that drive high-quality advertising media. IAS equips advertisers and publishers with both the insight and technology to protect their advertising investments from fraud and unsafe environments as well as to capture consumer attention, and drive business outcomes. Founded in 2009, IAS is headquartered in New York with global operations in 17 offices across 13 countries. IAS is part of the Vista Equity Partners portfolio of software companies. For more on how IAS is powering great impressions for top publishers and advertisers around the world, visit integralads.com ( http://integralads.com/ ).

Equal Opportunity Employer:
IAS is an equal opportunity employer, committed to our diversity and inclusiveness. We will consider all qualified applicants without regard to race, color, nationality, gender, gender identity or expression, sexual orientation, religion, disability or age. We strongly encourage women, people of color, members of the LGBTQIA community, people with disabilities and veterans to apply.

To learn more about us, please visit http://integralads.com/ ( http://integralads.com/ ) and https://muse.cm/2t8eGlN ( https://muse.cm/2t8eGlN )

Attention agency/3rd party recruiters: IAS does not accept any unsolicited resumes or candidate profiles. If you are interested in becoming an IAS recruiting partner, please send an email introducing your company to recruitingagencies@integralads.com. We will get back to you if there's interest in a partnership."
1,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Why VillageMD?

VillageMD is changing the trajectory of healthcare by empowering primary care physicians to make informed decisions and engage patients in meaningful ways. We work with thousands of clinicians and healthcare disruptors across the country to build and contribute to our platform to improve patient health while driving down the cost to deliver it.

We are a mission-oriented organization and are thrilled about the work that we do every day. We're transparent, collaborative, and relentless in pursuit of our mission, all while doing so with humility and a low ego. We believe that diverse backgrounds and experiences create the best opportunity for innovation and the community that we are creating is greater than any individual.

We've built our technology using the best of cloud and open-source technologies to create an open, data-first platform that is enriched with analytical models and modernly connected to internal and external apps. These apps drive clinical decision support, patient engagement, and other facilitators of innovative, information-enriched health experiences.

Data Engineers at VillageMD build distributed components, pipelines, and tools that enable our organization to make analytical, data-driven decisions. We're in a unique position to impact everyone in primary care from independent, family-owned practices to world-class health systems. We aggregate, process, and deliver rich datasets to improve the effectiveness of primary care for our doctors and patients.

What are examples of work that Data Engineers have done at VillageMD?


Built and implemented a data profiling tool to reverse engineer data schemas from new data sources facilitating normalization of the data into our data model
Created a summary data platform supporting our presentation layer that allows clinicians and operators in our practices to pinpoint interventions on-demand to patients most in need
Analyzed and designed the best ways to expand our data model to incorporate more data that's mission critical

What will make you successful here?


Strong analytical and technical skills
A real passion for problem solving and learning new technology
Vision to balance speed and maintainability in solution design
The ability to handle multiple, concurrent projects
Crafting and implementing requirements, keeping projects on track, and engaging partners
Challenging the status quo to improve our processes and tools
Communicating complex technical details in meaningful business context
A low ego and humility; an ability to gain trust by doing what you say you will do

What you might do in your first year:

Own ten projects to design and implement best-in-class data processing enabling clean data flow directly to our data model and on to our presentation layer
Work with analytics, engineering and operations to design and implement a new analytics product that supports improving patient health
Design a new concept within our data model to meet a new operational or analytical need

The following experience is relevant to us:

5+ years of full-time experience including extensive experience with healthcare data
Ability to understand and design relational data structures required
Very strong capabilities manipulating data using SQL
Knowledge of, and/or willingness to learn, non-relational data structures and other technologies (e.g. Postgres, Redshift, Cassandra, MongoDB, Neo4j, S3, etc.)
Experience or willingness to learn building information pipelines utilizing Python or Java a plus
BS/MS in computer science, math, engineering, or other related fields is required.
Track record of successfully executing projects with multiple partners

What can we offer you?


Competitive salary, bonus, and health benefits
Paid gym membership
Fun, fast-paced, startup environment (with snacks)
Pre-tax savings on commute expenses
Remote flexibility
A highly-collaborative, conscientious, forward-thinking environment that welcomes the impact you can make from Day 1.
A clear link between our daily work on products and services and the improved quality of healthcare that this work facilitates for patients.

At VillageMD, we see diversity and inclusion as a source of strength in transforming healthcare. We believe building trust and innovation are best achieved through diverse perspectives. To us, acceptance and respect are rooted in an understanding that people do not experience things in the same way, including our healthcare system. Individuals seeking employment at VillageMD are considered without regard to race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
2,Cloud Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Who We Are!
At Maven Wave, we are relentless in hiring the industry’s top talent. Each employee is hand-picked not only for their skills, but for their personality and broad expertise. We are looking for this rare combination of talent that sets us apart in the industry.
Maven Wave helps leading companies make the shift to digital and shorten the fuse to innovation. We combine the expertise of top-tier consulting with the agility of a cutting-edge technology firm. This multidisciplinary blend of skills allows us to create unique digital advantages for our clients. Maven Wave’s digital solutions are agile, mobile, rooted in analytics, and built in the cloud.

Maven Wave, Google, and YOU: Help us build data driven cloud solutions.
We are looking for a skilled Cloud Data Engineer to design, build, and test data ingestion and ETL programs with a strong focus on performance and data quality management.

Your Life As a Maven:
Build and implement complex data solutions in the cloud (AWS, GCP and/or Microsoft).
Uncover and recommend remediations for data quality anomalies.
Investigate, recommend and implement data ingestion and ETL performance improvements.
Document data ingestion and ETL program designs, present findings, conduct peer code reviews.
Develop and execute test plans to validate code.
 Your Expertise:
4+ years experience building complex ETL programs with Informatica, DataStage, Spark, Dataflow, etc.
3+ years experience in Python and/or Java, developing complex SQL queries, and working with relational database technologies.
Experience configuring big data solutions in a cloud environment (AWS, Azure or GCP).
Experience using cloud storage and computing technologies such as BigQuery, RedShift, or Snowflake.
Experience developing complex technical and ETL programs within a Hadoop ecosystem.
 Your X-Factor:
Aptitude - You have an innate capacity to transition from project to project without skipping a beat.
Communication - You have excellent written and verbal communication skills for coordination across projects and teams.
Impact - You are a critical thinker with an emphasis on creativity and innovation.
Passion - You have the drive to succeed paired with a continuous hunger to learn.
Leadership - You are trusted, empathetic, accountable, and empower others around you.
Why We’re Proud To Be Mavens!
Google Cloud North America Services Partner of the Year 2019, 2018
#21 Best Workplaces in Chicago, FORTUNE, 2018
Great Place To Work Certification, Great Place to Work, 2017 & 2018
Fast Fifty, Crain's Chicago Business
101 Best and Brightest Companies to Work For, National Association for Business Resources (NABR)
Top Google Cloud Partner, Clutch
Fastest Growing Consulting Firms in North America (#11, #37), Consulting Magazine
Top IT Services Companies, Clutch
Google Global Rising Star Partner of the Year
Ready to Learn More?
Life as a Maven
Check out the Apps and Data Team
See what Glassdoor has to say
Real Customer Stories"
3,Azure Data Architect,"Chicago, IL",Chicago,IL,None Found,None Found,"At least 5 years of consulting or client service delivery experience on Azure
",DevOps on an Azure platform,None Found,None Found," Proven ability to build, manage and foster a team-oriented environment
","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Azure Technical Architect is a highly performant Azure Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data solutions on cloud. Using Azure public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today's corporate and emerging digital applications.

Role & Responsibilities:Work with Sales and Bus Dev teams in providing Azure Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Qualifications
Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 10 years of experience in big data, database and data warehouse architecture and delivery
Minimum of 5 years of professional experience in 2 of the following areas:
§ Solution/technical architecture in the cloud
§ Big Data/analytics/information analysis/database management in the cloud
§ IoT/event-driven/microservices in the cloud
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
 - Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
Strong in Power BI, Java, C##, Spark, PySpark, Unix shell/Perl scripting
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
4,Big Data Engineer - Senior Consultant,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.

We are...

 • The Industry-recognized data and analytics leaders
 • Passionate problem solvers across a broad spectrum of technologies and industries
 • Value seekers for measurable business outcomes
 • Continuous learners through training and education
 • Focused on a work-life balance with an unlimited paid time off policy

Big Data engineers are challenged with building the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally. Our engagements typically target a variety of use cases across data engineering, data science, data governance, and visualization.
Big Data Engineers deliver value through...
Hands-on, self-directed design and development of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools
Demonstration of technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven insights
Collaboration with team members, business stakeholders and data SMEs to elicit requirements and to develop business metrics and analytical insights
Internal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO
A Big Data Engineer's skills include, but are not limited to...
Bachelors Degree and 5+ years of work experience
5+ years of professional IT work experience
SQL, SQL, SQL!
2+ years of Spark
Programming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)
Linux / Windows (Command line)
Big Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop)
Cloud Platforms (AWS, Azure, Google Cloud Platform)
Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)
Data Integration Tools (Talend, DataStage, Informatica, SSIS)
Databases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)
Markup Languages (JSON, XML, YAML)
Code Management Tools (Git/GitHub, SVN, TFS)
DevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)
Testing / Data Quality (TDD, unit, regression, automation)
Solving complex data and technology problems
Leading technical teams of 2+ consultants
Ability to design components of a larger implementation
Excellent communication to narrate data driven insights and technical approach

If this sounds like you, let’s talk!

Candidates must be comfortable with a national travel model to client locations weekly (M-TH is typical).

Clarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.
 
#LI-NT1
GLDR"
5,Consultant - Data Engineer,"Rolling Meadows, IL",Rolling Meadows,IL,None Found,None Found,None Found,"Development knowledge for integrating components and contributing to core code base - Java preferred
Solid understanding of database and data warehousing technologies
Knowledge of SQL as well as NoSQL queries, syntax, and technologies
Knowledge of big data requirements, applications, and technologies such as Hadoop
Knowledge of ETL methods and approaches including triggers, named views, temporary tables, etc.
Linux expertise
","Specific responsibilities include:
Help design an architecture for federated data stores and data fusion
Help design methods for storing data in a way that facilitates extremely fast data parsing and management
Implement ""glue code"" that connects middle tier components with backend components
Implement data management and analytics code utilizing data architecture (e.g. map reduce)
Collaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores
Collaborate with data architects to understand the applications we integrate with and the data they produce
Review requirements for new approaches to big data storage and analytics
Design methods for caching, paging, and integrating real-time data with historical data stores
",None Found,"Development knowledge for integrating components and contributing to core code base - Java preferred
Solid understanding of database and data warehousing technologies
Knowledge of SQL as well as NoSQL queries, syntax, and technologies
Knowledge of big data requirements, applications, and technologies such as Hadoop
Knowledge of ETL methods and approaches including triggers, named views, temporary tables, etc.
Linux expertise
","Insygnum needs a Consultant - Data Engineer to help our clients for data analysis, data integration and data quality. Our Chicago-based team is small but growing fast and we need to complement our in-house experts who knows how to tame challenging data. This is a unique opportunity to not only work with cool technology, but also to create a new methodologies and techniques. You'll get in on the ground floor of a new company, help shape its future, and benefit directly from your work.
Why work here
Joining insygnum now offers several unique opportunities
You will receive competitive salary, benefits, and stock options
You will be working on hard, interesting problems
You will help shape the culture of the company as we grow
You will have the opportunity to apply your skills in a meaningful way and have a real-world impact
Responsibilities
You'll help design a new system for capturing, storing, analyzing, and acting on performance, security, and network data. The ideal candidate will have a solid grasp of several different database and data warehousing technologies to help architect ETL for. Technologies to be used may include some combination of relational databases (PostgreSQL, Teradata, Aster, HANA), NoSQL, Hadoop, Object-based stores, and OLAP.
Specific responsibilities include:
Help design an architecture for federated data stores and data fusion
Help design methods for storing data in a way that facilitates extremely fast data parsing and management
Implement ""glue code"" that connects middle tier components with backend components
Implement data management and analytics code utilizing data architecture (e.g. map reduce)
Collaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores
Collaborate with data architects to understand the applications we integrate with and the data they produce
Review requirements for new approaches to big data storage and analytics
Design methods for caching, paging, and integrating real-time data with historical data stores
Desired Skills and Experience
Requirements
Development knowledge for integrating components and contributing to core code base - Java preferred
Solid understanding of database and data warehousing technologies
Knowledge of SQL as well as NoSQL queries, syntax, and technologies
Knowledge of big data requirements, applications, and technologies such as Hadoop
Knowledge of ETL methods and approaches including triggers, named views, temporary tables, etc.
Linux expertise
Bonus Points
Java is strongly preferred (e.g. for working with map reduce) but not ultimately a requirement if you excel in other areas
Strong SQL skills are highly desirable
OLAP experience
Experience with ETL tools like Informatica, Boomi, Pentaho, AbIntio, Datastage, etc.,
Contact: HR Manager
Email: hr@insygnum.com
Phone: (224)-800-1002"
6,Lead Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.

As a Lead Data Engineer, here's what we'll be looking for you to bring:


Hands-on Engineering Leadership
Proven track record of Innovation and expertise in Data Engineering
Tenure in coding, architecting and delivering complex projects
Deep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others
Deep understanding of streaming data architectures and technologies for real-time and low-latency data processing
Deep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies
Understanding of how to architect solutions for data science and analytics such as productionizing machine learning models and collaborating with data scientists
Understanding of agile development methods including: core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Passion for software craftsmanship
A rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..
Strong stakeholder management and interaction experience at different levels

There's no typical day or engagement for our Senior Data Engineers. Here's what you'll do:


Be the SME. Develop modern data architectural approaches to meet key business objectives and provide end to end data solutions
You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems.
On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.
It could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.
Whatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.
You have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.
You recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.

A few important things to know:
-------------------------------

Projects are almost exclusively on customer site, so candidates should be flexible and open to travel.

Candidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.

Not quite ready to apply? Or maybe this isn't the right role for you? That's OK, you can stay in touch with AccessThoughtWorks ( https://www.thoughtworks.com/careers/access?utm_source=apply-jobs&utm_medium=jd&utm_campaign=access-thoughtworks ), our learning community (click ""contact me about recruitment opportunities"" to hear about jobs in the future).

It is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.

#LI-NA"
7,Google Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
",DevOps on an GCP platform. Multi-cloud experience a plus.,None Found,None Found,"Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet today’s high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps – Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
8,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,"Breadth - Substantial experience as a high-performance Engineer across multiple environments on high-performance data systems that process data across various sources at a near real-time pace
At least 1 project where you personally designed, implemented, and operated large scale, high throughput data pipelines with a focus on high data quality
At least 1 other project where you built a secure, reliable, scalable system on AWS that saw significant traffic
Depth - You can go up and down the stack from deep in the infrastructure up to the data, application, and client layers
Speed - Experience with small teams that move fast - all members are expected to be able to achieve maximum results with minimal direction
Ownership and accountability - You own the things that you build
Drive - When you see a need, you fill a need. You can step in where you see a need and push us all forward
Modern Infra - Hands-on experience with Kubernetes, Airflow, and Kafka
Modern CI/CD - Hands-on experience with Codefresh, Spinnaker, Jenkins or similar
",None Found,None Found,None Found,None Found,"At Otus, a fast-growing EdTech company based at 1K Fulton in the West Loop of Chicago, you will work with people passionate about improving the lives of teachers and students. We are a group of talented designers, developers, coaches, and leaders. We love our work and strive to do our best each day.

On our team, you will find musicians, beer enthusiasts, designer toy collectors, table tennis fanatics, and more.


Your Role
Otus is a building a next generation Platform for EdTech in order to support our transformative vision for K-12 education across the US
Model and architect our data in a way that will scale with the increasingly complex ways we’re analyzing it
Build robust pipelines that make sure data is where it needs to be, when it needs to be there
Build frameworks and tools to help others design and build their own data pipelines in a self-service manner
Performance testing and engineering to ensure that our systems always scale to meet our needs
Key member of the team focused on pure hands-on contribution to the implementation and operation of our data platform


Qualifications
Breadth - Substantial experience as a high-performance Engineer across multiple environments on high-performance data systems that process data across various sources at a near real-time pace
At least 1 project where you personally designed, implemented, and operated large scale, high throughput data pipelines with a focus on high data quality
At least 1 other project where you built a secure, reliable, scalable system on AWS that saw significant traffic
Depth - You can go up and down the stack from deep in the infrastructure up to the data, application, and client layers
Speed - Experience with small teams that move fast - all members are expected to be able to achieve maximum results with minimal direction
Ownership and accountability - You own the things that you build
Drive - When you see a need, you fill a need. You can step in where you see a need and push us all forward
Modern Infra - Hands-on experience with Kubernetes, Airflow, and Kafka
Modern CI/CD - Hands-on experience with Codefresh, Spinnaker, Jenkins or similar


Benefits and Perks
Competitive salary and stock options
20 PTO days a year, plus a day off for your birthday (or a loved one), an interest day (to do something you love) and paid holidays
Up to 12 weeks of parental leave, and great work/life balance
Flexible hours and work from home policy so you can be at your most productive
Excellent medical, dental, and vision insurance
Life Insurance and disability benefits
401K with an employer match (up to 4%)
Working in arguably the best neighborhood in the city with the excellent food, drinks, coffee and donuts all within walking distance
Working in an amazing office building with gym, rooftop deck, frequent social events, shuttle to and from train stations
Otus is an Equal Opportunity Employer and embraces diversity of every kind. You must be legally authorized to work in the US. Unfortunately, the company is unable to support sponsorships at this time."
9,Sr. Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,"
Ingestion of data from multiple, unstructured sources using multiple analytics tools
Implementing ETL process
Monitoring performance and advising any necessary infrastructure changes
Defining data retention policies",None Found,"
Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.","Job Title: Data Engineer

Location: San Francisco, Chicago, San Jose, Palo Alto, Austin, TX

Terms: Full-time, Contract, Contract-2-Hire

About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.

What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.

As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do

Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:

Cloud
Analytics
Digitization
Infrastructure
Security

Sr. Data Engineer
Job Description
Responsibilities
Ingestion of data from multiple, unstructured sources using multiple analytics tools
Implementing ETL process
Monitoring performance and advising any necessary infrastructure changes
Defining data retention policies

Requirements
3+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct query performance tuning
Strong skills in one of the scripting language (Python, Ruby, Bash)
1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)

We are Growing Rapidly: 2019 Highlights

Trianz is growing rapidly. Here are some highlights.

Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.

Won the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.

Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.

Featured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.

Achieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.

Talk to us, Join us & Develop into Leaders
Come join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.
 We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
 Equal Opportunity Employer
Trianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law)."
10,Software Development Engineer I,"Chicago, IL 60661",Chicago,IL,60661,None Found,None Found,None Found,None Found,None Found,None Found,"Expedia
Do you have a passion for travel and e-Commerce? Do you have a passion for operating globally in a high growth, energizing environment? Expedia’s eCommerce Platform (eCP) team has an opening for a Software Development Engineer I (Data Engineer) as part of our Financial Systems and Core Transactional Services (FCTS) team. The online travel market is a $2 trillion industry that never stands still. This 400-person organization operating between Expedia’s Global Brands, Supply Offerings, and Finance Teams, is the transactional heartbeat of Expedia Inc. FCTS is composed of two key platform components – Financial Systems, primarily based on the Oracle E-Business Suite (EBS); and the Core Transaction Platform that comprises Order and Fulfillment Orchestration, Trip Management, Incentive Services, and Booking Data Engineering. Through our technology and operations infrastructure, we handle billions of dollars of bookings, collections, and payments every month. This role will primarily focus on the Core Transaction Platform, one of the platforms operated by FCTS.
What you’ll do
Design and Architect scalable systems with a strong bias for BI & analytics approach
Create ETLs/ELTs to take data from various operational systems and provide data for analytics and reporting
Work with multi-terabyte data sets using relational databases (RDBMS) and SQL
Ability to handle ambiguity
Support business decisions with ad hoc analysis as needed.
Excellent problem-solving and debugging prowess with an understanding of testing practices
Communication and cross group partnership skills
Who you are
You will have at least 2+ years of experience with detailed knowledge of data warehouse technical architectures, ETL/ELT and reporting/analytic tools
2+ years of experience with any scripting language (Ruby, python, etc.)
2+ years of experience using Informatica, Teradata and similar technologies
Experience working with multi-terabyte data sets using relational databases (RDBMS) and SQL
1+ year of experience with BI implementation in the Cloud
Good familiarity with Linux/Unix
Expedia Group recognizes our success is dependent on the success of our people. We are the world's travel platform, made up of the most knowledgeable, passionate, and creative people in our business. Our brands recognize the power of travel to break down barriers and make people's lives better – that responsibility inspires us to be the place where exceptional people want to do their best work, and to provide them the tools to do so.
Whether you're applying to work in engineering or customer support, marketing or lodging supply, at Expedia Group we act as one team, working towards a common goal; to bring the world within reach. We relentlessly strive for better, but not at the cost of the customer. We act with humility and optimism, respecting ideas big and small. We value diversity and voices of all volumes. We are a global organization but keep our feet on the ground, so we can act fast and stay simple. Our teams also have the chance to give back on a local level and make a difference through our corporate social responsibility program, Expedia Cares.
If you have a hunger to make a difference with one of the most loved consumer brands in the world and to work in the dynamic travel industry, this is the job for you.
Our family of travel brands includes: Brand Expedia®, Hotels.com®, Expedia® Partner Solutions, Egencia®, trivago®, HomeAway®, Orbitz®, Travelocity®, Wotif®, lastminute.com.au®, ebookers®, CheapTickets®, Hotwire®, Classic Vacations®, Expedia® Media Solutions, CarRentals.com™, Expedia Local Expert®, Expedia® CruiseShipCenters®, SilverRail Technologies, Inc., ALICE and Traveldoo®.LI-AN1
Expedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. This employer participates in E-Verify. The employer will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization."
11,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"McDonalds is looking to hire a Data Engineer. The
role will work closely with data architects and includes designing,
architecting, and building data pipeline to support business use cases.
Responsibilities also include collaborating with business leaders to translate
business requirements into technical, scalable solution.

Responsibilities

·
Bachelor’s
or Master’s Degree in Mathematics, Computer Science, or Information technology
preferred

·
Ability
to present to senior leadership and partners

·
Experience
managing applications in AWS and familiarity with core services including EC2,
S3, RDS, Redshift, EMR.

·
Experience
in ETL and data warehouse technologies (Oracle, SQL Server, etc.)

·
Skilled
manipulating Big Data using HDFS/Hadoop eco system tools

·
Familiarity
with modern Machine Learning techniques

·
Experience
and desire to work in a Global delivery environment is a plus

·
Strong
knowledge of relational and multi-dimensional database architecture

·
Strong
verbal and written communication skills, and ability to synthesize technical
information for a business audience

Minimum Requirements

Who
are we?

We are proud to be one of the most recognized brands
in the world, with restaurants in over 100 countries and billions of customers
served each year. McDonald's is people business just as much as we are a
restaurant business. We strive to be the most inclusive brand on the globe by
building a workforce with different strengths who make delicious, feel good
moments that are easy for everyone to enjoy.

At McDonald's, we are dedicated to using our scale
for good: good for people, our industry and the planet. We see every single day
as a chance to have a lasting impact on our customers, our people and our
partners. We will continue to pursue big, global initiatives while remaining
kind neighbors and supporters of our local communities.

We are moving fast and are building a passionate
team to help us. This means the company is looking for innovators, leaders,
sprinters who are focused on crafting memorable experiences for our customers,
employees and partners. Joining McDonald's means thinking big daily and
preparing for a career that can have impact around the world.

We are an
equal opportunity employer and value diversity at our company. We do not
discriminate on the basis of race, religion, color, national origin, gender,
sexual orientation, age, marital status, veteran status, or disability status.

CountryUnited States
Requisition Number

7033BR

EOE Statement

McDonald’s Corporation is an equal opportunity employer committed to a diverse and inclusive workforce."
12,Data Engineer,"Chicago, IL 60654",Chicago,IL,60654,None Found,"
Minimum 3-5 years of experience is required
B.S. in Computer Science or closely related field preferred, but not required. Real-world experience and proven track records count as much, if not more.
Programming Languages: Python and C# (C++ and R are a plus).
Experience with databricks is a plus.
Established expertise developing ETL pipelines on serverless cloud solutions. (AWS Lambda, Azure App Services, etc..)
Linux/Unix
Docker
Databases: SQL, JSON, object storage, etc. Experience with graph databases like Neo4j and TigerGraph is a plus.
Strong technical writing skills will be heavily stressed
",None Found,"
Build automated ETL pipelines for cloud environments including, AWS, GCP, Azure, and Heroku.
Develop and support data integrity reporting and alerting for ETL pipelines.
Ability to work closely with Engineering, Product and Customer Success Teams.
Build robust and deployable software in Python, C++ or C#.
Build, deploy, and maintain RESTful APIs to access datasets.
Parse and extract data from common formats including, XML, JSON, CSV, and Pipe delimited.
Integrate with customer provided APIs.
Build, organize, and maintain datamarts using any of SQL, JSON, Blob, or other databases as needed.
Write and maintain excellent documentation of all work.
",None Found,None Found,"Chicago, IL

Data Engineering Full-Time
Company Description

rMark Bio helps life science companies solve for the complexities that come with digital transformation by developing end-to-end AI solutions that deliver personalized business intelligence through integrated applications and API accessible services.

Healthcare innovation is best served when individuals with diverse backgrounds come together with a common purpose and clear objectives to improve patient lives.
We are product strategists, engineers, data scientists and designers who are experts in our domain and passionate about our mission to accelerate innovation, collaboration and scientific discovery for life sciences
Job Description

rMark Bio, is searching for an experienced Data Engineer to work within an agile team of data scientists, software architects and developers to implement and maintain secure, scalable, cloud-based software solutions, build customer integration solutions, and streamline the team’s software delivery tools and processes. Technical aptitude is a must as well as the right team-minded attitude and the ability to work interchangeably with others. The core team is as a small SWAT-style team with everyone pulling their own weight, playing a variety of roles, and covering each others’ responsibilities when needed. Applicants should be qualified in collecting, cleaning, and maintaining large datasets that are critical to customer and product success.
Job Responsibilities

Build automated ETL pipelines for cloud environments including, AWS, GCP, Azure, and Heroku.
Develop and support data integrity reporting and alerting for ETL pipelines.
Ability to work closely with Engineering, Product and Customer Success Teams.
Build robust and deployable software in Python, C++ or C#.
Build, deploy, and maintain RESTful APIs to access datasets.
Parse and extract data from common formats including, XML, JSON, CSV, and Pipe delimited.
Integrate with customer provided APIs.
Build, organize, and maintain datamarts using any of SQL, JSON, Blob, or other databases as needed.
Write and maintain excellent documentation of all work.
Experience and Qualifications

Minimum 3-5 years of experience is required
B.S. in Computer Science or closely related field preferred, but not required. Real-world experience and proven track records count as much, if not more.
Programming Languages: Python and C# (C++ and R are a plus).
Experience with databricks is a plus.
Established expertise developing ETL pipelines on serverless cloud solutions. (AWS Lambda, Azure App Services, etc..)
Linux/Unix
Docker
Databases: SQL, JSON, object storage, etc. Experience with graph databases like Neo4j and TigerGraph is a plus.
Strong technical writing skills will be heavily stressed
If you are a recruiter or placement agency, please do not submit resumes to any person or email address at rMark Bio prior to having a signed agreement from rMark Bio’s HR department. rMark Bio is not liable for and will not pay placement fees for candidates submitted by any agency other than its prior-approved recruitment partners. Furthermore, any resumes sent to us without a written signed agreement in place will be considered your company’s gift to rMark Bio. and may be forwarded to our recruiters for their attention. Thank you.

rMark Bio is an equal opportunity employer. All qualified applicants for employment will be considered without regard to race, color, religion, sex, gender identity, sexual orientation, national origin, status as an individual with a disability, veteran status, or any other basis protected by federal, state, or local law."
13,Senior Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,"
Mastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role
",None Found,None Found,None Found,None Found,"Join SADA as a Sr. Data Engineer!

Your Mission

As a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.

You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Mastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Hihg
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
14,Data Engineer - GAMMA,"Chicago, IL 60654",Chicago,IL,60654,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer - GAMMA
9553BR
GAMMA Data Engineering
WHAT YOU'LL DO
BCG's Advanced Analytics Group, GAMMA Solutions & Services (S) delivers powerful analytics-based insights designed to help our clients tackle their most pressing business problems. We partner with BCG case teams and practices across the full analytics value-chain: framing new business challenges, building fact-bases, designing innovative analytics workflows, training colleagues in new methodologies, and interpreting findings for our clients. The GAMMA S team is a global resource, working with clients in every BCG region and in every industry area. It is a core member of a rapidly growing Analytics enterprise at BCG – a constellation of teams focused on driving practical results for BCG clients by applying leading edge analytics approaches.

 As Senior Analyst on our Data Engineering team you will have the chance to roll up your sleeves and apply cutting edge engineering and data pipelining techniques to real-world business situations across a variety of industries. Successful candidates are intellectually curious builders who are biased toward action, scrappy, and communicative.
YOU'RE GOOD AT
Are comfortable in a client-facing role when necessary
Are able to explain technical data pipelining topics to a non-technical audience
Love to build elegant data flows with cutting edge, modern tools
Enjoy finding solutions across disparate technologies
Have a software development or DevOps experience within an agile based environment
YOU BRING (EXPERIENCE & QUALIFICATIONS)
Bachelor’s Degree in a related field required. Master’s Degree in computer science, machine learning, or other data centric disciplines with 1-2 years of relevant industry work experience solutions strongly preferred.
5+ years of experience working in a software, DevOps or consulting environment preferred
Hands-on experience working with Spark, Hadoop, Docker and CI. Some clients like Airflow as well
Experience with AWS, Azure or similar cloud infrastructures
Fluency in SQL, Python a must. Other programming languages (Java, Scala) a plus
CITY
Chicago
COUNTRY
United States
YOU'LL BE TRAVELLING
Yes (40%)
YOUR EMPLOYEE TYPE IS
Regular
YOUR JOB TYPE IS
Full time
WHO WE ARE
BCG pioneered strategy consulting more than 50 years ago, and we continue to innovate and redefine the industry. We offer multiple career paths for the world’s best talent to have a real impact on business and society. As part of our team, you will benefit from the breadth and diversity of what we are doing today and where we are headed next. We count on your authenticity, exceptional work, and strong integrity. In return we are committed to supporting you in discovering the most fulfilling career journey possible—and unlocking your potential to advance the world.

BCG GAMMA combines innovative skills in computer science, artificial intelligence, statistics, and machine learning with deep industry expertise. The BCG GAMMA team is comprised of world-class data engineers, data scientists and business consultants who specialize in the use of advanced analytics to get breakthrough business results. Our teams own the full analytics value-chain end to end: framing new business challenges, building fact-bases, designing innovative algorithms, creating scale through designing tools and apps, and training colleagues and clients in new solutions. Here at BCG GAMMA, you’ll have the chance to work with clients in every BCG region and every industry area. We are also a core member of a rapidly growing analytics enterprise at BCG – a constellation of teams focused on driving practical results for BCG clients by applying leading edge analytics approaches, data, and technology.
EQUAL OPPORTUNITY
The Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under federal, state or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws. In addition, as a federal government contractor, BCG maintains an affirmative action program which furthers its commitment and complies with recordkeeping and reporting requirements under certain federal civil rights laws and regulations. BCG is an E-Verify Employer. Click here for more information on E-Verify. VEVRAA Federal Contractor."
15,BCG Omnia - ETL Data Engineer,"Chicago, IL 60654",Chicago,IL,60654,None Found,None Found,None Found,None Found,None Found,None Found,"BCG Omnia - ETL Data Engineer
9958BR
Technology & Engineering
WHAT YOU'LL DO
We are seeking a motivated and innovative candidate with a “start-up” mentality to join a fast-moving team of engineers that augments traditional consulting engagements with software. As a ETL Data Engineer joining our Chicago-based BCG Omnia team, you will be supporting the Retail Catalyst platform, a cloud-based analytics hub & suite of analytical apps for Retailers. In this role, you will come up with better ways to wrangle, structure, and Hadoop-query massive amounts of (client-) data, all while improving BCG’s ways of working and ability to scale our IP. The ideal candidate has 2-3 years of ETL experience (SQL is a must) with heavy focus on building scalable and repeatable data schemas. Ideally he or she also brings a deep understanding of Hadoop / Spark cluster configuration, cloud platforms (GCP and Azure), Knowledge of and hands-on experience with visualized ETL (e.g., Alteryx; Dataiku), Python / Flask, and Retail industry (data) is a big plus.
YOU'RE GOOD AT
ETL and wrangling huge data sets is in your blood
You are a master of SQL and Hadoop/Spark-based data querying
Building fast, scalable, and clean data schemas which can be re-used over and over
Being a self-learner and highly proactive / ambitious in your work
Constantly connecting the dots – technical and business context + anticipating issues before they arise
You enjoy working in a small startup-like team and don’t mind working across disciplines (e.g., digging into the frontend apps to troubleshoot data issues)
You thrive in a fairly complex multi-cloud, multi-tenant environment and building containerized web applications
Utilizing an 80/20 mind-set of value vs. time to deliver
Thoroughly testing schemas and code; proactively identifying bugs and issues
YOU BRING (EXPERIENCE & QUALIFICATIONS)
Degree in computer science or related field
2-3 years ETL / Data Engineering experience
Deep expertise in SQL (ideally PostgreSQL) and standardized data schemas / architecture
Experience with visualized ETL (e.g., Alteryx or Dataiku)
Experience in setting up and optimizing Hadoop / Spark cluster configuration
Experience working in collaborative, fast-moving environments
Experience in cloud technology (Google Cloud / Microsoft Azure; Docker; Kubernetes)
Strong knowledge of security and networking protocols
Self-driven, entrepreneurial, and strong team player
YOU'LL WORK WITH
Our technology consultants and specialists partner with our clients and colleagues to build and implement digital solutions through a broad spectrum of activities. Technology jobs and engineering jobs include design of IT architectures, large-scale transformation, agile development, software engineering, cybersecurity consulting, and risk management.
CITY
Chicago
COUNTRY
United States
YOU'LL BE TRAVELLING
Yes (10%)
YOUR EMPLOYEE TYPE IS
Regular
YOUR JOB TYPE IS
Full time
WHO WE ARE
BCG pioneered strategy consulting more than 50 years ago, and we continue to innovate and redefine the industry. We offer multiple career paths for the world’s best talent to have a real impact on business and society. As part of our team, you will benefit from the breadth and diversity of what we are doing today and where we are headed next. We count on your authenticity, exceptional work, and strong integrity. In return we are committed to supporting you in discovering the most fulfilling career journey possible—and unlocking your potential to advance the world. BCG Omnia works with Boston Consulting Group’s practice areas—specific industries and capabilities—in order to transform the firm’s unique intellectual property into professional, scalable products to support our client service efforts. BCG Omnia employs leading-edge technologies and specialized experts in product design and development, visualization, benchmarking, and custom analysis to provide world-class solutions.
EQUAL OPPORTUNITY
Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, protected veteran status, or any other characteristic protected under federal, state or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws."
16,Sr. Data Engineer,"Chicago, IL 60601",Chicago,IL,60601,None Found,None Found," Experience with cloud-based systems like AWS, AZURE or Google Cloud.",None Found,None Found,None Found,"We have a wide variety of career opportunities around the world — come find yours.

Information Technology
United Airlines is seeking talented people to join the Data Engineering team. Data Engineering organization is responsible for driving data driven insights & innovation to support the data needs for commercial and operational projects with a digital focus.
 You will partner with various teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives that create sustainable revenue and share growth
 Design, develop, and implement streaming and near-real time data pipelines that feed systems that are the operational backbone of our business.
 Execute unit tests and validating expected results to ensure accuracy & integrity of data and applications through analysis, coding, writing clear documentation and problem resolution.
 This role will also drive the adoption of data processing and analysis within the Hadoop environment and help cross train other members of the team.
 Leverage strategic and analytical skills to understand and solve customer and business centric questions.
 Coordinate and guide cross-functional projects that involve team members across all areas of the enterprise, vendors, external agencies and partners.
 Leverage data from a variety of sources to develop data marts and insights that provide a comprehensive understanding of the business.
 Develop and implement innovative solutions leading to automation.
 Use of Agile methodologies to manage projects.
 Mentor and train junior engineers.
Required
 BS/BA, in computer science or related STEM field
 We are seeking creative, driven, detail-oriented individuals who enjoy tackling tough problems with data and insights. Individuals who have a natural curiosity and desire to solve problems are encouraged to apply
 10+ years of IT experience in software development
 5+ years of development experience using Java, Python, Scala
 5+ years of experience with Big Data technologies like Spark, Hadoop, Hive, HBASE, Kafka, Nifi
 4+ years of experience with relational database systems like MS SQL Server, Oracle, Teradata
 Must be legally authorized to work in US for any employer without sponsorship - Successful completion of interview required to meet job qualification
 Reliable, punctual attendance is an essential function of the position
Preferred Skills:
 Master’s in computer science or related STEM field.
 Experience with cloud-based systems like AWS, AZURE or Google Cloud.
 Certified Developer / Architect on AWS.
 Strong experience with continuous integration & delivery using Agile methodologies - Data engineering experience with transportation/airline industry
 Strong problem-solving skills.
 Strong knowledge in Big Data


Equal Opportunity Employer – Minorities/Women/Veterans/Disabled/LGBT"
17,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Please make sure to read the job posting in its entirety as it reflects both the University roles and responsibilities, followed by the specific description.
Department
13760 Urban Crime Labs
About the Unit
By 2050, the global urban population will nearly double to 6.4 billion. This unprecedented growth in the global urban population creates incredible opportunities but also intensifies the most difficult problems cities face, such as concentrated poverty, crime, poor-quality schooling, and pollution. The University of Chicago founded Urban Labs to help address these challenges. Urban Labs is a set of highly synergistic labs focused on undertaking inquiry and having impact on five essential dimensions of urban life: crime, education, health, poverty, and energy & environment. Urban Labs partners with cities to identify and rigorously evaluate the policies and programs with the greatest potential to improve human lives at scale. Urban Labs’ evidence-based approach gives policymakers and practitioners the knowledge they need to effectively achieve the greatest social good per dollar spent. In sum, UChicago Urban Labs: • Identifies promising solutions to urban challenges• Tests the most promising urban policies and programs • Scales-up the most effective and cost-efficient policies and programs. For more information about the UChicago Urban Labs, go to http://urbanlabs.uchicago.edu/
Job Family
Research
Responsible for all aspects of research projects and research facilities. Plans and conducts clinical and non-clinical research; facilitates and monitors daily activities of clinical trials or research projects. Directs engineering and technical support activities to develop and maintain tools and computational methods needed to gather and analyze data.
Career Track and Job Level
Data Science
Conducts data investigation, including data wrangling, cleaning, sampling, management, exploratory analysis, regression and classification, prediction, and data communication. Implements foundational concepts of data computation, such as data structure, algorithms, parallel computing, simulation, and analysis. Utilizes knowledge in game theory, statistical quality control, exponential smoothing, seasonally adjusted trend analysis, or data visualization to gain insights, develop new strategies, and cultivate actionable business intelligence in diverse career tracks across the University.
P3: Requires in-depth knowledge and experience. Uses best practices and knowledge of internal or external University issues to improve products or services. Solves complex problems; takes a new perspective using existing solutions. Works independently, receives minimal guidance. Acts as a resource for colleagues with less experience.
Role Impact
Individual Contributor
Responsibilities
The job uses best practices and knowledge of data manipulation, statistical applications, programming, analysis and modeling in order to implement projects related to the University's various internal data systems as well as from external sources. The job is responsible for managing operational protocols.
1) Has a deep understanding of methods to analyze complex data sets for the purpose of extracting and purposefully using applicable information. May develop and maintain infrastructure that connects data sets., 2) Guides staff or faculty members in defining the project and applies principals of data science in manipulation, statistical applications, programming, analysis and modeling., 3) Calibrates data between large and complex research and administrative datasets. Guides and may set the operational protocols for collecting and analyzing information from the University's various internal data systems as well as from external sources., 4) Designs and evaluates statistical models and reproducible data processing pipelines using expertise of best practices in machine learning and statistical inference. Provides expertise for high level or complex data-related requests and engages other IT resources as needed. Partners with other campus teams to assist faculty with data science related needs., 5) Performs other related work as needed.
Unit-specific Responsibilities
1) Leads the design and implementation of standardized ETL processes for our most widely used datasets.
2) Builds tools and APIs that enable easy loading of clean data. These tools and APIs must work for users of R, Python, and Stata.
3) Creates design requirements by talking to our analysts and designing to fit their needs.
4) Mentors analysts and runs internal workshops on best practices of software engineering.
5) Creates user-friendly documentation that encourages tool adoption and promotes the maintainability of tools and infrastructure.
Unit-preferred Competencies
1) Proficiency in either Python or R, with a working knowledge of the other.
2) Proficiency in SQL.
3) Ability to create robust, high-quality, and tested data pipeline.
4) Strong interpersonal skills.
5) Strong initiative and a resourceful approach to problem solving and learning.
6) Ability to mentor junior staff.
Education, Experience, and Certifications
Minimum requirements include a college or university degree in related field.
Minimum requirements include knowledge and skills developed through 5-7 years of work experience in a related job discipline.
Preferred Qualifications
Education
1) Bachelor’s or master’s degree in computer science, statistics, data science, economics or a closely related field.
Experience
1) Five years of related experience
Required Documents
1) Resume
2) Cover letter
3) A sample of data work, such as a final report for a class project, link to a GitHub repo, or blog post
NOTE: When applying, all required documents MUST be uploaded under the Resume/CV section of the application.
FLSA Status
Exempt
Pay Frequency
Monthly
Pay Grade
Depends on Qualifications
Scheduled Weekly Hours
37.5
Benefits Eligible
Yes
Drug Test Required
No
Health Screen Required
No
Motor Vehicle Record Inquiry Required
No
Posting Date
2019-07-24-07:00
Remove from Posting On or Before
2020-01-24-08:00"
18,Sr Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"PREFERRED SKILLS:
Bachelor’s degree in quantitative field, MS preferred
Several years of experience with data warehouse, analytics, ETL – exceptional command of sql in different flavors
Very strong knowledge of data modeling in relational DBs
Few years of experience with big data technologies - Hadoop, spark
Strong programming skills in a high-level, general purpose programming language – java, scala, or python
Strong communication skills, ability to work effectively with remote members of the team and collaborate over long-distance
Experience in a cloud infrastructure is a plus – AWS, GCP, Azure
Familiarity with functional programming is a plus
Familiarity with CI/CD concepts is a plus – git, Jenkins, etc.

Gogo is the inflight internet company. Our worldwide inflight Wi-Fi services have made internet and video entertainment a regular part of flying. We are a diverse and mission-minded group of professionals all working together in extraordinary harmony. And that’s just the beginning. We connect the aviation industry and air travelers with innovative technology and applications, and we do it all in a high-energy environment that welcomes the next challenge. Be prepared to join a performance-obsessed team that is passionate about bringing the internet to every device, every flight, everywhere.
Equal Opportunity Employer/Vets/Disabled
Gogo participates in E-Verify. Details in English and Spanish. Right to Work Statement in English and Spanish."
19,Application Software Engineer,"Chicago, IL 60290",Chicago,IL,60290,None Found,None Found,None Found,None Found,None Found,None Found,"Where good people build rewarding careers.
Think that working in the insurance field can’t be exciting, rewarding and challenging? Think again. You’ll help us reinvent protection and retirement to improve customers’ lives. We’ll help you make an impact with our training and mentoring offerings. Here, you’ll have the opportunity to expand and apply your skills in ways you never thought possible. And you’ll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.
Job Description
Allstate is seeking to hire a Application Software Engineer to join our Innovation team at our downtown Chicago location. Allstate’s Innovation Team is at the forefront of change and growth within the company, driving the creation of standalone businesses or reinvention of existing ones. Innovation projects typically include cross-functional team members within Business Strategy, Product, Technology, Design, and Go to Market functions, who use data-driven prototypes and testing to achieve desired business outcomes. The team is seeking to augment its analytics capabilities further by growing a small team of predictive analytics researchers and data engineers with a senior data engineer position. Past projects in the team have included the development of a new consumer “digital footprint” technology, announced at CES 2019, and new approaches to risk prediction for traditional insurance products.
The Application Software Engineer will develop novel techniques for on-boarding, transforming, and persisting structured and semi-structured data to create solutions for advanced risk prediction, reporting, and other types of analytics. This individual will also assist in efforts to evaluate, extract, scrub/prep, analyze and visualize a host of internal and external datasets and communicate the results to various stakeholders across and outside the company.
Responsibilities include:
Using Sqoop, SQL, Hadoop stack, Spark, and other Big Data technologies to create an end-to-end ETL and Data Mining pipelines gathering inputs from disparate sources within the enterprise and transforming the data to a format suitable for Data Science work
Working closely with our Data Science team as well as the business team to understand complex requirements around data acquisition, transformation, streaming, and other data pipeline requirements
Wearing multiple hats including Data Engineering, Web Development, DevOps, Reporting and other disciplines as necessary in a challenging highly complex environment
Representing Allstate in customer meetings, external functions, projects and consortia, as necessary
Having readiness to challenge the current process and proposing new solutions based on extensive previous experience
Job Qualifications
Bachelor’s degree in computer science or a similar technical field with 5 or more years of experience in highly data driven projects. OR
A Master’s or Ph.D. in computer science or a similar technical field with 3 or more years of experience in highly data driven projects.
Strong proficiency in Python, Scala, Java or equivalent.
Strong proficiency in SQL.
Strong proficiency in Linux.
Significant experience working in a distributed environment including Hadoop (HDFS, Hive, etc.) and Spark (ETL) ecosystems
Significant experience in a technically complex product development environment (e.g. database or software applications).
Background in information systems, big data and analytics experience with familiarity of contemporary tools and agile methodologies.
Ability to execute in a cross-functional team environment spread across many departments.
Ability to operate in a dynamic, fast moving environment, and communicate complex ideas in a clear, concise manner both verbally and in writing.
Results oriented as demonstrated by proven ability to meet short deadlines and execute against multiple competing priorities with minimum direct supervision.
Preferred Qualifications
Strong proficiency in TDD, CI/CD, Git and one of Maven, SBT, Gradle.
Deep knowledge of a web framework like Spring Boot or equivalent (Play, Rails, Django).
Experience with Dimensional Modeling and Star Schemas.
Experience with reporting tools like Tableau or Business Objects.
Experience with Cloudera or other Hadoop distributions.
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.


Good Work. Good Life. Good Hands®.

As a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.

Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.

Allstate generally does not sponsor individuals for employment-based visas for this position.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.

For jobs in San Francisco, please click ""here"" for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click ""here"" for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.

It is the policy of Allstate to employ the best qualified individuals available for all jobs without regard to race, color, religion, sex, age, national origin, sexual orientation, gender identity/gender expression, disability, and citizenship status as a veteran with a disability or veteran of the Vietnam Era."
20,Data Engineer,"Chicago, IL 60654",Chicago,IL,60654,None Found,None Found,None Found,None Found,"1-3 years of experience in quantitative analysis experience.
Bachelor's degree in Computer Science, Statistics, Math or other technical field required. Graduate degrees preferred.
",None Found,"The Data Engineer is responsible for applying your expertise in quantitative analysis, database and data warehousing, partnered with operation and product teams, to solve problems and identify trends and opportunities. The Data Engineer role has to work across the following areas:
Database maintenance
Building and analyzing dashboards and reports
Evaluating and defining metrics and perform exploratory analysis
Monitoring key product metrics and understanding root causes of changes in metrics
Empower and assist operation and product teams through building key data sets and data-based recommendations
Automating analyses and authoring pipelines via SQL/python based ETL framework
Key Competencies
Superb SQL programming skill.
Understanding of ETL tools and database architecture.
Advanced knowledge of data warehousing.
Demonstrable familiarity with code and programming concepts. Experience with Python is preferred but not required.
A product mindset - you ask and address the most important analytical questions with a view on enhancing product impact.
Passionate and attentive self-starters, great communicators.
Education and Experience
1-3 years of experience in quantitative analysis experience.
Bachelor's degree in Computer Science, Statistics, Math or other technical field required. Graduate degrees preferred.
Job Classification
This is a full time exempt position
SMS Assist is an Equal Opportunity Employer (EOE) that welcomes and encourages all applicants to apply regardless of age, race, color, religion, sex, sexual orientation, gender identify and/or expression, national origin, disability, veteran status, marital or parental status, ancestry, citizenship status, pregnancy or other reasons prohibited by law.
#ZP
#Indeed"
21,Cloud Solutions Architect,"Chicago, IL",Chicago,IL,None Found,None Found,"
Expertise in at least one of the following domain areas:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes the full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio.
Application Development: building custom web and mobile applications on top of the GCP stack.
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Excellent written and verbal communication skills with the ability to interface with and communicate complex technical concepts to a broad range of stakeholders.
Hands-on experience with cloud computing, traditional on-premises and enterprise data-center technologies.
Experience working with engineering and sales teams.
Experience producing technical assets or writing technical documentation, including, but not limited to, architecture designs and documentation, statements of work, project plans, and working code samples.
Time management with the ability to manage multiple streams.
",None Found,None Found,None Found,None Found,"Join SADA as a Cloud Solutions Architect!

Your Mission

As a Cloud Solutions Architect at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You’re also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.

You will be an established contributor within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures successfully. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.

Pathway to Success

#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.

You will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.

As you continue to execute successfully, we will build a customized development plan together that leads you through the solutions architecture or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events.
Customer Facing - This is very customer-facing role. You will usually interact with customers on a daily basis. You will participate on calls and onsite customer meetings to qualify consultative engagements with the engineering teams. You will present architecture proposals and code samples to build trust, confidence, and consensus both externally and internally.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Cloud Architect Certified

[https://cloud.google.com/certification/cloud-architect] and/or Google
Professional Data Engineer Certified
[https://cloud.google.com/certification/data-engineer], or able to complete one of the above within the first 45 days of employment.

Required Qualifications:

Expertise in at least one of the following domain areas:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes the full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio.
Application Development: building custom web and mobile applications on top of the GCP stack.
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Excellent written and verbal communication skills with the ability to interface with and communicate complex technical concepts to a broad range of stakeholders.
Hands-on experience with cloud computing, traditional on-premises and enterprise data-center technologies.
Experience working with engineering and sales teams.
Experience producing technical assets or writing technical documentation, including, but not limited to, architecture designs and documentation, statements of work, project plans, and working code samples.
Time management with the ability to manage multiple streams.

Useful Qualifications:

Direct experience working with a variety of cloud technologies as well as designing and recommending elegant solutions that drive business outcomes.
Understanding of infrastructure automation, continuous integration/deployment, relational/NoSQL data stores, security, networking, and cloud-based delivery models.
Ability to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance and security.
Thought leadership with the ability to recommend cloud-native approaches to solve customer business and technical challenges.
Understanding of best practices, design patterns and reference architectures with an uncanny ability to recommend these as needed.

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
22,Data Engineer,"Rosemont, IL",Rosemont,IL,None Found,None Found,"Bachelor’s degree in Computer Science, Information Systems, Business Administration, or other related field required
Minimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources
Minimum 2 years of experience with PowerBI
Minimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications
Minimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired
Experience working with both structured and unstructured data
Demonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types (star, snowflake, etc.)
Data modeling experience in building logical and physical data models
Applied knowledge of Microsoft Security/Authentication Concepts (Active Directory, IIS, Windows OS)
Strong technical planning skills with the ability to prioritize and multitask across a number of work streams
Must have a passion for continued improvement, learning, and mentoring
Polished presentation skills; experience creating and presenting findings to executive level staff
Strong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively
Must be highly collaborative with the ability to manage and motivate project teams and meet deliverables
Ability to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders
Experience with Data Warehouse is a plus
Knowledge of SSRS is a plus
Healthcare industry experience a plus",None Found,None Found,None Found,None Found,"You are known for your development and deployment of innovative big data platforms for advanced analytics and data processing. You lead innovation through exploration, benchmarking, making recommendations, and implementing big data technologies for platforms. You are excited by defining and building data pipelines that will enable faster, better, data-informed decision-making within the business. You have a passion for continued improvement, learning and mentoring, and leverage this enthusiasm when building stakeholder relationships. You are collaborative, insightful and want to work for an organization making a difference in the field of orthopaedics on behalf our members and their patients.
If this sounds like you, read on!
The Data Engineer is primarily responsible for maintaining and enhancing the Registry’s data acquisition, integration, and ETL pipelines in support of both operational and business intelligence data stores. The incumbent is responsible for applying diverse data cleansing and transformation techniques as well as the ongoing management and monitoring of all Registry databases. This includes addressing issues pertaining to the ongoing operations and optimization of the data environment including performance, reliability, logging, scalability, etc. This position will also provide support for the Academy’s database systems, warehouse, marts, and supporting applications.
Lead the effort to develop a unified enterprise data model for the Academy. Design, develop, and maintain high-performance data platforms on premise and in Microsoft Azure cloud-based environments including leading the development of a data warehouse environment to support the Registry’s business intelligence roadmap. Champion efforts that will ensure that the Academy’s business intelligence applications remain relevant for use by internal business groups by actively participating in strategy and project planning discussions. Work collaboratively with Registry participants and internal support teams, identify and implement changes that improve system performance and the user experience.
Design, develop, and maintain the ETL pipelines using SSIS that standardize raw data from multiple data sources and optimize both the operational and dimensional/star schema data model necessary for transactional systems and business intelligence applications. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions aligning to an overall data architecture. Extract, transform, and load data to and from various data sources including relational databases, NoSQL databases, web services, and flat files. Produce various technical documents such as ER diagrams, table schemas, data lineage, API documents, etc.
Provide leadership and oversight on database architectural design for existing Academy systems including database administration, performance monitoring, and troubleshooting. Provide complex analysis, conceptualize, design, implement, and develop solutions for critical data-centric projects. Perform dataflow, system and data analysis, and develop meaningful and useful presentation of data in downstream applications. Plan and implement standards, define/code conformed global and reusable objects, perform complex database design and data repository modelling.
Monitor ETL processes, system audits, dashboard reporting, and presentation layer functioning and performance. Proactively identify and implement procedures that resolve performance and/or data reporting issues. Support the optimal performance of the Academy’s data and BI systems. Monitor database performance, provide optimization recommendations, and implement recommendations. Follow the release cycles and implement on-time delivery of task assignments, defect correction, change requests, and enhancements. Troubleshoot and solve technical problems. Perform other responsibilities as assignment by management.
Required Qualifications:
Bachelor’s degree in Computer Science, Information Systems, Business Administration, or other related field required
Minimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources
Minimum 2 years of experience with PowerBI
Minimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications
Minimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired
Experience working with both structured and unstructured data
Demonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types (star, snowflake, etc.)
Data modeling experience in building logical and physical data models
Applied knowledge of Microsoft Security/Authentication Concepts (Active Directory, IIS, Windows OS)
Strong technical planning skills with the ability to prioritize and multitask across a number of work streams
Must have a passion for continued improvement, learning, and mentoring
Polished presentation skills; experience creating and presenting findings to executive level staff
Strong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively
Must be highly collaborative with the ability to manage and motivate project teams and meet deliverables
Ability to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders
Experience with Data Warehouse is a plus
Knowledge of SSRS is a plus
Healthcare industry experience a plus
If this describes YOU, please apply by sharing the following:
Clearly communicate why you are the ideal candidate for this role, providing specific examples and experiences as proof points.
Attach your resume, cover letter and any additional materials that support your application.
Salary expectations must be included to be considered for this opportunity."
23,Data Engineer,"Chicago, IL 60601",Chicago,IL,60601,None Found,None Found,None Found,None Found,None Found,None Found,"About Avanade

Avanade leads in providing innovative digital services, business solutions and design-led experiences for its clients, delivered through the power of people and the Microsoft ecosystem. Our professionals combine technology, business and industry expertise to build and deploy solutions to realize results for clients and their customers. Avanade has 37,000 digitally connected people across 24 countries, bringing clients the best thinking through a collaborative culture that honors diversity and reflects the communities in which we operate. Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com

Why Avanade?

14-time winner of Microsoft Partner of the Year
24,000+ certifications in Microsoft technology
90+ Microsoft partner awards
17 Gold Competencies
3,500 analytics professionals worldwide
1,000 data engineers
Implemented analytics systems for more than 550 clients
400 AI practitioners
300 cognitive service experts

How We Support You:
We believe in gender equity and an inclusive community. We offer a comprehensive benefits package: generous vacation allowance disability coverage, retirement plans, paid maternity and paternity leave, life insurance, hotel and travel discounts, extended benefits to cover items that support your well-being, health, dental and vision insurance, professional development and paid Microsoft certification opportunities.

Role Overview:
As an Data Engineer you will collect, aggregate, store, and reconcile data in support of Client business decisions. You will help design and build data pipelines, data streams, reporting tools, information dashboards, data service APIs, data generators and other end-user information portals and insight tools. You will be a critical part of the data supply chain, ensuring that business partners can access and manipulate data for routine and ad hoc analysis to drive business outcomes using Advanced Analytics.

Key Role Responsibilities:
Day-to-day, you will:
Translate business requirements to technical solutions using strong business insight.
Analyzes current business practices, processes, and procedures as well as identifying future business opportunities for demonstrating Microsoft Azure Data & Analytics PaaS Services.
Support the planning and implementation of data design services, providing sizing and configuration assistance and performing needs assessments.
Delivery of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies.
Design and Build Modern Data Pipelines and Data Streams.
Design and Build Data Service APIs.
Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics.
Expose data to end-users using Power BI, Azure API Apps or other modern visualization platform or experience.
Implement effective metrics and monitoring processes.

Key Role Skill & Capability Requirements:
Your technical/non-technical skills include:
Demonstrable experience of turning business use cases and requirements to technical solutions.
Experience in business processing mapping of data and analytics solutions.
Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.
The ability to apply such methods to take on business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration.
T-SQL is required.
Knowledge of Azure Data Factory, Azure Data Lake, Azure SQL DW, and Azure SQL, Azure App Service, Databricks, and Azure Data Warehouse.
Experience preparing data for Data Science and Machine Learning.
Knowledge of Lambda and Kappa architecture patterns.
Knowledge of Master Data Management (MDM) and Data Quality tools and processes.
Strong collaboration ethic and experience working with remote teams.
Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals.
Working experience with Visual Studio, PowerShell Scripting, and ARM templates.
Experience with Git/TFS/VSTS is a requirement.

Preferred Education Background:
You likely possess a Bachelor's degree in Computer Science, Information Technology, Business, or another relevant field. An equivalent combination of education and experience will also suffice.

Preferred Years of Work Experience:
You likely have about 5+ years of relevant professional experience."
24,Data Architect,"Chicago, IL 60606",Chicago,IL,60606,None Found,None Found,None Found,"Facilitate collaboration for development of long-term strategic plans of production databases in conjunction with data owners, business units, IT product owners, and Enterprise Architecture
Assist business units and project management in developing the budget projections based on short- and long-term goals and objectives
Develop and maintain data models and data flows that represent essential data consumed and produced
Develop and maintain documentation that maps data models to information systems and business applications
Be resourceful in the use of enterprise data, including guidance in understanding and exploitation of the underlying data and business models, and identifying optimal data sourcing and mapping
Analyze the logical and physical database models to ensure models are in accordance with standards
Assist in building of enterprise architecture information blueprints that illustrate how information is stored, processed, and accessed.
Participate in enterprise information architecture discussions around needs and alignment to business goals
Collaborate with Database Administrators and other staff to resolve data issues, performance issues and to ensure the highest possible degree of data integrity
Validate and fix information management issues related to data quality framework (completeness, accuracy, availability, timeliness, consistency, etc.)
Enforce enterprise information standards, guidelines, and principles
",None Found,"5+ years of experience as a Data Architect/Data Engineer
10 years of strong working knowledge of Microsoft SQL server, SSIS and SSAS (multidimensional model)
In-depth understanding of database structure principles
Ability to write complex SQL queries for ETL or reporting
Must be project oriented with an ability to meet deadlines.
Excellent written and oral communications
Familiarity with data visualization tools (e.g. Tableau, XLCubed, ESRI, SSRS etc.)
Familiarity with Python for data analysis is a plus
Experience with Yardi or any other Real Estate Property/Accounting/Asset Management system is a plus
Bachelor's degree in Computer Science, preferred
","Job Description:
The Data Architect leads and supports all components of a database design environment, including the most complex database designs for both transactional and data warehouse systems (test through production). This role will also facilitate and collaborate with IT and business units to develop, maintain, and support system data models. The Data Architect will define and deliver database design solutions based on requirements from both business and technology disciplines.
Responsibilities:
Facilitate collaboration for development of long-term strategic plans of production databases in conjunction with data owners, business units, IT product owners, and Enterprise Architecture
Assist business units and project management in developing the budget projections based on short- and long-term goals and objectives
Develop and maintain data models and data flows that represent essential data consumed and produced
Develop and maintain documentation that maps data models to information systems and business applications
Be resourceful in the use of enterprise data, including guidance in understanding and exploitation of the underlying data and business models, and identifying optimal data sourcing and mapping
Analyze the logical and physical database models to ensure models are in accordance with standards
Assist in building of enterprise architecture information blueprints that illustrate how information is stored, processed, and accessed.
Participate in enterprise information architecture discussions around needs and alignment to business goals
Collaborate with Database Administrators and other staff to resolve data issues, performance issues and to ensure the highest possible degree of data integrity
Validate and fix information management issues related to data quality framework (completeness, accuracy, availability, timeliness, consistency, etc.)
Enforce enterprise information standards, guidelines, and principles
Requirements:
5+ years of experience as a Data Architect/Data Engineer
10 years of strong working knowledge of Microsoft SQL server, SSIS and SSAS (multidimensional model)
In-depth understanding of database structure principles
Ability to write complex SQL queries for ETL or reporting
Must be project oriented with an ability to meet deadlines.
Excellent written and oral communications
Familiarity with data visualization tools (e.g. Tableau, XLCubed, ESRI, SSRS etc.)
Familiarity with Python for data analysis is a plus
Experience with Yardi or any other Real Estate Property/Accounting/Asset Management system is a plus
Bachelor's degree in Computer Science, preferred
Ventas, Inc. offers a competitive compensation and benefits package to the successful candidate.
Ventas, Inc. is an Equal Opportunity Employer.
Ventas, Inc. does not accept unsolicited resumes from staffing agencies, search firms or any third parties."
25,OSP Fiber Design Engineer,"Chicago, IL 60601",Chicago,IL,60601,None Found,None Found,None Found,None Found,None Found,None Found,"The Outside Plant Fiber Design Engineer is responsible for Fiber Optic detailed network design for fiber-based networks. This position requires an in depth understanding of current fiber optic design standards and industry processes. Typical OSP design responsibilities range from the initial determination of the pathway routing and construction alternatives based on field/site walk outs to final detailed design generation.
The OSP fiber design engineer is fully responsible and accountable for the final design content and document generation to support the placement of the physical fiber infrastructure supporting Outside and Inside Plant communication facilities.
Responsibilities include:
Create high level preliminary route design and facility specifics to manage fielding request instructions.
Self-managed with the ability to multi-task and meet deadlines through strong organization and communication skills.
Have a complete understanding of outside plant design principles, work order procedures, and telephone industry standards for aerial and underground designs.
Knowledge of methods and interpretation of boring, trenching, aerial work and ROW with fiber optics.
Be knowledgeable in local, state and railroad permitting regulations regarding ROW, Property Rights and Easements.
Create and perform quality checks of redlines and designed prints with specifications, standards, practices and guidelines.
Create drawings and records of fiber routes, splice locations, construction notes, bills of materials and project overviews.
Project Execution Responsibilities Include:
Driving to various locations across the Chicago Metro Area to collect field data.
Engineer preliminary and final fiber layout designs for aerial, buried and underground construction.
Finding creative solutions to the challenges of route selection, network design, and right of way issues.
Understand project milestones. Monitor status of milestones and provide regular updates.
Support QC and as built efforts with design changes/updates.
Preferred Job Qualifications:
Â· Bachelor's degree in engineering or construction or equivalent in work experience, 7+ years telecommunications experience involving OSP telecom design.
Â· Related work experience in OSP and/or ISP fiber detailed design & fiber splicing.
Â· Possess a deep understanding of inside and/or outside plant fiber optic network infrastructure alternatives available.
Â· Familiar with construction focused on fiber builds.
Â· Strong interpersonal, analytical and communication skills.
Â· Flexible and able to think quickly working to resolve design issues and makes decisions.
Â· Experience in working on multiple projects simultaneously.
Â· Functions as a technical specialist with minimal supervision.
Preferred Technical Competencies:
Â· Fiber Network Planning.
Â· Fiber routing for aerial and underground installations.
Â· Proficient in Microsoft Office â€“ 5+ years.
Â· Manhole design including splicing and racking.
Â· Design of fiber terminals, switches, routers and security.
Â· Customer equipment room design.
Â· Knowledge of construction & constructability practices & principles.
Â· Knowledge of Aramis, AutoCad, and Micro Station is a plus.
About KDM Engineering:
KDM Engineering is a WBE/MBE engineering consulting firm specializing in primary distribution design in the Chicagoland area. Our core staff has a wealth of design and project management experience. Our aim is to provide our clients with excellent project support, a high attention to detail, and great customer service.
Benefits & Top Reasons to Work for Us:
Competitive base salary
Comprehensive medical, dental and vision plans
401(k) plans with substantial company match
PTO, company paid holidays and great work/life balance
Fun and Flexible work environment that puts people first
Dynamic leadership team that are experts in their field
Equal Employment Opportunity:
KDM Engineering strongly supports equal employment opportunity for all applicants regardless of race, color, religion, sex, gender identity, pregnancy, national origin, ancestry, citizenship, age, marital status, physical disability, mental disability, medical condition, sexual orientation, genetic information, or any other characteristic protected by state or federal law."
26,Data Architect,"Chicago, IL 60654",Chicago,IL,60654,None Found,None Found,None Found,"Facilitate collaboration for development of long-term strategic plans of production databases in conjunction with data owners, business units, IT product owners, and Enterprise Architecture
Assist business units and project management in developing the budget projections based on short- and long-term goals and objectives
Develop and maintain data models and data flows that represent essential data consumed and produced
Develop and maintain documentation that maps data models to information systems and business applications
Be resourceful in the use of enterprise data, including guidance in understanding and exploitation of the underlying data and business models, and identifying optimal data sourcing and mapping
Analyze the logical and physical database models to ensure models are in accordance with standards
Assist in building of enterprise architecture information blueprints that illustrate how information is stored, processed, and accessed.
Participate in enterprise information architecture discussions around needs and alignment to business goals
Collaborate with Database Administrators and other staff to resolve data issues, performance issues and to ensure the highest possible degree of data integrity
Validate and fix information management issues related to data quality framework (completeness, accuracy, availability, timeliness, consistency, etc.)
Enforce enterprise information standards, guidelines, and principles
",None Found,"5+ years of experience as a Data Architect/Data Engineer
10 years of strong working knowledge of Microsoft SQL server, SSIS and SSAS (multidimensional model)
In-depth understanding of database structure principles
Ability to write complex SQL queries for ETL or reporting
Must be project oriented with an ability to meet deadlines.
Excellent written and oral communications
Familiarity with data visualization tools (e.g. Tableau, XLCubed, ESRI, SSRS etc.)
Familiarity with Python for data analysis is a plus
Experience with Yardi or any other Real Estate Property/Accounting/Asset Management system is a plus
Bachelor's degree in Computer Science, preferred
","Job Description:
The Data Architect leads and supports all components of a database design environment, including the most complex database designs for both transactional and data warehouse systems (test through production). This role will also facilitate and collaborate with IT and business units to develop, maintain, and support system data models. The Data Architect will define and deliver database design solutions based on requirements from both business and technology disciplines.
Responsibilities:
Facilitate collaboration for development of long-term strategic plans of production databases in conjunction with data owners, business units, IT product owners, and Enterprise Architecture
Assist business units and project management in developing the budget projections based on short- and long-term goals and objectives
Develop and maintain data models and data flows that represent essential data consumed and produced
Develop and maintain documentation that maps data models to information systems and business applications
Be resourceful in the use of enterprise data, including guidance in understanding and exploitation of the underlying data and business models, and identifying optimal data sourcing and mapping
Analyze the logical and physical database models to ensure models are in accordance with standards
Assist in building of enterprise architecture information blueprints that illustrate how information is stored, processed, and accessed.
Participate in enterprise information architecture discussions around needs and alignment to business goals
Collaborate with Database Administrators and other staff to resolve data issues, performance issues and to ensure the highest possible degree of data integrity
Validate and fix information management issues related to data quality framework (completeness, accuracy, availability, timeliness, consistency, etc.)
Enforce enterprise information standards, guidelines, and principles
Requirements:
5+ years of experience as a Data Architect/Data Engineer
10 years of strong working knowledge of Microsoft SQL server, SSIS and SSAS (multidimensional model)
In-depth understanding of database structure principles
Ability to write complex SQL queries for ETL or reporting
Must be project oriented with an ability to meet deadlines.
Excellent written and oral communications
Familiarity with data visualization tools (e.g. Tableau, XLCubed, ESRI, SSRS etc.)
Familiarity with Python for data analysis is a plus
Experience with Yardi or any other Real Estate Property/Accounting/Asset Management system is a plus
Bachelor's degree in Computer Science, preferred
Lillibridge offers a competitive compensation and benefits package to the successful candidate.
Lillibridge is an Equal Opportunity Employer.
Lillibridge does not accept unsolicited resumes from staffing agencies, search firms or any third parties."
27,Sr. Consultant - Data Engineer,"Rolling Meadows, IL",Rolling Meadows,IL,None Found,None Found,None Found,"Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies
Minimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation
Minimum five years experience using one or more data integration tools including Data Quality and ETL
Development knowledge for integrating components and contributing to core code base - Java preferred
Solid understanding of database and data warehousing technologies
Experienced in advanced SQL as well as NoSQL queries, syntax, and technologies
Experienced in big data requirements, applications, and technologies such as Hadoop
Proficient in ETL methods and approaches including triggers, named views, temporary tables, etc.
Experienced in Linux environments
","Identify data warehouse needs and develop strategy for implementing a warehousing solution including investigating data sources, rationalizing information sources, identifying technology components, building roadmaps and reference architecture stacks
Work with Business Analysts and other information management professionals through all phases of project development, from envisioning to architecture definition and end solution realization
Provide direction and collaborate with Data Engineers to implement enterprise solutions that will support organizational business intelligence and analytics requirements
Work with the business to identify opportunities where technology can be leveraged to solve existing problems and can assist with new market opportunities
Meet with technology vendors, convey technical requirements and business use cases, develop scorecards, install vendor products in a lab environment, and summarize findings and results of testing
Technologies to be used may include some combination of relational databases (PostgreSQL, Teradata, Aster, HANA), NoSQL, Hadoop, Object-based stores, and OLAP.
Specific responsibilities include:
Help design an architecture for federated data stores and data fusion
Help design methods for storing data in a way that facilitates extremely fast data parsing and management
Implement ""glue code"" that connects middle tier components with backend components
Implement data management and analytics code utilizing data architecture (e.g. map reduce)
Collaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores
Collaborate with enterprise architects to understand the applications we integrate with and the data they produce
Review requirements for new approaches to big data storage and analytics
Design methods for caching, paging, and integrating real-time data with historical data stores
Desired Skills and Experience
Requirements
Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies
Minimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation
Minimum five years experience using one or more data integration tools including Data Quality and ETL
Development knowledge for integrating components and contributing to core code base - Java preferred
Solid understanding of database and data warehousing technologies
Experienced in advanced SQL as well as NoSQL queries, syntax, and technologies
Experienced in big data requirements, applications, and technologies such as Hadoop
Proficient in ETL methods and approaches including triggers, named views, temporary tables, etc.
Experienced in Linux environments
Bonus Points
Experience working with IT strategy teams, business teams and business analysts to define information systems, services and management
Experience with RDBMS including SQL Server, Oracle 11g, MySQL; Big Data including SQL Data Warehouse Appliance, Oracle Exadata, Netezza, Greenplum, Vertica, Teradata, Aster Data, SAP HANA, Hadoop a plus; Analytics including SAS, SPSS, Spotfire, Tableau, Qlikview, R, Oracle Endeca; BI Tools including Oracle OBIEE, SAP Business Objects, SAS and other Analytics Vendors with BI components; ETL & MDM including Informatica, SAS Dataflux, IBM, Siperion, Rochade, Map/Reduce for ETL is a plus
Java is strongly preferred (e.g. for working with map reduce) but not ultimately a requirement if you excel in other areas
Strong SQL skills are highly desirable
OLAP experience",None Found,"Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies
Minimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation
Minimum five years experience using one or more data integration tools including Data Quality and ETL
Development knowledge for integrating components and contributing to core code base - Java preferred
Solid understanding of database and data warehousing technologies
Experienced in advanced SQL as well as NoSQL queries, syntax, and technologies
Experienced in big data requirements, applications, and technologies such as Hadoop
Proficient in ETL methods and approaches including triggers, named views, temporary tables, etc.
Experienced in Linux environments
","Insygnum needs a Consultant - Data Engineer to help our clients for data analysis, data integration and data quality. Our Chicago-based team is small but growing fast and we need to complement our in-house experts who knows how to tame challenging data. This is a unique opportunity to not only work with cool technology, but also to create a new methodologies and techniques. You'll get in on the ground floor of a new company, help shape its future, and benefit directly from your work.
Why work here
Joining insygnum now offers several unique opportunities
You will receive competitive salary, benefits, and stock options
You will be working on hard, interesting problems
You will help shape the culture of the company as we grow
You will have the opportunity to apply your skills in a meaningful way and have a real-world impact
Responsibilities
Identify data warehouse needs and develop strategy for implementing a warehousing solution including investigating data sources, rationalizing information sources, identifying technology components, building roadmaps and reference architecture stacks
Work with Business Analysts and other information management professionals through all phases of project development, from envisioning to architecture definition and end solution realization
Provide direction and collaborate with Data Engineers to implement enterprise solutions that will support organizational business intelligence and analytics requirements
Work with the business to identify opportunities where technology can be leveraged to solve existing problems and can assist with new market opportunities
Meet with technology vendors, convey technical requirements and business use cases, develop scorecards, install vendor products in a lab environment, and summarize findings and results of testing
Technologies to be used may include some combination of relational databases (PostgreSQL, Teradata, Aster, HANA), NoSQL, Hadoop, Object-based stores, and OLAP.
Specific responsibilities include:
Help design an architecture for federated data stores and data fusion
Help design methods for storing data in a way that facilitates extremely fast data parsing and management
Implement ""glue code"" that connects middle tier components with backend components
Implement data management and analytics code utilizing data architecture (e.g. map reduce)
Collaborate with machine learning folks to determine how to analyze various data sets and set up methods for querying data stores
Collaborate with enterprise architects to understand the applications we integrate with and the data they produce
Review requirements for new approaches to big data storage and analytics
Design methods for caching, paging, and integrating real-time data with historical data stores
Desired Skills and Experience
Requirements
Minimum ten years of experience in an IT environment with at least five years designing and working with large enterprise database warehouses, designing robust data models and partitioning strategies
Minimum five years of data architecture experience, including data modeling with project teams, data governance strategy, metadata management, system architecture, design, and implementation
Minimum five years experience using one or more data integration tools including Data Quality and ETL
Development knowledge for integrating components and contributing to core code base - Java preferred
Solid understanding of database and data warehousing technologies
Experienced in advanced SQL as well as NoSQL queries, syntax, and technologies
Experienced in big data requirements, applications, and technologies such as Hadoop
Proficient in ETL methods and approaches including triggers, named views, temporary tables, etc.
Experienced in Linux environments
Bonus Points
Experience working with IT strategy teams, business teams and business analysts to define information systems, services and management
Experience with RDBMS including SQL Server, Oracle 11g, MySQL; Big Data including SQL Data Warehouse Appliance, Oracle Exadata, Netezza, Greenplum, Vertica, Teradata, Aster Data, SAP HANA, Hadoop a plus; Analytics including SAS, SPSS, Spotfire, Tableau, Qlikview, R, Oracle Endeca; BI Tools including Oracle OBIEE, SAP Business Objects, SAS and other Analytics Vendors with BI components; ETL & MDM including Informatica, SAS Dataflux, IBM, Siperion, Rochade, Map/Reduce for ETL is a plus
Java is strongly preferred (e.g. for working with map reduce) but not ultimately a requirement if you excel in other areas
Strong SQL skills are highly desirable
OLAP experience
Contact: HR Manager
Email: hr@insygnum.com
Phone: (630)-799-1556"
28,Data Engineer,"Chicago, IL 60601",Chicago,IL,60601,None Found,"
Bachelor’s Degree
At least 3 years of SDLC experience using Java technologies
At least 3 years experience with leading big data technologies like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper
At least 1 years experience in one of the following Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform
",None Found,None Found,None Found,None Found,"77 West Wacker Dr (35012), United States of America, Chicago, Illinois

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer

Do you want to work for a tech company that writes its own code, develops its own software, and builds its own products? We experiment and innovate leveraging the latest technologies, engineer breakthrough customer experiences, and bring simplicity and humanity to banking. We make a difference for 65 million customers. We're changing banking for good. At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who love to solve real problems and meet real customer needs. We want you to be curious and ask “what if?” Capital One started as an information strategy company that specialized in credit cards, and we have become one of the most impactful and disruptive players in the industry. We have grown to see ourselves as a technology company in consumer finance, with great opportunities for software engineers who want to build innovative applications to give users smarter ways to save, transact, borrow and invest their money, as we seek to disrupt the industry again. As a Capital One Software Engineer, you'll work on everything from customer-facing web and mobile applications using cutting-edge open source frameworks, to highly-available RESTful services, to back-end Java based systems using the hottest techniques in Big Data.

You'll bring solid experience in emerging and traditional technologies such as: node.js, Java, AngularJS, React, Python, REST, JSON, XML, Ruby, HTML / HTML5, CSS, NoSQL databases, relational databases, Hadoop, Chef, Maven, iOS, Android, and AWS/Cloud Infrastructure to name a few.

You will:

Work with product owners to understand desired application capabilities and testing scenarios
Continuously improve software engineering practices
Work within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies
Lead the craftsmanship, availability, resilience, and scalability of your solutions
Bring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community
Encourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity

Basic Qualifications:

Bachelor’s Degree
At least 3 years of SDLC experience using Java technologies
At least 3 years experience with leading big data technologies like Cassandra, Accumulo, Python, HBase, Scala, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper
At least 1 years experience in one of the following Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform

Preferred Qualifications:

Master's Degree
2+ year experience with Spark
3+ years experience developing software solutions to solve complex business problems

At this time, Capital One will NOT sponsor a new applicant for employment authorization for this position."
29,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"master Solid understanding of workflow systems and their application to customer business process improvement Experience working with HL7 interface engines such as Mirth Connect, Cloverleaf, Corepoint, or Ensemble General understanding of healthcare revenue cycle management (RCM), including patient accounting, claims processing, follow-up processes, and reimbursement practices Familiarity with Salesforce.com
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire."
30,Data Engineer,"Chicago, IL 60647",Chicago,IL,60647,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer - 40558

Data Science - USA Chicago, Illinois

This hands-on position will take a leading role in designing and building the Data Science pipeline for a key Media initiative: Connected TV Measurement.

Job Responsibilities

In this role, you’ll work directly with cross-functional internal teams to design and implement Data Science workflows, delivering stable, scalable infrastructure used for methodology decisioning and custom client analysis. You will be responsible for:

Building and integrating functional modules to create an end to end testing suite for Data Scientists

Exerting technical influence over teams, increasing their productivity and effectiveness by sharing your deep knowledge and experience in software engineering, cloud infrastructure, machine learning, and automation

Driving technical solutions to complex business problems, from lightweight automation to designing and building robust, scalable solutions, with a strong focus on optimization

Providing research support for the identification and implementation of methods and best practices to improve performance and scalability of data modeling approaches

Collaborating with Application Development and Technology teams to integrate and test new modules in production pipelines

Representing Nielsen within academic and research councils

Role Requirements E=essential, P=preferred.

E- 3-5+ years of professional work experience in pipeline management, software development, and/or related disciplines with direct experience developing Data Science pipelines

E- Extensive experience in data aggregation techniques, automation, generalization.

E- Experience with sophisticated data mining, modeling, and deep learning solutions and the tools that support them such as Scikit-learn and PyTorch

E- Experience with implementing and managing software in cloud-based environments such as Azure or AWS

E - Extensive experience with relational and distributed database systems with expertise in tools such as Spark and SQL

E- Familiarity with DevOps tools and CI/CD workflows including github and Jenkins

E- Familiarity with Airflow, Docker, and/or Kubernetes or related

P- Advanced degree in Computer Science, Information Systems, Data Science or other related field

P- Domain expertise in Digital or Television measurement

ABOUT NIELSEN

We’re in tune with what the world is watching, buying, and everything in between. If you can think of it, we’re measuring it. We sift through the small stuff and piece together big pictures to provide a comprehensive understanding of what’s happening now and what’s coming next for our clients. Today’s data is tomorrow’s marketplace revelation.

We like to be in the middle of the action. That’s why you can find us at work in over 100 countries. From global industry leaders to small businesses, consumer goods to media companies, we work with them all. We’re bringing in data 24/7 and the possibilities are endless. See what’s next with us at Nielsen: careers.nielsen.com

Nielsen is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class.

Job Type: Regular

Primary Location: Chicago,Illinois

Secondary Locations: FL - Tampa - Oldsmar, NY - New York City, ,

Travel: Yes, 10% of the Time"
31,"Big Data Engineer, Senior","Chicago, IL 60654",Chicago,IL,60654,None Found,"Bachelor’s degree or equivalent in an engineering or technical field such as Computer Science, Information Systems, Statistics, Engineering, or similar.
5-10 years of quantitative and qualitative experience in building ETL data flows in Big Data Ecosystem.
","Designing, building and operationalizing large-scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, S3, EC2, DynamoDB, RedShift, Kinesis, Lambda, Glue, Snowflake etc.
Hands-on experience analyzing, re-architecting and re-platforming on-premise data warehouses to data platforms on AWS cloud using AWS/3rd party services.
Knowledge about other NoSQL databases, such as MongoDB, Cassandra, HBase, etc.
Building and migrating the complex ETL pipelines on Redshift and Elastic Map Reduce to make the system grow elastically
Hands-on knowledge in using advanced SQL queries (analytical functions), experience in writing and optimizing highly efficient SQL queries
Proficiency with Python scripting
Experienced in testing and monitoring data for anomalies and rectifying them.
Advanced communication skills to be able to work with business owners to develop and define key business uses and to build data sets that address them.
Experience in working with Data visualization tools such a Tableau
 ","Responsible for designing, building & managing the advanced analytics platform to support downstream data science and the business intelligence teams
Work with the product owner, data scientist and various internal data users to understand the business requirements and implement optimal data solutions
Keen eye towards configuration driven approach to automate repeatable processes and tasks
Build and operate stable, scalable and highly performant data pipelines that cleanse, structure and integrate disparate big data sets into a readable and accessible format for end user analyses and targeting.
Develop data quality and governance framework that supports data lineage and ensures delivery of high-quality data to internal and external stakeholders
Using analytical and problem-solving skills to take complex business requests and transform them into clean, simple data solutions.
Implement/improve version control, deployment strategies, notifications to ensure product quality, agility and recoverability.
Understand and work with technology & IT teams to support database procedures, such as upgrade, backup, recovery, migrations, etc.
",None Found,None Found,"Summary
The Big Data Engineer is responsible for building scalable data platforms, and large-scale processing systems that enable advanced analytics and support data teams across the enterprise. This hands-on role requires some experience working with AWS cloud platform in addition to expertise in a variety of technologies. The Big Data Engineer will develop and manage the enterprise data warehouse while sourcing data from various databases/applications & web APIs using stream and batch processing architectures.
Responsibilities
Responsible for designing, building & managing the advanced analytics platform to support downstream data science and the business intelligence teams
Work with the product owner, data scientist and various internal data users to understand the business requirements and implement optimal data solutions
Keen eye towards configuration driven approach to automate repeatable processes and tasks
Build and operate stable, scalable and highly performant data pipelines that cleanse, structure and integrate disparate big data sets into a readable and accessible format for end user analyses and targeting.
Develop data quality and governance framework that supports data lineage and ensures delivery of high-quality data to internal and external stakeholders
Using analytical and problem-solving skills to take complex business requests and transform them into clean, simple data solutions.
Implement/improve version control, deployment strategies, notifications to ensure product quality, agility and recoverability.
Understand and work with technology & IT teams to support database procedures, such as upgrade, backup, recovery, migrations, etc.
Role Specific Skills
Designing, building and operationalizing large-scale enterprise data solutions and applications using one or more of AWS data and analytics services in combination with 3rd parties - Spark, EMR, S3, EC2, DynamoDB, RedShift, Kinesis, Lambda, Glue, Snowflake etc.
Hands-on experience analyzing, re-architecting and re-platforming on-premise data warehouses to data platforms on AWS cloud using AWS/3rd party services.
Knowledge about other NoSQL databases, such as MongoDB, Cassandra, HBase, etc.
Building and migrating the complex ETL pipelines on Redshift and Elastic Map Reduce to make the system grow elastically
Hands-on knowledge in using advanced SQL queries (analytical functions), experience in writing and optimizing highly efficient SQL queries
Proficiency with Python scripting
Experienced in testing and monitoring data for anomalies and rectifying them.
Advanced communication skills to be able to work with business owners to develop and define key business uses and to build data sets that address them.
Experience in working with Data visualization tools such a Tableau
 Professional Skills
These are the professional skills we would expect from an individual fully established in this role.
Customer Service - Advanced
Verbal Communication - Advanced
Written Communication - Advanced
Teamwork - Advanced
Relationships – proficient
Learning Agility - Expert
Analysis - Expert
Problem Solving - Expert
Process Orientation - Expert
Prioritization - Proficient
Qualifications
Bachelor’s degree or equivalent in an engineering or technical field such as Computer Science, Information Systems, Statistics, Engineering, or similar.
5-10 years of quantitative and qualitative experience in building ETL data flows in Big Data Ecosystem.
___________________________________________________________________________________
Please note, this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities, and schedule may change at any time with or without notice.
SMS Assist is an Equal Opportunity Employer (EOE) that welcomes and encourages all applicants to apply regardless of age, race, color, religion, sex, sexual orientation, gender identify and/or expression, national origin, disability, veteran status, marital or parental status, ancestry, citizenship status, pregnancy or other reasons prohibited by law.
#ZP
#Indeed"
32,Data Engineering Manager,"Chicago, IL",Chicago,IL,None Found,None Found,"
Experience leading, managing and hiring a team of talented engineers
Expertise in at least one of the following engineering domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Expertise in at least one of the following data domains: * Predictive analytics (e.g., recommendation systems, predictive maintenance)
Natural language processing (e.g., conversational chatbots)
Document understanding
Image classification
Marketing analytics
IoT systems
Experience writing software in one or more languages such as Python or Java/Scala
Experience in technical consulting or customer-facing role
Excellent critical thinking, problem-solving and analytical skills
",None Found,None Found,None Found,None Found,"Join SADA as a Data Engineering Manager!

Your Mission

As a Data Engineering Manager at SADA, you will build and lead a growing Data Engineering team as we deliver robust data solutions for our clients on Google Cloud Platform (GCP). You will be responsible for managing a blended team of data engineers and data scientists, so a broad background in Big Data, data warehouse modernization, analytics, disaster recovery, data science, and machine learning is highly advantageous.

The diversity of customers that SADA works with ensures a steady flow of challenging data work. Be prepared to tackle real-world data problems that our customers find too difficult or time-consuming to solve themselves. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of data domain areas. Management here at SADA also means developing people and being a leader.

In this role, you will:

Be comfortable working with customer executives to align business outcomes with technical vision and goals.
Guide the day-to-day activities of a geographically distributed team, including hiring world-class talent, reviewing work and setting goals.
Provide technical and professional leadership and mentorship on a diverse range of subject matter areas, such as Big Data pipelines and data warehouses to statistics and machine learning.
Develop and codify best practices for your team that can be replicated across multiple customer engagements.
Partner with your team to develop services and offerings that scale and are repeatable.
Participate in key technical and design discussions with technical leads as a hands-on manager.
Partner with other practice leads, architects, project managers, executives and sales personnel to develop statements of work, and then oversee execution by your team with high levels of agility and quality.

Pathway to Success

#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our employees know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing data practice area with vision and passion. You will be measured by your team’s performance on customer engagements, how well your team achieves internal organizational goals, how well you collaborate with and support your team and peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the management growth track.

Expectations


Required Travel - 15-25% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Experience leading, managing and hiring a team of talented engineers
Expertise in at least one of the following engineering domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Expertise in at least one of the following data domains: * Predictive analytics (e.g., recommendation systems, predictive maintenance)
Natural language processing (e.g., conversational chatbots)
Document understanding
Image classification
Marketing analytics
IoT systems
Experience writing software in one or more languages such as Python or Java/Scala
Experience in technical consulting or customer-facing role
Excellent critical thinking, problem-solving and analytical skills

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience in a large scale, high-volume data warehouse environment
Experience operationalizing machine learning models on large datasets
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
33,Big Data Engineer-Data Warehouse,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions for the advertising industry and we're looking for a Data Engineer to join our Data Engineering team. If you are excited by technology that has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day; and evaluate thousands of data-points in real-time all while responding in just a few milliseconds, then IAS is the place for you!

As a Data Engineer you will build and expand upon the testing framework and testing infrastructure of IAS' core ad verification, analytics and anti ad fraud software products.

The ideal candidate is naturally curious, dedicated, detail-oriented with a strong desire to work with awesome people in a highly collaborative environment.

What you'll do:

Planning delivery of data pipelines and api endpoints for IAS (Integral Ad Science) product offerings
Extracting, transforming, and analyzing large volumes of structured and unstructured data
Implementing data processing solutions using Big data stack including but not limited to Hadoop, Spark, Vertica, and Hive
Writing SQL queries and other types of data requests in the context of massive data sets
Building data pipelines and data stores optimized for data science needs
Implementing systems for large-scale data analysis and machine learning. Building tools for use across the Data Science and Data Engineering teams
Identifying and implementing automated tests for software components. Deploying and maintaining data processing systems in production
Partnering with engineering teams on deployments of data science driven solutions

Who you are and what you have:

Up to 5yrs. experience as a Technology enthusiast eager to learn and use new technologies
Excellent interpersonal and communication skills
Team player
Bachelors or Masters in Computer Engineering, Computer Science, Electronics Engineering, or related fields
Competent programming in languages such as Java, C++, Scala or Python
Good understanding of algorithms and data structures
Good understanding of Big data technologies such as MapReduce, Hive, Kafka, Spark and Flink

What puts you over the top:

Experience working with databases such as MySql. Vertica or Snowflake
Experience scripting using Python or Shell
Experience working with AWS technologies

About Integral Ad Science

Integral Ad Science (IAS) is the global market leader in digital ad verification, offering technologies that drive high-quality advertising media. IAS equips advertisers and publishers with both the insight and technology to protect their advertising investments from fraud and unsafe environments as well as to capture consumer attention, and drive business outcomes. Founded in 2009, IAS is headquartered in New York with global operations in 17 offices across 13 countries. IAS is part of the Vista Equity Partners portfolio of software companies. For more on how IAS is powering great impressions for top publishers and advertisers around the world, visit integralads.com ( http://integralads.com/ ).

Equal Opportunity Employer:
IAS is an equal opportunity employer, committed to our diversity and inclusiveness. We will consider all qualified applicants without regard to race, color, nationality, gender, gender identity or expression, sexual orientation, religion, disability or age. We strongly encourage women, people of color, members of the LGBTQIA community, people with disabilities and veterans to apply.

To learn more about us, please visit http://integralads.com/ ( http://integralads.com/ ) and https://muse.cm/2t8eGlN ( https://muse.cm/2t8eGlN )

Attention agency/3rd party recruiters: IAS does not accept any unsolicited resumes or candidate profiles. If you are interested in becoming an IAS recruiting partner, please send an email introducing your company to recruitingagencies@integralads.com. We will get back to you if there's interest in a partnership."
34,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Turn it up to 11 as a Data Engineer!

Do you want to solve some of the biggest challenges facing companies today using data and analytics? Do you want the opportunity to work with a team of incredibly intelligent, fun-loving, and collaborative individuals? Are you looking to grow and develop your skills across a variety of technologies and tools?

If you answered yes to any of those questions, then Inspire11 could be a great fit for you! We are a full services local consulting firm that is building out customized solutions to help our clients stay relevant in an ever-changing technological environment. Our project work spans across a variety of specialties from data warehousing to data science and a variety of industries.

We partner with our clients to optimize their business, and ultimately blow their minds with the solutions we're able to implement. Our team is always tackling the most difficult problems that client teams face, and are never stuck in maintenance mode.

Our team is always learning from each other and pushing the boundaries of what's possible.

You're still not sold? There's More:

Growth opportunity: We are always learning new technologies and educating our clients on what's available in the data space. You also will have the opportunity to engage in growing the team and there are ample opportunities to take ownership
Work life balance: We are respectful of people's boundaries and have unlimited time off so that our team has time to recharge and do their best work
Flexible hours: We understand that our team members have different needs and do our best to work with their schedules while accommodating client needs
Inclusive environment: We are committed to building an inclusive environment where all teammates feel comfortable and supported
Fun! We love to keep things fun, both within our client work and at company wide events

So what do I have to do to join?


Participate in the full life-cycle of development, through definition, design, implementation, and testing
Identify data sources, provide data flow diagrams and documents source to target mapping and process
Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes
Work with the client and consulting team to help gather requirements; understand different processes as it relates to different parts of the business and where there is overlap
Assist with report development using tools such as: Tableau, PowerBI, Qlickview
Regularly contribute to ongoing improvements in engineering process and product development
Support business decisions with ad hoc analysis as needed

I can do that! Any other skills that I need?


5+ years of experience as a data/software engineer
Strong understanding of data modeling concepts and design
Strong understanding of data warehousing technologies, ETL processes and data flow architectures and tools
Experience in Software Development Lifecycle (SDLC) utilizing the Agile approach
Organized, detailed oriented, and can manage multiple projects at the same time
Excellent communication skills
Comfortable working in fast paced environments, are able to wear many hats, and have a general fear of being bored

We believe that everyone drives change, and everyone is an owner. Nothing excites us more than having the ability to collaborate with intelligent, highly-motivated and talented people on challenging problems as we work to change the face of the digital and analytics space."
35,Lead Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Us

We are a full stack data science company and a wholly owned subsidiary of The Kroger Company. We own 10 Petabytes of data, and collect 35+ Terabytes of new data each week sourced from 62 Million households. As a member of our engineering team you will use various cutting edge technologies to develop applications that turn our data into actionable insights used to personalize the customer experience for shoppers at Kroger. We use agile development methodology, starting with Big Room Planning bringing everyone into the planning process to build scalable enterprise applications.

Lead Data Engineer – What you'll do

As a data engineer, we develop strategies and solutions to ingest, store and distribute our big data. Our engineers use Big Data tools such as, but not limited to, Scala, Hadoop, Spark, Pyspark, Hive, JSON, and SQL in 10 week long scrum teams to developer the products, tools and features.

Responsibilities

Take ownership of features and drive them to completion through all phases of the entire 84.51° SDLC. This includes external facing and internal applications as well as process improvement activities such as:


Lead design of Hadoop and SQL based solutions
Perform development of Hadoop and SQL based solutions
Perform unit and integration testing
Collaborate with senior resources to ensure consistent development practices
Provide mentoring to junior resources
Participate in retrospective reviews
Participate in the estimation process for new work and releases
Bring new perspectives to problems
Be driven to improve yourself and the way things are done

Requirements


Bachelor's degree (typically in Computer Science, Management Information Systems, Mathematics, Business Analytics or another technically strong program) plus 5 years of experience or Master's degree (typically in Computer Science, Management Information Systems, Mathematics, Business Analytics or another technically strong program) and 3 years experience.
Solid experience with Big Data tools to include but not limited to, Scala, Hadoop, Spark, Pyspark, and Hive
Strong understanding of Agile Principles (Scrum)
Proficient with relational data modeling
Proven ability of developing with SQL (Oracle, SQLServer)
Proven ability of developing with Hadoop/HDFS
Full understanding of ETL concepts
Full understanding of data warehousing concepts
Exposure to VCS (Git, SVN)
Solid experience developing with either Java, Scala or Python

Preferred Skills – Experience in the following


Exposure to NoSQL (Mongo, Cassandra)
SOA
Junit
CI/CD

"
36,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,"A deep passion for working with data and developing software to address data processing challenges
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
Proficiency within one or more programming languages including C, C++, Python, R or JavaScript.
Proficiency with multiple data platforms including RDBMS, NoSQL, MongoDB, Spark, Hadoop
Experience with some of the following areas: Distributed Computing, Natural Language Processing, Machine Learning, Cloud Platform Development, Networking, and/or REST Service Development
Strong written and verbal communications skills
Ability to manage multiple tasks and thrive in a fast-paced team environment
","A deep passion for working with data and developing software to address data processing challenges
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
Proficiency within one or more programming languages including C, C++, Python, R or JavaScript.
Proficiency with multiple data platforms including RDBMS, NoSQL, MongoDB, Spark, Hadoop
Experience with some of the following areas: Distributed Computing, Natural Language Processing, Machine Learning, Cloud Platform Development, Networking, and/or REST Service Development
Strong written and verbal communications skills
Ability to manage multiple tasks and thrive in a fast-paced team environment
",None Found,None Found,None Found,"Job Description
Role Summary
At Citadel Securities, a leading global market maker, our team of Data Engineers are tasked with building next generation data analysis platforms. Our data analysis methods evolve on a daily basis and we empower our engineers to make bold decisions that support critical functions across the business.
Opportunities available in Chicago, New York, London
Objectives
Design, develop, test, and deploy elegant software solutions across the firm
Partner with business leaders to define priorities and deliver custom solutions
Skills and Preferred Qualifications
A deep passion for working with data and developing software to address data processing challenges
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
Proficiency within one or more programming languages including C, C++, Python, R or JavaScript.
Proficiency with multiple data platforms including RDBMS, NoSQL, MongoDB, Spark, Hadoop
Experience with some of the following areas: Distributed Computing, Natural Language Processing, Machine Learning, Cloud Platform Development, Networking, and/or REST Service Development
Strong written and verbal communications skills
Ability to manage multiple tasks and thrive in a fast-paced team environment
About Citadel Securities
Citadel Securities is a leading global market maker across a broad array of fixed income and equity securities. Our world-class capabilities position us to meet the liquidity demands of our diverse group of institutional clients in all market conditions. In partnering with us, our clients, including asset managers, banks, broker-dealers, hedge funds, government agencies and public pension programs are able to gain a powerful trading advantage and are better positioned to meet their investment goals.

The team makes its mark every day from our offices around the world: Chicago, New York, London, Hong Kong, Toronto, Shanghai, Sydney, Dublin."
37,Senior Cloud Engineer/Architect,"Itasca, IL 60143",Itasca,IL,60143,None Found,None Found,None Found,None Found,None Found,None Found,"SENIOR CLOUD ENGINEER/ARCHITECT
JOB SUMMARY
Riverside Insights is looking for a savvy, energetic and innovative Cloud Data Engineer/Architect to support the delivery of our next generation of products and services. As a leader on our Cloud Services team, will help drive our engineering culture by designing, building, and evangelizing technical solutions that enable new business capabilities on our cloud-based, distributed compute environment. The ideal candidate has experience and exceptional knowledge in designing and developing highly-scalable solutions in a cloud/hybrid environment. They must have good communication skills and be comfortable providing technical leadership to multiple development teams and business stakeholders.
This position reports directly to the Director of Software and Cloud Architecture.
JOB DETAILS
Industry: Educational and Clinical Assessments
Reports to: Sr. Director of Software & Engineering
Department: Engineering & IT
Date: July 2019
Location: Itasca, IL
KEY RESPONSIBILITIES
Lead a team of engineers, consisting of both on-premise and offshore resources, developing cloud-native applications and services
Design and implement distributed compute clusters and services that will form the core of our Microservices platform
Plan and execute large-scale migrations of systems from private infrastructure to public cloud platforms (particularly AWS or Azure)
Assess cloud readiness of applications and remediation, enabling portability/compatibility between public and private clouds
Identify, design, and implement internal process improvements: automating manual processes, optimizing delivery, re-designing infrastructure for greater scalability, etc.
Participate with vision, strategic plans, and leadership to achieve peer/stakeholder buy-in and successful alignment with business vision
Review technology trends, monitor data standards/technologies and ensure that our technology strategy aligns with the business strategy
Provide estimates and oversight within a Scrum environment
Work with internal business partners to gather requirements
Support and troubleshoot issues (process & system), identify cause, and proactively recommend sustainable corrective actions
TECHNICAL QUALIFICATIONS
Solid understanding of REST, containerization, polyglot programming, caching, as well as Event-Driven and Microservice-based architectures
Real world experience designing, developing and defending a modern distributed compute platform at scale (Nomad/Mesos/Kubernetes)
Expertise with one or more major cloud providers (AWS, Azure, Google)
Comfortable writing code in multiple languages, confident in choosing the right strongly or dynamically typed language for the job. Preferred language familiarity: Java, .NET, Python, Go
Solid understanding of use cases for relational and non-relational data and experience writing code against several different database platforms (PostgreSQL, Cassandra, Oracle, SQL Server
Experience/Familiarity with many of the following technologies:
Programming Languages: Java, C#, Python, Go
Databases: PostgreSQL, Cassandra, Apache Ignite, MongoDB, Cosmos, Oracle, MS SQL Server
AWS Services: EC2, EMR, RDS, Redshift, Kinesis, Elastisearch, S3, etc.
Azure Services: Service Fabric, Cosmos, etc.
Miscellaneous Frameworks: GraphQL, .NET Core, Docker
Dashboards/Reporting: Grafana, Kibana
Messaging and Streaming: Kinesis, Kafka, MSMQ, RabbitMQ
Workflow Management: Airflow, Vagrant
SOFT-SKILL QUALIFICATIONS
Â· Responsive to customer requests and needs
Â· Follows-through on commitments made to customers and Riverside employees
Â· Able to take complex problems to simple solutions
Â· Able to maintain a positive attitude throughout the support process
Â· Takes a calm and mature approach to dealing with difficult problems and people
Â· Perseveres through difficult problem resolution
Â· Takes a teamwork oriented approach in achieving objectives and solving problems
Â· Is typically more proactive vs. reactive in addressing issues and solving problems
Â· Capable of presenting a professional image in front of customers and partners
Â· Adapts quickly to change
Â· Ability to establish service credibility with a wide range of customer contacts
Â· Ability to â€œreadâ€� the customer and adapt behavior / approach
COMPENSATION
Competitive compensation based on prior experience and earnings.

RIVERSIDE INSIGHTS IS AN EQUAL OPPORTUNITY EMPLOYER
Riverside Insights provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
DISCLAIMER
The above statements are intended to describe the general nature and level of the work being performed by people assigned to this work. This is not an exhaustive comprehensive list of all possible duties, tasks and responsibilities. Riverside Insights management reserves the right to amend and change responsibilities to meet business and organizational needs as necessary.
ABOUT RIVERSIDE INSIGHTS
Assessments play a critical role in unlocking a personâ€™s true potential. Actionable insights â€” derived from accurate, reliable and consistent data â€” create opportunities that help people thrive.
Through our proven portfolio of trusted and precise assessment solutions, Riverside Insights offers the clarity and perspective needed to create a meaningful impact at the state, district and individual levels. We're dedicated to creating and providing a broad range of high-quality, time-tested professional testing products and services as well as delivering meaningful information that can enhance the lives of children and adults.
At Riverside Insights, we are fueled by a powerful mission: to provide insights that help elevate potential. This stems from a core belief that every person should have the ability to understand their potential and be guided to the resources that enables them to realize that potential.
We aspire to enrich 1B lives globally by 2030 by providing insights born of research-based assessments to students, clinical patients, and employees/companies.
Elevating potential is what we doâ€¦ and for over 80 years, developing and providing research-based assessments has been how we do it. Riverside is one of the preeminent and most longstanding assessment developers/publishers nationally.
Riverside is a â€œpurpose-fueled, values-ledâ€� company that is aligned behind a â€œPeopleFirstâ€� ethos and a set of 5 very important core values that guide who we work with, how we make decisions, and the standard that we hold ourselves to.
DIVERSITY & INCLUSION
Riverside Insights strives for excellence. We recognize that we must have the best people, and the best people are drawn from the broadest pool of applicants. The people we need can be found only by looking across the full spectrum of race, color, religion, creed, sex, age, national origin, citizenship status, disability, qualified veteran status, genetic information, marital status, sexual orientation and gender identity.
For our firm to excel, all team members must feel that they are operating in an inclusive environment that welcomes and supports differences, and that encourages input from all perspectives. Every Riverside Insights employee has the right to expect a workplace in which the richness of their lives and experience is welcomed and valued by their team and by the company.
Our employees always come first. To continue providing our customers with the best service and solutions, our people must be diverse. We must be fully capable of dealing with different cultures in an informed and nuanced manner. Experience has shown us that we can best serve our customers interests by tapping the insights, talents, and judgments of a diverse workforce. Riverside Insights knows and understands that our diversity is critical to our competitive advantage.

Go to www.riversideinsights.com to learn more about our organization.
Come join our growing team and be a part of our mission help elevate potential globally!"
38,Senior Cloud Solutions Architect,"Chicago, IL",Chicago,IL,None Found,None Found,"
Mastery in at least one of the following domain areas:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio
Application Development: building custom web and mobile applications on top of the GCP stack
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Experience providing oversight and direction of cloud projects
Experience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states
Experience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP
Experience across multiple cloud platforms: GCP, AWS, Azure
Experience with container engines: Kubernetes, Docker, AWS Elastic Container Service
Experience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation
Experience working with engineering and sales teams to elicit customer requirements
Ability to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders
Time management skills with the ability to manage multiple streams and lead less experienced architects
Experience as a technical consultant or another customer-facing technical role
",None Found,None Found,None Found,None Found,"Join SADA as a Sr. Cloud Solutions Architect!

Your Mission

As a Sr. Cloud Solutions Architect at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You’re also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.

You will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures that other engineers and architects demur to you for lack of expertise. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.

Pathway to Success

#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.

You will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.

As you continue to execute successfully, we will build a customized development plan together that leads you through the solution architecture or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events.
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives
Training - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Cloud Architect Certified

[https://cloud.google.com/certification/cloud-architect] and/or Google
Professional Data Engineer Certified
[https://cloud.google.com/certification/data-engineer], or able to complete one of the above within the first 45 days of employment.

Required Qualifications:

Mastery in at least one of the following domain areas:
Infrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio
Application Development: building custom web and mobile applications on top of the GCP stack
Data Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.
Experience providing oversight and direction of cloud projects
Experience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states
Experience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP
Experience across multiple cloud platforms: GCP, AWS, Azure
Experience with container engines: Kubernetes, Docker, AWS Elastic Container Service
Experience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation
Experience working with engineering and sales teams to elicit customer requirements
Ability to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders
Time management skills with the ability to manage multiple streams and lead less experienced architects
Experience as a technical consultant or another customer-facing technical role

Useful Qualifications:

Hands-on experience designing and recommending elegant solutions that drive business outcomes
Experience building, designing and migrating complex cloud architectures
Strong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team
Ability to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security
Deep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed
Knowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes
Highly self-motivated and able to work independently as well as in a team environment

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
39,Data Engineer Intern (Summer 2020),"Chicago, IL 60604",Chicago,IL,60604,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer Intern (Summer 2020)

Data Engineer Intern (Summer 2020) at Wolverine Trading
What You’ll Do
As a Wolverine Data Engineer Intern, you will have the opportunity to be involved in all aspects of a performance-driven database infrastructure geared towards a fast paced trading environment. On the technical front, the Database team touches every layer of the data stack from hardware to the application layer. That means that the team manages design and implementation from the physical disk-arrays, fibre channel, servers, and OS through to SQL development and administration. You don’t need to be an expert on everything but be ready to leverage your strengths while learning a lot to improve your weaknesses. On the business front, this team works directly with software engineers and traders alike, so you will interact closely with both team members and stakeholders. The ability to have a keen technical understanding but communicate in layman’s terms is important. Day to day tasks can vary between database design, support, tuning, SQL report writing, scripting and anything else that could touch the database layer of the firm.
Afraid you won't be a fit because you don't know about the trading industry? No worries. Wolverine offers classroom education, hands-on training, and mock trading. Wolverine also provides fully furnished apartments that are located close to the office, making your morning commute quick and easy. Additionally, Wolverine will host several social activities throughout the summer to ensure an optimal work-life balance.
What We Look For
Major in CS, MIS, engineering, or an applied science.
0-3 years of experience.
Excellent problem solving skills.
Rising Junior, Senior, or graduate student.
Database related coursework.
Key elements for the internship. Elaborate on these when applying:
MS SQL Server experience.
CHashtag development skills.
Programming or scripting experience (T-SQL preferred).
Financial or trading industry experience.
Database design and performance optimization experience.
Database administration (backups/restores, security, HA).
Networking, DNS, AD, SAN, RAID.
About Wolverine
Founded in 1994, the Wolverine companies comprise a number of diversified financial institutions specializing in proprietary trading, asset management, order execution services, and technology solutions. We are recognized as a market leader in derivatives valuation, trading, and value-added order execution across global equity, options, and futures markets. With a focus on innovation, achievement, and integrity, we take pride in serving the interests of both our clients and colleagues. The Wolverine companies are headquartered in Chicago with offices in New York and San Francisco and a proprietary trading affiliate office located in London.
Sponsorship is not available for this position.





Are you a returning applicant?


Previous Applicants:

Email:

Password:




If you do not remember your password click here."
40,Data Engineer,"Chicago, IL 60654",Chicago,IL,60654,None Found,None Found,None Found,None Found,None Found,None Found,"Grow as a developer by building things that help local businesses around the world thrive

Groupon’s mission is to become the daily habit in local commerce and fulfill our purpose of building strong communities through thriving small businesses by connecting people to a vibrant, global marketplace for local services, experiences and goods. In the process, we’re positively impacting the lives of millions of customers and merchants globally. Even with thousands of employees spread across multiple continents, we still maintain a culture that inspires innovation, rewards risk-taking and celebrates success. If you want to take more ownership of your career, then you're ready to be part of Groupon.
Are you a passionate, energetic and technology enthusiast eager to work at a rapid pace with the flexibility to work across our suite of technologies? Are you are a problem solver; someone who enjoys debugging code, resolving issues, and creating solutions for common problems? Do you get a little obsessed with the details?
We are looking for a software engineer to join our Data Platform team to support and evolve the design, implementation of our Data Lake initiative.
We're a ""best of both worlds"" kind of company. We're big enough to have the resources and scale, but small enough that a single person has a surprising amount of autonomy and can make a meaningful impact. We're curious, fun, a little intense, and kind of obsessed with helping local businesses thrive. Does that sound like a compelling place to work?
The Data Platform Team will
Research, Evangelize, Apply, Lead, Monitor cloud adoption for backend systems.
Set architecture standards and frameworks for our Data Lake
Define Ingestion patterns for all incoming data in batch and streams.
Develop best practices to provide guard rails

You’ll spend time on the following:
You will automate data processing and onboard data producers to our data lake
Conceive and develop checks and bounds to provide guardrails for our engineers operating on AWS
You will provide input into the technical aspects of our AWS/Cloud technologies, ensuring that the platform is being used to its fullest potential through designing and building standards and frameworks around our customer’s needs
Assist our customers and business partners with migration of their data sets.
Work directly with our internal engineering teams to ensure that our technology infrastructure is seamlessly and effectively designed for cloud
Provide support for AWS related issues; Triage and fix issues reported by other teams to help with early resolution

We’re excited about you if you have:
Bachelor’s degree
At least 8 years of industry experience
At least 5 years experience writing code using AWS APIs with Python or Go and using AWS Config Rules.
At least 5 years experience working with Kafka, Avro, and Delta or similar technologies
At least 3 years experience developing on Spark on Elastic Map Reduce
At least 1 year experience with Airflow, NiFi, Luigi or Azkaban
Understanding of REST, microservices for distributed architecture, support and planning utilizing Scala, Python, Go, or Ruby
Clear understanding of testing methodologies and AWS Best Practices
Experience with any of Elastic Search, Aurora, MySQL, Postgres, Redshift, Snowflake, DynamoDB, or Redis
Working experience with schema evolution design patterns
Big plus if you have experience with Apache Atlas
A big plus if you have AWS Professional Certification
A big plus if you have a MS Computer science

We value engineers who are:
Customer-focused: We believe that doing what’s right for the customer is ultimately what will drive our business forward.
Obsessed with Quality: Your Production code Just Works & scales linearly
Team players. You believe that more can be achieved together. You listen to feedback and also provide supportive feedback to help others grow/improve.
Fast learners: We are willing to disrupt our existing business to trial new products and solutions. You love learning how to use new technologies and then rapidly apply them to new problems.
Pragmatic: We do things quickly to learn what our customers desire. You know when it’s appropriate to take shortcuts that don’t sacrifice quality or maintainability.
Owners: Engineers at Groupon know how to positively impact the business.


Groupon's purpose is to build strong communities through thriving small businesses. To learn more about the world's largest local ecommerce marketplace, click here for the latest Groupon news. Plus, be sure to check out the values that shape our culture, guide our strategy and make our company a great place to work. And just don't take our word for it. Hear from real Groupon team members and learn more about our inclusive employee groups. If all of this sounds like something that's a great fit for you, then click apply and let's see where this takes us.


Groupon is an Equal Opportunity Employer

Qualifications for employment, promotion, and other terms and conditions of employment are based upon the ability to perform the job. Equal-employment opportunities are provided to all applicants and employees without regard to race, creed, religion, color, age, national origin, sex, disability, medical condition, sexual orientation, gender identity or expression, genetic information, ancestry, marital status, military discharge status (excluding dishonorable discharge), veteran status, citizenship status, or other legally protected status. We are all responsible for maintaining this policy. Groupon is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may email us at hraccommodations at groupon.com. If you have concerns related to Groupon’s equal employment opportunities, you may contact Groupon's Ethics Reporting Service Ethicspoint."
41,Data Engineer - Associate,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,"
Experience with RDBMS applications (SQL Server preferred)
Good communication skills and experience working with cross-functional teams
Exposure to the concepts of data warehouse design
SQL programming familiarity in large RDBMS systems (T-SQL preferred)","
Troubleshoot and resolve issues as they arise related to all BI Tools
Manage iteration and release cycles and deployments
Assist in data modeling and design sessions
Proactively maintain documentation and training materials",None Found,None Found,"Data Engineer (early career)
Austin or Chicago
(Visa sponsorship not currently offered)

Mattersight is a leader in enterprise analytics focused on customer and employee interactions and behaviors. Mattersight's Behavioral Analytics service captures and analyzes customer and employee interactions, employee desktop data, and other contextual information to improve operational performance and predict future customer and employee outcomes. Mattersight’s analytics are based on millions of proprietary algorithms and the application of unique behavioral models. The company's SaaS+ delivery model combines analytics in the cloud with deep customer partnerships to drive significant business value. Mattersight's applications are used by leading companies in Healthcare, Insurance, Financial Services, Telecommunications, Cable, Utilities and Government. See What Matters™ by visiting www.Mattersight.com.

Data Engineer Role & Responsibilities:
The Data Engineer will be part of the Routing Analytics R&D team, which provides the data sources and analytical tools used to help our clients derive maximum value from our Behavioral Routing solution. This position will support the day to day reporting needs of the Routing clients, including Business Monitoring, Insights, Product Development, Analysis & Testing, and others. Responsibilities include troubleshooting issues that occur with existing report deliverables as well as developing new reports and integrating them into the overall service catalog. This role will require development, testing, and configuration management of all BI deliverables in coordination with Data Engineers, Software Engineers, and Testers within the organization. Additionally, the Data Engineer will be responsible for maintaining any documentation and training materials required to support the various business units the group serves.

This individual will also support the Data Warehouse Specialist to troubleshoot issues relating to the warehouse, especially as they impact the reporting environment.

To summarize, the Data Engineer will be responsible for, but not limited to, the following tasks:

Troubleshoot and resolve issues as they arise related to all BI Tools
Manage iteration and release cycles and deployments
Assist in data modeling and design sessions
Proactively maintain documentation and training materials

Because of the data-centric culture and rapid growth of NICE Mattersight, a rich career path exists for the Data Engineer within Mattersight.

Preferred Skills/Attributes
Experience with RDBMS applications (SQL Server preferred)
Good communication skills and experience working with cross-functional teams
Exposure to the concepts of data warehouse design
SQL programming familiarity in large RDBMS systems (T-SQL preferred)
Exposure to ETL and data integration processes

Required Knowledge, Skills & Abilities

Previous Experience
For this role, Mattersight is not requiring previous work experience in a technical role though some experience in a Business Intelligence environment working with BI visualization tools, relational databases, and/or data warehousing systems is helpful. Experience with ETL applications and data modeling/UML software is also helpful. Required is an interest in data pipelines, data aggregations, and creatively solving data-related problems as well as the motivation to dive deeper into the data engineering world. We’d also like to see a candidate who displays evidence of strong communication skills and the ability to work under pressure.
Ideal CandidateAnalyticalDetail OrientedStrong CommunicatorEntrepreneurialResults OrientedTask AgilityOperations-MindedAdept Time ManagerProblem SolverTeam PlayerSeeker of ExcellenceHigh Knowledge Bandwidth

Company Culture & Facts
Corporate Culture
Mattersight values diversity amongst its employees. Employees from all levels of experience and backgrounds are mingled together and are encouraged to learn about projects others are working on. Mattersight fosters teamwork as well as self motivation.
Eligibility & Location
Mattersight seeks candidates authorized to work in the United States. The Routing Analytics Data Engineer role will be based out of Mattersight’s Austin location; candidates should anticipate little to no travel.
Compensation
Mattersight is prepared to offer a highly competitive benefits and compensation package for the ideal candidate.
For more information about Mattersight, visit http://www.mattersight.com/. Mattersight is committed to equal opportunity and affirmative action in all employment matters: Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, ancestry, marital or domestic partner status, national origin, disability or medical condition, pregnancy, veteran or military status, sexual orientation, gender identity, or on account of membership or affiliation with anyone in any of the foregoing categories, or any other protected category under federal, state or local law. Although particular legal provisions may differ in various locations in which we do business, our principles are the same worldwide."
42,Data Engineer - SQL / Python,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer
Are you an innovative and passionate Data Engineer with a strong Python background and SQL skills? This is a great opportunity to thrive in an Agile, self-driven environment where your ideas, creativity, and decisions matter.
Our client is looking to expand their data team as they continue to grow their data platform. Looking for a talented Data Engineer with a strong background with Python and SQL to implement and maintain ETL jobs, using Python to ingest external data sources into the Data Warehouse, and working closely with the Product and Data Science teams to deliver data in useable formats and to the appropriate data sources.
The ideal candidate should be a self-starter who is interested in learning new systems and building new solutions. Looking for a collaborative individual to work closely with the Data Science team to identify interesting data points for use by the Data Science team. Great opportunity to use a polyglot data model using many cutting-edge data platforms. Their delivery framework is comprised of Python/Docker on ECS, Spark on EMR, and Jenkins for CI/CD!

REQUIREMENTS:
Advance Experience with SQL
Python, Docker, and data warehouse expertise
Experience using Github / Jenkins (CI/CD) or comparable delivery methods
Experience planning and designing maintainable data schemas
Experience with Data Warehouse environments
A passion for problem solving and learning new technologies
Excellent communication skills and strong enthusiasm
Ability to handle multiple priorities and manage within deadlines
Ability to work independently and within a team environment and to foster a positive work environment

OTHER DESIREABLE SKILLS
Experience working in an Agile environment preferred
Experience with Postgres, Elastic Search, AWS EMR, and AWS ECS
Experience with AWS Redshift, MPP, or Dynamo DB
Experience with Kinesis/Kafka
Experience working with large enterprise data lakes
Excellent communication skills

RESPONSIBILITIES:
Architect and implement ETL jobs for various functions
Work with product, data science, analytics, and engineering teams to learn project data needs and define project scope.
Design and planning of data services
Building and delivery of Python/Docker feed framework data pipeline jobs and services.
Contribute to the Data Engineering team delivery framework including building of re-usable code, implementing industry best practices, and maintain a common delivery framework.
Monitoring, maintenance, documentation, and incident resolution of scheduled production data jobs supporting internal and external customers data needs.
Support the development teams by optimizing data access
Work with data science teams to deliver metrics to consumers"
43,Senior Big Data Engineer,"Chicago, IL 60290",Chicago,IL,60290,None Found,"
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.",None Found,"
Responsible for design, prototyping and delivery of software solutions within the big data eco-system
Leading projects and/or serving as analytics SME to provide new or enhanced data to the business
Improving data governance and quality increasing the reliability of our data
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
",None Found,None Found,"Where good people build rewarding careers.
Think that working in the insurance field can’t be exciting, rewarding and challenging? Think again. You’ll help us reinvent protection and retirement to improve customers’ lives. We’ll help you make an impact with our training and mentoring offerings. Here, you’ll have the opportunity to expand and apply your skills in ways you never thought possible. And you’ll have fun doing it. Join a company of individuals with hopes, plans and passions, all using and developing our talents for good, at work and in life.
About our team
360 Finance Advanced Analytics data engineering team works with multiple internal and external data sources to deliver data that is readily available, easily accessible, accurate and complete. They are responsible for building a centralized data lake/hub using the Hadoop ecosystem that will be used by Reporting & Operational Analytics teams and the Machine learning teams.
Job Description
This Lead Consultant is an experienced professional who is responsible for leveraging data and analytics to help automate and optimize Claims Analytics Data processes enabling our Claims employees to focus on serving our customers and delivering the most advanced claims experience on the planet. They will be responsible for the strategy around how we bring together complex data into clean and useful data structures making our valuable data more approachable.
Key Responsibilities
Responsible for design, prototyping and delivery of software solutions within the big data eco-system
Leading projects and/or serving as analytics SME to provide new or enhanced data to the business
Improving data governance and quality increasing the reliability of our data
Influencing the creation of a single, trusted source for key Claims business data that can be shared across the Enterprise
Key Responsibilities (Cont'd)
Responsible for designing and building new Big Data systems for turning data into actionable insights
Train and mentor junior team members on Big Data/Hadoop tools and technologies
Identifies opportunities for improvement and presents recommendations to management
Seeks out and evaluates emerging big data technologies and open-source packages
Participate in strategic planning discussions with technical and non-technical partners
Uses, teaches, and supports a wide variety of Big Data and Analytics tools to achieve results (i.e., Python, Hadoop, HIVE, Scala, Impala and others).
Uses, teaches, and supports a wide variety of programming languages on Big Data and Analytics work (i.e. Java, Python, SQL, R)
Job Qualifications
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Ability to work with broad parameters in complex situations
Experience in developing, managing, and manipulating large, complex datasets
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.
Job Qualifications (Cont'd)
Some understanding and exposure to - streaming toolsets such as Kafka, FLINK, spark streaming a plus.
Experience with source control solutions (ex git, GitHub, Jenkins, Artifactory) required
4-5+ years of experience with big data and the Hadoop ecosystem (HDFS, SPARK, SQOOP, Hive, Impala, Parquet) required
Experience with Agile development methodologies and tools to iterate quickly on product changes, developing user stories and working through backlog (Continuous Integration and JIRA a plus)
Experience with Airflow a plus
Undergraduate degree in Computer Science, Mathematics, Engineering (or related field) or equivalent experience preferred
5-7 years of experience preferred in a data integration, ETL and/or business intelligence/analytics related function
Expert high-level coding skills such as SQL and Python and/or other scripting languages(UNIX) required. Scala is a plus.
The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.
Good Work. Good Life. Good Hands®.
As a Fortune 100 company and industry leader, we provide a competitive salary – but that’s just the beginning. Our Total Rewards package also offers benefits like tuition assistance, medical and dental insurance, as well as a robust pension and 401(k). Plus, you’ll have access to a wide variety of programs to help you balance your work and personal life - including a generous paid time off policy.
Learn more about life at Allstate. Connect with us on Twitter, Facebook, Instagram and LinkedIn or watch a video.
Allstate generally does not sponsor individuals for employment-based visas for this position.
Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.
For jobs in San Francisco, please click “here” for information regarding the San Francisco Fair Chance Ordinance.
For jobs in Los Angeles, please click “here” for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.
To view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs
To view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.
It is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment."
44,"Data Analyst - ENTERPRISE DATA ANALYTICS, Full-time","Chicago, IL 60611",Chicago,IL,60611,None Found,None Found,"
Bachelor’s degree in Computer Science, Information Systems, Data Science, Engineering or related field. Masters preferred
Minimum of 2 years of related experience with a comprehensive knowledge of data modeling and database tools (e.g. FRWin, SQL, Oracle, Excel, etc.) and continues developing professional expertise.
Data cleaning, engineering, munging, experience with scripting and programming languages such as Python, PERL, Ruby, php, Java, R/Shiny, etc.
Familiarity with Visualization and web development such as CSS, JavaScript, D3/Node.js, Ajax, etc.
Healthcare experience highly desired, especially familiarity with ICD-9,10, CPT and other healthcare vocabularies
Hands-on experience with statistical software packages and data mining tools (e.g. SAS, R, SPSS, Weka, Rapid Miner, etc.)
Hands-on experience with industry leading data visualization and BI tools (e.g. Business Objects, Microsoft PowerBI, Cognos, Tableau, OBIEE, Qlickview, Knowi, etc)
Strong strategic, analytical, technical, and project management skills
Demonstrated ability in engaging and communicating with stakeholders, across both business and technology functions.
","Gathers, profiles, and analyzes data to support requirements analysis and/or analytics.
Collaborates with technical experts and business units to determine the best possible reporting/dash-boarding mechanisms.
Analyzes data and generates reports that ensure data integrity by applying statistical and data science methods.
Performs data manipulation on large sets of data to extract actionable insights.
Combines data from different sources and conducts source to target mapping
Identifies trends and patterns in data and creates data visualizations
Performs complex analysis including statistical regression and forecasting
Drives analytical rigor across the organization to guide decision making
Performs other duties that may be assigned in the best interest of the Shirley Ryan AbilityLab.
",None Found,None Found,"General Summary

The Data Analyst/Engineer will be part of the Data Innovation and Analytics Lab (DIAL). Responsible for data analytics to support the needs of the business units by utilizing the Enterprise Data Warehouse. The Data Analyst/Data Engineer/Informatics Specialist will utilize data science to work with cross-functional team of stakeholders at the Shirley Ryan AbilityLab, to help gather insights for research, quality, operations and outcomes.

The Data Analyst/Engineer will consistently demonstrate support of the Shirley Ryan AbilityLab statement of Vision, Mission and Core Values by striving for excellence, contributing to the team efforts and showing respect and compassion for patients and their families, fellow employees, and all others with whom there is contact at or in the interest of the institute.

The Data Analyst/Engineer will demonstrate Shirley Ryan AbilityLab Core Attributes: Communication, Accountability, Flexibility/Adaptability, Judgment/Problem Solving, Customer Service and Core Values (Hope, Compassion, Discovery, Collaboration, & Commitment to Excellence) while fulfilling job duties.


Principal Responsibilities

The Data Analyst/Engineer:
Gathers, profiles, and analyzes data to support requirements analysis and/or analytics.
Collaborates with technical experts and business units to determine the best possible reporting/dash-boarding mechanisms.
Analyzes data and generates reports that ensure data integrity by applying statistical and data science methods.
Performs data manipulation on large sets of data to extract actionable insights.
Combines data from different sources and conducts source to target mapping
Identifies trends and patterns in data and creates data visualizations
Performs complex analysis including statistical regression and forecasting
Drives analytical rigor across the organization to guide decision making
Performs other duties that may be assigned in the best interest of the Shirley Ryan AbilityLab.
Reporting Relationships

Reports directly to the Chief Data Officer as part of the Data Innovation and Analytics Lab (DIAL).


Knowledge, Skills & Abilities Required

Bachelor’s degree in Computer Science, Information Systems, Data Science, Engineering or related field. Masters preferred
Minimum of 2 years of related experience with a comprehensive knowledge of data modeling and database tools (e.g. FRWin, SQL, Oracle, Excel, etc.) and continues developing professional expertise.
Data cleaning, engineering, munging, experience with scripting and programming languages such as Python, PERL, Ruby, php, Java, R/Shiny, etc.
Familiarity with Visualization and web development such as CSS, JavaScript, D3/Node.js, Ajax, etc.
Healthcare experience highly desired, especially familiarity with ICD-9,10, CPT and other healthcare vocabularies
Hands-on experience with statistical software packages and data mining tools (e.g. SAS, R, SPSS, Weka, Rapid Miner, etc.)
Hands-on experience with industry leading data visualization and BI tools (e.g. Business Objects, Microsoft PowerBI, Cognos, Tableau, OBIEE, Qlickview, Knowi, etc)
Strong strategic, analytical, technical, and project management skills
Demonstrated ability in engaging and communicating with stakeholders, across both business and technology functions.


Working Conditions

Normal office environment with little or no exposure to dust or extreme temperature.


The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties and skills required of personnel so classified.


SRAlab is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
Required Skills

Required Experience"
45,Technical Data Engineer Lead,"Chicago, IL",Chicago,IL,None Found,None Found,"Must have a Bachelor’s degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Master’s degree (preferred) in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.
Understand Hadoop cluster administration concepts.
3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.
Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.
Must have experience with batch and real-time data pipelines.
Must have experience as a Hadoop Technical Lead / Architect
Must have experience with design, development and deployment in the specified technologies.
Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.
Writing complex SQL queries, extracting and importing large amounts of data.
Must be willing to work in a fast-paced environment with an on shore – off shore distributed Agile teams.
Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.
Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.
Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.
Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through
",None Found,"Lead a development team of data engineers
Implement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases (Hbase) and EMR (Hadoop)
Responsible for design, development, testing oversight and implementation
Works closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks
Coordinate with data engineers and API developers to drive program delivery.
Drive technical development and application standards across enterprise data lake
Benchmark and debug critical issues with algorithms and software as they arise.
Lead and assist with the technical design and implementation of the Big Data cluster in various environments.
Guide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts.
Perform other duties and/or special projects as assigned
",None Found,"Must have a Bachelor’s degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Master’s degree (preferred) in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.
Understand Hadoop cluster administration concepts.
3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.
Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.
Must have experience with batch and real-time data pipelines.
Must have experience as a Hadoop Technical Lead / Architect
Must have experience with design, development and deployment in the specified technologies.
Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.
Writing complex SQL queries, extracting and importing large amounts of data.
Must be willing to work in a fast-paced environment with an on shore – off shore distributed Agile teams.
Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.
Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.
Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.
Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through
","Job Description:
Role Summary/Purpose:
We are looking for a Technical Data Engineer Lead to lead the development of consumer-centric low latency analytic environment leveraging Big Data technologies and transform the legacy systems.
Essential Responsibilities:
Lead a development team of data engineers
Implement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases (Hbase) and EMR (Hadoop)
Responsible for design, development, testing oversight and implementation
Works closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks
Coordinate with data engineers and API developers to drive program delivery.
Drive technical development and application standards across enterprise data lake
Benchmark and debug critical issues with algorithms and software as they arise.
Lead and assist with the technical design and implementation of the Big Data cluster in various environments.
Guide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts.
Perform other duties and/or special projects as assigned
Qualifications/Requirements:
Must have a Bachelor’s degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Master’s degree (preferred) in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.
Understand Hadoop cluster administration concepts.
3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.
Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.
Must have experience with batch and real-time data pipelines.
Must have experience as a Hadoop Technical Lead / Architect
Must have experience with design, development and deployment in the specified technologies.
Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.
Writing complex SQL queries, extracting and importing large amounts of data.
Must be willing to work in a fast-paced environment with an on shore – off shore distributed Agile teams.
Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.
Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.
Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.
Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through
Desired Characteristics:
Extensive experience working with data warehouses and big data platforms
Demonstrated experience building strong relationships with senior leaders
Strong leadership and influencing skills
Outstanding written and verbal skills and the ability to influence and motivate teams
Eligibility Requirements:
You must be 18 years or older
You must have a high school diploma or equivalent
You must be willing to take a drug test, submit to a background investigation and submit fingerprints as part of the onboarding process
You must be able to satisfy the requirements of Section 19 of the Federal Deposit Insurance Act.
New hires (Level 4-7) must have 9 months of continuous service with the company before they are eligible to post on other roles. Once this new hire time in position requirement is met, the associate will have a minimum 6 months’ time in position before they can post for future non-exempt roles. Employees, level 8 or greater, must have at least 24 months’ time in position before they can post. All internal employees must have at least a “consistently meets expectations” performance rating and have approval from your manager to post (or the approval of your manager and HR if you don’t meet the time in position or performance requirement).
Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
Reasonable Accommodation Notice:
Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.
If you need special accommodations, please call our Career Support Line so that we can discuss your specific situation. We can be reached at 1-866-301-5627. Representatives are available from 8am – 5pm Monday to Friday, Central Standard Time.
The salary range for this position is 85,000.00 - 170,000.00 USD Annual
Salaries are adjusted according to market in CA and Metro NY and some positions are bonus eligible.
Grade/Level: 12
Job Family Group:
Information Technology"
46,Data Engineer,"Rosemont, IL 60018",Rosemont,IL,60018,None Found,"Bachelor’s degree in Computer Science, Information Systems, Business Administration, or other related field required
Minimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources

Minimum 2 years of experience with PowerBI

Minimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications

Minimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired

Experience working with both structured and unstructured data
Demonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types (star, snowflake, etc.)
Data modeling experience in building logical and physical data models

Applied knowledge of Microsoft Security/Authentication Concepts (Active Directory, IIS, Windows OS)

Strong technical planning skills with the ability to prioritize and multitask across a number of work streams

Must have a passion for continued improvement, learning, and mentoring

Polished presentation skills; experience creating and presenting findings to executive level staff

Strong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively

Must be highly collaborative with the ability to manage and motivate project teams and meet deliverables

Ability to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders
Experience with Data Warehouse is a plus

Knowledge of SSRS is a plus

Healthcare industry experience a plus
",None Found,None Found,None Found,None Found,"You are known for your development and deployment of innovative big data platforms for advanced analytics and data processing. You lead innovation through exploration, benchmarking, making recommendations, and implementing big data technologies for platforms. You are excited by defining and building data pipelines that will enable faster, better, data-informed decision-making within the business. You have a passion for continued improvement, learning and mentoring, and leverage this enthusiasm when building stakeholder relationships. You are collaborative, insightful and want to work for an organization making a difference in the field of orthopaedics on behalf our members and their patients.
If this sounds like you, read on!
The Data Engineer is primarily responsible for maintaining and enhancing the Registry’s data acquisition, integration, and ETL pipelines in support of both operational and business intelligence data stores. The incumbent is responsible for applying diverse data cleansing and transformation techniques as well as the ongoing management and monitoring of all Registry databases. This includes addressing issues pertaining to the ongoing operations and optimization of the data environment including performance, reliability, logging, scalability, etc. This position will also provide support for the Academy’s database systems, warehouse, marts, and supporting applications.
Lead the effort to develop a unified enterprise data model for the Academy. Design, develop, and maintain high-performance data platforms on premise and in Microsoft Azure cloud-based environments including leading the development of a data warehouse environment to support the Registry’s business intelligence roadmap. Champion efforts that will ensure that the Academy’s business intelligence applications remain relevant for use by internal business groups by actively participating in strategy and project planning discussions. Work collaboratively with Registry participants and internal support teams, identify and implement changes that improve system performance and the user experience.
Design, develop, and maintain the ETL pipelines using SSIS that standardize raw data from multiple data sources and optimize both the operational and dimensional/star schema data model necessary for transactional systems and business intelligence applications. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions aligning to an overall data architecture. Extract, transform, and load data to and from various data sources including relational databases, NoSQL databases, web services, and flat files. Produce various technical documents such as ER diagrams, table schemas, data lineage, API documents, etc.
Provide leadership and oversight on database architectural design for existing Academy systems including database administration, performance monitoring, and troubleshooting. Provide complex analysis, conceptualize, design, implement, and develop solutions for critical data-centric projects. Perform dataflow, system and data analysis, and develop meaningful and useful presentation of data in downstream applications. Plan and implement standards, define/code conformed global and reusable objects, perform complex database design and data repository modelling.
Monitor ETL processes, system audits, dashboard reporting, and presentation layer functioning and performance. Proactively identify and implement procedures that resolve performance and/or data reporting issues. Support the optimal performance of the Academy’s data and BI systems. Monitor database performance, provide optimization recommendations, and implement recommendations. Follow the release cycles and implement on-time delivery of task assignments, defect correction, change requests, and enhancements. Troubleshoot and solve technical problems. Perform other responsibilities as assignment by management.
Required Qualifications:
Bachelor’s degree in Computer Science, Information Systems, Business Administration, or other related field required
Minimum of 3 years of relevant work experience in a data engineering role leveraging SQL, SSIS; including design and support of ETL routines that support the import of data from multiple data sources

Minimum 2 years of experience with PowerBI

Minimum of 3 years of data warehousing experience including the design, development, and ongoing support of star or snowflake data schemas to support business intelligence applications

Minimum 5 years of database administration or database development experience in a SQL or MySQL environment; knowledge of Microsoft technology stack; background in Azure Infrastructure as a Service environment desired

Experience working with both structured and unstructured data
Demonstrated understanding of Business Intelligence and data solutions including cubes, data warehouse, data marts, and supporting schema types (star, snowflake, etc.)
Data modeling experience in building logical and physical data models

Applied knowledge of Microsoft Security/Authentication Concepts (Active Directory, IIS, Windows OS)

Strong technical planning skills with the ability to prioritize and multitask across a number of work streams

Must have a passion for continued improvement, learning, and mentoring

Polished presentation skills; experience creating and presenting findings to executive level staff

Strong written, verbal and interpersonal communication skills, with an ability to communicate ideas and solutions effectively

Must be highly collaborative with the ability to manage and motivate project teams and meet deliverables

Ability to build strong stakeholder relationships and translate complex technical concepts to non-technical stakeholders
Experience with Data Warehouse is a plus

Knowledge of SSRS is a plus

Healthcare industry experience a plus

If this describes YOU, please apply by sharing the following:
Clearly communicate why you are the ideal candidate for this role, providing specific examples and experiences as proof points.
Attach your resume, cover letter and any additional materials that support your application.
Salary expectations must be included to be considered for this opportunity."
47,"Manager, Data Engineering","Chicago, IL",Chicago,IL,None Found,None Found,None Found,"
BS in Computer Science or equivalent education/professional experience is required.
5+ years in a data-engineering role with demonstrable experience with data integration and data warehouse projects.
Experience architecting and building data warehouses, customer profile databases, data marts, etc.
Knowledge of Apache Beam and programming languages including Java and Python.
Experience with MPP systems (Google Big Query, AWS Redshift, Azure Datawarehouse).
Experience with data modeling, warehouse design, and fact/dimension concepts.
Experience working with different query languages (i.e. T-SQL, PostgreSQL, PL-SQL).
Experience in data integration projects and automation via ETL Tools (i.e. Talend, Informatica, SSIS, etc.).
Experience in Hadoop (Hive, Spark, Impala, etc.) ecosystem is a plus.
Experience working with code repositories and continuous integration (i.e. Git, Jenkins, etc.)
Understanding of development and project methodologies.
Ability to work collaboratively in teams with other specialized individuals.
Able to work in a fast-paced, technical environment.
Good verbal and written communication skills.","
Deliver quality work on defined tasks with little oversight and direction.
Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews.
Participate in integrated test sessions of components and subsystems on test and production servers.
Serve as technical resource during software development life cycle to solve business issues through the process of identifying and analyzing detailed requirements that translate into data integration and database system designs.
Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs.
Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers
Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed.
Ability to transform data into actionable information and convert the results of the analysis into a format that is easy to draw insights from and to share with colleagues and peers.",None Found,None Found,"This is a senior data engineer within our North American team. The data engineer will collaborate within a team of technologists to produce enterprise scale solutions for our clients’ needs. This position will be focused on building out a customer data hubs/profile databases and building data warehouse solutions.

Primary Responsibilities:
Deliver quality work on defined tasks with little oversight and direction.
Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews.
Participate in integrated test sessions of components and subsystems on test and production servers.
Serve as technical resource during software development life cycle to solve business issues through the process of identifying and analyzing detailed requirements that translate into data integration and database system designs.
Solve business issues through the process of identifying and analyzing detailed requirements that translate business requirements into technical system designs.
Use information gained through prior experience, knowledge sharing with other Technology Associates, education and training to resolve issues and remove project barriers
Provide status updates to team members on a regular basis and clearly escalate issues and risks to project management as needed.
Ability to transform data into actionable information and convert the results of the analysis into a format that is easy to draw insights from and to share with colleagues and peers.
Required Skills and Experience:
Responsible for the maintenance, improvement, cleaning, and manipulation of data in the business’s customer data platform and analytics databases. Works with the business’s data analytics teams, data scientists, and software engineers in order to understand and aid in the implementation of database requirements, analyze performance, and troubleshoot any issues. Defines and builds the ETL and data pipelines to enable faster, better, data-informed decision-making within the business.
BS in Computer Science or equivalent education/professional experience is required.
5+ years in a data-engineering role with demonstrable experience with data integration and data warehouse projects.
Experience architecting and building data warehouses, customer profile databases, data marts, etc.
Knowledge of Apache Beam and programming languages including Java and Python.
Experience with MPP systems (Google Big Query, AWS Redshift, Azure Datawarehouse).
Experience with data modeling, warehouse design, and fact/dimension concepts.
Experience working with different query languages (i.e. T-SQL, PostgreSQL, PL-SQL).
Experience in data integration projects and automation via ETL Tools (i.e. Talend, Informatica, SSIS, etc.).
Experience in Hadoop (Hive, Spark, Impala, etc.) ecosystem is a plus.
Experience working with code repositories and continuous integration (i.e. Git, Jenkins, etc.)
Understanding of development and project methodologies.
Ability to work collaboratively in teams with other specialized individuals.
Able to work in a fast-paced, technical environment.
Good verbal and written communication skills."
48,Big Data Engineer,"Chicago, IL 60604",Chicago,IL,60604,None Found,"
3+ years of experience in programming languages such as Java or Python.
2+ years of experience in big data engineering.
1+ years of experience as Spark Developer.
","
Ability to develop spark jobs to cleanse/enrich/process large amounts of data.
Ability to develop spark streaming jobs to read data from Kafka.
Experience with tuning spark jobs for efficient performance including execution time of the job, execution memory, etc.
Good understanding of various file formats and compression techniques.
Experience with source code management systems such as GIT and developing CI/CD pipelines with tools such as Jenkins for data.
Ability to understand deeply the entire architecture for a major part of the business and be able to articulate the scaling and reliability limits of that area; design, develop and debug at an enterprise level and design and estimate at a cross-project level.
Ability to mentor developers and lead projects of medium to high complexity.
Excellent communication and collaboration skills.
",None Found,None Found,None Found,"Cars.com is one of Chicago's original tech companies. Our online platform makes it easier for consumers to shop for, sell, and service their cars. With our expert content, mobile app features, millions of new and used vehicle listings, a comprehensive set of research tools and the largest database of consumer reviews in the industry, Cars.com offers innovative products to connect consumers with dealers across the country.

Data is the driver of our future at Cars. We're searching for highly collaborative, analytical, and innovative Engineers to build and scale our big data and ML platform. If you are passionate about using data to solve problems and build game changing products, we'd love to work with you.

About the Role:
Working within a dynamic and fast paced team environment, the Senior Engineer is responsible for the design, construction, and maintenance of mission-critical, highly visible big data and machine learning applications in direct support of Cars.com business objectives. Furthermore, this person is responsible for working with other engineers to develop the technical design by fully understanding the technical details, integration, and functions of multiple applications across their development team. The ideal candidate should have good mentoring and cross-functional skills.

About the Team:
The Big Data and Machine Learning team at Cars.com is responsible for building big data pipelines and deriving insights out of the data using advanced analytic techniques, streaming and machine learning at scale.

Qualifications:

3+ years of experience in programming languages such as Java or Python.
2+ years of experience in big data engineering.
1+ years of experience as Spark Developer.

Required Skills:

Ability to develop spark jobs to cleanse/enrich/process large amounts of data.
Ability to develop spark streaming jobs to read data from Kafka.
Experience with tuning spark jobs for efficient performance including execution time of the job, execution memory, etc.
Good understanding of various file formats and compression techniques.
Experience with source code management systems such as GIT and developing CI/CD pipelines with tools such as Jenkins for data.
Ability to understand deeply the entire architecture for a major part of the business and be able to articulate the scaling and reliability limits of that area; design, develop and debug at an enterprise level and design and estimate at a cross-project level.
Ability to mentor developers and lead projects of medium to high complexity.
Excellent communication and collaboration skills.

Bonus:

Experience developing Big Data applications in the cloud, especially AWS.
Experience tuning HIVE queries.
Experience in deploying ML models into production and integrating them into production applications for use.
Experience with Spark ML.
Experience with machine learning / deep learning using R, Python, Jupyter, Zeppelin, TensorFlow, etc.

"
49,Data Engineer,"Chicago, IL 60654",Chicago,IL,60654,None Found,None Found,None Found,None Found,None Found,None Found,"Working for Trunk Club
----------------------

When you join Trunk Club, you join the Nordstrom family. Our fast-paced and entrepreneurial environment is paired with the strong history and experience of a retail legacy. We have access to some of the greatest minds in retail and technology and are constantly creating innovative strategies to develop the ultimate apparel solutions. We welcome your adaptability, your curiosity, and your passion to contribute to our unparalleled shopping experience!

Who we are
----------

At Trunk Club, we're building the future of retail, enabled through technology. On the Data Services team, we believe data has the power to drive a business forward, and we push every day to enable data-driven decisions across all aspects of the organization. We have hundreds of data sources, a robust enterprise data warehouse and use Tableau for visual analytics. Some of the tools and platforms we work with are Python, SQL, AWS (S3, Lambda, EC2), Spark and many other open-source technologies.

Responsibilities
----------------


Building end-to-end data integration and data warehousing solutions for analytics teams.
Designing and creating services and system architecture.
Building and maintaining enterprise data platforms.
Working daily with SQL and Python and using Spark for big data processing.
Using analytical and problem-solving skills to take complex business requests and transform them into clean, simple data solutions.
Brainstorming and contributing ideas to our technology, algorithms, and products.
Working with an agile mindset to deliver small projects quickly in order to provide value and gain feedback in a production environment.
Bringing insight and experience to the Data Services team through mentoring other engineers.
Building out architecture and API integrations for tools and platforms
Taking ownership of projects to lead them to completion
Creating amazing user experiences for both internal and external customers.

What we are looking for
-----------------------


Someone whom has implemented serverless microservices
Familiarity in containerization with Docker
2-3 years experience building Big Data pipelines in a streaming fashion
Deep understanding of Hadoop, Hive or Spark with repeated experience
Ability to mentor others in Big Data technologies

How we work
-----------


With collaboration. We frequently pair program. We work at pairing stations - two keyboards, two minds, one outcome.
With transparency. We work in an open team room; no cubicles or private offices. Communication is key to our process, and we don't want to hinder it with walls.
With agility. We don't believe in following a process for process's sake. We ship frequently and focus on delivering incremental value.
With open minds. We are committed to building a diverse team of people with unique perspectives. This encourages a healthy and inclusive environment that builds a more sustainable, successful company.
With pride. We value our people most of all. We invest in ourselves by applying our own strengths and interests to company needs.

A few of our perks
------------------


Lunch-and-learns
Annual stipend for continuous education
Tech all-hands lunches every other Friday
Hack days
Team outings
Nordstrom discount
Flexible work environment
Social environment with built-in bars

Who you are
-----------


An Analyst. You are able to take complex requests and transform them into clean, simple data solutions.
A Learner. You have an insatiable thirst for knowledge and greater understanding.
A Pragmatist. Your goal is to create useful products, not build technology for technology's sake.
A Closer. You are someone who loves to get stuff done, and done well.

"
50,Senior Data Engineer,"Chicago, IL 60647",Chicago,IL,60647,None Found,None Found,None Found,None Found,None Found,None Found,"Senior Data Engineer - 41803

Technology and Engineering - USA Chicago, Illinois

Senior Data Engineer - Chicago, IL

ABOUT THIS JOB

Around here we’re all numbers people, but it’s the 1s and 0s behind our data that make what we do possible. Software engineers strike a balance between precision and disruption, between reliability and innovation. Nielsen is a tech company backed by nearly a century of forward momentum to show the world what’s next—and we couldn’t have done it without our engineers.

In this role, you will be working alongside data scientists and engineers within Nielsen’s Marketing Effectiveness business, to build a data platform on AWS to ingest data from external sources and perform ETL tasks. The role requires you to have experience working with large datasets with complex schemas, a can-do approach towards automation, with an emphasis on the implementation of best practice cloud security principles.

RESPONSIBILITIES

Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability
Collaborate with product and technology teams to design and validate the capabilities of the data platform
Develop, implement and maintain good programming standards and practices across the ecosystem
Experience building and optimizing data pipelines in a distributed environment
Experience supporting and working with cross-functional teams
Excellent communication skills and the ability to present complex ideas in a clear and concise manner to a variety of audiences

QUALIFICATIONS

4+ years of advanced working knowledge of SQL, Python, and Spark
2+ years of experience with using a broad range of AWS technologies - S3, Lambda, Glue, Athena, IAM, SQS, CloudWatch, CloudFormation
Experience with the Hadoop ecosystem
Proficiency working in Linux environment
Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline
Experience with platform monitoring and alerts tools
The training to back your work: a master’s degree, bachelor’s degree, or, as we recognize there isn’t one formula for success, equivalent work experience
ABOUT NIELSEN

We’re in tune with what the world is watching, buying, and everything in between. If you can think of it, we’re measuring it. We sift through the small stuff and piece together big pictures to provide a comprehensive understanding of what’s happening now and what’s coming next for our clients. Today’s data is tomorrow’s marketplace revelation.

We like to be in the middle of the action. That’s why you can find us at work in over 100 countries. From global industry leaders to small businesses, consumer goods to media companies, we work with them all. We’re bringing in data 24/7 and the possibilities are endless. See what’s next with us at Nielsen: careers.nielsen.com

Nielsen is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class.

Job Type: Regular

Primary Location: Chicago,Illinois

Secondary Locations: , , ,

Travel: No"
51,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,"
Design, build and maintain our data pipelines and automate analyses using SQL and Python based ETL framework
Develop ETL ecosystem tools
Analyze query pattern of internal users and adjust analytics schemas based on those patterns
Collaborate with data science and stakeholders across the organization to raise the bar for data best practices and management
Demonstrate and communicate a deep understanding of your chosen languages and frameworks to be able to make tradeoffs. Able to do more with less complexity.
Advocate for internal and external customers to break down problems, set priorities and follow up on performance and functionality
Build and maintain internal data processing and visualization tools to ensure that stakeholders have timely access to data.
Participate in pairing sessions, code reviews and take initiative on research projects/ requirements
",None Found,"
Professional experience in ETL systems and database architecture
Production-level Python experience a must, Scala and AWS experience are pluses as they're part of our ecosystem.
Understanding of different types of data storage and their trade-offs with regards to availability, consistency, read/write throughput and maintenance cost.
Ability to communicate effectively with engineering peers, data analytics, and business stakeholders.
","Reverb is a leading online marketplace for buying and selling new, used, and vintage musical instruments. Since launching in 2013, Reverb has grown into a vibrant community of buyers and sellers all over the world. By focusing on inspiring content, price transparency, musician-focused eCommerce tools, a music-savvy customer service team, and more, Reverb has created an online destination where the global music community can connect over the perfect piece of music gear.

As part of the Reverb Data Engineering team, you’ll help build the platform to enable data-driven decisions and products that scale along with our business. We’re using Python and Scala in our data pipeline to support our BI users. We’re a small, eager team so we’re looking for engineers who can take a high degree of initiative and enjoy working across team boundaries.

Everyone at Reverb takes creative initiative, helps set their own priorities, and comes up with new ways to grow the business. Our engineers take pride in building great software but take even more pride in shipping great features for our customers. If you want to learn more, check out
this video on working at Reverb
[https://www.youtube.com/watch?time_continue=45&v=rROUwUytfDU].

Responsibilities :

Design, build and maintain our data pipelines and automate analyses using SQL and Python based ETL framework
Develop ETL ecosystem tools
Analyze query pattern of internal users and adjust analytics schemas based on those patterns
Collaborate with data science and stakeholders across the organization to raise the bar for data best practices and management
Demonstrate and communicate a deep understanding of your chosen languages and frameworks to be able to make tradeoffs. Able to do more with less complexity.
Advocate for internal and external customers to break down problems, set priorities and follow up on performance and functionality
Build and maintain internal data processing and visualization tools to ensure that stakeholders have timely access to data.
Participate in pairing sessions, code reviews and take initiative on research projects/ requirements

Requirements :

Professional experience in ETL systems and database architecture
Production-level Python experience a must, Scala and AWS experience are pluses as they're part of our ecosystem.
Understanding of different types of data storage and their trade-offs with regards to availability, consistency, read/write throughput and maintenance cost.
Ability to communicate effectively with engineering peers, data analytics, and business stakeholders.

What you'll get:
To complement our competitive compensation and equity plans, we offer:

No-bureaucracy environment where ownership and initiative are valued.
Health insurance and a healthy work environment.
401k with company match.
Paid parental leave.
Flexible vacation and sick days.
Pre-tax commuter benefits.
Bi-monthly lunches.
A MacBook, monitor, keyboard, mouse of your choice and standing desk.
Discounts on music gear.

This is a local position in Chicago, please no remote workers or recruiters. Please send us a link to your Github!"
52,Market Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,"You have a deep understanding of either Java or C++ and a practical understanding of the other (C++ is preferred)
You have experience writing high-performance feed handlers in a Linux environment
You have experience with multi-threaded programming and distributed application architecture
You have strong analytic and design capabilities
You have the ability to quickly triage issues and drive the resolution effort through completion
You prefer simple, cohesive, and practical solutions that are maintainable and extensible
You demonstrate a high level of interpersonal skills and are skilled in building collaborative relationships",None Found,"You design and build Market Data server-based components and frameworks
You collaborate with developers on other teams including operations, option pricing, exchange access, inventory and risk management
You develop requirements, propose solutions and deliver software into production environment in a timely and robust manner
You participate in design and code reviews
You partner with QA and support teams to ensure that the software delivered is high quality and easy to manage in a production environment
You keep up to date on the latest technologies that could benefit our system
",None Found,None Found,"Are you passionate about market data? Do you think about market microstructure and the trade-offs between low latency and ultra-low latency? Do you love collaborating with others to solve tough problems? Do you crave being involved in mission-critical initiatives that have direct and meaningful impact on an organization’s daily performance?

We are looking for a software developer with experience in the processing, distribution, and management of financial data. This includes market data, reference data, trade management, and several data-related services. These services are primarily developed in Java and C++ and deployed on Linux. Software development cycles at CTC are short and releases of proprietary software are frequent. You would join our team in a fast-paced, dynamic environment where our team-mates quickly see the results of their effort, and their impact on our business.

Responsibilities
You design and build Market Data server-based components and frameworks
You collaborate with developers on other teams including operations, option pricing, exchange access, inventory and risk management
You develop requirements, propose solutions and deliver software into production environment in a timely and robust manner
You participate in design and code reviews
You partner with QA and support teams to ensure that the software delivered is high quality and easy to manage in a production environment
You keep up to date on the latest technologies that could benefit our system
Qualifications
You have a deep understanding of either Java or C++ and a practical understanding of the other (C++ is preferred)
You have experience writing high-performance feed handlers in a Linux environment
You have experience with multi-threaded programming and distributed application architecture
You have strong analytic and design capabilities
You have the ability to quickly triage issues and drive the resolution effort through completion
You prefer simple, cohesive, and practical solutions that are maintainable and extensible
You demonstrate a high level of interpersonal skills and are skilled in building collaborative relationships"
53,Tax Services Senior – National Tax – Tax Technology and Transformation (TTT) – Data Scientist – Advanced Technologies - Chicago,"Chicago, IL 60606",Chicago,IL,60606,None Found,None Found,"Strong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.
Experience with common data science programming languages, such as Python R
Strong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential
Experience with data visualization tools, such as PowerBI, D3.js, etc.
Experience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB
Strong scripting and programming skills
Ownership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.
Ability to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies
Ability to consolidate tax data to make analysis and planning more efficient
Focus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities
Willingness to support project team members in any way needed to help ensure timely completion of deliverables",None Found,None Found,None Found,"Tax Technology and Transformation offers services to companies in response to the impact of existing and emerging technology, including the growing data burden that many businesses face, driving efficiencies to create a cost-effective tax function and the need to understand how to make data an asset. The underlying objective of the combined offerings is to help businesses navigate the digital age of tax transparency alongside new trends in tax compliance and tax audit methods, as well as helping to solve the most pressing challenges that businesses face. Tax Technology and Transformation is composed of the following services:
Digital tax transformation
Tax applications-as-a-service
Tax data and improvement
Tax analytics and reporting enhancement
Emerging tax technology, including robotic process automation (RPA), artificial intelligence (AI), blockchain, cloud solutions, data lake development and business intelligence innovation
Tax technology program mobilization
Custom tax technology application development and deployment
Tax technology strategy and road mapping
Direct and indirect tax systems implementation and configuration
Post-transaction (M&A) tax function operational services
Tax operating model transformation, including process improvement, risk and controls
Tax function assessments
The opportunity

Tax Technology and Transformation is an area that has seen significant growth and investment recently, and you will see that reflected in your experience. It is no exaggeration to say that you will be working on highly publicized projects. The field of taxation is constantly changing as new laws, regulations, and technologies are created, and this is your opportunity to be part of that development.

Key responsibilities

We are looking for an ambitious, self-motivated data scientist or data engineer who will help us discover the information hidden in vast amounts of data, and help us deliver even better products to our clients. Your primary focus will be in applying data mining techniques, doing statistical analysis and building high quantity prediction systems integrated with our products. You will be expected to team on a national and even global scale, so strong communication skills, attention to detail, and ability to effectively drive results are essential.
Selecting features and, building and optimizing classifiers using machine learning techniques
Data mining using state-of-the-art methods
Enhancing data collection procedures to include information that is relevant for building analytic systems
Processing, cleansing and verifying the integrity of data used for analysis
Doing ad-hoc analysis and presenting results in a clear manner

Depending on your unique skills and ambitions, you could be supporting various client projects, ranging from assisting in the production of leading edge machine models, to designing and implementing robust data pipelines that can handle data at a multinational, enterprise scale. Whatever you find yourself doing, you will contribute and help toward developing a highly trained team, all the while handling activities with a focus on quality and commercial value. This is a highly regulated industry, so it is all about maintaining our reputation as trusted advisors by taking on bold initiatives and owning new challenges.

Skills and attributes for success
Strong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.
Experience with common data science programming languages, such as Python R
Strong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential
Experience with data visualization tools, such as PowerBI, D3.js, etc.
Experience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB
Strong scripting and programming skills
Ownership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.
Ability to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies
Ability to consolidate tax data to make analysis and planning more efficient
Focus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities
Willingness to support project team members in any way needed to help ensure timely completion of deliverables
To qualify for the role, you must have
A bachelor’s degree in information system, tax technology, management information systems or computer science or related field and a minimum of two years of related work experience
A passionate interest in data science and its role in the organization
Excellent communication and business writing skills
A natural flair for problem solving and an entrepreneurial approach to work
Strong organizational and time management skills, with exceptional client-serving consulting skills
Demonstrated ability to capture and synthesize business requirements
Desire and demonstrated ability to provide leadership within a team
Ideally, you’ll also have
Experience with Apache Spark
Experience with Hadoop and/or distributed database systems
Experience working in the Microsoft Azure Cloud environment
Experience developing ETL solutions using SSIS or other tools
ERP experience, including SAP and/or Oracle-preferred but not required
Practical experience or strong theoretical understanding of neural networks
What we look for

We are looking for knowledgeable data science professionals with a passion for turning data into actionable insight. You will need strong business acumen and a firm strategic vision, so if you are ready to use those skills to develop your team, this role is for you.

What working at EY offers

We offer a competitive compensation package where you will be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, a minimum of three weeks of vacation plus ten observed holidays and three paid personal days; and a range of programs and benefits designed to support your physical, financial and social well-being. We also offer:
Support and coaching from some of the most engaging colleagues in the industry
Opportunities to develop new skills and progress your career
The freedom and flexibility to handle your role in a way that is right for you
About EY

As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.
EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

If you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible.

Make your mark. Apply today.

."
54,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,"
Bachelor's Degree in computer science or equivalent experience required.
2+ years of experience in the design and development of data pipelines and tasks.
Strong analytical and problem solving ability with strong attention to detail and accuracy.
Good understanding of data warehousing concepts and dimensional data modeling.
Hands-on experience with troubleshooting performance issues and fine tuning queries.
Proven experience extracting data from structured data sources (SQL, Excel, CSV files) and unstructured data sources (Couchbase, Splunk, log files) both on-premise and in the cloud
Experience consuming data from web services, SOAP and REST technologies, HTML, XML and JSON.
Knowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation.
Proficient in at least one programming language: Python, Java, Go, C#, Ruby, C, C++.
Experience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required.
Familiar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.
","
Design, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources.
Monitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues.
Perform unit testing, system integration testing and assist with user acceptance testing.
Adapt data components to accommodate changes in source data and new business requirements.
Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks.
Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.
Collaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.
Ability to work with the rest of the Data Engineering Team to cross-train and provide support for other BI tasks such as Cube Maintenance, Data Analytics and Requirements Gathering.
",None Found,None Found,"GoHealth is looking for Data Engineers who will be responsible for the design, development, and delivery of data transformation tasks used in transforming data into a format that can be easily analyzed. We are seeking candidates who have experience in data analysis, collection, and optimization for the purpose of informing business decisions. The Data Engineer will work with other team members in owning data pipelines including execution, documentation, maintenance, and metadata management. In this role, you will also support the development of the data infrastructure necessary for full scale data science, predictive analytics and machine learning.

Responsibilities:

Design, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources.
Monitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues.
Perform unit testing, system integration testing and assist with user acceptance testing.
Adapt data components to accommodate changes in source data and new business requirements.
Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks.
Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.
Collaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.
Ability to work with the rest of the Data Engineering Team to cross-train and provide support for other BI tasks such as Cube Maintenance, Data Analytics and Requirements Gathering.

Skills and Experience:

Bachelor's Degree in computer science or equivalent experience required.
2+ years of experience in the design and development of data pipelines and tasks.
Strong analytical and problem solving ability with strong attention to detail and accuracy.
Good understanding of data warehousing concepts and dimensional data modeling.
Hands-on experience with troubleshooting performance issues and fine tuning queries.
Proven experience extracting data from structured data sources (SQL, Excel, CSV files) and unstructured data sources (Couchbase, Splunk, log files) both on-premise and in the cloud
Experience consuming data from web services, SOAP and REST technologies, HTML, XML and JSON.
Knowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation.
Proficient in at least one programming language: Python, Java, Go, C#, Ruby, C, C++.
Experience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required.
Familiar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.

Benefits and Perks:

Open vacation policy
401k program with company match
Medical, dental, vision, and life insurance benefits
Flexible spending accounts
Commuter and transit benefits
Professional growth opportunities
Casual dress code
Generous employee referral bonuses
Happy hours, ping-pong tournaments, and more company-sponsored events
Subsidized gym memberships
GoHealth is an Equal Opportunity Employer

"
55,Advanced Data Engineer,"Rosemont, IL",Rosemont,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Advanced Data Engineer is a key position within the Data Management team, responsible for the design, development, test and implementation of best-in-class reporting solutions for the organization. This is an advanced role that will be engaged in all phases of the reporting & analytics development lifecycle, and will be called upon to mentor less experienced team members as necessary. Successful candidate will have a proven track record demonstrating in-depth technical and business knowledge.
Key Accountabilities
Design and develop best-in-class, interactive, reporting and dashboard solutions focused on answering specific business need. Develop and maintain analytical models in support of key decision making processes
Perform data analysis for projects and tasks. Identify, analyze, and interpret patterns in complex data sets
Assist with data governance initiatives including data quality, and other data stewardship activities
Pursues self-development and effective relationships with others by sharing resources, information, and knowledge with coworkers and customers. Seeks opportunities to deliver continuous process improvement

Qualifications


Bachelor degree in Math, Statistics, Economics, MIS or other quantitative field
5+ years’ experience working in a customer facing, BI or Analytical role
Demonstrates strong analytical and problem solving skill
Role is customer facing - must have excellent communication skills
Knowledge of BI centric Azure Cloud services, including PowerBI is a plus.
Strong experience in designing & creating Advanced Tabular models using SQL Server Analysis Services.
Experience in Optimizing SSAS Tabular models (Processing, Querying, Partitioning, and RLS) with strong knowledge in DAX.
Experience in Data warehousing ETL inclusive of dimensional modeling concepts.
Experience with Data Analytics using scripting languages such as Python, Perl, Spark and so forth is a plus
Proven ability to work within a team environment and interacting with data professionals and business data SME’s throughout the organization
Desire to work in a fast growing environment with strong time management ability
Must be self-directed and have excellent initiative and organization skills.
Proven track record of meeting commitments with the highest standards of ethics and integrity.
Experience working in the financial services industry or other highly regulated environment is desirable
As your local community bank, we work hard to support the neighborhoods we serve. We are members of many local chambers of commerce, and we’re active in local charities, local government, school boards and community development activities. At Wintrust you can “Have it All” with access to the big bank resources and the customer service you can only get from a local community bank that makes decisions locally and still calls customers by their name. We invite you to be part of the Wintrust team by submitting your application today!
Wintrust is a financial holding company with assets of over $30 billion whose common stock is traded on the NASDAQ Global Select Market. Built on the ""HAVE IT ALL"" model, Wintrust offers sophisticated technology and resources of a large bank while focusing on providing service-based community banking to each and every customer. Wintrust operates fifteen community bank subsidiaries, with over 160 banking locations located in the greater Chicago and southern Wisconsin market areas. Additionally, Wintrust operates various non-bank business units including business units which provide commercial and life insurance premium financing in the United States, a premium finance company operating in Canada, a company providing short-term accounts receivable financing and value-added out-sourced administrative services to the temporary staffing services industry, a business unit engaging primarily in the origination and purchase of residential mortgages for sale into the secondary market throughout the United States, and companies providing wealth management services and qualified intermediary services for tax-deferred exchanges.

Wintrust Financial Corporation, including community banking and financial services subsidiaries, is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, ethnicity, gender, sexual orientation, gender identity, national origin, veteran status, or disability."
56,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Engineer will be part of the Capture & Analytics Data Services team, which provides the data sources and analytical tools used to help our clients derive maximum value from our Behavioral Analytics and Routing solutions. This position will support the day to day reporting needs of numerous clients, including Business Monitoring, Insights, Product Development, Analysis & Testing, Telephony Integration and others. Responsibilities include troubleshooting issues that occur with existing report deliverables as well as developing new reports and integrating them into the overall service catalog. This role will require development, testing, and configuration management of deliverables in coordination with Data Engineers and Testers within the organization, as well as external vendor and customer resources. Additionally, the Data Engineer will be responsible for maintaining any documentation and training materials required to support the various business units the group serves.
This individual will also support the Data Warehouse to troubleshoot issues relating to the warehouse, especially as they impact the reporting environment.

To summarize, the Data Engineer will be responsible for, but not limited to, the following tasks:

1. Troubleshoot and resolve issues as they arise related to all Data Services
2. Help to improve automation within the data warehouse and customer environments
2. Collaborate with vendor and customer resources to build complex data feeds for reportings
3. Manage iteration and release cycles and deployments
4. Assist in data modeling and design sessions
5. Proactively maintain documentation and training materials

Because of the data-centric culture and rapid growth of NICE-Mattersight, a rich career path exists for the Data Engineer within Mattersight.

What you definitely have:
2–5 years report development experience, specifically with Business Intelligence tools (Tableau preferred)Solid experience with RDBMS applications (SQL Server preferred)Good communication skills and experience working with cross-functional teamsStrong understanding of data warehouse design and implementation best practicesAbility to explain principles of data visualizationSQL programming familiarity in large RDBMS systems (T-SQL preferred)Exposure to ETL and data integration processes
What we’d also love to see:
Project experience preparing use cases and user requirementsDesign and development of micro-services-oriented ArchitectureExposure to reporting interfaces that integrate with popular telephony platforms
Be You

We are all different and that is powerful. Variability fuels our business and unites our work. It teaches us that strength lies in differences. To see what matters is our culture, and our culture starts with you.

Your different perspectives inspire us to be better. Your diversity fosters creativity and accelerates our innovation. Your unique skills and abilities make us stand out. Your background and experiences help us reach our full potential.

We are committed to a workplace that is increasingly diverse and inclusive, so be your best you.

NICE Systems is an Equal Opportunity/Affirmative Action Employer, M/F/D/V."
57,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"IMC-Where Technology drives Trading


Trading nowadays happens in a highly competitive technological landscape; the best trading idea alone doesn’t cut it anymore. Instead, only the best trading ideas that are enabled via robust, scalable and fast technology win.

Do you enjoy the process of problem solving, a process where you recognize areas of improvement and iterate and innovate to improve? Does your curiosity and desire to learn drive you?


DATA ENGINEERING AT IMC:

As a Data Engineer at IMC, you’ll build and administer data workflows in an evolving, modern Hadoop-based environment. You’ll also:
Develop and extend in-house data toolkits based in Python and Java.
Consult and educate internal users on Hadoop technologies and assist them in finding and effectively utilizing the best solutions for their problem space.
Improve the performance of financial analytics platforms built around the Hadoop ecosystem.

WHAT MAKES IT FUN?
IMC is on the cutting edge of financial applications of Hadoop, processing terabytes of data daily for mission critical trading systems.
We operate at the bleeding edge of technology. If something new can potentially bring an advantage we will adopt and incorporate the new technology.
The landscape is always changing creating new and exciting challenges. What we focus on today is very different than what we focused on two years ago.
We really believe in sharing knowledge and technology between the different offices. Much of our technology stack is shared globally between our offices, and we provide opportunities to travel between the regions both for personal growth and to assist where it has the biggest impact.
Working at IMC is a great way to gain exposure to and learn about financial markets and technology. We know from experience that a lot of people really enjoy learning about a field beyond their immediate area of expertise, it’s one of the things that makes this job more interesting than others.
We employ a broad range of people with varying backgrounds. What they have in common is their superior technical expertise, their extraordinary smarts and their collaborative approach.

WHO YOU ARE:
3+ years of experience working with Hadoop 2 (YARN), cluster management experience preferable
3+ year of experience with Hadoop SQL interfaces including Hive and Impala
2+ years of experience developing solutions using Spark
Experience with common data-science toolkits, Python-based preferred
Strong Java, SQL, and Python development skills
Strong statistical analysis skills
Strong systems background, preferably including Linux administration
Unix scripting experience (bash, tcsh, zsh, python, etc)
Experience with DevOps tools such as SALT and Puppet as part of a CI/CD development and deployment process.
Demonstrated ability to troubleshoot and conduct root-cause analysis

Experience with the below (not required, but definitely desired):
Developing with Apache Kafka
Containerization and Docker
OSS scheduling tools, preferably Luigi
Developing solutions in the Machine learning space, with an emphasis on Change/Anomaly detection
Building Cube/Cube-like products

OUR CULTURE:

We are at the core a trading firm, however we value trading and technology equally and we believe that cooperation between traders and technologists is one of our great strengths. This is also reflected in our organizational and remuneration policies. We believe in fostering a truly flat environment in which great ideas can be recognized as well as put into practice from anybody within our organization.


WHO WE ARE:

IMC is among the world’s leading proprietary trading firms, and a market maker in securities listed on exchanges across the globe. Our cutting-edge technology drives everything we do. High performance algorithms, smart strategies and collaborative teams are the core of our business.

Today, IMC is 600+ people working together to build software and trade financial products in our offices in Amsterdam, Chicago and Sydney. What does this mean for you? The chance to join a multi-national, multi-cultural team of exceptional individuals, focused on making IMC the world’s best trading firm."
58,Sr Data Engineer ETL Developer,"Lombard, IL",Lombard,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job ID: JP-1018028
Description:
BASIC FUNCTION:

As a Data Engineer ETL Developer, you will contribute to a group that designs, builds, test, and implements high quality business solutions and capabilities within the Analytic and Reporting Department. Function in an agile framework to understand complex business problems and brainstorm flexible solutions which adhere to organization’s architecture and solution patterns. Support the organization’s journey legacy technology tools to modern data lake platform.

ESSENTIAL FUNCTIONS:

Work closely with customers, business analysts, data modeler, and other developers to deliver high quality analytic and reporting capabilities.
Design, develop, automate, monitor and maintain Extract Transform Load (ETL) data movement applications using our preferred ETL tools and techniques.
Align solutions with business and IT strategies and comply with the organization’s architectural standards.
Actively engage with agile team to deliver analytic and reporting functionality through iterative process. Help department improve development life cycle through innovation and challenging internal process.
Interact with scrum master and agile team to estimate development efforts and ensure accurate requirements fulfillment.
Participate in daily stand-up meetings, planning meetings and review sessions (using Scrum / Agile methodology).
Champion recommendations towards the development of new code or reuse of existing code. Responsibilities may also include participation in component and data architecture design, performance monitoring, product evaluation and buy versus build recommendations.
Practice systems analysis, design and a solid understanding of development, quality assurance and integration methodologies.
Troubleshoot data issues, recommend, test and implement solutions
Develop and document technical requirements and solutions
Regularly communicate with peers, business and management team.
Comply with HIPAA, Diversity Principles, Corporate Integrity, Compliance Program policies and other applicable corporate and departmental policies.


JOB REQUIREMENTS:

Bachelor's Degree in Computer Science, Information Systems or other related field. Or equivalent work experience.
Experience working with ETL Tools such as Data Stage, Ab Initio, Informatica
5 years of programming/systems analysis experience.
Requires in-depth knowledge and experience
Uses best practices and knowledge of internal or external business issues to improve products or services
Independently solves complex problems; takes a new perspective using existing solutions
Works independently, receives minimal guidance
Acts as a resource for colleagues with less experience; may direct the work of other staff members
PREFERRED REQUIREMENTS:

Experience with ETL Tools and technologies such as Talend, Spark, Kafka, Python, Hadoop, Cloud, etc
CA
Dearborn National is committed to diversity in the workplace and to providing equal opportunity and affirmative action to employees and applicants. We are an Equal Opportunity Employment / Affirmative Action employer dedicated to workforce diversity and a drug-free and smoke-free workplace. Drug screening and background investigation are required, as allowed by law. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status.
Requirements:
Expertise Information Technology

Job Type Full-Time Regular

Location IL - Lombard

Preferred Language English"
59,"Data Engineer, Zoro","Chicago, IL 60603",Chicago,IL,60603,None Found,None Found,"Strong ELT/ ETL designer/developer
Strong SQL
Strong Python
Structured & unstructured data expertise
Cloud environment development & operations experience (e.g. AWS, GCP)
Preference for candidates experienced with:
Google Cloud Platform (GCP) and associated services; e.g. BigQuery, GCS, Cloud Composer, Dataproc, Dataflow, Dataprep, Cloud Pub/Sub, Metadata DB, Data Studio, Datalab, other
Other important Zoro tools: Apache Airflow (scheduler), Bitbucket and git (version control), Stackdriver (ops monitoring), Opsgenie (alert notification), Docker
Real-time data replication/streaming tools
Data Modeling
Excellent verbal and written communications
Strong team player
","Participate in Requirements Gathering: work with key business partner groups (e.g. Product Mgt) and other Data Engineering personnel to understand department-level data requirements for the ZDP
Design Data Pipelines: work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the ZDP
Build Data Pipelines: leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the ZDP
Support Data Quality Program: work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure ZDP maintains consistently high quality data
Support Operations: triage alerts channeled to you and remediate as necessary
Technical Documentation: leverage templates provided and create clear, simple and comprehensive documentation for your development
Key contributor to defining, implementing and supporting:
Data Services
Data Dictionary
Tool Standards
Best Practices
Data Lineage
User Training
",None Found,None Found,"Position: Data Engineer
Location: Chicago (primary), Buffalo Grove (secondary)
Reports To: Director – Data Engineering

In the past six years, Zoro has grown from a group of 6 people working out of a 2,000 square foot building, offering fewer than 100,000 products to a group of 450+ working out of a 60,000+ square foot building, offering more than 3,000,000 unique products.

Imagine what you could help us achieve as a Data Engineer!

The Data Engineer will collaborate with various other IT groups, business partners and external service providers and play a key role in the design, development and operations of our new analytics platform, the “Zoro Data Platform (ZDP)”.

 Job Responsibilities Include:
Participate in Requirements Gathering: work with key business partner groups (e.g. Product Mgt) and other Data Engineering personnel to understand department-level data requirements for the ZDP
Design Data Pipelines: work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the ZDP
Build Data Pipelines: leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the ZDP
Support Data Quality Program: work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure ZDP maintains consistently high quality data
Support Operations: triage alerts channeled to you and remediate as necessary
Technical Documentation: leverage templates provided and create clear, simple and comprehensive documentation for your development
Key contributor to defining, implementing and supporting:
Data Services
Data Dictionary
Tool Standards
Best Practices
Data Lineage
User Training
Skills & Responsibilities:
Strong ELT/ ETL designer/developer
Strong SQL
Strong Python
Structured & unstructured data expertise
Cloud environment development & operations experience (e.g. AWS, GCP)
Preference for candidates experienced with:
Google Cloud Platform (GCP) and associated services; e.g. BigQuery, GCS, Cloud Composer, Dataproc, Dataflow, Dataprep, Cloud Pub/Sub, Metadata DB, Data Studio, Datalab, other
Other important Zoro tools: Apache Airflow (scheduler), Bitbucket and git (version control), Stackdriver (ops monitoring), Opsgenie (alert notification), Docker
Real-time data replication/streaming tools
Data Modeling
Excellent verbal and written communications
Strong team player

Success Criteria
Strong analytical thinking and problem solving skills
Superior communication and business-technical interaction skills
Positive, “get it done” attitude
Ability to multi-task and manage multiple activities with varying timelines

To qualify, you must possess the following skills:
Bachelor’s degree in computer science, management information systems, or a related discipline
5+ years hands-on ETL/ELT design/development experience
Key resource on team(s) that have delivered successful enterprise-level analytics platforms

Zoro is an Equal Opportunity Workplace and an Affirmative Action Employer.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status."
60,"Sr Data Engineer, Zoro","Chicago, IL 60603",Chicago,IL,60603,None Found,None Found,"Expert-level data modeler (back-end and semantic layer)
Expert-level ETL/ELT designer/developer
Strong database administration and operations experience & proficiency
Strong SQL
Structured & unstructured data expertise
Cloud environment development & operations experience (e.g. Google Cloud Platform/GCP experience a plus)
Excellent verbal and written communications
Strong team player
Working knowledge of eCommerce data a plus
Prior experience with Git, Terraform, GCP Deployment Manager, CICD, Docker, Kubernetes, Apache Airflow, Apache Beam, Apache Spark experience is a plus

","Primary responsibility for Zoro Data Platform (ZDP):
Data model ongoing design & development
Conceptual, logical and physical design (database, ODS, aggregates, etc.)
Database administration
Capacity analysis & management
MDM Lead
Identify key domains that’d benefit from an MDM approach (e.g. Product, Customer), along with best data sources & necessary attributes, and integrate into the ZDP
Define governance strategy with associated roles & responsibilities (e.g. Data Steward, Quality Specialist)
Define & implement Policies & SOPs
Monitor operations, develop and report quality metrics to key stakeholders
Data Pipeline development:
Participate in Requirements Gathering: work with key business partner groups (e.g. Product Mgt) and other Data Engineering personnel to understand department-level data requirements for the ZDP
Design Data Pipelines: work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the ZDP
Build Data Pipelines: leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the ZDP
Support Data Quality Program: work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure ZDP maintains consistently high quality data
Support Operations: triage alerts channeled to you and remediate as necessary
Technical Documentation: leverage templates provided and create clear, simple and comprehensive documentation for your development
Key contributor to defining, implementing and supporting:
Data Services
Data Dictionary
Tool Standards
Best Practices
Data Lineage
User Training
Define Best Practices and Guidelines for other Data Engineering team members
Lead the team in developing new technical skills necessary for cloud-native data engineering platform
Explores new tech
Shares and documents learnings
Productionalizes proof of concepts

",None Found,None Found,"Position: Senior Data Engineer
Location: Chicago (primary), Buffalo Grove (secondary)
Reports To: Director – Data Engineering

In the past six years, Zoro has grown from a group of 6 people working out of a 2,000 square foot building, offering fewer than 100,000 products to a group of 450+ working out of a 60,000+ square foot building, offering more than 3,000,000 unique products.

Imagine what you could help us achieve as a Senior Data Engineer!

The Senior Data Engineer will collaborate within Data Engineering and with other IT groups, business partners and external service providers, to play a key role in building, maintaining and supporting our new analytics platform, the “Zoro Data Platform (ZDP)”. S/he will also provide direction to, and oversee various service providers and junior engineers’ activities.

Job Responsibilities Include:
Primary responsibility for Zoro Data Platform (ZDP):
Data model ongoing design & development
Conceptual, logical and physical design (database, ODS, aggregates, etc.)
Database administration
Capacity analysis & management
MDM Lead
Identify key domains that’d benefit from an MDM approach (e.g. Product, Customer), along with best data sources & necessary attributes, and integrate into the ZDP
Define governance strategy with associated roles & responsibilities (e.g. Data Steward, Quality Specialist)
Define & implement Policies & SOPs
Monitor operations, develop and report quality metrics to key stakeholders
Data Pipeline development:
Participate in Requirements Gathering: work with key business partner groups (e.g. Product Mgt) and other Data Engineering personnel to understand department-level data requirements for the ZDP
Design Data Pipelines: work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the ZDP
Build Data Pipelines: leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the ZDP
Support Data Quality Program: work with Data QA Engineer to identify automated QA checks and associated monitoring & alerting to ensure ZDP maintains consistently high quality data
Support Operations: triage alerts channeled to you and remediate as necessary
Technical Documentation: leverage templates provided and create clear, simple and comprehensive documentation for your development
Key contributor to defining, implementing and supporting:
Data Services
Data Dictionary
Tool Standards
Best Practices
Data Lineage
User Training
Define Best Practices and Guidelines for other Data Engineering team members
Lead the team in developing new technical skills necessary for cloud-native data engineering platform
Explores new tech
Shares and documents learnings
Productionalizes proof of concepts

Skills & Responsibilities:
Expert-level data modeler (back-end and semantic layer)
Expert-level ETL/ELT designer/developer
Strong database administration and operations experience & proficiency
Strong SQL
Structured & unstructured data expertise
Cloud environment development & operations experience (e.g. Google Cloud Platform/GCP experience a plus)
Excellent verbal and written communications
Strong team player
Working knowledge of eCommerce data a plus
Prior experience with Git, Terraform, GCP Deployment Manager, CICD, Docker, Kubernetes, Apache Airflow, Apache Beam, Apache Spark experience is a plus

Success Criteria:
Expert knowledge of data modeling concepts and data relationships
Advanced Analytical Thinking and Problem Solving skills
Solid experience in architecture, advanced reporting and dashboards
Strong SQL skills and experience with performance tuning are required
“Get it done” attitude
Superior Communication and Business-Technical Interaction skills

To qualify, you must possess the following skills:
Bachelor’s degree in computer science, management information systems, or a related discipline
15+ years hands-on data warehouse-data modeling experience
15+ years hands-on database admin/ops experience
10+ years hands-on ETL/ELT design/development experience
Key resource on team(s) that have delivered successful enterprise-level analytics platforms

Zoro is an Equal Opportunity Workplace and an Affirmative Action Employer.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status."
61,Software Engineer,"Chicago, IL 60606",Chicago,IL,60606,None Found,"3+ years’ experience programming in C# with .NET framework
Experience developing enterprise web applications
Experience querying relational databases
Working knowledge or better of React and/or Redux a plus
Working knowledge or better of Azure a plus
Working knowledge or better of service-oriented architecture a plus
",None Found,"Participate in the requirements gathering, coding, testing, troubleshooting, and documenting of engineering software applications
Develop applications using React/Redux, C#, Azure, SQL
Demonstrate the ability to adapt and work with team members of various experience level
Work with stakeholders from around the business to assist with applications-related issues and support their needs
",None Found,None Found,"Job Overview
GREAT OPPORTUNITY for a software engineer to create real impact and work with a modern tech stack as a part of our organization. You’ll play a critical role on our applications team, helping to develop the systems that run our business. We’re rebuilding our company’s technology, working with React/Redux, C#, microservices, Azure, SQL Server, and more.
This role will participate in all aspects of the software development life-cycle, including gathering requirements, estimating, technical design, implementation, documentation, testing, deployment and support of Endurance’s applications. Working in a team environment, you will take direction from team leads on development activities.
If creating impact is something you look for in a job, definitely reach out to us! Your choice to work out of Northbrook or Chicago (just blocks from Metra train stations).
Responsibilities for Software Engineer
Participate in the requirements gathering, coding, testing, troubleshooting, and documenting of engineering software applications
Develop applications using React/Redux, C#, Azure, SQL
Demonstrate the ability to adapt and work with team members of various experience level
Work with stakeholders from around the business to assist with applications-related issues and support their needs

Qualifications for Data Engineer
3+ years’ experience programming in C# with .NET framework
Experience developing enterprise web applications
Experience querying relational databases
Working knowledge or better of React and/or Redux a plus
Working knowledge or better of Azure a plus
Working knowledge or better of service-oriented architecture a plus

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability status, veteran status, marital status, citizenship status, sexual orientation, gender identity or any other characteristic protected by law. Applicants must be US Citizen or Green card holder, no sponsorship available.

Seniority Level
Mid-level
Industry
Information Technology & Services
Computer Software
Employment Type
Full-time
Job Functions
Engineering
Information Technology"
62,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,"
Expertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role
",None Found,None Found,None Found,None Found,"Join SADA as a Data Engineer!

Your Mission

As a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.

You will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.

Pathway to Success

#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.

Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.

As you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.

Expectations

Required Travel - 30% travel to customer sites, conferences, and other related events
Customer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.
Training - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.

Job Requirements

Required Credentials:

Google Professional Data Engineer Certified

[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment

Required Qualifications:

Expertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.
Experience writing software in one or more languages such as Python, Java, Scala, or Go
Experience building production-grade data solutions (relational and NoSQL)
Experience with systems monitoring/alerting, capacity planning and performance tuning
Experience in technical consulting or other customer-facing role

Useful Qualifications:

Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)
Experience with IoT architectures and building real-time data streaming pipelines
Applied experience operationalizing machine learning models on large datasets
Knowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem

About SADA

Values: We built our core values
[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.

1. Make them rave
2. Be data driven
3. Be one step ahead
4. Be a change agent
5. Do the right thing

Work with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the
2018 Global Partner of the Year
[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded
Best Place to Work
[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!

Benefits : Unlimited PTO
[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,
professional development reimbursement program
[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.

Business Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud."
63,"Data Strategy Specialist - Business & Data Analysis, Cloud, AWS, Azure, Big Data","Chicago, IL",Chicago,IL,None Found,None Found,None Found, 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:,None Found,None Found,None Found,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The North America Data Strategy & Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.

 Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model

Required Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:
o Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling
o Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.
o Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel
 Able to travel up to 100% (Mon-Thu)

Optional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model
 Experience in compiling business cases and roadmaps for data, analytics and technology investments

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
64,Google Cloud Partner Architect,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,"
5+ years cloud technology experience with 2+ Google cloud required
Self-starter with in-depth hands-on work experience with large-scale implementations of Google Cloud Platform (GCP)
Knowledge of J2EE, .NET, and cloud technologies development languages and tools, to continue with development
Experience with distributed computing, cloud security, IaaS & PaaS architectures, knowledge of big data/analytics, CI/CD, DevOps, SRE and networking concepts
Experience helping development teams build and deploy microservices-based applications in the public cloud. Experience with Docker, Kubernetes and other Container Orchestration and API exposure platforms
Ability to work independently and make recommendations and decisions autonomously
Understanding and passion for human-centered design and the agile software development lifecycle including Continuous Integration & Continuous Deployment tools and processes
Extroverted and empathetic that thrives in fast-paced environments and has a passion for emerging technologies that meet business outcomes
Google certifications highly preferred in Google Professional Cloud Architect, Data Engineer, Cloud Developer
Up to 20% travel required
","Solstice is an innovation and emerging technology firm that helps Fortune 500 companies seize new opportunities through world-changing digital solutions. As strategists and consultants, we help organizations evolve their digital strategy to solve mission-critical problems. As designers and developers, we build incredible hardware and software solutions that transcend a standalone product and transform an organization's relationship with its customers. As instructors and coaches, we help companies transform from the inside out by adopting a high-speed culture of innovation. We're strategists, researchers, designers and engineers hell-bent on changing the way the world does business. We're headquartered in Chicago and have delivery offices in New York, London, and Buenos Aires.

We are looking for a technology-driven and detail-oriented Google Cloud Partner Architect to join our Partnerships team and continue Solstice's rapid growth. You'll be joining an arm of our thought leaders and consultants focused on expanding our partnership relationships with Google Cloud. You will operate in a fast-paced environment on a small, nimble team. Translation: you have an opportunity to make a big impact on Solstice and your career. In this role, you'll support critical partnership management, expansion of our GCP footprint and drive new innovative work across our capabilities.

What you'll be doing:

The primary technical architect for our Google Partnership team being able to deeply integrate with Google's technical staff and ensure the strength of our services with them
Work to create technical content and deliverables for the Google Partner team surrounding the priority GCP workloads that Solstice is working to build.
Be the glue between our Solstice's Google Partner Manager and Solstice's Engineering staff to provide support of existing GCP projects, management learning development, and certifications.
Provide pre-sales and new opportunity pursuit for all active opportunities that are Google identified leaders by providing subject matter expertise, architecture diagrams, develop proof of concepts and other strategic activities to win the business.
Create marketing content via blogs, meetup, podcasts and/or speaking engagements to raise our presence in the market as a thought leader on the GCP platform.
Work with our capability leadership teams to ensure the growth of that these skills are aligned along the GCP platform for their area of impact such as Cloud Native, Artificial Intelligence, Data Platforms, Cloud Security, IoT, Mobile, and Web Experiences.
As needed, work on specific client projects to bring subject matter expertise to the project team and conduct necessary engineering task in order to deliver the client outcomes.
Define the GCP Cloud Architecture for both hybrid and non-hybrid cloud solution. Be the go-to person for GCP Cloud Architecture. You should have a deep understanding of IaaS and PaaS services offered on cloud platforms and understand how to use them together to build complex solutions.

Job Requirements:

5+ years cloud technology experience with 2+ Google cloud required
Self-starter with in-depth hands-on work experience with large-scale implementations of Google Cloud Platform (GCP)
Knowledge of J2EE, .NET, and cloud technologies development languages and tools, to continue with development
Experience with distributed computing, cloud security, IaaS & PaaS architectures, knowledge of big data/analytics, CI/CD, DevOps, SRE and networking concepts
Experience helping development teams build and deploy microservices-based applications in the public cloud. Experience with Docker, Kubernetes and other Container Orchestration and API exposure platforms
Ability to work independently and make recommendations and decisions autonomously
Understanding and passion for human-centered design and the agile software development lifecycle including Continuous Integration & Continuous Deployment tools and processes
Extroverted and empathetic that thrives in fast-paced environments and has a passion for emerging technologies that meet business outcomes
Google certifications highly preferred in Google Professional Cloud Architect, Data Engineer, Cloud Developer
Up to 20% travel required

We welcome Solsties to show up as their full selves everyday. Because this is so important to us, Solstice is proud to be an equal opportunity employer."
65,"Data Science, Specialist","Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Please make sure to read the job posting in its entirety as it reflects both the University roles and responsibilities, followed by the specific description.
Department
66703 UChicago Professional Education
About the Unit
Job Family
Information Technology
Responsible for the design, implementation, and maintenance of new and existing applications, systems architecture, and network infrastructure. Ensures operation and security of all servers and networks. Configures, installs, maintains and upgrades applications and hardware for the organization's infrastructure and for end-user devices.
Career Track and Job Level
Business Systems Analysis
Acts as a liaison between the IT group and business units for the development and implementation of new systems and enhancement of existing systems Evaluates new applications and identifies systems requirements. Evaluates new IT developments and evolving business requirements, and recommends appropriate systems alternatives and/or enhancements to current systems.
P3: Requires in-depth knowledge and experience. Uses best practices and knowledge of internal or external University issues to improve products or services. Solves complex problems; takes a new perspective using existing solutions. Works independently, receives minimal guidance. Acts as a resource for colleagues with less experience.
Role Impact
Individual Contributor
Responsibilities
The job uses best practices and knowledge to develop new and existing systems. Solves problems and anticipates evolving business requirements to identify, design, and implement appropriate process improvements, systems alternatives, and/or enhancements to current systems.
1) Has a deep understanding of business process analyses, needs assessments, and preliminary cost/benefit analyses. Solves complex problems relating to computer equipment capacity, limitations, and operation time., 2) Utilizes thorough understanding of business systems and industry requirements to translate business and user needs into system requirement specifications., 3) Recommends process improvements in existing applications and revises existing system logic difficulties as necessary. Designs, develops, and implements new applications, systems architecture, network systems, and applications infrastructures., 4) Communicates and presents on system enhancements and/or alternatives to colleagues in IT management., 5) Performs other related work as needed.
Unit-Specific Responsibilities
1) Serve as the primary data engineer for a small unit (50+ staff with a 4-person data/IT team) with a high growth profile.
2) Design and build, standardized and secure ETL tools and APIs for critical data sets that must work for users of R, Python and Tableau; data are sourced from OBIEE, public data API's, Hadoop utilities, and other enterprise systems.
3) Collaborate with central IT and business intelligence units to leverage institutional economies of scale.
4) Work with analysts and end users to understand unit needs and produces best-practice documents for data extraction and use.
5) Advance institutional mission by supporting data-driven and policy-compliant decision making.
6) Evaluate SaaS licensing contracts.
Unit-Preferred Competencies
1) Proficiency in SQL.
2) Exposure to big data environments, particularly Hadoop.
3) Exposure to statistical languages, including either R or Python.
4) Experience with Java or JavaScript.
5) Create robust, high-quality, and tested data pipeline.
6) Background in understanding GDPR-compliant data storage practices.
7) Background in higher education or public sector.
8) Interest in small-team environments.
9) Strong initiative and a resourceful approach to problem solving and strategic thinking.
10) Strong interpersonal skills.
Education, Experience, and Certifications
Minimum requirements include a college or university degree in related field.
Minimum requirements include knowledge and skills developed through 5-7 years of work experience in a related job discipline.
Preferred Qualifications
Experience
1) Five years plus experience in a related field.
Education
1) Master's degree in computer science, information systems, data science, analytics, economics, statistics, or a closely related field.
Required Documents
1) Resume
2) Cover Letter
3) Reference Contact Information
NOTE: When applying, all required documents MUST be uploaded under the Resume/CV section of the application.
FLSA Status
Exempt
Pay Frequency
Monthly
Pay Grade
Depends on Qualifications
Scheduled Weekly Hours
37.5
Benefits Eligible
Yes
Drug Test Required
No
Health Screen Required
No
Motor Vehicle Record Inquiry Required
No
Posting Date
2019-08-29-07:00
Remove from Posting On or Before
2020-02-29-08:00"
66,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Avant is a high growth financial technology company, dedicated to lowering the barriers of borrowing for all consumers. Since 2012, Avant has secured more than $4 billion in funding and connected customers to over $5 billion through 800,000 transactions and counting. Through big data and machine learning, Avant’s mission is to provide better access to responsible financial products. Avant has been featured in The Wall Street Journal, The New York Times, TechCrunch, Fortune, Bloomberg, and has raised over $600 million of equity capital. Visit www.avant.com, for more information.

Are you passionate about process improvement and automation? Are you excited about systems that manage complex data pipelines? The Campaign Execution team in the Data Engineering organization at Avant is looking for a Data Engineer to build out tools to execute, manage, automate, and monitor these data pipelines, as well as their interfaces with our production systems. Join us and contribute to systems and tools that deliver high-quality data and make a huge impact on our decision-making processes!
What you do at Avant:
Develop, maintain, test, and document data transformation pipelines that power essential day-to-day business operations
Review code and provide feedback to fellow data engineers
Act as a liaison between the Data organization and stakeholders of our data pipelines
Be a part of a strong culture of ownership, pragmatism, and testing
Contribute to tooling as well as our technology architecture and direction
What you might own at Avant:
Data pipelines that drive a variety of campaign-driven decision-making at Avant
A software framework that enables the team to manage and execute on data pipelines at scale
Software and infrastructure that sends data to Avant’s production systems
Interfaces between the Campaign Execution systems and our data warehouses
Why you are a fit for Avant:
Have a bachelor’s degree in computer science, mathematics, physics, or other quantitative field
Experience programming in Python, and have the ability and desire to become an excellent software engineer
Experience using relational databases such as Oracle, PostgresSQL, SQL Server, Hive QL
Understand how to use object-oriented and functional programming to write production-quality code
Comfortable collaborating with both technical and non-technical colleagues
Familiar with big data technologies such as Hadoop and Spark
Familiar with cloud compute and storage solutions such as AWS EC2, S3, and Batch
Familiar with REST API’s and development frameworks such as Rails or Django
Why Avant is a fit for you:

At Avant, we believe our values make a difference:
We value, support, and help each other grow
We are committed to active inclusion and diversity
We are transparent and believe the best idea wins
We succeed when our customers succeed
We get sh!t done… responsibly
And we keep it fun!

We believe that ideas are more important than titles, everything is more fun together, everyone drives change, and everyone is an owner. While we believe the perks and benefits that we offer are terrific, nothing excites us more than having the ability to collaborate with intelligent, highly-motivated and talented people on challenging problems as we work to change the face of online lending."
67,Data Engineer (Operations),"Chicago, IL",Chicago,IL,None Found,None Found,"
Bachelor’s degree in computer science or another technology/business field of study
4+ years of T-SQL experience
4-5 years of progressive experience within application support/IT operations organizations
Experience in Microsoft .NET Technologies, C#, Windows Forms
Analytical thinker with a problem solving and an entrepreneurial mindset
Ability to communicate and work with both technical and non-technical audiences","
Strong attention to detail, ensuring processes are followed and root cause remediation is planned and executed for each issue, and that actions are fully documented.
Critical thinker with the vision to work both tactically and strategically.
Exceptional verbal and written communication skills, ability to modify communication style to match the appropriate level of the audience targeted, with strong understanding of the impact of a message on the organization or customer.","
Provide technical and process leadership support to management in driving issue resolution, escalations, SLA adherence, and support process improvements
Collaborate with other team members across IT, Operations, and business functions to troubleshoot and resolve support tickets
Intermediate level T-SQL experience in writing/maintaining stored procedures and complex queries
Reverse engineer application code and configurations and write custom/ad-hoc T-SQL queries to diagnose critical application issues reported by end users
Troubleshoot and provide necessary bug fixes related to client and user reported issues
Provide maintenance support to existing applications
Conduct presentations of the work where requested and participate in knowledge sharing sessions with others on the team",None Found,None Found,"It’s Time For A Change…
Your Future Evolves Here
Evolent Health has a bold mission to change the health of the nation by changing the way health care is delivered. Our pursuit of this mission is the driving force that brings us to work each day. We believe in embracing new ideas, challenging ourselves and failing forward. We respect and celebrate individual talents and team wins. We have fun while working hard and Evolenteers often make a difference in everything from scrubs to jeans.
Are we growing? Absolutely about 40% in year-over-year revenue growth in 2018 . Are we recognized? Definitely. We have been named one of “Becker’s 150 Great Places to Work in Healthcare” in 2016, 2017, 2018 and 2019, and One of the “50 Great Places to Work” in 2017 by Washingtonian. We recognize employees that live our values, give back to our communities each year, and are champions for bringing our whole selves to work each day. If you’re looking for a place where your work can be personally and professionally rewarding, don’t just join a company with a mission. Join a mission with a company behind it.
Our Technical team is looking for a Data Engineer to work on critical, customer impacting events, communication activity, and escalations while the incidents are occurring. Post incident resolution, the Senior Data Engineer assists with technology improvement activities to ensure continuous improvement of all services provided by the technical team. The Senior Data Engineer combines a passion for protecting customers with an ability to think quickly and take decisive action; beyond just putting out fires, working with other teams to ensure they don’t happen again.

Required Skills
Strong attention to detail, ensuring processes are followed and root cause remediation is planned and executed for each issue, and that actions are fully documented.
Critical thinker with the vision to work both tactically and strategically.
Exceptional verbal and written communication skills, ability to modify communication style to match the appropriate level of the audience targeted, with strong understanding of the impact of a message on the organization or customer.

Responsibilities
Provide technical and process leadership support to management in driving issue resolution, escalations, SLA adherence, and support process improvements
Collaborate with other team members across IT, Operations, and business functions to troubleshoot and resolve support tickets
Intermediate level T-SQL experience in writing/maintaining stored procedures and complex queries
Reverse engineer application code and configurations and write custom/ad-hoc T-SQL queries to diagnose critical application issues reported by end users
Troubleshoot and provide necessary bug fixes related to client and user reported issues
Provide maintenance support to existing applications
Conduct presentations of the work where requested and participate in knowledge sharing sessions with others on the team

Qualifications
Bachelor’s degree in computer science or another technology/business field of study
4+ years of T-SQL experience
4-5 years of progressive experience within application support/IT operations organizations
Experience in Microsoft .NET Technologies, C#, Windows Forms
Analytical thinker with a problem solving and an entrepreneurial mindset
Ability to communicate and work with both technical and non-technical audiences
Highly Desired Additional Skills
Healthcare industry experience (desirable)
Experience with JIRA Service Desk ticketing system/Confluence
Understanding of Healthcare related EDI ANSI X12, HL7, etc. data formats
Experience in performance tuning databases and SQL statements

Evolent Health is an equal opportunity employer and considers all qualified applicants equally without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin."
68,"Associate, Big Data Engineer","Chicago, IL 60654",Chicago,IL,60654,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description Summary
CCC is looking to hire a Big Data Engineering Associate to join our Enterprise Analytics team. The engineer will build platforms using Python, Spark, Hadoop (and more) which will provide insights to CCC's client within auto property damage and repair, medical claims and telematics IoT industries. Job Duties

Our data engineers use big data technology to create best-in-industry analytics capability. This position is an opportunity to use Python, Spark and Hadoop ecosystems to achieve micro-batch and streaming analytics for TB's worth of data. Qualifications

1.) 0-2 years of experience within Python, Spark, Hadoop and other ""Big Data"" technologies. Internship experience counts.
2.) Someone who has a strong understanding of the Python language and corresponding Python frameworks.
3.) A person who knows the difference between PySpark and Spark powered by Scala (and can prove it).
4.) Masters graduate preferred, but if you have the experience and projects to back it up, let's see it!
5.) Someone curious about how data can help solve challenging business problems.
6.) Someone who can add diversity to our already diverse company."
69,Sr. Data Scientist,"Rosemont, IL 60018",Rosemont,IL,60018,None Found,None Found,"A minimum of 6 years of relevant experience
BS or higher degree in Math, Statistics, Data Science, Computer Science, or a related field
A solid understanding of data science, analysis, and software engineering principles
Demonstrated experience building machine learning models
Demonstrated experience deploying, maintaining, versioning, and A/B testing machine learning models
Demonstrated experience working with AWS, including Lambda and infrastructure-as-code
Strong database skills, in at least one of AWS Redshift, Oracle, SQL Server, or MySql
Demonstrated expertise using SQL to write complex queries across large volumes of data
Demonstrated experience and skill developing and deploying full-stack solutions in Python
Experience with, and the ability to follow, standardized development practices and tools, including TFS/GIT, code standards, and process standards.
Experience with Octopus Deploy or other CI/CD tools or environments
Demonstrated proficiency writing unit tests using standard unit test frameworks
The ability to understand and create process documentation
Advanced knowledge of statistical techniques, concepts, methods, and approaches, and experience with their application
Proficiency and comfort using multivariate calculus and linear algebra
Demonstrated ability to prioritize and execute tasks
An enjoyment of technical challenges and eagerness to explore
A willingness to ask for help and ability to communicate what you need to do your best work
","Leverages machine learning and data mining techniques
Combines data extraction, preparation, and cleaning with model training and evaluation
Designs, develops, and implements programs, processes, and systems to generate actionable insights and drive business decisions
Develops and codes software programs, algorithms and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources
Interprets and communicates insights and findings from analysis and experiments for a multi-department audience
Supports organizational decisions with well-understood and actionable insights, models, and artifacts
Implements advanced statistical and predictive modeling techniques, focusing on clear, algorithmic methods, models, and visualizations
Accesses data from IMO’s internal databases and sources, including warehoused and pre-modeled data
Mentors junior data engineers
Stays current on data science and analytics tools, techniques, and practices
Works on an Agile (Scrum) team within IMO Software Engineering
Follows the Software Engineering department’s code and process standards
Writes clean, reliable, and testable code that supports rapid delivery via CI/CD and automated deployments
",None Found,None Found,"We’re looking to add a crucial piece to our amazing team at IMO with a Contract to hire/full time salaried benefited Data Engineer/Scientist in Rosemont, IL.
Applicants Only, No Recruiters.
Join our growing Data Engineering department as a Data Engineer 2 (Data Scientist) to help design, create, and support high quality solutions that support 85+% of US clinicians and build the application of Data Science within IMO.
Overview of Duties and Responsibilities
Leverages machine learning and data mining techniques
Combines data extraction, preparation, and cleaning with model training and evaluation
Designs, develops, and implements programs, processes, and systems to generate actionable insights and drive business decisions
Develops and codes software programs, algorithms and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources
Interprets and communicates insights and findings from analysis and experiments for a multi-department audience
Supports organizational decisions with well-understood and actionable insights, models, and artifacts
Implements advanced statistical and predictive modeling techniques, focusing on clear, algorithmic methods, models, and visualizations
Accesses data from IMO’s internal databases and sources, including warehoused and pre-modeled data
Mentors junior data engineers
Stays current on data science and analytics tools, techniques, and practices
Works on an Agile (Scrum) team within IMO Software Engineering
Follows the Software Engineering department’s code and process standards
Writes clean, reliable, and testable code that supports rapid delivery via CI/CD and automated deployments
Knowledge, Skills, and Attributes
Must have:
A minimum of 6 years of relevant experience
BS or higher degree in Math, Statistics, Data Science, Computer Science, or a related field
A solid understanding of data science, analysis, and software engineering principles
Demonstrated experience building machine learning models
Demonstrated experience deploying, maintaining, versioning, and A/B testing machine learning models
Demonstrated experience working with AWS, including Lambda and infrastructure-as-code
Strong database skills, in at least one of AWS Redshift, Oracle, SQL Server, or MySql
Demonstrated expertise using SQL to write complex queries across large volumes of data
Demonstrated experience and skill developing and deploying full-stack solutions in Python
Experience with, and the ability to follow, standardized development practices and tools, including TFS/GIT, code standards, and process standards.
Experience with Octopus Deploy or other CI/CD tools or environments
Demonstrated proficiency writing unit tests using standard unit test frameworks
The ability to understand and create process documentation
Advanced knowledge of statistical techniques, concepts, methods, and approaches, and experience with their application
Proficiency and comfort using multivariate calculus and linear algebra
Demonstrated ability to prioritize and execute tasks
An enjoyment of technical challenges and eagerness to explore
A willingness to ask for help and ability to communicate what you need to do your best work
Nice to have:
Experience with TensorFlow and TensorFlow Serving
Experience with Jupyter Notebooks
Experience with Terraform
Statistical modeling experience using R or Matlab
Experience using big data frameworks such as Hadoop MapReduce
Experience designing or maintaining a data warehouse or data marts
Experience with fact and dimension tables
Experience with C#
Experience with Looker or Qlikview
Our Culture Supports You To...
Take initiative by bringing and developing new ideas
Embrace failure as a crucial step toward success and an opportunity to learn
Pair program with fellow team members to develop shared patterns and receive/give mentoring in various languages and tools
Develop personally and professionally with regular training, workshops, conferences, collaborative initiatives, Hackathons, and R&D projects
Be valued for your new perspective or depth of experience
Our tech stack:
Application stack: .NET, C#, Angular, JavaScript, Python, IIS
Database: Oracle, SQL Server, MySQL, DynamoDB, Redshift
CI/CD: TFS, Octopus, Git, Terraform
Cloud: AWS, Azure, Serverless
Monitoring: New Relic, Cloudwatch, AppInsights, PRTG, Dynatrace
SCM: Puppet, PowerShell, Cloud Formation
At IMO, a core team of clinicians, software developers and engineers combine computer science and medical expertise to help patients and healthcare professionals’ access high quality health information quickly and easily to improve total patient health. IMO is a privately held multi-million-dollar corporation enjoying explosive growth. If a challenging, fast paced, energetic work environment appeals to your career interests, do come and join our fast-growing team!
IMO has a casual dress work environment, healthy living initiatives, bonus, Equity Participation Plan, and a strong benefits package including medical/dental /vision; 401K with a history of strong match; competitive PTO and much more. Downtown commuters will enjoy free shuttle service to IMO’s Rosemont door step from Metra Station.
Note: Employee may come in contact with ePHI as part of their job responsibilities and he/she would need to follow appropriate IMO policies and procedures applicable under HIPAA regulations."
70,Data Engineer,"Chicago, IL 60601",Chicago,IL,60601,None Found,None Found,None Found,"
Work as part of a team to design and develop cloud data solutions at local Chicago clients
Deliver on the technical scope of projects & demonstrate thought leadership at clients as well as internally at Slalom
Gather technical requirements, assess client capabilities and analyze findings to provide appropriate cloud solution recommendations and adoption strategy
Research, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems
Assist in designing multi-phased cloud data strategies, including designing multi-phased implementation roadmaps
Analyze, architect, design, and actively develop cloud data warehouse, data lakes, and other cloud-based data solutions
Design and develop scalable data ingestion frameworks to transform variety of datasets
Serve as a subject matter expert in a cloud platform for larger the Slalom practice and contribute back to community",None Found,None Found,"Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of what’s possible—together.

Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to over 8,000 employees. We were named one of Fortune’s 100 Best Companies to Work For in 2018 and are regularly recognized by our employees as a best place to work. You can find us in 32 cities across the U.S., U.K., and Canada.
Slalom is proud to be a Premier AWS Partner, Microsoft Gold Partner, Google Premier Partner, 5x Tableau Partner of the Year, and 2018 Snowflake Partner of the Year.

The Data & Analytics practice at Slalom is a full-service data practice with competencies across information strategy, modern data architecture, data visualization, and data science. We are seeking a Cloud Data Engineer to join our local Chicago consulting team who is passionate about developing innovative solutions to help organizations drive strategic business outcomes and enable data-driven insights. Slalom Cloud Data Engineers are cloud data platform engineering specialists responsible for client delivery, complex solutioning and knowledge management.

Responsibilities
Work as part of a team to design and develop cloud data solutions at local Chicago clients
Deliver on the technical scope of projects & demonstrate thought leadership at clients as well as internally at Slalom
Gather technical requirements, assess client capabilities and analyze findings to provide appropriate cloud solution recommendations and adoption strategy
Research, analyze, recommend and select technical approaches for solving challenging and complex development and integration problems
Assist in designing multi-phased cloud data strategies, including designing multi-phased implementation roadmaps
Analyze, architect, design, and actively develop cloud data warehouse, data lakes, and other cloud-based data solutions
Design and develop scalable data ingestion frameworks to transform variety of datasets
Serve as a subject matter expert in a cloud platform for larger the Slalom practice and contribute back to community

Experience
4+ years of data engineering and/or data warehousing experience
2+ years of deep experience building cloud data solutions (Azure, AWS, GCP, Snowflake)
Experience migrating from an on-prem data to cloud data platform.
Deep experience with designing and deploying end to end solutions with a cloud platform’s analytic services including storage, permissions, private cloud, database services, virtual machines, and parallel processing technologies.
Experience with big data application development and/or with cloud data warehousing (e.g. Hadoop, Spark, Redshift, Snowflake, Azure SQL DW, BigQuery)
Working knowledge of agile development including DevOps concepts
Experience with cloud SDKs and programmatic access services
Proficient in a relevant programming language for cloud platform e.g. Python/Java/C#/Unix
Proficient in SQL
Working experience with version control platforms e.g. Git
Strong communication skills

Preferred
One or more cloud certifications
Consulting experience
Expert programming skills in Python and a software development background
Experience writing “infrastructure as code” deployments e.g. ARM, CloudFormation, Terraform
Understanding of cloud strategies and best practices expanding into cloud networking, cloud security, encryption, private cloud configuration, and overall cloud governance approaches
Strong background in data warehousing concepts, ETL development, data modeling, metadata management, and data quality

Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."
71,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Us

Teza is a quantitative asset management firm that strives to develop innovative, high-Sharpe investment products for its clients. Originally founded in 2009 as a science and technology-driven global quantitative trading business, Teza derives its unique edge in asset management from its high-frequency trading past and science-based investment approaches. Under the leadership of CEO Misha Malyshev, Teza's innovative approaches to quantitative research and platform engineering distinguish us from other quant trading firms. We have successfully attracted and assembled a group of top talent, including widely recognized experts in quantitative trading. Teza has over 50 professionals worldwide with offices in Austin, Chicago, New York, and Shanghai.

About the Role

Teza Technologies is looking for a data engineer to join our core services technology team, this role can be based in Austin, TX, or Chicago, IL. Data drives systematic trading and is critical to all aspects of the firm's business. This is a hands-on position on a team of 2 data engineers with significant growth potential, as this team will grow rapidly over the next couple of years. The firm is looking for outstanding technical skills, strong attention to detail, and experience architecting and building data platforms.

Responsibilities


Timely onboarding of structured and alternative data vendors:
Communicate with quantitative researchers and other end-users to understand their requirements and potential future requests
Investigate vendor data thoroughly to become a subject matter expert on its characteristics and irregularities
Design and implement efficient reusable ETL processing components using cutting edge technologies, and write robust tests for on-going quality control
Build flexible APIs in iterations with end-users to ensure their needs are met
Enhance existing data infrastructure and services:
Optimize data IO and load balancing for grid computation
Automate data storage management

Requirements


Detail oriented
Strong programming skills, preference for Python, Java is a plus
Proficient with SQL
Experience with AWS, Hadoop, Spark is a plus

"
72,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description

Our Data Engineering team builds and maintains a secure, scalable, flexible and user-friendly analytics hub that allows us to make informed and data-driven decisions. They also construct and curate business-critical data sets that allow us to realize the value of all the data we collect.
A Data Engineer utilizes a multidisciplinary approach to providing ETL solutions for the business, combining technical, analytical, and domain knowledge. The perfect applicant for this role has strong development skills, experience transforming and profiling data to determine risks associated with proposed analytics solutions, a willingness to continually interface with analysts in order to determine an optimal approach, and an eagerness to explore data sources to understand the availability, utility, and integrity of our data.
What you'll own:
Data pipeline / ETL development:
Building and enhancing data curation pipelines using tools like SQL, Python, Glue, Spark and other AWS technologies
Focus on data curation on top of datalake data to produce trusted datasets for analytics teams
Data Curation:
Processing and cleansing data from a variety of sources to transform collected data into an accessible and curated state for Analysts and Data Scientists
Migrating self-serve data pipeline to centrally managed ETL pipelines
Advanced SQL development and performance tuning
Some exposure to Spark, Glue or other distributed processing frameworks helpful
Work with business data stewards & analytics team to research and identify data quality issues to be resolved in the curation process
Data Modeling:
Design and build master dimensions to support analytic data requirements
Replacing legacy data structures with new datasets sourced from streaming data feeds from the core product and other operational systems
Design, build and support pipelines to deliver business critical datasets
Resolve complex data design issues & provide optimal solutions that meet business requirements and benefit system performance
Query Engine Expertise & Performance Tuning:
Assist Analytics teams with tuning efforts
Curated dataset design for performance
Orchestration:
Management of job scheduling
Dependency management mapping and support
Documentation of issue resolution procedures
Data Access
Design and management of data access controls mapped to curated datasets
Leveraging devops best practices, such as IAC and CI/CD to build upon a scalable and extensible data environment

Experience you'll need:
Strong experience designing and building end-to-end data pipelines
Extensive SQL development experience
Knowledge of data management fundamentals and data storage principles
Data modeling:
Normalization
Dimensional/OLAP design and data warehousing
Master data management patterns
Modeling trade-offs impacting data management & processing/query performance
Knowledge of distributed systems as it pertains to data storage, data processing and querying
Extensive experience in ETL and DB performance tuning
Hands on experience with a scripting language (Python, bash, etc.)
Some experience with Hadoop, Spark, Kafka, Impala, or other big data technologies helpful

Familiarity with the technology stacks available for:
Metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Data management, data processing and curation:
Postgres, Hadoop, Hive, Impala, Presto, Spark, Glue, etc.
Experience in data modeling for batch processing and streaming data feeds; structured and unstructured data
Experience in data security / access management, data cataloging and overall data environment management

Experience with cloud services such as AWS and APIs helpful
You’d be a great fit if your current track record looks like this:
5+ years of progressive experience data engineering and data warehousing
Experience with a variety of data management platforms (e.g. RDBMS (Postgres), Hadoop (CDH, EMR))
Experience with high performance query engines (Hive, Impala, Presto, Athena, MPP engines like RedShift)
Strong capability to manipulate and analyze complex, high-volume data from a variety of sources
Effective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language
Ability to problem solve independently and prioritize work based on the anticipated business value

Qualifications

null

Additional Information

All your information will be kept confidential according to EEO guidelines."
73,Data Engineer Intern,"Chicago, IL 60606",Chicago,IL,60606,None Found,"
Actively pursuing a degree in Information Science & Technology, Computer Science, or related field.
Relevant Computer Science coursework, development bootcamp, or experience on any platform as well as a passion for coding",None Found,"
Helping transform data into usable information based on business needs and requirements
Working closely on a cross functional team including other engineers, analysts, and architects
Identifying and implementing automation opportunities that streamline and enhance the process
Interacting with customers and creating technology solutions to help their business succeed
Learning new technologies with an eagerness to learn innovative and unfamiliar languages quickly
Participating in information gathering sessions with the clients to understand data requirements and objectives
Designing reports, dashboards, and metrics to summarize information and leverage solutions",None Found,None Found,"Only accepting applicants local to the state of the posting.

Are you looking to gain real world experience in cutting-edge technology firm with an innovative culture?
Consider an internship and start building your career at Concurrency. Our competitive intern program is an exciting experience that offers interns like yourself exposure to the real, business world and a meaningful work experience. Interns are treated as valuable team members while learning from experienced professionals in a collaborative, team environment.
Concurrency is an award-winning Microsoft Gold Partner with over 30 years of helping customers achieve digital transformation. Concurrency helps clients find better ways to leverage technology to fulfill their strategies and ultimately improve their businesses.
Responsibilities
Helping transform data into usable information based on business needs and requirements
Working closely on a cross functional team including other engineers, analysts, and architects
Identifying and implementing automation opportunities that streamline and enhance the process
Interacting with customers and creating technology solutions to help their business succeed
Learning new technologies with an eagerness to learn innovative and unfamiliar languages quickly
Participating in information gathering sessions with the clients to understand data requirements and objectives
Designing reports, dashboards, and metrics to summarize information and leverage solutions
Qualifications
Actively pursuing a degree in Information Science & Technology, Computer Science, or related field.
Relevant Computer Science coursework, development bootcamp, or experience on any platform as well as a passion for coding
Preferred skills
Excellent analytical and problem-solving skills
Familiarity in data analysis code (SQL, SAS, Oracle, Tableau, PowerBI, Python)
Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makes Concurrency a great place to work. Concurrency full-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities and a comprehensive benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs, excellent training program and bonus opportunities."
74,Senior Data Engineer,"Chicago, IL 60654",Chicago,IL,60654,None Found,None Found,None Found,None Found,None Found,None Found,"Working for Trunk Club
----------------------

When you join Trunk Club, you join the Nordstrom family. Our fast-paced and entrepreneurial environment is paired with the strong history and experience of a retail legacy. We have access to some of the greatest minds in retail and technology and are constantly creating innovative strategies to develop the ultimate apparel solutions. We welcome your adaptability, your curiosity, and your passion to contribute to our unparalleled shopping experience!

Who we are
----------

At Trunk Club, we're building the future of retail, enabled through technology. On the Data Services team, we believe data has the power to drive the business forward, and we push every day to enable data-driven decisions across all aspects of the organization. We have hundreds of data sources, a robust enterprise data warehouse and use Tableau for visual analytics. Some of the tools and platforms we work with are Python, SQL, AWS, Spark, R and many other open-source technologies.

Responsibilities
----------------


Building end-to-end data integration and data warehousing solutions for analytics teams.
Designing and creating services and system architecture.
Building and maintaining enterprise data platforms.
Working daily with SQL and Python and using Spark for big data processing.
Using analytical and problem-solving skills to take complex business requests and transform them into clean, simple data solutions.
Brainstorming and contributing ideas to our technology, algorithms, and products.
Working with an agile mindset to deliver small projects quickly in order to provide value and gain feedback in a production environment.
Creating amazing user experiences for both internal and external customers.

Who you are
-----------


An analyst. You are able to take complex requests and transform them into clean, simple data solutions.
A teacher. You are a mentor to your team, and you are able to clearly communicate technical concepts across teams and departments.
A learner. You have an insatiable thirst for knowledge and greater understanding.
A pragmatist. Your goal is to create useful products, not build technology for technology's sake.

How we work
-----------


With collaboration. We frequently pair program. We work at pairing stations - two keyboards, two minds, one outcome.
With transparency. We work in an open team room; no cubicles or private offices. Communication is key to our process, and we don't want to hinder it with walls.
With agility. We don't believe in following a process for process's sake. We ship frequently and focus on delivering incremental value.
With open minds. We are committed to building a diverse team of people with unique perspectives. This encourages a healthy and inclusive environment that builds a more sustainable, successful company.
With pride. We value our people most of all. We invest in ourselves by applying our own strengths and interests to company needs.

A few of our perks
------------------


Lunch and learns
Annual stipend for continuous education
Tech all-hands lunches every other Friday
Hack days
Team outings
Nordstrom discount
Flexible work environment
Social environment with built-in bars

"
75,Data Engineer,"Chicago, IL 60616",Chicago,IL,60616,None Found,None Found,None Found,None Found,None Found,"
Bachelor's degree
Developing applications in a linux environment in at least one of the following languages: Python, Scala, Java
Experience with file formats JSON, CSV, XML
Experience maintaining shared code bases
Previous experience using APIs and web services: SOAP, REST
Deep understanding of relational and dimensional database designs
Experience designing and maintaining relational databases such as MySQL and columnar databases such as Redshift
Experience with integrating multiple data sources and managing large data structures (data warehouse, data lake) in cloud architectures (Azure, AWS)
Proven ability to communicate effectively with technical and non-technical stakeholders
Excellent written and verbal communication skills
Exceptional time management skills and ability to prioritize workflow
Full-time position, 40+ hour work week
Some weekend and holiday work may be required (depending on the project)
","Description:
U.S. Soccer Overview
We are U.S. Soccer and we are the future of sport in the United States. Our mission is to make soccer the preeminent sport in the United States. We embrace diversity, technology and global connections to drive the growth of our sport and by supporting members, impacting athletes, and serving fans. We seek motivated, passionate, skilled people who can think, create and work on a team.

U.S. Soccer is a growing company that looks for team members to grow with it. U.S. Soccer offers a comprehensive compensation package, casual work environment, an inclusive culture and an atmosphere for professional development.

Position Description
The Data Engineer will be responsible for assisting in managing the interchange of data between transactional systems and analytical applications.
The position will initially focus on unifying data from discrete sources into a data management platform but will also be responsible for long term strategic design and implementation of a suite of highly robust applications. The position focuses on programmatic solution building but also requires a strong understanding of sporting business, technology, and a passion for soccer.

Primary Responsibilities

Help manage U.S. Soccer’s data pipeline
Collaborate with the data engineering team to design, integrate and maintain schemas to meet the needs of various platforms
Focus on security and data protection
Employ cloud-based data storage best practices
Review new technologies and create best practices in enterprise data architecture and their suitability for the organization’s mission and operations
Collaborate with all U.S. Soccer departments and partner organizations to evolve datasets and create scalable solutions
Collaborate with the Technology team to facilitate long-term application and systems integration through centralized cloud-based services
Additional duties as assigned.

Requirements:
Minimum Qualifications

Bachelor's degree
Developing applications in a linux environment in at least one of the following languages: Python, Scala, Java
Experience with file formats JSON, CSV, XML
Experience maintaining shared code bases
Previous experience using APIs and web services: SOAP, REST
Deep understanding of relational and dimensional database designs
Experience designing and maintaining relational databases such as MySQL and columnar databases such as Redshift
Experience with integrating multiple data sources and managing large data structures (data warehouse, data lake) in cloud architectures (Azure, AWS)
Proven ability to communicate effectively with technical and non-technical stakeholders
Excellent written and verbal communication skills
Exceptional time management skills and ability to prioritize workflow
Full-time position, 40+ hour work week
Some weekend and holiday work may be required (depending on the project)

Desired Qualifications

Bachelor's degree in computer science, information science, engineering or a similar field
Graduate degree is a plus
Knowledge of BI, Analytics, AI, and machine learning
Experience creating data pipelines using Apache Spark
Passion for soccer and a solid understanding of the soccer landscape in the United States

Working at U.S. Soccer is a unique opportunity. Employees who work at U.S. Soccer have the following attributes:

Adopt a company centric approach—Serve the Athlete and the Fan
Embrace and see learning as a lifelong pursuit
Possess a growth mindset—keeps an open mind and seeks new challenges
Practice self-assessment and self-reflection
Open to criticism and does not make excuses
Possess a tireless work ethic
Wants to be part of a team that wins
Has the ability to be firm but fair
Communicate in a direct, open and honest manner
Build relationships through genuine interpersonal skills

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law."
76,Data Engineer - Senior Associate,"Chicago, IL 60606",Chicago,IL,60606,None Found,None Found,None Found,None Found,None Found,None Found,"Company Description
Home Partners of America (HPA) is a residential real estate investment company committed to providing responsible households with a New Path to Homeownership through its innovative Lease with a Right to Purchase Program.

Objective
The Data Engineer Senior Associate is expected to become a subject matter expert for the data analytics team on data storage and ETL. They will work to standardize and improve data collection and management procedures. They will be trusted to work on projects independently and in small teams. They will be trusted to manage stakeholder meetings and to successfully report back on follow up items to the rest of the Analytics team.

Projects will center around working with large datasets related to real estate, credit underwriting, consumer behavior, and property-level information. They will be responsible to oversee the successful implementation of ETL processes and the management of the Analytics data warehouse.

In addition to a strong work ethic and motivation to learn, the Data Engineer Senior Associate position requires the following skills: meticulous attention to detail, developing communication (both written and verbal) skills, a high level of organization, creativity, resourcefulness, and an ability to work independently and multi-task.

Responsibilities
â€¢ Create ETL processes and manage the data warehouse.
â€¢ Work closely with the Data Engineering team and other data users to own data integrity checks
â€¢ Notification plan to alert all parties when there is a data integrity issue
â€¢ Comfort working in a small team0 and start up environment
â€¢ Create and implement data backup procedures to enable uninterrupted access / ability to conduct analyses
â€¢ Develop and implement complex independent tests to validate the model implementation
â€¢ Develop and implement repeatable back testing suites for both new and existing models
â€¢ Assess the models by ensuring that the data used is valid
â€¢ Document testing and validation activities
â€¢ Communicate with business heads and other departments about issues and concerns
â€¢ Provide expert knowledge on recommendations throughout validation processes
â€¢ Create logical and innovative solutions to complex problems
â€¢ Knowledge of Python a must.

Requirements
â€¢ Undergraduate Degree or Masterâ€™s Degree in Computer Science, Engineering, or commensurate technical experience.
â€¢ 2-3 years of experience in a technical role.
â€¢ 1-2 years previous scripting experience (Python,etc.).
â€¢ Demonstrated ability to manage multiple projects simultaneously, work under pressure, work independently and meetdeadlines.
â€¢ Technical familiarity with analysis techniques such as machine learning and regression analysis.
â€¢ Previous experience with SQL; geospatial analysis a big plus
â€¢ Proficiency with AWS services is a plus.

Home Partners of America and Pathlight Property management is an Equal Opportunity Employer and prohibits discrimination and harassment of any kind. All employment decisions including the decision to hire, promote, discipline or discharge are based on business needs, job requirements and individual qualifications without regard to race, color, religion or belief, sex (including pregnancy, gender identity or sexual orientation), national origin, age, disability status, genetic information, veteran or any other status protected by the laws or regulations in the locations in which we operate."
77,Senior Big Data Engineer,"Chicago, IL 60604",Chicago,IL,60604,None Found,"
5+ years of experience in programming languages such as Java or Python.
3+ years of experience in big data engineering.
2+ years of experience as Spark Developer.
","
Ability to develop spark jobs to cleanse/enrich/process large amounts of data.
Ability to develop spark streaming jobs to read data from Kafka.
Experience with tuning spark jobs for efficient performance including execution time of the job, execution memory, etc.
Good understanding of various file formats and compression techniques.
Experience with source code management systems such as GIT and developing CI/CD pipelines with tools such as Jenkins for data.
Ability to understand deeply the entire architecture for a major part of the business and be able to articulate the scaling and reliability limits of that area; design, develop and debug at an enterprise level and design and estimate at a cross-project level.
Ability to mentor developers and lead projects of medium to high complexity.
Excellent communication and collaboration skills.
",None Found,None Found,None Found,"Cars.com is one of Chicago's original tech companies. Our online platform makes it easier for consumers to shop for, sell, and service their cars. With our expert content, mobile app features, millions of new and used vehicle listings, a comprehensive set of research tools and the largest database of consumer reviews in the industry, Cars.com offers innovative products to connect consumers with dealers across the country.

Data is the driver of our future at Cars. We're searching for highly collaborative, analytical, and innovative Engineers to build and scale our big data and ML platform. If you are passionate about using data to solve problems and build game changing products, we'd love to work with you.

About the Role:
Working within a dynamic and fast paced team environment, the Senior Engineer is responsible for the design, construction, and maintenance of mission-critical, highly visible big data and machine learning applications in direct support of Cars.com business objectives. Furthermore, this person is responsible for working with other engineers to develop the technical design by fully understanding the technical details, integration, and functions of multiple applications across their development team. The ideal candidate should have good mentoring and cross-functional skills.

About the Team:
The Big Data and Machine Learning team at Cars.com is responsible for building big data pipelines and deriving insights out of the data using advanced analytic techniques, streaming and machine learning at scale.

Qualifications:

5+ years of experience in programming languages such as Java or Python.
3+ years of experience in big data engineering.
2+ years of experience as Spark Developer.

Required Skills:

Ability to develop spark jobs to cleanse/enrich/process large amounts of data.
Ability to develop spark streaming jobs to read data from Kafka.
Experience with tuning spark jobs for efficient performance including execution time of the job, execution memory, etc.
Good understanding of various file formats and compression techniques.
Experience with source code management systems such as GIT and developing CI/CD pipelines with tools such as Jenkins for data.
Ability to understand deeply the entire architecture for a major part of the business and be able to articulate the scaling and reliability limits of that area; design, develop and debug at an enterprise level and design and estimate at a cross-project level.
Ability to mentor developers and lead projects of medium to high complexity.
Excellent communication and collaboration skills.

Bonus:

Experience developing Big Data applications in the cloud, especially AWS.
Experience tuning HIVE queries.
Experience in deploying ML models into production and integrating them into production applications for use.
Experience with Spark ML.
Experience with machine learning / deep learning using R, Python, Jupyter, Zeppelin, TensorFlow, etc.

"
78,Data Engineer,"Chicago, IL 60606",Chicago,IL,60606,None Found,None Found,None Found,None Found,None Found,None Found,"This position can be remote, but US based candidates only.
About Us:
Dealer Inspire (DI) is a leading disruptor in the automotive industry through our innovative culture, legendary service, and kick-ass website, technology, and marketing solutions. Our mission is to future-proof local dealerships by building the essential, mobile-first platform that makes automotive retail faster, easier, and smarter for both shoppers and dealers. Headquartered in Naperville, IL, our team of nearly 600 work friends are spread across the United States and Canada, pushing the boundaries and getting **** done every day, together.
DI offers an inclusive environment that celebrates collaboration and thinking differently to solve the challenges our clients face. Our shared success continues to lead to rapid growth and positive change, which opens up opportunities to advance your career to the next level by working with passionate, creative people across skill sets. If you want to be challenged, learn every day, and work as a team with some of the best in the industry, we want to meet you. Apply today!
Want to learn more about who we are? Check us out here!
Job Description:
Dealer Inspire is changing the way car dealerships do business through data. We are assembling a team of engineers and data scientists to help build the next generation distributed computing platform to support data driven analytics and predictive modeling.
We are looking for a Data Engineer to join the team and play a critical role in the design and implementing of sophisticated data pipelines and real time analytics streams that serve as the foundation of our data science platform. Candidates should have the following qualifications
Required Experience
2-5 years experience as a data engineer in a professional setting
Knowledge of the ETL process and patterns of periodic and real time data pipelines
Experience with data types and data transfer between platforms
Proficiency with Python and related libraries to support the ETL process
Working knowledge of SQL
Experience with linux based systems console (bash, etc.)
Knowledge of cloud based AWS resources such as EC2, S3, and RDS
Able to work closely with data scientists on the demand side
Able to work closely with domain experts and data source owners on the supply side
An ability to build a data pipeline monitoring system with robust, scalable dashboards and alerts for 24/7 operations.
Preferred Experience
College degree in a technical area (Computer Science, Information Technology, Mathematics or Statistics)
Experience with Apache Kafka, Spark, Ignite and/or other big data tools
Experience with Java Script, Node.js, PHP and other web technologies.
Working knowledge of Java or Scala
Familiarity with tools such as Packer, Terraform, and CloudFormation
What we are looking for in a candidate:
Experience with data engineering, Python and SQL
Willingness to learn new technologies and a whatever-it-takes attitude towards building the best possible data science platform
A person who loves data and all things data related, AKA a self described data geek
Enthusiasm and a ""get it done"" attitude!
Perks:
Health Insurance with BCBS, Delta Dental (Orthodontics coverage available), Eye Med Vision
401k plan with company match
Tuition Reimbursement
13 days paid time off, parental leave, and selected paid holidays
Life and Disability Insurance
Subsidized gym membership
Subsidized internet access for your home
Peer-to-Peer Bonus program
Work from home Fridays
Weekly in-office yoga classes
Fully stocked kitchen and refrigeratorNot a complete, detailed list. Benefits have terms and requirements before employees are eligible.
gMFr77HZuj"
79,Senior SQL Developer,"Chicago, IL 60602",Chicago,IL,60602,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for a Senior SQL Developer to join our team with at least 5 years of experience
Key Responsibilities:
Facilitate collaboration for development of long-term strategic plans of production databases in conjunction with data owners, business units, IT product owners
Participate in enterprise information architecture discussions around needs and alignment to business goals
Collaborate with Database Administrators and other staff to resolve data issues, performance issues and to ensure the highest possible degree of data integrity
Validate and fix information management issues related to data quality framework (completeness, accuracy, availability, timeliness, consistency, etc.)
Assist business units and project management in developing the budget projections based on short- and long-term goals and objectives
Develop and maintain data models and data flows that represent essential data consumed and produced
Develop and maintain documentation that maps data models to information systems and business applications
Be resourceful in the use of enterprise data, including guidance in understanding and exploitation of the underlying data and business models, and identifying optimal data sourcing and mapping
Write PL-SQL code objects, especially stored procedures, according to team standards
Experience with webservice client on Oracle databases
Requirements & Preferred Skills:
Bachelor s degree in a technical field or equivalent work experience
5+ years of experience as a Data Engineer
5+ years of strong working knowledge of Oracle database
In-depth understanding of database structure principles
Ability to write complex SQL queries for ETL or reporting
Familiarity with data visualization tools (e.g. Tableau, XLCubed, ESRI, Business Objects)
4+ years of experience with SQL and PL/SQL development
3+ years of experience on Oracle SQL/PL-SQL performance tuning
Clarity Partners, LLC is a privately held full-service management, technology, and trial consulting firm. We were founded in Chicago, IL in 2004 on the principle of providing big firm consulting best practices with the innovation, efficiency, and personal attention typically found in smaller firms. Inc. Magazine has named us one of the fastest-growing companies in America for the last five years.
Clarity Partners, LLC is an Equal Employment Opportunity Employer.
We believe in treating each employee and applicant for employment fairly and with dignity. We base our employment decisions on merit, experience, and potential, without regard to race, color, national origin, sex, sexual orientation, gender identity, marital status, age, religion, disability, veteran status, or any other characteristic prohibited by federal, state or local law.
w6veaYFs5g"
80,Data Engineer,"Chicago, IL 60654",Chicago,IL,60654,None Found,None Found,"Intermediate to advanced level in Structured Query Language (SQL)
",None Found,"Bachelor Level preparation in computer, mathematical, information sciences or equivalent training –","Flexibility and ability to work with individuals of diverse backgrounds and educational levels
","Job Title: Data Engineer Reports To:
Deputy Director/Chief Informatics
Officer
Location: Chicago Travel Required:
Level/Salary Range: DOE Position Type: Exempt
HR Contact: Claudria Hurt Date Posted: 07/31/2019
External Posting URL:
Applications Accepted By:
Email: careers@alliancechicago.org or Fax: 312.274.0069
Subject Line: Data Engineer
Job Description:
Position Overview:
Responsible for building out and migrating the AllianceChicago data warehouse to the Health Catalyst platform,
and then expanding and maintaining the warehouse. This position has primary responsibility for the extraction of
health information from diverse clinical and business sources and its transformation into objects in a standardized
data model. These objects will be the primary source for quality and performance reporting activities for
AllianceChicago, its members, stakeholders and user community or primary care delivery sites. The platform also
supports research and grant reporting needs, queries for public health surveillance, and other deliverables. The
data extraction work involves design, construction, and maintenance of modules using the Health Catalyst toolset.

The work provides the data for analysists and presentation specialists working in Power BI, SQL, SSRS, Excel and
Health Catalyst presentation tools, and benefits from familiarity with these technologies. Overall the position
contributes to the design, build, testing, and maintenance of the AllianceChicago’s data warehouse.
Essential Duties:
Overseeing the initial load to Health Catalyst data lake, a process that will be staffed by Health Catalyst
Working with AllianceChicago subject matter experts to design the rules to transform data lake data into
standardized models of health data (Health Catalysts DOS environment) including encounters, problems,
screenings, interventions, tests, and medications
Acquiring from Health Catalyst the knowledge to implement the design of the data model
Manage the transition and go-live from the current data warehouse environment based on Microsoft
APS/PDW and SSIS extraction to the Health Catalyst environment
Maintenance of the extract and transformation process
Supports Alliance SQL query analysts, report writers and Alliance partners who perform data analysis
from the DOS model or the data lake
Works with query writers and report designers to resolves data issues with current reports.
Is a member of the Alliance Data Warehouse team, and works collaboratively with its members and other
stakeholders
Oversees bringing new source systems into the Catalyst’s Data Operating System
Ability to dig into the data and understand business logic within the source system data
Perform data validation tests to ensure extractions and transformations are true to the source
Required Skills:
Intermediate to advanced level in Structured Query Language (SQL)
Experience working with EMR\EHR systems and an understanding of the healthcare clinical domain
Exposure to Extract, Transform and Load (ETL) concepts and processes
Excellent analytical and troubleshooting skills
Working knowledge of database principles, processes, technologies and tools
Other Requirements:
Flexibility and ability to work with individuals of diverse backgrounds and educational levels
Ability to function in a collaborative and collegial environment as a team player.
Ability to coordinate and communicate effectively with other team members
Ability to generate trust and build alliances with coworkers
Self-motivated; comfortable working under general direction
Strong sense of customer service to consistently and effectively address client needs
Ability to work in variety of settings
Detail oriented; highly organized; ability to prioritize and set expectations
Strong technical writing skills and written communications skills
Strong Analytical skills
Ability to work independently to organize work in a manner that ensures accuracy and efficiency
Familiarity with agile development process


Education/Training/Expertise:

Bachelor Level preparation in computer, mathematical, information sciences or equivalent training –
Master Level Preferred
Experience/Years:

At least three years of experience with Microsoft SQL Server 2008 or higher including SSIS and SSAS,
and familiarity with the MS BI stack. Familiarity with other data, analytic and reporting tools
5-8 years of experience in computer science or information science related field
Experience with Relational Databases and data mining

Working Conditions:
General office setting, extensive telephone and desk work at computer terminal
May be required to lift, carry, bend, reach and stand with parcels up to 25 lbs.
Will work in a close multidisciplinary team environment
May interface with clients in various settings and may be working during on-site visits in clinical
environments where medical equipment, chemicals and where communicable diseases and certain
pathogens are present.

ORGANIZATIONAL OVERVIEW:
Founded by four partner Community Health Centers in 1997, AllianceChicago’s three core areas of focus are
Health Care Collaboration, Health Information Technology, and Health Research & Education. AllianceChicago
supports the use of HIT to improve quality, efficiency, and access to services in a national network of community
Safety Net health care organizations. The mission of AllianceChicago is to improve personal, community, and
public health through innovative collaboration.

ADA Statement: The Americans with Disabilities Act prohibits discrimination and ensures equal opportunity for
persons with disabilities in employment, state and local government services, public accommodations, commercial
facilities, and transportation. It also mandates the establishment of TDD/telephone relay services.

EEO Statement: AllianceChicago believes that all applicants and employees are entitled to equal employment
opportunities and maintains a policy of non‐discrimination with respect to religion, color, sex, sexual orientation,
national origin, age, veteran status, marital status, physical or mental disability, or any other legally protected class
in accordance with applicable law, except where a bona fide occupational qualification exists. AllianceChicago will
comply with all phases of employment including, but not limited to, hiring practices, transfers, promotions, benefits,
discipline, and discharge.


Disclaimer: The above statements are intended to describe the general nature and level of work being performed
by employees assigned to this position. They are not intended to be construed as an exhaustive list of all
responsibilities, duties, and skills required of personnel as qualified."
81,Data Engineer,"Chicago, IL 60654",Chicago,IL,60654,None Found,"Experience with JavaScript/Java/ Python or Jitterbit and other developer languages.
Experience with Data Analytics.
Experience with Web Services and APIs.
Experience in the development of batch and real-time data integration and data consolidation processes.
Experience with machine learning, AI, and data lakes.
Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views.
Strong analytical skills with ability for problem-solving.
Understands the importance of data provenance and the ability to demonstrate it to clients.
Detail oriented, organized, self-motivated.

",None Found,"Develop strategy for new multi-platform data integration and analytics.
Develop strategy for new multi-platform-sourced data lake.
Contribute to API strategy to facilitate application connectivity and analytics.
Contribute to the maintenance and evolution of best practices.
Contribute to process documentation.
Perform multiple proofs of concept (POCs).
Contribute to implementation plan for decided-upon solution(s).

",None Found,None Found,"Riskonnect is the leading integrated risk management software solution provider that empowers organizations to anticipate, manage and respond in real-time to strategic and operational risks across the extended enterprise. Riskonnect is the only provider ranked in the leadership and visionary quadrants by world renowned industry analysts - Gartner, Forrester and Advisen RMIS Review. We employ more than 500 risk professionals in the Americas, EMEA and Asia Pacific and serve over 900 customers across 6 continents. The combination of innovative risk technology, a customer success mindset, and employee-first belief makes Riskonnect a sought after place to work.
Responsibilities:
Develop strategy for new multi-platform data integration and analytics.
Develop strategy for new multi-platform-sourced data lake.
Contribute to API strategy to facilitate application connectivity and analytics.
Contribute to the maintenance and evolution of best practices.
Contribute to process documentation.
Perform multiple proofs of concept (POCs).
Contribute to implementation plan for decided-upon solution(s).

Required Qualifications:
Experience with JavaScript/Java/ Python or Jitterbit and other developer languages.
Experience with Data Analytics.
Experience with Web Services and APIs.
Experience in the development of batch and real-time data integration and data consolidation processes.
Experience with machine learning, AI, and data lakes.
Proficiency in TSQL/PLSQL query-writing, stored procedure development, and views.
Strong analytical skills with ability for problem-solving.
Understands the importance of data provenance and the ability to demonstrate it to clients.
Detail oriented, organized, self-motivated.

Preferred Qualifications:
Experience with Salesforce.
Experience in the Risk Management, Healthcare, Financial, and/or Insurance industries is recommended.
Experience with Financial data sets, involving financial validation."
82,Data Engineer,"Chicago, IL 60601",Chicago,IL,60601,None Found,None Found,None Found,None Found,None Found,None Found,"Cooler Screens is backed and led by some of the most prominent Chicago & Silicon Valley leaders, advisers, and investors and has developed a digital solution to create and transform a multi-billion dollar industry and positively affect the buying experiences of consumers in the US and beyond. If you're looking to get on the ground floor of the next digital revolution and Chicago tech success story – this opportunity may be for you.
As the Data Engineer at Cooler Screens, you will design and maintain an enterprise data warehouse platform architecture that structures data for analytical processes to support BI and data integration management, which will be foundational in building out our data processing pipeline. You will be responsible for constructing how we identify, absorb, store, and query data across the Cooler Screens platform that scales to the rising growth of the company.
Who You Are:
You're excited to contribute to building a robust scalable and robust data platform and shaping an early stage startup
You're passionate about building scalable data processing systems and turning data into insights
You know how to develop solutions to transform and optimize data and have expertise in solutions to build robust data pipelines and repositories
You're comfortable working with a variety of BI tools and helping users to develop solutions to gain key data insights
You're a critical thinker and enjoy measuring, analyzing, and solving complex problems
You're experienced and comfortable in managing data from a variety of sources and formats and using best practices to ensure consistency and quality
You thrive working both independently and in a team environment
What You Need to Succeed
You have 4+ years of data engineering development experience
You have a deep understanding of the data architecture and relevant technologies
You have hands-on experience with Big Data technologies such as Spark and Hadoop
You have an advanced understanding of cloud data warehousing solutions
You have extensive experience with database modeling and design
About Cooler Screens
We are transforming retail cooler surfaces into IoT-enabled screens. Our media platform reimagines the brick-and-mortar shopping experience for consumers in the cooler and freezer aisle while delivering new marketing opportunities and smart merchandising for brands and retailers.
We have an excellent benefits package that includes medical, dental, vision, 401(k), life insurance, paid time off, and many other perks. Come join our fast-growing team at our headquarters in the heart of Chicago.
We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, native origin, sexual orientation, age, citizenship, marital status, disability, gender identity or veteran status.

Tpgl3Av2Le"
83,Data Engineer (Mid - Sr.),"Chicago, IL 60614",Chicago,IL,60614,None Found,None Found,None Found,None Found,None Found,None Found,"The Opportunity:
As a Data Engineer, you will:
Have a view of Vivid Seats data architecture and understand how a specific system works with the application
Create data models or designs while thinking of the requirements and get buy in from stakeholder
Interact with a variety of other engineering teams within the company
Guide junior data engineers
Stay up to date on new and emerging technologies, planning accordingly to incorporate valuable concepts to enhance our data schema and capabilities.
As a Data Engineer focusing on databases, you will:
Build databases and tooling around data stores for use by our production systems.
Model and assist in designing our transactional data stores while partnering with our engineering teams.
Design the life cycle of the data transformation working taking into account needs from our BI team, data governance, data quality and security. This approach will include batch processing and streaming of data feeds.
To be successful, you’ll need:
Experience in either modeling transaction or data warehousing with an interest in learning both methodologies
Expert knowledge of a relational database platform such as MySQL, Postgres, Oracle or SQL Server
Familiarity with data migration tools such as Liquibase or Flyway
Experience with one of the newer data warehouse technologies such as Redshift, Big Query or Snowflake
Cloud experience with either GCP, AWS or Azure and experience running data platforms within a cloud environment
ETL Pipeline and tooling experience
A willingness to being in an on-call rotation
Additional Experience of Interest:
Coding and scripting experience using Java, Python, or Bash
Experience with Configuration as Code tools such as Ansible, Terraform etc.
Experience with continuous integration, testing, and deployment using tools such as Git or Jenkins
What We Offer:
Vivid Seats is the largest independent online ticket marketplace, sending tens of millions of fans to live events. Experiences Matter- which is why we continue to grow year over year. Working at Vivid Seats provides an opportunity to scale our best in class platform, allowing our fans to sit closer and see more.
At Vivid Seats, you will have the opportunity to work with the flexibility and speed of a startup; while operating at massive, profitable scale. We keep our teams lean, allowing each and every employee direct accountability to creating a positive ticket buying experience. We are relentless and move quickly to release new features and content to our applications. Good ideas are heard and implemented, and hard work rewarded. Being a part of our team means having the ability to drive impact and own the innovation that connects our tens of millions of unique monthly users to the memorable experiences that only live events create.

We are passionate about creating memorable experiences for our fans and the best in class experience for our employees. Vivid Seats offers competitive compensation levels, individual and team-based bonus opportunities, generous benefits package and Flex PTO policy plus a variety of workplace perks. The most exciting one: We offer our employees $100 worth of credits each month to spend on Vivid Seats tickets along with promotional discounts. At the heart of it, we are all fans of great live events. We want to help you get there more often.

Location:

111 N Canal Suite #800
Chicago, IL 60606



#LI-TA1"
84,Data Engineering Manager,"Chicago, IL 60606",Chicago,IL,60606,None Found,None Found,None Found,None Found,None Found,None Found,"Company Description
Home Partners of America (HPA) is a residential real estate investment company committed to providing responsible households with a New Path to Homeownership through its innovative Lease with a Right to Purchase Program.

Objective
The Data Engineering Manager position will be expected to manage the day to day activities of 2-5 data engineers, providing both technical and nontechnical feedback. They will be expected to communicate priorities to senior leadership both within and external to the Analytics team. They will conduct one on ones with those team members and ensure that the team is meeting deadlines.

Projects will center around working with large datasets related to real estate, credit underwriting, consumer behavior, and property-level information. They will be responsible to oversee the successful implementation of ETL processes and the management of the Analytics data warehouse.

In addition to a strong work ethic and motivation to learn, the Data Engineer Manager position requires the following skills: meticulous attention to detail, developing communication (both written and verbal) skills, a high level of organization, creativity, resourcefulness, and an ability to work independently and multi-task.

Responsibilities
â€¢ Lead and educate other data engineers to remove obstacles and to assist in managing the day to day.
â€¢ Manage scrum boards and make sure the team is meeting deadlines.
â€¢ Give feedback and design ETL process on third party data sources.
â€¢ Notification plan to alert all parties when there is a data integrity issue
â€¢ Create and implement data backup procedures to enable uninterrupted access / ability to conduct analyses
â€¢ Develop and implement complex independent tests to validate the model implementation
â€¢ Develop and implement repeatable back testing suites for both new and existing models
â€¢ Assess the models by ensuring that the data used is valid
â€¢ Document testing and validation activities
â€¢ Communicate with business heads and other departments about issues and concerns
â€¢ Provide expert knowledge on recommendations throughout validation processes
â€¢ Create logical and innovative solutions to complex problems
â€¢ Knowledge of Python a must.

Requirements
â€¢ Undergraduate Degree or Masterâ€™s Degree in Computer Science, Engineering, or commensurate technical experience.
â€¢ 4-5 years of experience in a technical role, management experience is a plus.

â€¢ 1-2 years previous scripting experience (Python,etc.).
â€¢ Demonstrated ability to manage multiple projects simultaneously, work under pressure, work independently and meetdeadlines.
â€¢ Technical familiarity with analysis techniques such as machine learning and regression analysis.
â€¢ Previous experience with SQL and geospatial analysis a big plus
â€¢ Proficiency with AWS services is a plus.

Home Partners of America and Pathlight Property management is an Equal Opportunity Employer and prohibits discrimination and harassment of any kind. All employment decisions including the decision to hire, promote, discipline or discharge are based on business needs, job requirements and individual qualifications without regard to race, color, religion or belief, sex (including pregnancy, gender identity or sexual orientation), national origin, age, disability status, genetic information, veteran or any other status protected by the laws or regulations in the locations in which we operate."
85,Data Engineer - Hux,"Chicago, IL 60606",Chicago,IL,60606,None Found,"
4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
1+ years of experience on distributed, high throughput and low latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.",None Found,None Found,None Found,None Found,"Hux Data Engineer
Locations: New York, NY – Greensboro, NC - Chicago, IL – Raleigh, Durham, Chapel-Hill, NC - Denver, CO
What is Hux? Hux is the Human Experience Platform by Deloitte Digital.
In today’s world, customers expect companies to know who they are and what they want. Customers want to have products, services or experiences that best suit their needs delivered to them seamlessly across physical and digital channels.
Customers are human first: driven by dynamic wants, needs, and desires. The ability for brands to make personal, meaningful connections on a human level has never been greater and Hux by Deloitte Digital delivers on those experiences in a way that allows companies to own the customer journey end to end. We help companies connect key data sources to understand what matters most to people; connect to advanced technologies like AI and machine learning to sense and respond to those needs at scale; and connect their systems to unlock insights, create collaboration and drive acquisition, engagement and loyalty. Most importantly, we empower companies to connect with customers in personal, meaningful ways that respect them as people, not just customers.
Hux by Deloitte Digital gives companies the ability to build and leverage the connections – between people, systems, data and technologies – so they can deliver personalized, contextual experiences to customers at scale.

Work you’ll do
As a Hux Data Engineer, you’ll design, implement, and maintain a full suite of real-time and batch jobs that fuels our cutting edge AI to provide real-time marketing intelligence to our existing clients.
You’ll develop, test and deliver production grade code to help our clients solve their marketing challenges using cutting-edge big-data tools. You’ll also ensure data integrity, resolve production issues, and assist in the support and maintenance of our overall Platform.
As you grow your capabilities and learn how to build a platform that can ingest, load and process billions of data points, you’ll enjoy new challenges and opportunities to showcase your development skills by joining project teams to build innovative new-client platforms and execute high-value strategic development projects with high visibility.
Your responsibilities will include:
Design, construct, install, test and maintain highly scalable data pipelines with state-of-the-art monitoring and logging practices.
Bring together large, complex and sparse data sets to meet functional and non-functional business requirements.
Design and implements data tools for analytics and data scientist team members to help them in building, optimizing and tuning our product.
Integrate new data management technologies and software engineering tools into existing structures.
Help build high-performance algorithms, prototypes, predictive models and proof of concepts.
Use a variety of languages, tools and frameworks to marry data and systems together.
Recommend ways to improve data reliability, efficiency and quality.
Collaborate with Data Scientists, DevOps and Project Managers on meeting project goals.
Tackle challenges and solve complex problems on a daily basis.
Qualifications
Required:
4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
1+ years of experience on distributed, high throughput and low latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.
Preferred:
Producing high-quality code in Python.
Passionate about testing, and with extensive experience in Agile teams using SCRUM you consider automated build and test to be the norm.
Proven ability to communicate in both verbal and writing in a high performance, collaborative environment.
Follows data development best practices, and enjoy helping others learn to do the same.
An independent thinker who considers the operating context of what he/she is developing.
Believes that the best data pipelines run unattended for weeks and months on end.
Familiar with version control, you believe that code reviews help to catch bugs, improves code base and spread knowledge.
Helpful, but not required:
Knowledge in:
Experience with large consumer data sets used in performance marketing is a major advantage.
Familiarity with machine learning libraries is a plus.
Well-versed in (or contributes to) data-centric open source projects.
Reads Hacker News, blogs, or stays on top of emerging tools in some other way
Data visualization
Industry-specific marketing data
Technologies of Interest:
Languages/Libraries – Python, Java, Scala, Spark, Kafka, Hadoop, HDFS, Parquet.
Cloud – AWS, Azure, Google
The team
Advertising, Marketing & Commerce
Our Advertising, Marketing & Commerce team focuses on delivering marketing and growth objectives aligned with our clients’ brand values for measurable business growth. We do this by creating content, communications, and experiences that engage and inspire their customers to act. We implement and operate the technology platforms that enable personalized content, commerce and marketing user-centric experiences. In doing so, we transform our clients’ marketing and engagement operations into modern, data-driven, creatively focused organizations. Our team brings deep experience in creative and digital marketing capabilities, many from our Digital Studios.

We serve our clients through the following types of work:Cross-channel customer engagement strategy, design and development(web, mobile, social, physical)eCommerce strategy, implementation and operationsMarketing Content and digital asset management solutionsMarketing Technology and Advertising Technology solutionsMarketing analytics implementation and operationsAdvertising campaign ideation, development and executionAcquisition and engagement campaign ideation, development and executionAgile based, design-thinking, user-centric, empirical projects that accelerate results

How you’ll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.
Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitte’s culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.
Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.
Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.
kwhux"
86,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,"
C# programming and .NET platform experience
Azure Data Architecture
Experience with data lakes, MS SQL, Data Bricks
NoSQL environments such as CosmosDB
Engineering and/or software development experience and demonstrable architecture experience at enterprise scale
History of working successfully with cross-functional engineering teams
Structured, Unstructured, Semi-Structured Data techniques and processes
Working knowledge of open source Apache products, such as Hadoop",None Found,"Profile and analyze data in designing scalable solutions
Develop highly scalable and extensible Big Data platforms which enable collection, storage, modeling, and analysis of massive data sets including those from streaming data
Leverage technologies such as CosmosDB and Gremlin API / Graph Model to host high speed applications at global scale.
Analyze, develop, and maintain data pipelines from internal and external sources
Ability to adapt to new technologies and learn quickly",None Found,None Found,"Overview
UL is seeking a Data Engineer to join the iON Compliance team in Chicago, IL.

iON Compliance is a big data platform combining data ingest, analytics, machine learning and elements of artificial intelligence within a data lake. iON is now looking to add a Data Engineer to our team to accelerate the growth of the most innovative UL digital technology investment in its history. Our goal is to enable the UL client base to effectively apply the recent developments in the digital revolution, by giving them the ability to truly leverage their own corpus of data with internal core competences and win in a rapidly evolving digital eco-system.

iON aims to disseminate Artificial Intelligence & Machine Learning based cutting edge techniques and solutions to solve challenging and complex industrial problems. Joining the iON UL team gives you the opportunity to:
Work on disruptive products that are still in its early stages
Solve challenging problems that will revolutionize text processing and text mining
Work for a company that’s a recognized leader in data engineering and processing
Be involved in the fast growing, big data space
We are looking for data engineers with expertise and passion for building large analytical platforms systems.
Responsibilities
Profile and analyze data in designing scalable solutions
Develop highly scalable and extensible Big Data platforms which enable collection, storage, modeling, and analysis of massive data sets including those from streaming data
Leverage technologies such as CosmosDB and Gremlin API / Graph Model to host high speed applications at global scale.
Analyze, develop, and maintain data pipelines from internal and external sources
Ability to adapt to new technologies and learn quickly
Qualifications
Skills & Qualifications:
C# programming and .NET platform experience
Azure Data Architecture
Experience with data lakes, MS SQL, Data Bricks
NoSQL environments such as CosmosDB
Engineering and/or software development experience and demonstrable architecture experience at enterprise scale
History of working successfully with cross-functional engineering teams
Structured, Unstructured, Semi-Structured Data techniques and processes
Working knowledge of open source Apache products, such as Hadoop
Optional Additional Experience:
Big Data platforms e.g. Azure DW, SQL PDW, Cloudera, Hortonworks
Visualization: Qlikview, Tableau, D3.js
ETL tools such as Microsoft SSIS, Azure Data Factory, Pentaho PDI or Talend or Informatica
Continuous delivery and deployment using Agile Methodologies.
Data Warehouse and DataMart design and implementation
NoSQL environments such as MongoDB, Cassandra
Data modeling of relational and dimensional databases
Architect/design data management processes and capabilities (e.g governance, archiving, metadata, master data mgmt., data modeling, data quality)
A nice to have would be knowledge of ML / AI environments and challenges – particularly as it relates to data management
Experience & Education Requirements:
Bachelor’s Degree in Computer Science, Engineering, Mathematics, Statistics or related field
3+ years’ experience with data and cloud technologies"
87,Junior Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Your Future Evolves Here
Evolent Health has a bold mission to change the health of the nation by changing the way health care is delivered. Our pursuit of this mission is the driving force that brings us to work each day. We believe in embracing new ideas, challenging ourselves and failing forward. We respect and celebrate individual talents and team wins. We have fun while working hard and Evolenteers often make a difference in everything from scrubs to jeans.
Are we growing? Absolutely—56.7% in year-over-year revenue growth in 2016. Are we recognized? Definitely. We have been named one of “Becker’s 150 Great Places to Work in Healthcare” in 2016 and 2017, and one of the “50 Great Places to Work” in 2017 by Washingtonian, and our CEO was number one on Glassdoor’s 2015 Highest-Rated CEOs for Small and Medium Companies. If you’re looking for a place where your work can be personally and professionally rewarding, don’t just join a company with a mission. Join a mission with a company behind it.

We are looking for a Data Engineer in our Payor Data Services department. This position involves the programming and analysis of healthcare data with an emphasis on payer data, coding and data analytics.
What you'll be doing:
Utilize SQL programs to build metadata for various data feeds
Develop SAS programs (once trained) to integrate and analyze payer data from multiple sources
Load and synthesize healthcare data from multiple sources
Review data requirements/design and implement logic to achieve data needs
Implement and develop data quality control protocols and monitor their impact
Assist in designing, programming and standardizing processes and reports
The Experience you'll need (must haves):
Competency in use of SQL language and scripting to load SQL Server environments OR Familiarity with SAS and a desire to expand SAS programming knowledge is a plus
Competency in data manipulation and analysis: accessing raw data in varied formats with different methods and analyzing and processing data
Must be analytical, detail oriented, and possess desire to advance and grow personally and professionally
Ability to multi-task and manage multiple projects with varying timelines
Must have a passion for data and healthcare
BS, BA, or Masters in Computer Science, Data Analytics, Informatics, or a comparable program with a quantitative emphasis.
4+ years of SAS and SQL programming experience, including SAS macros, PROC SQL, and/or Enterprise Guide
Finishing Touches (preferred)
Experience in healthcare

Evolent Health is an equal opportunity employer and considers all qualified applicants equally without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin."
88,Data Engineer,"Chicago, IL 60606",Chicago,IL,60606,None Found,None Found,None Found,None Found,None Found,None Found,"Job Overview
We are looking for a Data Engineer to join our growing team of analytics experts, where you will have the opportunity to author and manage data pipelines from ingest to insights and all the plumbing in between. The Data Engineer will support our data scientists with both existing and net new projects, ensuring that data delivery is consistent and optimized. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets to meet functional and non-functional business requirements.
Author the pipeline code required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Work with Data Scientists and Systems Engineers to design data delivery architecture.
Work with stakeholders including the Executive and Product teams to assist with data-related technical issues and support their data insight needs.
Create data tools for team members that assist them in building and optimizing our products.
Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. PostgreSQL administration familiarity a plus.
Strong Python scripting skills. Ruby a plus.
Familiarity with API endpoint interactions and techniques for handling query complications.
Understanding of Containerization, Micro-Service, and Server-less a strong plus.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Working knowledge of message queuing, stream processing, and highly scalable data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
The ideal candidate would have 3+ years of experience in a Data Engineer role, or 5+ years in any Software Engineering role with demonstrable familiarity of the Data Engineering/Science space. Work experience, academic instruction, and/or code portfolio will all considered.
sEaphUQBLW"
89,NXT – Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"At Citadel, data is the core of the investment process. Data Engineers architect and build our data platforms which drive how we source, enrich, and store data that integrates into the investment process. These Data Engineers own the entire data pipeline starting with how we ingest data from the outside world, transforming that information into actionable insights, and ultimately designing the interfaces and APIs that our investment professionals and quantitative researchers use to monetize ideas. Throughout the process, our Data Engineers partner with top investment professionals and data scientists to design systems that solve our most critical problems and answer the most challenging questions in finance.
We’re looking for premier Data Engineers to join the NXT program at Citadel. NXT is a two-year program that offers a combination of training, experience, and exposure to get you to the next level in your career.
YOUR OPPORTUNITY:
Develop solutions that enable investment professionals to efficiently extract insights from data. This includes owning the ingestion (web scrapes, S3/FTP sync, sensor collection), transformations (Spark, SQL, Kafka, Python/C++/R), and interface (API, schema design, events)
Partner with the industry’s top investment professionals, quantitative researchers, and data scientists to design, develop, and deploy solutions that answer fundamental questions about financial markets
Build tools and automation capabilities for data pipelines that improve the efficiency, quality and resiliency of our data platform
Drive our evolution of our data strategy by challenging the status quo and identifying opportunities to enhance our platform
YOUR SKILLS & TALENTS:
Passion for working with data and developing software to solve data processing challenges
Experience with Distributed Computing, Natural Language Processing, or Machine Learning
Experience with any of the following systems: Apache Airflow, AWS/GCE/Azure, Jupyter, Kafka, Docker, Nomad/Kubernetes, or Snowflake
Proficiency with one or more programming languages such as Java, Python, R or JavaScript
Proficiency with RDBMS, NoSQL, distributed compute platforms such as Spark, Dask or Hadoop
Strong written and verbal communications skills
Bachelor’s, Master’s or PhD degree in Computer Science or equivalent experience
About Citadel
Citadel is a global investment firm built around world-class talent, sound risk management, and innovative leading-edge technology. For a quarter of a century, Citadel’s hedge funds have delivered meaningful and measurable results to top-tier investors around the world, including sovereign wealth funds, public institutions, corporate pensions, endowments and foundations.

With an unparalleled ability to identify and execute on great ideas, Citadel’s team of more than 675 investment professionals, operating from offices including Chicago, New York, San Francisco, London, Hong Kong and Shanghai, deploy capital across all major asset classes, in all major financial markets."
90,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,"
C# programming and .NET platform experience
Azure Data Architecture
Experience with data lakes, MS SQL, Data Bricks
NoSQL environments such as CosmosDB
Engineering and/or software development experience and demonstrable architecture experience at enterprise scale
History of working successfully with cross-functional engineering teams
Structured, Unstructured, Semi-Structured Data techniques and processes
Working knowledge of open source Apache products, such as Hadoop",None Found,"Profile and analyze data in designing scalable solutions
Develop highly scalable and extensible Big Data platforms which enable collection, storage, modeling, and analysis of massive data sets including those from streaming data
Leverage technologies such as CosmosDB and Gremlin API / Graph Model to host high speed applications at global scale.
Analyze, develop, and maintain data pipelines from internal and external sources
Ability to adapt to new technologies and learn quickly",None Found,None Found,"Overview
UL is seeking a Data Engineer to join the iON Compliance team in Chicago, IL.

iON Compliance is a big data platform combining data ingest, analytics, machine learning and elements of artificial intelligence within a data lake. iON is now looking to add a Data Engineer to our team to accelerate the growth of the most innovative UL digital technology investment in its history. Our goal is to enable the UL client base to effectively apply the recent developments in the digital revolution, by giving them the ability to truly leverage their own corpus of data with internal core competences and win in a rapidly evolving digital eco-system.

iON aims to disseminate Artificial Intelligence & Machine Learning based cutting edge techniques and solutions to solve challenging and complex industrial problems. Joining the iON UL team gives you the opportunity to:
Work on disruptive products that are still in its early stages
Solve challenging problems that will revolutionize text processing and text mining
Work for a company that’s a recognized leader in data engineering and processing
Be involved in the fast growing, big data space
We are looking for data engineers with expertise and passion for building large analytical platforms systems.
Responsibilities
Profile and analyze data in designing scalable solutions
Develop highly scalable and extensible Big Data platforms which enable collection, storage, modeling, and analysis of massive data sets including those from streaming data
Leverage technologies such as CosmosDB and Gremlin API / Graph Model to host high speed applications at global scale.
Analyze, develop, and maintain data pipelines from internal and external sources
Ability to adapt to new technologies and learn quickly
Qualifications
Skills & Qualifications:
C# programming and .NET platform experience
Azure Data Architecture
Experience with data lakes, MS SQL, Data Bricks
NoSQL environments such as CosmosDB
Engineering and/or software development experience and demonstrable architecture experience at enterprise scale
History of working successfully with cross-functional engineering teams
Structured, Unstructured, Semi-Structured Data techniques and processes
Working knowledge of open source Apache products, such as Hadoop
Optional Additional Experience:
Big Data platforms e.g. Azure DW, SQL PDW, Cloudera, Hortonworks
Visualization: Qlikview, Tableau, D3.js
ETL tools such as Microsoft SSIS, Azure Data Factory, Pentaho PDI or Talend or Informatica
Continuous delivery and deployment using Agile Methodologies.
Data Warehouse and DataMart design and implementation
NoSQL environments such as MongoDB, Cassandra
Data modeling of relational and dimensional databases
Architect/design data management processes and capabilities (e.g governance, archiving, metadata, master data mgmt., data modeling, data quality)
A nice to have would be knowledge of ML / AI environments and challenges – particularly as it relates to data management
Experience & Education Requirements:
Bachelor’s Degree in Computer Science, Engineering, Mathematics, Statistics or related field
3+ years’ experience with data and cloud technologies"
91,Senior Data Engineer - Hux,"Chicago, IL 60606",Chicago,IL,60606,None Found,"8+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
4+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
3+ years of experience on distributed, high-throughput and low-latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
Limited immigration sponsorship may be available.",None Found,None Found,None Found,None Found,"Hux Senior Data Engineer
Locations: New York, NY – Greensboro, NC - Chicago, IL – Raleigh, Durham, Chapel-Hill, NC
What is Hux? Hux is the Human Experience Platform by Deloitte Digital.
In today’s world, customers expect companies to know who they are and what they want. Customers want to have products, services or experiences that best suit their needs delivered to them seamlessly across physical and digital channels.
Customers are human first: driven by dynamic wants, needs, and desires. The ability for brands to make personal, meaningful connections on a human level has never been greater and Hux by Deloitte Digital delivers on those experiences in a way that allows companies to own the customer journey end to end. We help companies connect key data sources to understand what matters most to people; connect to advanced technologies like AI and machine learning to sense and respond to those needs at scale; and connect their systems to unlock insights, create collaboration and drive acquisition, engagement and loyalty. Most importantly, we empower companies to connect with customers in personal, meaningful ways that respect them as people, not just customers.
Hux by Deloitte Digital gives companies the ability to build and leverage the connections – between people, systems, data and technologies – so they can deliver personalized, contextual experiences to customers at scale.
Work you’ll do

As a Senior Data Engineer-Hux, you’ll design, implement and maintain a full suite of real-time and batch jobs that fuels our cutting-edge AI to provide real-time marketing intelligence to our existing clients.
You’ll develop, test and deliver production-grade code to help our clients solve their most critical marketing challenges using cutting-edge big-data tools. You’ll also ensure data integrity, resolve production issues, and assist in the support and maintenance of our overall platform.
As you grow your capabilities and learn how to build a platform that can ingest, load and process billions of data points, you’ll enjoy new challenges and opportunities to showcase your development skills by joining project teams to build innovative new-client platforms and execute high-value strategic development projects with high visibility.
Your responsibilities will include:
Design, construct, install, test and maintain highly scalable data pipelines with state-of-the-art monitoring and logging practices.
Bring together large, complex and sparse data sets to meet functional and non-functional business requirements.
Design and implement data tools for analytics and data scientist team members to help them in building, optimizing and tuning our product.
Integrate new data management technologies and software engineering tools into existing structures.
Help in building high-performance algorithms, prototypes, predictive models and proof of concepts.
Use a variety of languages, tools and frameworks to marry data and systems together.
Recommend ways to improve data reliability, efficiency and quality.
Collaborate with Data Scientists, DevOps and Project Managers on meeting project goals.
Tackle challenges and solve complex problems on a daily basis.
Qualifications
Required:
8+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.
4+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.
3+ years of experience on distributed, high-throughput and low-latency architecture.
1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.
Limited immigration sponsorship may be available.
Preferred:
A successful track-record of manipulating, processing and extracting value from large disconnected datasets.
Producing high-quality code in Python.
Passionate about testing, and with extensive experience in Agile teams using SCRUM; you consider automated build-and-test to be the norm.
Proven ability to communicate in both verbal and writing in a high performance, collaborative environment.
Follows data development best practices, and enjoy helping others learn to do the same.
An independent thinker who considers the operating context of what he/she is developing.
Believes that the best data pipelines run unattended for weeks and months on end.
Familiar with version control, you believe that code reviews help to catch bugs, improves code base and spread knowledge.
Ability to travel 5-10% of the time
Helpful, but not required:
Knowledge in:
Experience with large consumer data sets used in performance marketing is a major advantage.
Familiarity with machine learning libraries is a plus.
Well-versed in (or contributes to) data-centric open source projects.
Reads Hacker News, blogs, or stays on top of emerging tools in some other way
Data visualization
Industry-specific marketing data
Technologies of Interest:
Languages/Libraries – Python, Java, Scala, Spark, Kafka, Hadoop, HDFS, Parquet.
Cloud – AWS, Azure, Google

The team
Advertising, Marketing & Commerce
Our Advertising, Marketing & Commerce team focuses on delivering marketing and growth objectives aligned with our clients’ brand values for measurable business growth. We do this by creating content, communications, and experiences that engage and inspire their customers to act. We implement and operate the technology platforms that enable personalized content, commerce and marketing user-centric experiences. In doing so, we transform our clients’ marketing and engagement operations into modern, data-driven, creatively focused organizations. Our team brings deep experience in creative and digital marketing capabilities, many from our Digital Studios.

We serve our clients through the following types of work:Cross-channel customer engagement strategy, design and development(web, mobile, social, physical)eCommerce strategy, implementation and operationsMarketing Content and digital asset management solutionsMarketing Technology and Advertising Technology solutionsMarketing analytics implementation and operationsAdvertising campaign ideation, development and executionAcquisition and engagement campaign ideation, development and executionAgile based, design-thinking, user-centric, empirical projects that accelerate results
How you’ll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.
Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitte’s culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.
Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.
Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.
#LI:PTY
#IND:PTY"
92,Data Engineer - SQL / Big Data / Java,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,"
Experience Range: 2-5 years (Fresh Graduates must be top-ranked and exceptionally qualified)
Bachelor or higher Degree in information systems or computer science
Understanding of Data Integration flavors
Solid understanding of SQL and good grasp of relational and analytical database management theory and practice.
Good knowledge of software development and architectural patterns.
Technical skills include Java development, JDBC, XML, Web Service related APIs, experience with version control systems (e.g. SVN, git).
Basic experience in Big Data, NoSQL, and InMemory environments is welcome.
Experience in Windows & Linux (and UNIX) operating systems in server environments.
Personal and Relationship qualities: Professional curiosity and the ability to enable yourself in new technologies and tasks. Active listener. Curiosity and continuous learning. Creativity. Team worker.
Communications: Good written/verbal communication skills in English (other international languages a plus) are essential for interaction with clients, making presentations, attending meetings and writing technical documentation.
Willingness to travel.","
Conception, implementation, and execution of customer-specific integration projects based on the Denodo Platform.
Education, coaching and support during the introduction as well as ongoing projects of the Denodo Platform to achieve high level of client satisfaction.
Diagnose and resolve clients inquiries related to operating Denodo software products in their environment.
Participate in problem escalation and call prevention projects to help clients and other technical specialists increase their efficiency when using Denodo products.
Contribute to knowledge management activities and promote best practices for project execution.
Implement product demos and pilots to showcase Data Virtualization in enterprise scenarios, cloud deployments and Big Data projects.
Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues.",None Found,None Found,"Job Description

The Opportunity
Your consulting projects will include integrating data in a virtual manner for operational and/or informational purposes - Integration of 100+ data sources for a Customer Service Multichannel IT Infrastructure; implementation of Logical Data Warehouses and Virtual Datamarts to enable modern Business Intelligence solutions, Integration Layers for Hadoop-based Data Lakes, and support for Agile Operational Reporting on a diverse Big Data infrastructure are just a few flavours of your future projects.
Be part of an elite team in a rapidly growing international software product company. Your career with us will combine cutting edge technology, exposure to worldwide clients across all industries (Financial Services, Automotive, Insurance, Pharma, etc.), exciting growth path for technical, product and customer-facing roles, direct mentorship, and access to senior management as part of a global team. Your mission is to help our clients and prospects in the region to realize their full potential through accelerated adoption and productive use of Denodo's data virtualization capability in many solutions.

Duties & Responsibilities
As a Data Engineer Virtualization (f/m) you will successfully employ a combination of high technical expertise, client communication and coordination skills between clients and internal Denodo teams to achieve your mission.
Conception, implementation, and execution of customer-specific integration projects based on the Denodo Platform.
Education, coaching and support during the introduction as well as ongoing projects of the Denodo Platform to achieve high level of client satisfaction.
Diagnose and resolve clients inquiries related to operating Denodo software products in their environment.
Participate in problem escalation and call prevention projects to help clients and other technical specialists increase their efficiency when using Denodo products.
Contribute to knowledge management activities and promote best practices for project execution.
Implement product demos and pilots to showcase Data Virtualization in enterprise scenarios, cloud deployments and Big Data projects.
Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues.

Qualifications

Desired Skills & Experience
Experience Range: 2-5 years (Fresh Graduates must be top-ranked and exceptionally qualified)
Bachelor or higher Degree in information systems or computer science
Understanding of Data Integration flavors
Solid understanding of SQL and good grasp of relational and analytical database management theory and practice.
Good knowledge of software development and architectural patterns.
Technical skills include Java development, JDBC, XML, Web Service related APIs, experience with version control systems (e.g. SVN, git).
Basic experience in Big Data, NoSQL, and InMemory environments is welcome.
Experience in Windows & Linux (and UNIX) operating systems in server environments.
Personal and Relationship qualities: Professional curiosity and the ability to enable yourself in new technologies and tasks. Active listener. Curiosity and continuous learning. Creativity. Team worker.
Communications: Good written/verbal communication skills in English (other international languages a plus) are essential for interaction with clients, making presentations, attending meetings and writing technical documentation.
Willingness to travel.
Additional Information

Employment Practices
We are committed to equal employment opportunity.
We respect, value and welcome diversity in our workforce.
We do not accept resumes from headhunters or suppliers that have not signed a formal fee agreement. Therefore, any resume received from an unapproved supplier will be considered unsolicited, and we will not be obligated to pay a referral fee."
93,Sr. Data Engineer,"Chicago, IL 60654",Chicago,IL,60654,None Found,None Found,None Found,"
Serve as a member of a data team that solves complex challenges and builds working database solutions using SQL Server, T-SQL, SSIS, stored procedures, views, user-defined functions, and table functions
Develop solutions and contributing to development, leveraging Object-Oriented programming techniques (.Net), Software Development Lifecycles, Unit Test Techniques, and Debugging/Analytical Techniques.
Collaborate with the team to develop database structures that fit into the overall architecture of the systems under development.
Code, install, optimize, and debug database queries and stored procedures using appropriate tools and editors.
Perform code reviews and provide feedback in a timely manner.
Promote collective code ownership for everyone to have visibility into the feature codebase.
Present technical ideas and concepts in business-friendly language.
Provide recommendations, analysis, and evaluation of systems improvements, optimization, development, and maintenance efforts, including capacity planning.
Identify and correct performance bottlenecks related to SQL code.
Support timely production releases and adherence to release activities.
Contribute to data retention strategy.
",None Found,None Found,"Position Purpose

As a Senior Data Engineer on the Echo Global Logistics team, you will contribute to database management of large scale web-based applications through the use of SQL, C#, and .NET tools. These technologies enable Echo's business while supporting architectural vision of quality, scalability, performance and function. Our proprietary software is created with the goal to simplify transportation for our customers and carriers, and is one of our largest competitive advantages in an ever growing market.

Echo Global Logistics has recently been ranked the third largest digital company by employee size in Chicago and we are continuing to see increased growth in virtually all of our technical teams. Additionally, we have been ranked as the #1 Inbound Logistics 3PL for 2017 and look forward to speaking with you further about our team!

Essential Position Functions

You...


See technology as a passion, not something you just do between 9-5
Possess the ability to create new solutions; we operate on a web based platform and constantly facing unchartered waters
Possess strong fundamentals within coding technologies and a willingness to wear several hats when called upon
Do not wait for something to break; find a problem before it becomes one and constantly aiming to improve
Having a willingness to vocalize these ideas and pick yourself up if you get knocked down

We…


Value passionate technologists, go-getters, and people who never stop seeking ways to improve existing technology
Have a high focus on career development and the runway to get you there
Work hard, period
Offer competitive compensation, benefits, 401k, challenging projects, company wide events, coworkers and leaders who will push you to get better, a sense of community not found anywhere else

Responsibilities:
Data Engineer's work in conjunction with Software Engineers, DBA's, Business Analysts, Quality Assurance and business owners


Serve as a member of a data team that solves complex challenges and builds working database solutions using SQL Server, T-SQL, SSIS, stored procedures, views, user-defined functions, and table functions
Develop solutions and contributing to development, leveraging Object-Oriented programming techniques (.Net), Software Development Lifecycles, Unit Test Techniques, and Debugging/Analytical Techniques.
Collaborate with the team to develop database structures that fit into the overall architecture of the systems under development.
Code, install, optimize, and debug database queries and stored procedures using appropriate tools and editors.
Perform code reviews and provide feedback in a timely manner.
Promote collective code ownership for everyone to have visibility into the feature codebase.
Present technical ideas and concepts in business-friendly language.
Provide recommendations, analysis, and evaluation of systems improvements, optimization, development, and maintenance efforts, including capacity planning.
Identify and correct performance bottlenecks related to SQL code.
Support timely production releases and adherence to release activities.
Contribute to data retention strategy.

Position Requirements


1-3 years in commercial-grade business applications environment leveraging the following:
SQL Server, T-SQL, SSIS, stored procedures, user-defined functions and table functions
Managing design risk
1-3 years leveraging OO programming techniques
Software development lifecycles, Unit test techniques, debugging/analytical techniques

What's in it for you?


Help career growth by joining industry leader and continuing to advance Echo web based technologies
Working with an organization with defined market goals, products, customers, revenue, and development teams
Experienced mentors to learn and adopt new practices
Ability to introduce your own views and takes on our product offerings
Work in wide variety of data management
Ability to constantly enhance and improve applications
Have a clearly defined career growth track with enough flexibility to pave your own way

All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, status as a qualified individual with a disability, or Vietnam era or other protected veteran."
94,AWS Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,"At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.","DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud",None Found,None Found," Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
§ Certified AWS Developer - Associate
§ Certified AWS DevOps – Professional (Nice to have)
§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
95,Data Engineer,"Chicago, IL 60654",Chicago,IL,60654,None Found,"2+ years' experience building data solutions including AWS S3, EMR, and Redshift, Tableau and Tableau server
Expert SQL scripting skills; R, Python, or SAS preferred but not required
Ability to effectively build relationships across the business at all levels
Self-starter, entrepreneurial, high-energy who can take initiative in a fast-moving environment
Strong technical understanding of current and emerging business intelligence and analytics technologies
",None Found,"
Act as data strategist responsible for the short and long term vision of data management, warehouse performance, and data architecture supporting both business intelligence and prescriptive analytics
Partner with both internal stakeholders and external vendors involved in project definition, design and planning, and mapping the data journey from source through consumer (data visualization, application, or predictive model)
Gather, document, and analyze business requirements; establish and prioritize efforts to deliver data models that support business needs
Design, develop, test and deploy data models, data collection, and transformation components. Determine best point for transformations, calculations, and joins (e.g. data lake, data warehouse, or Tableau data source)
Troubleshoot and support existing data workflow processes; deliver fixes and optimizations where appropriate
Work closely with CIO, Chief Architect, and Chief Analytics Officer on data governance and planning - ensure scalability and sustainability of business intelligence data architecture",None Found,None Found,"Job Summary:

The Data Engineer is responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The Data Engineer will create processes and data models to enable performance management reporting as well as data science, advanced analytics, and personalization efforts.

 The Data Engineer will work closely with Technology, Finance, and Commercial Analytics to drive value through best in class data architecture and data model design.

Key Responsibilities

Act as data strategist responsible for the short and long term vision of data management, warehouse performance, and data architecture supporting both business intelligence and prescriptive analytics
Partner with both internal stakeholders and external vendors involved in project definition, design and planning, and mapping the data journey from source through consumer (data visualization, application, or predictive model)
Gather, document, and analyze business requirements; establish and prioritize efforts to deliver data models that support business needs
Design, develop, test and deploy data models, data collection, and transformation components. Determine best point for transformations, calculations, and joins (e.g. data lake, data warehouse, or Tableau data source)
Troubleshoot and support existing data workflow processes; deliver fixes and optimizations where appropriate
Work closely with CIO, Chief Architect, and Chief Analytics Officer on data governance and planning - ensure scalability and sustainability of business intelligence data architecture
Required Qualifications:
2+ years' experience building data solutions including AWS S3, EMR, and Redshift, Tableau and Tableau server
Expert SQL scripting skills; R, Python, or SAS preferred but not required
Ability to effectively build relationships across the business at all levels
Self-starter, entrepreneurial, high-energy who can take initiative in a fast-moving environment
Strong technical understanding of current and emerging business intelligence and analytics technologies
 Education:

Bachelor's Degree in Technology, Computer Science, or Data Science preferred"
96,Azure Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,"At least 5 years of consulting or client service delivery experience on Azure
",DevOps on an Azure platform,None Found,None Found," Proven ability to build, manage and foster a team-oriented environment
","Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
97,Data Engineer,"Chicago, IL 60606",Chicago,IL,60606,None Found,None Found,None Found,None Found,None Found,None Found,"JOB DESCRIPTION
DATA ENGINEER

WHO IS SPR?

SPR is a digital technology consultancy that develops elegant solutions to transform the way people do business. We’re 300+ strategists, developers, designers, architects, consultants, thinkers, and doers in Chicago. We work with 160 clients in 10 unique industries – everything from corporate finance and global logistics to local breweries and Chicago startups.

We think about the end users and rigorously apply the latest technologies and frameworks to address our clients’ needs. We enable companies to do more with data, engage with other people, build disruptive solutions, and operate productively. To do this, we hire smart technologists and sharp business leaders who are excellent communicators and have an interest in working on multiple projects across industries.

SPR offers a great environment for employees to learn, to build systems that make an impact, and to tackle exciting challenges. With our office’s “Maker Space”, you can explore your IoT side and develop fun projects with 3D printing and CNC machining. We operate in a fun, casual work environment and have great benefits including: competitive salary, bonuses, generous vacation time, big fitness incentives, and medical/dental/vision insurance.

By joining the SPR team, you’ll be using your brain, working hard and making an impact through your projects – and you’ll be rewarded for it.

WHAT IS THE POSITION?

As a Data Engineer at SPR, you must have experience building and operating data pipelines (both streaming and batch, utilizing both ETL and ELT architectures). You will be building data pipeline solutions by designing, adopting and applying big data strategies and architectures. You must be experienced in large-scale system implementations with a focus on complex data processing and analytics pipelines. You must demonstrate an understanding of data integration best practices, and expertise in data integration, data transformation, data modeling and data cleansing. The Data Engineer must be able to demonstrate innovative approaches to complex problems which deliver industry-leading experiences for our clients.

PROFESSIONAL QUALIFICATIONS

Experience in designing and implementing innovative data integration solutions, utilizing Python with Spark clusters.
Familiarity with architectural patterns for data-intensive solutions
Expertise in real-time streaming and migrating batch-style data processing to streaming and micro-batch solutions
Knowledge of the RDBMS core principles; set up, tune, design, as well as newer unstructured data tools
Familiarity with consulting and traditional application design
Excellent written and verbal communication skills
Display solid problem-solving abilities in the face of ambiguity
Must be a hands-on individual who is comfortable leading by example
Experience with Agile Methodology
Possess excellent interpersonal and organizational skills
Able to manage your own time and work well both independently and as part of a team

TECHNOLOGIES WE USE

Cloud (Azure, AWS, Cloud Foundry, Heroku, Mesos, DC/OS) / / RDBMS (SQL Server, PostgreSQL, Oracle, DB2) /NoSQL (Mongo, Raven, DocumentDB, Cassandra, Maria, Riak) / Python (including Databricks) / / Big Data (Cloudera & Hortonworks Hadoop distributions, including Hive, Pig, Sqoop, Spark) / Integration Tools (Apache Nifi, Cloudera Streamsets, Azure Data Factory, AWS Glue, Talend) / ELK (ElasticSearch, Logstash, Kibana) / Machine Learning (Azure ML tooling, TensorFlow, AWS Sagemaker, scikit-learn) / Data Visualization (Grafana, Kibana) / Microsoft PowerShell / AWS SDK / Fast Data (Apache Ignite / Gridgain, Apache Geode/Pivotal Gemfire)

EDUCATION & EXPERIENCE

3-5 years of professional experience
BA or BS, preferably in Computer Science, Engineering or Science/Technology-based discipline

If this sounds like the kind of challenge you would be up for every day, we would love to hear from you."
98,"Senior Software Data Engineer, Employee Cloud","Chicago, IL 60614",Chicago,IL,60614,None Found,None Found,None Found,None Found,None Found,None Found,"We're not your traditional tech company and we don't aim to be. Going against the grain is in our DNA. Building a revolutionary product begins with revolutionary thinking. That's why we value diversity of background and lived experience. Together, we empower restaurants of all sizes to build great teams, increase revenue, improve operations, and delight their guests. We pair our deep understanding of the restaurant industry with powerful cloud based software and restaurant-grade hardware to deliver an intuitive all-in-one platform. Join us on our mission to empower the restaurant community to delight guests, do what they love, and thrive.

Bready* to make a change?

Do you like to solve complex problems at scale? We are looking for a backend focused Software Engineer to play a senior role on our Employee Cloud team. A restaurant's employees are a critical part of every restaurant, and we need your help to revolutionize our payroll and team management products that are used by thousands of restaurants and hundreds of thousands of employees every day.

About this roll*:
Some projects we've been working on recently include:

Automating the entire restaurant tip management process from point of sale to payroll
Allowing restaurants to self onboard themselves in order to facilitate the rapid expansion of our customer base
Re-architecting our monolithic systems into microservices, utilizing AWS Serverless technologies

As a Senior Software Engineer on the Employee Cloud team, you will be:


Improving/expanding our database architecture and application data workflows in order to support a rapidly growing customer base
Making hands-on contributions in all phases of the software lifecycle, including: product solutioning, technical design and architecture, implementation, testing, deploying, and maintaining.
Mentoring and developing team members

Do you have the right ingredients?


Strong experience with relational databases and database development
Proficient with non-relational databases and alternative data stores
Proficient in .NET, Javascript, or other object oriented languages
A quality driven approach to development with a desire to keep things simple
Enjoy collaborating with and mentoring colleagues
Passionate about technology and learning new things
Desire to make continuous improvements to teams and technology
Prior experience with distributed systems, microservices and capacity planning is a plus

Our Tech Stack:
We have a pretty varied tech stack! We are fully in AWS, and have traditionally been on the .NET stack, leveraging ASP.NET, C#, some VB.NET. We have since been re-architecting our systems to React based front ends while transitioning to serverless stacks with Lambda/API Gateway where systems are being written in a mix of Node.js and .NET Core. On the database side, we are using Aurora MySQL for relational data storage, and DynamoDB for non-relational data stores. We have invested heavily in the areas of build, test, and deploy automation, using GoCD for Continuous Delivery pipelines, and CloudFormation for Infrastructure as Code."
99,Data Engineer,"Chicago, IL 60601",Chicago,IL,60601,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer

 US-IL-Chicago


Job Description

Chicago, IL

Full Time Perm


Do you love working in a collaborative environment with free breakfast, coffee/tea/soda, social events and much more? Then read on because Omnitracs is looking for the best and brightest to help us disrupt the freight and logistics industry! What sets us apart from other logistics technology companies is our rich history in data! In 1988, Omnitracs (then Qualcomm) fundamentally changed the way fleets operate and we’re doing it again today. With over a million assets in over 70 countries, Omnitracs has a lot of data. Omnitracs’ newly formed Innovation Lab is innovating on this data to create new products - helping our customers not just survive, but thrive, in today’s complex transportation ecosystem. We are looking for you, Data Engineer, to join our fast-paced Agile team in Chicago. Who You Are As a Data Engineer, you will be responsible for data management tasks including design, development, and technical administration in an AWS environment. You will also provide technical leadership to the team and be responsible for maintaining technical specifications. As a Data Engineer, you will have a heavy focus on designing the solutions to deliver data products. To be successful in this role you will need a solid understanding of data management concepts and how cloud technology can solve data issues.


Responsibilities and Duties As the Cloud Data Architect, your responsibilities will include, but are not limited to:
• Oversight of the design and standards of AWS (S3, Redshift/Snowflake/SQL Server, Glue,) data applications • Train and coach data team developers • Oversee and implement security features, access and standards around data management • Assist in capacity and budgeting for data systems • Provide estimates and oversight within a Scrum environment • Strong SQL skills in data warehouse environment • Spark, Python and/or Scala experience • Lead code reviews Qualifications and Skills • At least 3 years IT experience in AWS data services • Hands on experience working with complex Data Warehouses and or customer linking systems • Data Lake experience using Spark, Scala, EMR and/or Glue • Data Modeling experience • Proficient with SQL • Solid S3 understanding • Experience with AWS Aurora, Oracle and/or SQL Server developer experience • Hands on experience using AWS RDS Nice to Have • Experience with Redshift or Snowflake using Matillion or other ETL tools • Identity & Access Management (Security Provisioning) understanding and knowledge"
100,Senior Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.

As a Senior Data Engineer, here's what we'll be looking for you to bring:
Hands-on Engineering Leadership
Proven track record of Innovation and expertise in Data Engineering
Tenure in coding, architecting and delivering complex projects
Deep understanding and application of modern data processing technology stacks. For example Spark, Kafka, Hadoop, ecosystem technologies, and others
Deep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies
Deep understanding of relational database technologies and database development techniques
Understanding of how to architect solutions for data science and analytics
Data management for reporting and BI experience is a plus
Understanding of “Agility”, including core values, guiding principles, and key agile practices
Understanding of the theory and application of Continuous Integration/Delivery
Passion for software craftmanship
A rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..
Strong stakeholder management and interaction experience at different levels
Any experience building and leading an offshore/outsourcing function would be highly beneficial.
There's no typical day or engagement for our Senior Engineers. Here’s what you’ll do:

Be the SME. Develop Big Data architectural approach to meet key business objectives and provide end to end development solution
You might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that Big Data has to solve their most pressing problems.
On other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.
It could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.
Whatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.
You have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.
You recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.
Regardless of what you do at ThoughtWorks, you’ll always have the opportunity to:

Think through hard problems, and work with a team to make them reality.
Learn something new every day.
Work in a dynamic, collaborative, transparent, non-hierarchal, and ego-free culture where your talent is valued over a role title
Travel the world.
Speak at conferences.
Write blogs and books.
Develop your career outside of the confinements of a traditional career path by focusing on what you’re passionate about rather than a predetermined one-size-fits-all plan
Be part of a company with Social and Economic Justice at the heart of its mission.
A few important things to know:
Projects are almost exclusively on customer site, so candidates should be flexible and open to travel.

Candidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.

Not quite ready to apply? Or maybe this isn’t the right role for you? That’s OK, you can stay in touch with AccessThoughtWorks, our learning community (click ""contact me about recruitment opportunities"" to hear about jobs in the future).

It is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment."
101,Senior Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,"3+ years experience programming with Python is required
3+ years in an ETL or Data Engineering role, building and implementing data pipelines
Strong skills with PySpark and SQL with the ability to write efficient queries
Familiarity with AWS big data services: S3, Redshift, Glue, EC2, Lambda, SageMaker, Dynamo
Experience working in a highly collaborative environment - we do Agile using sprints, planning, retro, etc.
Experience with Airflow and other open source technologies
Excited and willing to learn new things
Background in computer science, engineering, mathematics, related field, or equivalent work experience",None Found,"Collaborate with our reporting, analytics, and data science teams to understand data sources and business requirements
Work within a collaborative team, adhering to Agile best practices, documentation, and knowledge sharing
Gather, clean, enrich, and transform data to feed internal and external client needs
Define, build, test, and implement data pipelines, batch and streaming
Monitor pipeline performance and document infrastructure changes
Make code decisions and adhere to best practices for ETL and programming
Contribute to our overall architecture and pipeline design and make contributions to the product road map",None Found,None Found,"Location:
Chicago, Illinois
We're looking for a hands-on, collaborative Senior Data Engineer to join our Data Engineering team. When you get out of bed in the morning, you look forward to building solutions, you love working with data, and you enjoy working as a team.
In this role, you'll have the opportunity to make code decisions and build cloud infrastructure and pipelines that will deliver data solutions to our Food and Beverage and Sports industry clients. You will report to our Director of Data Engineering and help bring best practices to the team while working with a team of Data Engineers.
If you have a git repository, we'd be excited to see it!
The Company is not able to sponsor employment visas for this position.
Core Responsibilities
Collaborate with our reporting, analytics, and data science teams to understand data sources and business requirements
Work within a collaborative team, adhering to Agile best practices, documentation, and knowledge sharing
Gather, clean, enrich, and transform data to feed internal and external client needs
Define, build, test, and implement data pipelines, batch and streaming
Monitor pipeline performance and document infrastructure changes
Make code decisions and adhere to best practices for ETL and programming
Contribute to our overall architecture and pipeline design and make contributions to the product road map
Minimum Qualifications
3+ years experience programming with Python is required
3+ years in an ETL or Data Engineering role, building and implementing data pipelines
Strong skills with PySpark and SQL with the ability to write efficient queries
Familiarity with AWS big data services: S3, Redshift, Glue, EC2, Lambda, SageMaker, Dynamo
Experience working in a highly collaborative environment - we do Agile using sprints, planning, retro, etc.
Experience with Airflow and other open source technologies
Excited and willing to learn new things
Background in computer science, engineering, mathematics, related field, or equivalent work experience
Preferred Qualifications
Interest or experience in DevOps and CI/CD
Experience with building data lake solutions
Experience with JavaScript
Experience with Netezza and Data Stage

About E15
At E15, we are the spark that ignites. Our team delivers next-generation insights based on data, not hunches, to drive business in MLB, NHL, NBA, NFL, College Sports, and beyond. E15 brings unmatched industry intelligence and cutting edge analytics to sports, entertainment, hospitality, and retail industries to help companies make forward-looking decisions to benefit their business, fans and customers. www.e15group.com

About Levy
Levy is the leader in Sports and Entertainment dining, catering such renowned sports venues at Wrigley Field in Chicago, STAPLES Center and Dodger Stadium in Los Angeles, Ford Field in Detroit and Churchill Downs in Louisville. Levy also caters events including Super Bowls, World Series, NASCAR Racing, the Kentucky Derby, the U.S. Open Tennis Tournament and the Grammy Awards. www.levyrestaurants.com

E15 is an equal opportunity employer. At E15 we are committed to treating all Applicants and Team Members fairly based on their abilities, achievements, and experience without regard to race, national origin, sex, age, disability, veteran status, sexual orientation, gender identity, or any other classification protected by law."
102,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Chicago / Data / Full-time


Data Engineering is the foundational layer of our Data & Analytics stack at Arrive. Data Engineers not only build and optimize data pipelines that transform and store data in a way that allows the rest of the organization (analysts, data scientists, other stakeholders) analyze and consume data. They are also in charge of building the systems and establishing the processes to enable the rest of the data team to develop, test, and deploy analyses and code in an efficient and scalable way.


We’re looking for an experienced Data Engineer to help us grow our data infrastructure and platform. If you're seeking a role that is high impact and full of ownership....please read on.
What you'll tackle:

Design new enterprise data models and ETL processes to populate them
Extract and transform data from production databases and 3rd party services to provide consumable data and support functions across the organization
Detect quality issues, track them to their root source, and implement fixes and preventative audits
Manage and optimize Redshift clusters/data lake to ensure current health and performance and future scaling needs
Help maintain the process we use to develop, test, and deploy good code
Become the “go to” expert of our data. Work closely with staff to understand all data from our core systems, partner services, and any other platforms we rely on
What you brIng to the table:

Experience with AWS; expertise in Redshift, Postgres or other RDBSs (preferably column-oriented)
Expertise in SQL and ability to write and optimize complex queries
Experience with Docker, Elastic Container Service, Lambda a plus
Ability to write customized software in Python, Bash, Go or other common open source languages. Experience with Airflow or similar scheduling service a plus
Experience with CI/CD tools like Jenkins or Drone
Creativity in approaching data organization challenges with an understanding of the end goal
A collaborative nature and entrepreneurial spirit. Prior startup experience a huge plus
More about the team:

We are a tight knit team that is fun, inclusive, and hard working. We have a firm, non-negotiable no jerk policy. We accept you for who you are and consider everyone on an equal opportunity basis without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status."
103,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Cameo:
Our mission is to create the most personalized and authentic fan experiences in the world. We're a marketplace where fans can book personalized video shoutouts from their favorite people. We've helped create over 300,000 moments for our customers and built a marketplace for over 20,000 talent to connect with their biggest fans. We're breaking down the exclusivity myth of celebrity by building personal relationships between fans and talent.
Cameo is one of LinkedIn's Top 50 Startups to Work For (https://www.google.com/url?q=https%3A%2F%2Fwww.linkedin.com%2Fpulse%2Flinkedin-top-startups-2019-50-hottest-us-companies-work-jessi-hempel%2F&sa=D&sntz=1&usg=AFQjCNFQz4KGF3FEAqrh6xYc_kUjnYGLgA) and also recognized on TIME Magazine's 50 Most Genius Companies (https://www.google.com/url?q=https%3A%2F%2Ftime.com%2Fcollection%2Fgenius-companies-2018%2F5412492%2Fcameo%2F&sa=D&sntz=1&usg=AFQjCNE9Df7YI_B-XbUZVC-4pYCcxbjFSw) list. We are a global company, headquartered in Chicago, IL in the Fulton Market neighborhood and HQ2 in Venice, CA.
We recently closed our Series B round (https://www.google.com/url?q=https%3A%2F%2Ftechcrunch.com%2F2019%2F06%2F25%2Fcameo%2Famp%2F&sa=D&sntz=1&usg=AFQjCNFY84hBjAlCkm3dKa11-ir5rctW5A) led by Kleiner Perkins (https://www.google.com/url?q=https%3A%2F%2Fwww.kleinerperkins.com&sa=D&sntz=1&usg=AFQjCNHCRd_6JnbpDjifoex74E-_YO49_A) who has backed other tech giants including Google, Spotify, and Amazon. Join our team and be able to experience a rocketship from its early days. We want you to be excited about coming to work every day, knowing that the work you dedicate yourself to will have a material impact and help shape the direction of the next great tech company.

About the Role:
You will be helping us improve & scale the platform that connects fans and talent to create the perfect video shoutout. In particular, you will help us make sure we can support our growing data team's needs securely and performantly.
As a data engineer, you will be working across multiple technologies, primarily our data infrastructure (AWS redshift, AWS Lambda, Mixpanel, Google Analytics, Tableau), backend API (node.js + express), database (mongoDB), and infrastructure (Heroku + Redis + AWS + Atlas).
Design and implement data schemas, real-time pipelines, and batch processing jobs to support analytical modeling and reporting needs
Work with stakeholders to develop data expertise and resolve upstream issues relating to data quality
Define best practices and design for the management of data
Partner with internal stakeholders to build and maintain internal data processing and visualization tools
Translate requests into replicable analytic reports using varying applications
Create tools to serve data such as APIs and packages
You will help drive a Data mindset and culture across the company

Engineering Culture and Values:
Check out our Key Values (https://www.google.com/url?q=https%3A%2F%2Fwww.keyvalues.com%2Fcameo&sa=D&sntz=1&usg=AFQjCNEsr_WDoiQuRejuZZZRBpEp3Fccsg) profile
A few things about yourself:
2+ years experience working as a data engineer, in particular data pipelines and SQL dbs
1+ years experience with NoSQL databases (MongoDB or Elasticsearch preferred)
3+ years experience with programming languages (Python, Java, R, and/or Scala preferred)
Familiarity with a variety of data processing technologies (e.g. Spark, Kafka, Hadoop)
Excellent communication skills, including a knack for clear documentation
Experience managing data transformation processes and making data available through service applications and databases.
What we hope you'll bring to the table:
Experience working with Redshift, Postgres, Airflow, Docker
Experience supporting product analytics, with an emphasis on web/mobile applications
1+ years experience with Javascript
Some experience with frontend web-development
Experience defining and implementing APIs
Cameo is an equal opportunity employer. We are committed to creating an inclusive and welcoming environment for every person who walks through our doors. All employment is decided on the basis of qualifications, merit, and business need. Cameo celebrates and embraces diversity."
104,Entry Level Data Engineer,"Chicago, IL 60601",Chicago,IL,60601,None Found,None Found,None Found,None Found,None Found,None Found,"Do you know how excellent data design can support critical business decision-making? So do we.

Data is more than just information: its valuable insight that gives businesses the knowledge to transform.

About you

You help to design data solutions that enable clients to see the whole picture and provide insightful and accurate analysis that helps to build successful businesses.

You have a passion for learning teamed with good business sense and time management and can manage your clients’ expectations effectively. You have earned a bachelor’s degree or equivalent in a relevant Analytics field and have one to two years of academic experience in your field.

About the job

In Data Engineering, you’ll use modern data engineering techniques and advanced analytics methods to give your clients the information they need. You collect, aggregate, store and reconcile data from various sources, helping to design and build data pipelines, streams, reporting tools, data generators and a whole range of tools to provide information and insight. Your work gives people the tools they need to find and use data for routine and non-routine analysis.

Day to day, you will:
Support the planning and implementation of data design services, providing sizing and configured assistance and performing needs assessments, as well as delivering data warehouse and storage architectures

Provide leadership and vision for designing and implementing data analytics and modelling strategies, you identify new analytics tools and techniques and build environments that generate and study information that supports creative, solutions based on the data, and using advanced statistical, data mining and machine-learning techniques.

Deliver data using Microsoft SQL Server Reporting Services (SRSS), Microsoft Excel, PowerPivot and Microsoft SharePoint Performance Point

Customize data storage and extraction, data mining, database architecture, metadata and repository creation

Implement effective metrics and monitoring processes

Travel as required - at times could be 80% of the work week.

Your skills

You really know your way around database technology and data reporting tools. And you also understand the point of it all—to produce confirmed insights to drive business decisions. You have an analytical mind on top of good technical skills.

Yours skills and experience include:
Knowledge of database storage, collection and aggregation models, techniques and technologies and the ability to apply such methods to address business problems

Knowledge of structured problem-solving assignments

Excellent project management and people management

Knowledge of Microsoft SharePoint, PowerPivot, SRSS, Excel (with embedded Pivot Tables and macros)

Some SQL experiences.

Avanade® Is An Equal Opportunity Employer. Avanade prohibits discrimination and harassment against any employee or applicant for employment because of race, color, age, religion, sex, national origin, gender identity or expression, sexual orientation, disability, veteran, military or marital status, genetic information or any other protected status."
105,Data Engineer,"Oakbrook Terrace, IL",Oakbrook Terrace,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"SIRVA is a leading partner for corporations to outsource their mobility needs, relocating and moving their executives and staff globally. SIRVA offers an extensive portfolio of mobility services across approximately 170 countries providing an end-to-end solution to deliver an enhanced mobility experience and program control and security for customers. SIRVA has a portfolio of well-known and recognizable brands including Allied Van Lines, northAmerican Van Lines, SMARTBOX, and Allied Pickfords. For more information please visit www.sirva.com.

SIRVA brings together strong, collaborative people in a dynamic culture of mutual respect, support, and passion for the brand and product. We believe innovation drives winning performance, and we constantly challenge ourselves to be the very best we can in every aspect of our business. You will be surrounded by some of the brightest and most driven people in the industry. At SIRVA, you will be in great company!

The Data Engineer, which is an emerging role in the SIRVA data and analytics team, will play a pivotal role in operationalizing the most-urgent data and analytics initiatives for SIRVA digital business initiatives. The bulk of the data engineer’s work would be in building, managing and optimizing data pipelines and then moving these data pipelines effectively into production for key data and analytics consumers (like business/data analysts, data scientists or any personal that needs curated data for data and analytics use cases). Data engineers also be responsible to ensure compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines.
FUNCTIONS AND RESPONSIBILITIES
10% -Create functional & technical documentation - e.g. Data pipelines, source to target mappings, ETL specification documents, run books
80% -Build data pipelines, apply automation in data integration and management, tracking data consumption patterns, performing intelligent sampling and caching
10% -Tests, debugs, and documents Pipelines and data integration processes, SQL and/or no-SQL queries QUALIFICATIONS AND PREFERRED SKILLS

Strong experience with advanced analytics tools for Object-oriented/object function scripting using languages such as [R, Python, Java, Scala, others]. • Strong ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management. The ability to work with both IT and business in integrating analytics and data science output into business processes and workflows. • Strong experience with popular database programming languages including [SQL, PL/SQL, others] for relational databases and certifications on upcoming [NoSQL/Hadoop oriented databases like HBase, Cassandra, others] for nonrelational databases. • Strong experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies. These should include [ETL/ELT, data replication/CDC, message-oriented data movement, API design and access] and upcoming data ingestion and integration technologies such as [stream data integration, CEP and data virtualization]. • Strong experience in working with SQL on Hadoop tools and technologies including [HIVE, Impala, Presto, others] from an open source perspective and [Hortonworks Data Flow (HDF), Talend, others] from a commercial vendor perspective. • Strong experience in working with and optimizing existing ETL processes and data integration and data preparation flows and helping to move them in production. • Basic experience working with popular data discovery, analytics and BI software tools like [Sisense, Tableau, Qlik, PowerBI and others] for semantic-layer-based data discovery. • Strong experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms. • Basic understanding of popular open-source and commercial data science platforms such as [Python, R, KNIME, Alteryx, others] is a strong plus but not required/compulsory. • Demonstrated ability to work across multiple deployment environments including [cloud, on-premises and hybrid], multiple operating systems. • Adept in agile methodologies and capable of applying DevOps and increasingly DataOps principles to data pipelines to improve the communication, integration, reuse and automation of data flows between data managers and consumers across an organization • Has good judgment, a sense of urgency and has demonstrated commitment to high standards of ethics, regulatory compliance, customer service and business integrity. • Strong experience supporting and working with cross-functional teams in a dynamic business environment.

EDUCATION AND CERTIFICATION REQUIREMENTS

A bachelor's or master's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is required. • Advanced degree in computer science, statistics, applied mathematics is preferred.

SIRVA is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, military status, genetic information or any other consideration made unlawful by applicable federal, state, or local laws. SIRVA also prohibits harassment of applicants and employees based on any of these protected categories. It is also SIRVA's policy to comply with all applicable state and federal laws respecting consideration of unemployment status in making hiring decisions. The Federal EEO Law Poster may be found at http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf If you need a reasonable accommodation because of a disability of any part of the employment process, please send an email to Human Resources at HRSIRVA@SIRVA.com and let us know the nature of your request and your contact information."
106,Data Engineer Intern,"Naperville, IL 60563",Naperville,IL,60563,None Found,"Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.
Interest in Machine Learning algorithms, techniques and methodologies and growing in that space
Familiarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies
Familiarity with one or more general purpose programming language including but not limited to Python and C++.
Demonstrated ability to learn quickly and has a passion for emerging digital technologies
Comfortable with communicating clearly and concisely across a variety of audiences including technology and business.
Ability to think strategically as well as tactically in order to drive ideas into action.
Ability to collaborate in a team-oriented, dynamic environment","Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.
Interest in Machine Learning algorithms, techniques and methodologies and growing in that space
Familiarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies
Familiarity with one or more general purpose programming language including but not limited to Python and C++.
Demonstrated ability to learn quickly and has a passion for emerging digital technologies
Comfortable with communicating clearly and concisely across a variety of audiences including technology and business.
Ability to think strategically as well as tactically in order to drive ideas into action.
Ability to collaborate in a team-oriented, dynamic environment","Conceptual, analytical thinker, with a real passion for data exploration and design
Exposure to business intelligence which includes Databases, ETL, Machine Learning, and Data Mining
Participate in hands-on technical projects and enthusiastic to tackle problems supporting our team.
Shows great attention to detail",None Found,"Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.
Interest in Machine Learning algorithms, techniques and methodologies and growing in that space
Familiarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies
Familiarity with one or more general purpose programming language including but not limited to Python and C++.
Demonstrated ability to learn quickly and has a passion for emerging digital technologies
Comfortable with communicating clearly and concisely across a variety of audiences including technology and business.
Ability to think strategically as well as tactically in order to drive ideas into action.
Ability to collaborate in a team-oriented, dynamic environment","Overview
KeHE-a natural, organic, specialty and fresh food distributor-is all about ""good"" and is growing, so there's never been a more exciting time to join our team. If you're enthusiastic about working in an environment with a people-first culture and an organization committed to good living, good food and good service, we'd love to talk to you!

Where KeHE goes, goodness follows. KeHE has grown tremendously over the years by exceeding expectations and bringing goodness in all we do. We’re now bringing that same goodness to our workforce by creating friction-free processes.
We’re focused on simple and intuitive ways to work. We’re building systems where our employees come together each day and do what truly makes us human; collaborate, share, and connect.
You’ll have the opportunity to expand and apply your skills in new and interesting ways. And you’ll have fun doing it. Join a team where our collective intelligence, collective effort, and collective passion is brought together to do something worth doing!
Primary Responsibilities
KeHE Distributors is looking for ambitious and energetic people to join our Summer 2020 Technology Internship Program. A technology internship at KeHE will help you develop the skills you need to help you thrive throughout your career. Working alongside the best technologists in the industry, you will perform relevant work with direct impact on our business and customers. The Internship Program also provides opportunities for interns to tour a warehouse, participate in a trade show, and opportunities for networking.

Opportunities available in the corporate office in Naperville, IL, lasting between 10-12 weeks long. Our technology organization focuses on new, visionary ideas. You will learn about KeHE's continuous research, effective deployment, and rapid adoption of groundbreaking technology platforms. KeHE prides itself on its culture and values. During your time as KeHE technical intern, you will begin to build a foundation around understanding our culture and values as well as receive an introduction to the food distribution industry and the technologies that support it.
Essential Functions
Conceptual, analytical thinker, with a real passion for data exploration and design
Exposure to business intelligence which includes Databases, ETL, Machine Learning, and Data Mining
Participate in hands-on technical projects and enthusiastic to tackle problems supporting our team.
Shows great attention to detail
Minimum Requirements, Qualifications, Additional Skills, Aptitude
Pursuing a Bachelor's degree in Technology, with a focus on Machine Learning and / or Data Science.
Interest in Machine Learning algorithms, techniques and methodologies and growing in that space
Familiarity with Semi-Structured, and Unstructured databases, algorithms, techniques and methodologies
Familiarity with one or more general purpose programming language including but not limited to Python and C++.
Demonstrated ability to learn quickly and has a passion for emerging digital technologies
Comfortable with communicating clearly and concisely across a variety of audiences including technology and business.
Ability to think strategically as well as tactically in order to drive ideas into action.
Ability to collaborate in a team-oriented, dynamic environment
Requisition ID
2019-5992"
107,Data Engineer,"Chicago, IL 60604",Chicago,IL,60604,None Found,None Found,None Found,"Prioritizes and executes rapid raw data collection from source systems, targets and implements efficient storage of, employs fast and reliable access patterns.
Understands system protocols, how systems operate and data flows. Aware of current and emerging technology tools and their benefits. Expected to independently develop a full software stack. Understands the building blocks, interactions, dependencies, and tools required to complete software and automation work. Independent study of evolving technology is expected.
Drives engineering projects by developing software solutions; conducting tests and inspections; building reports and calculations.
Strong focus on innovation and enablement, contributes to designs to implement new ideas which improve an existing and new system/process/service. Understands and can apply new industry perspectives to our existing business and data models. Reviews existing designs and processes to highlight more efficient ways to complete existing workload more effectively through industry perspectives.
Maintains knowledge of existing technology documents. Writes basic documentation on how technology works using collaboration tools like Confluence. Creates clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption and consideration at the manager level.
Collaborates with technical teams and utilizes system expertise to deliver technical solutions. Continuously learns and teaches others existing and new technologies. Contributes to the development of others through mentoring or in-house workshops and learning sessions.
Drives team practices and procedures to achieve repeatable success and defined expectation of services
Provides a significant collaborative role in long-term department planning, with focus on initiatives achieving data empowerment, operational efficiency and sustainability
Monitors and evaluates overall strategic data infrastructure; tracks system efficiency and reliability; identifies and recommends efficiency improvements and mitigates operational vulnerabilities.
",None Found,"Bachelor’s degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.
Prior experience in Capital Markets strongly preferred.
5+ years of experience developing software in a professional environment (preferably financial services but not required)
3 years of hands on Data Driven Enterprise Application development, preferable in financial industry
Strong understanding of Enterprise architecture patterns, Object Oriented & Service Oriented principles, design patterns, industry best practices
Foundational knowledge of data structures, algorithms, and designing for performance.
Proficiency in programming in Java, C# or Python and willingness to learn and adopt new languages as necessary
Experience in database technology like MSSQL and one of key value and document databases like MongoDb, Dynamo Db, Casandra.
Exposure to containers, microservices, distributed systems architecture, orchestrators and cloud computing.
Comfortable with core programming concepts and techniques (e.g. concurrency, memory management)
Enjoys working with algorithms and data structures (e.g. trees, hash maps, queues)
Data Analytics and Data Science experience will be a plus.
Good sense of user interaction and usability design to provide an intuitive, seamless end user experience.
Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts.
Ability to work and potentially lead in an Agile methodology environment.
","Location: Chicago, IL Job Code: HCR# 2534
Description
Position Purpose:
The Data Engineer is responsible for empowering the Data team to achieve its primary objectives: ingesting, mastering and exposing real-time, event-driven data streams pertaining to the firm’s data assets. The ideal candidate will exhibit passion for continuous improvement and a dedicated focus on enabling consumers to achieve their goals by making data driven decisions.
Primary Accountabilities/Responsibilities:
Prioritizes and executes rapid raw data collection from source systems, targets and implements efficient storage of, employs fast and reliable access patterns.
Understands system protocols, how systems operate and data flows. Aware of current and emerging technology tools and their benefits. Expected to independently develop a full software stack. Understands the building blocks, interactions, dependencies, and tools required to complete software and automation work. Independent study of evolving technology is expected.
Drives engineering projects by developing software solutions; conducting tests and inspections; building reports and calculations.
Strong focus on innovation and enablement, contributes to designs to implement new ideas which improve an existing and new system/process/service. Understands and can apply new industry perspectives to our existing business and data models. Reviews existing designs and processes to highlight more efficient ways to complete existing workload more effectively through industry perspectives.
Maintains knowledge of existing technology documents. Writes basic documentation on how technology works using collaboration tools like Confluence. Creates clear documentation for new code and systems used. Documenting systems designs, presentations, and business requirements for consumption and consideration at the manager level.
Collaborates with technical teams and utilizes system expertise to deliver technical solutions. Continuously learns and teaches others existing and new technologies. Contributes to the development of others through mentoring or in-house workshops and learning sessions.
Drives team practices and procedures to achieve repeatable success and defined expectation of services
Provides a significant collaborative role in long-term department planning, with focus on initiatives achieving data empowerment, operational efficiency and sustainability
Monitors and evaluates overall strategic data infrastructure; tracks system efficiency and reliability; identifies and recommends efficiency improvements and mitigates operational vulnerabilities.

Job Requirements:
Bachelor’s degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.
Prior experience in Capital Markets strongly preferred.
5+ years of experience developing software in a professional environment (preferably financial services but not required)
3 years of hands on Data Driven Enterprise Application development, preferable in financial industry
Strong understanding of Enterprise architecture patterns, Object Oriented & Service Oriented principles, design patterns, industry best practices
Foundational knowledge of data structures, algorithms, and designing for performance.
Proficiency in programming in Java, C# or Python and willingness to learn and adopt new languages as necessary
Experience in database technology like MSSQL and one of key value and document databases like MongoDb, Dynamo Db, Casandra.
Exposure to containers, microservices, distributed systems architecture, orchestrators and cloud computing.
Comfortable with core programming concepts and techniques (e.g. concurrency, memory management)
Enjoys working with algorithms and data structures (e.g. trees, hash maps, queues)
Data Analytics and Data Science experience will be a plus.
Good sense of user interaction and usability design to provide an intuitive, seamless end user experience.
Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts.
Ability to work and potentially lead in an Agile methodology environment.

Physical requirements/Working conditions:
Climate controlled office environment
Minimal physical requirements other than occasional light lifting of boxed materials • Dynamic, time-sensitive, trade room environment
Travel as needed

We encourage applicants of all ages and experience, as we do not discriminate on the basis of the applicant's age."
108,Data Engineer,"Chicago, IL 60601",Chicago,IL,60601,None Found,None Found,None Found,None Found,None Found,None Found,"C-K (Cramer-Krasselt) is one of the largest independent, totally integrated agencies in the country with over $700 million in billings, almost $400 million in media assets under management and 61% of revenue from digital.

With a mission to Make Friends, Not Ads®, C-K has built a reputation for changing perception and behaviors that lead to purchasing action for brands. It’s how we helped Porsche achieve seven years of consecutive record-breaking growth, how Corona became the #1 import and Pacifico grew 19% in a single year.

We’ve done it by interconnecting an ever-expanding range of disciplines from strategic branding to digital, social, analytics, media/programmatic, SEM, PR, UX and more.
Major brands include Benihana, Cedar Fair Entertainment Company (Knott’s Berry Farm, Cedar Point and nine others), Cintas, Corona Extra, Edward Jones, Kroger Divisions, Pacifico Beer and Porsche.
About C-K Chicago:

Chicago’s independent spirit makes it the perfect headquarters for C-K, one of the nation’s oldest and largest independent agencies. C-K’s Chicago office, located just off the Magnificent Mile, is full of native Midwesterners and transplants alike that are united in the values of hard work and collaboration.
At C-K in Chicago, we work with major global and national brands like Porsche, Corona Extra, Cedar Fair, BIC, Edward Jones and many others. We’re proud of these partnerships and the integrated approach we take to every client and every element of a campaign, be it large or small.
Our experts here have backgrounds spanning every marketing, advertising and communications discipline across all categories imaginable. C-K Chicago is a hub of innovation and the nerve center of the agency.
We also apply our work ethic to the community. Our flagship charity in the Chicago office is Off the Street Club (OTSC), the oldest boys and girls club in the city. Throughout the year we participate in the planning and execution of several key OTSC fundraising events such as Swing for the Kids, The Firefly Ball, Summer Bake Sale and Holiday Luncheon. We also participate in the Battle for Hope, the Chicago-area advertising agency battle of the bands. In 2015, our house band, the Angry Pickles, won the Battle for Hope and helped raise thousands for OTSC in the process.


The Data Engineer will partner with data scientists and analysts to develop data pipelines and data services that address agency needs. The candidate should be comfortable exploring, recommending and building solutions for extracting, transforming, and loading data from various disparate sources (APIs, databases, flat files, etc.).

About the role:
Collaborate with analysts and data scientists to understand data needs and arrive at implementation requirements.
Design and develop data pipeline architecture.
Maintain and enhance existing data services and database infrastructure.
Define and manage standards for performance, reliability, and monitoring data pipelines, and databases.
May require overnight travel up to 5%.
About you:
2-3 years of experience working with a data lake/data warehouse.
Experience building and maintaining data pipelines.
Familiar working with RDBMS, normalized data modeling.
Skilled at query writing and database optimization skills.
Strong knowledge with at least one of: Python, Java, Scala, Ruby or Go.
Comfortable working within Linux command-line and Linux servers.
Ability to adapt to a quickly changing environment and work on a broad project/pipeline base with diverse needs and functional responsibilities.
Ability to communicate technical ideas, solutions, and issues with technical and non-technical team members.
You enjoy getting to the root of client business problems, thinking critically about solutions to them, and developing those solutions."
109,Google Technical Architect,"Chicago, IL",Chicago,IL,None Found,None Found,"Minimum 5 years of Consulting or client service delivery experience on Google GCP
",DevOps on an GCP platform. Multi-cloud experience a plus.,None Found,None Found,"Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google Cloud Platform (GCP) Technical Architect Delivery is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would also be responsible for developing and delivering Google GCP cloud solutions to meet todays high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Google GCP Technical Architect is a highly performant GCP Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data soltuions on cloud. Using Google GCP public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications.

Role & Responsibilities:Work with Sales and Bus Dev teams in providing Data and GCP Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
Minimum 5 years of Consulting or client service delivery experience on Google GCP
Minimum 10 years of experience in big data, database and data warehouse architecture and delivery
Bachelors degree or 12 years previous professional experience
Able to travel 100% (M-TH)
Minimum of 5 years of professional experience in 2 of the following areas:
Solution/technical architecture in the cloud
Big Data/analytics/information analysis/database management in the cloud
IoT/event-driven/microservices in the cloud
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc.:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Tensorflow & Sheets

Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Certified GCP Solutions Architect - Associate
Certified GCP Solutions Architect – Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)
Certified GCP AI/ML Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP
Strong in Java, C##, Spark, PySpark, Unix shell/Perl scripting
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
- Multi-cloud experience beyond GCP a plus - AWS and Azure

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
110,Technical Data Engineer Lead,"Chicago, IL 60606",Chicago,IL,60606,None Found,"
Must have a Bachelor’s degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Master’s degree (preferred) in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.
Understand Hadoop cluster administration concepts.
3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.
Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.
Must have experience with batch and real-time data pipelines.
Must have experience as a Hadoop Technical Lead / Architect
Must have experience with design, development and deployment in the specified technologies.
Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.
Writing complex SQL queries, extracting and importing large amounts of data.
Must be willing to work in a fast-paced environment with an on shore – off shore distributed Agile teams.
Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.
Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.
Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.
Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through",None Found,"
Lead a development team of data engineers
Implement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases (Hbase) and EMR (Hadoop)
Responsible for design, development, testing oversight and implementation
Works closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks
Coordinate with data engineers and API developers to drive program delivery.
Drive technical development and application standards across enterprise data lake
Benchmark and debug critical issues with algorithms and software as they arise.
Lead and assist with the technical design and implementation of the Big Data cluster in various environments.
Guide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts.
Perform other duties and/or special projects as assigned",None Found,"
Must have a Bachelor’s degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Master’s degree (preferred) in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.
Understand Hadoop cluster administration concepts.
3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.
Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.
Must have experience with batch and real-time data pipelines.
Must have experience as a Hadoop Technical Lead / Architect
Must have experience with design, development and deployment in the specified technologies.
Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.
Writing complex SQL queries, extracting and importing large amounts of data.
Must be willing to work in a fast-paced environment with an on shore – off shore distributed Agile teams.
Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.
Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.
Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.
Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through","Job Description:
Role Summary/Purpose:
We are looking for a Technical Data Engineer Lead to lead the development of consumer-centric low latency analytic environment leveraging Big Data technologies and transform the legacy systems.
Essential Responsibilities:
Lead a development team of data engineers
Implement a big data enterprise data lake, BI and analytics system using Hive LLAP, Spark, Kafka, Sqoop, Hive, Sqoop, NoSQL databases (Hbase) and EMR (Hadoop)
Responsible for design, development, testing oversight and implementation
Works closely with program manager, scrum master, and architects to convey technical impacts to development timeline and risks
Coordinate with data engineers and API developers to drive program delivery.
Drive technical development and application standards across enterprise data lake
Benchmark and debug critical issues with algorithms and software as they arise.
Lead and assist with the technical design and implementation of the Big Data cluster in various environments.
Guide/mentor development team for example to create custom common utilities/libraries that can be reused in multiple big data development efforts.
Perform other duties and/or special projects as assigned
Qualifications/Requirements:
Must have a Bachelor’s degree in Computer Science, Engineering, or a related field, plus 4 years of related work experience in the IT industry; OR a Master’s degree (preferred) in Computer Science, Engineering, or a related field, plus 2 years of related work experience in the IT industry with Big Data Technologies.
Understand Hadoop cluster administration concepts.
3+ years of hands-on experience with large scale Big Data environments such as Hortonworks, Cloudera.
Must have experience in Java, Spark, Map-Reduce, RDBMS, Hive, Pig, Hbase, Kafka, Scala, Python, Linux/Unix technologies.
Must have experience with batch and real-time data pipelines.
Must have experience as a Hadoop Technical Lead / Architect
Must have experience with design, development and deployment in the specified technologies.
Must have strong experience with OOPS concepts, design principles, and patterns in Scala, java and Python.
Writing complex SQL queries, extracting and importing large amounts of data.
Must be willing to work in a fast-paced environment with an on shore – off shore distributed Agile teams.
Must have strong technical background and hands-on experience in building enterprise-wide data warehouse solutions.
Must have the ability to develop and maintain strong collaborative relationships at all levels across IT and Business Stakeholders.
Must have the ability to prioritize multiple tasks and deal with urgent requests in a fast-paced and demanding environment.
Excellent written and oral communication skills. Adept and presenting complex topics, influencing and executing with timely / actionable follow-through
Desired Characteristics:
Extensive experience working with data warehouses and big data platforms
Demonstrated experience building strong relationships with senior leaders
Strong leadership and influencing skills
Outstanding written and verbal skills and the ability to influence and motivate teams
Eligibility Requirements:
You must be 18 years or older
You must have a high school diploma or equivalent
You must be willing to take a drug test, submit to a background investigation and submit fingerprints as part of the onboarding process
You must be able to satisfy the requirements of Section 19 of the Federal Deposit Insurance Act.
New hires (Level 4-7) must have 9 months of continuous service with the company before they are eligible to post on other roles. Once this new hire time in position requirement is met, the associate will have a minimum 6 months’ time in position before they can post for future non-exempt roles. Employees, level 8 or greater, must have at least 24 months’ time in position before they can post. All internal employees must have at least a “consistently meets expectations” performance rating and have approval from your manager to post (or the approval of your manager and HR if you don’t meet the time in position or performance requirement).
Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
Reasonable Accommodation Notice:
Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.
If you need special accommodations, please call our Career Support Line so that we can discuss your specific situation. We can be reached at 1-866-301-5627. Representatives are available from 8am – 5pm Monday to Friday, Central Standard Time.
The salary range for this position is 85,000.00 - 170,000.00 USD Annual
Salaries are adjusted according to market in CA and Metro NY and some positions are bonus eligible.
Grade/Level: 12
Job Family Group:
Information Technology"
111,Senior Big Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Integral Ad Science (IAS) is a global technology and data company that builds verification, optimization, and analytics solutions for the advertising industry and we're looking for a Senior Big Data Engineer to join our Data Engineering team. If you are excited by technology that has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day; and evaluate thousands of data-points in real-time all while responding in just a few milliseconds, then IAS is the place for you!

Our platform is the engine that powers the verification, optimization, and analytics solutions we provide. It has the power to handle hundreds of thousands of transactions per second; collect tens of billions of events each day and evaluate thousands of data-points in real-time all while responding in just a few milliseconds.

What you'll do:

Working on Big Data technologies such as Hadoop, MapReduce, Kafka, and/or Spark in columnar databases
Architect, design, code and maintain components for aggregating tens of billions of daily transactions
Lead the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for streaming and batch ETL's and RESTful API's
Mentor junior team members

Who you are and what you have:

5+ years of recent hands-on Java experience
Strong knowledge of collections, multi-threading, JVM memory model, etc.
Great understanding of designing for performance, scalability, and reliability
Superb understanding of algorithms, scalability and various tradeoffs in a Big Data setting
In-depth understanding of object oriented programming concepts
Excellent interpersonal and communication skills
Understanding of full software development life cycle, agile development and continuous integration
Good knowledge of Linux command line tools
Experience with Hadoop MapReduce, Spark, Pig
Solid understanding of database fundamentals, good knowledge of SQL

What puts you over the top:

Exposure to messaging frameworks like Kafka or RabbitMQ
Some exposure to functional programming languages like Scala
Experience with Spark

About Integral Ad Science

Integral Ad Science (IAS) is the global market leader in digital ad verification, offering technologies that drive high-quality advertising media. IAS equips advertisers and publishers with both the insight and technology to protect their advertising investments from fraud and unsafe environments as well as to capture consumer attention, and drive business outcomes. Founded in 2009, IAS is headquartered in New York with global operations in 17 offices across 13 countries. IAS is part of the Vista Equity Partners portfolio of software companies. For more on how IAS is powering great impressions for top publishers and advertisers around the world, visit integralads.com ( http://integralads.com/ ).

Equal Opportunity Employer:
IAS is an equal opportunity employer, committed to our diversity and inclusiveness. We will consider all qualified applicants without regard to race, color, nationality, gender, gender identity or expression, sexual orientation, religion, disability or age. We strongly encourage women, people of color, members of the LGBTQIA community, people with disabilities and veterans to apply.

To learn more about us, please visit http://integralads.com/ ( http://integralads.com/ ) and https://muse.cm/2t8eGlN ( https://muse.cm/2t8eGlN )

Attention agency/3rd party recruiters: IAS does not accept any unsolicited resumes or candidate profiles. If you are interested in becoming an IAS recruiting partner, please send an email introducing your company to recruitingagencies@integralads.com. We will get back to you if there's interest in a partnership."
112,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Your Future Evolves Here
Evolent Health has a bold mission to change the health of the nation by changing the way health care is delivered. Our pursuit of this mission is the driving force that brings us to work each day. We believe in embracing new ideas, challenging ourselves and failing forward. We respect and celebrate individual talents and team wins. We have fun while working hard and Evolenteers often make a difference in everything from scrubs to jeans.
Are we growing? Absolutely—56.7% in year-over-year revenue growth in 2016. Are we recognized? Definitely. We have been named one of “Becker’s 150 Great Places to Work in Healthcare” in 2016 and 2017, and one of the “50 Great Places to Work” in 2017 by Washingtonian, and our CEO was number one on Glassdoor’s 2015 Highest-Rated CEOs for Small and Medium Companies. If you’re looking for a place where your work can be personally and professionally rewarding, don’t just join a company with a mission. Join a mission with a company behind it.

We are looking for a Data Engineer in our Payor Data Services department. This position involves the programming and analysis of healthcare data with an emphasis on payer data, coding and data analytics.
What you'll be doing:
Utilize SQL programs to build metadata for various data feeds
Develop SAS programs (once trained) to integrate and analyze payer data from multiple sources
Load and synthesize healthcare data from multiple sources
Review data requirements/design and implement logic to achieve data needs
Implement and develop data quality control protocols and monitor their impact
Assist in designing, programming and standardizing processes and reports
The Experience you'll need (must haves):
Competency in use of SQL language and scripting to load SQL Server environments
Familiarity with SAS and a desire to expand SAS programming knowledge is a plus
Competency in data manipulation and analysis: accessing raw data in varied formats with different methods and analyzing and processing data
Must be analytical, detail oriented, and possess desire to advance and grow personally and professionally
Ability to multi-task and manage multiple projects with varying timelines
Must have a passion for data and healthcare
BS, BA, or Masters in Computer Science, Data Analytics, Informatics, or a comparable program with a quantitative emphasis.
4+ years of SAS and SQL programming experience, including SAS macros, PROC SQL, and/or Enterprise Guide
Finishing Touches (preferred)
Experience in healthcare

Evolent Health is an equal opportunity employer and considers all qualified applicants equally without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin."
113,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About JLL

We’re JLL. We’re a professional services and investment management firm specializing in real estate. As a Fortune 500 company, we help real estate owners, occupiers and investors achieve their business ambitions. We have nearly 300 corporate offices across 80 countries, with a team of more than 86,000 individuals. If you’re looking to step up your career, JLL is the perfect professional home. With us, you’ll have a chance to innovate with the world’s leading businesses, put that expertise into action on landmark projects, and work on game-changing real estate initiatives. You’ll also make long-lasting professional connections and be inspired by the best. We’re focused on opportunity and want to help you make the most of yours. Achieve your ambitions—join us at JLL!

What this job involves

We are looking for a Data Engineer who is self-starter to work in a diverse and fast-paced environment that can join our Enterprise Data team. This is an individual contributor role that is responsible for architecting, designing and developing of data solutions that are strategic for the business and built on the latest technologies and patterns. This a global role that requires partnering with the broader JLL Technology, Data, and Information Management (TDIM) team at the country, regional and global level by utilizing in-depth knowledge of data, infrastructure, technologies and data engineering experience.


Contributes to the design of information infrastructure, and data management processes to move the organization to a more sophisticated, agile and robust target state data architecture
Develop systems that ingest, cleanse and normalize diverse datasets, develop data pipelines from various internal and external sources and build structure for previously unstructured data
Interfaces with internal colleagues and external professionals to determine requirements, anticipate future needs, and identify areas of opportunity to drive data development
Develop good understanding of how data will flow & stored through an organization across multiple applications such as CRM, Broker & Sales, Finance, HR, MDM, ODS, Data Lake, & EDW
Develop data solutions that enable non-technical staff to make data-driven decisions
Design & develop data management and data persistence solutions for application use cases leveraging relational, non-relational databases and enhancing our data processing capabilities
Develop POCs to influence platform architects, product managers and software engineers to validate solution proposals and migrate
Architect and develop data lake solution to store structured and unstructured data from internal and external sources and provide technical guidance to help migrate colleagues to modern technology platform

Sound like you? To apply you need to be:

Bachelor’s degree in Information Science, Computer Science, Mathematics, Statistics or a quantitative discipline in science, business, or social science.
Hands-on engineering lead who is curious about technology, should be able to quickly adopt to change and one who understands the technologies supporting areas such as Cloud Computing (AWS, Azure(preferred), etc.), Micro Services, Streaming Technologies, Network, Security, etc.
3 to 5 years of experience as a data developer using Python-spark, Kafka, Spark Streaming, Azure SQL Server, Cosmos DB/Mongo DB, Azure Event Hubs, Azure Data Lake Storage, Azure Search etc.
Hands-on Experience for building Data Pipelines in Cloud and well versed with CICD and DevOps process.
Design & develop data management and data persistence solutions for application use cases leveraging relational, non-relational databases and enhancing our data processing capabilities.
Experience handling un-structured data, working in a data lake environment, leveraging data streaming and developing data pipelines driven by events/queues
Experience building and maintaining a data warehouse/ data lake in a production environment with efficient ETL design, implementation, and maintenance
Team player, Reliable, self-motivated, and self-disciplined individual capable of executing on multiple projects simultaneously within a fast-paced environment working with cross functional teams

What you can expect from us

We’re an entrepreneurial, inclusive culture. We succeed together—across the desk and around the globe. We believe the best inspire the best, so we invest in supporting each other, learning together and celebrating our success.
Our Total Rewards program reflects our commitment to helping you achieve your ambitions in career, recognition, well-being, benefits and pay. We’ll offer you a competitive salary and benefits package.
With us, you’ll develop your strengths and enjoy a career full of varied experiences. We can’t wait to see where your ambitions take you at JLL.

Apply today!

Apply quoting reference [REQ74412] at jll.com/careers.

#LI #dibot"
114,"Cloud Data Engineer, Google Professional Services","Chicago, IL 60607",Chicago,IL,60607,None Found,None Found,None Found,"
Act as a trusted technical advisor to customers and solve complex Big Data challenges.
Create and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders.
Travel up to 30% of the time.
Communicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.",None Found,None Found,"Note: By applying to this position your application is automatically submitted to the following locations: Chicago, IL, USA; Atlanta, GA, USA
Minimum qualifications:

BA/BS degree in Computer Science, Mathematics or related technical field, or equivalent practical experience.
Experience with data processing software (such as Hadoop, Spark, Pig, Hive) and with data processing algorithms (MapReduce, Flume).
Experience in writing software in one or more languages such as Java, C++, Python, Go and/or JavaScript.
Experience managing internal or client-facing projects to completion; experience troubleshooting clients' technical issues; experience working with engineering teams, sales, services, and customers.

Preferred qualifications:
Experience working data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments.
Experience in technical consulting.
Experience working with big data, information retrieval, data mining or machine learning as well as experience in building multi-tier high availability applications with modern web technologies (such as NoSQL, MongoDB, SparkML, Tensorflow).
Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments.
About the job
As a Cloud Data Engineer, you'll guide customers on how to ingest, store, process, analyze and explore/visualize data on the Google Cloud Platform. You will work on data migrations and transformational projects, and with customers to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential platform challenges.
In this role you are the Google Engineer working with Google's most strategic Cloud customers. Together with the team you will support customer implementation of Google Cloud products through: architecture guidance, best practices, data migration, capacity planning, implementation, troubleshooting, monitoring and much more.
The Google Cloud Platform team helps customers transform and evolve their business through the use of Google’s global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you will help shape the future of businesses of all sizes use technology to connect with customers, employees and partners.
Responsibilities
Act as a trusted technical advisor to customers and solve complex Big Data challenges.
Create and deliver best practices recommendations, tutorials, blog articles, sample code, and technical presentations adapting to different levels of key business and technical stakeholders.
Travel up to 30% of the time.
Communicate effectively via video conferencing for meetings, technical reviews and onsite delivery activities.
At Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form."
115,Data Engineer,"Chicago, IL 60611",Chicago,IL,60611,None Found,None Found,None Found,None Found,None Found,None Found,"Strong Analytics is seeking a data engineer to collaborate with our team building and managing ETL pipelines, embedding statistical algorithms in robust applications, and deploying machine learning applications to the cloud.

This role requires high-level Python programming, high-level SQL, experience deploying applications to AWS, and experience in distributed data processing (e.g., Hadoop, Spark, Hive).

We offer a comprehensive compensation package, including:
Competitive salary
Profit sharing or equity, based on experience
Health insurance
Four weeks paid vacation
Work-from-Home Wednesdays

Candidates will be evaluated based on their skills with the following technologies/workflows:
Python
SQL
Relational Databases (e.g., Postgres, MySQL)
Distributed Computation (e.g., Spark, Hive, Hadoop)
Deploying and Managing Cloud Services (we use AWS/Azure)
Interacting with and building RESTful APIs
Git
All applicants will be considered based on their experience and demonstrated skill/aptitude, not formal education.

Applicants should have the ability to travel infrequently (<5% of your time) for team meetings, conferences, and occasional client site visits."
116,Data Engineer,"Chicago, IL 60626",Chicago,IL,60626,None Found,None Found,None Found,"Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Microsoft Azure technologies such as Azure Data Factory and Databricks.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Craft analytics tools that utilize the data pipeline to deliver actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with partners including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Orchestrate large, complex data sets that meet functional / non-functional business requirements.
Seek out, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
Partner with data and analytics talent to strive for greater functionality in our data systems.",None Found,"Interested candidates must submit an application and resume/CV online to be considered
Must be 18 years of age or older
Must be willing to submit to a background investigation; any offer of employment is conditioned upon the successful completion of a background investigation
Must have unrestricted work authorization to work in the United States. For U.S. employment opportunities, Gallagher hires U.S. citizens, permanent residents, asylees, refugees, and temporary residents. Temporary residence does not include those with non-immigrant work authorization (F, J, H or L visas), such as students in practical training status. Exceptions to these requirements will be determined based on shortage of qualified candidates with a particular skill. Gallagher will require proof of work authorization
Must be willing to execute Gallagher's Employee Agreement or Confidentiality and Non-Disclosure Agreement, which require, among other things, post-employment obligations relating to non-solicitation, confidentiality and non-disclosure","Gallagher is a global leader in insurance, risk management and consulting services. We help businesses grow, communities thrive and people prosper. We live a culture defined by The Gallagher Way, our set of shared values and guiding tenets. A culture driven by our people, over 30,000 strong, serving our clients with customized solutions that will protect them and fuel their futures.

Position Summary:
Consider joining our growing team of data and analytics experts responsible for expanding our data and pipeline architecture, as well as optimizing data flow and collection for multiple functional teams. We support our data analysts and data scientists on data initiatives and ensure optimal data delivery architecture is consistent throughout ongoing projects. You will engage in supporting the data needs of multiple teams, systems and products. Do you find the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives most exciting? We really should explore together.

Essential Duties and Responsibilities:
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Microsoft Azure technologies such as Azure Data Factory and Databricks.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Craft analytics tools that utilize the data pipeline to deliver actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with partners including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Orchestrate large, complex data sets that meet functional / non-functional business requirements.
Seek out, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
Partner with data and analytics talent to strive for greater functionality in our data systems.


Required:
Bachelor's degree in Computer Science, Engineering or Information Systems Management or related field.
At least 5 years of data engineering experience/ETL experience leveraging tools such as Integration Services (SSIS), Azure Data Factory, Informatica and Ab Initio.
U.S. Eligibility Requirements:
Interested candidates must submit an application and resume/CV online to be considered
Must be 18 years of age or older
Must be willing to submit to a background investigation; any offer of employment is conditioned upon the successful completion of a background investigation
Must have unrestricted work authorization to work in the United States. For U.S. employment opportunities, Gallagher hires U.S. citizens, permanent residents, asylees, refugees, and temporary residents. Temporary residence does not include those with non-immigrant work authorization (F, J, H or L visas), such as students in practical training status. Exceptions to these requirements will be determined based on shortage of qualified candidates with a particular skill. Gallagher will require proof of work authorization
Must be willing to execute Gallagher's Employee Agreement or Confidentiality and Non-Disclosure Agreement, which require, among other things, post-employment obligations relating to non-solicitation, confidentiality and non-disclosure
Gallagher offers competitive salaries and benefits, including: medical/dental/vision plans, life and accident insurance, 401(K), employee stock purchase plan, educational expense reimbursement, employee assistance program, flexible work hours (availability varies by office and job function) training programs, matching gift program, and more.


Gallagher believes that all persons are entitled to equal employment opportunity and does not discriminate against nor favor any applicant because of race, sex, color, disability, national origin, religion, creed, age, marital status, citizenship, veteran status, gender, gender identity / expression, actual or perceived sexual orientation, or any other protected characteristic. Equal employment opportunity will be extended in all aspects of the employer-employee relationship, including, but not limited to, recruitment, hiring, training, promotion, transfer, demotion, compensation, benefits, layoff, and termination. In addition, Gallagher will make reasonable accommodations to known physical or mental limitations of an otherwise qualified applicant with a disability, unless the accommodation would impose an undue hardship on the operation of our business."
117,"Data Engineer, Python","Chicago, IL",Chicago,IL,None Found,None Found,None Found,"
Bachelor's Degree in computer science or equivalent experience required.
2+ years of experience in the design and development of data pipelines and tasks.
Good understanding of data warehousing concepts and dimensional data modeling.
Hands-on experience with troubleshooting performance issues and fine tuning SQL queries.
Experience in Python including in modules/libraries such as pandas, numpy, Flask, scikit-learn, and sci-py.
Proven experience extracting data from structured data sources (SQL, Excel, CSV files, Couchbase) and unstructured data sources
(Splunk, log files) both on-premise and in the cloud.
Experience consuming data from web services, REST and SOAP, HTML, XML and JSON.
Knowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation.
Experience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required.
Familiar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.
","
Design, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources.
Monitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues.
Perform unit testing, system integration testing and assist with user acceptance testing.
Adapt data components to accommodate changes in source data and new business requirements.
Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks.
Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.
Collaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.
",None Found,None Found,"GoHealth is looking for Data Engineers who will be responsible for the design, development, and delivery of data transformation tasks used in transforming data into a format that can be easily analyzed. We are seeking candidates who have experience in data analysis, collection, and optimization for the purpose of informing business decisions. The Data Engineer will work with other team members in owning data pipelines including execution, documentation, maintenance, and metadata management. In this role, you will also support the development of the data infrastructure necessary for full scale data science, predictive analytics and machine learning.

Responsibilities:

Design, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources.
Monitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of issues.
Perform unit testing, system integration testing and assist with user acceptance testing.
Adapt data components to accommodate changes in source data and new business requirements.
Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline tasks.
Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.
Collaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and deliver new data-related processes and/or reports.

Skills and Experience:

Bachelor's Degree in computer science or equivalent experience required.
2+ years of experience in the design and development of data pipelines and tasks.
Good understanding of data warehousing concepts and dimensional data modeling.
Hands-on experience with troubleshooting performance issues and fine tuning SQL queries.
Experience in Python including in modules/libraries such as pandas, numpy, Flask, scikit-learn, and sci-py.
Proven experience extracting data from structured data sources (SQL, Excel, CSV files, Couchbase) and unstructured data sources
(Splunk, log files) both on-premise and in the cloud.
Experience consuming data from web services, REST and SOAP, HTML, XML and JSON.
Knowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation.
Experience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required.
Familiar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.

Benefits and Perks:

Open vacation policy
401k program with company match
Medical, dental, vision, and life insurance benefits
Flexible spending accounts
Commuter and transit benefits
Professional growth opportunities
Casual dress code
Generous employee referral bonuses
Happy hours, ping-pong tournaments, and more company-sponsored events
Subsidized gym memberships
GoHealth is an Equal Opportunity Employer

"
118,"Data Engineer - Senior Consultant (Spark) - Chicago, IL","Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.

We are...

 • The Industry-recognized data and analytics leaders
 • Passionate problem solvers across a broad spectrum of technologies and industries
 • Value seekers for measurable business outcomes
 • Continuous learners through training and education
 • Focused on a work-life balance with an unlimited paid time off policy

Data engineers are challenged with building the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally. Our engagements typically target a variety of use cases across data engineering, data science, data governance, and visualization.
Data engineers deliver value through...
Hands-on, self-directed design and development of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools
Demonstration of technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven insights
Collaboration with team members, business stakeholders and data SMEs to elicit requirements and to develop business metrics and analytical insights
Internal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO
A data engineer's skills include, but are not limited to...
Bachelors Degree and 5+ years of work experience
5+ years of professional IT work experience
SQL, SQL, SQL!
2+ years of Spark
Programming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)
Linux / Windows (Command line)
Big Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop)
Cloud Platforms (AWS, Azure, Google Cloud Platform)
Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)
Data Integration Tools (Talend, DataStage, Informatica, SSIS)
Databases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)
Markup Languages (JSON, XML, YAML)
Code Management Tools (Git/GitHub, SVN, TFS)
DevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)
Testing / Data Quality (TDD, unit, regression, automation)
Solving complex data and technology problems
Leading technical teams of 2+ consultants
Ability to design components of a larger implementation
Excellent communication to narrate data driven insights and technical approach

If this sounds like you, let’s talk!

Candidates must be comfortable with a national travel model to client locations weekly (M-TH is typical).

Clarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.

 #LI-RC1
GLDR"
119,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"JOB RESPONSIBILITIES:
Develops data pipelines for traditional Data Warehousing and Big Data solutions
Assists Data Architecture with rapid prototyping, technologies research, performance testing
Participates in designs discussion with data architecture and system architecture to support decisions and design choices with data and experiment designs
Collaborates with remove team members in an agile development framework – scrum, sprints
PREFERRED SKILLS:
Bachelor’s degree in quantitative field, MS preferred
Ability to learn new technologies and experiment quickly
Good knowledge of CI/CD concepts – git, Jenkins, etc.
Several years of experience with data warehouse, analytics, ETL – exceptional command of sql in different flavors
Very strong knowledge of data modeling in relational DBs
Good programming skills in a high-level, general purpose programming language – java, scala, or python
Good communication skills, ability to work effectively with remote members of the team and collaborate over long-distance
Familiarity with big data technologies is a plus - Hadoop, spark
Experience in a cloud infrastructure is a plus – AWS, GCP, Azure
Generally proficient in performing data transformations via scripting, stored procedures, or an ETL framework
Good understanding of the 4Vs of data and development strategies for accommodating them in integration
Some experience developing and supporting all aspects of a big data cluster: Ingestion (rsync), Processing (Apache Nifi), Parsing (Java), integration (Python, Spark, Scale and PIG), data movement (SQOOP), workflow management (OOZIE and ActiveBatch), and querying (HIVE and Impala)
Some proficiency in at least one of the following programming languages: Java, JavaScript, Python, and Scala
Proficiency in Unix and Linux operating systems
Capable of navigating and working effectively in a DevOps model including leveraging related technologies: Jenkins, GitLab, etc….
Strong foundational experience with SQL
Experience and Education
A Bachelor’s Degree in Computer Science or related field required
Gogo’s worldwide inflight Wi-Fi services have made internet and video entertainment a regular part of flying. We are a diverse group of technologists, marketers, strategists, and any other function you can think of- all working together in extraordinary harmony. And that’s just the beginning.
We connect the aviation industry and its travelers with innovative technology and applications, and we do it all in a high-energy environment that welcomes the next challenge. Be prepared for a dynamic ride with people who are passionate about what they’re building.

Gogo is an equal opportunity employer and works in compliance with both federal and state laws. We are committed to the concept regarding Equal Employment opportunity. Qualified candidates will be considered for employment regardless of race, color, religion, age, sex, national origin, marital status, medical condition or disability. The EEO is the law and is available here.

Gogo participates in E-Verify. Details in English and Spanish. Right to Work Statement in English and Spanish."
120,Senior Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Kenna Security is revolutionizing cyber risk with a SaaS-based platform that uses data science to combine vulnerability data with exploit intelligence to measure risk, predict attacks and prioritize remediation. We are leading the way in helping enterprises reduce their risk while increasing their efficiency and preventing attacks. Kenna Security was recently named one of the top 10 hottest start-ups and named to the Inc. 500 fastest growing companies list.


We are looking for a Sr Engineer to join our growing Research and Data Science team. This person will be responsible for expanding and optimizing our data and data pipeline, as well as optimizing data flow and collection for cross-functional teams. The Engineer will support our data scientists, researchers and software engineers on data initiatives and will ensure optimal data delivery is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited to support our next generation of products and data driven initiatives.
RESPONSIBILITIES
Create and maintain optimal data pipeline.
Building functional prototypes to support Research team initiatives.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Work with data scientists and researchers to design, implement, extend, tune and scale Data Science and Machine Learning libraries, frameworks, algorithms, pipelines, and tooling
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and other ‘big data’ technologies.
Create tools for Research team members that assist them in building and optimizing our product.
Work with Engineering and Product to build interfaces that allow our platform to consume insights from Research and Data Science teams.
Research, Prototype, Test, & Implement new technologies
QUALIFICATIONS
Advanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Conceptual familiarity with common machine learning techniques: classification, regression, decision trees, etc.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with machine learning libraries and frameworks such as Tensorflow, Keras, and SciKit Learn.
Experience with relational SQL and NoSQL databases, including Postgres and MySQL.
Experience with storage/search and related logging/presentation techniques (e.g. Elasticsearch/Kibana,S3, fluentd, Solr/Lucene)
Experience with cloud services: EC2, EMR, RDS, Redshift, BigQuery.
Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.
Experience deploying and supporting software in a production SaaS environment.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex or national origin."
121,Senior Data Engineer – DBA (Data Base Admin),"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,"
BS degree in a computer discipline or relevant certification
3+ years of database management with expertise in backup, recovery and replication
1+ years of managing an AWS Ecosystem with knowledge in areas of RDS, Dynamo DB, MySQL, Maria DB, and/or AWS Aurora
Strong practical knowledge of SQL query performance, resolution and overall tuning
Proven knowledge of identifying and setting up system and performance monitoring
Understanding of the development lifecycle and service management processes including Code Promotion, Change Control and Incident/Problem Management
Hands on Security and Compliance (PCI / SOX) on data infrastructure
Backup/recovery, replication, cluster failover and disaster recovery
Willingness to participate in On-call rotation and off hour releases
Worked on DB Migrations with zero downtime
Excellent oral and written communication",None Found,"Manage data users to enable appropriate data distribution to the user in a timely manner
Manage transaction recovery and backup data
Minimize database downtime and manage parameters to provide fast query responses
Determine, enforce, and document database policies, procedures, and standards
Perform regular tests and evaluations to ensure data security, privacy and integrity
Monitor database performance, implement changes and apply new patches and versions when required
Automate SOX security compliance group checks
Required Education and Experience
BS degree in a computer discipline or relevant certification
3+ years of database management with expertise in backup, recovery and replication
1+ years of managing an AWS Ecosystem with knowledge in areas of RDS, Dynamo DB, MySQL, Maria DB, and/or AWS Aurora
Strong practical knowledge of SQL query performance, resolution and overall tuning
Proven knowledge of identifying and setting up system and performance monitoring
Understanding of the development lifecycle and service management processes including Code Promotion, Change Control and Incident/Problem Management
Hands on Security and Compliance (PCI / SOX) on data infrastructure
Backup/recovery, replication, cluster failover and disaster recovery
Willingness to participate in On-call rotation and off hour releases
Worked on DB Migrations with zero downtime
Excellent oral and written communication
Nice to have
Demonstration of use of Source Code Management Tools E.g. Git
Experience in cross-region replication with RDS
Worked in an agile environment and participated in Daily Scrum activities
Knowledge of programming such as Python
Gogo is the inflight internet company. Our worldwide inflight Wi-Fi services have made internet and video entertainment a regular part of flying. We are a diverse and mission-minded group of professionals all working together in extraordinary harmony. And that’s just the beginning. We connect the aviation industry and air travelers with innovative technology and applications, and we do it all in a high-energy environment that welcomes the next challenge. Be prepared to join a performance-obsessed team that is passionate about bringing the internet to every device, every flight, everywhere.
Equal Opportunity Employer/Vets/Disabled
Gogo participates in E-Verify. Details in English and Spanish. Right to Work Statement in English and Spanish"
122,Senior Cloud Data Engineer,"Downers Grove, IL 60515",Downers Grove,IL,60515,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description
Cloud Data Engineer - Downers Grove, Illinois

FTD Companies, we continually evolve the business by promoting a culture where solutions from the technology group help transform our business through customer-meaningful solutions, innovative technologies, and business-impacting projects.

This highly crucial Cloud Data Engineer position will take a leading role in helping analyze the data platform within our multi-brand $1B+ E-Commerce company. We're actively searching for top-level engineers to join newly created teams at FTD Companies who will be tasked with building an entirely new state-of-the-art platform and architecture from scratch.

What You Will Contribute:

We participate with a multi-functional team (technical & non-technical members) on highly visible strategic projects and work independently as needed. You are going to have the opportunity to tackle relevant Big Data engineering problems and interact with top-caliber software engineers, web developers, and QA automation resources contributing to our growing ecosystem of cutting-edge SOA based applications.

You'll manage a large-scale, high-availability, rapidly-growing BIG DATA infrastructure on Google Cloud Services that supports multi data sources. You will also be working closely with DevOps and other teams within FTD-IT area to provide quick Data Solutions.

We need your help to design, architect and build our data platform while using a variety of BIG DATA technologies
Work closely in the team to analyze and develop data architecture; ETL processes, ERD modeling and physical database implementation with GCP Data Services: Big Query, Big Table, Data Flow
Work with us to design, develop and roll out new application features that impact databases
You can develop and maintain an in-depth understanding of the data/ETL architecture and the general application functionality used to maintain data integrity
You will develop Data Flow jobs to answer complex analytical and real-time operational questions.
You'll innovate by exploring, recommending, benchmarking, and implementing data-centric platform technologies.
Provide hardware architectural guidance, estimate cluster capacity, and create roadmaps for Hadoop or BIG DATA Cloud services.
You provide support for both analytics and operational platforms.
Work closely with team-members including IT managers to deliver defect-free solutions in a timely manner. Update work status on a frequent (as often as daily) basis
You follow and improve upon processes and policies for database application development methodologies and lifecycles
Work on multiple projects at a time either independently or as a team member
Work with developers and business owners to provide database needs for the entire FTD platform
Oversee the development and release of solutions to non-production environments
Are you'll willing to collaborate with some of the best Java architects to establish platform standards when new technologies are introduced in the FTD platform?
Are you curious and want to continually investigate new technologies and their possible application to the company's business requirements?
Do you wish to assist in the development of application development processes, policies, and standards?
What we seek:

We seek for you to have a BS in computer science, engineering or similar technical discipline. Alternatively, you can have an equivalent combination of education and experience. Ideally, you have 3 years of experience developing solutions following ""standard methodologies"" within a large/complex environment. Agile process experience preferred while implementing solutions to complex business and technical problems

You are capable of logical/physical database design, development, analysis, architecture, and modeling
You possess experience in architecting multi-tier, distributed database applications
You have 1 year of experience with Apache Beam and 5 or more in Java or Python and with Data Transformation
You've worked for 3 years with the Hadoop stack and 1 or more working with GCP services like Data Flow, Big Table, Big Query, and GCP Data Storage Buckets
You are passionate about, Data, BIG DATA, and ML-learning new technologies
You're experienced in designing and developing large scale applications utilizing BIG DATA tech
You will have a good sense of engineering trade-offs, with an ability to understand the impact of software changes on extendibility, scalability, performance, and maintainability
You have experience with Kafka/ Pub-Sub, SQL programming, and performance tuning skills
Do you want the ability to oversee a wide range of assignments by managing dependencies, expectations, and effectively communicating with partners? Apply now.
What we offer:

A stupendous Senior Cloud Data Engineer opportunity to be a part of a team that builds and evolves high performance, scalable order processing systems handling large transactional volumes. We encourage and welcome out of the box ideas in all areas like artificial intelligence, data processing, and information retrieval, voice capabilities, automation, computer vision, and other ideas welcome. We much enjoy all the benefits of working for a digital E-Commerce Company (flexible hours, a healthy vacation plan, summer hours) in a relaxed work environment.

We firmly embrace the definition of a ""flat"" organization where you will spend time with business leaders, managers, developers, product management and executive leadership. There are job opportunities, and then there are career opportunities... Let us provide you with a career where you look forward to Mondays. Careers that have a positive impact and offer you a future as we change, grow, expand and explore new and exciting experiences for our global customer base

Company Description
FTD has been a leader in the floral industry for over a century. We are a private equity-backed company with one of the largest florist networks in the world, supported by the iconic Mercury Man® logo displayed in over 30,000 floral shops in more than 125 countries. We partner with local florists to hand-craft floral arrangements available for same-day delivery on FTD.com and ProFlowers.com. In addition to delivering flowers, we support locally-owned retail florists by providing technology, marketing, and digital services to members of our florist network. For all of life's occasions and everyday moments, visit FTD.com, ProFlowers.com, and ProPlants.com, and follow us on Facebook and Instagram at @ftdflowers. We love helping our customers #SayMorewithFlowers."
123,Tax Staff – National Tax – Tax Technology and Transformation (TTT) – Data Scientist – Advanced Technologies - Chicago,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,"
Strong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.
Experience with common data science programming languages, such as Python R
Strong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential
Experience with data visualization tools, such as PowerBI, D3.js, etc.
Experience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB
Strong scripting and programming skills
Ownership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.
Ability to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies
Ability to consolidate tax data to make analysis and planning more efficient
Focus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities
Willingness to support project team members in any way needed to help ensure timely completion of deliverables",None Found,None Found,None Found,"Tax Technology and Transformation offers services to companies in response to the impact of existing and emerging technology, including the growing data burden that many businesses face, driving efficiencies to create a cost-effective tax function and the need to understand how to make data an asset. The underlying objective of the combined offerings is to help businesses navigate the digital age of tax transparency alongside new trends in tax compliance and tax audit methods, as well as helping to solve the most pressing challenges that businesses face. Tax Technology and Transformation is composed of the following services:
Digital tax transformation
Tax applications-as-a-service
Tax data and improvement
Tax analytics and reporting enhancement
Emerging tax technology, including robotic process automation (RPA), artificial intelligence (AI), blockchain, cloud solutions, data lake development and business intelligence innovation
Tax technology program mobilization
Custom tax technology application development and deployment
Tax technology strategy and road mapping
Direct and indirect tax systems implementation and configuration
Post-transaction (M&A) tax function operational services
Tax operating model transformation, including process improvement, risk and controls
Tax function assessments
The opportunity
Tax Technology and Transformation is an area that has seen significant growth and investment, and you will see that reflected in your experience. It is no exaggeration to say that you will be working on highly publicized projects. The field of taxation is constantly changing as new laws, regulations, and technologies are created, and this is your opportunity to be part of that development.

Key responsibilities
We are looking for an ambitious, self-motivated data scientist or data engineer who will help us discover the information hidden in vast amounts of data, and help us deliver even better products to our clients. Your primary focus will be in applying data mining techniques, doing statistical analysis and building high quantity prediction systems integrated with our products. You will be expected to team on a national and even global scale, so strong communication skills, attention to detail, and ability to effectively drive results are essential.
Selecting features and, building and optimizing classifiers using machine learning techniques
Data mining using state-of-the-art methods
Enhancing data collection procedures to include information that is relevant for building analytic systems
Processing, cleansing and verifying the integrity of data used for analysis
Doing ad-hoc analysis and presenting results in a clear manner
Depending on your unique skills and ambitions, you could be supporting various client projects, ranging from assisting in the production of leading edge machine models, to designing and implementing robust data pipelines that can handle data at a multinational, enterprise scale. Whatever you find yourself doing, you will contribute and help toward developing a highly trained team, all the while handling activities with a focus on quality and commercial value. This is a highly regulated industry, so it is all about maintaining our reputation as trusted advisors by taking on bold initiatives and owning new challenges.

Skills and attributes for success
Strong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.
Experience with common data science programming languages, such as Python R
Strong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential
Experience with data visualization tools, such as PowerBI, D3.js, etc.
Experience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB
Strong scripting and programming skills
Ownership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.
Ability to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies
Ability to consolidate tax data to make analysis and planning more efficient
Focus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities
Willingness to support project team members in any way needed to help ensure timely completion of deliverables
To qualify for the role, you must have
A bachelor’s degree in information system, tax technology, management information systems or computer science or related field and a minimum of one year of related work experience
A passionate interest in data science and its role in the organization
Excellent communication and business writing skills
A natural flair for problem solving and an entrepreneurial approach to work
Strong organizational and time management skills, with exceptional client-serving consulting skills
Demonstrated ability to capture and synthesize business requirements
Desire and demonstrated ability to provide leadership within a team
Ideally, you’ll also have
Experience with Apache Spark
Experience with Hadoop and/or distributed database systems
Experience working in the Microsoft Azure Cloud environment
Experience developing ETL solutions using SSIS or other tools
ERP experience, including SAP and/or Oracle-preferred but not required
Practical experience or strong theoretical understanding of neural networks
What we look for
We are looking for knowledgeable data science professionals with a passion for turning data into actionable insight. You will need strong business acumen and a firm strategic vision, so if you are ready to use those skills to develop your team, this role is for you.

What working at EY offers
We offer a competitive compensation package where you will be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, a minimum of three weeks of vacation plus ten observed holidays and three paid personal days; and a range of programs and benefits designed to support your physical, financial and social well-being. We also offer:
Support and coaching from some of the most engaging colleagues in the industry
Opportunities to develop new skills and progress your career
The freedom and flexibility to handle your role in a way that is right for you
About EY
As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.

EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

If you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible.

Make your mark. Apply today.

."
124,Senior Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Enova:
Hard working people need access to fast, trust-worthy credit and Enova uses advanced technology and analytics to provide that to them. With a focus on non-prime customers and small businesses, we've served over 5 millions customers through our six businesses in the US and abroad. We're born and raised in Chicago and we pride ourselves on hiring smart, driven people who like solving challenging business problems. Our philosophy is simple, ""Life's short. Work some place awesome.""

Many of us consider our people to be Enova's best perk. We have 1,500+ employees and your teammates are as passionate about their work as you are. Your manager and team will encourage you to think outside the box and will celebrate your wins with you along the way. We're big on career growth and make sure you have the tools you need to succeed. On top of that, we also offer competitive salaries, health care benefits, a 401k matching plan, summer hours, tuition reimbursement and a sabbatical program. Our Chicago headquarters even offers over 100 different kinds of snacks, a game room, onsite massages/barbers/nail technicians, and a variety of different social events.

What you'll be doing:
In this role, you will build technical solutions to help improve the scalability and performance of our data stores and our overall systems. As a Senior Engineer, you will focus on efforts that will provide increased flexibility and accessibility to our data, such as our cloud based data warehouse and data pipeline initiatives. In addition, you will have the opportunity to take ownership of driving Enova forward with technologies and architecture patterns that will best serve the business.

Your core priorities will be to:

Collaborate with other teams in Software Engineering, Analytics and Infrastructure to implement new solutions.
Serve as an advocate for best practices and standards for database related efforts, and work toward optimizing the interfaces between our applications and our data stores.
Provide increased flexibility and accessibility to our data.
Mentor other engineers on best practices and patterns.

What you should have:

10+ years of experience in software engineering with a focus on database related technologies
Deep technical knowledge of SQL and database related technologies (particularly PostgreSQL)
Strong knowledge of relational database modeling principles and techniques
Exposure to various non-relational / NoSQL database management systems
Experience with various service providers such as AWS
Comfort with orchestration and scheduling tooling such as Jenkins/Airflow/Rundeck
Working knowledge of one or more programming languages
Bachelor's degree in CS, IT, or related study

About Engineering:
The Software Engineering Team, one of the largest groups in the company, is responsible and accountable for meeting the demands of our current and future businesses. We help create the ""Tech"" in FinTech.

We are structured into small full-stack teams, each aligned to specific business lines or core services. Each team is responsible for defining and delivering solutions through smart interactive development. We code in Ruby, Go, Java, and Swift. We use Vue and other JS frameworks for front-end development. However, we welcome engineers from different technical backgrounds and have created a training program to get you up to speed on our tech stack. Baseline is our self-paced training program, which provides a suite of exercises for all new engineering hires to work through during their first few weeks, ensuring they have the knowledge needed to be successful in their role.

Although we are divided into unique teams, our culture of collaboration promotes and encourages engagement across every team and department within the company - no team is a silo. This enables us to align our core values and create strong, best practices."
125,Senior Big Data Engineer - Procurement Team,"Chicago, IL 60642",Chicago,IL,60642,None Found,None Found,None Found,None Found,None Found,None Found,"Mars Commercial (Procurement) manages a significant part of Mars’ P&L, ensuring that our organization continuously improves our cost base while driving improvements in quality and innovation. This team is embarking on an ambitious and exciting journey to transform our analytics capabilities. This will involve building core foundations to push information to our Commercial associates while delivering fast results to our category teams by developing decision-making tools that solve complex problems.

What you’ll do

Keeping the data flowing and readily available to solve problems that need immediate solutions is core to what you’ll do. Your passion for big data challenges around innovation, emerging technologies and legacy environments drives your desire to design and implement next-gen solutions to traditional business challenges. You’re ready to step up and take charge of developing data-driven processes that improve the functionality of your stakeholders and result in leaner operations, processes and profitability.
Some of your day-to-day work will include, but won’t be limited to:

Leading the effort to develop well-designed modular code that’s usable and easily maintained
Taking ownership of establishing proprietary internal libraries of shareable functions to establish a standardization for engineering work
Engineering data-centric solutions from scratch through the incorporation of ongoing emerging technologies
Taking ownership of an untamed data lake and turning into highly functional, pristine data sources
Supporting the big data strategies of your stakeholders through self-service analytics, real-time decision engines and AI/ML advances
Staying abreast of emerging technologies, architecture patterns, programming languages and ML algorithms and assuming the mentorship and training of other team members
Collecting functional and non-functional client requirements while monitoring technical environments, business constraints and enterprise requirements

What you’ll need

Minimum 7 years of experience preferred within a Big Data Azure and/or AWS environment
Prior experience within a Procurement environment is preferred; knowledge of SAP Procurement-related modules will be helpful
Experience with or exposure to the FMCG/CPG industry or Procurement
Possess the ability to bring together divergent data sets that meet the requirements of the Data Science and Data Analytics teams
Proficiency with data modelling, query techniques and complexity analysis
Experience with cloud, container and micro service infrastructures
Technical expertise with emerging Big Data technologies, such as: Python, Scala, Spark, Hadoop, Clojure, Git, Flink, Elasticsearch, SQL, scikit-learn, TensorFlow, and DNB (databricks); visualization tools: Tableau and PowerBI
A good understanding of and adherence to data security standards
A Bachelor’s in computer science or similar disciplines

Inspiring the whole Mars business to adopt data driven decision-making by developing advanced analytics methods using Machine Learning/AI is huge in your role. How? By mining vast amounts of data from company databases for insights that will help solve business problems and ultimately make Mars more profitable.

#LI-SA1

This is an exciting time at Mars. We’re using digital, data and user insights to transform our business by finding answers to problems that we’ve often never asked ourselves before. From joining the dots to improve our data ecosystem, to streamlining the efficiency and automation of our supply chain and quality operations, we’re already seeing some brilliant results. In fact, we’ve built so much momentum that we’re now looking for industry leaders in Business Translation, Data Science and Data Engineering with different and complementary skills to influence how we operate and grow beyond anything we’ve achieved before. Join us, and discover a company set up to develop your capabilities and ambitions and a group of colleagues ready to support and inspire you. Working together, we’ll create a better world for our planet, our communities and our pets.

Mars is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request."
126,Spark with Big Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"7 years development experience in one of the Big Data Technologies Hadoop eco system Pig Hive HBASE Spark Scala KafkaCross trained in Talend is added advantageDW basic conceptsUnix scripting is added advantageTo be proficient in designing efficient and robust ETL workflowsTo be able to work with cloud computing environmentsGather and process raw data at scale including writing scripts web scraping calling APIs write SQL queries etcSecondary skillset Knowledge on JIRA Github Jenkins Zena Healthcare Claims KnowledgeExperience in Agile Development methodologiesExcellent communication skills and team dynamics. Should have good hands on experience in Hadoop ecosystem like Spark, Scala, Hive, Pig, Sqoop and HDFS.Job Types: Full-time, ContractContract Length:More than 1 yearContract Renewal:LikelyFull Time Opportunity:YesWork Location:One location"
127,Data Engineer Midwest,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are:

Applied Intelligence, the people who love using data to tell a story. We’re also the world’s largest team of data scientists, data engineers, and experts in machine learning and AI. A great day for us? Solving big problems using the latest tech, serious brain power, and deep knowledge of just about every industry. We believe a mix of data, analytics, automation, and responsible AI can do almost anything—spark digital metamorphoses, widen the range of what humans can do, and breathe life into smart products and services. Want to join our crew of sharp analytical minds? Visit us here to find out more about Applied Intelligence.


You are:

An expert engineer with an eye for AI. You want to change how the world works and lives by taking AI out of the lab and into everyday life.


The work:

You’ll be part of a team with incredible end-to-end digital transformation capabilities that shares your passion for digital technology and takes pride in making a tangible difference. If you want to contribute on an incredible array of the biggest and most complex projects in the digital space, consider a career with Accenture Digital.

Here’s what you need:
Minimum 2+ years of experience in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments using Spark, pySpark, SparkSQL, with Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)
Minimum 1 year of designing and building performant data tiers (or refactoring existing ones), that supports scaled AI and Analytics, using different Cloud native data stores on AWS (Kinesis, S3. GLUE, DynamoDB etc.) or Azure (HDInsights, AzureData Factory) or GCP (DataProc, PubSub, BigQuery) as well as using NoSQL and Graph Stores.
Minimum 1 year of designing and building streaming data ingestion, analysis and processing pipelines using Kafka, Kafka Streams, Spark Streaming and similar cloud native technologies
Minimum 1-year performance engineering, profiling, debugging very large big data and ML production solutions on Spark and native Cloud technologies
Bonus points if:

Minimum 6 months of experience in implementation with Databricks.
Minimum 1 year of designing and building secured and governed Big Data ETL pipelines, using Talend or Informatica technologies; for data curation and analysis of large le production deployed solutions.
Experience implementing smart data preparation tools such as Palate, Trifacta, Tamr for enhancing analytics solutions.
Minimum 1 year of building Business Data Catalogs or Data Marketplaces for powering business analytics using technologies such as Alation, Collibra, Informatica or custom solutions


Important information

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.

Accenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.


Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration."
128,Data Engineer,"Chicago, IL",Chicago,IL,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Solstice is an innovation and emerging technology firm that helps Fortune 500 companies seize new opportunities through world-changing digital solutions. As strategists and consultants, we help organizations evolve their digital strategy to solve mission-critical problems. As designers and developers, we build incredible hardware and software solutions that transcend a standalone product and transform an organization's relationship with its customers.

Solstice is looking to hire a Data Engineer to join our growing capabilities team. If you are innovative, passionate about data and AI technologies, and looks to continually learn and enjoys sharing expertise, read on!

About you


You have a strong passion for building innovative and intelligent solutions around data
Experience with designing data models and ETL.
Experience in working with message queuing, stream processing, and highly scalable big data stores.
Experience with big data tools like Google BigQuery, Hadoop, Spark, Kafka, Elasticsearch etc.
Experience with relational SQL and NoSQL databases such as Postgres and Cassandra.
Experience with data pipeline and workflow management tools (any in particular?)
Experience with stream-processing systems such as Storm and Spark-Streaming
Strong background in Python, Java and/or .NET, knowledge with Kotlin and Scala is a huge plus
Familiar with Microservice design patterns including Serverless and BFF
Experience designing, building, integrating and testing with RESTful APIs
Experience in developing and implementing scripts for database maintenance, monitoring, and performance tuning to be applied across the business
You have the ability to effectively communicate technical topics to product owners, stakeholders and other business team members
Experience with data visualization and reporting tools like Looker, Tableau or PowerBi
Experience with cloud technologies such as Google Cloud Platform (GCP) or Amazon Web Services (AWS)
Strong verbal and written communication is a must
Experience working in an Agile Scrum development environment, or in a consulting capacity is a plus
Experience in Machine Learning is a plus

What You Will be Doing


Designing, Migrating, Building, and Testing large scale data processing architectures
Building enterprise applications on the cloud and technologies such as Google Cloud, BigQuery, AutoML, Google Data Studio
Helping clients implement ways to improve data reliability, efficiency, and quality, and build intelligent solutions leveraging data
Working with designers to help visualize data to provide insights to end-users
Performing ad-hoc analyses of data stored in the business's MySQL/MS SQL databases and writing SQL scripts, stored procedures, functions, and views
Interfacing with our clients and providing technical recommendations
Helping evaluate emerging cross-platform frameworks and enterprise application platforms
Bridging the gap between elegant front end design and existing enterprise back end architectures
Working with experienced data engineers, data scientists, and data architects to foster your experience and growth

We welcome Solsties to show up as their full selves everyday. Because this is so important to us, Solstice is proud to be an equal opportunity employer."
