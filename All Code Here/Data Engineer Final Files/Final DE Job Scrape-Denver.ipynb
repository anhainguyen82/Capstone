{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=Denver%2C+CO&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>CirrusMD helps health plans create happier, healthier, and more engaged members by giving them access to on-demand virtual care solutions that they love to engage with. Our chat-powered care delivery platform connects members to a dedicated, board-certified physician in under 90 seconds from any web-enabled device, with no cost and no time limits attached. CirrusMD enables stress-free, human care conversation that doesn’t end until members get the answers (and peace of mind) they need to manage their wellness.\\nCirrusMD has partnered with over a dozen major national payers and healthcare systems to deliver extraordinary virtual care to millions of lives across the nation. The company was founded in 2012 and is headquartered in Denver, CO.\\nWhy Work Here?\\nJoin our team and help us deliver Care Without Barriers. Our company offers significant opportunity for motivated self-starters who thrive in a fast-paced environment that is quickly transitioning from a startup to a highly recognized healthcare industry disruptor. We offer an exceptional benefits package including health, dental and vision, 401k savings, flexible vacation and working policies, competitive salaries and stock options and an EcoPass. CirrusMD is located in the Catalyst HTI building in Denver’s RiNo neighborhood, a newly built office space, with access to open-air shared workspaces and community areas, and a highly engaged community of healthcare and tech innovation leaders. Subsidized parking, on-site gym and shower facilities are also available to our team. Join us and see for yourself!\\nWho We’re Looking For:\\nAn experienced engineer who can confidently contribute to our mission of redefining the healthcare experience for patients and providers. You are comfortable working independently as well as pairing with other engineer(s). You are comfortable breaking larger tasks down into an executable action plan for yourself, and perhaps others. We seek a motivated, self-starter who knows how to buckle-down and deliver; yet knows when to push back if the demand is too high or the directed path unclear.\\nWhat You’ll Be Responsible For Achieving:\\nBuilding and optimizing data pipelines, architectures and data sets\\nAutomating and optimizing ingestion of outside data from customers\\nA reputation for reliably delivering well considered, well tested and performant code\\nContributions to our evolving development and testing standards and best practices\\nHigh collaboration with Analytics + Product\\nWhat Will Make You Successful:\\nAdvanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.\\nAdvanced programming skills in Python, Java, and/or Go (Golang)\\nExperience designing and managing complex relational database structures\\nProficient with AWS especially S3, EC2, RDS (Postgres), Redshift, Athena (Presto) or EMR (Spark)\\nYour philosophy aligns with Agile methodologies and processes\\nConfidence pairing with other engineers of all levels\\nWhat Will Make You Stand Out:\\nSignificant experience with data warehouses like Snowflake and ETL tools like Fivetran\\nExperience with 1 or more additional backend core technologies (Scala / Rust / Ruby / Elixir)\\nA reputation for superb communication skills with other engineers and teammates\\nYou have a reputation for a high level of craftsmanship about your work\\nStrong analytic skills related to working with structured and unstructured datasets\\nBackend experience designing REST &amp; gRPC APIs\\n\\nTo Apply:\\nPlease submit resume including your salary requirements as PDF to EngineeringJobs@CirrusMD.com indicating \"Data Engineer\" in the subject line.\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\\nNotice to recruiters and placement agencies:\\nIf you are a recruiter or placement agency, please do not submit résumés to any person or email address at CirrusMD prior to having a signed agreement with Human Resources. CirrusMD is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Also, any résumés sent to us without an agreement in place will be considered your company's gift to CirrusMD and may be forwarded to our Talent Acquisition team.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Boulder, CO</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and seek to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun and most importantly to each other’s success. Learn more about Splunk careers and how you can become a part of our journey!\\nSplunk is located in Boulder, CO. This team is made up of a strong mix of very talented and driven individuals, with a proven senior leadership team with a strong technical background.\\nThe office layout is open and the floor plan helps to foster the casual, collaborative environment that has played a key role in the company’s innovation of its product. In addition, you will find everything you would expect in the office of a progressive and fun company – stocked kitchen with snacks and drinks to keep you nourished, rooftop patio, and no less than two craft beers on tap for your enjoyment.\\nOur Team's Mission and Methods\\nThe Data Analytics Team at Splunk focuses on delivering actionable data on a silver platter to both external customers and internal data scientists, data analysts and users.\\nOur new Data Analytics Platform was designed to satisfy the data quality requirements of our high-value and high-complexity data, while also providing fast query performance, ease of maintenance, automatic self-healing, immutable data, and fully automated CI/CD code deployment. Our target technology stack includes Kinesis, S3, Lambdas, Kubernetes, Athena (Presto), Postgres, Python, and Scala.\\nThe team's influences and backgrounds come from data warehousing, data lakes, big data, analytical data engineering, business intelligence, software engineering, and devops.\\nWe're currently building and transitioning into the Data Analytics Platform - and so are continuing to experiment, explore and evaluate these methods as we move forward.\\nAnd we are committed to making the work fun, interesting, and exciting while collaborating, mentoring, and supporting one another.\\nWho You Are\\nOver the last five or more years you've been working as a data engineer to build and maintain custom data pipelines and data at rest in support of reporting and data analysis using a number of elements from our technology stack described above.\\nYou think of data as one of the most valuable resources an organization has, not just an inconvenient by-product of a process, but an opportunity-rich source of future features and capabilities. You understand that the success of a data engineering team is measured by the success of our data consumers and so think deeply and creatively about ways to deliver high-quality, high-functionality and high-performance data for these users.\\nYou are a software engineer for whom automated testing is as important as clean, performant and reusable code.\\nYou are an enthusiast - of analyzing data, of helping people make a difference with data, of building great analytical solutions, and of automated testing and software delivery.\\nAnd as a Senior Data Engineer, you will be responsible for designing, developing and supporting data pipelines, data at rest, and machine learning applications. Software will be written in Scala, Python, SQL, and Spark using an Agile development process.\\n\\nBS in Computer Science or related plus 8 years of related professional experience\\nWhat We Offer You:\\nA constant stream of new things for you to learn. We're always expanding into new areas, bringing in open source projects and contributing back, and exploring new technologies.\\nA set of exceptionally talented and dedicated peers, all the way from engineering and QA to product management and user experience.\\nA stable, collaborative and supportive work environment within a team that is fully committed to everyone's success and joy.\\nWe take work-life balance seriously: we prioritize a sustainable engineering culture over heroism, prioritize work that makes your life better, and many of us work from home two days a week.\\nWe value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying.\\nFor job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Engineer, Data Informatics</td>\n",
       "      <td>Englewood, CO</td>\n",
       "      <td>Englewood</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Senior Data Engineer, Data Informatics\\nCPAADSC2.Analyst, Data Science\\n\\nttec Digital Consulting’s Insights Practice is looking for a Data Engineer who is passionate about solving complex data problems and intricate engineering issues with large systems.\\n\\nWe combine human insight and the speed of technology to transform our clients’ interactions with their customers. Advanced analytics of the customer experience and customer interactions is our expertise.\\n\\nCheck out our website below to learn more about what we do and how we help our clients.\\nhttps://www.ttec.com/ttec-digital\\nWhat you’ll be doing:\\n\\nThis person will be a part of our Humanify Insights Platform focused on helping our clients and coordinating with internal business consulting, analyst, data science, and technology teams.\\nWork closely with data scientists to create impactful insights from complex data\\nDrive and participate in the definition, the direction, and the development of our analytics platform\\nImprove platform features and functionality by keeping up with the latest innovations in data technology\\nDesign, test, and maintain robust, scalable, secure, and fault-tolerant data management systems\\nAssist with process improvement with a customer focused, progressive mindset\\nCommunication of project status/issues to clients and internal management\\nPartner with various internal teams\\nDocument implementation requirements and expected effort\\nResearch and recommend data management best practices\\nAggressively and continuously advance skill set\\n\\nWhat you’ll bring to us:\\nBachelor’s degree qualifying under STEM with 2-3 years of relevant experience\\nExtensive knowledge of various databases\\nProficiency with Azure or AWS ecosystem\\nExperience with Big Data ETL/technologies – Hadoop, Hive, Spark\\nExperience working with structured, semi-structured, and unstructured data sources\\nProficiency with SQL\\nExperience with NoSQL solutions such as MongoDB is preferred\\nProficient in Python or Scala\\nExperienced in consuming third-party REST APIs (JSON) and SDKs\\nProficiency in Linux environment\\nUnderstanding of complex data flows, identification of data processing bottlenecks and designing and implementing solutions\\nA broad set of technical skills and knowledge across hardware, software, systems and solutions development and across more than one technical domain\\nPassion for innovation, delivering quality results, self-driven discovery, outside the box thinking, and complex problem solving\\n\\nWhat skills you’ll need:\\nProven ability to balance and manage multiple, competing priorities.\\nCollaborative interpersonal skills and ability to work within cross-functional teams.\\nSelf-starter who relies on experience and judgment to plan and accomplish goals in complex fast-paced environment to ensure quality of all data integration points.\\nExcellent customer service skills.\\nCreative problem-solving and analysis skills.\\nAbility to handle problem situations quickly, inventively, and resourcefully.\\nProject management skills including:\\nAbility to prioritize and manage tasks\\nAbility to plan, commit, and deliver to schedules\\nAbility to identify, escalate, and manage project issues\\nWillingness to work extended hours on an as-needed basis\\nSome travel is necessary\\nWhat We Offer:\\nVariable incentive bonus plan, 401K company match, tuition reimbursement\\nGlobal career mobility, employee recognition programs, professional development\\nState of the art technology which allows for seamless global connectivity\\nRich wellness program and health incentives\\nLead Everyday w Do the Right Thing w Reach for Amazing w Seek First to Understand w Act as One w Live life Passionately\\n\\n\\nPrimary Location: US-CO-Englewood\\nJob: Consulting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Principal Data Engineer &amp; DBA</td>\n",
       "      <td>Broomfield, CO</td>\n",
       "      <td>Broomfield</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nOver five years of experience as a DBA\\nWorking knowledge of Oracle Autonomous Data Warehouse environments in Oracle’s OCI (Oracle Cloud Infrastructure)\\nExperience in tuning and optimizing DB performance\\nKnowledge of DB security best practices, and back and recovery processes\\nExperience in developing data architectures\\nYears of experience in ETL (Extract, Load, and Transform) from various data sources\\nKnowledge of Big Data and Time-series databases\\nData Cleansing expertise\\nWorking knowledge of data analysis / data science</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nOver five years of experience as a DBA\\nWorking knowledge of Oracle Autonomous Data Warehouse environments in Oracle’s OCI (Oracle Cloud Infrastructure)\\nExperience in tuning and optimizing DB performance\\nKnowledge of DB security best practices, and back and recovery processes\\nExperience in developing data architectures\\nYears of experience in ETL (Extract, Load, and Transform) from various data sources\\nKnowledge of Big Data and Time-series databases\\nData Cleansing expertise\\nWorking knowledge of data analysis / data science</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Principal Data Engineer &amp; DBA-19000YP6\\n\\n\\nPreferred Qualifications\\n\\nPosition Responsibilities\\n\\nThe Oracle Cloud development is one of the biggest initiatives in the history of Oracle. We are introducing revolutionary SAAS, IAAS and PAAS products to the market place, and because many of these products are being built new, we are experiencing tremendous growth in our cloud storage product development team. Our systems move a huge amount of data and we need you to help build both the internal and external infrastructure to support it.\\n\\nWe are looking for an experienced data engineer / Database Administrator to help ETL, manage, and administrate our growing data warehouse. This person will be responsible for all aspects of a DBA role, managing our Cloud based Autonomous Data Warehouse on a daily basis. The person will also be responsible for structuring new DB schemas and working with stakeholders to understand they needs and requirements. Proposals for how to structure the data will need to be created and vetted with upper management. Part of duties includes ETL’ing data from various sources into our ADW instance for easy access and analysis. If this opportunity sounds exciting, look no further and join a growing and dynamic team to help develop a data and analytics based infrastructure in the Oracle Cloud organization. Our team has a start-up feel, but with the stability Oracle gives.\\n\\nDESIRED QUALIFICATIONS:\\nThe ideal candidate will have experience working with large dataset from multiple sources. They should have years of experience as a DBA and familiar with DB tuning. In addition, they should work well in teams, and understand how to create/optimize database schemas. The candidate will be expected to create simple and complex ETL jobs. Specific experience the ideal candidate will demonstrate includes:\\nOver five years of experience as a DBA\\nWorking knowledge of Oracle Autonomous Data Warehouse environments in Oracle’s OCI (Oracle Cloud Infrastructure)\\nExperience in tuning and optimizing DB performance\\nKnowledge of DB security best practices, and back and recovery processes\\nExperience in developing data architectures\\nYears of experience in ETL (Extract, Load, and Transform) from various data sources\\nKnowledge of Big Data and Time-series databases\\nData Cleansing expertise\\nWorking knowledge of data analysis / data science\\n\\nADDITIONAL SKILLS SOUGHT:\\nThe successful candidate is expected to demonstrate the following additional requirements:\\n\\nMS in Computer Science or Data Engineering\\n5+ years of experience working in as a DBA &amp; Data Engineer\\nStrong knowledge of data structures, algorithms\\nHighly skilled in Python and SQL\\nExperience with OCI, AWS, and/or other IaaS environments.\\n\\nOracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.\\n\\n\\nDetailed Description and Job Requirements\\n\\nDesign, develop, troubleshoot and debug software programs for databases, applications, tools, networks etc.\\n\\nAs a member of the software engineering division, you will assist in defining and developing software for tasks associated with the developing, debugging or designing of software applications or operating systems. Provide technical leadership to other software developers. Specify, design and implement modest changes to existing software architecture to meet changing needs.\\n\\nDuties and tasks are varied and complex needing independent judgment. Fully competent in own area of expertise. May have project lead role and or supervise lower level personnel. BS or MS degree or equivalent experience relevant to functional area. 4 years of software engineering or related experience.\\n\\nOracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Strategy Specialist - Business &amp; Data Analysis, Cloud, AWS, Azure, Big Data</td>\n",
       "      <td>Denver, CO 80203</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80203</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe North America Data Strategy &amp; Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.\\n\\n Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model\\n\\nRequired Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:\\no Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling\\no Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.\\no Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel\\n Able to travel up to 100% (Mon-Thu)\\n\\nOptional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model\\n Experience in compiling business cases and roadmaps for data, analytics and technology investments\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Centennial, CO 80112</td>\n",
       "      <td>Centennial</td>\n",
       "      <td>CO</td>\n",
       "      <td>80112</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years working with SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\\nExperience building and optimizing data pipelines, architectures and data sets.\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\nStrong analytic skills related to working with unstructured datasets.\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nA successful history of manipulating, processing and extracting value from large disconnected datasets.\\nWorking knowledge of message queuing and stream processing\\nStrong project management and organizational skills.\\nExperience supporting and working with cross-functional teams in a dynamic environment.\\nWe are looking for a candidate with 3+ years of experience in a Data Engineer or Database Developer role\\nExperience with big data tools: Azure SQL Server Analysis Services, Azure Data Warehouse\\nExperience with relational SQL and NoSQL databases\\nExperience with SQL Server Enterprise and Microsoft Master Data Services would be a plus\\nExperience with data pipeline and workflow management tools: Azure Service Bus, Azure Data Pipelines, Dell Boomi or similar technologies\\nExperience with Azure cloud services\\nExperience with object-oriented/object function scripting languages: C#, C++, JavaScript, etc.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Create and maintain master data solutions for critical business data\\nCreate and maintain optimal data pipeline architecture\\nAssemble large, complex data sets that meet functional / non-functional business requirements.\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure or similar cloud ‘big data’ technologies.\\nWork with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\\nWork with data and analytics experts to strive for greater functionality in our data systems.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>By joining the Bio-Techne team you will have an impact on future cutting-edge research. Bio-Techne, and all of its brands, provides tools for researchers in Life Sciences and Clinical Diagnostics.\\nPosition Summary\\nWe are looking for a Data Engineer to join our growing team. As a Data Engineer you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The core function will be driving the creation of master data repositories for critical business data. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.\\nKey Responsibilities\\nCreate and maintain master data solutions for critical business data\\nCreate and maintain optimal data pipeline architecture\\nAssemble large, complex data sets that meet functional / non-functional business requirements.\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure or similar cloud ‘big data’ technologies.\\nWork with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\\nWork with data and analytics experts to strive for greater functionality in our data systems.\\nQualifications\\nQualifications\\n3+ years working with SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\\nExperience building and optimizing data pipelines, architectures and data sets.\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\nStrong analytic skills related to working with unstructured datasets.\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nA successful history of manipulating, processing and extracting value from large disconnected datasets.\\nWorking knowledge of message queuing and stream processing\\nStrong project management and organizational skills.\\nExperience supporting and working with cross-functional teams in a dynamic environment.\\nWe are looking for a candidate with 3+ years of experience in a Data Engineer or Database Developer role\\nExperience with big data tools: Azure SQL Server Analysis Services, Azure Data Warehouse\\nExperience with relational SQL and NoSQL databases\\nExperience with SQL Server Enterprise and Microsoft Master Data Services would be a plus\\nExperience with data pipeline and workflow management tools: Azure Service Bus, Azure Data Pipelines, Dell Boomi or similar technologies\\nExperience with Azure cloud services\\nExperience with object-oriented/object function scripting languages: C#, C++, JavaScript, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Engineer - Population Health</td>\n",
       "      <td>Denver, CO 80202</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80202</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Our Population Health Data Integration Engineer will deliver top notch data design and integrations to drive meaningful patient interventions. He/she will work with a high functioning team and have a bias towards action. He/She will be able to collaborate with cross-functional teams, and is obsessive about breaking down complex data problems to provide our clinicians with a holistic view of a patient.\\nAll About DaVita IKC.\\nDaVita Integrated Kidney Care (DaVita IKC) is an the integrated care division of DaVita Inc. working on DaVita’s vision to provide integrated care to all ESRD patients, who are some of the most medically complex and vulnerable patient populations in the US. We provide services to help improve the lives of our patients by working with them to prevent complications, reduce the number of avoidable hospitalizations and improve overall health.\\n\\nLet's face it…the world of healthcare is dynamic and moves at the speed of light. Unlike many healthcare companies in the market today, we acknowledge that our teammates are our most important asset and we want to help them to feel fulfilled in their careers.\\n\\nWhen you join the DaVita Village, you're joining a winning team. Through our commitment to training, growth and quality we consistently achieve superior clinical outcomes while giving teammates the opportunity to excel in an award-winning environment that enables them to thrive both professionally and personally.\\nEssential Duties &amp; Responsibilities:\\nDevelops overall data strategy for implementing population health data and integrations platform following SDLC to manage development, testing and deployment\\nIdentifies and partners with integrating systems both internal and external. Creating relationships with other system owners to organize work around data integration\\nBuilds expertise on data sources, types of data, and challenges\\nStrong relational database concepts and a demonstrated ability to comprehend complex technical systems and/or processes\\nAbility to create data mapping to standardize data to a single data model\\nAbility to use multiple technical tools to answer questions and conduct analysis\\nQualifications:\\nBachelors in Computer Science, MIS, or Information Management\\nAbility to leverage multiple tools and technologies to analyze and manipulate large data sets from disparate data sources\\nStrong leadership and stakeholder communication skills. Strong logical, analytical and presentation skills.\\nKnowledge of population health concepts, data related government and regulatory requirements and emerging trends and issues (e.g. information privacy and data protection laws).\\nExcellent problem-solving, organizational and analytical skills with the ability to evolve product strategy based on research, data and industry trends\\nDemonstrated ability to easily build expertise in unfamiliar domains, and achieve stretch goals in an innovative environment\\nExperience working in an Agile environment\\nDatabase modeling experience preferred\\nAdvanced MS Excel skills required\\nAdvanced experience with SQL/PL SQL\\nHere is what you can expect when you join our Village:\\nA \"community first, company second\" culture based on Core Values that really matter.Clinical outcomes consistently ranked above the national average.Award-winning education and training across multiple career paths to help you reach your potential.Performance-based rewards based on stellar individual and team contributions.A comprehensive benefits package designed to enhance your health, your financial well-being and your future.Dedication, above all, to caring for patients suffering from chronic kidney failure across the nation. Join us as we pursue our vision \"To Build the Greatest Healthcare Community the World has Ever Seen.\"\\nWhy wait? Explore a career with DaVita today.\\n\\nGo to http://careers.davita.com to learn more or apply.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Broomfield, CO 80021</td>\n",
       "      <td>Broomfield</td>\n",
       "      <td>CO</td>\n",
       "      <td>80021</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a Data Engineer with a passion for transforming data architecture and building, new, cloud-based (AWS) solutions. This role is important in building functional and scalable data infrastructure to provide the organization the ability to make informed decisions. The ideal candidate has a strong background in the end-to-end data architecture, SQL and data warehousing. You will add your knowledge in the “how” vision for data engineering and will help to drive leading-edge data warehousing and business intelligence practices at a company which is high on collaboration and innovation, but low on egos. This is an opportunity to have a definitive impact on a company's data practice. Apply today!\\n\\nIn this role you will:\\nWork with internal business customers in understanding the business requirements and develop prototypes and proof of concepts for selected projects\\nImplement complex data solutions with a focus on collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into insights using multiple platforms\\nWork with our global team and provide technical direction in building solutions and architecting distributed systems\\nActively participate in knowledge sharing sessions, code and design reviews etc.\\nDevelop scalable data lake for data acquisition, storage, access, and analysis required for structured, semi-structured, and unstructured data in a cloud (AWS) environment\\nEnhance the data infrastructure framework by evaluating new and existing technologies and techniques to create efficient processes around data extraction, aggregation, and analytics\\nAssist in establishing and maintaining standards and guidelines for the design, development, tuning, deployment, and maintenance of information and advanced data analytics\\nYou bring to the table:\\n3+ years in Data Engineering\\nHands on experience building Data Lake in AWS (preferably) while sourcing data from heterogeneous sources\\nExperience with AWS technologies, such as S3, Aurora, Athena, Redshift, Kinesis, EMR, etc.\\nExperience with distributed systems and NoSQL databases\\nHands on experience with Spark, Python and PySpark\\nHands on experience with Data Modeling\\nYou enjoy learning new technologies, data analysis, and identifying data patterns and trends\\nAbility to recommend innovative solutions to solve business and technical problems in a straight forward manner to business partners\\nExtensive experience with SQL programming (TSQL, PLSQL)\\nStrong communication skills\\nStrong problem solving and analytical skills\\nAt Webroot, we do more than secure our customers' personal computers, mobile devices and networks. We also nurture our employees' most critical assets – their talents, experience, and career aspirations. Webroot has the energy of a start-up with the strength and stability of an Internet security market leader. We foster the innovative culture you'd expect of a company that's making a statement. Webroot is a company in which you can invest yourself fully, knowing that you're not only protecting our customers around the world, but also that your talents and innovation will be recognized and rewarded. We encourage you to learn more about us and explore our job openings. Secure your future. Ensure the same for your career. Principals only - no third parties, please. Webroot Inc. is an Equal Opportunity Employer. At Webroot, we believe in rewarding success. Our total rewards philosophy includes highly competitive salaries and a robust benefits package that you can view here.\\n\\nJob Information\\nJob Id: WEBR-DataEngineerCO1\\nCompany: Webroot\\nLocation: Broomfield, CO, United States\\nPosition: Data Engineer - AWS\\nType: Full Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Supply Chain Data Engineer</td>\n",
       "      <td>Englewood, CO</td>\n",
       "      <td>Englewood</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Education - Sys/Div/Mkt/Local Manager - Bachelor's Degree and minimum of 3 years leadership experience OR minimum of 5 years leadership experience in the discipline OR Master's Degree and no experience. Bachelor's Degree in Information Systems, Business, Engineering or closely related field\\nExperience -\\n5+ years of experience in data engineering and/or devops\\n1 years of production experience with the SQL Server ecosystem\\n2 years of production experience with cloud or distributed platforms and architectures such as Azure, AWS, Spark, and/or Docker\\nSkills -\\nPython or Scala; advanced SQL\\nKnowledge of and experience with DevOps concepts, tools, and architectures\\nComfort with Linux and Windows server environments, web APIs\\nExperience with Airflow or similar orchestration tools. SSIS experience helpful.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Design, build, test, and maintain data pipelines integrating data from source systems into a cohesive data warehouse and other repositories for reporting and analysis.\\nExpand and maintain source control, build scripts, and other elements supporting increasingly continuous deployment.\\nInstitute a testing framework for data pipelines and schemas and write tests.\\nCoordinate the automated transfer of data between local and remote systems.\\nWork with data science and analytics team to deploy machine learning models.\\nImplement tools to monitor data quality, availability, and pipeline performance.\\nWork with IT to plan and execute further improvements to Supply Chain's data platform as needed, potentially including cloud (e.g, Azure or AWS), containerization (e.g., Docker), or other (e.g., Spark) components.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Dignity Health is one of the nation's largest health care systems. As of June 30, 2017, Dignity Health operated more than 400 care centers, including hospitals, urgent and occupational care, imaging and surgery centers, home health, and primary care clinics in 22 states, through its network of more than 9,000 physicians and more than 60,000 employees. Headquartered in San Francisco, CA, Dignity Health is dedicated to providing compassionate, high-quality, and affordable patient-centered care with special attention to those who are poor and underserved. In its fiscal year ended June 30, 2017, Dignity Health provided $2.6 billion in charitable care and services. More information on Dignity Health is available at www.dignityhealth.org .\\n\\nResponsibilities\\nJob Summary:\\n\\nThe supply chain data engineer builds and maintains the data pipelines that make data accessible to supply chain staff, our partners, and the larger organization. This includes identifying and deploying best-in-class tools and infrastructure to improve the reliability and efficiency of those pipelines, as well as writing ELT code, tests, and documentation to democratize access to data. The data engineer will work with analytics, business, and IT stakeholders to identify, plan, and execute projects to radically improve the availability, quality, and scope of our data.\\n\\nCore Duties:\\nDesign, build, test, and maintain data pipelines integrating data from source systems into a cohesive data warehouse and other repositories for reporting and analysis.\\nExpand and maintain source control, build scripts, and other elements supporting increasingly continuous deployment.\\nInstitute a testing framework for data pipelines and schemas and write tests.\\nCoordinate the automated transfer of data between local and remote systems.\\nWork with data science and analytics team to deploy machine learning models.\\nImplement tools to monitor data quality, availability, and pipeline performance.\\nWork with IT to plan and execute further improvements to Supply Chain's data platform as needed, potentially including cloud (e.g, Azure or AWS), containerization (e.g., Docker), or other (e.g., Spark) components.\\nNon-Essential Job Responsibilities:\\nOther duties as assigned by management\\n~LI-DH\\ntb91119\\n\\nQualifications\\nMinimum Qualifications:\\n\\nEducation - Sys/Div/Mkt/Local Manager - Bachelor's Degree and minimum of 3 years leadership experience OR minimum of 5 years leadership experience in the discipline OR Master's Degree and no experience. Bachelor's Degree in Information Systems, Business, Engineering or closely related field\\nExperience -\\n5+ years of experience in data engineering and/or devops\\n1 years of production experience with the SQL Server ecosystem\\n2 years of production experience with cloud or distributed platforms and architectures such as Azure, AWS, Spark, and/or Docker\\nSkills -\\nPython or Scala; advanced SQL\\nKnowledge of and experience with DevOps concepts, tools, and architectures\\nComfort with Linux and Windows server environments, web APIs\\nExperience with Airflow or similar orchestration tools. SSIS experience helpful.\\n\\nPreferred Qualifications:\\n\\nFamiliarity with dbt, R, Tableau, or related tools helpful\\nRelevant experience in a healthcare environment and computer systems preferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Consultant, Cloud Data Engineer</td>\n",
       "      <td>Denver, CO 80202</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80202</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n3+ years previous consulting experience preferred\\n3+ years architecting and implementing Microsoft Azure, Amazon Web Services, and/or Google Cloud Platform infrastructure and topologies\\nExperience implementing core infrastructure, networking, and cloud-based services for business teams or consumers\\nExperience implementing Lambda architecture-based data designs\\nDeep product knowledge and understanding of Azure, AWS, and/or GCP\\nExperience configuring and tuning virtual private clouds\\nPractical experience sizing hardware and storage needs\\nStrong analytical problem solving ability\\nGood written and verbal communication + presentation skills\\nSelf-starter with the ability to work independently or as part of a project team\\nCapability to execute performance analysis, troubleshooting, and remediation\\nKnowledge of High Availability and Disaster Recovery principles, patterns, and usage\\nUnderstanding of the cloud ecosystem and leading emerging technologies/interdependencies</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nWork as part of a team to design and develop cloud data solutions\\nGather technical requirements, assess client capabilities, and analyze findings to provide appropriate cloud solution recommendations and adoption strategy\\nDefine Cloud Data strategies, including designing multi-phased implementation roadmaps\\nLead analysis, architecture, design, and development of data warehouse and business intelligence solutions\\nBe versed in Amazon Web Services, Google Cloud Platform, and/or Microsoft Azure cloud solutions, architecture, related technologies, and their interdependencies\\nResearch, analyze, recommend, and select technical approaches for solving development and integration problems\\nLearn and adopt new tools/techniques to increase performance, automation, and scalability\\nAssist business development teams with pre-sales activities and RFPs\\nUnderstand how to translate business goals and drivers into a technical solution\\nProvide technical direction and oversight to cloud implementation teams</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Slalom is a modern consulting firm focused on strategy, technology, and business transformation. In 30 markets across the U.S., U.K., and Canada, Slalom's teams have autonomy to move fast and do what's right. They're backed by regional innovation hubs, a global culture of collaboration, and partnerships with the world's top technology providers.\\nFounded in 2001 and headquartered in Seattle, Slalom has organically grown to over 7,000 employees. Slalom was named one of Fortune's 100 Best Companies to Work For in 2019 and is regularly recognized by employees as a best place to work.\\n\\nJob Title:\\nCloud Data Engineer\\nAs a Cloud Data Engineer in our Data &amp; Analytics practice at Slalom, you will analyze, design, and architect cloud-based solutions to address our clients’ needs for infrastructure-as-a-service, platform-as-a-service, and software-as-a-service. We are looking for sharp, disciplined, and self-motivated individuals who have a passion for utilizing cloud solutions from Amazon Web Services, Microsoft Azure, and Google Cloud Platform to solve real business problems for our clients.\\nResponsibilities:\\nWork as part of a team to design and develop cloud data solutions\\nGather technical requirements, assess client capabilities, and analyze findings to provide appropriate cloud solution recommendations and adoption strategy\\nDefine Cloud Data strategies, including designing multi-phased implementation roadmaps\\nLead analysis, architecture, design, and development of data warehouse and business intelligence solutions\\nBe versed in Amazon Web Services, Google Cloud Platform, and/or Microsoft Azure cloud solutions, architecture, related technologies, and their interdependencies\\nResearch, analyze, recommend, and select technical approaches for solving development and integration problems\\nLearn and adopt new tools/techniques to increase performance, automation, and scalability\\nAssist business development teams with pre-sales activities and RFPs\\nUnderstand how to translate business goals and drivers into a technical solution\\nProvide technical direction and oversight to cloud implementation teams\\nQualifications:\\n3+ years previous consulting experience preferred\\n3+ years architecting and implementing Microsoft Azure, Amazon Web Services, and/or Google Cloud Platform infrastructure and topologies\\nExperience implementing core infrastructure, networking, and cloud-based services for business teams or consumers\\nExperience implementing Lambda architecture-based data designs\\nDeep product knowledge and understanding of Azure, AWS, and/or GCP\\nExperience configuring and tuning virtual private clouds\\nPractical experience sizing hardware and storage needs\\nStrong analytical problem solving ability\\nGood written and verbal communication + presentation skills\\nSelf-starter with the ability to work independently or as part of a project team\\nCapability to execute performance analysis, troubleshooting, and remediation\\nKnowledge of High Availability and Disaster Recovery principles, patterns, and usage\\nUnderstanding of the cloud ecosystem and leading emerging technologies/interdependencies\\n\\nSlalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Senior Big Data Engineer</td>\n",
       "      <td>Englewood, CO</td>\n",
       "      <td>Englewood</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s in computer science or related field is required (masters preferred)You have a minimum of 7 years of experience in the design, development, and deployment of large-scale, distributed, and cloud-deployed software services.Expected to be an expert in SQL and RDBMS. Good at modeling data for relational, analytical and big data workloadsYou have a minimum of 4 years of experience in Big Data software development technologies (e.g., Hadoop, Hive, Spark, Kafka) and exposure to resource/cluster management technologies.Minimum of 3 year of experience with AWS (e.g., EC2, S3, EMR, SNS, SQS, Aurora, Redshift).Experience with various software technologies/solutions and understand where to use them.2 years of experience with Data Virtualization like Denodo</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Ability to quickly identify an opportunity and recommend possible technical solutions.Utilize multiple development languages/tools such as Python, SPARK, Hive to build prototypes and evaluate results for effectiveness and feasibility.Operationalize data ingestion and data-analytic tools for enterprise use.Utilize tools available to you across AWS ServicesDevelop real-time data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, NIFI, Python, HBase and Hadoop.Custom Data pipeline development (Cloud and locally hosted)Work heavily within AWS and Hadoop ecosystemsProvide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Company\\n\\nHitachi Vantara combines technology, intellectual property and industry knowledge to deliver data-managing solutions that help enterprises improve their customers’ experiences, develop new revenue streams, and lower the costs of business. Hitachi Vantara elevates your innovation advantage by combining IT, operational technology (OT) and domain expertise. Come join our team and our employee-focused culture and help drive our customers’ data to meaningful customer outcomes.\\n\\nThe Role\\n\\nAs a Sr. Big Data Engineer you will provide engineering knowledge to create and enhance data solutions enabling seamless delivery of data across our enterprise. You will be on the cutting edge of finding and integrating new technology and tools for data centric projects. Additionally, you will provide technical consulting to peer data engineers during design and development of highly complex and critical data projects. Some of these projects will include designing and developing data ingestion and processing/transformation frameworks leveraging open source tools such as NiFi, EMR, Java, Scala, Spark APIs, AWS Glue, etc. Additionally, you will work on real time processing solutions using tools such as Spark Streaming, MQ, Kafka, and AWS Kinesis. You will deploy application code using CI/CD tools and techniques.\\n\\nResponsibilities:\\n\\n Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.\\nAbility to quickly identify an opportunity and recommend possible technical solutions.Utilize multiple development languages/tools such as Python, SPARK, Hive to build prototypes and evaluate results for effectiveness and feasibility.Operationalize data ingestion and data-analytic tools for enterprise use.Utilize tools available to you across AWS ServicesDevelop real-time data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, NIFI, Python, HBase and Hadoop.Custom Data pipeline development (Cloud and locally hosted)Work heavily within AWS and Hadoop ecosystemsProvide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc\\n\\nQualifications\\nBachelor’s in computer science or related field is required (masters preferred)You have a minimum of 7 years of experience in the design, development, and deployment of large-scale, distributed, and cloud-deployed software services.Expected to be an expert in SQL and RDBMS. Good at modeling data for relational, analytical and big data workloadsYou have a minimum of 4 years of experience in Big Data software development technologies (e.g., Hadoop, Hive, Spark, Kafka) and exposure to resource/cluster management technologies.Minimum of 3 year of experience with AWS (e.g., EC2, S3, EMR, SNS, SQS, Aurora, Redshift).Experience with various software technologies/solutions and understand where to use them.2 years of experience with Data Virtualization like Denodo\\n\\nWe are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.\\n\\n#LI-DNI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Greenwood Village, CO</td>\n",
       "      <td>Greenwood Village</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Innovar Group is looking for a Sr. Data Engineer in Greenwood Village, CO\\n\\nCLICK APPLY NOW TO LEARN MORE ABOUT THIS JOB\\nInnovar Group is seeking a talented, driven, and motivated Sr. Data Engineer for an exciting Contract-to-hire opportunity in the Denver Tech Center. This is an outstanding opportunity to work for a cutting-edge software team and help design and develop the new iteration of their product.\\nExperience: Contract\\nAbout Us: Innovar Group is comprised of senior talent agents who deliver top recruitment services to clients throughout the United States. We bring a new era of recruiting to the industry by aligning state-of-the-art technology with outstanding talent.\\n\\nInnovar - derived from the Latin “to innovate” - this is what we set out to do each and every day. We strive to go beyond the norm to utilize innovative next-gen recruiting tools connecting us to the world at large in order to uncover high-impact TECHNOLOGY talent for our valued clients. Our goal is straightforward: the unmitigated satisfaction of each client. The difference is significant; we provide consummate service and foster long-term, thriving relationships with our clients. In essence, we work as a vital member of your talent acquisition team.\\n\\nKeywords: Ideal Candidate / Analysis / Analysis / Teamwork / Design / Implementation / Support / Write / Experience / Knowledge / Communication /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Jr. Data Engineer</td>\n",
       "      <td>Lone Tree, CO 80124</td>\n",
       "      <td>Lone Tree</td>\n",
       "      <td>CO</td>\n",
       "      <td>80124</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Your Opportunity\\nDo you want to be part of a Data Solutions Delivery team managing over 150+ terabytes of data and building the next generation analytics platform for a leading financial firm with over $3.2 trillion in assets under management? At Schwab, the Global Data Technology (GDT) organization governs the strategy and implementation of the enterprise data warehouse and emerging data platforms. We help Marketing, Finance and executive leadership make fact-based decisions by integrating and analyzing data.\\nWe are looking for a Data Engineer who has passion for data and comes with data engineering background. Someone who has experience in designing and coding batch as well as real time ETL and one who wants to be part of a team that is actively designing and implementing the big data lake and analytical architecture on Hadoop. You will have the opportunity to grow in responsibility, work on exciting and challenging projects, train on emerging technologies and help set the future of the Data Solution Delivery team.\\nWhat you’re good at\\nDesigning schemas, data models and data architecture for Hadoop and HBase environments\\nBuilding and maintaining code for real time data ingestion using Java, MapR-Streams (Kafka) and STORM.\\nImplementing data flow scripts using Unix / Hive QL / Pig scripting\\nDesigning, building and support data processing pipelines to transform data using Hadoop technologies\\nDesigning, building data assets in MapR-DB (HBASE), and HIVE\\nDeveloping and executing quality assurance and test scripts\\nWorking with business analysts to understand business requirements and use cases\\nWhat you have\\nMinimum of 2 years of experience in understanding of best practices for building and designing ETL code Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required\\nKnowledge in schema design, developing data models and proven ability to work with complex data is preferred\\nHands-on experience in Java object oriented programming (At least 2 years)\\nHands-on experience with Hadoop, MapReduce, Hive, Pig, Flume, STORM, SPARK, Kafka and HBASE is preferred\\nUnderstanding Hadoop file format and compressions is preferred\\nFamiliarity with MapR distribution of Hadoop is preferred\\nUnderstanding of best practices for building Data Lake and analytical architecture on Hadoop is preferred\\nScripting / programming with UNIX, Java, Python, Scala etc. is preferred\\nStrong SQL experience with the ability to develop, tune and debug complex SQL applications is required\\nKnowledge in real time data ingestion into Hadoop is preferred\\nExperience in working in large environments such as RDBMS, EDW, NoSQL, etc. is preferred\\nKnowledge of Big Data ETL such as Informatica BDM and Talend tools is preferred\\nUnderstanding security, encryption and masking using Kerberos, MapR-tickets, Vormetric and Voltage is preferred\\nExperience with Test Driven Code Development, SCM tools such as GIT, Jenkins is preferred\\nExperience with Graph database is preferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sr Data Engineer</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Responsibilities\\nApply proven expertise and build high-performance scalable data warehouses\\nDesign, build and launch efficient &amp; reliable data pipelines to move and transform data (both large and small amounts)\\nSecurely source external data from numerous partners\\nIntelligently design data models for optimal storage and retrieval\\nDeploy inclusive data quality checks to ensure high quality of data\\nOptimize existing pipelines and maintain of all domain-related data pipelines\\nOwnership of the end-to-end data engineering component of the solution\\nCollaboration with the Data Center SMEs, Data Scientists, and Program Managers\\nSupport on-call shift as needed to support the team\\nDesign and develop new systems in partnership with software engineers to enable quick and easy consumption of data\\nMinimum Qualification\\nBachelor’s degree or Maser degree in Computer Science, Software Engineering, or related field\\n5 + years of SQL (Oracle, Vertica, Hive, etc.) experience and relational databases experience (Oracle, MySQL)\\n5 + years of experience in custom or structured (i.e. Informatica/Talent/Pentaho) ETL design, implementation and maintenance\\n5 + years’ experience in data engineering, experience in applying DWH/ETL best practic\\n5 + years of Java and/or Python development experience\\n5 + years of experience in LAMP and the Big Data stack environments (Hadoop, MapReduce, Hive)\\n5 + years of experience working with enterprise DE tools and experience learning in-house DE tools\\n3+ years exp in AWS data solutions stack – EMR, S3, redshift, Kinesis, ECS, Docker\\n3+ years exp in CI/CD stack – Jenkins , Git\\nPreferred Qualifications\\nMaster’s degree or Bachelor degree in a related field.\\nCloudera Administrator certification.\\nWorking Conditions\\nOffice environment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience working in a Microsoft SQL Server environment, especially with SSIS, stored procedures and query development.\\nKnowledge of data warehousing best practices, concepts and processes.\\nStrong analytical and problem-solving skills, with demonstrated change management experience.\\nDemonstrated ability to set and meet project timelines and deliverables.\\nEffective interpersonal and communications skills with the ability to interact with various levels of personnel.\\nMust be flexible, organized, self-directed, able to prioritize multiple tasks, and able to manage a full workload.\\nExperienced in Microsoft Office and Microsoft Operating Systems.\\nMust be able to work in CCMCN's main office and travel to all required meetings\\nFluency in written and spoken English.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Location: 1212 S Broadway, Denver, CO 80210\\n\\nCategory: Full time, Exempt\\n\\nSalary Range: (DOE)\\n\\nReports To: CEO\\nColorado Community Managed Care Network (CCMCN) is comprised of Colorado’s Community Health Centers with over 190 clinic sites (including school based clinics, pharmacies, and mobile units). CCMCN was founded as a non-profit organization in 1994 to respond pro-actively to the advent of mandatory Medicaid managed care, and has evolved to a multi-faceted organization that serves its members in areas where a network solution optimizes collaborative endeavors. Areas of focus include population health, accountable care, shared services, health information technology and clinical quality improvement programming. CCMCN’s vision is that all Coloradoans have access to high quality, integrated, accountable health care.\\n\\nCCMCN is a one-third founding partner in Colorado Access, a health plan focusing on Medicaid, Medicare and Child Health Plan Plus (CHP+) and CCMCN has been a HRSA-supported Health Center Controlled Network (HCCN) since 1995. CCMCN is governed by a Board of Directors comprised of organizational representatives from each of its health center members as well as representation from Colorado Community Health Network (CCHN). The Board membership includes clinician representatives, elected at-large, and carries out policy and decision-making duties.\\n\\nPosition Description:\\nThe Data Operations department provides data management, integration, and reporting services for multiple external and internal consumers. The Data Engineer will be responsible for all aspects of data management that support CCMCN’s production services. Candidate must have experience in Microsoft SQL database development, data integration, ETL (extract, transform and load) tools and methods, analytics, reporting, and documentation.\\n\\nEssential Functions:\\nProvide development and maintenance support for data integrations between systems.\\nProvide development and maintenance support for the EDW and supporting databases.\\nProvide automated solutions whenever possible and proactively suggest alternative solutions.\\nAssist in the development of new databases and associated processes as necessary.\\nProvide data analytics report development for specific projects as needed.\\nDevelop data validation reports and analysis where applicable.\\nDevelop technical documentation of data integrations and processes.\\nCommunicate and collaborate with other team members and clients to develop innovative data solutions.\\nUtilize up-to-date knowledge of database and data quality best practices to produce effective solutions.\\nRemain knowledgeable in healthcare data standards, measures and code sets as well as applicable data privacy practices and legal requirements.\\nRequired Skills and Experience:\\nExperience working in a Microsoft SQL Server environment, especially with SSIS, stored procedures and query development.\\nKnowledge of data warehousing best practices, concepts and processes.\\nStrong analytical and problem-solving skills, with demonstrated change management experience.\\nDemonstrated ability to set and meet project timelines and deliverables.\\nEffective interpersonal and communications skills with the ability to interact with various levels of personnel.\\nMust be flexible, organized, self-directed, able to prioritize multiple tasks, and able to manage a full workload.\\nExperienced in Microsoft Office and Microsoft Operating Systems.\\nMust be able to work in CCMCN's main office and travel to all required meetings\\nFluency in written and spoken English.\\nAdditional Preferred Skills:\\nStrong business and technical writing abilities.\\nKnowledge of healthcare data standards such as HL7, CCD, CCR and claims data.\\nKnowledge of standard healthcare code sets like LOINC, ICD9/10, CPT4 and SNOMED.\\nKnowledge of IHI triple aim, clinical quality improvement, and primary care operations/workflow.\\nAbility to stay current on healthcare reporting requirements such as UDS, PQRI, NQF, Meaningful Use and Patient Centered Medical Home.\\nAbility to attend conferences and workshops for further education to expand and improve management skills.\\nCCMCN is an equal opportunity employer offering a generous benefit package, generous vacation and holiday schedule, casual work environment, and a flexible work schedule.\\n\\nTO APPLY:\\nPlease submit a resume and cover letter by email to jobs@ccmcn.com.\\nPLEASE INCLUDE THE JOB TITLE IN THE SUBJECT OF THE EMAIL.\\nNo phone calls please.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Denver, CO 80221</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80221</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMust have a solid understanding of data engineering, integration, and warehousing concepts and patterns.\\nMust have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem\\nYou’re proficient with Linux operations and development, including basic commands and shell scripting\\nYou can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices\\nMust be fluent in Python, R, and Java\\nMust have excellent experience command of SQL\\nMust have good experience and knowledge with Data Modeling concepts.\\nYou have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description Summary\\nWe are looking for Data/Machine Learning engineers at all levels to help us build a robust and scalable data platform to support AI/ML data pipelines, reporting and data analysis as our business scales. We use cloud native (AWS) cutting-edge technologies like Spark, Kinesis/Kafka Streaming, Graph , infrastructure as code, CI/CD to deliver high-quality data solutions to analysts, data scientists, and partners. We’re looking for an engineer that takes ownership in their work, has a strong focus on quality, and enjoys working in a collaborative environment. At Transamerica, we believe achieving a secure future requires both smart financial planning and a healthy lifestyle. We’re using data science, machine learning, computer vision, natural language processing, and Iot to revolutionize the way our customers save, invest, protect, and retire and to help them develop better wellness habits. As part of the Data Engineering team in our Analytics Execution group, you will work with data scientists and analytics engagement managers to develop innovative data-based solutions that transform the way we do business.\\nJob Description\\nQualifications:\\nMust have a solid understanding of data engineering, integration, and warehousing concepts and patterns.\\nMust have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem\\nYou’re proficient with Linux operations and development, including basic commands and shell scripting\\nYou can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices\\nMust be fluent in Python, R, and Java\\nMust have excellent experience command of SQL\\nMust have good experience and knowledge with Data Modeling concepts.\\nYou have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills\\nPreferred Qualifications:\\nMust have 3 -5 years of experience building data productionalized pipelines.\\nMust have strong experience ingesting huge volumes of structured and unstructured data both in streaming and batch ingestions patterns.\\n2 - 4 years of Cloud development experience with AWS and or Azure stack.\\nExposure with and have solid experience with statistical analysis and machine learning libraries\\nMust have previous experience with NoSQL database implementations\\nYou understand the fundamentals of lambda architectures and serverless. applications\\nMust be proficient in Tableau\\nMust be comfortable with leveraging ETL tools, like Informatica.\\nYou are proficient in Scala or Node.js\\nYou have a master’s degree in a quantitative field\\nJob Description:\\nPartner with data scientists, analytics engagement managers, and other data engineers to discover, collect, cleanse, and refine the data needed for analysis and modeling\\nAnalyze large data sets to extract actionable insights and inform experimental design and model development\\nDesign robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data\\nBuild models using basic statistical and machine learning techniques, partnering with data scientists for education and guidance\\nWe’re looking for an engineer that takes ownership in their work, has a strong focus on quality, and enjoys working in a collaborative environment.\\nWorking Conditions\\nOffice environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cloud Based Analytics Data Engineer</td>\n",
       "      <td>Englewood, CO</td>\n",
       "      <td>Englewood</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nLinux command line experience.\\nPython experience.\\nWorking knowledge and experience with AWS, S3, RDS/Postgres.\\nExperience with Php / Laravel framework.\\nExperience with JIRA.\\nExperience working in an Agile/SCRUM environment.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience with Relational Database\\nExperience with query tools like SQL\\nExperience using a report designer such as Power BI\\nAbility to work in a team environment.\\nStrong communication skills.</td>\n",
       "      <td>Whether it’s unlocking the potential of digital content, powering breakthrough innovations, creating entertainment that enriches lives, or keeping nations secure, Quantum works with customers and partners to make the world a happier, safer and smarter place.\\nJob Summary and Duties:\\nWe are looking for an experienced candidate to be part of an international team of senior engineers that develop a global data warehouse and information system for scalar tape library devices. This is a business critical role that will enable servicing our hyper scale customers.\\nSpecific duties include but are not limited to:\\nDesign and implement data ETL from logfiles, database dumps, etc. into database target to support managed services for global install base. A typical parsing script will be in Python.\\nAnalyze source data and define target data.\\nImplement ad hoc and scheduled reporting, with Power BI and other reporting tools\\nGather, document, and implement requirements for new reporting.\\n\\nJob Requirements:\\nExperience with Relational Database\\nExperience with query tools like SQL\\nExperience using a report designer such as Power BI\\nAbility to work in a team environment.\\nStrong communication skills.\\n\\nDesired Skills:\\nLinux command line experience.\\nPython experience.\\nWorking knowledge and experience with AWS, S3, RDS/Postgres.\\nExperience with Php / Laravel framework.\\nExperience with JIRA.\\nExperience working in an Agile/SCRUM environment.\\nExperience with containers, AWS cloud, and Azure cloud.\\nQuantum is proud to be an equal opportunity and affirmative action employer. Female/Minority/Veteran/Disabled/Sexual Orientation/Gender Identity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sr Data Architect</td>\n",
       "      <td>Boulder, CO</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and seek to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun and most importantly to each other’s success. Learn more about Splunk careers and how you can become a part of our journey!\\nSplunk is located in Boulder, CO. This team is made up of a strong mix of very talented and driven individuals, with a proven senior leadership team with a strong technical background.\\nThe office layout is open and the floor plan helps to foster the casual, collaborative environment that has played a key role in the company’s innovation of its product. In addition, you will find everything you would expect in the office of a progressive and fun company – stocked kitchen with snacks and drinks to keep you nourished, rooftop patio, and no less than two craft beers on tap for your enjoyment.\\nOur Team's Mission and Methods\\nThe Data Analytics Team at Splunk focuses on delivering actionable data on a silver platter to both external customers and internal data scientists, data analysts and users.\\nOur new Data Analytics Platform was designed to satisfy the data quality requirements of our high-value and high-complexity data, while also providing fast query performance, ease of maintenance, automatic self-healing, immutable data, and fully automated CI/CD code deployment. Our target technology stack includes Kinesis, S3, Lambdas, Kubernetes, Athena (Presto), Parquet, Postgres, Python, and Scala.\\nThe team's influences and backgrounds come from data warehousing, data lakes, big data, analytical data engineering, business intelligence, software engineering, and devops.\\nWe're currently building and transitioning into the Data Analytics Platform - and so are continuing to experiment, explore and evaluate these methods as we move forward.\\nAnd we are committed to making the work fun, interesting, and exciting while collaborating, mentoring, and supporting one another.\\nWho You Are\\nOver the last five or more years you've been working as a data architect or data engineer building and maintaining custom data pipelines and data at rest in support of reporting and data analysis using a number of elements from our technology stack described above.\\nYou think of data as one of the most valuable resources an organization has, not just an inconvenient by-product of a process, but an opportunity-rich source of features and capabilities. You understand that the success of a data engineering team is measured by the success of its data consumers and so think deeply and creatively about ways to deliver high-quality, high-functionality and high-performance data for these users.\\nWhile primarily a data architect with exceptional skills in dimensional modeling, parallel query technology, columnar storage, metadata, and data management; you are also a software engineer comfortable writing clean code and automated tests.\\nYou are an enthusiast - of analyzing data, of helping people make a difference with data, of building great analytical solutions, and of automated testing and software delivery.\\nAnd as a Senior Data Architect, your focus will be primarily on:\\nThe overall flow of data between applications across our incident-management activity and ultimately through our platform and to users.\\nHelping to determine the best technology for different data uses and how to implement it.\\nA deep understanding of all data attributes and business rules within our platform.\\nDeveloping over the wire and at-rest data models - involving considerations of partitioning, queries, write-performance, latency, and usability\\nCollaborating with other teams on our incoming data.\\nAssisting consumers in working with our data\\nAnd since we're a small team, you will also get to occasionally wear the data engineer hat and participate in building, testing and supporting our pipelines.\\nWhat We Offer You:\\nA constant stream of new things for you to learn. We're always expanding into new areas, bringing in open source projects and contributing back, and exploring new technologies.\\nA set of exceptionally talented and dedicated peers, all the way from engineering and QA to product management and user experience.\\nA stable, collaborative and supportive work environment within a team that is fully committed to everyone's success and joy.\\nWe take work-life balance seriously: we prioritize a sustainable engineering culture over heroism, prioritize work that makes your life better, and many of us work from home two days a week.\\nWe value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying.\\nFor job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BI Data Engineer</td>\n",
       "      <td>Boulder, CO</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Seeking of 3-5+ years of experience with data analysis, data warehousing, and delivering reporting\\nFluent in at least SQL syntax (Read functions)\\nExperience with Tableau, Power BI, or Looker\\nProficiency with MS Excel\\nExperience with statistical models and methodologies\\nExperience with eCommerce analysis (CLV, AOV, Retention Rate, etc) a plus\\nExperience with Python, Javascript, and R are a plus\\nExperience with Financial Management, Supply Chain, Warehouse Management, and Manufacturing\\nExperience working with an eCommerce platform\\nBachelor’s Degree in Information Technology or Mathematics or Statistics, or related field preferred\\nExcellent planning and project management skills\\nSelf-starter with a high level of initiative and strong sense of ownership\\nStrong communication and interpersonal skills\\nExperience working in the CPG industry preferred\\nExperience with both B2C and B2B business models preferred\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Charlotte's Web™:\\nCharlotte's Web™ products are made from our world-renowned hemp genetics grown 100% in the USA. Founded by the Stanley Brothers of Colorado, Charlotte’s Web™ leads the industry in quality, safety, consistency and social responsibility to improve thousands of lives daily through the use of Charlotte's Web™. At Charlotte’s Web™, we are driven by principles that extend far beyond the bottom line. It is our goal to provide products of the highest possible quality, while contributing to the sustainability of the communities we have the privilege of serving.\\nPosition Summary:\\nThe BI Data Engineer will be responsible for managing and compiling data and metrics to help drive decision support for all functional business units.\\n\\nEssential Duties:\\nDevelop, implement, and maintain SAAS based data warehouse systems, integrations, and data frameworks.\\nProvide source to target mappings and data model specifications\\nWorking with the Business Intelligence Manager to interface with the business to establish needs and data standards.\\nDeveloping and implementing data collection systems and other tactics that optimize data efficiency and accuracy\\nAcquiring data from primary or secondary data sources and maintaining (through 3rd party) databases\\nScoping actionable analysis projects with functional business units\\nPrioritizing analysis projects on the basis of actionable outputs and potential business impact\\nDeliver summarized analysis, implications for action, and recommended use of insights to stakeholders\\nComplete and deliver large-scale analysis projects pertaining to product selection, customer behavior patterns, and business efficiency\\nCommunicate data-driven insights to functional business groups and ensure a level of understanding that is actionable and impacts the business\\nCollaborate with the Business Intelligence Reporting Analyst to standardize and operationalize deep-dive analyses into relevant reporting dashboards\\nConvey analysis methodology to functional team\\nEnsure that all related documentation (business requirements, technical designs, data dictionary, etc.) are created, maintained and available in a central repository.\\nCoordinate cross-functionally with other departmental managers and Subject Matter Experts (SMEs) in Accounting / Finance, Marketing, Ecommerce, Sales, Supply Chain, Manufacturing, and Cultivation\\nSupport corporate objectives and global growth strategies\\nProvides recommendations for improvements\\nResponsible for escalating support issues to third-parties, as needed\\nProvide regular project status reports\\nPredict project risk factors and address proactively\\nQualifications:\\nSeeking of 3-5+ years of experience with data analysis, data warehousing, and delivering reporting\\nFluent in at least SQL syntax (Read functions)\\nExperience with Tableau, Power BI, or Looker\\nProficiency with MS Excel\\nExperience with statistical models and methodologies\\nExperience with eCommerce analysis (CLV, AOV, Retention Rate, etc) a plus\\nExperience with Python, Javascript, and R are a plus\\nExperience with Financial Management, Supply Chain, Warehouse Management, and Manufacturing\\nExperience working with an eCommerce platform\\nBachelor’s Degree in Information Technology or Mathematics or Statistics, or related field preferred\\nExcellent planning and project management skills\\nSelf-starter with a high level of initiative and strong sense of ownership\\nStrong communication and interpersonal skills\\nExperience working in the CPG industry preferred\\nExperience with both B2C and B2B business models preferred\\n\\nBenefits: We offer best-in-class benefits, including:\\nCompany-Paid Medical, Dental, and Vision\\n3 Weeks of Paid Vacation Your First Year\\n401K Match with Automatic Vesting\\nUp to 9 Weeks Paid Parental Leave\\nSelf-Tailored Wellness Program\\nGenerous Employee Discount\\n\\nCharlotte’s Web™ provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.\\nThis policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Charlotte’s Web™ is an At-Will Employer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Greenwood Village, CO</td>\n",
       "      <td>Greenwood Village</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Innovar Group is looking for a Data Engineer in Greenwood Village, CO\\n\\nCLICK APPLY NOW TO LEARN MORE ABOUT THIS JOB\\nInnovar Group is seeking a talented, driven, and motivated Data Engineer for an exciting Contract-to-hire opportunity in the Denver Tech Center. This is an outstanding opportunity to work for a cutting-edge software team and help design and develop the new iteration of their product.\\nExperience: Contract\\nAbout Us: Innovar Group is comprised of senior talent agents who deliver top recruitment services to clients throughout the United States. We bring a new era of recruiting to the industry by aligning state-of-the-art technology with outstanding talent.\\n\\nInnovar - derived from the Latin “to innovate” - this is what we set out to do each and every day. We strive to go beyond the norm to utilize innovative next-gen recruiting tools connecting us to the world at large in order to uncover high-impact TECHNOLOGY talent for our valued clients. Our goal is straightforward: the unmitigated satisfaction of each client. The difference is significant; we provide consummate service and foster long-term, thriving relationships with our clients. In essence, we work as a vital member of your talent acquisition team.\\n\\nKeywords: Ideal Candidate / Analysis / Create / Support / Teamwork / Implementation / Architecture / Experience / Communication / Knowledge / Technologies /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Engineering Analyst</td>\n",
       "      <td>Broomfield, CO 80020</td>\n",
       "      <td>Broomfield</td>\n",
       "      <td>CO</td>\n",
       "      <td>80020</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Reach Your Peak at Vail Resorts. You're someone who pushes boundaries and challenges the status quo. You're brave, ambitious and passionate in everything you do. And we want you on our team. Pursue your fullest potential and never settle in the quest to deliver extraordinary guest service. Join one of the world's most innovative companies and re-imagine a mountain resort experience with us. Welcome to Vail Resorts. Reach Your Peak.\\nWelcome to Vail Resorts. Reach Your Peak.\\nDEPARTMENT PURPOSE\\nThe mission of the Marketing Analytics team is to leverage Vail Resort’s proprietary guest data to generate actionable insights that unlock value for the business. The team is responsible for informing insight-driven strategic decision making across the entire enterprise.\\n\\nPOSITION PURPOSE\\nAs the Data Engineer you will be developing the data products that enable and accelerate data science and analytics at Vail. You will have opportunity to explore new data sources, prepare, clean, and promote data through our analytic ecosystem. The Data Engineer will partner across the organization to create products aligned with our technology stack and explore new tools and methods to enhance the capabilities of our existing systems. This team will be the back bone of our advanced analytics and data science discipline by providing rich, trusted, and diverse sources.\\n\\nWhy this role is special\\nWe are a technology and data-driven business with a proprietary guest database unmatched by any other company in the industry\\nYou will be a critical part in the acceleration of our advanced analytics and data science programs\\nWe are an incredibly fast-growing company at the forefront of the Travel and Tourism industry – redefining the ski industry in the 21st century\\nWe are a group of motivated, ambitious, and forward looking leaders\\nAll the free skiing / riding you and your dependents can handle – free Epic passes, discounted lodging and retail\\n\\nRESPONSIBILITIES\\n [Develop] Create and support an efficient, clean, and valuable set of data products that push forward what is possible with our expanding data ecosystem.\\n[Innovate] Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\\n[Build] Employ an array of technological languages and tools to connect systems together\\n[Quality Assurance] Partner with team members to ensure the quality of every product produced. Due to the complexity of our work and the credibility we are trusted with, every piece we do must receive high levels of scrutiny.\\n [Knowledge Base] Collaborate with business and IT resources to facilitate documentation of process, tools, and meta-data via our data governance platform in order to advance our discipline and protect our data assets.\\n[Policy and Procedure] Adhere to all Vail Data Governance policies and procedures in enriching and protecting our data assets.\\n\\nJOB REQUIREMENTS\\nAt least 2 years’ experience with Enterprise Data required.\\nAdvanced working SQL knowledge and experience working with relational databases\\nPrevious exposure to development, documentation and maintenance of business data models\\nCandidates must be highly organized and detail-oriented.\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nStrength in written and verbal communication. Excellent interpersonal, with the ability to communicate difficult concepts effectively.\\nHighly collaborative approach and open communication style\\n\\nCOMPUTER SKILLS\\nProficient with MS Office Suite, Outlook &amp; Internet applications\\nHigh level of comfort with technology\\nRelational database skills/understanding in a MS SQL environment\\nExperience with a Data Science tool set including: R, Python, Alteryx, SAS\\nExperience with analytic tools/software such as SQL, SAS, SPSS, Alteryx, MS Excel\\nWorking in on premises and cloud environments\\n\\nPREFERRED KNOWLEDGE\\nFamiliarity with Vail Resorts and its operations is helpful\\n\\nWe offer a variety of career opportunities at our world-class resorts and corporate headquarters near Boulder, Colorado in fields like Finance &amp; Accounting, Human Resources, Information Technology, Legal, Public Affairs &amp; Sustainability, Marketing, Sales &amp; Communications and more. Our corporate team shares both a passion for the outdoors and a drive to re-imagine the mountain resort experience around the world. Learn more at www.vailresortscareers.com\\nVail Resorts is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or any other status protected by applicable law.\\nVRMRKT\\nRequisition ID 171722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Greenwood Village, CO 80111</td>\n",
       "      <td>Greenwood Village</td>\n",
       "      <td>CO</td>\n",
       "      <td>80111</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview\\nBryterCX is seeking a Senior Data Engineer to work with our Denver, CO office. Our ideal candidate has extensive experience and working knowledge of a distributed software architectures, RDBMS, and distributed database technologies. Experience with analytics, time series data, optimization, and creating pipelines for analytics tools and data science is required.\\nKey Responsibilities\\nParticipate in a scrum-based agile delivery team.\\nContribute to architecting and designing performant analytical software systems.\\nEvaluate and tune RDBMS technology and architectures to improve the operational and processing capabilities.\\nAssist in data modeling and query optimization for RDBMS technologies.\\nDrive RDBMS and MPP RDBMS design, creation, and implementations.\\nIdentify and implement ETL improvements to automate, validate and audit data ingestion.\\nImplement software patterns for data retrieval and persistence to support complex analytical queries and storage needs.\\nIdentify technical debt and be pro-active in reducing debt by articulating the value to leadership and project teams.\\nWrite production quality application and database code as required\\nParticipate in driving policies and standards for modeling, structuring, naming, describing, securing, and formatting data\\nQualifications\\nBachelor's degree in Computer Science or equivalent\\nSignificant experience with RDBMS systems, particularly Postgres is required.\\nExperience with data extraction and transformation is required.\\nExperience creating data models and performing query optimization is required.\\nExceptional communication and management skills is required.\\nWorking knowledge of MPP concepts and systems is required.\\nExperience with creating and consuming time series data, data visualization/analytics is preferred.\\nUnderstanding of the Hadoop ecosystem and experience with Hadoop based data lakes and solutions is preferred.\\nExperience in creating technical Proof of Concepts is preferred.\\nExperience in these technologies is preferred: Postgres, Greenplum.\\nExperience in some of these technologies is desirable: Hive, Impala, Spark.\\nKnowledge and experience in these technologies is a plus: Cloud Services, microservices, Python, Map Reduce, Java, Scala, HBase, MapReduce tuning.\\nCompany Overview:\\nBryterCX pioneered and remains the industry leader in providing the premier Journey Analytics (Fox) solution for optimizing self-service information systems. BryterCX unique software modeling solutions enable its customers to translate complex customer interactions across multiple self-service channels, including Interactive Voice Response, Web/Intranet, Email, Chat, Agent Desktop, Routing, and other enterprise applications. Through ongoing application of the ClickFox Customer Behavior Intelligence system, companies can dramatically reduce operational costs, improve customer satisfaction and revenue generation and enhance the overall interactive customer experience.\\n\\nsxCLpsnEXp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data Engineer (Up to 25% Profit Sharing Benefit!)</td>\n",
       "      <td>Denver, CO 80225</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80225</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nClearance: Active TS/SCI with current Poly\\nEducation: Bachelor’s degree or equivalent (Master's preferred)\\nExperience: A minimum of five years of related work experience</td>\n",
       "      <td>\\nMinimum 5 years experience with Linux/Unix environments\\nStrong software scripting skills in Python and other scripting languages (Bash, Perl, etc.)\\nStrong understanding of RDBMS databases (PostgreSQL, Oracle, MySql, etc.)\\nPrior experience working with the Git version-control system\\nStrong systems engineering background\\nMust be certified to meet DoD 8570 level IAT-II qualifications. Security+ certification or CISSP preferred.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\nBITS, a CACI Company, offers very rewarding and unique benefits, which equates to 50% of compensation on TOP of your base salary! The first part is a tax-qualified profit-sharing retirement plan, to which BITS annually contributes up to 25% of your base salary (not in excess of applicable IRS limits) to your retirement account. The second component consists of BITS' Individual Benefit Account (IBA), which is used for premiums, medical reimbursements, dependent care, education and Paid Time Off (PTO) policy. Both components of the BITS benefit package are paid for by BITS, in addition to your base salary and potential performance bonuses. We believe in a healthy home/work balance and both of our locations offer a wide variety of activities to balance with your work life. Learn more at http://www.caci.com/bit-systems/\\nWe are seeking a passionate Data Engineer. The commitment of our employees to \"Engineering Results\" is the catalyst that has propelled BITS to becoming a leader in software development, R&amp;D, sensor development and signal processing. Our engineering teams are highly adept at solving complex problems with the application of leading-edge technology solutions, empowering decision-makers to make better mission-critical decisions. Our operational team excels at signal collection, processing and analysis. We have operational personnel stationed around the world in support of our customers' missions.\\nWhat You’ll Get to Do:\\nAs our Data Engineer you will have the opportunity to:\\nWork with external data providers and stakeholders to engineer the ingest of and distribution of mission-relevant data feeds into the FADE data ecosystem.\\nFocus on the operations of COTS and GOTS software to ensure low latency, high data integrity, quality, and availability for over 400 data feeds (and growing).\\nWork with users to help them troubleshoot and resolve issues with FADE data.\\nWork in a fast moving environment with many moving parts and must be able to juggle several tasks at once.\\nWork with a highly qualified team to maintain the integrity of data flow by responding to real-time operational alerts.\\nBring a continuous delivery mindset while striving for a 99.9% uptime to the flow of data through the system.\\nSpecific duties include:\\nConfigure and maintain data ingest workflows (ETL) across several production systems\\nInstall, configure, and update a wide array of COTS/GOTS and homegrown software applications\\nSupport and troubleshoot diverse IT infrastructure hardware platforms and protocols\\nWork with software development and systems administration staff to monitor and troubleshoot production systems\\nGenerate and maintain systems documentation and diagrams\\nTroubleshoot network issues and establish new connectivity\\nMonitor and maintain a variety of databases\\nYou’ll Bring These Qualifications:\\nClearance: Active TS/SCI with current Poly\\nEducation: Bachelor’s degree or equivalent (Master's preferred)\\nExperience: A minimum of five years of related work experience\\nRequired Skills:\\nMinimum 5 years experience with Linux/Unix environments\\nStrong software scripting skills in Python and other scripting languages (Bash, Perl, etc.)\\nStrong understanding of RDBMS databases (PostgreSQL, Oracle, MySql, etc.)\\nPrior experience working with the Git version-control system\\nStrong systems engineering background\\nMust be certified to meet DoD 8570 level IAT-II qualifications. Security+ certification or CISSP preferred.\\nThese Qualifications Would be Nice to Have:\\nRed Hat Enterprise Linux administration experience\\nUnderstanding of Amazon Web Services – EC2, RDS, S3\\nHadoop, Accumulo and Map Reduce techniques\\nFamiliarity with monitoring tools such as Grafana and Kibana\\nUnderstands compiled languages including Java\\nFamiliarity with Software Development Programs\\nFamiliarity with Agile Development methodologies\\nWhat We can Offer You:\\nWe’ve been named a Best Place to Work by the Washington Post and one of the Top Workplaces in the Denver, Co by the Denver Post.\\nOur employees value the flexibility at CACI that allows them to balance quality work and their personal lives.\\nWe offer competitive benefits and learning and development opportunities.\\nWe are mission-oriented and ever vigilant in aligning our solutions with the nation’s highest priorities.\\nFor over 55 years, the principles of CACI’s unique, character-based culture have been the driving force behind our success.\\nTH-JOBS!BB\\nHJ-BLITZ-101019!\\nJob Location\\nUS-Denver-CO-DENVER\\n\\n\\nCACI employs a diverse range of talent to create an environment that fuels innovation and fosters continuous improvement and success. At CACI, you will have the opportunity to make an immediate impact by providing information solutions and services in support of national security missions and government transformation for Intelligence, Defense, and Federal Civilian customers. CACI is proud to provide dynamic careers for employees worldwide. CACI is an Equal Opportunity Employer - Females/Minorities/Protected Veterans/Individuals with Disabilities.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Boulder, CO 80301</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>CO</td>\n",
       "      <td>80301</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Company Overview\\n\\nFanatics is the global leader in licensed sports merchandise and changing the way fans purchase their favorite team apparel and jerseys. Through an innovative, tech-infused approach to making and selling fan gear in today's on-demand culture, Fanatics operates more than 300 online and offline stores, including the e-commerce business for all major professional sports leagues (NFL, MLB, NBA, NHL, NASCAR, MLS, PGA), major media brands (NBC Sports, CBS Sports, FOX Sports) and more than 200 collegiate and professional team properties, which include several of the biggest global soccer clubs (Manchester United, Real Madrid, Chelsea, Manchester City). Fanatics offers the largest collection of timeless and timely merchandise whether shopping online, on your phone, in stores, in stadiums or on-site at the world's biggest sporting events.\\n\\nAbout the Team\\n\\nFanatics is first and foremost a technology company. We are powered by cutting-edge tech created by our small agile teams using the latest tools and technologies under our highly analytical, forward thinking, and open-minded leadership. As the global leader in licensed sports merchandise, we challenge ourselves by improving our new fully responsive NodeJS cloud commerce platform, Elasticsearch engine, and deep data science capabilities while building the best-in-class retail manufacturing and supply chain technologies. Our tech teams work together to revolutionize data science and engineering initiatives, provide highly scalable real-time and streaming platforms, and create secure e-commerce and in-stadium fan experience products. Our own e-commerce platform transacts in over 190 countries, 17 languages, and 14 currencies. Our motto is “#GSD”—get stuff done—and we do just that. If you want to be at the nexus of sports, commerce, and technology, come be a part of our industry-leading team here at Fanatics Tech.\\n\\nOur inventory intelligence team in close collaboration with data science team has a charter to build data-driven applications &amp; services to develop supply chain &amp; inventory management excellence at Fanatics. The team plays a key role in building data pipelines that extract and process raw data into useful data analytics and aid data scientists to develop predictive models to meet our business’s growing activities and potential. The pipelines are core to inventory replenishment algorithms, pricing optimization, assortment optimization, and deriving key business insights for our merchandising and fulfillment operations. We also build automation tools and monitoring systems to improve our development cycle.\\n\\nWe are seeking for a Senior Data Engineer who has strong architectural skills and upkeeps scalability, availability and excellence when building the next generation of our data pipelines and platform. You are an expert in various data processing technologies and data stores, appreciate the value of clear communication and collaboration, and devote to continual capacity planning and performance fine-tuning for emerging business growth. As the Senior Data Engineer, you will be designing and building inventory intelligence data pipelines and application platform services that power business decisions.\\nWhat will you do?\\nArchitect and build inventory intelligence data pipelines and platform that can parse raw data algorithmically from different data sources, and deliver quality real-time analytical reports for all our replenishment team and our business analytics\\nDevelop clean, safe, testable and cost-efficient solutions; Build fast and reliable pipeline, platform with underlying data model that can scale according to business needs and growth\\nWork with backend engineers to create services that can ingest and supply data to and from external sources, provide data streaming solutions and ensure data quality and timeliness\\nWork with product manager to translate business requirements into scalable solutions, prioritize workload and deliver quality and functional products on a timely manner that can grow over time\\nMake well-informed decisions with deep knowledge of both the internal and external impacts to teams and projects\\nUnderstand the system you are building, foresee shortcomings ahead of time and be able to resolve or compromise appropriately\\nWhat are we looking for?\\nExcellent understanding of data structures algorithms and at least 4 years of experience in distributed systems\\nKnowledge of common design patterns used in Big Data processing\\nStrong development experience using programming languages: Scala, Java, C++, Python\\nProficiency in big data technologies: Spark, Hadoop, Flink, Hive\\nProficiency in Streaming technologies: Apache Kafka, Kafka Streams, KSQL, Spark, Spark Streaming is desired\\nExperience with and deep understanding of traditional, NoSQL and columnar databases such as Oracle, MySQL, PostgreSQL, DynamoDB, Redshift, Vertica\\nKnowledge and experience in designing and developing data modeling &amp; mining, ETL, data warehouse, deployment and infrastructure management, and performance tuning\\nExperience in partnering with architects, engineers in data environments that are complex, enterprise wide, multi-tenant, and host large scale of data\\nAbility to build systems that balance scalability, availability and latency while solving different problems\\nAdvocator of continual deployment and automation tools that can help improve the lives of our engineers\\nA good communicator and team player who has a proven track record of building strong relationships with management, co-workers and customers.\\nA desire to learn and grow, push yourself and your team, share lessons with others and provide constructive and continuous feedbacks, and receptive to feedback from others\\nTryouts are open at Fanatics! Our team is passionate, talented, unified, and charged with creating the fan experience of tomorrow. The ball is in your court now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AWS Systems Engineer</td>\n",
       "      <td>Englewood, CO 80113</td>\n",
       "      <td>Englewood</td>\n",
       "      <td>CO</td>\n",
       "      <td>80113</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBe available to work onsite out of our American Fork, UT or Englewood, CO offices\\nHave a 4-year college degree in Computer Science / Information Technology, master’s degree is preferred or equivalent professional experience\\n3+ years experience in AWS.\\n5+ years experience in Hadoop.\\n5+ years experience with Java programming.</td>\n",
       "      <td>Sling TV L.L.C. provides an over-the-top (internet delivered) television experience on TVs, tablets, gaming consoles, computers, smartphones, smart TVs and other streaming devices. Distributed across a variety of strategic device partners, including Google, Amazon, Apple TV, Microsoft, Roku, Samsung, LG, Comcast, and many others, Sling TV offers two primary domestic streaming services that collectively include more than 100 channels of top content. Featured programmers include Disney/ESPN, NBC, AMC, A&amp;E, EPIX, NFL Network, NBA TV, NHL Networks, Pac-12 Networks, Hallmark, Viacom, and more. For Spanish-speaking customers, Sling Latino offers a suite of standalone and extra Spanish-programming packages tailored to the U S. Hispanic market. And for those seeking International content, Sling International currently provides more than 300 channels in 20 languages (available across multiple devices) to U.S. households.\\n\\nSling TV is the #1 Live TV Streaming Service Sling TV is a next-generation service that meets the entertainment needs of today’s contemporary viewers. Visit www.Sling.com. We are driven by curiosity, pride, adventure, and a desire to win – it’s in our DNA. We’re looking for people with boundless energy, intelligence, and an overwhelming need to achieve to join our team as we embark on the next chapter of our story.\\n\\nOpportunity is here. We are Sling.\\nBasic Requirements:\\nA successful Big Data Engineer will:\\nBe available to work onsite out of our American Fork, UT or Englewood, CO offices\\nHave a 4-year college degree in Computer Science / Information Technology, master’s degree is preferred or equivalent professional experience\\n3+ years experience in AWS.\\n5+ years experience in Hadoop.\\n5+ years experience with Java programming.\\n\\nTechnologies in our environment:\\nHere are some of the key technologies that make up our environment. While we do not expect you to have a detailed understanding of each, the more of these you are familiar with the better.\\n\\nGoLang, Java, Python, JavaScript, Type Script\\nAutomated testing of applications &amp; Continuous Integration / TDD / BDD\\nConfluent Stack / Kafka / ELK Stack / Couchbase / Cassandra / PostGreSQL / Elasticsearch\\nCloud Native tools: Kubernetes / Docker / Rancher / Consul / Vault / Salt Stack / Jenkins / Terraform / AWS / Jaeger / gRPC / Istio / Calico / Envoy\\nBig Data tools: Kestrel / Storm / Spark / Apache Drill / Hive / Phoenix / MapReduce / Yarn / Pig / Hive / HDFS / HBase\\nCI / CD &amp; DevOps Culture\\n12 Factor Applications\\n\\n#LI-SLING2\\nAbout the position\\nOur mission is to build the next generation, web scale platform for SlingTV. Our environment is…\\n\\nComplex\\nHighly elastic\\nBased on some of the latest and greatest cloud native technologies\\nVery fast paced\\nYour team will be…\\n\\nBuilding the AWS based enterprise data lake for the organization\\nDriving a customer centric, highly personalized approach to the evolution of our platform\\nDelivering microservices into a Kubernetes based, web scale environment\\nDelivering software in a SAFe based agile environment, continuously\\nIn order to be successful in this role, you will need to be…\\n\\nHighly motivated, driven, hardworking and open to learning new things\\nNot afraid to fail\\nCapable of architecting and leading the implementation of an enterprise data lake in AWS\\nHelping drive the big data teams’ strategic goals\\nAble to build highly available, best practice based &amp; horizontally scalable solutions\\nAssisting in performance monitoring and capacity planning\\nDriving a data driven organizational culture\\nA team player. We have a great group of diverse folks working together in harmony. Big egos and “super heroes” need not apply.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Google Data Engineer</td>\n",
       "      <td>Denver, CO 80203</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80203</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum of 3 years previous Consulting or client service delivery experience on Google GCP\\n</td>\n",
       "      <td>DevOps on an GCP platform. Multi-cloud experience a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet today’s high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:\\nProvide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on GCP and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security (Cloud IAM, Data Loss Prevention API, etc)Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.\\n- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nBasic Qualifications\\nMinimum of 3 years previous Consulting or client service delivery experience on Google GCP\\nMinimum of 3 years of RDBMS experience\\nMinimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutionsMinimum of 3 years of hands-on experience in GCP and Big Data technologies such as Java, Node.js, C##, Python, PySpark, Spark/SparkSQL, Hadoop, Hive, Pig, Oozie and streaming technologies such as Kafka, Stream Ingestion API, Unix shell/Perl scripting etc.\\nExtensive experience providing practical direction with the GCP Native and Hadoop ecosystem\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using GCP services etc:\\nData Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core\\nData Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore\\nStreaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam\\nData Warehousing &amp; Data Lake : BigQuery, Cloud Storage\\nAdvanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow &amp; Sheets\\nExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\nAble to trval 100% M-TH\\n\\nCandidate Must Have Completed The Following Certifications\\nCertified GCP Developer - Associate\\nCertified GCP DevOps – Professional (Nice to have)\\nCertified GCP Big Data Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an GCP platform. Multi-cloud experience a plus.\\nExperience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion\\nIoT, event-driven, microservices, containers/Kubernetes in the cloud\\nExperience in Apache Maven a plus\\nUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus\\n\\n\\nProfessional Skill Requirements\\nProven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum of a Bachelor’s degree in Computer Science, MIS or related degree and five (5) years of relevant development or engineering experience or combination of education, training and experience.Expert/Advanced level experience with ETL Tools, ODI preferably.Expert Level experience with Oracle as a Database Platform.Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases.Experience or understanding of programming languages like Python, Java, R etc.Experience or understanding of Cloud Data Platforms a plus.Strong understanding of Data Warehousing concepts.Financial Services Industry knowledge is a plus.May occasionally work a non-standard shift including nights and/or weekends and/or have on-call responsibilities.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Builds scalable and reliable Data Integration solutions which are flexible, scalable and elastic.Develops low latency Data Integration solutions to provision data near real time for multiple consumers.Collaborates with Data Engineers, Data Architects and Service developers to build optimal and efficient ETL and Database code.Produces dynamic, data driven solutions to support the strategic business goals.Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute to meet business objectives.Help develop an enterprise scale Data WarehouseDesign and develop new systems and tools to enable stakeholders to consume and understand data fasterSupports ETL processing.Provides on-call support of Data Integration processes on a rotating basis and other on-call as required.Produces dynamic, data driven solutions to support the strategic business goals.Performs other duties and responsibilities as assigned.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description\\n\\nAbout the role:\\nData Engineer works in the Data Engineering team and has primary responsibility for building Enterprise Data Integration solutions by working on enterprise class data integration initiatives. The Data Engineer will be responsible for building solutions which are flexible, performant and scalable. In this role, you are the primary resource on the most complex or escalated issues and may provide direction, guidance and mentoring to team members. You will apply specialized business knowledge, technical skills and creativity to significant deliverables and projects that involve multiple IT departments and business units which have enterprise scope. The Data Engineer should be able to explore newer technology options, if need be, and must have a high sense of ownership over every deliverable.\\nResponsibilities:\\nBuilds scalable and reliable Data Integration solutions which are flexible, scalable and elastic.Develops low latency Data Integration solutions to provision data near real time for multiple consumers.Collaborates with Data Engineers, Data Architects and Service developers to build optimal and efficient ETL and Database code.Produces dynamic, data driven solutions to support the strategic business goals.Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute to meet business objectives.Help develop an enterprise scale Data WarehouseDesign and develop new systems and tools to enable stakeholders to consume and understand data fasterSupports ETL processing.Provides on-call support of Data Integration processes on a rotating basis and other on-call as required.Produces dynamic, data driven solutions to support the strategic business goals.Performs other duties and responsibilities as assigned.\\n\\nQualifications\\nMinimum of a Bachelor’s degree in Computer Science, MIS or related degree and five (5) years of relevant development or engineering experience or combination of education, training and experience.Expert/Advanced level experience with ETL Tools, ODI preferably.Expert Level experience with Oracle as a Database Platform.Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases.Experience or understanding of programming languages like Python, Java, R etc.Experience or understanding of Cloud Data Platforms a plus.Strong understanding of Data Warehousing concepts.Financial Services Industry knowledge is a plus.May occasionally work a non-standard shift including nights and/or weekends and/or have on-call responsibilities.\\nLicenses/Certifications:\\nNone required</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMust have a solid understanding of data engineering, integration, and warehousing concepts and patterns.\\nMust have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem\\nYou’re proficient with Linux operations and development, including basic commands and shell scripting\\nYou can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices\\nMust be fluent in Python, R, and Java\\nMust have excellent experience command of SQL\\nMust have good experience and knowledge with Data Modeling concepts.\\nYou have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Qualifications:\\nMust have a solid understanding of data engineering, integration, and warehousing concepts and patterns.\\nMust have experience with design, build, and maintain batch and streaming data solutions at scale in both on-premises and cloud environments, specifically in the Hadoop ecosystem\\nYou’re proficient with Linux operations and development, including basic commands and shell scripting\\nYou can demonstrate experience with DevOps methodologies and continuous integration/continuous delivery practices\\nMust be fluent in Python, R, and Java\\nMust have excellent experience command of SQL\\nMust have good experience and knowledge with Data Modeling concepts.\\nYou have a passion for data science and machine learning with a strong desire to develop your analysis and modeling skills\\nPreferred Qualifications:\\nMust have 3 -5 years of experience building data productionalized pipelines.\\nMust have strong experience ingesting huge volumes of structured and unstructured data both in streaming and batch ingestions patterns.\\n2 - 4 years of Cloud development experience with AWS and or Azure stack.\\nExposure with and have solid experience with statistical analysis and machine learning libraries\\nMust have previous experience with NoSQL database implementations\\nYou understand the fundamentals of lambda architectures and serverless. applications\\nMust be proficient in Tableau\\nMust be comfortable with leveraging ETL tools, like Informatica.\\nYou are proficient in Scala or Node.js\\nYou have a master’s degree in a quantitative field\\nJob Description:\\nPartner with data scientists, analytics engagement managers, and other data engineers to discover, collect, cleanse, and refine the data needed for analysis and modeling\\nAnalyze large data sets to extract actionable insights and inform experimental design and model development\\nDesign robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data\\nBuild models using basic statistical and machine learning techniques, partnering with data scientists for education and guidance\\nWe’re looking for an engineer that takes ownership in their work, has a strong focus on quality, and enjoys working in a collaborative environment.\\nWorking Conditions\\nOffice environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a Senior Data Engineer, here's what we'll be looking for you to bring:\\n\\n\\nHands-on Engineering Leadership\\nProven track record of Innovation and expertise in Data Engineering\\nTenure in coding, architecting and delivering complex projects\\nDeep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others\\nDeep understanding of streaming data architectures and technologies for real-time and low-latency data processing\\nDeep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies\\nUnderstanding of how to architect solutions for data science and analytics such as productionizing machine learning models and collaborating with data scientists\\nUnderstanding of agile development methods including: core values, guiding principles, and key agile practices\\nUnderstanding of the theory and application of Continuous Integration/Delivery\\nPassion for software craftsmanship\\nA rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..\\nStrong stakeholder management and interaction experience at different levels\\n\\nThere's no typical day or engagement for our Senior Data Engineers. Here's what you'll do:\\n\\n\\nBe the SME. Develop modern data architectural approaches to meet key business objectives and provide end to end data solutions\\nYou might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems.\\nOn other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.\\nIt could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.\\nWhatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.\\nYou have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.\\nYou recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.\\n\\nA few important things to know:\\n-------------------------------\\n\\nProjects are almost exclusively on customer site, so candidates should be flexible and open to extensive travel.\\n\\nCandidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.\\n\\nNot quite ready to apply? Or maybe this isn't the right role for you? That's OK, you can stay in touch with AccessThoughtWorks ( https://www.thoughtworks.com/careers/access?utm_source=apply-jobs&amp;utm_medium=jd&amp;utm_campaign=access-thoughtworks ), our learning community (click \"contact me about recruitment opportunities\" to hear about jobs in the future).\\n\\nIt is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.\\n\\n#LI-NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Data Engineer - Robotic Systems</td>\n",
       "      <td>Boulder, CO</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BS/MS/PhD in Engineering or Computer Science.Experience working in Python, C++, or other Object-Oriented languages.Experience with database schema design and database administration (both No-SQL and SQL a plus)Basic linux system administration skills.Excellent communicator able to act as a bridge between multiple engineering groups.Familiarity with fast growing companies and the associated deadlines and high-energy culture that goes along with launching new products.\\n\\nRecently acquired by Amazon Robotics, Canvas Technology is using spatial AI to provide end-to-end autonomous delivery of goods. By using state-of-the-art cameras and other sensors, the system perceives its surroundings with unrivaled vision and fidelity. Vehicle fleets combine a mix of high-performance sensors with simultaneous localization and mapping software that continuously builds and refines maps in real-time. The vehicles have the capability to ‘see’ and identify different objects, people, other vehicles, and places as they perform their work in a dynamic environment.\\n\\nWork with a world-class team and help develop one of the most advanced 3D computer vision systems in the world. You'll be a key contributor to our top-tier computer vision team, have a huge impact in a developing sector and see your research come to life building indoor and outdoor autonomous vehicles.\\n\\nWe are seeking a talented and highly self-motivated Data Engineer to help stand up a modern data analytics pipeline focused on autonomous vehicle data. We are passionate about building a highly scalable big data system to support our analytics, machine learning, and data applications. A successful candidate should have a background in business analytics, data science, data visualization, and data engineering.\\n\\nResponsibilities:\\nLead the development of a suite of backend extract-translate-load (ETL) applications to aggregate large amounts of vehicle and ground station data.Define the data infrastructure architecture to store large quantities of data and serve higher level consumers of this data.Define the frontend framework to provide a data visualization user interface to serve the engineering community.Work with the software quality and development teams to build targeted analytics applications to help drive autonomous vehicle development and field deployment\\nAmazon is an equal opportunity employer and encourages women and minorities to apply.\\n\\nProven track record standing up and scaling a backend/frontend data analytics toolchainExperience working with AWS Big Data Technologies (EMR, Redshift, S3)Experience with the Python data stack for data cleansing, analysis, and visualization.Experience working with Open Source Big Data tools (Parquet, Spark, Hadoop, Presto)·Experience with Apache Airflow for implementing complex ETL pipelines.Experience with both SQL and No-SQL database administration.Working knowledge of web frontend technologies including javascript and CSS.Basic statistics, statistical signal processing, and statistical data visualization.\\nFamiliarity with modern CI/CD toolchains including source control, issue tracking, automated test, and automated deploy.\\n#canvas #canvastech #canvastechnology#canvas #canvastech #canvastechnology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Denver, CO 80221</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80221</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s degree in a technical field (e.g. Comp Science, Math, Engineering) or related experience\\n2+ years of collective experience in data engineering, data analysis, data warehousing, data integration or business intelligence, in a similarly sized organization\\n2+ years of experience architecting, building and administering big data and real-time streaming analytics architectures in both on premises and cloud environments (AWS, Azure, Google) leveraging technologies such as Hadoop, Spark, S3, EMR, Aurora, DynamoDB, Redshift, Neptune, Cosmos DB\\n1+ years of experience architecting, building and administering large-scale distributed applications\\n1+ years of experience with Linux operations and development, including basic commands and shell scripting\\n2+ years of experience with execution of DevOps methodologies and Continuous Integration/Continuous Delivery within a large scale data delivery environment\\nSoftware development experience in least two or more of following languages: Java, Python, Scala, Node.js\\nExpertise in usage of SQL for data profiling, analysis and extraction</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nWorking collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:\\nArchitect, build and support the operation of our Cloud and On-Premises enterprise data infrastructure and tools\\nDesign robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data\\nBuild data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications\\nAssist in selection and integration of data related tools, frameworks and applications required to expand our platform capabilities\\nUnderstand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description Summary\\nAt Transamerica, we are innovating the next generation of data solutions and capabilities to help our customers achieve a lifetime of financial security. As part of the Data Engineering team in our Enterprise Data Services group, you will apply your engineering skills and passion in developing modern architectures to enable our data-driven digital business.\\nJob Description\\nData Engineers are responsible for the design, architecture and support of the systems, services and applications required for the collection, storage, processing, and analysis of all forms of data in order to enable data-driven decisions and outcomes within the organization.\\nResponsibilities:\\nWorking collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:\\nArchitect, build and support the operation of our Cloud and On-Premises enterprise data infrastructure and tools\\nDesign robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data\\nBuild data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications\\nAssist in selection and integration of data related tools, frameworks and applications required to expand our platform capabilities\\nUnderstand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage\\nQualifications:\\nBachelor’s degree in a technical field (e.g. Comp Science, Math, Engineering) or related experience\\n2+ years of collective experience in data engineering, data analysis, data warehousing, data integration or business intelligence, in a similarly sized organization\\n2+ years of experience architecting, building and administering big data and real-time streaming analytics architectures in both on premises and cloud environments (AWS, Azure, Google) leveraging technologies such as Hadoop, Spark, S3, EMR, Aurora, DynamoDB, Redshift, Neptune, Cosmos DB\\n1+ years of experience architecting, building and administering large-scale distributed applications\\n1+ years of experience with Linux operations and development, including basic commands and shell scripting\\n2+ years of experience with execution of DevOps methodologies and Continuous Integration/Continuous Delivery within a large scale data delivery environment\\nSoftware development experience in least two or more of following languages: Java, Python, Scala, Node.js\\nExpertise in usage of SQL for data profiling, analysis and extraction\\nPreferred Qualifications:\\nMaster’s Degree in a technical field (e.g. Comp Science, Math, Engineering) or related experience\\n1+ years of experience with advanced analytics and machine learning concepts and technology implementations (Tensorflow, H20)\\n2+ years of experience with NoSQL implementations (Mongo, Cassandra, HBase)\\n3+ years of experience in implementing serverless architecture leveraging AWS Lambda or similar technology\\n1+ years of experience with data visualization tools such as Tableau and PowerBI\\nSolid understanding of the Hadoop ecosystem (e.g. HDFS, MapReduce, HBase, Pig, Scoop, Spark, Hive)\\nSolid understanding of the Hadoop ecosystem (e.g. HDFS, MapReduce, HBase, Pig, Scoop, Spark, Hive)\\n2+ years of experience with data warehousing architecture and implementation, including hands on experience developing ETL (Informatica, SSIS, etc.)\\nRelevant technology or platform certification (AWS Certified, Microsoft Certified)\\nBehavioral &amp; Leadership Competencies:\\nAttention to detail and results oriented, with a strong customer focus\\nThe ability to work within a team environment\\nProblem-solving and effective technical communication skills\\nMeet tight deadlines, multi-task, and prioritize workload\\nWilling to learn and keep pace with the latest advances in the related field (Grasp new technologies rapidly as needed to progress varied initiatives )\\nOur Culture:\\nAt Transamerica we promote a Future Fit mindset. What is a Future Fit mindset?\\nActing as One fosters an environment of positive collaboration\\nAccountability allows us to own the problem as well as the solution\\nAgility inspires new ideas, innovation and challenges the status quo\\nCustomer Centricity encourages an above and beyond approach to our customer\\nWorking Environment:\\nOffice environment\\nDue to the nature of the role, work outside of normal business hours may be required as needed\\nOccasional travel less than 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Data Engineer (Imagine Analytics)</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nYou will be responsible for architecting, building and maintaining data pipelines, storage systems, analysis flows, and procedures to support IA’s product objectives and business goals.\\nYou should be comfortable working in a fast-paced environment with quick changes and a high degree of uncertainty.\\nA high level of proficiency in tools and architectures for building modern data infrastructure is a must, as is a desire to learn quickly to adapt to changing technologies and business challenges.\\nAs a member of the engineering team, you will help guide both the technical and usability aspects of the software we build and will be involved all stages of the cycle of iterative product development.\\nYou’ll work with partners and stakeholders to analyze data sources and gather requirements, with designers to define user flows and look and feel, with other engineers to implement functionality, and with customers to provide support and gather feedback.\\nYou will have a high degree of freedom to pick the tools and frameworks that you feel best address the problems at hand and will be responsible for managing and maintaining those technology investments over their entire lifetime.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Why Join IMA?\\nImagine a Company…\\n…that is embarking on a journey to rethink how 300-year-old processes can be engineered to be simple, easy and inspired by the best of consumer tech. A company that looks for the best athletes but loves the art of winning as a team. A company that knows relationships are the currency of the business, and that digitizing them, to reflect their importance today – will only make the future stronger and the industry better. A company working to limit risk every day, while risking it all.\\n\\nImagine being part of a team, where impact is tangible, your voice is part of the story and the work is industry changing. This is not imaginary, it is Imagine Analytics. Explore the frontier of insurance with us!\\n\\nWorking at Imagine Analytics\\nWe are doing highly impactful work in an established industry with a lot of complexity and a rich array of interesting challenges. Learning about insurance markets and working closely with partners to identify opportunities and create efficiencies is what drives us. And we have the best of both worlds, the stability of 40 years in business with the speed and energy of a startup.\\n\\nOur team is a small group of experienced designers, engineers and entrepreneurs. We believe that software development is a team sport, and that the best products are built by teams with diverse backgrounds who are empowered with a high degree of autonomy. We strive for rapid iteration and continuous improvement in both our products themselves and our approaches to building them. We work at a sustainable pace and know from experience that building a lasting product organization is a marathon, not a sprint.\\n\\nWe are building a greenfield product on a modern technology stack, which currently includes React/Apollo, Scala/Spark/Python, Elasticsearch, Postgres, and various serverless technologies. We iterate quickly and deploy continuously, while striving to keep the quality of our user experience high and our codebases tidy. And we have a hell of a lot of fun doing it, together.\\nWhat You’ll Do\\nImagine Analytics (IA) is looking for a Data Engineer to build and maintain technology products for our mid-market insurance data platform. They will work in a highly interdisciplinary fashion with other functions within the team to prioritize, scope, design, build, deliver, and manage features and systems within IA’s product portfolio, operating with a high degree of autonomy and flexibility. The ideal candidate will have prior experience building and managing data systems, as well as working in an early-stage company environment.\\nRoles &amp; Responsibilities\\nYou will be responsible for architecting, building and maintaining data pipelines, storage systems, analysis flows, and procedures to support IA’s product objectives and business goals.\\nYou should be comfortable working in a fast-paced environment with quick changes and a high degree of uncertainty.\\nA high level of proficiency in tools and architectures for building modern data infrastructure is a must, as is a desire to learn quickly to adapt to changing technologies and business challenges.\\nAs a member of the engineering team, you will help guide both the technical and usability aspects of the software we build and will be involved all stages of the cycle of iterative product development.\\nYou’ll work with partners and stakeholders to analyze data sources and gather requirements, with designers to define user flows and look and feel, with other engineers to implement functionality, and with customers to provide support and gather feedback.\\nYou will have a high degree of freedom to pick the tools and frameworks that you feel best address the problems at hand and will be responsible for managing and maintaining those technology investments over their entire lifetime.\\nYou Should Have:\\nBachelor's degree, or related field or equivalent experience\\n2+ years of experience building data pipelines and storage systems. Experience with data warehousing and/or machine learning a plus.\\nDesign Centered: A high degree of product sense &amp; strong appreciation for UX\\nExcitement to explore new technologies and platforms that could add system value\\n\\nThis Job Description is not a complete statement of all duties and responsibilities comprising this position.\\n\\nThe IMA Financial Group, Inc. provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, The IMA Financial Group, Inc. complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>C/C++ Software Developer</td>\n",
       "      <td>Denver, CO 80221</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80221</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>2 or more years experience writing code using C/C++ and Unix\\nBasic understanding of RDBMS databases such SQL Server and Oracle.\\nBasic understanding of modern software design and development methodologies (e.g., OO).\\nBasic understanding of modern SCM (software configuration management).\\nBasic understanding of testing tools and unit test scripting, and testing methodologies.\\nExperience using (or an understanding of the use of) an Integrated Development Environment (e.g., Eclipse, Visual Studio).\\nUnderstanding of basic Database Administration.\\nUnderstanding of quality and security standards. Good verbal and written communication skills.</td>\n",
       "      <td>Participates as a member of development team.\\nCompletes development of units with designs prepared by more senior developers.\\nParticipates in code reviews. Prepares and executes unit tests.\\nApplies growing technical knowledge to maintain a technology area (e.g. Web- site Development).\\nMay perform unit design.\\nApplies company and 3rd party technologies to software solutions of moderate complexity.\\nConfigures end-user or enterprise systems designed by more senior technologists.</td>\n",
       "      <td>Typically a technical Bachelor's degree or equivalent experience and a minimum of 2 years of related experience or a Master's degree and up to two years of experience.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description:\\nApplies specialized knowledge to conceptualize, design, develop, unit-test, configure, and implement portions of new or enhanced (upgrades or conversions) business and technical software solutions through application of appropriate standard software development life cycle methodologies and processes. Interacts with the Client and project roles (e.g., Project Manager, Business Analyst, Data Engineer) as required, to gain an understanding of the business environment, technical context, and organizational strategic direction. Defines scope, plans, and deliverables for assigned components. Understands and uses appropriate tools to analyze, identify, and resolve business and or technical problems. Applies metrics to monitor performance and measure key project parameters. Prepares system documentation. Conforms to security and quality standards. Stays current on emerging tools, techniques, and technologies.\\nResponsibilities:\\nParticipates as a member of development team.\\nCompletes development of units with designs prepared by more senior developers.\\nParticipates in code reviews. Prepares and executes unit tests.\\nApplies growing technical knowledge to maintain a technology area (e.g. Web- site Development).\\nMay perform unit design.\\nApplies company and 3rd party technologies to software solutions of moderate complexity.\\nConfigures end-user or enterprise systems designed by more senior technologists.\\nEducation and Experience Required:\\nTypically a technical Bachelor's degree or equivalent experience and a minimum of 2 years of related experience or a Master's degree and up to two years of experience.\\nKnowledge and Skills:\\n2 or more years experience writing code using C/C++ and Unix\\nBasic understanding of RDBMS databases such SQL Server and Oracle.\\nBasic understanding of modern software design and development methodologies (e.g., OO).\\nBasic understanding of modern SCM (software configuration management).\\nBasic understanding of testing tools and unit test scripting, and testing methodologies.\\nExperience using (or an understanding of the use of) an Integrated Development Environment (e.g., Eclipse, Visual Studio).\\nUnderstanding of basic Database Administration.\\nUnderstanding of quality and security standards. Good verbal and written communication skills.\\nNOTE: This position can be located in El Paso, Texas, OR Conway, Arkansas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>US Contractor - Data Engineer (Big Data)</td>\n",
       "      <td>Louisville, CO</td>\n",
       "      <td>Louisville</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s degree in Computer Science, Computer Engineering or Information Technology\\n8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills\\n5 years of experience of ETL development in a big data environment\\n5 years working in an agile development environment.\\nExperience developing in an AWS environment using S3, EC2, Redshift, Glue, Athena and RDS\\n</td>\n",
       "      <td>\\nBachelor’s degree in Computer Science, Computer Engineering or Information Technology\\n8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills\\n5 years of experience of ETL development in a big data environment\\n5 years working in an agile development environment.\\nExperience developing in an AWS environment using S3, EC2, Redshift, Glue, Athena and RDS\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We work to solve deep technical problems that improve the world of Healthcare. These problems span a variety of core topics in computer science ranging from databases to distributed systems. We are looking for an Experienced Data Engineer for a 6-month contract (possibility to extend or convert to FTE) to join our new Data Organization.\\n\\nPrinciple duties and responsibilities:\\n\\nLeads backend and ETL development effort for the Data Team.\\nDesigns, develops, and performance-tunes extraction, transformation, and load (ETL) processes using SQL, PySpark or Python source-to-target data mappings\\nAnalyzing Use Case requirements and working with teammates to implement defined functionality\\nProvide architectural guidance and development/build standards for the team\\nPromoting collaboration through activities such as design sessions, design reviews, pair programming, etc.,\\nRequired Qualifications &amp; Skills:\\n\\nBachelor’s degree in Computer Science, Computer Engineering or Information Technology\\n8 years of data engineering experience building business intelligence applications with exceptional SQL, PL/SQL, and/or Python skills\\n5 years of experience of ETL development in a big data environment\\n5 years working in an agile development environment.\\nExperience developing in an AWS environment using S3, EC2, Redshift, Glue, Athena and RDS\\nStrong ownership, urgency, and drive to ship code\\nAbility to think outside the normal concepts to implement\\nAbility to communicate technical concepts and designs to cross functional teams and off shore teams with varying degrees of technical experience.\\nDisplays flexibility in adapting to changing conditions.\\nStrong team player, makes a valuable contribution to team objectives, displays trust and mutual understanding, accepts constructive feedback, and handles confrontation constructively.\\nPreferred Qualifications &amp; Skills\\n\\nApplication/system architecture experience\\nExperience with AWS Services RedShift Spectrum, Elastic Search, API Gateway and Lambda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>AWS Data Engineer</td>\n",
       "      <td>Denver, CO 80203</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80203</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.</td>\n",
       "      <td>DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:\\nProvide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.\\n- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)\\n\\nBasic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\n§ Certified AWS Developer - Associate\\n§ Certified AWS DevOps – Professional (Nice to have)\\n§ Certified AWS Big Data Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud\\nExperience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus\\n\\nProfessional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Denver, CO 80202</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80202</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Ibotta is looking for a Senior Data Engineer to build something great with us. As part of the Data Services team, you will work with both Engineering and Analytics to develop and own stable, scalable, and repeatable data-driven features. We're looking for a self-motivated engineer who has a passion for working with an event-based architecture heavily leveraging the AWS cloud data stack &amp; tools. The data engineering team is core to driving and delivering the current and future data, analytics, and decisioning platforms across Ibotta.\\n\\nHere is what you'll be doing:\\n\\nWork with engineering, analytics, and product management to implement data-driven features\\nBe a contributor and architect of distributed systems, frameworks, and design patterns of BI and Data Science/Machine Learning\\nUse Scala, Java or Python to utilize Hadoop/Spark to collect and analyze large-scale datasets in batch and real-time\\nDesign, implement and maintain distributed messaging systems\\nBuild, monitor, and maintain data ETL pipelines\\nManage Data Governance and Security\\nAdminister and maintain our data infrastructure\\nMentor junior and mid-level data engineers in principles and best practices\\nShare relevant knowledge and evangelize Data Engineering with Engineering and Analytics teams\\n\\nHere is what we're looking for:\\n\\nBachelor's degree in Computer Science, Engineering or a related field or equivalent work experience\\n5+ years of experience in software development, preferably with Scala, Java, or Python\\n3+ years of experience working in the Hadoop ecosystem, using tools such as Hive, Spark, or Pig\\nProven expertise in taking large data projects from conception to implementation\\nSubstantial experience with Event-driven architecture design patterns and practices\\nSignificant experience in database design and architecture principles, and expert-level SQL abilities\\nExtensive experience with:\\nAWS DynamoDB, Hive, Cassandra, Bigtable, or other big data stores\\nPython and Java\\nEvent platforms such as Kafka or Kinesis\\nETL tools and processes (Airflow or other similar tools)\\nAgile (Kanban or Scrum) development experience\\n\\nNice to have:\\n\\nExperience with managed, cloud-based data warehouses; e.g. Snowflake, Vertica, etc\\nExperience with BI tools; e.g. Looker, Tableau, etc\\nExperience with data serialization technologies, e.g. Avro, Protobuf, etc\\nExperience with Qubole\\n\\nAbout Us:\\nHeadquartered in Denver, CO, Ibotta (\"I bought a...\") is a free app that's transforming the shopping experience by making every purchase rewarding. The company partners with leading brands and retailers to offer real cash back on groceries, travel, electronics, clothing, gifts, home and office supplies, dining out, and much more. Ibotta is the ultimate starting point for savings, and having paid out more than $500 million in cash rewards to its Savers, it's no surprise why Ibotta is one of the most downloaded shopping apps in the United States\\n\\nLearn more about Ibotta here: https://liferewarded.ibotta.com/press-and-media/ ( https://liferewarded.ibotta.com/press-and-media/ )\\n\\nAdditional Details:\\n\\nThis position is located in Denver, CO and includes competitive pay, benefits package (including medical, dental, vision), 401k, commuter stipend, and equity.\\nIbotta provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, and genetics.\\nApplicants must be currently authorized to work in the United States on a full-time basis.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Responsibilities\\nWork collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment.\\nArchitect, build, and support the operation of Cloud and On-Premises enterprise data infrastructure and tools.\\nDesign robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data.\\nBuild data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications.\\nAssist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities.\\nUnderstand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage.\\n\\nQualifications\\nBachelor’s degree in computer science, math, engineering, or relevant technical field\\nFour years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration, and data integration concepts and methodologies\\nThree years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environments\\nTwo years of experience architecting, building, and administering large-scale distributed applications\\nTwo years of experience with Linux operations and development, including basic commands and shell scripting\\nTwo years of experience with execution of DevOps methodologies and continuous integration/continuous delivery\\nExpertise in SQL for data profiling, analysis, and extraction\\nFamiliarity with data science techniques and frameworks\\nCreative thinker with strong analytical skills\\nResults oriented with a strong customer focus\\nAbility to work in a team environment\\nStrong technical communication skills\\nAbility to prioritize work to meet tight deadlines\\nAbility to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables\\nPreferred Qualifications\\nMaster’s degree in a technical field (e.g. computer science, math, engineering)\\nSolid understanding of big data and real time streaming analytics processing architecture and ecosystems\\nPractical experience with data warehousing architecture and implementation, including hands on experience with source to target mappings and developing ETL code\\nExperience with advanced analytics and machine learning concepts and technology implementations\\nExperience with data analysis and using data visualization tools to describe data\\nRelevant technology or platform certification (AWS, Microsoft, etc.)\\nSoftware development experience in relevant programming languages (i.e. Java, Python, Scala, Node.js)\\nWorking Conditions\\nOffice environment\\nOccasional travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Associate Data Engineer</td>\n",
       "      <td>Boulder, CO 80302</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>CO</td>\n",
       "      <td>80302</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Development of jobs &amp; pipelines from multiple production data sources into Data Lake environments\\nEngineers production ready solutions, inclusive of alerting and error handling\\nWorks with Cloud based tools (Google GCP, Big Query, Dataproc, Composer, Steamsets, Looker, etc.) to deliver best-in-class cloud based data solutions\\nWorks collaboratively with DBA team for operational execution and reliability of data solutions, both in Oracle and BigQuery\\nAssists in maintaining data governance through documentation of data solutions, through ERDs, Confluence documentation, or external tools\\nEngineer &amp; model curated and keyed Data Warehouse solutions that meet business objectives that perform efficiently and effectively\\nWorks in Agile product management method, managing tasks &amp; objectives (user stories) through JIRA and providing updates to SCRUM master\\nPartners with Product Manager (PO) to understand business requirements across multiple functional areas; Store Operations, Merchandising, Supply Chain, Finance, Digital, Customer &amp; Loyalty, Legal, &amp; Data Science\\nSupport current Data Warehouse ETL jobs, respond to tickets and inquiries from business partners when data quality issues occur\\nOther projects and duties as assigned\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Come work for us!\\nWe are looking for dedicated employees to join our team to help our customers have the best experience possible every time they enter a Finish Line Store.\\nOur employees are key to our success.\\nJob Summary:\\nEngineer, Enterprise Data Solutions performs activities related to the data foundation of Finish Line/JD Sports; including development &amp; support of pipelines, ETL jobs and tools, data marts, data lake, and data warehouse in multiple environments. The Engineering position partners with the Product Manager to understand business requirements (user stories), and develops solutions to meet business objectives. The Engineering team (EDS) is responsible for data from production system to BI tool, including movement and transformation. This role reports to the Consulting Engineer.\\nKey Responsibilities and Tasks:\\nDevelopment of jobs &amp; pipelines from multiple production data sources into Data Lake environments\\nEngineers production ready solutions, inclusive of alerting and error handling\\nWorks with Cloud based tools (Google GCP, Big Query, Dataproc, Composer, Steamsets, Looker, etc.) to deliver best-in-class cloud based data solutions\\nWorks collaboratively with DBA team for operational execution and reliability of data solutions, both in Oracle and BigQuery\\nAssists in maintaining data governance through documentation of data solutions, through ERDs, Confluence documentation, or external tools\\nEngineer &amp; model curated and keyed Data Warehouse solutions that meet business objectives that perform efficiently and effectively\\nWorks in Agile product management method, managing tasks &amp; objectives (user stories) through JIRA and providing updates to SCRUM master\\nPartners with Product Manager (PO) to understand business requirements across multiple functional areas; Store Operations, Merchandising, Supply Chain, Finance, Digital, Customer &amp; Loyalty, Legal, &amp; Data Science\\nSupport current Data Warehouse ETL jobs, respond to tickets and inquiries from business partners when data quality issues occur\\nOther projects and duties as assigned\\nRequired Education and/or Experience\\nBachelor’s degree (B.A.) in Information Systems or other related field from a four-year college or university, or equivalent combination of education and experience. 1-3 years of proven work ability in data analytics, data engineering, and process documentation required. Must have experience with partnering with stakeholders of all levels of the organization to plan and solve problems.\\nPhysical Demands – The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\\nEEOC Statement:\\nThe Finish Line, Inc. is an Equal Opportunity Employer and is committed to complying with all federal, state, and local EEO laws. The Finish Line, Inc. prohibits discrimination against employees and applicants for employment based on the individual's race or color, religion or creed, national origin, alienage or citizenship status, marital status, sex, pregnancy status, age, military status, disability, or any other protected characteristic or class protected by law. The Finish Line, Inc. provides reasonable accommodation for disabilities in accordance with applicable laws.\\n\\nNeed accessibility assistance to apply?\\n\\nApplicants who require accessibility assistance to submit an employment application can either call Finish Line at ( 317) 613-6890 or email us at talentacquisition@finishline.com. A member of our Talent Acquisition team will respond as soon as reasonably possible. ( This email address and phone number is only for individuals seeking accommodation when applying for a job. )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMicrosoft SQL Server\\nPostgreSQL\\nMongoDB\\nMicrosoft SSIS\\nBIRST BI\\nValen InsureRight platform</td>\n",
       "      <td>\\n2+ years Data Engineering experience with TSQL, python, map reduce or functional programming\\nBachelor’s degree in programming or related technical areas\\nDeveloping and supporting an end user production system\\nReporting/data warehousing experience\\nInsurance industry knowledge or experience with insurance data a plus</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join a high performing and rapidly growing team\\n\\nValen Analytics is a rapidly expanding advanced data and predictive analytics company headquartered in downtown Denver. Valen’s state-of-the-art analytics and predictive modeling products are built on Valen’s unique industry-wide consortium data platform, specifically designed for property and casualty insurance carriers.\\n\\nValen Analytics is looking for a Data Engineer to expand our growing data processing needs. We are looking for candidates with at least 2 years of experience, who demonstrate a curious analytical mind with ability to understand business objectives, ask insightful questions, and be detail oriented in implementation.\\n\\nAs a Data Engineer, you will work with customers, Valen team members, and 3rd party data providers, to develop, maintain, and enhance our data engineering capabilities in support of our data and predictive analytic offerings to the market.\\n\\nResponsibilities\\n\\nThis position will be part of an existing team whose primary responsibilities are to identify, acquire, validate, cleanse, and produce data and datasets to be used in advanced analytics and predictive modeling initiatives by our customers and internal teams. This is accomplished by combining data processing experience with software engineering concepts into solutions that are hosted in our cloud-based platform, InsureRight.\\n\\nThis position will leverage the following tools and platforms:\\n\\nMicrosoft SQL Server\\nPostgreSQL\\nMongoDB\\nMicrosoft SSIS\\nBIRST BI\\nValen InsureRight platform\\nThis position will make sure of the following skills:\\n\\nData extraction, transformation, and cleansing\\nData profiling and visualization\\nCollaborating with data scientists, software engineers, production operations, subject matter experts and customers\\nFostering continuous delivery pipelines\\nManaging and maintaining metadata\\nThis position requires the ability to:\\n\\nWork in a fast-paced environment as part of a small team\\nCollaborate with team members in the development and maintenance of our solutions\\nIdentify opportunities to automate data engineering tasks and workflow\\nFostering continuous delivery pipelines\\nManaging and maintaining metadata\\nEducation &amp; Experience\\n\\n2+ years Data Engineering experience with TSQL, python, map reduce or functional programming\\nBachelor’s degree in programming or related technical areas\\nDeveloping and supporting an end user production system\\nReporting/data warehousing experience\\nInsurance industry knowledge or experience with insurance data a plus\\nThe Valen Team\\nValen’s mission is to help our clients achieve their goals and solve problems by leveraging data to make more informed decisions.\\n\\nGuide Customer Success: We relentlessly pursue making our customers successful.\\n\\nLive the Golden Rule: We treat our customers, employees, vendors and shareholders how we expect to be treated as customers, employees, vendors and shareholders…period.\\n\\nBe Agile: Valen is a test and learn environment. We organize everything we do around our customer’s success to provide something of value quickly. We learn and then adapt. Then, we learn some more.\\n\\nHave Fun: We have great attitudes and we have fun. We do not take ourselves too seriously, we celebrate our successes and we enjoy our work. Most of all, we live passionately.\\n\\nEmbrace Simplicity: We endeavor to make everything we provide our customers ridiculously easy.\\n\\nExpect Ownership: At Valen we take responsibility for our actions and we build trusting relationships by making and meeting our commitments.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Denver, CO 80202</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80202</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nData modeling and corporate-level data management experience\\nPreviously worked with Windows Workflow Foundation, Windows Workflow Designer and Windows Presentation Foundation technologies\\nExperience with Elasticsearch, Aurora, MySQL, Spark, Streamsets, Kafka, Python, and Scala is a plus</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a Lead Data Engineer on Healthgrades' Facility Data as a Service team, you will develop and maintain database functionality to support corporate products and services, focused on mastering healthcare claims data for consumption across Healthgrades' products. You will collaborate with multi-functional database and product development teams using Agile / Scrum, SQL Server, .Net and Open Source technology.\\nIn this role, you will implement database technologies and development processes to support database development for a Data Platform that is changing the game. On this product, we are currently transitioning our Microsoft technology stacks to other Open Source technology stacks, so experience or interest in learning those tools is a plus. If you are passionate about growing your expertise in these technologies, this will be a great opportunity for you.\\n\\nWhat You Will Do:\\nOversee day-to-day operational matters, provide training, and supervise performance related to company and individual OKRs\\nLead and manage complex development projects including plans, designs, technical leadership, schedules, and resource allocation while also being a hands-on engineer\\nBuild and maintain complex T-SQL statements that perform efficiently against large data sets\\nPerformance tune large Data Warehousing platforms, with a focus on schema bound views, database partitioning, and etl/query optimization\\nBuild and maintain Windows Workflow Engine Packages and Models using Windows Workflow Foundation\\nCreate and maintain automated ETL processes with special focus on data flow, error recovery, and exception handling and reporting\\nCreate and modify data models and implementations as they relate to RDBMS, DW, and BI\\nDesign of specialized data structures for the purpose of data consumption by a public facing website and/or Business Analytics data visualization\\nLoad, process and migrate incoming data feeds and create outgoing data extracts\\nCreate and maintain documentation to support developed applications\\n\\nWhat You Will Bring:\\nAbility to participate in a culture of communication, collaboration and creativity\\nPrevious experience in a lead or management role with direct reports\\nStrong RDBMS and Microsoft SQL Server 2014/2016 skills\\nSQL Programming / ETL and data architecture management experience\\nExperience building and maintaining database structures, ETL processes, stored procedures, audit reports, data extracts, SSIS, SSAS, SSRS, etc. to meet project objectives\\nPerform Unit Tests and internal QA checks to insure high quality work\\nGood collaboration and idea sharing in team environment\\nA Bachelor’s Degree in related field or equivalent experience\\n\\nPreferred Qualifications:\\nData modeling and corporate-level data management experience\\nPreviously worked with Windows Workflow Foundation, Windows Workflow Designer and Windows Presentation Foundation technologies\\nExperience with Elasticsearch, Aurora, MySQL, Spark, Streamsets, Kafka, Python, and Scala is a plus\\n\\nWhy Healthgrades?\\nAt Healthgrades, we recognize that our people drive our greatest achievements. We are passionate about maintaining a fulfilling, rewarding and high-energy work environment while setting the stage for your continued success.\\nMeaningful Work – empowering consumers with data to make the right decisions for themselves and their families\\nChanging the Game - evolving, dynamic culture with career advancement opportunities\\nCommunity Builders- participating in local charity organizations and wellness initiatives\\nRobust Perks – generous PTO, 401k contributions, tuition assistance, entertainment discounts &amp; more!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Data Engineer - Node</td>\n",
       "      <td>Denver, CO 80246</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80246</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience with databases SQL and NoSQL\\nDeep knowledge of Node, JavaScript,\\nExperience with scaling backend of large-scale applications\\nAlgorithms\\nPositive attitude\\nCurious, eager to learn, and colaborate\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Required: Bachelor’s degree in Computer Science or equivalent work experience\\nPreferred: Master’s degree</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer – Node [Denver, CO]\\nBecome a part of something great at MeetingOne.\\n\\nMeetingOne is a rapidly growing software development and services company focused on audio and web conferencing technology solutions. Headquartered in Denver, Colorado, MeetingOne is a full-service audio and web conferencing, e-learning, event solution and consulting services provider. Since 1999, MeetingOne has enabled businesses and educational organizations around the world to communicate more effectively, using innovative virtual meeting and event technologies and services. More info at www.meetingone.com.\\n\\nThis position is a great opportunity to own the core technology stack that is vital to our growing company. Learn, develop, refactor and master the heart of our product portfolio that in turn achieves high velocity in driving value to our beloved clients. Demonstrate the skills and knowledge you collected over the years to help lead our development efforts to success.\\n\\nAs the Software Engineer, you will be reporting to the Software Development Manager. Your focus is on owning key layers in our technology. You will have the opportunity to imagine, create and deliver leading edge solutions that meet the needs of a rapidly expanding user base. You will have the ability to influence people and process to achieve this goal.\\n\\nYou will have the opportunity to mentor other developers, sharing your years of experience with tactics and strategies that lead to quality coding. With senior experience, you are a development team advocate. Ensuring the right tools are available and continuous training opportunities are available.\\n\\nYou will work closely with QA and Product to aid in the dependable release of value. You will be responsible for handling development related items escalated with respective core technology stack.\\n\\nAt the end of the day you love working on the most valuable technology the company owns. You strive at every opportunity to leverage bleeding techniques and technology to express your creative side. You enjoy having an impact on co-workers and the user community a-like.\\n\\nWhat does success look like?\\n\\nYou exhibit creative problem solving to achieve effective results.\\nYou prioritize action to drive achievements that delight.\\nYou enjoy independence to define, direct, and/or perform critical thinking to resolve complex issues.\\nQualifications/ Experience:\\n\\nExperience with databases SQL and NoSQL\\nDeep knowledge of Node, JavaScript,\\nExperience with scaling backend of large-scale applications\\nAlgorithms\\nPositive attitude\\nCurious, eager to learn, and colaborate\\nPluses:\\nExperience with audio, video conferencing, WebRTC, SFU, MCU, WebAssembly\\nExperience with other languages e.g. C++, LUA\\nExperience with IM/Presence (XMPP, Other,…)\\nExperience with Docker, AWS\\nTelecommunications knowledge\\nEducation OR equal work experience:\\n\\nRequired: Bachelor’s degree in Computer Science or equivalent work experience\\nPreferred: Master’s degree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Lone Tree, CO 80124</td>\n",
       "      <td>Lone Tree</td>\n",
       "      <td>CO</td>\n",
       "      <td>80124</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Your Opportunity\\nDo you want to be part of a Data Solutions Delivery team managing over 150+ terabytes of data and building the next generation analytics platform for a leading financial firm with over $3.2 trillion in assets under management? At Schwab, the Global Data Technology (GDT) organization governs the strategy and implementation of the enterprise data warehouse and emerging data platforms. We help Marketing, Finance and executive leadership make fact-based decisions by integrating and analyzing data.\\nWe are looking for a Data Engineer who has passion for data and comes with data engineering background. Someone who has experience in designing and coding batch as well as real time ETL and one who wants to be part of a team that is actively designing and implementing the big data lake and analytical architecture on Hadoop. You will have the opportunity to grow in responsibility, work on exciting and challenging projects, train on emerging technologies and help set the future of the Data Solution Delivery team.\\nWhat you’re good at\\nDesigning schemas, data models and data architecture for Hadoop and HBase environments\\nBuilding and maintaining code for real time data ingestion using Java, MapR-Streams (Kafka) and STORM.\\nImplementing data flow scripts using Unix / Hive QL / Pig scripting\\nDesigning, building and support data processing pipelines to transform data using Hadoop technologies\\nDesigning, building data assets in MapR-DB (HBASE), and HIVE\\nDeveloping and executing quality assurance and test scripts\\nWorking with business analysts to understand business requirements and use cases\\nWhat you have\\nMinimum of 3-5 years of experience in understanding of best practices for building and designing ETL code Strong SQL experience with the ability to develop, tune and debug complex SQL applications is required\\nKnowledge in schema design, developing data models and proven ability to work with complex data is required\\nHands-on experience in object oriented programming (At least 2 years)\\nHands-on experience with Hadoop, MapReduce, Hive, Pig, Flume, STORM, SPARK, Kafka and HBASE is required\\nUnderstanding Hadoop file format and compressions is required\\nFamiliarity with MapR distribution of Hadoop is preferred\\nUnderstanding of best practices for building Data Lake and analytical architecture on Hadoop is preferred\\nScripting / programming with UNIX, Java, Python, Scala etc. is preferred\\nStrong SQL experience with the ability to develop, tune and debug complex SQL applications is required\\nKnowledge in real time data ingestion into Hadoop is preferred\\nExperience in working in large environments such as RDBMS, EDW, NoSQL, etc. is required\\nKnowledge of Big Data ETL such as Informatica BDM and Talend tools is required\\nUnderstanding security, encryption and masking using Kerberos, MapR-tickets, Vormetric and Voltage is preferred\\nExperience with Test Driven Code Development, SCM tools such as GIT, Jenkins is preferred\\nExperience with Graph database is preferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Lone Tree, CO 80124</td>\n",
       "      <td>Lone Tree</td>\n",
       "      <td>CO</td>\n",
       "      <td>80124</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Your Opportunity\\nThe HR Technology organization is a highly skilled and passionate team that explores new ways to demonstrate our data as a strategic asset. Through hands-on initiatives with our business partners we contribute to the company’s customer experience, revenue and profitability. Our team is looking for a highly-motivated, self-driven individual contributor well versed in database development technologies and fluent in Agile.\\nWhat you’re good at\\nAs a member of a group with broad business impact, you will work directly with counterparts in the Incentives Administration organization and HR Technology in support of Schwab’s compensation applications. You will also work to uncover and define new needs, find appropriate solutions, and ensure implementation and delivery within specified timeframes.\\nDevelop and code based on the “How” as defined by the technical team lead.\\nWork closely with fellow developers to determine sizing and effort of user stories; code user story tasks as defined by Senior Development Team Members.\\nBe committed to Agile Methodology.\\nAs a developer you will be a self-driven individual contributor; provide efficiently organized and designed logic; be highly-motivated and well versed in developing easily maintainable applications.\\nWhat you have\\n3+ year’s experience as a Sr. Data Engineer\\nSignificant SQL skills with the ability to write code freehand completely from scratch and understand existing complex SQL\\nExperience with Python and/or Perl\\nDemonstrated experience with data modeling and performance tuning queries\\nTeradata experience a strong plus\\nTrack record of solid project execution with deliverables having minimal defects\\nMust be creative, critical thinker and solutions-oriented\\nStrong analytical and problem-solving skills\\nExcellent written and verbal communication skills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Data Engineer - Hux</td>\n",
       "      <td>Denver, CO 80203</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80203</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.\\n2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.\\n1+ years of experience on distributed, high throughput and low latency architecture.\\n1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.\\nA successful track-record of manipulating, processing and extracting value from large disconnected datasets.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Hux Data Engineer\\nLocations: New York, NY – Greensboro, NC - Chicago, IL – Raleigh, Durham, Chapel-Hill, NC - Denver, CO\\nWhat is Hux? Hux is the Human Experience Platform by Deloitte Digital.\\nIn today’s world, customers expect companies to know who they are and what they want. Customers want to have products, services or experiences that best suit their needs delivered to them seamlessly across physical and digital channels.\\nCustomers are human first: driven by dynamic wants, needs, and desires. The ability for brands to make personal, meaningful connections on a human level has never been greater and Hux by Deloitte Digital delivers on those experiences in a way that allows companies to own the customer journey end to end. We help companies connect key data sources to understand what matters most to people; connect to advanced technologies like AI and machine learning to sense and respond to those needs at scale; and connect their systems to unlock insights, create collaboration and drive acquisition, engagement and loyalty. Most importantly, we empower companies to connect with customers in personal, meaningful ways that respect them as people, not just customers.\\nHux by Deloitte Digital gives companies the ability to build and leverage the connections – between people, systems, data and technologies – so they can deliver personalized, contextual experiences to customers at scale.\\n\\nWork you’ll do\\nAs a Hux Data Engineer, you’ll design, implement, and maintain a full suite of real-time and batch jobs that fuels our cutting edge AI to provide real-time marketing intelligence to our existing clients.\\nYou’ll develop, test and deliver production grade code to help our clients solve their marketing challenges using cutting-edge big-data tools. You’ll also ensure data integrity, resolve production issues, and assist in the support and maintenance of our overall Platform.\\nAs you grow your capabilities and learn how to build a platform that can ingest, load and process billions of data points, you’ll enjoy new challenges and opportunities to showcase your development skills by joining project teams to build innovative new-client platforms and execute high-value strategic development projects with high visibility.\\nYour responsibilities will include:\\nDesign, construct, install, test and maintain highly scalable data pipelines with state-of-the-art monitoring and logging practices.\\nBring together large, complex and sparse data sets to meet functional and non-functional business requirements.\\nDesign and implements data tools for analytics and data scientist team members to help them in building, optimizing and tuning our product.\\nIntegrate new data management technologies and software engineering tools into existing structures.\\nHelp build high-performance algorithms, prototypes, predictive models and proof of concepts.\\nUse a variety of languages, tools and frameworks to marry data and systems together.\\nRecommend ways to improve data reliability, efficiency and quality.\\nCollaborate with Data Scientists, DevOps and Project Managers on meeting project goals.\\nTackle challenges and solve complex problems on a daily basis.\\nQualifications\\nRequired:\\n4+ years of experience in software development, a substantial part of which was gained in a high-throughput, decision-automation related environment.\\n2+ years of experience in working with big data using technologies like Spark, Kafka, Flink, Hadoop, and NoSQL datastores.\\n1+ years of experience on distributed, high throughput and low latency architecture.\\n1+ years of experience deploying or managing data pipelines for supporting data-science-driven decisioning at scale.\\nA successful track-record of manipulating, processing and extracting value from large disconnected datasets.\\nPreferred:\\nProducing high-quality code in Python.\\nPassionate about testing, and with extensive experience in Agile teams using SCRUM you consider automated build and test to be the norm.\\nProven ability to communicate in both verbal and writing in a high performance, collaborative environment.\\nFollows data development best practices, and enjoy helping others learn to do the same.\\nAn independent thinker who considers the operating context of what he/she is developing.\\nBelieves that the best data pipelines run unattended for weeks and months on end.\\nFamiliar with version control, you believe that code reviews help to catch bugs, improves code base and spread knowledge.\\nHelpful, but not required:\\nKnowledge in:\\nExperience with large consumer data sets used in performance marketing is a major advantage.\\nFamiliarity with machine learning libraries is a plus.\\nWell-versed in (or contributes to) data-centric open source projects.\\nReads Hacker News, blogs, or stays on top of emerging tools in some other way\\nData visualization\\nIndustry-specific marketing data\\nTechnologies of Interest:\\nLanguages/Libraries – Python, Java, Scala, Spark, Kafka, Hadoop, HDFS, Parquet.\\nCloud – AWS, Azure, Google\\nThe team\\nAdvertising, Marketing &amp; Commerce\\nOur Advertising, Marketing &amp; Commerce team focuses on delivering marketing and growth objectives aligned with our clients’ brand values for measurable business growth. We do this by creating content, communications, and experiences that engage and inspire their customers to act. We implement and operate the technology platforms that enable personalized content, commerce and marketing user-centric experiences. In doing so, we transform our clients’ marketing and engagement operations into modern, data-driven, creatively focused organizations. Our team brings deep experience in creative and digital marketing capabilities, many from our Digital Studios.\\n\\nWe serve our clients through the following types of work:Cross-channel customer engagement strategy, design and development(web, mobile, social, physical)eCommerce strategy, implementation and operationsMarketing Content and digital asset management solutionsMarketing Technology and Advertising Technology solutionsMarketing analytics implementation and operationsAdvertising campaign ideation, development and executionAcquisition and engagement campaign ideation, development and executionAgile based, design-thinking, user-centric, empirical projects that accelerate results\\n\\nHow you’ll grow\\nAt Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.\\nBenefits\\nAt Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.\\nDeloitte’s culture\\nOur positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.\\nCorporate citizenship\\nDeloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.\\nRecruiter tips\\nWe want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.\\nkwhux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Data Engineer (Mid and Senior)</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>2+ years designing and developing data analytics solutions\\n2+ years with RDBMS such as SQL Server, Oracle, MySQL\\n2+ years data warehouse, dimensional modeling design and architecture\\nA passion to learn and improve your skills to deliver the best possible solutions to customers\\nExperience with cloud based data services offered by Azure, AWS and Google\\nExperience with data visualization tools such as Power BI and Tableau\\nPrevious consulting experience preferred\\nDegree in computer science, information technology, engineering or business\\nMust be authorized to work in the US. We are unable to sponsor H-1B visas at this time.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Hands-on development and serve as technical expert on projects\\nDevelop data solutions leveraging traditional and cloud product offerings from leading vendors\\nDevelop data models to meet client needs\\nDevelop data models to meet client needs, including transactional, third-normal form, dimensional, columnar, distributed and NoSQL\\nDevelop ETL/ELT processes and patterns to efficiently move data\\nCreate data visualizations, dashboards and reports as needed\\nDevelop and scope requirements\\nTravel as needed (currently less than 5%)\\nMaintain effective communication with team and customers</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer, Mid to Senior Level\\nDatalere team members lead by example, focus on customer needs and have a thirst to learn all they can about data analytics. Successful candidates are self-starters and never shy away from challenges.\\nWe need team members that excel when working directly with clients to meet their goals. They understand the client's needs and requirements and build a collaborative environment to ensure a successful project delivery.\\nData Engineers analyze and develop on-premises and/or cloud data and ETL solutions to solve the client's challenges. They enjoy the challenges of consulting and thrive to knock the socks off of clients\\nPlease note that this role is vendor agnostic in regards to what ETL tools are used, so having multi vendor experience would be ideal.\\nResponsibilities:\\nHands-on development and serve as technical expert on projects\\nDevelop data solutions leveraging traditional and cloud product offerings from leading vendors\\nDevelop data models to meet client needs\\nDevelop data models to meet client needs, including transactional, third-normal form, dimensional, columnar, distributed and NoSQL\\nDevelop ETL/ELT processes and patterns to efficiently move data\\nCreate data visualizations, dashboards and reports as needed\\nDevelop and scope requirements\\nTravel as needed (currently less than 5%)\\nMaintain effective communication with team and customers\\nQualifications\\n2+ years designing and developing data analytics solutions\\n2+ years with RDBMS such as SQL Server, Oracle, MySQL\\n2+ years data warehouse, dimensional modeling design and architecture\\nA passion to learn and improve your skills to deliver the best possible solutions to customers\\nExperience with cloud based data services offered by Azure, AWS and Google\\nExperience with data visualization tools such as Power BI and Tableau\\nPrevious consulting experience preferred\\nDegree in computer science, information technology, engineering or business\\nMust be authorized to work in the US. We are unable to sponsor H-1B visas at this time.\\nAbout Us\\nAt Datalere, we work with our clients to transform their enterprise through the use of modern compute technologies and proven deployment processes providing cost effective durable solutions for the competitive world.\\nIf you are seeking new challenges, interested in staying up to date with the latest releases and can deliver uncompromised service to our customers, then we'd like to hear from you. If you are interested and meet the above qualifications, please submit your resume and cover letter indicating your interest to join our team.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Denver, CO 80202</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80202</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s degree in computer science, math, engineering, or relevant technical field\\n4+ years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration, and data integration concepts and methodologies\\n3+ years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environments\\n3+ years of experience with execution of DevOps methodologies and continuous integration/continuous delivery\\nObject-oriented/object function scripting languages: Python, R, C/C++, Java, Scala, etc.\\nSQL, relational databases and NoSQL databases\\nData integration tools (e.g. Talend, SnapLogic, Informatica) and data warehousing / data lake tools\\nAPI based data acquisition and management\\nMSSQL, PostgreSQL, MySQL, etc. - MemSQL, CrateDB, etc.\\nBusiness intelligence tools such as Tableau, PowerBI, Zoomdata, Pentaho, etc.\\nData modeling tools such as ERWin, Enterprise Architect, Visio, etc.\\nData integration tools such as Boomi, Pentaho, Talend, Informatica, SnapLogic, etc.\\nFamiliarity with cloud-based data engineering (AWS, GCP, or Azure)\\nFamiliarity with data science techniques and frameworks\\nCreative thinker with strong analytical skills\\nAbility to work in a team environment\\nStrong technical communication skills\\nAbility to prioritize work to meet tight deadlines\\nAbility to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nArchitect, build, and support the operation of enterprise data and analytical infrastructure and tools\\nDesign robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data\\nBuild data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications\\nAssist in the selection and integration of data related tools, frameworks, and applications required to expand platform capabilities\\nUnderstand and implement best practices in the management of enterprise data, including master data, reference data, metadata, data quality and lineage\\nDevelop and prepare strategies for Business Intelligence processes for the organization\\nManage and customize all ETL processes as per customer requirement and analyze all processes for same\\nPerform assessment on all reporting requirements and contribute to the development of a long-term strategy for various reporting solutions\\nCoordinate with data generator and ensure compliance to all enterprise data model according to data standards\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At Ping Identity, we're changing the way people think about enterprise security technology. With our innovative Identity Defined Security platform, we're helping to build a borderless world where people have total freedom to work wherever and however they want. Without friction. Without fear.\\n\\nWe're headquartered in Denver, Colorado, and we have offices and employees around the globe. And we serve the largest, most demanding enterprises worldwide, including over half of the Fortune 100. Because even in the most complex enterprise environments, security shouldn't be a source of anxiety. It should be one of your greatest competitive advantages.\\n\\nWe call this digital freedom. And it's not just something we provide our customers. It's something that drives our company. People don't come here to join a culture that's build on digital freedom. They come to cultivate it.\\n\\nThe Senior Data Engineer will have the opportunity to play a critical role in the early stages of developing Ping’s Business Intelligence and Data Analytics capabilities. This individual will collaborate with other teams across the organization (e.g., Product Management, Engineering, Sales, and Finance) to gain a quick understanding of Ping products, business process areas, and/or technologies, and build the analytical framework to enable the business to better understand and leverage complex data sets. The Senior Data Engineer will be expected to architect and build core datasets, implement efficient ETL processes, and own the design, development and maintenance of critical metrics, reports, analyses, dashboards, etc. to drive key business decisions. In addition, this individual must be able to adapt and thrive in a fast-paced and changing business and technical environment.\\n\\nKey Responsibilities:\\n\\nArchitect, build, and support the operation of enterprise data and analytical infrastructure and tools\\nDesign robust, reusable, and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data\\nBuild data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications\\nAssist in the selection and integration of data related tools, frameworks, and applications required to expand platform capabilities\\nUnderstand and implement best practices in the management of enterprise data, including master data, reference data, metadata, data quality and lineage\\nDevelop and prepare strategies for Business Intelligence processes for the organization\\nManage and customize all ETL processes as per customer requirement and analyze all processes for same\\nPerform assessment on all reporting requirements and contribute to the development of a long-term strategy for various reporting solutions\\nCoordinate with data generator and ensure compliance to all enterprise data model according to data standards\\nEssential Qualifications:\\n\\nBachelor’s degree in computer science, math, engineering, or relevant technical field\\n4+ years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration, and data integration concepts and methodologies\\n3+ years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environments\\n3+ years of experience with execution of DevOps methodologies and continuous integration/continuous delivery\\nObject-oriented/object function scripting languages: Python, R, C/C++, Java, Scala, etc.\\nSQL, relational databases and NoSQL databases\\nData integration tools (e.g. Talend, SnapLogic, Informatica) and data warehousing / data lake tools\\nAPI based data acquisition and management\\nMSSQL, PostgreSQL, MySQL, etc. - MemSQL, CrateDB, etc.\\nBusiness intelligence tools such as Tableau, PowerBI, Zoomdata, Pentaho, etc.\\nData modeling tools such as ERWin, Enterprise Architect, Visio, etc.\\nData integration tools such as Boomi, Pentaho, Talend, Informatica, SnapLogic, etc.\\nFamiliarity with cloud-based data engineering (AWS, GCP, or Azure)\\nFamiliarity with data science techniques and frameworks\\nCreative thinker with strong analytical skills\\nAbility to work in a team environment\\nStrong technical communication skills\\nAbility to prioritize work to meet tight deadlines\\nAbility to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables\\nDesired Qualifications:\\n\\nExperience with advanced analytics and machine learning concepts and technology implementations\\nExperience in a fast-paced, ever-changing and growing environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.\\n\\nAs a Senior Data Engineer, here's what we'll be looking for you to bring:\\n\\n\\nHands-on Engineering Leadership\\nProven track record of Innovation and expertise in Data Engineering\\nTenure in coding, architecting and delivering complex projects\\nDeep understanding and application of modern data processing technology stacks. For example Spark, Kafka, Hadoop, ecosystem technologies, and others\\nDeep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies\\nDeep understanding of relational database technologies and database development techniques\\nUnderstanding of how to architect solutions for data science and analytics\\nData management for reporting and BI experience is a plus\\nUnderstanding of \"Agility\", including core values, guiding principles, and key agile practices\\nUnderstanding of the theory and application of Continuous Integration/Delivery\\nPassion for software craftmanship\\nA rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..\\nStrong stakeholder management and interaction experience at different levels\\nAny experience building and leading an offshore/outsourcing function would be highly beneficial.\\n\\nThere's no typical day or engagement for our Senior Engineers. Here's what you'll do:\\n\\n\\nBe the SME. Develop Big Data architectural approach to meet key business objectives and provide end to end development solution\\nYou might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that Big Data has to solve their most pressing problems.\\nOn other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.\\nIt could be much more about getting stuck into a delivery project where you're equally happy coding and tech leading the team to implement the solution.\\nWhatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.\\nYou have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.\\nYou recognize that building your network with a client is absolutely key to enable you to perform in your role. You'll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.\\n\\nRegardless of what you do at ThoughtWorks, you'll always have the opportunity to:\\n\\n\\nThink through hard problems, and work with a team to make them reality.\\nLearn something new every day.\\nWork in a dynamic, collaborative, transparent, non-hierarchal, and ego-free culture where your talent is valued over a role title\\nTravel the world.\\nSpeak at conferences.\\nWrite blogs and books.\\nDevelop your career outside of the confinements of a traditional career path by focusing on what you're passionate about rather than a predetermined one-size-fits-all plan\\nBe part of a company with Social and Economic Justice at the heart of its mission.\\n\\nA few important things to know:\\n-------------------------------\\n\\nProjects are almost exclusively on customer site, so candidates should be flexible and open to travel.\\n\\nCandidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.\\n\\nNot quite ready to apply? Or maybe this isn't the right role for you? That's OK, you can stay in touch with AccessThoughtWorks ( https://www.thoughtworks.com/careers/access?utm_source=apply-jobs&amp;utm_medium=jd&amp;utm_campaign=access-thoughtworks ), our learning community (click \"contact me about recruitment opportunities\" to hear about jobs in the future).\\n\\nIt is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.\\n\\n#LI-NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Data Engineer (ADX-98-19)</td>\n",
       "      <td>Boulder, CO 80301</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>CO</td>\n",
       "      <td>80301</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>POSITION SUMMARY:\\nThe Data Engineer will contribute key skills and expertise to our rapidly growing In Vitro Diagnostics program. The candidate will help plan, develop, and build data infrastructure critical to the program’s core data pipeline and will be responsible for ensuring the accuracy and usability of data central to regulatory approval of ArcherDX’s diagnostics. This person will work closely with program technical leads, data scientists, and quality assurance personnel to think creatively and achieve ambitious goals to advance the treatment of genetic diseases.\\n\\nRESPONSIBILITIES:\\nDevelop understanding of scientific, analytical, and regulatory needs for data infrastructure\\nThink creatively to plan adaptable data solutions for a growing and quickly evolving program\\nIntegrate custom software and databases with a commercial laboratory information management system\\nDevelop programmatic solutions to collect, parse, transform, and transfer data\\nBecome familiar with and employ industry-leading data standards, striving for findable, accessible, interoperable, and reusable data\\nAdhere to standard operating procedures and work in a manner compliant with regulatory standards\\n\\nREQUIRED QUALIFICATIONS:\\n4+ years of relevant industry experience including working as a Data Engineer or similar\\nExperience with cloud computing technologies (AWS preferred)\\nStrong expertise with SQL databases (NoSQL is a plus)\\nAvid Python programmer and experience working with APIs (experience with R is a plus)\\nExperience working with scientists (experience in a life science organization preferred)\\nAbility to work independently and proactively\\nExcellent communication skills and ability to explain technical concepts to non-experts\\nExcited to build infrastructure to power innovative biotechnology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Data Engineer - Broomfield, CO</td>\n",
       "      <td>Broomfield, CO</td>\n",
       "      <td>Broomfield</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Swisslog designs, develops and delivers best-in-class automation solutions for forward-thinking hospitals, warehouses and distribution centers. We offer integrated systems and services from a single source – from consulting to design, implementation and lifetime customer service. Behind the company’s success are 2 300 employees worldwide, supporting customers in more than 50 countries.\\n\\nThe Healthcare Solutions portfolio comprises automated material handling and drug management systems for hospital facilities that increase efficiency and enhance the patient experience in forward-thinking hospitals. Swisslog automated material handling solutions provide quick, flexible and safe transportation of medications, specimens and basic supplies throughout hospitals and across medical center campuses, while its medication management solutions address packaging, labeling, storage and dispensing for inpatient and outpatient pharmacies.\\nDATA ENGINEER - BROOMFIELD, CO\\nLOCATION - BROOMFIELD, CO\\nSwisslog looking for a passionate Data Engineer who loves data and database technologies and has experience bringing new analytics solutions from concept to market. We are building a new series of Products and Services based on Software as a Service (SaaS) model on AWS. Analytics will be our first service offering in that series. You will be joining at very early stage and will have the opportunity to influence the design and implementation. You must have deep knowledge of open source scripting, database and data warehouse technologies and be self-driven and highly motivated.\\n\\nThe Data Engineer is a critical member of the Software Development and Analytics team. You will help in sustaining and improving our current software solutions. Additionally, you will help steer new analytics offerings and establish our next generation data ingestion, transformation, and analytics capabilities and transform our current practices to take advantage of these capabilities.\\nYOUR RESPONSIBILITIES\\nOpen Source database technologies such as My-SQL and Postgres\\nNoSQL technologies such as: DynamoDB or MongoDB.\\nReal time ETL to Data Mart using data streams such as Kafka and Spark ETL process\\nData Modeling\\nAgile Data Warehouse and BI development\\nFine tune data flows and data search functions\\nExperience with AWS or other cloud based platforms\\nDesign the organization of data for data warehousing using the star schema and/or snowflake schema\\nDesign and implement complex queries utilizing SQL Commands, Views, Stored Procedures\\nWrite programming scripts to extract data to end users to make business decisions\\nCreate functions to provide custom functionality per the requirements\\nExtensive experience in writing database queries\\nParticipate in discussions involving the application creation and understand the requirements and provide the back-end functionality for the applications\\nAssist with critical analysis of test results and deliver solutions to problem areas\\nIdentify and research data issues; working with teams to resolve problems\\nAnalyze user problems and make suggestions for the prevention of future problems\\nYOUR PROFILE\\nProven experience in bringing an analytics solution from concept to market\\n10+ Years of experience in data processing and database technologies\\nMigration Experience with moving Data in/out from open source database like Postgres, and other similar technologies\\nExperience using scripting languages such as Python and Powershell to source API data\\nStrong preference for Agile/Scrum experience\\nStrong work ethic, loyal, achievement oriented, and ethical\\nExcellent interpersonal and communications skills (both verbal and written)\\nPossess a high level of professionalism and have excellent follow-through skills\\nHighly analytical and organized, with a strong attention to detail\\nBachelors Degree in Computer Science, Math or Engineering\\n\\nDesirable but not Essential:\\nExperience in the Healthcare industry a plus\\nDevelop reports in tools such as SQL, SSRS, Tableau, Qlik View, Power BI\\nExperience with statistical modelling and exposure to R\\nWE OFFER\\nSwisslog offers challenging work in a globally networked environment as well as competitive base salary, comprehensive benefits including health/dental and above-market 401K!\\n\\nOUR SOLUTIONS DELIVER RESULTS. OUR EMPLOYEES DELIVER SOLUTIONS.\\nSwisslog is an EEO Employer, Females/Minority/Veterans/Disabled/Sexual Orientation/Gender Identity\\n\\nSwisslog’s FMLA policy can be found at:\\nhttp://www.dol.gov/whd/regs/compliance/posters/fmlaen.pdf\\n\\nFederal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. If you require reasonable accommodation to complete the application or to perform your job, please contact Human Resources at jobs.healthcare.us@swisslog.com.\\nContact\\nAndy Levine\\nTalent Acquisition Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\n\\nThe Data Engineer, Analytics role falls into the Data Management &amp; Business Intelligence practice area at CapTech, through which our consultants provide a broad spectrum of services to help our clients define and implement a strategy to deliver lasting and mission-critical information capabilities. Our Data Integration consultants bridge the gap between the business and IT side of companies. By partnering with clients to fully understand both their business philosophy and IT strategy, CapTech consultants maintain the vision that data integration should be built to help the organization make better decisions by providing the right data at the right time.\\nSpecific responsibilities for the Data Engineer, Analytics position include:\\n\\nDesign, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, C, etc)\\nEvaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insights\\nDesign, develop, and implement data processing pipelines at scale\\nPresent programming documentation and design to team members and convey complex information in a clear and concise manner.\\nExtract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.\\nWrite and refine code to ensure performance and reliability of data extraction and processing.\\nCommunicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customers\\nParticipate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.\\nPartner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.\\nCollaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.\\nSome of our technologies might include: HDFS, Cassandra, Spark, Java, Scala, Informatica, SQL Server, Oracle, Ab Initio, Kafka.\\n\\nQualifications\\n\\nSpecific qualifications for the Data Engineer, Analytics position include:\\n\\nDevelopment experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system\\nStrong SQL development skills\\nDevelopment experience with at least two different programming languages (Python, Java, C, etc.)\\nDevelopment experience with Unix tools and shell scripts\\nDevelopment experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.)\\nMinimum of 3 years experience designing, developing, and testing software aligned with defined requirements\\nExperience tuning SQL queries to ensure performance and reliability\\nSoftware engineering best-practices, including version control (Git, TFS, JIRA, etc.) and test driven development\\nExposure to Business Intelligence tools such as Business Objects, Informatica, SSRS, Cognos, MicroStrategy, Tableau, QlikView, SpotFire, etc.\\nAdditional Information\\n\\nWe offer challenging and impactful jobs with professional career paths. All CapTechers can keep their hands-on technology no matter what position they hold. Our employees find their work exciting and rewarding in a culture filled with opportunities to have fun along the way.\\nAt CapTech we offer a competitive and comprehensive benefits package including, but not limited to:\\nCompetitive salary with performance-based bonus opportunities\\nSingle and Family Health Insurance plans, including Dental coverage\\nShort-Term and Long-Term disability\\nMatching 401(k)\\nCompetitive Paid Time Off\\nTraining and Certification opportunities eligible for expense reimbursement\\nTeam building and social activities\\nMentor program to help you develop your career\\nCapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace.\\nCandidates must be eligible to work in the U.S. for any employer directly (we are not open to contract or “corp to corp” agreements). At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.\\nCapTech is a Drug-Free work place.\\nCandidates must have the ability to work at CapTech’s client locations.\\nAll positions include the possibility of travel.\\nCapTech has not contracted/does not contract with any outside vendors in its recruitment process. If you are interested in this position, please apply to CapTech directly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Data Engineer (Green Chef)</td>\n",
       "      <td>Boulder, CO</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Green Chef is the leading and first certified organic company in the meal kit space in the US. We are passionate about making it easier to eat healthfully at home. Founded in September 2014, Green Chef is already a national force with hundreds of employees and tens of thousands of customers relying on us to help them eat well.\\n\\nIn 2018 Green Chef and Hello Fresh partnered to become the world's largest meal kit company. Together, we tirelessly work to disrupt the food industry.\\n\\nGreen Chef remains focused on delivering healthy, organic ingredients, designed for busy people. We believe in creating thoughtful food experiences from planting to the plating. We strive to create greater interaction and meaningful connections with the food that nourishes us, the people who provide it, and the ones we share it with. Green Chef offers a variety of specialty lifestyle plans.\\n\\nAs a Data Engineer at Green Chef, you will be a key member of foundational projects in data architecture and business intelligence that drive customer insights and operational efficiency. This is an opportunity to get in on the ground floor of a talented and diverse cross-functional team tackling high visibility data projects. You will work with talented team members to design and implement scalable data solutions that enable Green Chef to be a truly data-driven company. This is a unique chance to be involved in marketing, e-commerce, physical product, operations and supply chain data at a food-tech company where data is key to growth and innovation.\\n\\nOur ideal candidate will have excellent communication and problem-solving skills as well as a deep passion for learning. You will take full ownership of systems and projects, and enjoy the challenge of navigating a fast-paced start-up environment while being able to have some fun.\\n\\nOur infrastructure runs on AWS and we have data warehouses in MongoDB, Postgres, and Redshift.\\n\\nResponsibilities\\n\\n\\nArchitect and build data pipelines that can parse production data from different sources and deliver quality data to our business analytics teams.\\nPartner with engineering to create services to ingest and supply data and provide the associated streaming solutions.\\nAbility to deal with large, complex amounts of data that will be used 24/7 by the business as a foundation for all decision making at all levels of the company.\\nTest, assess, and secure the data generated from the transformations while keeping our data secure with all the privacy policies in place.\\nWork closely with BI, product, marketing, operations, and engineering teams to understand how data can influence our company's strategic direction and decision-making.\\nYou'll be part of the global BI group of HelloFresh working with many different cultures, backgrounds, and tech stacks as we work with the various HelloFresh brands.\\nMake well-informed decisions with deep knowledge of both the internal and external impacts on teams and projects.\\nEvaluate and integrate third-party data platforms, APIs and machine learning services.\\nKeep track of industry trends in data science and engineering to ensure cutting-edge implementations\\nExpand and optimize our data and data pipeline architecture and data flow\\n\\nRequirements\\n\\n\\nAt least 3 years' experience as a Data Engineer.\\nYou are a builder. You not only get excited about data architecture, but you have the technical knowledge and experience building complex pipelines to set us up with the tools we need to provide data to the greater company.\\nDemonstrated strength in data modeling, ETL development, and data warehousing\\nStrong fundamentals in data structures, algorithms, data modeling, and database performance on NoSQL/SQL\\nProficient in at least one of the scripting languages: Python/PySpark or Ruby\\nProficient in streaming services (Kafka or Kinesis)\\nExperience with automated testing.\\nKnowledge of software engineering best practices across the development lifecycle (agile methodologies, coding standards, code reviews, build processes, and testing)\\nExcellent written and oral communication skills.\\nBachelors in Computer Science, Engineering or Mathematics.\\nExperience with:\\nMongoDB, Postgres, Redshift, AWS RDS\\nDocker and containerization\\nAirflow, Luigi, AWS Glue, MoSQL\\nKafka or AWS Kinesis\\nExperience building custom ETL solutions\\nAWS S3, Athena, EMR\\n\\nOur team is diverse, high-performing and international, helping us to create a truly inspiring work environment in which you will thrive!\\n\\nIt is the policy of Green Chef and HelloFresh not to discriminate against any employee or applicant for employment because of race, color, religion, sex, sexual orientation, gender identity, national origin, age, marital status, genetic information, disability or because he or she is a protected veteran.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ETL Software Engineer</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview\\n\\nWe’re looking for a developer / data engineer to create, debug, and streamline imports of investment data from various financial sources into our SQL database.\\nWe already have a custom data import framework in place, built using C#, and we need to add new data sources and interfaces.\\nInvolves writing C# code, querying data in SQL, and reconciliation of investment finance data.\\n\\n\\nWhat qualifications will help me be successful at FinFolio?\\n\\nIntermediate level SQL required (SQL Server specifically a plus).\\nBeginner to Intermediate level C# required.\\nExcellent written and verbal communication skills.\\nMust be reliable and self starting, with a strong attention to detail.\\nFamiliarty with IT / infrastructure is a plus.\\nInvestment finance knowledge is a plus.\\n\\n\\nWhat will you do at this job?\\n\\nDevelop a custom tool to load and transform data into FinFolio’s SQL database.\\nWork with various financial data from brokers and advisory systems, typically: Accounts, Transactions, Custodial Balances, Tax Lots, Securities, Prices, etc.\\nSoftware is written in C# (with WPF), and it may need bug fixes or new features. You will be responsible for both coding these changes and coming up with your own improvements.\\nYou will need to reconcile data import errors by using SQL queries and FinFolio desktop software. This will involve determining why transactions are not impacting balance correctly, why account data does not match, etc.\\nYou may need to interact with clients to request data extracts from their current systems and work with them to make sure the correct data is extracted.\\nEventually we hope for this position to branch out and encompass a broader range of duties, including infrastructure and general software development.\\n\\n\\nWho is FinFolio?\\n\\nWe make wealth management simpler!\\nA SaaS solution that is the backoffice for professional wealth managers.\\nOur software reports, trades, and bills investment accounts.\\nFounded in 2008, we recently re-launched our product to rave reviews.\\n20+ employees, 50% are remote, grew 100% over 24 months.\\nExciting early-stage startup, adding a new client every 12 days.\\nPassionate and excited about great software and making our clients happy.\\n\\n\\nWhy should you work here?\\n\\nWork with a team that is excited and passionate about what they do.\\nHelp support an amazing, best-in-class product that is fun to use.\\nCompetitive salary + health/dental + paid time off + 401K match.\\nInteract with interesting and successful financial advisor clients.\\nFun co-working office in Denver RINO: https://www.denver.org/about-denver/neighborhood-guides/river-north-art-district/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Data Engineer with TS/SCI with Poly</td>\n",
       "      <td>Aurora, CO 80011</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>CO</td>\n",
       "      <td>80011</td>\n",
       "      <td>None Found</td>\n",
       "      <td>United States’ citizen with current TS/SCI, SSBI and polyBachelor’s Degree with at least 4-8 years of applicable experience or Masters degree with 2-6 years of experience or 4 additional years in lieu of degree</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Support of production data processing and data distribution systemsWork with data providers and customers to ensure data quality and availabilityGather requirements and work with data providers to enable distribution of new data sourcesSupport software deployments and integration of geographically diverse computing systemsProvide direct support to end usersConfigure and maintain data ingest workflows (ETL) across several production systemsInstall, configure, and update a wide array of COTS/GOTS and homegrown software applicationsSupport and troubleshoot diverse IT infrastructure hardware platforms and protocolsWork with software development and systems administration staff to monitor and troubleshoot production systemsGenerate and maintain systems documentation and diagramsTroubleshoot network issues and establish new connectivityMonitor and maintain a variety of databases</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description\\nJob Description:\\nThe National Solutions Group at Leidos has an opening for a passionate Data Engineer to provide a variety of software and IT support services to ensure customer satisfaction with production software and systems. The position specifically focuses on the operations of GOTS software to ensure data integrity and availability for a large ETL system, but includes general system troubleshooting and direct customer support. The position is for a mid-senior level engineer with experience in both software development and systems administration. Candidates must be able to work in a fast moving environment with many moving parts and must be able to juggle several tasks at once. In addition, they must be willing to share on-call responsibilities to troubleshoot customer issues during non-business hours.\\nPrimary Roles &amp; Responsibilities:\\nAs a Data Engineer you will have the opportunity to:Support of production data processing and data distribution systemsWork with data providers and customers to ensure data quality and availabilityGather requirements and work with data providers to enable distribution of new data sourcesSupport software deployments and integration of geographically diverse computing systemsProvide direct support to end usersConfigure and maintain data ingest workflows (ETL) across several production systemsInstall, configure, and update a wide array of COTS/GOTS and homegrown software applicationsSupport and troubleshoot diverse IT infrastructure hardware platforms and protocolsWork with software development and systems administration staff to monitor and troubleshoot production systemsGenerate and maintain systems documentation and diagramsTroubleshoot network issues and establish new connectivityMonitor and maintain a variety of databases\\nMinimum Qualifications:United States’ citizen with current TS/SCI, SSBI and polyBachelor’s Degree with at least 4-8 years of applicable experience or Masters degree with 2-6 years of experience or 4 additional years in lieu of degreeStrong grasp of LinuxAutomating tasks by writing quality codeStrong coding skills (Java, Javascript, shell scripting, Perl, Python)Configuration managements tools (Puppet, Chef)Monitoring complex systems (Nagios, ElasticSearch, Grafana)Automation tools (Jenkins, Bamboo)Source-control systems (Git, SVN)Candidate must be certified to meet DoD 8570 level IAT-II qualifications. A Security+ certification is requiredWilling to share on-call responsibilities to include coming into work during non-business hours to troubleshoot customer issues\\nPreferred Qualifications:Master’s DegreeRed Hat Enterprise Linux administration experienceUnderstanding of Amazon Web Services – EC2, RDS, S3Hadoop, Accumulo and Map Reduce techniquesUnderstands compiled languages including JAVASoftware versioning control systems – GIT/SVNFamiliarity with Software Development ProgramsFamiliarity with Agile Development methodologiesJava Programming experience\\nExternal Referral Bonus:\\nIneligible\\nPotential for Telework:\\nNo\\nClearance Level Required:\\nTop Secret/SCI with Polygraph\\nTravel:\\nYes, 10% of the time\\nScheduled Weekly Hours:\\n40\\nShift:\\nDay\\nRequisition Category:\\nProfessional\\nJob Family:\\nSoftware Engineering\\nLeidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.\\nPay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.\\nLeidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.\\nAll qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>DATA ENGINEER</td>\n",
       "      <td>Denver, CO 80202</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80202</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Design, Develop and Maintain Data Storage and Data Analytics Products, Systems and Solutions for clients in the Media Industry\\nDevelop, Enhance &amp; Configure data transformation and storage solutions in support of our product offerings\\nWork closely with other internal engineering teams focused on front-end and back-end APIs development and configuration in direct support of various products\\nWork closely with internal product and project management teams to design and prioritize features and their associated delivery timelines\\nDiagnose and correct system and product faults, designing &amp; implementing solutions to correct\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Decentrix is offering an exciting opportunity to an individual with the right skill set and background to join our elite team and work with the most advanced Media Advertising Enhancement/Optimization Technologies. You would be using your business, technical &amp; development skills in an extremely fast paced environment providing services to forward thinking media corporations. See www.bianalytix.com for more information.\\nPosition Responsibilities:\\nDesign, Develop and Maintain Data Storage and Data Analytics Products, Systems and Solutions for clients in the Media Industry\\nDevelop, Enhance &amp; Configure data transformation and storage solutions in support of our product offerings\\nWork closely with other internal engineering teams focused on front-end and back-end APIs development and configuration in direct support of various products\\nWork closely with internal product and project management teams to design and prioritize features and their associated delivery timelines\\nDiagnose and correct system and product faults, designing &amp; implementing solutions to correct\\nTechnical Proficiencies\\nScala Language development with experience in full stack testing\\nApache Spark and distributed data processing methodologies\\nT-SQL / PL SQL Development\\nComplex data processing methodologies\\nMassive scale data processing methodologies\\nAmazon AWS technologies and products (ECS, EC2, EMR, S3, etc.)\\nLinux\\nDocker image development &amp; container execution\\nAdvanced knowledge of RDBMS systems and associated storage optimization techniques\\nAnalytic data structure knowledge and experience\\nWorking knowledge of software engineering and applying database methodologies, techniques, and tools\\nPersonal\\nThe experience in the design of data storage and data transformation processes to leverage and analyze complex data relationships\\nThe skills to quickly diagnose and solve technical problems associated to large data solutions and the associated technologies\\nInterest and ability to work at the technical installation level with major media corporations\\nMaturity to manage task associated to products and/or services to an installation contract\\nExcellent communication skills, including good verbal and written abilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Cloud Data Engineer</td>\n",
       "      <td>Denver, CO 80209</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80209</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDegree in Computer Engineering/Science or related field, with 4+ years of professional experience in database/data lake development\\nProficient with processing data on relational databases like Oracle/SQL Server/MySQL/etc.\\nExperience with developing on an MPP database Redshift/Teradata/Snowflake\\nProficient handling large data sets using SQL and databases in a business and engineering environment\\nExperience with operations in a Public Cloud Environment (AWS/Azure/GCP)\\nExperience with ETL and Data Warehouse/Lake processes\\nExcellent verbal and written communication skills\\nStrong troubleshooting and problem-solving skills\\nThrive in a fast-paced, innovative environment\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Businessolver delivers market-changing benefits administration technology supported by an intrinsic and unwavering responsiveness to client needs. Our clients trust Businessolver to take care of them and their employees with a configurable and secure SaaS platform and a culture of service, all aimed at total and measurable success and our clients' complete delight.\\n\\nWe work with some of the most recognizable brands in the U.S. We look to our rock-star employees to help these clients maximize the investment in their benefits program, minimize their exposure to risk, engage their employees with our easy-to-use solution and full suite of communication tools, and empower their employees to use their benefits wisely.\\n\\nAt Businessolver you will have opportunities for individual development through our common language: Trust through transparency. Assume positive intent. Be real. Live a growth attitude. Embrace the reverse golden rule.\\n\\nThe Cloud Data Engineer (CDE) will be responsible for architecting, developing, implementing, and operating stable, scalable, low cost solutions to source data from production systems into the data lake (AWS) and data warehouse (Redshift) and into end-user facing applications (AWS Quicksight). The ideal candidate should be able to work with Infrastructure, Data Analysts, and Machine Learning Engineers in a fast-paced environment, understanding the business requirements, and implementing ETL, machine learning and cloud solutions. This role will serve on the Cloud Data Engineering team.\\n\\nQualifications:\\n\\nDegree in Computer Engineering/Science or related field, with 4+ years of professional experience in database/data lake development\\nProficient with processing data on relational databases like Oracle/SQL Server/MySQL/etc.\\nExperience with developing on an MPP database Redshift/Teradata/Snowflake\\nProficient handling large data sets using SQL and databases in a business and engineering environment\\nExperience with operations in a Public Cloud Environment (AWS/Azure/GCP)\\nExperience with ETL and Data Warehouse/Lake processes\\nExcellent verbal and written communication skills\\nStrong troubleshooting and problem-solving skills\\nThrive in a fast-paced, innovative environment\\n\\nPreferred Qualifications:\\n\\nOracle, Postgres, EMR, Redshift, Linux experience\\nFamiliar with computer science fundamentals including object-oriented design, data structures, algorithm design, problem solving, and complexity analysis\\nExperience with Agile Methodologies\\nExperience with complex/large data sets (Big Data)\\nExperience operating a Data Lake\\nExperience with Cloud Architecture/Engineering\\n\\nThe Businessolver Way…\\n\\nOur team has spent nearly two decades crafting a culture that challenges each employee to perform at the top of their game – and have fun doing it! If you desire to use your skills and experience in an environment where you can make a difference, we want to hear from you! Businessolver employees experience a vibrant work culture with extensive workplace perks including:\\n\\n\\nCompetitive pay, great benefits, and vacation time. We are an equal opportunity employer with competitive benefits including medical, dental, life insurance, disability, 401(k) with company match, among others.\\nSmart Casual Dress. No need to suit up, but we also have on-site dry cleaning services for those that prefer to dress-up!\\nWeekly catered meals. Breakfast every other Mondays, lunch Wednesdays, and afternoon appetizers on Fridays encourage collaboration across our teams.\\nFully-stocked kitchens. We know it takes fuel to perform, so we provide a kitchen stocked with healthy cereals, fruit, snacks, and beverages to keep you at the top of your game.\\nFitnessolver. If you need a boost, visit our on-site fitness facility to clear your head.\\nMassages. With a \"work hard/play hard\" atmosphere we all need a little stress relief at times.\\nCharity and community involvement. Participate in a variety of ways to support those around us.\\nLearning &amp; Development. Continue to learn about the industry through our online and instructor-led classes.\\nRecognition. Want some swag? Earn tons of it by helping out your co-workers through our employee recognition program.\\nCulture. Want a culture most dream of? Most companies talk about it, we live it. Come find out for yourself!\\n\\nInterested? Great, we look forward to reading your application - make sure it includes:\\n\\n\\nA cover letter that highlights why you think you'd be great for the gig, focusing on how your past work experience has prepared you for this kind of position – or why you think you can rock the job even though you don't have past work experience that's perfectly aligned.\\nYour resume.\\n\\nYou will receive an auto-reply confirming that we've received your application, and you will hear from us again after we've reviewed your application and decided whether or not to move you forward in our recruiting process.\\n\\nIf you do decide to apply, please know that every complete application will be carefully reviewed. Seriously! We know it is a time commitment to prepare an application. We will respect that effort by thoughtfully reviewing every single complete application and we are truly grateful for your interest.\\n\\nThanks for your interest in Businessolver!\\n\\nCheck us out on Twitter ( https://twitter.com/businessolver ), Facebook ( https://www.facebook.com/bsolver ) and LinkedIn ( https://www.linkedin.com/company/232793?trk=tyah&amp;trkInfo=tarId%3A1415406210925%2Ctas%3Abusinessolver%2Cidx%3A2-1-4 ) for a look at our vibrant culture.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Westminster, CO</td>\n",
       "      <td>Westminster</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Collect, store, and aggregate data to support the creation of great data products for the business and our customers\\nBe creative and cooperative in designing and building data pipelines\\nUse Cloud-based infrastructure and applications (we use AWS and maintain our own Kubernetes clusters)\\nContinually learn and seek ways to improve our data flows\\nCollaborate well in a team environment (we use agile)\\nValidate data and maintain healthy infrastructure and data flows\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Please review the job details below.\\nThe Data Intelligence Team is a central data engineering and analytics team that supports business decisions and company strategy across all parts of MAXAR. We collect data from a variety of interesting sources and build automated data flows that help enable actionable insights for many different teams and products. We are looking for data professionals that are excited to work with cloud-based tools and build high quality data flows. We are a team of people from a variety of data focused backgrounds. We support each other, and we help each other learn and grow. Come collaborate with us to help guide MAXAR and improve our collective understanding of our planet.\\nResponsibilities\\nCollect, store, and aggregate data to support the creation of great data products for the business and our customers\\nBe creative and cooperative in designing and building data pipelines\\nUse Cloud-based infrastructure and applications (we use AWS and maintain our own Kubernetes clusters)\\nContinually learn and seek ways to improve our data flows\\nCollaborate well in a team environment (we use agile)\\nValidate data and maintain healthy infrastructure and data flows\\nRequired\\nBachelor’s Degree in a technical field or equivalent work experience\\nExperience building and maintaining ETL pipelines\\n2 years of experience with Python, SQL, AWS (S3, EC2)\\nData visualization or reporting experience\\nGood verbal and written communication skills\\nPreferred\\nAny experience with…\\nAirflow or other ETL scheduling tools\\nBuilding or improving APIs\\nCollecting data from a variety of sources (APIs, Postgres, Oracle, SAP, Salesforce, etc.)\\nThoughtfully storing data in databases (especially Postgres) to support data products and reporting\\nAny experience with or willingness to learn…\\nBig data pipelines using Spark and streaming tools\\nMaintaining data pipelines used in a production environment\\nWorking with geospatial data\\nAdditional coding languages, especially JVM languages or Bash\\nAny of these or similar tools (Kubernetes, Ansible, Jenkins, PostgreSQL, Tableau)\\nAdditional AWS tools (EFS, Lambda, RDS/Aurora, Glue, Athena)\\nPerks\\nDiverse team that works and learns together\\nGreat benefits, flexible time off for family and travel, pet insurance discounts\\nWorking to improve products that helps us understand and protect our planet\\nWell-lit office space with on-site cafeteria and coffee shop\\nPrivate frisbee golf course and very nice game and quiet rooms for mental breaks\\nQuarterly self-guided work time to build what you want or learn new skills and technology\\nMAXAR Technologies offers a generous compensation package including a competitive salary; choice of medical plan; dental, life, and disability insurance; a 401(K) plan with competitive company match; paid holidays and paid time off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Tableau Developer Data Engineer (BHJOB22048_585)</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n3-5 Years BI Experience developing BI dashboards, reporting, visualizations and data models. Proven BI dashboard, report, and visualization design and delivery.\\n1-2 years Tableau Server experience. Proven Tableau performance tuning reporting and dashboard solutions.\\nTableau Server Experience: Configuration, Administration, Tuning &amp; Performance, Data Connections, APIs\\nTableau Desktop &amp; Tableau Prep Experience: Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience – including data extraction, transformation and load (ETL) from many sources; OLAP, OLTP, Datawarehouse’s, Hadoop/Cloudera, and Cloud platforms.\\nExperience with Tableau and Cognos Development.\\nExperience in data modeling and data prep (discovery, structuring, cleaning, enriching, validation, publishing). Tableau Prep preferred.\\nSource to Target mapping. Tableau Prep experience a major plus.\\nSQL Experience (joins, queries, select statements) – Oracle, SQL Server database back ends.\\nBI performance tuning across front end, ETL, Data back end optimization.\\nSolid Agile experience on BI projects – Gathering requirements, translating to use cases, BI stories, design/delivery of BI use cases – using SAFe, SCRUM, Agile, Kanban or similar method.\\nExposure Python.</td>\n",
       "      <td>Tableau BI Engineer – ITmPowered Tableau Engineer will design, develop, and deliver high performance Tableau dashboards, workbooks, and visualizations providing Business Intelligence on hundreds of enterprise technology projects. Provide deep Tableau Server knowledge and expertise to the team on how to build and maintain Tableau Server dashboards and reports. Tableau Developer design, develop and implement […]\\n\\nTableau BI Engineer – ITmPowered\\n\\nTableau Engineer will design, develop, and deliver high performance Tableau dashboards, workbooks, and visualizations providing Business Intelligence on hundreds of enterprise technology projects.\\nProvide deep Tableau Server knowledge and expertise to the team on how to build and maintain Tableau Server dashboards and reports.\\nTableau Developer design, develop and implement BI solutions leveraging Tableau Desktop, Tableau Online, Tableau Prep, and Tableau Server.\\nProvide Tableau development and support across dozens of systems and data sources. Support existing Tableau server, Tableau reports, and Tableau Dashboard solutions.\\nPerformance tune Tableau dashboards (optimize extracts, limit fields/records, marks, optimize/materialize calculations, query optimization, workbook cleanup).\\nEngineer BI dashboards for Technology Projects and related Finance information (Burn Rates, Accruals, Milestones, Expenses, budgets, Earned Value, ROI) as well as project KPIs.\\nIdentify BI performance bottlenecks and design optimal solutions at the report / dashboard level (Tableau, Cognos, PowerBI), SQL data munging level (SQL joins, pivots, enrichment, aggregations Oracle, Hadoop, SQL), or ETL / Data movement level (DataStage, Informatica, Sqoop).\\nTableau Server Configuration, Administration, Tuning &amp; Performance, Data Connections, APIs\\nTableau Desktop &amp; Tableau Prep Experience: Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience – including data extraction, transformation and load (ETL) from many sources; OLAP, OLTP, Datawarehouse’s, Hadoop/Cloudera, and Cloud platforms.\\nWork with end users to gather BI requirements (use cases, visualization, drill up/down, hierarchies, tables, pivots, outcomes, and data sources, etc.).\\nBuild data models. Prepare, wrangle, and model the Data to derive effective data models supporting performant BI solutions. Understand technical data sources, data structures, data quality and necessary transformations to aggregate, enrich, validate, and publish data.\\nSource to target mappings and work with ETL Engineers to optimize data flows, and data preparation.\\nDevelop and optimize BI Dashboards, Reports, Data Models, and data flows.\\nSupport existing BI environments Tableau, Cognos, data movement (DataStage, Informatica, sqoop), and backend data repositories (Oracle, SQL Server, Hadoop).\\nCloud BI migration – Migrate big data solutions to cloud in PowerBI / Azure – design and delivery.\\n Requirements:\\n\\n3-5 Years BI Experience developing BI dashboards, reporting, visualizations and data models. Proven BI dashboard, report, and visualization design and delivery.\\n1-2 years Tableau Server experience. Proven Tableau performance tuning reporting and dashboard solutions.\\nTableau Server Experience: Configuration, Administration, Tuning &amp; Performance, Data Connections, APIs\\nTableau Desktop &amp; Tableau Prep Experience: Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience – including data extraction, transformation and load (ETL) from many sources; OLAP, OLTP, Datawarehouse’s, Hadoop/Cloudera, and Cloud platforms.\\nExperience with Tableau and Cognos Development.\\nExperience in data modeling and data prep (discovery, structuring, cleaning, enriching, validation, publishing). Tableau Prep preferred.\\nSource to Target mapping. Tableau Prep experience a major plus.\\nSQL Experience (joins, queries, select statements) – Oracle, SQL Server database back ends.\\nBI performance tuning across front end, ETL, Data back end optimization.\\nSolid Agile experience on BI projects – Gathering requirements, translating to use cases, BI stories, design/delivery of BI use cases – using SAFe, SCRUM, Agile, Kanban or similar method.\\nExposure Python.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Senior Data Engineer, Identity Resolution</td>\n",
       "      <td>Denver, CO 80202</td>\n",
       "      <td>Denver</td>\n",
       "      <td>CO</td>\n",
       "      <td>80202</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About FullContact:\\nFullContact is the premier provider of SaaS-based identity resolution that empowers brands to improve their customer experience and authentically engage with consumers. Using a consumer-first approach with our product offerings, we aim to make relationships better and that starts with our employees.\\nWe offer excellent benefits for our teammates, including full medical and dental coverage, our famous \"paid, paid vacation\" and a generous stock option plan.\\nYou'll join an innovative, enthusiastic team whose hard work helped us achieve recognition from the API awards, MarTech Breakthrough awards, and inclusion on the Inc. 5000 list of fastest-growing companies.\\nThe Role\\nWork on the Identity Resolution team as Senior Data Engineer to design and construct performant algorithms, infrastructure and data pipelines to manage our graph database for our identity resolution offering. Our identity graph is constructed from billions of observations leveraging the latest in big data technologies.\\nThe Identity Resolution team is primarily focused on providing identity resolution capabilities to our internal customers enabling identity resolution across the board for our external customers. This involves integrating data sets, developing our patented identity resolution graph algorithms and scaling all this to a very large amount of data. The team works on both live streaming and batch systems and leverages machine learning so that we can be the best in class for enterprise identity resolution.\\n\\nWhat You'll Do\\nBe a senior member on the Identity Resolution team\\nDesign, build, test, deploy and maintain systems using JVM based languages, focusing on Scala and Java\\nDesign, build, test and deploy massively parallel graph algorithms\\nCreate and maintain microservices connected through APIs (1000s requests/sec in some cases)\\nProcess large amounts of data leveraging big data technologies such as Spark, Kafka and more.\\nExpose data and tools to internal teams through APIs and libraries\\nAccount for quality and security as you build\\nYour Traits\\nYou are creative and enjoy solving problems\\nYou are curious. You look for the root cause of issues and are a life learner\\nYou are collaborative and love working with people, whiteboarding and designing hard problems\\nYou have got grit and recognize that the harder things in life are more rewarding\\nYou are empathetic for both customers and team members\\nAbout You\\n5+ years of experience in Data Engineering or Software Engineering\\nDeep understanding and experience developing in the JVM\\nYou have a solid mathematical foundation\\nExperience with a variety of databases (SQL, NoSQL, In-Memory, Searchable, etc)\\nSolid Linux experience - CLI tools, scripting\\nGit and AWS familiarity\\nAuthorized to work in the United States on a full-time basis\\nBonus Points for experience with Machine Learning and Graph Theory\\n\\nvPccapMAd3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Boulder, CO 80301</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>CO</td>\n",
       "      <td>80301</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Company Overview\\n\\nFanatics is the global leader in licensed sports merchandise and changing the way fans purchase their favorite team apparel and jerseys. Through an innovative, tech-infused approach to making and selling fan gear in today's on-demand culture, Fanatics operates more than 300 online and offline stores, including the e-commerce business for all major professional sports leagues (NFL, MLB, NBA, NHL, NASCAR, MLS, PGA), major media brands (NBC Sports, CBS Sports, FOX Sports) and more than 200 collegiate and professional team properties, which include several of the biggest global soccer clubs (Manchester United, Real Madrid, Chelsea, Manchester City). Fanatics offers the largest collection of timeless and timely merchandise whether shopping online, on your phone, in stores, in stadiums or on-site at the world's biggest sporting events.\\n\\nAbout the Team\\n\\nFanatics is first and foremost a technology company. We are powered by cutting-edge tech created by our small agile teams using the latest tools and technologies under our highly analytical, forward thinking, and open-minded leadership. As the global leader in licensed sports merchandise, we challenge ourselves by improving our new fully responsive NodeJS cloud commerce platform, Elasticsearch engine, and deep data science capabilities while building the best-in-class retail manufacturing and supply chain technologies. Our tech teams work together to revolutionize data science and engineering initiatives, provide highly scalable real-time and streaming platforms, and create secure e-commerce and in-stadium fan experience products. Our own e-commerce platform transacts in over 190 countries, 17 languages, and 14 currencies. Our motto is “#GSD”—get stuff done—and we do just that. If you want to be at the nexus of sports, commerce, and technology, come be a part of our industry-leading team here at Fanatics Tech.\\n\\nThe FanStreams team at Fanatics has the grand vision of making many of the traditional batch-oriented processes into real time systems and is seeking a Senior Data Engineer with the passion and ingenuity to build streams of data across various domains of Fanatics world. These streams will power Data Science, BI &amp; Site optimizations. The FanStreams team is responsible for building, managing complex stream processing topolgies using the latest open source tech stack, build metrics and visualizations on the generated streams and create varied data sets for different forms of consumption and access patterns. We’re looking for a seasoned engineer to help us build and scale the next generation of streaming platforms and infrastructure at Fanatics.\\nWhat will you do?\\nBuild data platforms and streaming engines that are real time in nature\\nOptimizing existing data platforms and infrastructure while exploring other technologies\\nProvide technical leadership to the data engineering team on how data should be stored and processed more efficiently and quickly at scale\\nBuild and scale stream &amp; batch processing platforms using the latest open-source technologies\\nWork with data engineering teams and help with reference implementation for different use cases\\nImprove existing tools to deliver value to the users of the platform\\nWork with data engineers to create services that can ingest and supply data to and from external sources and ensure data quality and timeliness\\nWhat are we looking for?\\n5+ years of software development experience with at least 3+ years of experience on open-source big data technologies\\nKnowledge of common design patterns used in Complex Event Processing\\nProficiency in Streaming technologies: Apache Kafka, Kafka Streams, KSQL, Spark, Spark Streaming\\nProficiency in Java, Scala, SQL\\nExperience and deep understanding of traditional, NoSQL and columnar databases\\nExperience of building scalable infrastructure to support stream, batch and micro-batch data processing\\nBonus points\\nYou've worked with Amazon Web Services\\nTryouts are open at Fanatics! Our team is passionate, talented, unified, and charged with creating the fan experience of tomorrow. The ball is in your court now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Broomfield, CO 80021</td>\n",
       "      <td>Broomfield</td>\n",
       "      <td>CO</td>\n",
       "      <td>80021</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nSQL\\nPython\\nCloud Data Warehouse Systems\\nETL\\nWorkflow Tools\\nBatch Processing\\nSpark\\nCD/CI tools</td>\n",
       "      <td>\\n3+ years experience with Data Warehouse Systems and working on an ETL system, either a commercial one like Matillion or Fivetran, an open-source one like Airflow, or a custom one you or your company built\\nExperience with one major cloud analytics database (Snowflake, Redshift, Google Big Query), Snowflake preferred\\nStrong familiarity with SQL\\nPython development experience in production\\nFamiliarity with Spark\\nA strong desire to show ownership of problems you identify and proven ability to empower others to get more done\\nFamiliarity with modern BI and exploration tools, Looker is preferred.\\nFamiliarity with GitHub or other CD/CI tools\\nBasic AWS experience (S3, EC2) (1-2 years)\\nSome familiarity with streaming approaches preferred\\nCS Degree preferred\\nSome experience preferred with Jenkins, Docker, Kubernetes</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer\\n\\nPosition Overview\\nValidity is looking for a talented Data Engineer with 3+ years of experience in implementing modern data architectures. You will work closely with all areas of the business on engineering and analytical initiatives marked by greater complexity and less structure that will yield substantial product enhancements, uncover insights, and inform business decision making and focus. You will be working on one of the biggest opportunities at Validity: A major build-out of our data architecture. Your first projects will include helping to scale our data infrastructure and build out our data warehouse and analytics footprint. You will collaborate closely with Engineers and Product Managers to inform product decision making with data and to identify opportunities to create more value for our customers. This is a high-impact role that will help shape the future of Validity's products and services.\\nCompany Overview\\nValidity is a leading global provider of data integrity and compliance offerings that thousands of organizations worldwide rely on to trust their data. We're passionate about our people, our customers, our values and our culture!\\nJoin a passionate, driven team committed to bringing better insights and data-driven decisions to our internal and external customers. We're looking for people with a growth mindset and the insight to solve for today while building for the future. You will be working for a company that truly values the power of data.\\nEssential Position Duties and Responsibilities\\n3+ years experience with Data Warehouse Systems and working on an ETL system, either a commercial one like Matillion or Fivetran, an open-source one like Airflow, or a custom one you or your company built\\nExperience with one major cloud analytics database (Snowflake, Redshift, Google Big Query), Snowflake preferred\\nStrong familiarity with SQL\\nPython development experience in production\\nFamiliarity with Spark\\nA strong desire to show ownership of problems you identify and proven ability to empower others to get more done\\nFamiliarity with modern BI and exploration tools, Looker is preferred.\\nFamiliarity with GitHub or other CD/CI tools\\nBasic AWS experience (S3, EC2) (1-2 years)\\nSome familiarity with streaming approaches preferred\\nCS Degree preferred\\nSome experience preferred with Jenkins, Docker, Kubernetes\\nExperience/Skills\\nSQL\\nPython\\nCloud Data Warehouse Systems\\nETL\\nWorkflow Tools\\nBatch Processing\\nSpark\\nCD/CI tools\\n\\nWe are looking for someone to work in our Broomfield office but are open to a remote position depending on the situation.\\nfCVh3Ty9i9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               Title  \\\n",
       "0   Data Engineer                                                                      \n",
       "1   Sr. Data Engineer                                                                  \n",
       "2   Senior Data Engineer, Data Informatics                                             \n",
       "3   Principal Data Engineer & DBA                                                      \n",
       "4   Data Strategy Specialist - Business & Data Analysis, Cloud, AWS, Azure, Big Data   \n",
       "..                                                                               ...   \n",
       "60  Sr. Data Engineer                                                                  \n",
       "61  Tableau Developer Data Engineer (BHJOB22048_585)                                   \n",
       "62  Senior Data Engineer, Identity Resolution                                          \n",
       "63  Senior Data Engineer                                                               \n",
       "64  Data Engineer                                                                      \n",
       "\n",
       "                Location         City State         Zip     Country  \\\n",
       "0   Denver, CO            Denver       CO    None Found  None Found   \n",
       "1   Boulder, CO           Boulder      CO    None Found  None Found   \n",
       "2   Englewood, CO         Englewood    CO    None Found  None Found   \n",
       "3   Broomfield, CO        Broomfield   CO    None Found  None Found   \n",
       "4   Denver, CO 80203      Denver       CO    80203       None Found   \n",
       "..               ...         ...       ..      ...              ...   \n",
       "60  Westminster, CO       Westminster  CO    None Found  None Found   \n",
       "61  Denver, CO            Denver       CO    None Found  None Found   \n",
       "62  Denver, CO 80202      Denver       CO    80202       None Found   \n",
       "63  Boulder, CO 80301     Boulder      CO    80301       None Found   \n",
       "64  Broomfield, CO 80021  Broomfield   CO    80021       None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Qualifications  \\\n",
       "0   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "2   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "3   \\nOver five years of experience as a DBA\\nWorking knowledge of Oracle Autonomous Data Warehouse environments in Oracle’s OCI (Oracle Cloud Infrastructure)\\nExperience in tuning and optimizing DB performance\\nKnowledge of DB security best practices, and back and recovery processes\\nExperience in developing data architectures\\nYears of experience in ETL (Extract, Load, and Transform) from various data sources\\nKnowledge of Big Data and Time-series databases\\nData Cleansing expertise\\nWorking knowledge of data analysis / data science   \n",
       "4   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "..         ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "60  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "61  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "62  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "63  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "64  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "                                                                                                                   Skills  \\\n",
       "0   None Found                                                                                                              \n",
       "1   None Found                                                                                                              \n",
       "2   None Found                                                                                                              \n",
       "3   None Found                                                                                                              \n",
       "4    3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:   \n",
       "..                                                                                                                    ...   \n",
       "60  None Found                                                                                                              \n",
       "61  None Found                                                                                                              \n",
       "62  None Found                                                                                                              \n",
       "63  None Found                                                                                                              \n",
       "64  \\nSQL\\nPython\\nCloud Data Warehouse Systems\\nETL\\nWorkflow Tools\\nBatch Processing\\nSpark\\nCD/CI tools                  \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Responsibilities  \\\n",
       "0   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "1   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "2   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "3   \\nOver five years of experience as a DBA\\nWorking knowledge of Oracle Autonomous Data Warehouse environments in Oracle’s OCI (Oracle Cloud Infrastructure)\\nExperience in tuning and optimizing DB performance\\nKnowledge of DB security best practices, and back and recovery processes\\nExperience in developing data architectures\\nYears of experience in ETL (Extract, Load, and Transform) from various data sources\\nKnowledge of Big Data and Time-series databases\\nData Cleansing expertise\\nWorking knowledge of data analysis / data science                                                                                                                                                                                                                                                                                                       \n",
       "4   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "..         ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "60  Collect, store, and aggregate data to support the creation of great data products for the business and our customers\\nBe creative and cooperative in designing and building data pipelines\\nUse Cloud-based infrastructure and applications (we use AWS and maintain our own Kubernetes clusters)\\nContinually learn and seek ways to improve our data flows\\nCollaborate well in a team environment (we use agile)\\nValidate data and maintain healthy infrastructure and data flows\\n                                                                                                                                                                                                                                                                                                                                                                        \n",
       "61  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "62  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "63  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "64  \\n3+ years experience with Data Warehouse Systems and working on an ETL system, either a commercial one like Matillion or Fivetran, an open-source one like Airflow, or a custom one you or your company built\\nExperience with one major cloud analytics database (Snowflake, Redshift, Google Big Query), Snowflake preferred\\nStrong familiarity with SQL\\nPython development experience in production\\nFamiliarity with Spark\\nA strong desire to show ownership of problems you identify and proven ability to empower others to get more done\\nFamiliarity with modern BI and exploration tools, Looker is preferred.\\nFamiliarity with GitHub or other CD/CI tools\\nBasic AWS experience (S3, EC2) (1-2 years)\\nSome familiarity with streaming approaches preferred\\nCS Degree preferred\\nSome experience preferred with Jenkins, Docker, Kubernetes   \n",
       "\n",
       "     Education  \\\n",
       "0   None Found   \n",
       "1   None Found   \n",
       "2   None Found   \n",
       "3   None Found   \n",
       "4   None Found   \n",
       "..         ...   \n",
       "60  None Found   \n",
       "61  None Found   \n",
       "62  None Found   \n",
       "63  None Found   \n",
       "64  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Requirement  \\\n",
       "0   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "2   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "3   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "4   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "..         ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "60  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "61  \\n3-5 Years BI Experience developing BI dashboards, reporting, visualizations and data models. Proven BI dashboard, report, and visualization design and delivery.\\n1-2 years Tableau Server experience. Proven Tableau performance tuning reporting and dashboard solutions.\\nTableau Server Experience: Configuration, Administration, Tuning & Performance, Data Connections, APIs\\nTableau Desktop & Tableau Prep Experience: Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience – including data extraction, transformation and load (ETL) from many sources; OLAP, OLTP, Datawarehouse’s, Hadoop/Cloudera, and Cloud platforms.\\nExperience with Tableau and Cognos Development.\\nExperience in data modeling and data prep (discovery, structuring, cleaning, enriching, validation, publishing). Tableau Prep preferred.\\nSource to Target mapping. Tableau Prep experience a major plus.\\nSQL Experience (joins, queries, select statements) – Oracle, SQL Server database back ends.\\nBI performance tuning across front end, ETL, Data back end optimization.\\nSolid Agile experience on BI projects – Gathering requirements, translating to use cases, BI stories, design/delivery of BI use cases – using SAFe, SCRUM, Agile, Kanban or similar method.\\nExposure Python.   \n",
       "62  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "63  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "64  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FullDescriptions  \n",
       "0   CirrusMD helps health plans create happier, healthier, and more engaged members by giving them access to on-demand virtual care solutions that they love to engage with. Our chat-powered care delivery platform connects members to a dedicated, board-certified physician in under 90 seconds from any web-enabled device, with no cost and no time limits attached. CirrusMD enables stress-free, human care conversation that doesn’t end until members get the answers (and peace of mind) they need to manage their wellness.\\nCirrusMD has partnered with over a dozen major national payers and healthcare systems to deliver extraordinary virtual care to millions of lives across the nation. The company was founded in 2012 and is headquartered in Denver, CO.\\nWhy Work Here?\\nJoin our team and help us deliver Care Without Barriers. Our company offers significant opportunity for motivated self-starters who thrive in a fast-paced environment that is quickly transitioning from a startup to a highly recognized healthcare industry disruptor. We offer an exceptional benefits package including health, dental and vision, 401k savings, flexible vacation and working policies, competitive salaries and stock options and an EcoPass. CirrusMD is located in the Catalyst HTI building in Denver’s RiNo neighborhood, a newly built office space, with access to open-air shared workspaces and community areas, and a highly engaged community of healthcare and tech innovation leaders. Subsidized parking, on-site gym and shower facilities are also available to our team. Join us and see for yourself!\\nWho We’re Looking For:\\nAn experienced engineer who can confidently contribute to our mission of redefining the healthcare experience for patients and providers. You are comfortable working independently as well as pairing with other engineer(s). You are comfortable breaking larger tasks down into an executable action plan for yourself, and perhaps others. We seek a motivated, self-starter who knows how to buckle-down and deliver; yet knows when to push back if the demand is too high or the directed path unclear.\\nWhat You’ll Be Responsible For Achieving:\\nBuilding and optimizing data pipelines, architectures and data sets\\nAutomating and optimizing ingestion of outside data from customers\\nA reputation for reliably delivering well considered, well tested and performant code\\nContributions to our evolving development and testing standards and best practices\\nHigh collaboration with Analytics + Product\\nWhat Will Make You Successful:\\nAdvanced working SQL knowledge and experience working with relational databases, query authoring as well as working familiarity with a variety of databases.\\nAdvanced programming skills in Python, Java, and/or Go (Golang)\\nExperience designing and managing complex relational database structures\\nProficient with AWS especially S3, EC2, RDS (Postgres), Redshift, Athena (Presto) or EMR (Spark)\\nYour philosophy aligns with Agile methodologies and processes\\nConfidence pairing with other engineers of all levels\\nWhat Will Make You Stand Out:\\nSignificant experience with data warehouses like Snowflake and ETL tools like Fivetran\\nExperience with 1 or more additional backend core technologies (Scala / Rust / Ruby / Elixir)\\nA reputation for superb communication skills with other engineers and teammates\\nYou have a reputation for a high level of craftsmanship about your work\\nStrong analytic skills related to working with structured and unstructured datasets\\nBackend experience designing REST & gRPC APIs\\n\\nTo Apply:\\nPlease submit resume including your salary requirements as PDF to EngineeringJobs@CirrusMD.com indicating \"Data Engineer\" in the subject line.\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\\nNotice to recruiters and placement agencies:\\nIf you are a recruiter or placement agency, please do not submit résumés to any person or email address at CirrusMD prior to having a signed agreement with Human Resources. CirrusMD is not liable for and will not pay placement fees for candidates submitted by any agency other than its approved recruitment partners. Also, any résumés sent to us without an agreement in place will be considered your company's gift to CirrusMD and may be forwarded to our Talent Acquisition team.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "1   Join us as we pursue our disruptive new vision to make machine data accessible, usable and valuable to everyone. We are a company filled with people who are passionate about our product and seek to deliver the best experience for our customers. At Splunk, we’re committed to our work, customers, having fun and most importantly to each other’s success. Learn more about Splunk careers and how you can become a part of our journey!\\nSplunk is located in Boulder, CO. This team is made up of a strong mix of very talented and driven individuals, with a proven senior leadership team with a strong technical background.\\nThe office layout is open and the floor plan helps to foster the casual, collaborative environment that has played a key role in the company’s innovation of its product. In addition, you will find everything you would expect in the office of a progressive and fun company – stocked kitchen with snacks and drinks to keep you nourished, rooftop patio, and no less than two craft beers on tap for your enjoyment.\\nOur Team's Mission and Methods\\nThe Data Analytics Team at Splunk focuses on delivering actionable data on a silver platter to both external customers and internal data scientists, data analysts and users.\\nOur new Data Analytics Platform was designed to satisfy the data quality requirements of our high-value and high-complexity data, while also providing fast query performance, ease of maintenance, automatic self-healing, immutable data, and fully automated CI/CD code deployment. Our target technology stack includes Kinesis, S3, Lambdas, Kubernetes, Athena (Presto), Postgres, Python, and Scala.\\nThe team's influences and backgrounds come from data warehousing, data lakes, big data, analytical data engineering, business intelligence, software engineering, and devops.\\nWe're currently building and transitioning into the Data Analytics Platform - and so are continuing to experiment, explore and evaluate these methods as we move forward.\\nAnd we are committed to making the work fun, interesting, and exciting while collaborating, mentoring, and supporting one another.\\nWho You Are\\nOver the last five or more years you've been working as a data engineer to build and maintain custom data pipelines and data at rest in support of reporting and data analysis using a number of elements from our technology stack described above.\\nYou think of data as one of the most valuable resources an organization has, not just an inconvenient by-product of a process, but an opportunity-rich source of future features and capabilities. You understand that the success of a data engineering team is measured by the success of our data consumers and so think deeply and creatively about ways to deliver high-quality, high-functionality and high-performance data for these users.\\nYou are a software engineer for whom automated testing is as important as clean, performant and reusable code.\\nYou are an enthusiast - of analyzing data, of helping people make a difference with data, of building great analytical solutions, and of automated testing and software delivery.\\nAnd as a Senior Data Engineer, you will be responsible for designing, developing and supporting data pipelines, data at rest, and machine learning applications. Software will be written in Scala, Python, SQL, and Spark using an Agile development process.\\n\\nBS in Computer Science or related plus 8 years of related professional experience\\nWhat We Offer You:\\nA constant stream of new things for you to learn. We're always expanding into new areas, bringing in open source projects and contributing back, and exploring new technologies.\\nA set of exceptionally talented and dedicated peers, all the way from engineering and QA to product management and user experience.\\nA stable, collaborative and supportive work environment within a team that is fully committed to everyone's success and joy.\\nWe take work-life balance seriously: we prioritize a sustainable engineering culture over heroism, prioritize work that makes your life better, and many of us work from home two days a week.\\nWe value diversity at our company. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or any other applicable legally protected characteristics in the location in which the candidate is applying.\\nFor job positions in San Francisco, CA, and other locations where required, we will consider for employment qualified applicants with arrest and conviction records.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "2   Senior Data Engineer, Data Informatics\\nCPAADSC2.Analyst, Data Science\\n\\nttec Digital Consulting’s Insights Practice is looking for a Data Engineer who is passionate about solving complex data problems and intricate engineering issues with large systems.\\n\\nWe combine human insight and the speed of technology to transform our clients’ interactions with their customers. Advanced analytics of the customer experience and customer interactions is our expertise.\\n\\nCheck out our website below to learn more about what we do and how we help our clients.\\nhttps://www.ttec.com/ttec-digital\\nWhat you’ll be doing:\\n\\nThis person will be a part of our Humanify Insights Platform focused on helping our clients and coordinating with internal business consulting, analyst, data science, and technology teams.\\nWork closely with data scientists to create impactful insights from complex data\\nDrive and participate in the definition, the direction, and the development of our analytics platform\\nImprove platform features and functionality by keeping up with the latest innovations in data technology\\nDesign, test, and maintain robust, scalable, secure, and fault-tolerant data management systems\\nAssist with process improvement with a customer focused, progressive mindset\\nCommunication of project status/issues to clients and internal management\\nPartner with various internal teams\\nDocument implementation requirements and expected effort\\nResearch and recommend data management best practices\\nAggressively and continuously advance skill set\\n\\nWhat you’ll bring to us:\\nBachelor’s degree qualifying under STEM with 2-3 years of relevant experience\\nExtensive knowledge of various databases\\nProficiency with Azure or AWS ecosystem\\nExperience with Big Data ETL/technologies – Hadoop, Hive, Spark\\nExperience working with structured, semi-structured, and unstructured data sources\\nProficiency with SQL\\nExperience with NoSQL solutions such as MongoDB is preferred\\nProficient in Python or Scala\\nExperienced in consuming third-party REST APIs (JSON) and SDKs\\nProficiency in Linux environment\\nUnderstanding of complex data flows, identification of data processing bottlenecks and designing and implementing solutions\\nA broad set of technical skills and knowledge across hardware, software, systems and solutions development and across more than one technical domain\\nPassion for innovation, delivering quality results, self-driven discovery, outside the box thinking, and complex problem solving\\n\\nWhat skills you’ll need:\\nProven ability to balance and manage multiple, competing priorities.\\nCollaborative interpersonal skills and ability to work within cross-functional teams.\\nSelf-starter who relies on experience and judgment to plan and accomplish goals in complex fast-paced environment to ensure quality of all data integration points.\\nExcellent customer service skills.\\nCreative problem-solving and analysis skills.\\nAbility to handle problem situations quickly, inventively, and resourcefully.\\nProject management skills including:\\nAbility to prioritize and manage tasks\\nAbility to plan, commit, and deliver to schedules\\nAbility to identify, escalate, and manage project issues\\nWillingness to work extended hours on an as-needed basis\\nSome travel is necessary\\nWhat We Offer:\\nVariable incentive bonus plan, 401K company match, tuition reimbursement\\nGlobal career mobility, employee recognition programs, professional development\\nState of the art technology which allows for seamless global connectivity\\nRich wellness program and health incentives\\nLead Everyday w Do the Right Thing w Reach for Amazing w Seek First to Understand w Act as One w Live life Passionately\\n\\n\\nPrimary Location: US-CO-Englewood\\nJob: Consulting                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "3   Principal Data Engineer & DBA-19000YP6\\n\\n\\nPreferred Qualifications\\n\\nPosition Responsibilities\\n\\nThe Oracle Cloud development is one of the biggest initiatives in the history of Oracle. We are introducing revolutionary SAAS, IAAS and PAAS products to the market place, and because many of these products are being built new, we are experiencing tremendous growth in our cloud storage product development team. Our systems move a huge amount of data and we need you to help build both the internal and external infrastructure to support it.\\n\\nWe are looking for an experienced data engineer / Database Administrator to help ETL, manage, and administrate our growing data warehouse. This person will be responsible for all aspects of a DBA role, managing our Cloud based Autonomous Data Warehouse on a daily basis. The person will also be responsible for structuring new DB schemas and working with stakeholders to understand they needs and requirements. Proposals for how to structure the data will need to be created and vetted with upper management. Part of duties includes ETL’ing data from various sources into our ADW instance for easy access and analysis. If this opportunity sounds exciting, look no further and join a growing and dynamic team to help develop a data and analytics based infrastructure in the Oracle Cloud organization. Our team has a start-up feel, but with the stability Oracle gives.\\n\\nDESIRED QUALIFICATIONS:\\nThe ideal candidate will have experience working with large dataset from multiple sources. They should have years of experience as a DBA and familiar with DB tuning. In addition, they should work well in teams, and understand how to create/optimize database schemas. The candidate will be expected to create simple and complex ETL jobs. Specific experience the ideal candidate will demonstrate includes:\\nOver five years of experience as a DBA\\nWorking knowledge of Oracle Autonomous Data Warehouse environments in Oracle’s OCI (Oracle Cloud Infrastructure)\\nExperience in tuning and optimizing DB performance\\nKnowledge of DB security best practices, and back and recovery processes\\nExperience in developing data architectures\\nYears of experience in ETL (Extract, Load, and Transform) from various data sources\\nKnowledge of Big Data and Time-series databases\\nData Cleansing expertise\\nWorking knowledge of data analysis / data science\\n\\nADDITIONAL SKILLS SOUGHT:\\nThe successful candidate is expected to demonstrate the following additional requirements:\\n\\nMS in Computer Science or Data Engineering\\n5+ years of experience working in as a DBA & Data Engineer\\nStrong knowledge of data structures, algorithms\\nHighly skilled in Python and SQL\\nExperience with OCI, AWS, and/or other IaaS environments.\\n\\nOracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.\\n\\n\\nDetailed Description and Job Requirements\\n\\nDesign, develop, troubleshoot and debug software programs for databases, applications, tools, networks etc.\\n\\nAs a member of the software engineering division, you will assist in defining and developing software for tasks associated with the developing, debugging or designing of software applications or operating systems. Provide technical leadership to other software developers. Specify, design and implement modest changes to existing software architecture to meet changing needs.\\n\\nDuties and tasks are varied and complex needing independent judgment. Fully competent in own area of expertise. May have project lead role and or supervise lower level personnel. BS or MS degree or equivalent experience relevant to functional area. 4 years of software engineering or related experience.\\n\\nOracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "4   Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe North America Data Strategy & Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.\\n\\n Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model\\n\\nRequired Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:\\no Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling\\no Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.\\no Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel\\n Able to travel up to 100% (Mon-Thu)\\n\\nOptional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model\\n Experience in compiling business cases and roadmaps for data, analytics and technology investments\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ...  \n",
       "60  Please review the job details below.\\nThe Data Intelligence Team is a central data engineering and analytics team that supports business decisions and company strategy across all parts of MAXAR. We collect data from a variety of interesting sources and build automated data flows that help enable actionable insights for many different teams and products. We are looking for data professionals that are excited to work with cloud-based tools and build high quality data flows. We are a team of people from a variety of data focused backgrounds. We support each other, and we help each other learn and grow. Come collaborate with us to help guide MAXAR and improve our collective understanding of our planet.\\nResponsibilities\\nCollect, store, and aggregate data to support the creation of great data products for the business and our customers\\nBe creative and cooperative in designing and building data pipelines\\nUse Cloud-based infrastructure and applications (we use AWS and maintain our own Kubernetes clusters)\\nContinually learn and seek ways to improve our data flows\\nCollaborate well in a team environment (we use agile)\\nValidate data and maintain healthy infrastructure and data flows\\nRequired\\nBachelor’s Degree in a technical field or equivalent work experience\\nExperience building and maintaining ETL pipelines\\n2 years of experience with Python, SQL, AWS (S3, EC2)\\nData visualization or reporting experience\\nGood verbal and written communication skills\\nPreferred\\nAny experience with…\\nAirflow or other ETL scheduling tools\\nBuilding or improving APIs\\nCollecting data from a variety of sources (APIs, Postgres, Oracle, SAP, Salesforce, etc.)\\nThoughtfully storing data in databases (especially Postgres) to support data products and reporting\\nAny experience with or willingness to learn…\\nBig data pipelines using Spark and streaming tools\\nMaintaining data pipelines used in a production environment\\nWorking with geospatial data\\nAdditional coding languages, especially JVM languages or Bash\\nAny of these or similar tools (Kubernetes, Ansible, Jenkins, PostgreSQL, Tableau)\\nAdditional AWS tools (EFS, Lambda, RDS/Aurora, Glue, Athena)\\nPerks\\nDiverse team that works and learns together\\nGreat benefits, flexible time off for family and travel, pet insurance discounts\\nWorking to improve products that helps us understand and protect our planet\\nWell-lit office space with on-site cafeteria and coffee shop\\nPrivate frisbee golf course and very nice game and quiet rooms for mental breaks\\nQuarterly self-guided work time to build what you want or learn new skills and technology\\nMAXAR Technologies offers a generous compensation package including a competitive salary; choice of medical plan; dental, life, and disability insurance; a 401(K) plan with competitive company match; paid holidays and paid time off.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "61  Tableau BI Engineer – ITmPowered Tableau Engineer will design, develop, and deliver high performance Tableau dashboards, workbooks, and visualizations providing Business Intelligence on hundreds of enterprise technology projects. Provide deep Tableau Server knowledge and expertise to the team on how to build and maintain Tableau Server dashboards and reports. Tableau Developer design, develop and implement […]\\n\\nTableau BI Engineer – ITmPowered\\n\\nTableau Engineer will design, develop, and deliver high performance Tableau dashboards, workbooks, and visualizations providing Business Intelligence on hundreds of enterprise technology projects.\\nProvide deep Tableau Server knowledge and expertise to the team on how to build and maintain Tableau Server dashboards and reports.\\nTableau Developer design, develop and implement BI solutions leveraging Tableau Desktop, Tableau Online, Tableau Prep, and Tableau Server.\\nProvide Tableau development and support across dozens of systems and data sources. Support existing Tableau server, Tableau reports, and Tableau Dashboard solutions.\\nPerformance tune Tableau dashboards (optimize extracts, limit fields/records, marks, optimize/materialize calculations, query optimization, workbook cleanup).\\nEngineer BI dashboards for Technology Projects and related Finance information (Burn Rates, Accruals, Milestones, Expenses, budgets, Earned Value, ROI) as well as project KPIs.\\nIdentify BI performance bottlenecks and design optimal solutions at the report / dashboard level (Tableau, Cognos, PowerBI), SQL data munging level (SQL joins, pivots, enrichment, aggregations Oracle, Hadoop, SQL), or ETL / Data movement level (DataStage, Informatica, Sqoop).\\nTableau Server Configuration, Administration, Tuning & Performance, Data Connections, APIs\\nTableau Desktop & Tableau Prep Experience: Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience – including data extraction, transformation and load (ETL) from many sources; OLAP, OLTP, Datawarehouse’s, Hadoop/Cloudera, and Cloud platforms.\\nWork with end users to gather BI requirements (use cases, visualization, drill up/down, hierarchies, tables, pivots, outcomes, and data sources, etc.).\\nBuild data models. Prepare, wrangle, and model the Data to derive effective data models supporting performant BI solutions. Understand technical data sources, data structures, data quality and necessary transformations to aggregate, enrich, validate, and publish data.\\nSource to target mappings and work with ETL Engineers to optimize data flows, and data preparation.\\nDevelop and optimize BI Dashboards, Reports, Data Models, and data flows.\\nSupport existing BI environments Tableau, Cognos, data movement (DataStage, Informatica, sqoop), and backend data repositories (Oracle, SQL Server, Hadoop).\\nCloud BI migration – Migrate big data solutions to cloud in PowerBI / Azure – design and delivery.\\n Requirements:\\n\\n3-5 Years BI Experience developing BI dashboards, reporting, visualizations and data models. Proven BI dashboard, report, and visualization design and delivery.\\n1-2 years Tableau Server experience. Proven Tableau performance tuning reporting and dashboard solutions.\\nTableau Server Experience: Configuration, Administration, Tuning & Performance, Data Connections, APIs\\nTableau Desktop & Tableau Prep Experience: Advanced Data Visualizations, including custom visualizations, Advanced Data Modeling Experience – including data extraction, transformation and load (ETL) from many sources; OLAP, OLTP, Datawarehouse’s, Hadoop/Cloudera, and Cloud platforms.\\nExperience with Tableau and Cognos Development.\\nExperience in data modeling and data prep (discovery, structuring, cleaning, enriching, validation, publishing). Tableau Prep preferred.\\nSource to Target mapping. Tableau Prep experience a major plus.\\nSQL Experience (joins, queries, select statements) – Oracle, SQL Server database back ends.\\nBI performance tuning across front end, ETL, Data back end optimization.\\nSolid Agile experience on BI projects – Gathering requirements, translating to use cases, BI stories, design/delivery of BI use cases – using SAFe, SCRUM, Agile, Kanban or similar method.\\nExposure Python.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "62  About FullContact:\\nFullContact is the premier provider of SaaS-based identity resolution that empowers brands to improve their customer experience and authentically engage with consumers. Using a consumer-first approach with our product offerings, we aim to make relationships better and that starts with our employees.\\nWe offer excellent benefits for our teammates, including full medical and dental coverage, our famous \"paid, paid vacation\" and a generous stock option plan.\\nYou'll join an innovative, enthusiastic team whose hard work helped us achieve recognition from the API awards, MarTech Breakthrough awards, and inclusion on the Inc. 5000 list of fastest-growing companies.\\nThe Role\\nWork on the Identity Resolution team as Senior Data Engineer to design and construct performant algorithms, infrastructure and data pipelines to manage our graph database for our identity resolution offering. Our identity graph is constructed from billions of observations leveraging the latest in big data technologies.\\nThe Identity Resolution team is primarily focused on providing identity resolution capabilities to our internal customers enabling identity resolution across the board for our external customers. This involves integrating data sets, developing our patented identity resolution graph algorithms and scaling all this to a very large amount of data. The team works on both live streaming and batch systems and leverages machine learning so that we can be the best in class for enterprise identity resolution.\\n\\nWhat You'll Do\\nBe a senior member on the Identity Resolution team\\nDesign, build, test, deploy and maintain systems using JVM based languages, focusing on Scala and Java\\nDesign, build, test and deploy massively parallel graph algorithms\\nCreate and maintain microservices connected through APIs (1000s requests/sec in some cases)\\nProcess large amounts of data leveraging big data technologies such as Spark, Kafka and more.\\nExpose data and tools to internal teams through APIs and libraries\\nAccount for quality and security as you build\\nYour Traits\\nYou are creative and enjoy solving problems\\nYou are curious. You look for the root cause of issues and are a life learner\\nYou are collaborative and love working with people, whiteboarding and designing hard problems\\nYou have got grit and recognize that the harder things in life are more rewarding\\nYou are empathetic for both customers and team members\\nAbout You\\n5+ years of experience in Data Engineering or Software Engineering\\nDeep understanding and experience developing in the JVM\\nYou have a solid mathematical foundation\\nExperience with a variety of databases (SQL, NoSQL, In-Memory, Searchable, etc)\\nSolid Linux experience - CLI tools, scripting\\nGit and AWS familiarity\\nAuthorized to work in the United States on a full-time basis\\nBonus Points for experience with Machine Learning and Graph Theory\\n\\nvPccapMAd3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "63  Company Overview\\n\\nFanatics is the global leader in licensed sports merchandise and changing the way fans purchase their favorite team apparel and jerseys. Through an innovative, tech-infused approach to making and selling fan gear in today's on-demand culture, Fanatics operates more than 300 online and offline stores, including the e-commerce business for all major professional sports leagues (NFL, MLB, NBA, NHL, NASCAR, MLS, PGA), major media brands (NBC Sports, CBS Sports, FOX Sports) and more than 200 collegiate and professional team properties, which include several of the biggest global soccer clubs (Manchester United, Real Madrid, Chelsea, Manchester City). Fanatics offers the largest collection of timeless and timely merchandise whether shopping online, on your phone, in stores, in stadiums or on-site at the world's biggest sporting events.\\n\\nAbout the Team\\n\\nFanatics is first and foremost a technology company. We are powered by cutting-edge tech created by our small agile teams using the latest tools and technologies under our highly analytical, forward thinking, and open-minded leadership. As the global leader in licensed sports merchandise, we challenge ourselves by improving our new fully responsive NodeJS cloud commerce platform, Elasticsearch engine, and deep data science capabilities while building the best-in-class retail manufacturing and supply chain technologies. Our tech teams work together to revolutionize data science and engineering initiatives, provide highly scalable real-time and streaming platforms, and create secure e-commerce and in-stadium fan experience products. Our own e-commerce platform transacts in over 190 countries, 17 languages, and 14 currencies. Our motto is “#GSD”—get stuff done—and we do just that. If you want to be at the nexus of sports, commerce, and technology, come be a part of our industry-leading team here at Fanatics Tech.\\n\\nThe FanStreams team at Fanatics has the grand vision of making many of the traditional batch-oriented processes into real time systems and is seeking a Senior Data Engineer with the passion and ingenuity to build streams of data across various domains of Fanatics world. These streams will power Data Science, BI & Site optimizations. The FanStreams team is responsible for building, managing complex stream processing topolgies using the latest open source tech stack, build metrics and visualizations on the generated streams and create varied data sets for different forms of consumption and access patterns. We’re looking for a seasoned engineer to help us build and scale the next generation of streaming platforms and infrastructure at Fanatics.\\nWhat will you do?\\nBuild data platforms and streaming engines that are real time in nature\\nOptimizing existing data platforms and infrastructure while exploring other technologies\\nProvide technical leadership to the data engineering team on how data should be stored and processed more efficiently and quickly at scale\\nBuild and scale stream & batch processing platforms using the latest open-source technologies\\nWork with data engineering teams and help with reference implementation for different use cases\\nImprove existing tools to deliver value to the users of the platform\\nWork with data engineers to create services that can ingest and supply data to and from external sources and ensure data quality and timeliness\\nWhat are we looking for?\\n5+ years of software development experience with at least 3+ years of experience on open-source big data technologies\\nKnowledge of common design patterns used in Complex Event Processing\\nProficiency in Streaming technologies: Apache Kafka, Kafka Streams, KSQL, Spark, Spark Streaming\\nProficiency in Java, Scala, SQL\\nExperience and deep understanding of traditional, NoSQL and columnar databases\\nExperience of building scalable infrastructure to support stream, batch and micro-batch data processing\\nBonus points\\nYou've worked with Amazon Web Services\\nTryouts are open at Fanatics! Our team is passionate, talented, unified, and charged with creating the fan experience of tomorrow. The ball is in your court now.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "64  Data Engineer\\n\\nPosition Overview\\nValidity is looking for a talented Data Engineer with 3+ years of experience in implementing modern data architectures. You will work closely with all areas of the business on engineering and analytical initiatives marked by greater complexity and less structure that will yield substantial product enhancements, uncover insights, and inform business decision making and focus. You will be working on one of the biggest opportunities at Validity: A major build-out of our data architecture. Your first projects will include helping to scale our data infrastructure and build out our data warehouse and analytics footprint. You will collaborate closely with Engineers and Product Managers to inform product decision making with data and to identify opportunities to create more value for our customers. This is a high-impact role that will help shape the future of Validity's products and services.\\nCompany Overview\\nValidity is a leading global provider of data integrity and compliance offerings that thousands of organizations worldwide rely on to trust their data. We're passionate about our people, our customers, our values and our culture!\\nJoin a passionate, driven team committed to bringing better insights and data-driven decisions to our internal and external customers. We're looking for people with a growth mindset and the insight to solve for today while building for the future. You will be working for a company that truly values the power of data.\\nEssential Position Duties and Responsibilities\\n3+ years experience with Data Warehouse Systems and working on an ETL system, either a commercial one like Matillion or Fivetran, an open-source one like Airflow, or a custom one you or your company built\\nExperience with one major cloud analytics database (Snowflake, Redshift, Google Big Query), Snowflake preferred\\nStrong familiarity with SQL\\nPython development experience in production\\nFamiliarity with Spark\\nA strong desire to show ownership of problems you identify and proven ability to empower others to get more done\\nFamiliarity with modern BI and exploration tools, Looker is preferred.\\nFamiliarity with GitHub or other CD/CI tools\\nBasic AWS experience (S3, EC2) (1-2 years)\\nSome familiarity with streaming approaches preferred\\nCS Degree preferred\\nSome experience preferred with Jenkins, Docker, Kubernetes\\nExperience/Skills\\nSQL\\nPython\\nCloud Data Warehouse Systems\\nETL\\nWorkflow Tools\\nBatch Processing\\nSpark\\nCD/CI tools\\n\\nWe are looking for someone to work in our Broomfield office but are open to a remote position depending on the situation.\\nfCVh3Ty9i9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "\n",
       "[65 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_Denver.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
