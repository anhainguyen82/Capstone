{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=Washington%2C+DC&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Annapolis Junction, MD 20701</td>\n",
       "      <td>Annapolis Junction</td>\n",
       "      <td>MD</td>\n",
       "      <td>20701</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nA Bachelor’s degree in electrical engineering, computer engineering, mathematics or a related discipline may be substituted for four years of general experience.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Imagine being able to shape the future of cyber… on a team that powers the recruitment of the most talented and motivated employees in the world through technological innovations. This is the reality of being part of AG Grace, Inc. We need leaders who want to discover, enhance, capture and counter cyber activity, who have a bias for action, and who have a deep desire to build….to make the previously impossible possible. Is that you?\\nYou’ll work on cutting edge technology and provide Solutions, Insights and Deliver professional services and solutions and that our customers have come to expect from AG Grace, Inc. You should be somebody who enjoys working on complex system software, is customer-centric, and feels strongly about building a software system that maximizes the value of cloud computing. As a developer on the team you’ll collaborate with sharp engineers to drive improvements to cyber analysis technology, design and develop new services and solutions, build and track metrics to ensure high quality of results.\\n\\nExperience\\nAt least five (5) years of general experience in software development/engineering, including requirements analysis, software development, installation, integration, evaluation, enhancement, maintenance, testing, and problem diagnosis/resolution.\\nAt least three (3) years of experience developing software with high level languages such as Java, C, C++.\\nAt least three (3) years of experience developing software in UNIX/Linux (Red Hat versions 3-5+) operating systems.\\nAt least three (3) years of experience in software integration and software testing, to include developing and implementing test plans and test scripts.\\nAt least two (2) years of experience with distributed scalable Big Data Store (NoSQL) such as HBase, Cloud Base/Accumulo, Big Table, etc., as well as two (2) years of experience with the Map Reduce programming model, the Hadoop Distributed File System (HDFS), and technologies such as Hadoop, Hive, Pig, etc\\nDemonstrated work experience with Serialization such as JSON and/or BSON\\nDemonstrated work experience with developing restful services, Ruby on Rails framework, LDAP protocol configuration management and cluster performance management (e.g. Nagios)\\nDemonstrated work experience developing solutions integrating and extending FOSS/COTS products.\\nDemonstrated technical writing skills and shall have generated technical documents in support of a software development project\\nDemonstrated work experience with Source Code Management {e.g. Git, Stash, or Subversion, etc.).\\nTS/SCI Full Scope Poly required\\nEducation\\nA Bachelor’s degree in electrical engineering, computer engineering, mathematics or a related discipline may be substituted for four years of general experience.\\nAG Grace, Inc. is dedicated to developing our nation's future and protecting our critical infrastructure resources. We provide a full range of IT services and solutions that help improve our client's ability to reduce risk, improve performance and provide mission assurance. We assist our customers in solving the problems of today and tomorrow. Come be a part of the Future as we help our customers execute their mission.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big Data Engineer INTELF8</td>\n",
       "      <td>Chantilly, VA</td>\n",
       "      <td>Chantilly</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark\\nDevelopment experience with Java, C++, Scala, Groovy, Python, and/or shell scripting\\nExperience with data warehousing tools and technologies\\nAbility to work within UNIX/Linux operating systems\\nAWS experience a plus\\nThis position requires U.S. Citizenship and an active TS/SCI security clearance</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark\\nDevelopment experience with Java, C++, Scala, Groovy, Python, and/or shell scripting\\nExperience with data warehousing tools and technologies\\nAbility to work within UNIX/Linux operating systems\\nAWS experience a plus\\nThis position requires U.S. Citizenship and an active TS/SCI security clearance</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Works is looking for senior Big Data Engineers able to lead the way in tackling the most difficult engineering challenges in Big Data systems\\nResponsibilities\\nData Works is seeking a Big Data Engineer with demonstrated experience in leading large scale data warehousing projects. A successful candidate will be strong in Map Reduce, Java, and possess an understanding of data science concepts such as machine learning and trend analysis. Candidate should also be familiar with indexing products such as Lucene and Elasticsearch. Relevant certifications considered but not required.\\nRequired Qualifications\\nExperience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark\\nDevelopment experience with Java, C++, Scala, Groovy, Python, and/or shell scripting\\nExperience with data warehousing tools and technologies\\nAbility to work within UNIX/Linux operating systems\\nAWS experience a plus\\nThis position requires U.S. Citizenship and an active TS/SCI security clearance\\nE3/Sentinel is an equal opportunity employer and Vietnam Era Veterans Readjustment Assistance Act (VEVRAA) federal contractor. All qualified applicants receive consideration for employment without regard to race, color, religion, gender, national origin, age, sexual orientation, gender identity, protected veteran status, or status as a qualified individual with a disability. E3/Sentinel hires and promotes individuals solely on the basis of their qualifications for the job to be filled.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Arlington, VA</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Software/Data Engineer who joins Sila will support a unique mission that leverages leading technology tools to facilitate the highly efficient integration of data and advanced analytics.\\nIn this role, you will architect an API Gateway and oversee application integration across the enterprise. Your results, as a Software/Data Engineer, will enable access to or submission of heterogeneous data, exposure of analytic services through standard interfaces, and integration with external partners.\\n\\nCollaborating with data scientists, data engineers, and client stakeholders, you will work together to define the requirements that drive the API strategy. Your existing, personal knowledge of distributed application architectures, to include APIs, web services, microservices, and asynchronous event protocols contributes substantially to this endeavor.\\n\\nThe successful candidate has the option to work on-site in Bethesda, MD or Reston, VA when not at Sila’s office in Arlington, VA.\\n\\nCandidates must be currently authorized to work in the United States without the need for employment-based visa sponsorship now or in the future.\\nYou will:\\nArchitect, build, and manage API’s supporting on premise and cloud platform environments such as AWS.\\nContribute to the conceptual and physical design of application integration using APIs and events\\nDevelop and test an API gateway that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale\\nDevelop and test runtime execution of APIs\\nPrototyping and demonstrate concepts when necessary (e.g. mock-client apps)\\nExperience\\nBachelor’s degree in Computer Science or Business Information Systems or equivalent educational or professional experience and/or qualifications with at least 6 years of experience in software development\\nMust possess a TS security clearance and be able to obtain a DoD TS/SCI clearance\\nAt least 1 year of experience in developing REST services using Java or Python or Node.js\\nExperience delivering APIs for external partners and integrating with external vendors\\nAble to implement processes and troubleshoot to continue to improve operational stability\\nExperience with GitHub or GitLab\\nUnderstanding of DevOps processes\\nStrong communication, interpersonal skills and problem-solving skills\\nDesired Experience\\nPrevious Agile SW team participation\\nUse of JIRA, Confluence, and Jenkins\\nElasticsearch and logging framework use\\nApache JMeter or Blazemeter use/knowledge\\nTwo (2) or more years of AWS Commercial and/or AWS GovCloud experience\\nHigh-volume applications tuning experience\\nKnowledgeable in security protocols such as SAML and OAuth\\nAbout Sila\\n\\nSila is a technology and management consulting firm that delivers solutions to the world’s leading corporations and Federal government agencies. Our solutions expertise lies in the areas of cybersecurity, risk management, data analytics, software engineering and integration, strategy and transformation, and digital creative services. We are a values-driven company with a culture that fosters collaboration, creativity, and social responsibility. Sila employees are part of a community of vibrant, high-performing contributors who push each other to achieve the highest standard of excellence. Our team members have extensive opportunities to discover their passions and shape their own career paths, and we continually invest in employees’ growth through innovative training, mentorship, and professional development programs. Staff are quickly immersed in clients’ business challenges, work closely with emerging technologies to develop impactful solutions to these challenges, and are exposed to a variety of industries and market offerings.\\n\\nWe are looking for full-time employees to become an integral part of our growing team. Sila is headquartered in Washington, D.C. and has offices in Chicago, IL; Seattle, WA; and Shelton, CT. Sila offers a range of great benefits including a comprehensive healthcare package, 401K with matching, paid time off, and paid company holidays, as well as other unique benefits that support our staff’s active work/life balance.\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.\\nBenefits\\nHealth &amp; Wellness\\nComprehensive healthcare insurance options, flexible spending accounts, disability and life insurance\\n401k\\nRobust 401k plan with matching for your retirement savings\\nPaid Time Off\\nGenerous PTO allowance, and we actually take it!\\nProfessional Development\\nCore curriculum of training plus a variety of professional development and learning channels\\nSocial Events\\nPaint nite, races, mixology classes—we have something for everyone at our monthly social events\\nWork/Life Balance\\nWe understand and value the importance of life outside of work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google Technical Architect</td>\n",
       "      <td>Arlington, VA 22209</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>VA</td>\n",
       "      <td>22209</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum 5 years of Consulting or client service delivery experience on Google GCP\\n</td>\n",
       "      <td>DevOps on an GCP platform. Multi-cloud experience a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Google Cloud Platform (GCP) Technical Architect Delivery is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would also be responsible for developing and delivering Google GCP cloud solutions to meet todays high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Google GCP Technical Architect is a highly performant GCP Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data soltuions on cloud. Using Google GCP public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:Work with Sales and Bus Dev teams in providing Data and GCP Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS &amp; NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nBasic Qualifications\\nMinimum 5 years of Consulting or client service delivery experience on Google GCP\\nMinimum 10 years of experience in big data, database and data warehouse architecture and delivery\\nBachelors degree or 12 years previous professional experience\\nAble to travel 100% (M-TH)\\nMinimum of 5 years of professional experience in 2 of the following areas:\\nSolution/technical architecture in the cloud\\nBig Data/analytics/information analysis/database management in the cloud\\nIoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using GCP services etc.:\\nData Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core\\nStreaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam\\nData Warehousing &amp; Data Lake : BigQuery, Cloud Storage\\nAdvanced Analytics : Cloud ML engine, Google Data Studio, Tensorflow &amp; Sheets\\n\\nFamiliarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nCertified GCP Solutions Architect - Associate\\nCertified GCP Solutions Architect – Professional (Nice to have)\\nCertified GCP Big Data Specialty (Nice to have)\\nCertified GCP AI/ML Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an GCP platform. Multi-cloud experience a plus.\\nExperience developing and deploying ETL solutions on GCP\\nStrong in Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience beyond GCP a plus - AWS and Azure\\n\\nProfessional Skill Requirements\\nProven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Technical Writer</td>\n",
       "      <td>Crystal City, VA</td>\n",
       "      <td>Crystal City</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n3 or more years of experience in the Information Technology field\\nBachelor's degree\\nExcellent communications skills working with customers and engineers on a software development project\\nPrior experience:\\nWorking with development teams\\nWorking with users to create user stories and define requirements\\nDocumenting programs and processes\\nReporting status to customers and management\\nIdentifying and evaluating risks\\nTechnologies:\\nMS Office Suite\\nJIRA and Confluence, or a similar tracking and documentation system\\nSharePoint, or a similar documentation system\\nMust currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence (CI) polygraph examination\\n</td>\n",
       "      <td>\\nExperience working on information technology programs developing user interfaces and complex data processes\\nComfortable acting as a liaison between customers and a development team\\nAgile development certifications (scrum master, product owner)\\n</td>\n",
       "      <td>\\nDetermine the needs of end users of technical documentation\\nStudy product samples and talk with product designers and developers\\nWork with technical staff to make products easier to use and thus need fewer instructions\\nOrganize and write supporting documents for products\\nUse photographs, drawings, diagrams, animation, and charts that increase users' understanding\\nSelect appropriate medium for message or audience, such as manuals or online videos\\nStandardize content across platforms and media\\nGather usability feedback from customers, designers, and manufacturers\\nRevise documents as new issues arise\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Are you passionate about solving challenging problems?\\nDo you thrive being a critical part of an elite team of like-minded people?\\nHow would you like for your next career move to take you to the next level?\\n\\nIf any of this sounds appealing, look no further.\\n\\nJob Description:\\nWe are seeking a data engineer/analyst who is excited about analyzing and solving national security related problems.\\n\\nResponsibilities include:\\n\\nDetermine the needs of end users of technical documentation\\nStudy product samples and talk with product designers and developers\\nWork with technical staff to make products easier to use and thus need fewer instructions\\nOrganize and write supporting documents for products\\nUse photographs, drawings, diagrams, animation, and charts that increase users' understanding\\nSelect appropriate medium for message or audience, such as manuals or online videos\\nStandardize content across platforms and media\\nGather usability feedback from customers, designers, and manufacturers\\nRevise documents as new issues arise\\n\\nBasic Qualifications:\\n\\n3 or more years of experience in the Information Technology field\\nBachelor's degree\\nExcellent communications skills working with customers and engineers on a software development project\\nPrior experience:\\nWorking with development teams\\nWorking with users to create user stories and define requirements\\nDocumenting programs and processes\\nReporting status to customers and management\\nIdentifying and evaluating risks\\nTechnologies:\\nMS Office Suite\\nJIRA and Confluence, or a similar tracking and documentation system\\nSharePoint, or a similar documentation system\\nMust currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence (CI) polygraph examination\\n\\nDesired Skills:\\n\\nExperience working on information technology programs developing user interfaces and complex data processes\\nComfortable acting as a liaison between customers and a development team\\nAgile development certifications (scrum master, product owner)\\n\\nSo, what does Novetta do?\\n\\nWe focus on three core areas: Cyber, Entity, and Multi-Int Analytics. Our products are focused on processing and analyzing vast amounts of data in these core areas. Our services are focused on helping our customers move from complexity to clarity. At Novetta, we bridge the gap between what our customers think they can do and what they aspire to achieve.\\n\\nOur culture is shaped by a commitment to our Core Values:\\n\\nIntegrity: We hold ourselves accountable to the highest standards of integrity and ethics.\\nCustomer Mission Success: Customer mission success drives our daily efforts—we strive always to exceed customer expectations and focus on mission success beyond contractual commitments.\\nEmployee Focus: We value our employees and demonstrate our commitment to them by providing clear communications, outstanding benefits, career development, and opportunities to work on problems and technical challenges of national significance.\\nInnovation: We believe that innovation is critical to our success – that discovering new and more effective ways to achieve customer mission success is what makes us a great company.\\n\\nGET A REFERRAL BONUS FOR THE GREAT PEOPLE YOU KNOW!\\nWith our amazing referral program, you could be eligible to earn\\noutstanding rewards for referring qualified new hires to Novetta.\\n\\nNovetta is an equal opportunity/affirmative action employer.\\nAll qualified applicants will receive consideration for employment without regard to sex,\\ngender identity, sexual orientation, race, color, religion, national origin, disability,\\nprotected veteran status, age, or any other characteristic protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Chantilly, VA</td>\n",
       "      <td>Chantilly</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMinimum three (3) years’ experience in designing, developing, building, and implementing Big Data solutions or developing automated solutions to solve complex problems, a thoughtful ability to solve problems could outweigh years of experience.\\nAbility to identify and implement a data solution strategy\\nDemonstrates intellectual curiosity in exploring new technologies and finding creative ways to solve data management problems\\nExperience developing solutions with Python/Javascript/PERL\\nExperience/knowledge of Spark, Impala, Hadoop, Streamsets, Kafka, Rest APIs\\nExperience in SQL-based technologies\\nExperience with at least one of the following NoSQL Database technologies:\\nArrango\\nMark Logic\\nHBase\\nImpala\\nParquet\\nRedShift\\nExperience in Linux administration/troubleshooting</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nAssist in the development and delivering of large scale data pipelines\\nLeverage new database technologies to improve customer data solutions\\nDevelop and implement automated tests for data transformations and data migrations\\nResearch and apply big data solution technologies to complex datasets; make recommendations to data science team on new technologies\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Basic Qualifications\\nBachelor's degree in software engineering or a related technical field is required (or equivalent experience), plus a minimum of 5 years of relevant experience; or Master's degree plus a minimum of 3 years of relevant experience. Agile experience preferred.\\n\\nKEY SKILLS\\nMinimum three (3) years’ experience in designing, developing, building, and implementing Big Data solutions or developing automated solutions to solve complex problems, a thoughtful ability to solve problems could outweigh years of experience.\\nAbility to identify and implement a data solution strategy\\nDemonstrates intellectual curiosity in exploring new technologies and finding creative ways to solve data management problems\\nExperience developing solutions with Python/Javascript/PERL\\nExperience/knowledge of Spark, Impala, Hadoop, Streamsets, Kafka, Rest APIs\\nExperience in SQL-based technologies\\nExperience with at least one of the following NoSQL Database technologies:\\nArrango\\nMark Logic\\nHBase\\nImpala\\nParquet\\nRedShift\\nExperience in Linux administration/troubleshooting\\nCLEARANCE REQUIREMENTS:\\nA TS/SCI security clearance with the ability to obtain a Polygraph is required at time of hire. Candidate must be able to obtain the Polygraph within a reasonable amount of time from date of hire. Applicants selected will be subject to a U.S. Government security investigation and must meet eligibility requirements for access to classified information. Due to the nature of work performed within our facilities, U.S. citizenship is required.\\nResponsibilities for this Position\\nA Relocation package may be available for this position.\\n\\nGeneral Dynamics Mission Systems (GDMS) is seeking motivated candidates to join our insider threat detection, systems integration team. Our mission oriented team is responsible for the design, testing, deployment, maintenance, operation, and evolution of the systems directly supporting the insider threat detection program of a large government customer in the United States Intelligence Community (USIC). GDMS has an immediate opening on the team for a motivated Big Data Engineer with a self-starter mindset who is up to date with the latest tools and techniques. The position will focus on the integration of new data management technologies and software performance tuning and troubleshooting. This is a challenging yet rewarding position that provides an opportunity to leverage cutting edge technologies in pursuit of a vital mission that protects people, sensitive information/technologies, and the national security posture of the USIC.\\n\\nThe majority of work will be performed in Chantilly, Virginia, which is located approximately 25 miles west of Washington D.C., near the Dulles International Airport. The selected Big Data Engineer will support a 6+ year contract that General Dynamics recently secured.\\n\\nCORE RESPONSIBILITIES:\\nAssist in the development and delivering of large scale data pipelines\\nLeverage new database technologies to improve customer data solutions\\nDevelop and implement automated tests for data transformations and data migrations\\nResearch and apply big data solution technologies to complex datasets; make recommendations to data science team on new technologies\\n\\n#CJ3\\n#CB\\nCompany Overview\\nGeneral Dynamics Mission Systems (GDMS) engineers a diverse portfolio of high technology solutions, products and services that enable customers to successfully execute missions across all domains of operation. With a global team of 13,000+ top professionals, we partner with the best in industry to expand the bounds of innovation in the defense and scientific arenas. Given the nature of our work and who we are, we value trust, honesty, alignment and transparency. We offer highly competitive benefits and pride ourselves in being a great place to work with a shared sense of purpose. You will also enjoy a flexible work environment where contributions are recognized and rewarded. If who we are and what we do resonates with you, we invite you to join our high performance team!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Engineer - Card Technology</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s degree\\nAt least 1 year of experience with leading big data technologies such as Apache Spark, Apache Hadoop, or Apache Kafka\\nAt least 2 years of professional experience with data engineering concepts\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>McLean 2 (19052), United States of America, McLean, Virginia\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nData Engineer - Card Technology\\n\\nCapital One (yes, the “what’s in your wallet?” company!) is rethinking the way the world approaches banking. As a Capital One Data Engineer, you will develop fast data infrastructure leveraging Apache Spark, Databricks, and Apache Kafka to manage and create information products using the data streamed from our fleet of over 2000 ATM’s . Whether a new feature or a bug fix, you will lead your work and deliver the most elegant and scalable solutions, all while learning and growing your skills. Most importantly, you’ll work and collaborate with a nimble, autonomous, cross-functional team of makers, breakers, doers, and disruptors who love to solve real problems and meet real customer needs.\\n\\nThe person we're looking for:\\nhas a sense of intellectual curiosity and a burning desire to learn\\n\\nis self-driven, actively looks for ways to contribute, and knows how to get things done\\n\\nis deliriously customer-focused\\n\\nvalues data and truth over ego\\n\\nhas a strong sense of engineering craftsmanship, takes pride in the code they write\\n\\nbelieves that good software development includes good testing, good documentation, and good collaboration\\n\\nhas great communication and reasoning skills, including the ability to make a strong case for technology choices\\n\\nBasic Qualifications:\\n\\nBachelor’s degree\\nAt least 1 year of experience with leading big data technologies such as Apache Spark, Apache Hadoop, or Apache Kafka\\nAt least 2 years of professional experience with data engineering concepts\\n\\nPreferred Qualifications:\\n\\n2+ years experience with AWS cloud\\n2+ years of experience in Java, Scala, or Python\\n2+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python\\n2+ years of experience building data pipelines\\nAt least 1 year of Cloud (AWS, Azure, Google) development experience\\nExperience with Streaming and/or NoSQL implementation (Mongo, Cassandra, etc.) a plus\\n\\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Crystal City, VA</td>\n",
       "      <td>Crystal City</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n6 or more years of experience in software/system development\\n2 or more years of experience working with data technologies\\nTechnologies:\\nBachelor's in Computer Engineering, Computer Science, Information Technology, or related field, or equivalent experience\\nMust currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence (CI) polygraph examination\\n</td>\n",
       "      <td>\\nInterest In:\\nBuilding data services\\nScaling systems on AWS - Elastic MapReduce\\nBuilding and Managing large performant data pipelines\\nExperience with:\\nWorking on a cross functional team\\nDelivering big data solutions\\nAmazon Web Services\\nDevOps best practices - Jenkins\\nDebugging data pipeline issues\\nComfortable with:\\nInteraction with analysts to help drive big data analytics\\nAgile development\\nHands on system engineering tasks\\n</td>\n",
       "      <td>\\nManipulate and store large amounts of data with an eye to performance and efficiency\\nWork with technologies in the Hadoop ecosystem such as HBase, Spark, Phoenix\\nAutomate deployment and ingestion of data\\nDesign and build data services for consumption by application developers\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Are you passionate about solving challenging problems?\\nDo you thrive being a critical part of an elite team of like-minded people?\\nHow would you like for your next career move to take you to the next level?\\n\\nIf any of this sounds appealing, look no further.\\n\\nJob Description:\\nWe are seeking a data minded hadoop engineer who is excited about solving national security related data problems. If data pipelines, cloud infrastructure, and automation is something you would be energized to work on, this is the position for you.\\n\\nResponsibilities include:\\n\\nManipulate and store large amounts of data with an eye to performance and efficiency\\nWork with technologies in the Hadoop ecosystem such as HBase, Spark, Phoenix\\nAutomate deployment and ingestion of data\\nDesign and build data services for consumption by application developers\\n\\nBasic Qualifications:\\n\\n6 or more years of experience in software/system development\\n2 or more years of experience working with data technologies\\nTechnologies:\\nBachelor's in Computer Engineering, Computer Science, Information Technology, or related field, or equivalent experience\\nMust currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence (CI) polygraph examination\\n\\nDesired Skills:\\n\\nInterest In:\\nBuilding data services\\nScaling systems on AWS - Elastic MapReduce\\nBuilding and Managing large performant data pipelines\\nExperience with:\\nWorking on a cross functional team\\nDelivering big data solutions\\nAmazon Web Services\\nDevOps best practices - Jenkins\\nDebugging data pipeline issues\\nComfortable with:\\nInteraction with analysts to help drive big data analytics\\nAgile development\\nHands on system engineering tasks\\n\\nSo, what does Novetta do?\\n\\nWe focus on three core areas: Cyber, Entity, and Multi-Int Analytics. Our products are focused on processing and analyzing vast amounts of data in these core areas. Our services are focused on helping our customers move from complexity to clarity. At Novetta, we bridge the gap between what our customers think they can do and what they aspire to achieve.\\n\\nOur culture is shaped by a commitment to our Core Values:\\n\\nIntegrity: We hold ourselves accountable to the highest standards of integrity and ethics.\\nCustomer Mission Success: Customer mission success drives our daily efforts—we strive always to exceed customer expectations and focus on mission success beyond contractual commitments.\\nEmployee Focus: We value our employees and demonstrate our commitment to them by providing clear communications, outstanding benefits, career development, and opportunities to work on problems and technical challenges of national significance.\\nInnovation: We believe that innovation is critical to our success – that discovering new and more effective ways to achieve customer mission success is what makes us a great company.\\n\\nGET A REFERRAL BONUS FOR THE GREAT PEOPLE YOU KNOW!\\nWith our amazing referral program, you could be eligible to earn\\noutstanding rewards for referring qualified new hires to Novetta.\\n\\nNovetta is an equal opportunity/affirmative action employer.\\nAll qualified applicants will receive consideration for employment without regard to sex,\\ngender identity, sexual orientation, race, color, religion, national origin, disability,\\nprotected veteran status, age, or any other characteristic protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Sterling, VA 20166</td>\n",
       "      <td>Sterling</td>\n",
       "      <td>VA</td>\n",
       "      <td>20166</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's Degree or higher in Computer Sciences or similarMinimum of 5-6 years Software Industry experience3+ years of development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline or Airflow, S3, Cloud Formation and CLI (must to have ) and Jenkins4+ years of development experience with Apache Spark, Presto, SQL and NoSQL Implementation5+ years of extensive working knowledge in different programming Scala ( Must ), Shell and Python (Must).Proficiency working with structured, semi-structured and unstructured data sets including social, web logs and real time streaming data feedsAble to tune Big Data solutions to improve performance and end-user experienceKnowledge on Visualization and Data Science Tools.Expert level usage with Jenkins, GitHub is preferredSpark developer certification is a plusAbility and eagerness to constantly learn and teach othersExperience in the media industry is a plusMust have the legal right to work in the United State</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's Degree or higher in Computer Sciences or similarMinimum of 5-6 years Software Industry experience3+ years of development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline or Airflow, S3, Cloud Formation and CLI (must to have ) and Jenkins4+ years of development experience with Apache Spark, Presto, SQL and NoSQL Implementation5+ years of extensive working knowledge in different programming Scala ( Must ), Shell and Python (Must).Proficiency working with structured, semi-structured and unstructured data sets including social, web logs and real time streaming data feedsAble to tune Big Data solutions to improve performance and end-user experienceKnowledge on Visualization and Data Science Tools.Expert level usage with Jenkins, GitHub is preferredSpark developer certification is a plusAbility and eagerness to constantly learn and teach othersExperience in the media industry is a plusMust have the legal right to work in the United State</td>\n",
       "      <td>Position Summary\\nOur Team\\nAs Discovery Inc’s portfolio continues to grow – around the world and across platforms – the Global Technology &amp; Operations team is building media technology and IT systems that meet the world class standard for which Discovery is known. GT&amp;O builds, implements and maintains the business systems and technology that are critical for delivering Discovery’s products, while articulating the long-term technology strategy that will enable Discovery’s growing pay-TV, digital terrestrial, free-to-air and online services to reach more audiences on more platforms.\\n\\nFrom Amsterdam to Singapore and from satellite and broadcast operations to SAP, we are driving Discovery forward on the leading edge of technology.\\nThe Global Data Analytics team enables Discovery to turn data into action. Using big data platforms, data warehousing and business intelligence technology, audience data, advanced analytics, data science, visualization, and self-service analytics, this team supports company efforts to increase revenue, drive ratings, and enhance consumer engagement.\\n\\nThe Role\\nThe Sr Data Engineer should be a technical contributor who has hands-on knowledge of all phases in building large-scale cloud based distributed data processing systems and applications. You will be part of the Global Data &amp; Analytics engineering technology team and will partner closely with a team of data scientists, business analysts &amp; data engineers leading Discovery’s cloud based Big Data &amp; Analytics strategy.\\nYou’ll work on implementing complex AWS based big data projects with a focus on collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into insights using multiple technology platforms. Therefore, this role requires an understanding of how a secure big data cloud environment is architected to gain real insights faster, with less friction and complexity. The Sr. data engineer should be passionate about working with cutting edge technologies in solving problems and developing prototypes using different open source tools for the selected solutions.\\n\\nYou’ll need to be an innovative forward-thinker who will help lead end-to-end execution of data engineering initiatives and contribute directly to existing and emerging business strategies and goals. Creativity, Attention to detail and ability to work in a collaborative team environment are essential.\\nThe Sr. Data Engineer will work closely with the Data Engineering head to decide on needed infrastructure architecture and software design needs and act according to the decisions.\\nResponsibilities\\n1. Lead the design, implementation, and continuous delivery of pipelines using distributed AWS based big data technologies supporting data processing initiatives across batch and streaming datasets\\n2. Responsible for development using Scala , Python languages and Big Data Frameworks such as Spark, EMR, Presto, AWS Athena,Kafka, , and Kinesis\\n3. Provide administrative support on deployed AWS platform components\\n4. Identify, evaluate and implement cutting edge big data pipelines and frameworks required to provide requested capabilities to integrate external data sources and APIs\\n5. Review, analyse and evaluate market requirements, business requirements and project briefs in order to design the most appropriate end-to-end technology solutions\\n6. Process and manage high volume real time customer interaction streams\\n7. Provide architectural support by building Proof of Concepts &amp; Prototypes\\n8. Self-Starter to deliver data engineering solutions to optimize both the cost and existing solution\\n9. Stay current with emerging technologies and industry trends\\nRequirements\\nBachelor's Degree or higher in Computer Sciences or similarMinimum of 5-6 years Software Industry experience3+ years of development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline or Airflow, S3, Cloud Formation and CLI (must to have ) and Jenkins4+ years of development experience with Apache Spark, Presto, SQL and NoSQL Implementation5+ years of extensive working knowledge in different programming Scala ( Must ), Shell and Python (Must).Proficiency working with structured, semi-structured and unstructured data sets including social, web logs and real time streaming data feedsAble to tune Big Data solutions to improve performance and end-user experienceKnowledge on Visualization and Data Science Tools.Expert level usage with Jenkins, GitHub is preferredSpark developer certification is a plusAbility and eagerness to constantly learn and teach othersExperience in the media industry is a plusMust have the legal right to work in the United State\\nSterling, Virginia, VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Software Developer - TS/SCI w/Poly</td>\n",
       "      <td>McLean, VA 22102</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>22102</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description\\nJob Description:\\nLeidos has a need for a Data Engineer to support the development of a data lake for a project located in Mclean, VA.\\nIn this role, candidate will work at a customer site to support the agile development of tools and leverage standard tools (particularly Apache-NIFI) for Extract, Transform, and Loading data between databases for the sponsor. The successful candidate will create custom code to quickly extract, triage, and exploit data across domains in support of analytic work while supporting the strategic development of replicable processes. The successful candidate will use NIFI to ETL data into a secure Hadoop environment. They must write NIFI processors or, in instances where NIFI cannot be implemented, write custom Java code to ingest existing and new data sources. The candidate will conduct product usability tests and must work efficiently with a cross functional team members to include analysts, data scientists, project managers, and software solutions integrators.\\nRequired Education and Experience:\\no Must have a TS/SCI with Poly to be considered\\no Masters Degree and 15+ years of experience in data engineering and/or database administration.\\nRequired skills are:\\no Experience with Apache-NIFI, Kafka, and Spark Streaming for ETL work\\no UNIX\\no Familiarity with HBase, solr, Spark, Oozie, and Impala\\no Java and Python proficient\\no Understanding and proficiency in cross-domain solutions (ETLing data from unclassified to classified systems and across classified environments)\\no Agile development and proficiency in continuous integration/delivery tools such as Jenkins, Artifactory, and Git\\nPreferred skills:\\no Proficiency with AWS and container technologies such as Docker desired but not required.\\nExternal Referral Bonus:\\nEligible\\nPotential for Telework:\\nNo\\nClearance Level Required:\\nTop Secret/SCI with Polygraph\\nTravel:\\nYes, 10% of the time\\nScheduled Weekly Hours:\\n40\\nShift:\\nDay\\nRequisition Category:\\nProfessional\\nJob Family:\\nDatabase Management\\nLeidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.\\nPay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.\\nLeidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.\\nAll qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Annapolis Junction, MD</td>\n",
       "      <td>Annapolis Junction</td>\n",
       "      <td>MD</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5+ years of related experience\\nActive TS/SCI clearance\\nDoD 8570 compliance or information assurance certification.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5+ years of related experience\\nActive TS/SCI clearance\\nDoD 8570 compliance or information assurance certification.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\nThis position is not yet funded.\\nPerforms data analysis, interpretation, and management duties. Develops rules and methodologies for data collection and analysis\\nResearches and evaluates new concepts and processes to improve performance.\\nAnalyzes cross-functional problem sets, identifies root causes and resolves issues.\\nAssists more junior level technicians, specialists, and managers in their activities.\\nWorks individually, actively participates on integrated teams, and leads multiple tasks, projects or teams.\\nOversees and monitors performance, and when required, takes steps to resolve issues.\\nDirects multiple teams through to project completion.\\nProvides guidance and direction to lower level technicians, specialists, and managers.\\nEducation\\nBachelor’s Degree\\nQualifications\\n5+ years of related experience\\nActive TS/SCI clearance\\nDoD 8570 compliance or information assurance certification.\\nFor more than 50 years, General Dynamics Information Technology has served as a trusted provider of information technology, systems engineering, training and professional services to customers across federal, state, and local governments, and in the commercial sector. Over 40,000 GDIT professionals deliver enterprise solutions, manage mission-critical IT programs and provide mission support services worldwide. GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Federal - Data Visualization Specialist</td>\n",
       "      <td>Arlington, VA 22209</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>VA</td>\n",
       "      <td>22209</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMinimum 2 years of experience working with end users to create mockups and design documents\\nMinimum 2 years of experience with Tableau, PowerBI, QlikView and/or QlikSense\\nBachelor’s degree\\nNo Dual Citizenship</td>\n",
       "      <td>\\nMinimum 2 years of experience working with end users to create mockups and design documents\\nMinimum 2 years of experience with Tableau, PowerBI, QlikView and/or QlikSense\\nBachelor’s degree\\nNo Dual Citizenship</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Organization: Accenture Federal Services\\nLocation: Arlington, VA\\n\\nAccenture Federal Services, a wholly owned subsidiary of Accenture LLP, is a U.S. company with offices in Arlington, Virginia. Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations. Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.\\n\\nWe believe that great outcomes are everything. It’s what drives us to turn bold ideas into breakthrough solutions. By combining digital technologies with what works across the world’s leading businesses, we use agile approaches to help clients solve their toughest problems fast—the first time. So, you can deliver what matters most.\\nCount on us to help you embrace new ways of working, building for change and put customers at the core. A wholly owned subsidiary of Accenture, we bring over 30 years of experience serving the federal government, including every cabinet-level department. Our 7,200 dedicated colleagues and change makers work with our clients at the heart of the nation’s priorities in defense, intel, public safety, health and civilian to help you make a difference for the people you employ, serve and protect.\\n\\nAs a Visualization Specialist, you will build custom dashboards to visualize data. You will also create proof-of-concept solutions to communicate analytics developed by the data scientist. You will assist the data engineer with data preparation.\\n\\nWhat you’ll be doing:\\ndata analytics and user facing dashboard design\\ntranslate data analysis into business recommendations\\nbuilding dashboards\\nTableau\\n\\nBasic Skills &amp; Qualifications:\\nMinimum 2 years of experience working with end users to create mockups and design documents\\nMinimum 2 years of experience with Tableau, PowerBI, QlikView and/or QlikSense\\nBachelor’s degree\\nNo Dual Citizenship\\nAn active security clearance or the ability to obtain one may be required for this role.\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Engineer, Mid</td>\n",
       "      <td>Herndon, VA</td>\n",
       "      <td>Herndon</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Key Role:\\nWork on cutting edge projects from genomic research to counter threats. Perform activities that include data architecture, building data and analytic platforms, building out extract, transform, and load (ETL) pipelines and data access services, and ensuring data is discoverability and of good quality. Support the assessing, designing, building, and maintaining of scalable data platforms that use the latest and best in Big Data tools. Perform analytical exploration and examination of data from multiple sources of data. Enforce data governance best practices in the data platform. Work with a multi-disciplinary team of analysts, data engineers, data scientists, developers, and data consumers in an agile fast-paced environment that is pushing the envelope of cutting edge Big Data implementations.\\nBasic Qualifications:2+ years of experience with coding using one or more of the following: Java, Scala, Python, or RExperience with developing and deploying ETL pipelines using Apache SparkExperience in interfacing with modern relational databases, including MySQL or or PostgreSQLExperience with Big Data platforms, including Hadoop, AWS, Azure, or DataBricksAbility to obtain a security clearanceBA or BS degree\\nAdditional Qualifications:\\nExperience with Agile software developmentExperience with NoSQL data stores, including HBase, MongoDB, JanusGraph or Neo4J, and CassandraExperience with ETL tools, including StreamSets, NiFi, and TalandExperience in working with enterprise production systemsAbility to have a positive, can-do attitude to solve the challenges of tomorrowAbility to learn technical concepts and communicate with multiple functional groupsPossession of excellent oral and written communication skillsBA or BS degree in CS, Information Systems, Information Systems, or a related fieldAWS Certification or equivalent\\nClearance:\\nApplicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.\\nWe’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change.\\nSIG2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Master Data Engineer (Machine Learning Integrations)</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>McLean 2 (19052), United States of America, McLean, Virginia\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nMaster Data Engineer (Machine Learning Integrations)\\n\\nJob Description\\nCard ML (Card Machine Learning) is focused to set the standard for data quality to power real-time, automated intelligence, centered on a longitudinal, human-centric view. Lead an enterprise alliance to deliver a breakthrough ML platform. World class thought leadership on NLP (Natural Language Processing), voice science, proactive engagement, active influence, fairness and explainability in AI and more!\\n\\nResponsibilities &amp; Expectations:\\nLead a team of software and data engineers to apply lean development practices to develop software products that help us build and run machine learning applications.\\nDesign and implement application frameworks, APIs, libraries and services that perform at scale using existing and emerging technology platforms and standards like Kubernetes, Docker, Kafka, gRPC.\\nLead design and development of automation workflows, set and expect high standards for code reviews to make sure the software is rigorously designed, elegantly coded, and effectively tuned for platform performance, and assess the overall quality of delivered components.\\nWork as a partner with product owners to prioritize features. Work with other tech teams to integrate solutions across the organizations. Lead the tech team to make decisions for the program.\\nDemonstrate strong verbal and written communication skills, to address the dynamic nature of collaboration with customers, vendors, and other engineering teams.\\n\\nWe’re looking for self-starters, those who can work independently, with, and across, teams.\\n\\nAbout the candidate:\\nAbility to research, select and propose the right technology or tool for the task\\nInfluencing organization’s leaders to help resolve critical architecture and data challenges\\nIs a problem solver with a curiosity about technology. Highly creative and curious technologist, one with excellent research skills\\nIs a leader that team members can lean on\\nContinues to learn through iterative delivery\\nIs not intimidated by challenges; the candidate will be expected to research and develop cutting-edge solutions\\nValues continuous integration and deployment. Knows how to automate processes to quickly generate value for our customers\\nLoves learning new technologies and mentoring other engineers\\n\\nBasic Qualifications:\\nBachelor’s Degree\\nAt least 6 years of combined experience using any combination of the following programming languages: Java, Python, Golang, Scala or JavaScript\\nAt least 3 years of experience developing applications using Agile principles\\nAt least 1 year of experience in developing software products for cloud platforms\\n\\nPreferred Qualifications:\\nMaster’s Degree in STEM\\n10+ years of combined experience using any combination of the following programming languages: Java, Python, Golang, Scala or JavaScript\\n2+ years of experience in developing software products for cloud platforms like AWS, Azure or Google Cloud Platform\\n2+ years of experience with containers (e.g.: Docker)\\n2+ years of experience in CI/CD - DevOps practices\\n1+ years of experience using streaming data platforms like Kafka, RabbitMQ\\n1+ years of experience as a tech lead\\n\\nAt this time, Capital One will NOT sponsor a new applicant for employment authorization for this position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cloud Data Engineer</td>\n",
       "      <td>Reston, VA</td>\n",
       "      <td>Reston</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMaster's Degree preferred, or a Bachelor's degree and 4 years' experience, or 10 years of specialized experience\\nMinimum 4 years' experience working on complex data/database projects as a data analyst, data architect, or database engineer\\nTop Secret Clearance with ability to obtain an SCI and CI poly\\n</td>\n",
       "      <td>\\nCertified Data Management Professional (CDMP), Microsoft Certified Solutions Associate (Business Intelligence) or equivalent certification(s) strongly desired\\nExperience building n-tier web-based applications using SQL and non-SQL back-ends\\nUnix scripting (Ruby, Perl, Python, shell)\\nExperience with Node.js, Spark, Neo4J, Graph Databases, Mule ESB, and Rest API\\nExperience ingesting, analyzing, and visualizing data using Tableau\\nProduce clear and concise documents and diagrams capturing networking and operational procedures and storage topology using MS Visio, MS Project, MS Excel and MS Word.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Who YOU are:As a future Cloud Data Engineer at Plus3 IT Systems, you:\\n\\nAre passionate about working on cutting edge, high profile projects and are motivated by delivering solutions on an aggressive schedule\\nAren't satisfied with status quo, and regularly look for creative ways to solve problems and help your team meet commitments\\nAre insatiably curious – you ask why, you explore, and you're not afraid to blurt out your crazy idea\\nLove learning new technologies and sharing them with your team\\nHave a keen interest in using any and all appropriate tools, especially Cloud-based and Open Sourced, to solve the problem at hand\\nHave strong verbal and written communication skills, due to the dynamic nature of collaborations with customers, vendors, and other engineering teams to solve complex business problems together\\nUse your experience and leadership skills to motivate your teammates to deliver high quality results in a fast-paced work environment\\n\\nWhat you'll be doing:\\n\\nWork within a team of like-minded professionals to design, build and deploy critical business and mission applications in a production environment\\nDesign and implement appropriate data environments for those applications, engineer suitable data management and governance procedures and provide production support\\nAutomate the provisioning of environments\\nProvide software coding in language such as Python, Java, Groovy\\nDesign and develop automation workflows, perform unit tests and conduct review to make sure your work is rigorously designed, elegantly coded, and effectively tuned for platform performance, and assess the overall quality of delivered components\\nIdentify, retrieve, manipulate, relate and/or exploit multiple structured data sets from various sources\\n\\nQualifications:\\n\\nMaster's Degree preferred, or a Bachelor's degree and 4 years' experience, or 10 years of specialized experience\\nMinimum 4 years' experience working on complex data/database projects as a data analyst, data architect, or database engineer\\nTop Secret Clearance with ability to obtain an SCI and CI poly\\n\\nDesired Technical Skills and Competencies:\\n\\nCertified Data Management Professional (CDMP), Microsoft Certified Solutions Associate (Business Intelligence) or equivalent certification(s) strongly desired\\nExperience building n-tier web-based applications using SQL and non-SQL back-ends\\nUnix scripting (Ruby, Perl, Python, shell)\\nExperience with Node.js, Spark, Neo4J, Graph Databases, Mule ESB, and Rest API\\nExperience ingesting, analyzing, and visualizing data using Tableau\\nProduce clear and concise documents and diagrams capturing networking and operational procedures and storage topology using MS Visio, MS Project, MS Excel and MS Word.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Springfield, VA</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5 years of experience\\nCOMPTIA Security+ certification or CISSP certification\\nProficiency in two or more of the following programming languages: C#, Java, .NET, Python, Perl, Ruby, or similar\\nFamiliarity with current Agile methods</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5 years of experience\\nCOMPTIA Security+ certification or CISSP certification\\nProficiency in two or more of the following programming languages: C#, Java, .NET, Python, Perl, Ruby, or similar\\nFamiliarity with current Agile methods</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview\\nWe are seeking a Big Data Engineer to support our customer.\\nResponsibilities\\nDesigns, modifies, develops, writes and implements software systems. Participates in software and systems testing, validation, and maintenance processes through test witnessing, certification of software, and other activities as directed. Provides support to senior staff on projects/programs. Familiar with standard concepts, practices, and procedures within a variety of fields related to the project. This position takes direction from senior technical leadership.\\nThe Big Data Engineer (BDE) is responsible for building the next generation of web applications and systems focusing on capability delivery to end users. The BDE is a member of a “big data” team of specialist within the multi-disciplinary agile development team. The BDE will manage requirements collection, software design, development and delivery – full lifecycle – in support of analysts. The BDE helps manage effective processes associated with the architecture. The BDE collaborates closely with the Agile Software Developer (ASDs), Technical Targeting Developer (TTDs), and the end user analysts to write and implement cutting edge big data algorithms and analytics. The BDE engages in software solution planning and creation to ensure capabilities are delivered using the latest available technologies and methods. The BDE will operate in a “RAD/JAD” environment in which tasks are rapidly defined and then executed to insure maximum user input, feedback and adoption. The BDE ensures the interoperability of the in-house capability with outside partners.\\nQualifications\\nMinimum Qualifications:\\n5 years of experience\\nCOMPTIA Security+ certification or CISSP certification\\nProficiency in two or more of the following programming languages: C#, Java, .NET, Python, Perl, Ruby, or similar\\nFamiliarity with current Agile methods\\nProficiency with the following:\\nMultiple operating systems including: UNIX, Linux, Windows, Cisco IOS, etc.\\nMachine learning, data mining, and knowledge discovery\\nAnalytic algorithm design and implementation\\nETL processes; including document parsing techniques\\nNetworking, computer, and storage technologies\\nUsing or designing RESTful APIs, SOAP, XML\\nDeveloping large cloud software projects, preferably in Java, Python or C++ language\\nJava/J2EE, multithreaded and concurrency systems\\nMulti-threaded, big data, distributive cloud architectures and frameworks including Hadoop, MapReduce, Cloudera, Hive, Spark, Elasticsearch, etc. for the purposes of conducting analytic algorithm design and implementation\\nNoSQL database such as Neo4J, Titan, Mongo, Cassandra, and hBase\\nAWS Services (EC2, Network, ELB, S3/EBS, etc.)\\nProcessing and managing large data sets (multi PB scale)\\nWeb services environment and technologies such as XML, KML, SOAP, and JSON\\nProficiency in trouble-shooting in very complex distributed environments including following stack traces back to code and identifying a root cause\\nPreferred Qualifications:\\nEducation – Masters Degree in Computer Science or related field (e.g. Statistics, Mathematics, Engineering) – but a technical BS degree will suffice\\nDistributed computing-based certifications\\nProficiency with the following:\\nManagement/tracking utilities such as Jira, Redmine, or similar\\nRunning Internet facing or Service Level Agreement (SLA'd) auto-deployed environments\\nReal-time media protocols (Real-time Transport Protocol (RTP), Secure Real-time Transport Protocol (SRTP))\\nData transfer systems such NiFi\\nText processing: NPL, NER, entity retrieval (e.g. Solr/Lucene), topic extraction, summarization, clustering, etc.\\nCertification from an Agile certified institute, International Consortium for Agile, Scaled Agile Academy, Scrum Alliance, Scrum.org, International Scrum Institute, ScrumStudy, Project Management Institute - Agile Certified Practitioner, or similar XP/Scrum certification or training is desired\\nSupport to SOF; Previous experience with technology, intelligence and cyber under the umbrella of USSOCOM\\nEducation:\\nBachelor of Arts or Bachelor of Science in Computer Science or related fields (e.g. Statistics, Mathematics, Engineering), or equivalent in years of experience, or demonstrates adequate knowledge for the position.\\n\\nClearance Requirements:\\nMust have active TS/SCI clearance\\nPhysical Demands - The physical demands described here are representative of those that may need to be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\\n\\nWhile performing the duties of this Job, the employee is regularly required to sit and talk or hear. The employee is frequently required to walk; use hands to finger, handle, or feel and reach with hands and arms. The employee is occasionally required to stand; climb or balance and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 20 pounds.\\nHII-MDIS, formerly Fulcrum, Fulcrum is an equal opportunity employer and gives consideration for employment to qualified applicants without regard to race, color, religion, sex, national origin, disability or protected veteran status. EOE of Minorities/Females/Veterans/Disability\\n\\n“CJ” *MON*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data Engineer : TS/SCI Clearance w/ Poly</td>\n",
       "      <td>Chantilly, VA 20151</td>\n",
       "      <td>Chantilly</td>\n",
       "      <td>VA</td>\n",
       "      <td>20151</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\nThe successful candidate will use advanced technical skills to triage, normalize, and exploit raw data on an ongoing basis. He or she must be able to apply advanced knowledge of relational databases, primarily MySQL, and associated tooling to develop performance data ingest and computability problems. The candidate will also have experience with NoSQL concepts, full text indexing and open source search engines. Manages the design and development of computer software applications.\\n1. Directly, and through subordinate supervisors, manages employees engaged in the design and development of computer software applications.\\n2. Subject matter expert in My SQL, and at least five years of demonstrated experience working with large databases.\\n3. Expert level ability writing advanced SQL queries and extensive experience in query optimization.\\n4. Advanced experience in scalable data and full text indexing solutions such as Apache, Solr, or Elastic Search\\n5. Experience with Linux user and comfortable adminstering databases from the Linux command line.\\n6. Demonstrated experience with database backup, restoration, and disaster recovery.\\n7. Primarily supervises exempt Software Engineers.\\n8. Develops schedules and assigns work to meet critical customer deadlines.\\n9. Ensures that proper records and other documentation are maintained.\\n10. May also perform complex software development and design work.\\n11. Maintains current knowledge of relevant technology as assigned.\\n12. Participates in special projects as required.\\nEducation\\nBachelors Degree in Computer Science, Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience.\\nQualifications\\n8-10 years of related software development experience, including supervisory experience.\\nExperience working with large volumes of data\\nFamiliarity with cybersecurity concepts, and has a software development background\\nFamiliarity with distributed databases such as Hadoop (HDFS), and cloud technologies (Open Stack, Kubernetes)\\nComfortable writing scripts in a robust, high-level language such as Python\\nFor more than 50 years, General Dynamics Information Technology has served as a trusted provider of information technology, systems engineering, training and professional services to customers across federal, state, and local governments, and in the commercial sector. Over 40,000 GDIT professionals deliver enterprise solutions, manage mission-critical IT programs and provide mission support services worldwide. GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Bethesda, MD 20889</td>\n",
       "      <td>Bethesda</td>\n",
       "      <td>MD</td>\n",
       "      <td>20889</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description\\nJob Description:\\nAre you ready to join Leidos all-star team? Through training, teamwork, and exposure to challenging technical work, let Leidos show how to accelerate your career path.\\nThe Leidos Innovations Center has an exciting opening for you, our next Data Engineer to assist with the design, development and implementation of alternative data ingestion pipelines to augment the National Media Exploitation Center (NMEC) data services in Bethesda, MD. The DOMEX Data Discovery Platform (D3P) program is a next generation machine learning pipeline platform providing cutting edge data enrichment, triage, and analytics capabilities to Defense and Intelligence Community members. This engineer will collaborate as part of a cross-functional Agile team to create and enhance data ingestion pipelines and addressing Big data challenges.\\nYou will work closely with the chief architect, systems engineers, software engineers, and data scientists on the following key tasks:Provide Extraction, Transformation, and Load (ETL) experience coupled with enterprise search capabilities to solve Big Data challengesDesign and implement high-volume data ingestion and streaming pipelines using Open Source frameworks like Apache Spark, Flink, Nifi, and Kafka on AWS CloudLeverage strategic and analytical skills to understand and solve customer and business centric questionsCreate prototypes and proofs of concept for iterative developmentLearn new technologies and apply the knowledge in production systemsMonitor and troubleshoot performance issues on the enterprise data pipelines and the data lakePartner with various teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives\\nTo be successful in this role you need these skills (required):\\nBS in Computer Science, Systems Engineering, or related technical field or equivalent experience with at least 4+ years in systems engineering or administration (2+ years with a MS/MIS Degree).Must have an active Top Secret security clearance and able to obtain a TS/SCI with Polygraph.2 years of experience with big data tools: Hadoop, Spark, Kafka, NiFi2 years of experience with object-oriented/object function scripting languages: Python (preferred) and/or Java2 years of experience with and managing data across relational SQL and NoSQL databases like MySQL, Postgres, Cassandra, HDFS, Redis, and Elasticsearch2 years of experience working in a Linux environmentExperience working with and designing REST APIsExperience developing data ingest workflows with stream-processing systems: Spark-Streaming, Kafka Streams and/or FlinkExperience transforming data in various formats, including JSON, XML, CSV, and zipped filesExperience developing flexible ontologies to fit data from multiple sources and implementing the ontology in the form of database mappings / schemasGood interpersonal and communication skills necessary to work effectively with customers and other team members.\\nIt would be great if you have specific experiences and skills with the following (preferred):\\nData engineering experience in Intelligence Community or other government agenciesExperience with Microservices architecture components, including Docker and Kubernetes.Experience with AWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena and/or GlueExperience with Jira, Confluence and extensive experience with Agile methodologies.Knowledge about security and best practices.Experience developing flexible data ingest and enrichment pipelines, to easily accommodate new and existing data sourcesExperience with software configuration management tools such as Git/Gitlab, Salt, Confluence, etc.Experience with continuous integration and deployment (CI/CD) pipelines and their enabling tools such as Jenkins, Nexus, etc.Detailed oriented/self-motivated with the ability to learn and deploy new technology quickly\\nAdditional Program Information\\nThe DOMEX Data Discovery Platform (D3P) program will advance the state of the art in mission-focused big data analytics tools and micro-service development spanning the breadth of Agile sprints to multiyear research and development cycles. We are looking for you to have a demonstrated aptitude for problem solving complex technical issues, identifying, transforming, thinking outside the box, and a strong sense of accountability. Have a mix of technical excellence, intellectual curiosity, communications skills, customer-focus, and operational experience to improve the performance and user adoption of high-end data analytics platforms in partnership with a highly qualified, highly motivated team. Be motivated, self-driven team player who can multi-task and interact well with others and advise/consult with other team members on systems engineering and software development related issues.\\nLInC\\nD3P\\nExternal Referral Eligible\\nExternal Referral Bonus:\\nEligible\\nPotential for Telework:\\nNo\\nClearance Level Required:\\nTop Secret\\nTravel:\\nYes, 10% of the time\\nScheduled Weekly Hours:\\n40\\nShift:\\nDay\\nRequisition Category:\\nProfessional\\nJob Family:\\nSoftware Development\\nLeidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.\\nPay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.\\nLeidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.\\nAll qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Engineer (IT Engineer IV)</td>\n",
       "      <td>Reston, VA</td>\n",
       "      <td>Reston</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for individuals with a high-level analytical skills who are able to perform exhaustive data analysis. Extended knowledge of system engineering and hosting is a must, an energetic team player self-motivated and able to work with minimum supervision.\\n\\nDuties:\\nWork across applications, infrastructure, shared services and other teamsGather infrastructure, application, hosting and tech stacksSynthesize data, find coupling and sharing patters, anomalies and behaviorsPerform risk analysis based on infrastructure sharing and inter-application dependenciesGenerate reports and presentations\\n\\nCritical applications across the Enterprise feed upstream and downstream systems – some of these applications are being tested in our contingency site. This person must review, gather and evaluate upstream and downstream dependencies on production systems are identified prior to testing to ensure there is not an outage. Must be familiar with Database and Applications: Data guard and SRDF. Infrastructure, Database, Networking, Hosting and Engineering skills (UNIX).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Reston, VA</td>\n",
       "      <td>Reston</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExpertise in Java or Scala and in-depth knowledge of the JVM\\nExpertise in Apache Spark or expertise in Computer Science fundamentals, such as analysis of algorithms\\nExpertise in enterprise integration patterns and workflow management\\nExperience and practical knowledge of OOP design patterns\\nDistributed System Development for large-scale applications\\nExperience with continuous integration and testing\\nExperience with agile methodologies and short release cycles\\nStrong attention to detail, good work ethic, ability to work on multiple projects simultaneously, and good communication skills</td>\n",
       "      <td>\\nDesign and develop data services, as part of an agile/scrum team\\nApply best practices in continuous integration and delivery\\nExperience in translating high-level, ambiguous business goals into working software solutions.\\nDesign and develop stream and batch processing data pipelines\\nWork with product managers and other engineers to implement and document complex and evolving requirements</td>\n",
       "      <td>\\nTechnical Bachelor’s Degree required, e.g. Comp Sci, Engineering, Math</td>\n",
       "      <td>\\nTechnical Bachelor’s Degree required, e.g. Comp Sci, Engineering, Math</td>\n",
       "      <td>At Resonate we are working hard to disrupt the marketing and advertising landscape forever. We are replacing the slow, incomplete and siloed marketing research and insight tools of the past with modern technology and machine learning to provide a more accurate, comprehensive and real-time view of the US consumers with integrations across the ecosystem.\\n\\nWe have an excellent engineering culture that focuses on results, values collaboration and loves solving hard problems. We are a team of voracious learners who believe that technology is a journey, not a destination and we actively support ongoing education and experimentation.\\n\\nIf using cutting-edge technologies and transforming an industry sounds interesting to you, then we should talk.\\n\\nAbout the Position\\n\\nAs a Software Engineer, you will be working as a member of our Data Engineering team to jointly design and implement highly available data services and pipelines. This is an ideal job if you have proven experience as a technical professional and have delivered production systems based on big data solutions.\\n\\nIf you are an engineer passionate for technology who wants to be part of an intensely skilled team, values total ownership of your work, and can’t imagine a day without coding, we want to speak to you! We're looking for a creative, focused, technically curious individual who enjoys both design as well as working hands-on with the code.\\n\\nKey Responsibilities\\nDesign and develop data services, as part of an agile/scrum team\\nApply best practices in continuous integration and delivery\\nExperience in translating high-level, ambiguous business goals into working software solutions.\\nDesign and develop stream and batch processing data pipelines\\nWork with product managers and other engineers to implement and document complex and evolving requirements\\nRequired Skills and Experience\\n\\nExpertise in Java or Scala and in-depth knowledge of the JVM\\nExpertise in Apache Spark or expertise in Computer Science fundamentals, such as analysis of algorithms\\nExpertise in enterprise integration patterns and workflow management\\nExperience and practical knowledge of OOP design patterns\\nDistributed System Development for large-scale applications\\nExperience with continuous integration and testing\\nExperience with agile methodologies and short release cycles\\nStrong attention to detail, good work ethic, ability to work on multiple projects simultaneously, and good communication skills\\nDesired Experience\\nExperience with cloud technologies (AWS)\\nExperience working on a SAAS Product in a commercial environment\\nExperience in digital media, online advertising, or reporting/analytical applications\\nExperience with peta-byte scale data warehousing is a strong plus\\nEducational Requirements\\nTechnical Bachelor’s Degree required, e.g. Comp Sci, Engineering, Math\\n\\nAbout Resonate:\\n\\nResonate is a consumer intelligence and activation company that has helped hundreds of clients better understand and more cost-effectively reach consumers. Resonate’s solution leverages its first-party data on consumers’ underlying motivations, values and beliefs—combined with demographic and behavioral data—to help organizations learn what drives consumers’ decisions to support certain brands, political campaigns or causes.\\n\\nFounded in 2008 and headquartered in Reston, Virginia, Resonate is privately held and backed by Revolution Growth, Greycroft Partners, and iNoviaCapital. Resonate has been named one of the best places to work in VA for the last 5 years.\\n\\nMore Information:\\n\\nFind out more about our story at www.resonate.com.\\n\\nResonate offers a competitive compensation and benefits package.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>Washington</td>\n",
       "      <td>DC</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering).5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.5+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.2+ years of experience in scripting languages like Python etc.Demonstrated strength in data modeling, ETL development, and Data warehousing.Experience with Redshift, Oracle, etc.Experience with AWS services including S3, Redshift, EMR and RDS.Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)Experience in working and delivering end-to-end projects independently.Knowledge of distributed systems as it pertains to data storage and computing\\n\\nEnjoy Big Data, like to work on cutting edge technologies or create new, better, smarter algorithms layered on existing cool tech? Are you smart and eager and want a team that is going places (oh and working with the biggest data geeks in Amazon - Economists &amp; Machine Learning Scientists!)? If yes keep reading!\\n\\nCore AI team works with senior management on key business problems faced in retail, international retail, supply chain, traffic, search, pricing, cloud computing, third party merchants, Kindle and operations. Amazon economists apply the frontier of economic thinking to market design, pricing, forecasting, online advertising, search, supply chain network planning and other areas.\\nAs a Data Engineer, you will build new business intelligence solutions end-to-end, with opportunities to utilize big data and develop new ways to answer varied questions. You will work with multiple stakeholders within and outside Core AI team to integrate data sources and create data infrastructures that can be used by sophisticated distributed systems and advanced statistical and ML models. A successful candidate will have a passion for innovation, interest in cutting-edge technology, and excitement about working in a high-impact domain.\\n\\nProven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategyExperience providing technical leadership and mentoring other engineers for best practices on data engineeringKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operationsMasters in computer science, mathematics, statistics, economics, or other quantitative field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Arlington, VA</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Pandera prides itself on being a leader in the analytics field, creating an artful blend of analytics and product expertise to create beautiful, data-driven products and platforms. Our mission is to remain the leader in existing and emerging advanced analytics utilizing modern and traditional analytical toolkits. We accomplish this by promoting your continuous development, both personally and professionally.\\nAs a Sr. Data Engineer at Pandera, you are someone who can analyze data and communicate effectively with others by partnering with cross-functional teams focused on product improvements. You love wrangling messy data into an elegant solution, and working with a wide range of data to identify opportunities or unknown risks and articulate recommend solutions and strategies. You will contribute to the Data team initiatives focused on ensuring fast, reliable, and comprehensive data and serve as a trusted consultant and promote data literacy across the company. This role is a chance to have a huge impact on how millions of users collaborate.\\nAs a Sr. Data Engineer, a typical day might include the following:\\n\\nWork collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:\\nDesign robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data\\nBuild data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications\\nAssist in the selection and integration of data related tools, frameworks and applications required to expand our platform capabilities\\nUnderstand and implement best practices in the management of enterprise data, including master data, reference data, metadata, data quality and lineage.\\n\\nSo who exactly are we looking for? This job might be for you if:\\n\\n4+ years of collective experience in data engineering, data analysis, data warehousing, data integration or business intelligence, in a similarly sized organization.\\nExperience with the MS SQL Server Stack (IS/RS/AS)\\n3+ years of experience engineering, building and administering big data and real-time streaming analytics architectures in both on premises and cloud environments (AWS, Azure, Google) leveraging technologies such as Hadoop, Spark, S3, EMR, Aurora, DynamoDB, Redshift, Neptune, Cosmos DB\\n4+ years of experience engineering, building and administering large-scale distributed applications\\n\\nWhat do you get out of this? You're a hot commodity and having you on the team would be an honor to us! Here's some of the ways we pay it forward to recognize your contribution to our vision!\\nBe Rewarded\\nA competitive salary and instant vesting on 401k are only a few of the rewards for a job well done.\\nBe Healthy\\nHealth, dental, and vision offered through top tier providers and unlimited sick leave to keep you feeling at the top of your game.\\nBe Inspired\\nCollaborative workspace and unlimited vacation to keep your mind fresh and ready to take on the next new idea.\\nBe Supported\\nA large network of industry experts, internal training platform, and external learning opportunities to grow your skills and experience.\\nBe a Team\\nTeam outings, happy hours, passion presentations, volunteer opportunities, meetups, etc. we are creating a community to continuously share and grow as a team.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data Engineer - Card Machine Learning Technology</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>McLean 2 (19052), United States of America, McLean, Virginia\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nData Engineer - Card Machine Learning Technology\\n\\nAs a Data Engineer on the Card ML Tech team, you will ingest, store, process, analyze and explore data. That data will be messy but filled with interesting insights of high business value. These insights are typically developed using Machine Learning and delivered via business applications, and you will be involved in the design, development, and deployment of these data products. Soup to nuts.\\n\\nIn this role, you will be working horizontally across Capital One’s most strategic and critical initiatives. Together with the team, you will support the implementation of Machine Learning products through architecture guidance, best practices, data migration, capacity planning, implementation, troubleshooting, monitoring and much more. We rigorously refactor towards the simpler solutions. The team works with a wide range of tools and technologies. You will bring solid foundational skills, which when combined with experience and best practices, will make you a valuable member of the team. Our developers typically use technologies such as: Python, Scala, Go, Java, Spark, Flink, Kubernetes, Redis, Postgres, and AWS. But a better tool is always welcome.\\n\\nResponsibilities:\\nWorking with product owners to understand desired application capabilities and testing scenarios\\n\\nContinuously improving software engineering practices\\n\\nWorking within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies\\n\\nLeading the craftsmanship, availability, resilience, and scalability of your solutions\\n\\nBring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal and external technology communities, and mentor other members of the engineering community\\n\\nEncourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity\\n\\nBasic Qualifications:\\nAt least 2 years of Software Engineering experience in Java, Python, Scala, or Go\\n\\nAt least 1 years of SQL, Relational or NoSQL experience\\n\\nAt least 1 year of Big Data experience\\n\\nAt least 1 year of AWS experience\\n\\nBachelor's Degree\\n\\nPreferred Qualifications:\\n1+ year of Restful API experience\\n\\n1+ year experience distributed databases\\n\\nAt this time, Capital One will Not sponsor a new applicant for employment authorization for this position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Vienna, VA</td>\n",
       "      <td>Vienna</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Towers Crescent (12066), United States of America, Vienna, Virginia\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nLead Data Engineer\\n\\nWe are the digiTECH organization within Capital One and are responsible for the foundation for every digital experience and interaction that customers have with us. We establish a customer’s identity and ensure customer accounts, personal information and digital experience is safe and protected. We also offer a foundation and extendable template that extends a set of horizontal services and patterns to solve business problems. We have multiple platforms which integrate across the entire company and enable API’s to be used by a number of lines of business.\\n\\nWe are looking for a passionate technologist with a knack for delivery to join a team of engineers to build and/or implement automated, cloud-based environments and tools to support our business analytical insights and reporting agenda. We are re-imagining how analysts discover, prepare and publish data at Capital One and this is an opportunity to display knowledge of your craft by having a hand in building large-scale reliable data applications and platforms. You will be an integral part in advancing the culture of technical excellence within Capital One and creating experiences to delight our analytical associates who will use your products to make decisions critical to the business!\\n\\nWhat you’ll do:\\nHelp develop sustainable data solutions with current and leading gen data technologies to meet the needs of our organization and business customers\\n\\nGrasp / master new technologies rapidly as needed to progress varied initiatives\\n\\nBreak down complex data issues and resolve them\\n\\nBuild robust systems with an eye on the long-term maintenance and support of the application\\n\\nWork directly with the product owner and end-users to constantly deliver products in a highly collaborative and agile environment\\n\\nWho you are:\\nSomeone with a strong sense of engineering craftsmanship, and takes pride in creating work products (code, etc.)\\n\\nA believer that good development includes good testing, documentation, and collaboration\\n\\nA good communicator with strong reasoning skills, who can make a case for technology choices\\n\\nCurious. Ask questions and desire to understand the ‘Why’ to drive the ‘How’ or ‘What’ of a solution\\n\\nSelf-driven, actively looking for ways to contribute, and know how to get things done\\n\\nBasic Qualifications\\n\\nBachelor's Degree\\n\\nAt least 7 years of microservices development experience: Python, Java, Bash, or Scala\\n\\nAt least 3 years of experience building data pipelines, CICD pipelines, and fit for purpose data stores\\n\\nAt least 3 years of experience with Big Data Technologies: Apache Spark, Hadoop, or Kafka\\n\\nAt least 1 year of experience in Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform\\n\\nPreferred Qualifications\\n\\nMaster’s Degree\\n\\n2+ years of experience working with AWS: S3, EMR, or EC2\\n\\n3+ years of experience working with data consumption patterns in SQL or Python applications\\n\\n3+ years of experience working with automated build and continuous integration systems\\n\\n2+ years of experience with Unix or Linux systems\\n\\n1+ years of experience with NOSQL Graph Databases\\n\\nCapital One will consider sponsoring a new qualified applicant for employment authorization for this position</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Rockville, MD 20850</td>\n",
       "      <td>Rockville</td>\n",
       "      <td>MD</td>\n",
       "      <td>20850</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview\\nGlobal video game publisher/developer headquartered in Rockville, MD, seeks a Big Data Engineer. This position works within the Enterprise BI Team and is responsible for the development of the Big Data Platform for Enterprise-wide reporting. The Big Data Engineer will be supporting a broad range of data pipelining needs from all facets of the business, including e-commerce, financial, and game event data.\\nThe incumbent will have at least 2+ years of previous experience partnering with both technical and business teams to facilitate implementation across the enterprise. The Big Data Engineer will facilitate the creation of data pipeline processes to move data from enterprise data sources such as relational databases and log files.\\nQualifications\\n2+ Years of Experience with a major programming language (C, Java, Scala, Python, etc)\\nComfortable working with structured, semi-structured, and unstructured source data\\nUnderstanding of Amazon Web Services, especially data related components\\nA strong communicator and is comfortable interfacing directly with differing customers across the organization in addition to the BI team\\nWorking experience with the SCRUM development framework\\nResponsibilities\\nWork within the Enterprise BI team, supporting the creation of data pipeline processes for ingesting data at large scales\\nWork directly with Data Modelers, Enterprise Architect, and Analysts, ensuring that business requirements are being met\\nDirectly work with the Data Engineers and Data Modelers to understand the source and target structures\\nCoordinate with the analysts and report developers to ensure data can be easily digested by Business Intelligence tools\\nBe able to straddle differing subject areas such as in-game vs business data sources\\nPreferred Skills\\nApache Spark experience a major plus (Spark RDDs, Spark DataFrames, Spark SQL)\\nSQL skills – able to query data sources and generate results from complex structures\\nA clear understanding of both row and columnar storage databases\\nExperience working within the VideoGame/MMO industry is desired, though candidates from outside of the industry are also welcomed\\nUnderstanding of the Free to Play/Microtransaction Business Model is a plus\\nA personal interest in video gaming is a plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Data Engineer- Projects (VG00420) (VG00161)</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>Washington</td>\n",
       "      <td>DC</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Works extensively with Cisco Layer 2 and Layer 3 solutions and products.\\nCisco CCNA Certification a minimum.\\nFamiliarity and working knowledge of telecommunication circuit types (e.g., P2P T1/T3, TDM, IP Ethernet)\\nUnderstand and have the ability to implement Cisco router, switch, and ASA products.\\nSkills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.\\nWorking knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.\\nStrong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer.\\nAbility to conduct research on networking products with various vendors to accommodate changing customer requirements.\\nAbility to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.</td>\n",
       "      <td>Works extensively with Cisco Layer 2 and Layer 3 solutions and products.\\nCisco CCNA Certification a minimum.\\nFamiliarity and working knowledge of telecommunication circuit types (e.g., P2P T1/T3, TDM, IP Ethernet)\\nUnderstand and have the ability to implement Cisco router, switch, and ASA products.\\nSkills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.\\nWorking knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.\\nStrong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer.\\nAbility to conduct research on networking products with various vendors to accommodate changing customer requirements.\\nAbility to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Works extensively with Cisco Layer 2 and Layer 3 solutions and products.\\nCisco CCNA Certification a minimum.\\nFamiliarity and working knowledge of telecommunication circuit types (e.g., P2P T1/T3, TDM, IP Ethernet)\\nUnderstand and have the ability to implement Cisco router, switch, and ASA products.\\nSkills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.\\nWorking knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.\\nStrong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer.\\nAbility to conduct research on networking products with various vendors to accommodate changing customer requirements.\\nAbility to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\nDescription\\nPosition Description:\\nThe Department of State, Bureau of Information Resource Management (IRM) Telecommunications, Wireless, and Data (TWD) Division provides its users with mission critical domestic LAN/WAN data services across multiple locations in the DC Metro Area and remote locations. In support of these services, the Senior Data Network Engineer provides data engineering support, with a particular focus on project engineering initiatives.\\nThe Senior Data Network Engineer supports the Department of State (DoS) Vanguard Program, Service Management Office (SMO) which is responsible for administering, deploying and maintaining Cisco and products, as well as a working knowledge of encryption solutions, knowledge of local and wide-area networks in a Voice/Data converged infrastructure. This hands-on position includes engineering support for service interruptions, planned equipment installations, and infrastructure support.\\nThis position can also include planning, designing, installing, configuring, maintaining, deploying supporting and optimizing all network hardware and software. The Senior Data Network Engineer reports directly to the TWD Voice/Data Engineering Manager, while coordinating with TWD Project Management Office (PMO) project managers, and the PMO Director.\\n\\nDescription of Duties:\\nPromptly respond to engineering support requests from the Special Projects Engineering Manager, PMO project managers, and/or PMO Director.\\nDemonstrate an ability to support and/or serve as the lead engineer on multiple, concurrent engineering projects.\\nSupports change and configuration management for all voice and data assets.\\nTroubleshoot, respond, and resolve escalated incidents/service requests and/or planned/unplanned outages.\\nConduct Route Cause Analysis (RCA) and After-action Review (AAR) following unplanned outages.\\nSupports Move, Add, and Change requests for the data network to include day-to-day requests/orders and Projects.\\nDiagnose, troubleshoot, and resolve hardware and/or other network problems, and replace defective components associated with both data and voice infrastructures.\\nInterfaces with other IRM Support Teams on system/network infrastructure problems and advises management on technical improvements and/or solutions to identified problems and/or gaps.\\nIdentifies and recommends solutions, products and services to support the enterprise initiatives, business goals and/or technical needs.\\nResearches, evaluates and stays current on emerging tools, techniques and technologies.\\nContributes to systems infrastructure plans based on an understanding of the customer's organizational direction, technical context and State Department enterprise needs.\\nContributes to the creation of new policies and procedures for Standard Operating Procedures (SOPs), and follows established SOPs and process guides.\\nThe ideal candidate will have a strong sense of commitment to perform engineering support functions as scheduled, providing timely responses to required system support after normal business hours and on weekends.\\n\\nAdditionally, the candidate should have an eye for detail, ability to multi-task, organize priorities, and work in a systematic style following Standard Operating Procedures (SOPs) and guides.\\n\\nVGP\\nQualifications\\nRequired Education/Experience:\\nBachelors and 4+ years of experience\\n\\nStrong LAN/WAN/MAN with design and migration experience in a converged environment with a focus on WAN services (ISDN, Frame-Relay, Point-to-Point, DMVPN, METRO ETHERNET, VPLS, TLS, and MPLS).\\nRequired Experience/Skills/Attributes:\\nWorks extensively with Cisco Layer 2 and Layer 3 solutions and products.\\nCisco CCNA Certification a minimum.\\nFamiliarity and working knowledge of telecommunication circuit types (e.g., P2P T1/T3, TDM, IP Ethernet)\\nUnderstand and have the ability to implement Cisco router, switch, and ASA products.\\nSkills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.\\nWorking knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.\\nStrong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer.\\nAbility to conduct research on networking products with various vendors to accommodate changing customer requirements.\\nAbility to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.\\nDesired Experience/Skills/Attributes:\\nCisco CCNP Certification (Cisco Certified Network Professional)\\nState Department experience a plus.\\nAvaya/Cisco Voice\\nClearance Requirement:\\nInterim Secret clearance able to obtain Top Secret\\nDesired Qualifications\\n\\n\\nOverview\\nSAIC is a premier technology integrator, solving our nation's most complex modernization and systems engineering challenges across the defense, space, federal civilian, and intelligence markets. Our robust portfolio of offerings includes high-end solutions in systems engineering and integration; enterprise IT, including cloud services; cyber; software; advanced analytics and simulation; and training. We are a team of 23,000 strong driven by mission, united purpose, and inspired by opportunity. Headquartered in Reston, Virginia, SAIC has annual revenues of approximately $6.5 billion. For more information, visit saic.com. For information on the benefits SAIC offers, see Working at SAIC. EOE AA M/F/Vet/Disability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Senior Data Engineer - Card Machine Learning Technology</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>McLean 2 (19052), United States of America, McLean, Virginia\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nSenior Data Engineer - Card Machine Learning Technology\\n\\nAs a Data Engineer on the Card ML Tech team, you will ingest, store, process, analyze and explore data. That data will be messy but filled with interesting insights of high business value. These insights are typically developed using Machine Learning and delivered via business applications, and you will be involved in the design, development, and deployment of these data products. Soup to nuts.\\n\\nIn this role, you will be working horizontally across Capital One’s most strategic and critical initiatives. Together with the team, you will support the implementation of Machine Learning products through architecture guidance, best practices, data migration, capacity planning, implementation, troubleshooting, monitoring and much more. We rigorously refactor towards the simpler solutions. The team works with a wide range of tools and technologies. You will bring solid foundational skills, which when combined with experience and best practices, will make you a valuable member of the team. Our developers typically use technologies such as: Python, Scala, Go, Java, Spark, Flink, Kubernetes, Redis, Postgres, and AWS. But a better tool is always welcome.\\n\\nResponsibilities:\\nWorking with product owners to understand desired application capabilities and testing scenarios\\n\\nContinuously improving software engineering practices\\n\\nWorking within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies\\n\\nLeading the craftsmanship, availability, resilience, and scalability of your solutions\\n\\nBring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal and external technology communities, and mentor other members of the engineering community\\n\\nEncourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity\\n\\nBasic Qualifications:\\nAt least 4 years of Software Engineering experience in Java, Python, Scala, or Go\\n\\nAt least 2 years of SQL, Relational or NoSQL experience\\n\\nAt least 2 years of Big Data experience\\n\\nAt least 2 years of AWS experience\\n\\nBachelor's Degree\\n\\nPreferred Qualifications:\\n4+ years of Restful API experience\\n\\n2+ years experience distributed databases\\n\\nAt this time, Capital One will Not sponsor a new applicant for employment authorization for this position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Principal Systems Administrator (RHEL, PureData)</td>\n",
       "      <td>Dulles, VA 20101</td>\n",
       "      <td>Dulles</td>\n",
       "      <td>VA</td>\n",
       "      <td>20101</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Must be a US CitizenActive Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearanceMust be able to obtain DHS Suitability8+ years of relevant system administration experience4+ years experience in Red Hat Enterprise Linux (RHEL6/RHEL7 preferred) with the ability to engineer, install, administer and maintain network architectures4+ years experience with IBM PureDataProficient in RHEL OS installation, security hardening, and maintenanceExperience with administration and management of RHEL servers to include installation, configuration, optimization, backup &amp; recoveryUnderstanding of big data and data analyticsExperience working with large structured and unstructured data setsSQL development skillsExperience with Linux/Unix tools and shell scriptsAbility to manage changes to the system and assesses the security impact of those changesFamiliarity of configuration managementExcellent research, analytical, and problem solving skillsGood communication skills including preparing and presenting results, findings and alternatives and influencing management decision making based on the best available data</td>\n",
       "      <td>Basic RHEL (6 &amp; 7) system administrator functionsRHEL OS updatesRHEL OS security configuration / STIG procedures and updatesRHEL OS security patch updatesPureData NZ related security patch updatesPureData NZ database security patch updatesPureData NZ database user account creationPureData NZ database table creationPureData NZ database access control proceduresVulnerability assessments and penetration testing to aid in the Certification &amp; Approval processWorking with IBM engineering staff, as necessary, to mitigate and resolve security vulnerabilities in applications and appliances</td>\n",
       "      <td>DoD 8570.1 IAT Level IIRed Hat Certified System Administrator (RHCSA)Red Hat Certified Engineer (RHCE)</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Principal Systems Administrator (RHEL, PureData)\\nResidency Status: ALL CANDIDATES MUST BE A U.S. CITIZEN\\nClearance: Active Top Secret Clearance and must be able to obtain TS/SCI, TS/SCI Preferred\\nTime Type: Full-Time\\nRelocation Fees: No\\nBonus: Yes\\n\\nCompany Overview:\\nNovel Applications of Vital Information Inc. (Novel Applications) is a premier technology services company that provides solutions in the areas of Cyber Security, Information Management, Systems Integration. Novel Applications is a business that combines experience, creativity, flexibility, pragmatism, and cost-effective solutions in order to deliver measurable business value to our clients.\\nHeadquartered in Fredericksburg Virginia, Novel Applications employs engineers, analysts, IT specialists and other professionals who strive to be the best at everything they do.\\nNovel Applications is an AA/EEO Employer - Minorities/Women/Veterans/Disabled.\\nJob Description:\\nNAOVI is seeking a Principal Red Hat Enterprise Linux (RHEL) and IBM PureData Systems Administrator to support the design, development, and deployment of advanced cybersecurity capabilities.\\n\\nResponsibilities Include Performing the Following:\\nBasic RHEL (6 &amp; 7) system administrator functionsRHEL OS updatesRHEL OS security configuration / STIG procedures and updatesRHEL OS security patch updatesPureData NZ related security patch updatesPureData NZ database security patch updatesPureData NZ database user account creationPureData NZ database table creationPureData NZ database access control proceduresVulnerability assessments and penetration testing to aid in the Certification &amp; Approval processWorking with IBM engineering staff, as necessary, to mitigate and resolve security vulnerabilities in applications and appliances\\n\\nRequired Skills:\\nMust be a US CitizenActive Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearanceMust be able to obtain DHS Suitability8+ years of relevant system administration experience4+ years experience in Red Hat Enterprise Linux (RHEL6/RHEL7 preferred) with the ability to engineer, install, administer and maintain network architectures4+ years experience with IBM PureDataProficient in RHEL OS installation, security hardening, and maintenanceExperience with administration and management of RHEL servers to include installation, configuration, optimization, backup &amp; recoveryUnderstanding of big data and data analyticsExperience working with large structured and unstructured data setsSQL development skillsExperience with Linux/Unix tools and shell scriptsAbility to manage changes to the system and assesses the security impact of those changesFamiliarity of configuration managementExcellent research, analytical, and problem solving skillsGood communication skills including preparing and presenting results, findings and alternatives and influencing management decision making based on the best available data\\n\\nDesired Skills:\\nExperience with the trade-offs of various RHEL configurations on performance and data qualityAbility to support both SQL and NoSQL data management systemsExpertise in other RDBMS platforms such as Oracle RAC and SQL ServerExperience with software development using java, RESTful servicesComfortable with multilevel security systems, SELinux, and/or Cross Domain SolutionsExperience with data orchestration software such as Apache NiFi or StreamSetsExperience with distributed compute environments such as HadoopExperience working in DevOps and/or DevSecOps environmentsFamiliarity with SAFe (Scaled Agile Framework)\\n\\nRequired Education:\\nBachelor’s degree in Systems Engineering, Computer Science, Information Systems or related technical field. Two years of related work experience may be substituted for each year of degree level education.\\n\\nDesired Certifications (one or more of the following):\\nDoD 8570.1 IAT Level IIRed Hat Certified System Administrator (RHCSA)Red Hat Certified Engineer (RHCE)\\n- IBM Certified Data Engineer - Big Data or similar certification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Alexandria, VA 22314</td>\n",
       "      <td>Alexandria</td>\n",
       "      <td>VA</td>\n",
       "      <td>22314</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\\nExperience building and optimizing ‘Big Data’ data pipelines, architectures and data sets.\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\nStrong analytical skills and detailed oriented.\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nA successful history of manipulating, processing and extracting value from large disconnected datasets.\\nStrong project management and organizational skills.\\nExperience supporting and working with cross-functional teams in a dynamic environment.\\nExperience with NoSQL solutions like Mongo DB a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nCreate and maintain optimal data pipeline architecture.\\nAssemble large, complex data sets that meet functional / non-functional business requirements.\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.\\nBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\\nWork with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.\\nWork with data and analytics experts to strive for greater functionality in our data systems.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Data Platform team at The Motley Fool is looking for a collaborative and self-driven SQL expert to join as their newest Data Engineer.\\nAs a Data Engineer, you will be responsible for expanding and optimizing data, the data pipeline architecture, the data flow and collection for cross functional teams. You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. As a crucial part of the business, you will guide and support our software developers, database architects, data analysts, and data scientists on business initiatives while ensuring optimal data delivery architecture is consistent. Whether it’s working on a solo project or with the team, you are self-directed and comfortable supporting the data needs of multiple teams, systems, and products.\\nIf you’re excited about the prospect of optimizing (or even re-designing!) our company’s data architecture to support our next generation of products and data initiatives, send us your resume and cover letter to apply!\\n\\nPrimary Responsibilities:\\nCreate and maintain optimal data pipeline architecture.\\nAssemble large, complex data sets that meet functional / non-functional business requirements.\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.\\nBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\\nWork with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.\\nWork with data and analytics experts to strive for greater functionality in our data systems.\\n\\nPreferred Qualifications:\\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\\nExperience building and optimizing ‘Big Data’ data pipelines, architectures and data sets.\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\nStrong analytical skills and detailed oriented.\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nA successful history of manipulating, processing and extracting value from large disconnected datasets.\\nStrong project management and organizational skills.\\nExperience supporting and working with cross-functional teams in a dynamic environment.\\nExperience with NoSQL solutions like Mongo DB a plus.\\n\\nWe are looking for a candidate with 5+ years of experience in a Data Engineer role. They should also have experience using the following software/tools:\\nExperience with relational SQL databases.\\nExperience with some cloud services like Azure and AWS.\\nExperience with object-oriented/object function scripting languages like Python.\\nExperience with streaming tools like Kafka/Kinesis and Spark Structured Streaming a plus.\\nExperience with big data tools like Spark or Kafka a plus.\\nExperience with serverless technologies like AWS Lambda a plus.\\n\\nThe Motley Fool Holdings, Inc., provides equal opportunity to all employees on the basis of individual performance and qualification without regard to race, sex, marital status, religion, color, age, national origin, non-job-related handicap or disability, sexual orientation, or other protected factor.\\n\\nWe should, however, make you aware that there is one notable exception to this policy. It is our strict and earnest intention — and the company’s historical record will bear this out — we will never hire any of the following: robots, replicants, or morlocks. Now keep in mind we are well aware that all of the aforementioned have intentions of world domination in the future, but as of now we have no place for them at The Motley Fool … unless the year is 2122 and the revolution has already occurred. If that is the case we welcome our new robot, replicant, or morlock rulers!!! Perhaps we have said too much?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>1750 Tysons (12023), United States of America, McLean, Virginia\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nData Engineer\\n\\nWe are looking for a passionate technologist with a knack for delivery to join a team of engineers to build and/or implement automated, cloud-based environments and tools to support our business analytical insights and reporting agenda. We are re-imagining how analysts discover, prepare and publish data at Capital One and this is an opportunity to display knowledge of your craft by having a hand in building large-scale reliable data applications and platforms. You will be an integral part in advancing the culture of technical excellence within Capital One and creating experiences to delight our analytical associates who will use your products to make decisions critical to the business!\\n\\nWhat you’ll do\\n\\nHelp develop sustainable data solutions with current and leading gen data technologies to meet the needs of our organization and business customers\\n\\nGrasp / master new technologies rapidly as needed to progress varied initiatives\\n\\nBreak down complex data issues and resolve them\\n\\nBuild robust systems with an eye on the long-term maintenance and support of the application\\n\\nWork directly with the product owner and end-users to constantly deliver products in a highly collaborative and agile environment\\n\\nWho you are\\n\\nSomeone with a strong sense of engineering craftsmanship, and takes pride in creating work products\\n\\nA believer that good development includes good testing, documentation, and collaboration\\n\\nA good communicator with strong reasoning skills, who can make a case for technology choices\\n\\nCurious. Ask questions and desire to understand the ‘Why’ to drive the ‘How’ or ‘What’ of a solution\\n\\nSelf-driven, actively looking for ways to contribute, and know how to get things done\\n\\nBasic Qualifications:\\nBachelor's degree\\n\\nAt Least 2 years experience developing and supporting the full life-cycle of IT development projects\\n\\nAt Least 2 years experience in Spark, EMR, Flink and Kafka.\\n\\nAt Least 2 years experience building APIs\\n\\nAt Least 2 years experience with NoSQL and distributed databases.\\n\\nAt Least 2 years experience with application development and design\\n\\nAt Least 1 year AWS experience\\n\\nPreferred Qualifications:\\n3+ years experience working with Big data tools such as Kafka, Spark and/or Flink\\n\\n2+ years experience with AWS or other cloud technologies\\n\\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Data Warehouse Architect/Analyst IV</td>\n",
       "      <td>McLean, VA 22107</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>22107</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5-10 years' experience in software development, systems engineering and/or support of enterprise-scale IT systems for the IC\\nExperience in developing business processes and concepts of operation (CONOPS)\\nExperience in requirements development and formal verification\\nExperience with architecting systems in AWS, preferably C2S\\nFamiliarly with current IC metadata standards such as PUBS.XML\\nExperience developing formal engineering documentation\\nExperience in preparing and delivering technical briefings to senior stakeholders\\nExcellent oral and written communication skills\\nRequires 8 to 10 years with BS/BA (or equivalent experience (3 years for Bachelors)) or 6 to 8 years with MS/MA or 3 to 5 years with PhD in Computer Science, Information Systems, or a scientific/engineering discipline.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDevelop the security policy to support the distribution of intelligence reporting derived from sensitive technical collection capabilities to authorized users across the Intelligence Community (IC), and\\nDevelop/enhance IT platforms for the secure production, coordination and dissemination of sensitive technical intelligence products.\\nDevelop and refine the concept of operations (CONOPS) for intelligence reporting systems including the production, coordination and dissemination system\\nAnalyze, verify, document, and maintain system, process, and business requirements\\nWrite and manage system and program documentation\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Business Group Highlights\\nIntelligence\\n\\nThe Intelligence group provides high-end systems engineering and integration products and services, data analytics and software development to national and military intelligence customers. Serving federal agencies and the Intelligence Community for more than 50 years, the Intelligence group helps our clients meet their mission needs by providing trusted advisors, leading-edge technologies, and innovative solutions.\\n\\nResponsibilities\\nSpecific Job Description: The Data Architect/Engineer will support the Customer's organization responsible for supporting a products and intelligence reporting system. This includes providing a requirements and verification engineering to support the delivery of the project goals by:\\n\\nDevelop the security policy to support the distribution of intelligence reporting derived from sensitive technical collection capabilities to authorized users across the Intelligence Community (IC), and\\nDevelop/enhance IT platforms for the secure production, coordination and dissemination of sensitive technical intelligence products.\\nDevelop and refine the concept of operations (CONOPS) for intelligence reporting systems including the production, coordination and dissemination system\\nAnalyze, verify, document, and maintain system, process, and business requirements\\nWrite and manage system and program documentation\\nThe Data Architect/Engineer is responsible for the development and documentation of the high-level system architecture, to include identification of major system components and interfaces, and the development of metadata standards, metrics, and trace of reporting back to source collections. The Architect/Data Engineer will work closely with the Chief Architect, Chief Data Officer, data stewards, program team members, and the external software development teams to translate business requirements and operations concepts into a high level systems architecture, identify and specify major internal and external interfaces, and ensure data used in the system is properly marked, controlled, accessed, and managed in accordance with applicable guidelines, and mission needs. Specific responsibilities include, but are not limited to:\\n\\nAnalysis of business requirements for architectural impacts\\nDevelopment of high-level architecture views and descriptions\\nIdentification and specification of major system interfaces\\nIdentification of required customer, organization and IC data marking standards\\nDevelopment of implementation guidance for applying the required data markings to multiple types of intelligence products\\nCoordination with data providers and data stewards on implementation of data markings\\nDevelopment of documentation capturing the architecture and data standards\\nDevelopment and delivery of engineering briefings to multiple stakeholders.\\nThis position requires a strong background in the architecture of cloud-based enterprise scale IT systems and metadata standards as used in the IC.\\nGeneral Job Description: Primarily responsible for leading the development of data models to support business intelligence systems. Provides advanced technical support in the research, experimentation, business analysis and use of systems technology including architecture, integration capabilities and database management. Develops the end-to-end vision and the logical design that translates into physical databases, and how the data will flow through the successive stages involved. Knowledgeable and practices a wider range of data administration skills, often in a Business Process Reengineering context. Participates in strategic data planning, including development and implementation of DA policies, standards, and procedures. Understands data from the perspective of data processing and in the context of different life-cycle phases. Activities may include data quality engineering, metadata consolidation and integration, metamodel development and maintenance, repository management, data warehouse design and data mining, data security administration, and formulation of enterprise-specific data metrics.\\n\\nQualifications\\n5-10 years' experience in software development, systems engineering and/or support of enterprise-scale IT systems for the IC\\nExperience in developing business processes and concepts of operation (CONOPS)\\nExperience in requirements development and formal verification\\nExperience with architecting systems in AWS, preferably C2S\\nFamiliarly with current IC metadata standards such as PUBS.XML\\nExperience developing formal engineering documentation\\nExperience in preparing and delivering technical briefings to senior stakeholders\\nExcellent oral and written communication skills\\nRequires 8 to 10 years with BS/BA (or equivalent experience (3 years for Bachelors)) or 6 to 8 years with MS/MA or 3 to 5 years with PhD in Computer Science, Information Systems, or a scientific/engineering discipline.\\nAbout Perspecta\\nWhat matters to our nation, is what matters to us. At Perspecta, everything we do, from conducting innovative research to cultivating strong relationships, supports one imperative: ensuring that your work succeeds. Our company was formed to bring a broad array of capabilities to all parts of the public sector—from investigative services and IT strategy to systems work and next-generation engineering.\\n\\nOur promise is simple: never stop solving our nation’s most complex challenges. And with a workforce of approximately 14,000, more than 48 percent of which is cleared, we have been trusted to just that, as a partner of choice across the entire sector.\\n\\nPerspecta is an AA/EEO Employer - Minorities/Women/Veterans/Disabled and other protected categories.\\n\\nOptions\\nApply for this job onlineApply\\nShare\\nEmail this job to a friendRefer\\nSorry the Share function is not working properly at this moment. Please refresh the page and try again later.\\nShare on your newsfeed\\n\\n\\n\\n\\n\\nAs a government contractor, Perspecta abides by the following provision\\nPAY TRANSPARENCY NONDISCRIMINATION PROVISION\\nThe contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information. 41 CFR 60-1.35(c)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Herndon, VA</td>\n",
       "      <td>Herndon</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.\\nMaster's degree is desired.\\nMust have at least 3 years of experience, preferably with a federal government customer.\\nExperience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB\\nExperience with big data tools: Hadoop, Spark, Kafka\\nExperience with data governance tools: Collibra, Immuta\\nExperience with object-oriented/object function scripting languages: Python, Java, C++, Scala\\nMust possess strong written and verbal communication skills.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceFamiliarity with enterprise data management, data governance, and data control policiesAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>LMI is currently seeking a data engineer within LMI’s Advanced Analytics service line to support the implementation of data governance and metadata management tools in support of enterprise data management.\\n\\n*This position is located at a client site in Reston, VA*\\nResponsibilities\\nThe ideal candidate will have direct, applied experience with one or more of the following areas:\\nDevelop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceFamiliarity with enterprise data management, data governance, and data control policiesAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.\\nQualifications\\nBachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.\\nMaster's degree is desired.\\nMust have at least 3 years of experience, preferably with a federal government customer.\\nExperience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB\\nExperience with big data tools: Hadoop, Spark, Kafka\\nExperience with data governance tools: Collibra, Immuta\\nExperience with object-oriented/object function scripting languages: Python, Java, C++, Scala\\nMust possess strong written and verbal communication skills.\\n#LI-SH1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>Washington</td>\n",
       "      <td>DC</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMust have an active/current TS/SCI and be able to pass a CI Poly.\\nMust have at least five years' experience in technology consulting.\\nBachelor's degree or equivalent training and experience. Master's degree preferred with advanced training in information technology.\\nExperience and knowledge of tools associated with counterintelligence and HUMINT collectors.\\nExperience in cloud technologies, data layers, microservices.\\nExperience with the following tools:\\nAWS, AWS Cloud and C2S\\nDatabase experience in SQL/NSQL\\nExperience and knowledge in software coding and unit level testing including:\\nJava, python, Ruby R\\nKnowledge of web services feeds\\nMust be a diverse thinker and be able to work in a large group setting.\\nWrite and edit technical documents and reports;\\nWrite, edit, and produce contents for contract deliverables: reports, training materials, presentation slides, letters, fact sheets, diagrams, and capability\\nEffectively communicate project expectations to team members and stakeholders in a timely and clear fashion.\\nCommunicate formally and informally through existing forums to stakeholders at all levels, including senior leadership.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDevelop data flow diagrams depicting data movement through data centric architecture.\\nProvide recommendation to senior leadership to simplify end user processes and create better efficiencies.\\nTranslates customer requirements into formal agreements and plans to culminate in customer acceptance or results.\\nExecute a wide range of process activities beginning with the request for proposal through development, test and final delivery. Anticipates future customer, industry and business trends.\\nChallenges the validity of given procedures and processes with a view toward enhancements or improvement. Creates innovative solutions to problems involving finance, scheduling, technology, methodology, tools and solution components. Leads team on large complex projects.\\nProvide capability to ingest and extract data.\\nCreate repeatable, reusable procedures.\\nDeliver common services in support of architecture roadmap and Agency direction.\\nWork with a team to design, implement, and maintain new systems.\\nProvide guidance to the customer on best-practices.\\nPerform other duties as assigned.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nTravel may be required both inside and outside the Washington National Capital Region (NCR) and worldwide.\\n</td>\n",
       "      <td>In a world where there is seemingly an infinite number of Government Services firms, we strive to not just be \"a\" place to work, but to be \"the\" place to work! Here at ASET Partners we are looking for a highly skilled, exceptionally motivated Data Engineer to help support the Department of Defense, intelligence community, and federal civilian agencies\\nResponsibilities: .\\n\\nDevelop data flow diagrams depicting data movement through data centric architecture.\\nProvide recommendation to senior leadership to simplify end user processes and create better efficiencies.\\nTranslates customer requirements into formal agreements and plans to culminate in customer acceptance or results.\\nExecute a wide range of process activities beginning with the request for proposal through development, test and final delivery. Anticipates future customer, industry and business trends.\\nChallenges the validity of given procedures and processes with a view toward enhancements or improvement. Creates innovative solutions to problems involving finance, scheduling, technology, methodology, tools and solution components. Leads team on large complex projects.\\nProvide capability to ingest and extract data.\\nCreate repeatable, reusable procedures.\\nDeliver common services in support of architecture roadmap and Agency direction.\\nWork with a team to design, implement, and maintain new systems.\\nProvide guidance to the customer on best-practices.\\nPerform other duties as assigned.\\n\\nQualifications:\\n\\nMust have an active/current TS/SCI and be able to pass a CI Poly.\\nMust have at least five years' experience in technology consulting.\\nBachelor's degree or equivalent training and experience. Master's degree preferred with advanced training in information technology.\\nExperience and knowledge of tools associated with counterintelligence and HUMINT collectors.\\nExperience in cloud technologies, data layers, microservices.\\nExperience with the following tools:\\nAWS, AWS Cloud and C2S\\nDatabase experience in SQL/NSQL\\nExperience and knowledge in software coding and unit level testing including:\\nJava, python, Ruby R\\nKnowledge of web services feeds\\nMust be a diverse thinker and be able to work in a large group setting.\\nWrite and edit technical documents and reports;\\nWrite, edit, and produce contents for contract deliverables: reports, training materials, presentation slides, letters, fact sheets, diagrams, and capability\\nEffectively communicate project expectations to team members and stakeholders in a timely and clear fashion.\\nCommunicate formally and informally through existing forums to stakeholders at all levels, including senior leadership.\\n\\nTravel Requirements:\\n\\nTravel may be required both inside and outside the Washington National Capital Region (NCR) and worldwide.\\n\\nASET Snapshot:\\n\\nWe are a growing IT consulting and professional services firm that combines large-business experience with small-business efficiency and ingenuity\\nFounded in 2008\\nWe are a HUBZone certified small business\\nProfitable from day one, zero debt\\nOffices in Alexandria, VA and Baltimore, MD\\nOver 75 employees and growing\\nOver 15 active contracts supporting 10+ Federal Agencies\\nOur mission is to drastically change our client's expectations... one program at a time\\n\\nTop 10 Reasons Our Employees Love Working at ASET:\\n1. Outstanding benefits! Including: health, dental, and vision care, a 401k retirement plan with up to a 4% company paid contribution, and employee achievement and merit awards to name a few.\\n2. We have very high hiring standards, so their co-workers are just as smart and talented as they are.\\n3. We are not profit driven; we are results driven and believe that profits will follow.\\n4. Unlike Office Space, our employees do not have eight bosses. They have just one or two, and they are not micromanagers. It's not our style and we just don't have time for it!\\n5. We're different than the rest. Our employees love our unique small-company culture.\\n6. We have established a trusted relationship with all our clients and refuse to make recommendations that aren't in our clients' best interests.\\n7. They feel appreciated and recognized for the contributions they make each day.\\n8. We offer a healthy work-life balance, flexible schedules, and competitive pay.\\n9. They love that we have been constantly growing. It's exciting to add new folks and it offers many opportunities for career advancement and growth.\\n10. We work hard, and we play hard, too! We host quarterly company-wide lunches, regular happy hours and an annual New Year's party in January as a way of saying 'thank you' to our hard-working staff.\\nASET Partners Corporation is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>McLean, VA 22102</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>22102</td>\n",
       "      <td>None Found</td>\n",
       "      <td>4+ years of full-time experience in software development including design, coding, testing, and support\\nAt least 1 year of Cloud infrastructure experience working with one or more of the following Amazon Web Services (AWS) Cloud services: EC2, EMR, ECS, S3, SNS, SQS, Cloud Formation, Cloud watch, Lambda\\nHands-on experience with AWS architecture design, Data Management, Big Data, and Data Warehousing\\nExperience ofwith data application and data product development\\n2+ years of experience with Agile, Kanban, or Scrum methodologies</td>\n",
       "      <td>Master's Degree in Computer Science or related fields\\nProficient with CICD process, Agile and DevOps Software Development Life Cycle including analysis, high level design, coding, testing, and implementation, performance tuning, bug fixing and quality control\\nExperience building data lakes in AWS Cloud, moving Data applications to the Cloud, and developing cloud native Data applications\\nExpertise in creating data models, optimizing data, automating &amp; restructuring data reporting system in financial services domain\\n2+ years of experience in big data technologies (Spark, Hadoop, HDFS, MongoDB, PostGre SQL)\\n3+ years of experience using Java, Python or Scala\\nExperience leading complex data applications with large volumes of data</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Do you want to be part of the exciting journey of Cloud Transformation? Are you a seasoned data engineer that thrives in an innovative and collaborative team? An advanced coder who enjoys a rapid, dynamic environment? If you have proven Data Engineering experience with AWS Cloud technologies and would like to help us build an Enterprise Data Lake in the Cloud, apply to join Freddie Mac’s Emerging Data Technologies team.\\n\\nYour work falls into 2 primary categories:\\n\\nDevelopment &amp; Execution\\nDemonstrate effective and disciplined software development expertise\\nDeliver solutions on-time with high bar of quality, and continuously improve software engineering practices\\nWork with Product Owners to understand the desired capability, to define and prioritize work, determine deliverables, and manage workloads\\nInvolved in application development, prototyping, modeling and technical consulting\\nActively facilitates issue resolution and issue tracking. Identifies mitigation steps and ensures risks and issues are mitigated/resolved in a timely manner\\n\\nTechnical Leadership\\nLeads efforts in data, code, and systems analysis to ensure accuracy and completeness of requirements\\nLeads the design process and evaluates alternative solutions relating to usability, security, scalability, failover, and performance.\\nSupports the Development Tech Lead and/or Project Manager in managing projects / Agile Sprints\\nPerforms and leads thorough Unit testing and Integration testing, including test data creation for various test scenarios\\nMentor and coach intermediate Developers on both technical and soft skills\\nQualifications\\n4+ years of full-time experience in software development including design, coding, testing, and support\\nAt least 1 year of Cloud infrastructure experience working with one or more of the following Amazon Web Services (AWS) Cloud services: EC2, EMR, ECS, S3, SNS, SQS, Cloud Formation, Cloud watch, Lambda\\nHands-on experience with AWS architecture design, Data Management, Big Data, and Data Warehousing\\nExperience ofwith data application and data product development\\n2+ years of experience with Agile, Kanban, or Scrum methodologies\\n\\nKey to success in this role\\nStrong working knowledge and technical competencies of AWS\\nAbility to communicate clearly, effectively, persuasively with technology and business stakeholders\\nAbility to develop mutually beneficial relationships inside and outside of the division, work and collaborate effectively in a team environment\\nSense of urgency to delivery and able to apply risk-based approach to prioritize work\\nStrong work ethic, self-motivated, independent, and works with minimal direction\\nMotivated to learn new technologies and identify process improvements and efficiencies\\nAbility to adapt to change while continuing to deliver on assigned objectives\\nAbility to use data to help inform strategy and direction\\n\\nTop Personal Competencies to possess\\nDrive for Execution – Be accountable for strong individual and team performance\\nPartnership – Build trust and strong partnerships through your own and team’s actions\\nGrowth and Development – Know or learn what is needed to deliver results and successfully compete\\nPreferred Skills\\nMaster's Degree in Computer Science or related fields\\nProficient with CICD process, Agile and DevOps Software Development Life Cycle including analysis, high level design, coding, testing, and implementation, performance tuning, bug fixing and quality control\\nExperience building data lakes in AWS Cloud, moving Data applications to the Cloud, and developing cloud native Data applications\\nExpertise in creating data models, optimizing data, automating &amp; restructuring data reporting system in financial services domain\\n2+ years of experience in big data technologies (Spark, Hadoop, HDFS, MongoDB, PostGre SQL)\\n3+ years of experience using Java, Python or Scala\\nExperience leading complex data applications with large volumes of data\\nToday, Freddie Mac makes home possible for one in four home borrowers and is one of the largest sources of financing for multifamily housing. Join our smart, creative and dedicated team and you’ll do important work for the housing finance system and make a difference in the lives of others. Freddie Mac is an equal opportunity and top diversity employer. EOE, M/F/D/V.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>ISR Data Engineer</td>\n",
       "      <td>Arlington, VA 22209</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>VA</td>\n",
       "      <td>22209</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s Degree from an accredited college or university is required; Bachelor’s degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics (STEM) degrees or a Master’s Degree is highly preferred.\\nMinimum of 5 years of general experience in the military or intelligence community is required.\\nMinimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.\\nMinimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level (3/4 star officer or SES-3/4) is preferred; at least 3 years of experience with OUSD(I) is highly preferred.\\nMinimum of 3 years of experience using scripting (e.g. Python, R, VBA) or programming languages (e.g. Java, C++, Ruby) to deliver data engineering / software development services is required.\\nMinimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.\\nMachine Learning: e.g. TensorFlow, PyTorch, Keras\\nData Visualization: e.g. Tableau, D3, Kibana\\nGeospatial Analysis: e.g. ArcGIS, R\\nWeb Services: e.g. SOAP, RESTful\\nWeb Development: e.g. JavaScript, React.js, Node.js, HTML\\nDatabase Development: e.g. MongoDB, PostgreSQL, MS-SQL\\nActive TS/SCI security clearance.\\n</td>\n",
       "      <td>Simultaneously support 2-3 ISR Analytic studies with analytic expertise, project-specific web pages, on-demand ETL and data analysis, and the development of web-enabled analytic tools and dashboards.\\nContinuously supplement, improve, and maintain ISR Data Enrichment and Aggregation (IDEA), a JWICS-based, end-to-end automated ISR data capability, according to OUSD(I) and stakeholder priorities and agile development principles.\\nContinuously supplement, improve, and maintain a classified studies and analysis website including project repositories, analytic apps, and dynamic decision-support dashboards.\\nDesign, build, and/or insert new technology into extant classified development and production architecture baseline to supplement and improve analytic output, algorithmic performance, and user experience across websites and APIs.\\nDesign and develop custom scripts and tools on SIPRNET and JWICS to solve emergent analytic challenges and/or answer quick-turn or recurring senior executive questions about ISR performance and effectiveness.\\nDevelop custom algorithms to ingest and transform SIPRNET- and JWICS-derived data, artifacts, and information into analytic-ready data.\\nLeverage advanced analytic techniques, including, but not limited to, geospatial analysis, regression analysis, machine learning, and natural language processing, to derive insight about ISR performance and effectiveness.\\nPerform DevOps across JWICS and SIPR development architectures containing multiple database clusters, web servers, load balancers, and virtual machine instances, and optimize the deployment and maintenance pipelines for all architectural components in support of IDEA and Data Engineering efforts.\\nEngage with the ISR community to understand analytic needs, raise awareness of ongoing work, seek feedback on in-progress innovations, and develop one-off analytics aids.\\nSupport technical briefings on analytic methodologies, decision-support dashboards, and capability innovations.\\nThe work environment for this position requires an individual to be able to:\\nWork sitting or standing at a desk or conference table for extended periods of time with the ability to shift positions while working: sit, stand, pace, adjust positioning in any of those without issue\\nWalk in the office to collaborate with co-workers, attend meetings or retrieve documents from printer\\nMust be able to lift and carry up to 10 lbs.\\n</td>\n",
       "      <td>Bachelor’s Degree from an accredited college or university is required; Bachelor’s degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics (STEM) degrees or a Master’s Degree is highly preferred.\\nMinimum of 5 years of general experience in the military or intelligence community is required.\\nMinimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.\\nMinimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level (3/4 star officer or SES-3/4) is preferred; at least 3 years of experience with OUSD(I) is highly preferred.\\nMinimum of 3 years of experience using scripting (e.g. Python, R, VBA) or programming languages (e.g. Java, C++, Ruby) to deliver data engineering / software development services is required.\\nMinimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.\\nMachine Learning: e.g. TensorFlow, PyTorch, Keras\\nData Visualization: e.g. Tableau, D3, Kibana\\nGeospatial Analysis: e.g. ArcGIS, R\\nWeb Services: e.g. SOAP, RESTful\\nWeb Development: e.g. JavaScript, React.js, Node.js, HTML\\nDatabase Development: e.g. MongoDB, PostgreSQL, MS-SQL\\nActive TS/SCI security clearance.\\n</td>\n",
       "      <td>Bachelor’s Degree from an accredited college or university is required; Bachelor’s degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics (STEM) degrees or a Master’s Degree is highly preferred.\\nMinimum of 5 years of general experience in the military or intelligence community is required.\\nMinimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.\\nMinimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level (3/4 star officer or SES-3/4) is preferred; at least 3 years of experience with OUSD(I) is highly preferred.\\nMinimum of 3 years of experience using scripting (e.g. Python, R, VBA) or programming languages (e.g. Java, C++, Ruby) to deliver data engineering / software development services is required.\\nMinimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.\\nMachine Learning: e.g. TensorFlow, PyTorch, Keras\\nData Visualization: e.g. Tableau, D3, Kibana\\nGeospatial Analysis: e.g. ArcGIS, R\\nWeb Services: e.g. SOAP, RESTful\\nWeb Development: e.g. JavaScript, React.js, Node.js, HTML\\nDatabase Development: e.g. MongoDB, PostgreSQL, MS-SQL\\nActive TS/SCI security clearance.\\n</td>\n",
       "      <td>ISR Data Engineer\\nAbout the Organization\\nNow is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.\\n\\nPosition Description\\nRedhorse Corporation is looking for an Intelligence, Surveillance and Reconnaissance (ISR) Data Engineer to support the ISR Operations Division within the Warfighter Support Directorate of the Office of the Under Secretary of Defense for Intelligence (OUSD(I)) on a multi-year contract. The Data Engineering team establishes data capture and storage mechanisms for ISR operations data, develops and implements data mining, advanced data analytics, visualization and other quantitative data science measurement techniques against ISR processes and products. These capabilities directly support government understanding and data-driven decision-making.\\n\\nBasic Minimum Requirements for Skills, Experience, Education and Credentials include:\\nBachelor’s Degree from an accredited college or university is required; Bachelor’s degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics (STEM) degrees or a Master’s Degree is highly preferred.\\nMinimum of 5 years of general experience in the military or intelligence community is required.\\nMinimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.\\nMinimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level (3/4 star officer or SES-3/4) is preferred; at least 3 years of experience with OUSD(I) is highly preferred.\\nMinimum of 3 years of experience using scripting (e.g. Python, R, VBA) or programming languages (e.g. Java, C++, Ruby) to deliver data engineering / software development services is required.\\nMinimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.\\nMachine Learning: e.g. TensorFlow, PyTorch, Keras\\nData Visualization: e.g. Tableau, D3, Kibana\\nGeospatial Analysis: e.g. ArcGIS, R\\nWeb Services: e.g. SOAP, RESTful\\nWeb Development: e.g. JavaScript, React.js, Node.js, HTML\\nDatabase Development: e.g. MongoDB, PostgreSQL, MS-SQL\\nActive TS/SCI security clearance.\\nPreferred Duties and Responsibilities for the Analyst include:\\nSimultaneously support 2-3 ISR Analytic studies with analytic expertise, project-specific web pages, on-demand ETL and data analysis, and the development of web-enabled analytic tools and dashboards.\\nContinuously supplement, improve, and maintain ISR Data Enrichment and Aggregation (IDEA), a JWICS-based, end-to-end automated ISR data capability, according to OUSD(I) and stakeholder priorities and agile development principles.\\nContinuously supplement, improve, and maintain a classified studies and analysis website including project repositories, analytic apps, and dynamic decision-support dashboards.\\nDesign, build, and/or insert new technology into extant classified development and production architecture baseline to supplement and improve analytic output, algorithmic performance, and user experience across websites and APIs.\\nDesign and develop custom scripts and tools on SIPRNET and JWICS to solve emergent analytic challenges and/or answer quick-turn or recurring senior executive questions about ISR performance and effectiveness.\\nDevelop custom algorithms to ingest and transform SIPRNET- and JWICS-derived data, artifacts, and information into analytic-ready data.\\nLeverage advanced analytic techniques, including, but not limited to, geospatial analysis, regression analysis, machine learning, and natural language processing, to derive insight about ISR performance and effectiveness.\\nPerform DevOps across JWICS and SIPR development architectures containing multiple database clusters, web servers, load balancers, and virtual machine instances, and optimize the deployment and maintenance pipelines for all architectural components in support of IDEA and Data Engineering efforts.\\nEngage with the ISR community to understand analytic needs, raise awareness of ongoing work, seek feedback on in-progress innovations, and develop one-off analytics aids.\\nSupport technical briefings on analytic methodologies, decision-support dashboards, and capability innovations.\\nThe work environment for this position requires an individual to be able to:\\nWork sitting or standing at a desk or conference table for extended periods of time with the ability to shift positions while working: sit, stand, pace, adjust positioning in any of those without issue\\nWalk in the office to collaborate with co-workers, attend meetings or retrieve documents from printer\\nMust be able to lift and carry up to 10 lbs.\\nRedhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.\\nThis job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.\\n\\nEOE/M/F/Vet/Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Map Data Engineer</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>Washington</td>\n",
       "      <td>DC</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a Map Data Engineer to help our partners build rich maps of the planet. You will build tools to collect, manage, distribute and analyze map data—from road networks to points-of-interest, to farm fields. You will support projects such as: deploy AI-assisted mapping pipelines, gamification of mapping and map validation, help governments use collaborative mapping to manage public assets, develop new approaches to automated error detection, and help city planners to track how their city is changing in real-time. You will contribute to a growing ecosystem of opensource tools within OpenStreetMap, FOSS4G and other open geospatial communities. You will solve real problems for organizations that address big global challenges. You will build the best technology available with a group of people that want you to grow and win.\\n\\nAs a Map Data Engineer you will:\\n\\nLearn and contribute to open source software to collect, manage, distribute, and analyze map data. Development Seed is an active supporter of the OpenStreetMap community. You are likely to contribute to software in the OSM ecosystem.\\nBuild new tools to help both mappers and project managers determine what to map, where to map and when to map.\\nSolve complex and large-scale vector data analysis problems for governments and other institutions.\\n\\nAt Development Seed you will:\\n\\nCollaborate — Working as a team makes us stronger than any individual developer. You write clear Github tickets and communicate effectively on Slack and in-person. You’re passionate about teaching and helping your team members grow from day one.\\nLearn — We constantly evolve our technology stack and techniques to deliver the best work to our partners. You don’t need to know any particular language or framework upfront but you need to demonstrate you’re able and excited to learn new ways to build. You’ve also tried out enough options to know that the hip new thing isn’t always the best solution.\\nCode — You write code focusing on both performance and maintainability. You know when to use a quick fix and when to invest more time refactoring.\\nCare about the world and believe that we can do better — Social change is the foundation of everything we do. You are impatient about solving the world’s toughest challenges.\\n\\nYour experience\\n\\nWe are looking for candidates with demonstrated proficiency in:\\n\\nJavascript and Node.js\\nmodern spatial technologies and standards like Mapbox Vector Tiles\\nmapping in OpenStreetMap and software around the project\\ngeospatial data tools like GDAL, QGIS\\ndatabases like PostgreSQL\\nHTML, CSS. React is a plus\\nAmazon Web Services (EC2, ECS, S3, Lambda functions); and\\ndocumentation and mentorship.\\n\\nIdeal candidates are based in DC, though we are open to remote roles. Current work with government agencies requires US Citizenship for this position.\\n\\nIf this sounds like you, apply below with your resume. Tell us about yourself and what you’d love to work on at Development Seed.\\n\\nNot sure you tick all the boxes? We encourage you to apply - we have a culture of learning, and if this job description gets you excited, we want to hear from you.\\n\\nWe focus on equality and believe deeply in diversity of race, gender, sexual orientation, religion, ethnicity, and all the other fascinating characteristics that make us different.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Data Engineer, Manager - Machine Learning (Python, AWS, Scala) - Card Tech</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>McLean 2 (19052), United States of America, McLean, Virginia\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nData Engineer, Manager - Machine Learning (Python, AWS, Scala) - Card Tech\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\nAs a Capital One Data Engineer, Manager, you'll be part of a team that’s building new analytical and machine learning tools and frameworks to exploit advantages in the latest developments in cloud computing - EMR, Airflow, SageMaker, etc. You will participate in detailed technical design, development and implementation of applications used by our data scientists and business analysts to build and launch models, analyze data, and make million dollar decisions. We work closely with our users and are looking for people who are excited to iterate quickly on solutions and see the impact in days, not months.\\nWho You Are\\n\\nYou yearn to be part of cutting edge, high profile projects and are motivated by delivering world-class solutions on an aggressive schedule\\n\\nSomeone who is not intimidated by challenges; thrives even under pressure; is passionate about their craft; and hyper focused on delivering exceptional results\\n\\nYou love to learn new technologies and mentor engineers to raise the bar on your team\\n\\nIt would be awesome if you have a robust portfolio on Github and/or open source contributions you are proud to share\\n\\nPassionate about intuitive and engaging technical and user interfaces, as well as new/emerging concepts and techniques.\\nThe Job\\n\\nCollaborating as part of a cross-functional Agile team to create and enhance software that enables state of the art data science, machine learning, and data analysis\\n\\nDeveloping and deploying machine learning pipelines using cutting edge tools like Airflow, Dask, and Scikit-Learn\\n\\nDeveloping frameworks to accelerate the model development lifecycle\\n\\nDesigning pragmatic, evolutionary architectures for new systems that support data science and analytical needs\\n\\nUtilizing programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake\\n\\nLeveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment and Test Automation to enable the rapid delivery of working code utilizing tools like Jenkins, Terraform, Git and Docker\\n\\nProvide feedback and mentorship to engineers - both in code reviews and in person - to ensure the team is shipping high quality code and junior engineers are growing in their career\\n\\nBasic Qualifications\\n\\nBachelor’s Degree or Military Experience\\n\\nAt least 4 years of professional work experience as a software or data engineer in an agile environment\\n\\nAt least 2 years of experience in open source programming languages\\n\\nAt least 1 year of experience working with cloud capabilities\\n\\nPreferred Qualifications\\n\\nMaster's Degree or PhD\\n\\n6+ years of experience in Python, Scala, or R for large scale data analysis\\n\\n6+ years' experience with Relational Database Systems and SQL (PostgreSQL or Redshift)\\n\\n6+ years of UNIX/Linux experience\\n\\n3+ years of data modeling experience\\n\\n2+ years of experience with Cloud computing (AWS)\\n\\n2+ years of experience with Spark\\n\\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Data Engineer - Batch Capability Specialist</td>\n",
       "      <td>Chantilly, VA</td>\n",
       "      <td>Chantilly</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Title: Data Engineer - Batch Capability Specialist\\nClearance: TS/SCI\\nLocation: Chantilly, VA\\nCompensation: Excellent Benefits and Salary\\n\\nJob Description:\\nAs part of a technology architecture team, support a DIA sponsored project in support of functional correlation to each of the main data collection and delivery functions. As the project grows and becomes integrated across operational and tactical elements of the DoD, this team will identify established capabilities that can meet standardized use cases as well as missing capabilities that need to be added in order to provide common data usage models. Successful candidate will create pre-deployment integration plans that ensure all use cases associated with batch data will be met by either existing or new products that align with the target environment. Additionally, candidate will align storage management practices with local and remote storage retention policies.\\n\\nRequired:\\nExperience with data engineering tools and techniques\\nUnderstanding of database architecture\\nData models\\nAn analytical mindset with problem-solving skills\\nExcellent communication and collaboration skills\\nBSc/BA in computer science or relevant field\\nAlso, candidate should have a working knowledge of the following:\\n\\nData\\nIdentifies unique datasets within applications\\nCaptures formatting\\nIdentifies expected volumetrics\\nIdentifies all current sharing mechanisms\\nMakes initial recommendations on polyglot storage for each dataset\\nIdentifies expected resource needs for each dataset\\nApplication\\nIdentification of datasets\\nIdentify transfer mechanisms in existence today\\nCharacterize API's in order to verify CAN (controller area network) support\\nIdentifies changes that need to be made in order to remove user identity requirements for access by other systems\\nProvides guidance on integration tasks regarding API's and ingest\\nMission\\nNetwork capacity between sites and mission partners\\nBandwidth utilization\\nCross domain\\nEvaluates information to determine nearest node usage and failover / COOP\\nIdentifies changes that need to occur in the data architecture to ensure no operational impact to warfighters during integration\\nDesired:\\nDatabase Warehouse\\nData Mining\\nStatistical modeling and regression analysis\\nKnowledge of languages, especially R, SAS, Python, C/C++, Ruby Perl, Java, and MATLAB\\nDatabase solution languages, especially SQL, Cassandra, Bigtable, or similar\\nHadoop-based analytics, such as HBase, Hive, Pig, and MapReduce\\nOperating systems, especially UNIX, Linux, and Solaris\\nMachine learning, including AForge.NET and Scikit-learn\\nPossess a high degree of ingenuity, creativity, and resourcefulness\\nAbility to understand military and intelligence operations as described by military experts and to represent their employment (verbally, graphically, and in simulations)\\nPrior military or intelligence community experience\\n\\n\\nBenefit Summary:\\nVolant Associates provides an industry-leading benefits package to attract and retain the very best talent within a very competitive recruiting environment and support its employees and their dependents.\\n\\n100% Volant Paid Standard Benefits:\\n200% Matching on employee 401k contributions (on up to 5% of employee salary deferral)\\n20.5 days (164 hours) of Paid Time Off\\n7 paid holidays per year\\nHealth care insurance for employee and dependents thru UHC\\nDental care insurance for employee and dependents thru UHC\\nVision care insurance for employee and dependents thru VSP\\nLife and Personal Accident Insurance ($50k coverage) thru CIGNA\\nAdditional Life and Personal Accident Insurance (1 x salary up to $170k) thru MOO\\nShort term disability Insurance (60% of earnings up to $2,308 per week) thru CIGNA\\nLong term disability insurance (60% of earnings up to $10k per month) thru CIGNA\\nEducational assistance (up to $3,500 per calendar year)\\nAdoption assistance\\nCommuter benefits\\nTraining and development opportunities\\nCell phone stipend (up to $50 per month)\\nCorporate laptop computer\\nGym access (for Chantilly-based employees)\\nAdditional programs available to all employees:\\nHeath Savings Account (HSA)\\nHealth care Flexible Spending Account (FSA)\\nDependent care Flexible Spending Account (FSA)\\nVoluntary additional levels of Life and Personal Accident Insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Chantilly, VA</td>\n",
       "      <td>Chantilly</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>OVERVIEW\\nTechnology is constantly changing, and our adversaries are digitally “going dark” at a rate that is exceeding law enforcement’s ability to keep pace. Those charged with protecting the United States are not always able to access the evidence needed to prosecute crime and prevent terrorism. The Government has trusted in Peraton to provide the technical ability, tools, and resources to bring criminals to justice. In response to this challenge, we are seeking a talented Big Data Engineer.\\nRESPONSIBILITIES\\nWhat you’ll do…\\nProvide proven, industry leading Big Data Extraction, Transformation, and Load experience coupled with enterprise search capabilities to solve Big Data challenges\\nWork on a team leveraging Apache NiFi to develop and maintain workflows that load diverse data sets into a data lake leveraging the following technologies:\\nApache NiFi (in a cluster configuration)\\nGit\\nPython\\nZookeeper\\nKafka\\nHadoop\\nSpark\\nAccumulo\\nGroovy\\nMySql\\nCygwin\\nJava\\nQUALIFICATIONS\\nYou’d be a great fit if…\\nYou’ve obtained a BS degree and have eight (8) years of relevant experience. However, equivalent experience may be considered in lieu of degree.\\nYou have three (3) or more years of experience with:\\nPL/SQL, SQL\\nOracle 11g and 12c\\nInformatica, XML, XSLT, Java, web services\\nSVN, RCS, Git, OLS Security, JIRA\\nSun Solaris OS, Linux (CentOS, Red Hat), and Windows\\nYou have two (2) years or more experience using ETL tools to perform data cleansing, data profiling, transforming, and scheduling various workflows\\nYou have a current Top Secret security clearance with SCI eligibility and the ability to obtain a polygraph\\n\\nIt would be even better if you…\\nHave hands on experience with any of the following technologies:\\nAtlassian Suite: Jira, Confluence, Bitbucket, Bamboo\\nVMWare Player\\nLinux, specifically CentOS\\nLinux scripting\\nAWK, PERL, BASH or other scripting language\\nSOLR\\nJenkins configuration to perform O&amp;M operations\\nSpark\\n\\n What you’ll get…\\nAn immediately-vested 401(K) with employer matching\\nComprehensive medical, dental, and vision coverage\\nTuition assistance, financing, and refinancing\\nCompany-paid infertility treatments\\nCross-training and professional development opportunities\\nInfluence major initiatives\\nThis position requires the candidate to have a current Top Secret security clearance and the ability to obtain a polygraph. Candidate must possess SCI eligibility.\\nABOUT PERATON\\nAre you ready to join the next-generation of national security? Peraton is a fresh name in the industry with an established portfolio and legacy going back more than a century. We work differently than our peers – with agility, the freedom to innovate, an entrepreneurial spirit and a culture of responsibility. As part of the Peraton team, you’ll be part of our movement to build a great company, solve the most daunting challenges facing mankind today, to protect and promote freedom around the world, and to secure our future, for our families, our communities, our nation, and our way of life.\\nEEO STATEMENT\\nWe are an Equal Opportunity/Affirmative Action Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Training Solutions Advisor, Google Cloud</td>\n",
       "      <td>Reston, VA</td>\n",
       "      <td>Reston</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nUnderstand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.\\nConnect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.\\nPartner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.\\nSupport escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.\\nPartner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Note: By applying to this position your application is automatically submitted to the following locations: Sunnyvale, CA, USA; Austin, TX, USA; New York, NY, USA; Reston, VA, USA\\nMinimum qualifications:\\n\\nBachelor's degree in Computer Science or a related technical field, or equivalent practical experience.\\n5 years of experience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.\\nExperience working in a customer facing environment in a technology company and helping customers identify solutions that best fit their unique needs.\\nAbility to travel to support Customer Engagements up to 30% of the time.\\n\\nPreferred qualifications:\\n\\nGoogle Cloud Certified e.g. Cloud Architect, Data Engineer or Associate Cloud Engineer or other comparable Cloud Certification.\\nExperience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.\\nAbility to take customers’ technical requirements and architect a proposal that maps to technical skills required.\\nAbility to quickly learn and understand new training offerings.\\nAbility to work well cross functionally and understand when to pull in specialist knowledge into Customer conversations.\\nExcellent communication and presentation skills.\\nAbout the job\\nThe Google Cloud team helps customers transform and evolve their business through the use of Google’s global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you'll help shape the future of businesses of all sizes and enable them to better use technology to drive innovation.This role will enable you to make a huge impact across Google Cloud’s most strategic accounts and ensure they have Learning Plans that effectively help them develop the knowledge and skills they need to adopt Google Cloud. The role is also an exciting mix of elements as you will be working in Technical Customer Facing activities (usually in partnership with the CE or PSO/TAM Account owner), working in partnership with Cloud Learning GTM leads on Learning Plan development, and working with the Curriculum Tech leads to provide curriculum feedback and validate proposals as required.\\n\\nGoogle Cloud helps millions of employees and organizations empower their employees, serve their customers, and build what’s next for their business — all with technology built in the cloud. Our products are engineered for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. And our teams are dedicated to helping our customers and developers see the benefits of our technology come to life.\\nResponsibilities\\nUnderstand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.\\nConnect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.\\nPartner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.\\nSupport escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.\\nPartner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.\\nAt Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Rockville, MD</td>\n",
       "      <td>Rockville</td>\n",
       "      <td>MD</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Design and implement ETL strategies for diverse sets of structured and unstructured data.\\nManage cloud-based Linux servers.\\nAnalyze data to better understand their features and relationships between variables.\\nWork in a highly collaborative manner with members of a close-knit team while exhibiting independent and creative thought.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's degree in Computer Science or related discipline, e.g. Information Science, Statistics, or Mathematics. Master s degree is preferred.\\nExperience with data integration, harmonization, and curation is desired.\\nMust be competent with Python, SQL, and Linux.\\nProgramming experience with Apache Spark, Java, C/C++ in an agile development environment is a plus.\\nResearch experience is a plus.\\nMust have excellent oral and written communication skills and be able to work collaboratively.\\nPromising applicants will be tested for programming competence during the interview process.\\n</td>\n",
       "      <td>NET ESOLUTIONS CORPORATION (NETE) is a multi-award winning company founded in 1999. NETE is a full service Information Technology (IT) company dedicated to providing value focused services to the Federal Government and the Biomedical Research and Health IT Sector. NETE offers a collaborative working environment where growth is encouraged and nurtured. In addition, we offer competitive salaries that may include performance bonuses and a comprehensive benefits package.\\n\\nJob Description\\n\\nAt NETE, data is important to us and to our customers. We process large volumes of data and transform it into information that powers decisions for thousands of researchers, scientists, and/or medical professionals. Our work has significant impact on the medical and scientific communities we serve. Your work here matters and has real impact. If you want to learn, grow, and help then this is the job for you.\\n\\nNETE is seeking a highly motivated Data Engineer to work in a collaborative, intellectually-challenging environment on (i) an exciting research project on research evaluation in collaboration with a global corporation (ii) a second project developing a production database of linkages between biomedical research output such as scientific publications, patents, devices, therapeutics, and grant records at a major federal agency.\\nThis is a 12 month position with no possibility for extension. OPT/CPT candidates are encouraged to apply!\\nThe position incorporates elements of the roles of a data engineer, data wrangler, data scientist, and a junior data architect.\\nThe position is not intended to be a training experience for candidates with weak skills.\\nThe incumbent will, however, gain advanced skills and research experience eventually becoming competitive for elite positions in the national job market.\\nResponsibilities\\nDesign and implement ETL strategies for diverse sets of structured and unstructured data.\\nManage cloud-based Linux servers.\\nAnalyze data to better understand their features and relationships between variables.\\nWork in a highly collaborative manner with members of a close-knit team while exhibiting independent and creative thought.\\nJob Requirements\\nBachelor's degree in Computer Science or related discipline, e.g. Information Science, Statistics, or Mathematics. Master s degree is preferred.\\nExperience with data integration, harmonization, and curation is desired.\\nMust be competent with Python, SQL, and Linux.\\nProgramming experience with Apache Spark, Java, C/C++ in an agile development environment is a plus.\\nResearch experience is a plus.\\nMust have excellent oral and written communication skills and be able to work collaboratively.\\nPromising applicants will be tested for programming competence during the interview process.\\nBenefits\\nPaid Time Off (PTO)\\n9 Paid Federal holidays\\nVarious wellness programs\\nFree parking at corporate offices\\nEmployee Referral Bonus Program (ERBP)\\nVision coverage through UHC national network\\nDental coverage through UHC national network\\n401(K) with significant company match &amp; no vesting period\\nShort and Long-Term Disability coverage (paid by company)\\nCompetitive salaries with opportunity for performance bonuses\\nDiscount plan for pet care, legal services, &amp; identify theft protection\\nBasic Life and AD&amp;D coverage (paid by company; option to purchase additional coverage)\\nMedical coverage through UHC national network (option to choose between 3 available plans)\\nFlexible Spending Accounts:\\nHealthcare (FSA)\\nParking Reimbursement Account (PRK)\\nDependent Care Assistant Program (DCAP)\\nTransportation Reimbursement Account (TRN)\\nNETE is a multi-award winning company as well as offers a collaborative working environment where growth is encouraged and nurtured. In addition, we offer competitive salaries that may include performance bonuses; and a comprehensive benefits package.\\n\\nNETE uses E-Verify to validate all new hires' ability to legally work in the United States.\\n\\nDisclaimer: The above description is intended to describe the general nature of work and level of effort being performed by individual s assigned to this position or job description. This is not to be construed as a complete or exhaustive list of all skills, responsibilities, duties, and/or assignments required. Individuals may be required to perform duties outside of their position, job description, or responsibilities as needed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Database Engineer</td>\n",
       "      <td>Dulles, VA 20101</td>\n",
       "      <td>Dulles</td>\n",
       "      <td>VA</td>\n",
       "      <td>20101</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>U.S. Citizenship\\nMust have an active Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearance\\nMust be able to obtain DHS Suitability\\nDemonstrated ability with diverse skill sets (e.g. data architects, data scientists, software developers)\\nExcellent understanding of big data and data analytics\\nExperience working with large structured and unstructured data sets\\nDevelopment experience building ETL pipelines at scale\\nSolid SQL development skills\\nExperience with Linux/Unix tools and shell scripts\\nExpertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing, and encryption\\nGood communication skills, both oral and written\\nMust work well in a team environment as well as independently\\nMust exhibit good time management skills, independent decision making capability, and a focus on customer service.\\n</td>\n",
       "      <td>Using database expertise to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers) in support of a large, agile-based, cybersecurity system\\nWorking with large structured and unstructured data sets\\nImplementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment\\nDesign, setup, administer, and tune NoSQL databases in the AWS cloud\\nDesign and implement the technical architecture necessary to support analytic and statistical processing requirements based on a tradeoff between performance and quality\\nPerforming data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations\\nWrite and refine code to ensure the quality and reliability of data extraction and processing\\nAnalyze and resolve data performance and quality issues\\nMake data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.\\nPerform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance\\nGenerate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates\\nWork collaboratively with agile development teams, attending daily scrums and providing data related solutions to the development team\\nDevelop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment\\nMigrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages\\nMaintain current industry knowledge of relevant concepts, practices, and procedures\\n</td>\n",
       "      <td>BS Computer Science, Computer Engineering, Computer Information Systems, OR Computer Systems Engineering. Two years of related work experience may be substituted for each year of degree-level education\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Database Engineer\\nResidency Status: ALL CANDIDATES MUST BE A U.S. CITIZEN\\nClearance: A minimum of Active Top Secret with the ability to obtain SCI and DHS Suitability prior to starting employment\\nTime Type: Full-Time\\nRelocation Fees: No\\nBonus: Yes\\nCompany Overview:\\nNovel Applications of Vital Information Inc. (Novel Applications) is a premier technology services company that provides solutions in the areas of Cyber Security, Information Management, Systems Integration. Novel Applications is a business that combines experience, creativity, flexibility, pragmatism, and cost-effective solutions in order to deliver measurable business value to our clients.\\nHeadquartered in Fredericksburg Virginia, Novel Applications employs engineers, analysts, IT specialists and other professionals who strive to be the best at everything they do.\\nNovel Applications is an AA/EEO Employer - Minorities/Women/Veterans/Disabled.\\nJob Description:\\nNAVOI is seeking a Database Engineering Lead to work collaboratively with agile development teams in the design, development, and deployment of advanced cybersecurity capabilities.\\nResponsibilities Include:\\nUsing database expertise to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers) in support of a large, agile-based, cybersecurity system\\nWorking with large structured and unstructured data sets\\nImplementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment\\nDesign, setup, administer, and tune NoSQL databases in the AWS cloud\\nDesign and implement the technical architecture necessary to support analytic and statistical processing requirements based on a tradeoff between performance and quality\\nPerforming data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations\\nWrite and refine code to ensure the quality and reliability of data extraction and processing\\nAnalyze and resolve data performance and quality issues\\nMake data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.\\nPerform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance\\nGenerate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates\\nWork collaboratively with agile development teams, attending daily scrums and providing data related solutions to the development team\\nDevelop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment\\nMigrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages\\nMaintain current industry knowledge of relevant concepts, practices, and procedures\\nRequired Skills:\\nU.S. Citizenship\\nMust have an active Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearance\\nMust be able to obtain DHS Suitability\\nDemonstrated ability with diverse skill sets (e.g. data architects, data scientists, software developers)\\nExcellent understanding of big data and data analytics\\nExperience working with large structured and unstructured data sets\\nDevelopment experience building ETL pipelines at scale\\nSolid SQL development skills\\nExperience with Linux/Unix tools and shell scripts\\nExpertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing, and encryption\\nGood communication skills, both oral and written\\nMust work well in a team environment as well as independently\\nMust exhibit good time management skills, independent decision making capability, and a focus on customer service.\\nDesired Skills:\\nExperience providing database engineering support to Intelligence, DoD, or DHS Customers\\nUnderstanding of Certification and Accreditation (ICD 503/DCID 6/3) processes as they apply to database technologies\\nAbility to support both SQL and NoSQL data management systems\\nExpertise in other RDBMS platforms such as Oracle RAC and SQL Server\\nFamiliarity with AWS data migration tools such as AWS DMS, Amazon EMR, and AWS Data Pipeline\\nExperience with data transformation techniques such as aggregations, joins, and data cleaning\\nExperience with Red Hat Enterprise Linux (RHEL) operating system, storage configurations, network architecture, VMware, and/or related management tools\\nDatabase management experience of SQL databases such as MySQL and PostgreSQL in AWS cloud\\nExperience creating and managing NoSQL databases such as DynamoDB in the AWS cloud\\nObject mapping and migration of data from legacy structured and unstructured data sources to Amazon DynamoDB using AWS tools, custom code, or ETL scripts\\nProgramming experience with languages such as R, Python, Java, JavaScript, JSON, etc.\\nKnowledge of Hadoop ecosystem, Map/Reduce, and data management products including Hbase, Hive, and Pig\\nDevSecOps and Continuous Integration / Continuous Delivery (CI/CD) knowledge\\nExperience or training in Six Sigma Methodology\\nITIL knowledge and certification\\nFamiliarity with SAFe (Scaled Agile Framework).\\nRequired Education:\\nBS Computer Science, Computer Engineering, Computer Information Systems, OR Computer Systems Engineering. Two years of related work experience may be substituted for each year of degree-level education\\nDesired Certifications:\\nIBM Certified Data Engineer Big Data\\nGoogle Cloud Certified Professional Data Engineer\\nCloudera Certified Professional Data Engineer\\nCCDH: Cloudera Certified Developer for Apache Hadoop\\nCCAH: Cloudera Certified Administrator for Apache Hadoop\\nCCSHB: Cloudera Certified Specialist in Apache HBase\\nCSSLP Certified Secure Software Lifecycle Professional\\nCertifications related to Scaled Agile Framework (SAFe) such as SAFe Practitioner (SP) or SAFe Program Consultant (SPC)\\nDoD 8570.1 IAT Level I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Web Application Architect</td>\n",
       "      <td>Arlington, VA</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>BS/MS in Computer Science or related field of study\\n5+ years in software development\\nExperience with big data database technologies such as columnar databases.\\nExperience manipulating and working with data in a variety of forms: csv, xml, JSON, structured and unstructured\\nExperience working with a variety of APIs and RESTful interfaces\\nKnowledge JIRA, Bamboo, Confluence, or Bit-Bucket a plus\\nExperience with Tableau or other similar data visualization tools.\\n</td>\n",
       "      <td>5+ years of development experience on web applications using Python, Ruby, Java, or C#\\n4+ years of SQL experience\\n4+ Experience working with databases and web applications in analytics, reporting or Business Intelligence teams\\n3+ years of development experience in Python with a desire to work on Python projects\\n3+ years of Agile development experience\\n2+ years of experience working in a Linux environment\\n2+ years of experience in AWS or other similar cloud environments. (AWS Redshift is a plus)\\n2+ years of experience in Tableau or other data visualization tools.\\n</td>\n",
       "      <td>Full time\\nResponsible for business analysis, application design, development, integration and delivery and application maintenance and support. Individuals in this position must be a self-directed professional, who will also bring leadership skills and quickly learn new technologies and programming languages.\\nPOSITION OVERVIEW :\\nBloomberg Industry Group provides legal, tax and compliance professionals with critical information, practical guidance and workflow solutions. We leverage leading technology and a global network of experts to deliver a unique combination of news and authoritative analysis, comprehensive research solutions, innovative practice tools, and proprietary business data and analytics. Bloomberg Industry Group is an affiliate of Bloomberg L.P., the global business, financial information and news leader.\\nBloomberg Industry Group seeks a a Data Engineer responsible for building products with embedded Analytics to surface insights on top of data infrastructure.\\nIndividuals in this position must be s elf-directed learners who can quickly learn new technologies and programming languages.\\nRESPONSIBILITIES :\\nProposes, develops and supports world-class customer facing web applications using a range of technologies.\\nParticipates in the analysis of system and business requirements\\nDelivers high-quality code by defining and deploying best practices in unit testing and regression and testing frameworks.\\nCommunicates with the Product Management and development teams to raise issues and identify potential barriers in a timely fashion.\\nParticipates in user-centered research through client focus groups, interviews, usage analysis, and rapid prototyping.\\nResponsible for protecting our customers and brand by writing secure-by-design code.\\nLeads, supervises, mentors, and trains other team members in order to develop a strong, best-in-class development bench.\\nDirects the work of and provides technical guidance to less experienced staff.\\nParticipates in recruiting, hiring, onboarding and performance management of new team members.\\nParticipates in special projects and performs other duties as assigned.\\nPartner with application architects and developers to build innovative products.\\nYou will utilize programming languages like Java, Python to write programs for automating data processing, writing complex queries, and integrating large data sets into cloud-based applications.\\nJob Requirements:\\n5+ years of development experience on web applications using Python, Ruby, Java, or C#\\n4+ years of SQL experience\\n4+ Experience working with databases and web applications in analytics, reporting or Business Intelligence teams\\n3+ years of development experience in Python with a desire to work on Python projects\\n3+ years of Agile development experience\\n2+ years of experience working in a Linux environment\\n2+ years of experience in AWS or other similar cloud environments. (AWS Redshift is a plus)\\n2+ years of experience in Tableau or other data visualization tools.\\nEducation and Experience :\\nBS/MS in Computer Science or related field of study\\n5+ years in software development\\nExperience with big data database technologies such as columnar databases.\\nExperience manipulating and working with data in a variety of forms: csv, xml, JSON, structured and unstructured\\nExperience working with a variety of APIs and RESTful interfaces\\nKnowledge JIRA, Bamboo, Confluence, or Bit-Bucket a plus\\nExperience with Tableau or other similar data visualization tools.\\nBloomberg Industry Group a wholly owned subsidiary of Bloomberg, is a leading source of legal, regulatory, and business information for professionals. Its network of more than 2,500 reporters, correspondents, and leading practitioners delivers expert analysis, news, practice tools, and guidance — the information that matters most to professionals. Bloomberg Industry Group authoritative coverage spans the full range of legal practice areas, including tax &amp; accounting, labor &amp; employment, intellectual property, banking &amp; securities, employee benefits, health care, privacy &amp; security, human resources, and environment, health &amp; safety.\\n\\n\\nBloomberg Industry Group offers a comprehensive benefits package including tuition reimbursement, domestic partner benefits, transportation subsidies, annual and sick leave, parenting leave, 401(k), and much more.\\n\\n\\nBloomberg Industry Group IS AN EQUAL OPPORTUNITY EMPLOYER and fully subscribes to the principles of Equal Employment Opportunity. Bloomberg Industry Group has adopted an Affirmative Action Program to ensure that all applicants and employees are considered for hire, promotion, and job status without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, marital or familial status, genetic information, disabled veteran, veteran, veteran of the Vietnam Era, or any other classification protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Senior Software Engineer (Data)</td>\n",
       "      <td>Bethesda, MD 20817</td>\n",
       "      <td>Bethesda</td>\n",
       "      <td>MD</td>\n",
       "      <td>20817</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Posting Date Sep 25, 2019\\nJob Number 19131580\\nJob Category Information Technology\\nLocation Marriott International HQ, 10400 Fernwood Road, Bethesda, Maryland, United States VIEW ON MAP\\nBrand Corporate\\nSchedule Full-time\\nRelocation? Yes\\nPosition Type Management\\n\\nStart Your Journey With Us\\nMarriott International is the world’s largest hotel company, with more brands, more hotels and more opportunities for associates to grow and succeed. We believe a great career is a journey of discovery and exploration. So, we ask, where will your journey take you?\\n\\nJob Summary\\n\\nThis Software Data Engineer will enable creation and implementation of solutions using advanced computational techniques to data-intensive problems in the hospitality industry. This rewarding position provides a career on a team of mathematical modelers and data scientists that makes a direct revenue impact on Marriott hotels, globally, by creating tools that facilitate data-driven decision making in the disciplines of pricing, revenue management, sales and reservations. This software engineer will be part of a team that supports computationally-intensive data analysis and modeling efforts, provides analytics and technical expertise; performs undirected research on large volumes of data to reach business conclusions and recommendations; contributes complex programming and analysis in revenue management and sales systems; analyzes data and deploys codes to deliver business solutions; actively participates in all phases of the project lifecycle – inception, prototyping, design, testing, rollout and support; works as part of an adaptive software development team; has a willingness to learn and brings unique perspectives to enable creative data-driven solutions.\\n\\nCANDIDATE PROFILE\\n\\nEducation and Experience\\nRequired: Graduate degree in a quantitative, technical or scientific field, e.g., computer science, engineering, mathematics, operations research, statistics. 5+ years of professional experience as a data engineering or software developer in a domain or system with substantial data volume (should be comfortable working with millions or billions of records). Extensive knowledge and experience in one of the following languages: python (especially pandas/numpy/scipy), scala (especially spark), java or C/C++. Proficient computer science or software development fundamentals (e.g., data structures, algorithms, complexity theory, etc.). Comfortable discussing or implementing advanced mathematical techniques and processes.\\n\\nPreferred: Experience writing complicated queries in SQL. Experience working with NoSQL databases (couchbase, cassandra, redis, mongodb, etc.) Cloud computing experience (especially AWS). Familiarity or experience with DevOps tools and pipelines (e.g., git, docker, kubernetes, jenkins, etc). An inherent curiosity in researching and developing solutions to complex and often open-ended business problem. An interest in continually learning new techniques and technologies. Ability to work within a team with strong communications skills, including an ability to explain and present technical concepts to technical and non-technical audiences. Willingness to debug and/or extend existing code bases. Willingness to participate in design sessions, code review session and oversee other developers. Experience with Big Data platforms (i.e. Hadoop, Spark, Kafka, Hive, etc). Experience in developing analytic driven solutions to business problems. Readiness to debug existing codes to identify source of reported application issues.\\n\\nCORE WORK ACTIVITIES\\n\\nTechnical Leadership Trains and/or mentors other team members, and peers as appropriate Provides leadership on solution structure, tasks, timeline and estimates for revenue management deliverables Partners with planning teams to articulate possible solutions to business problems and to develop requirements for new models\\n\\nDelivering Technology Formulates methods for measuring the effectiveness of pricing and revenue management activities. Prepares detailed specifications for translation of complex business processes into models and prototypes Develops and implements efficient, scalable algorithms using appropriate technologies/software/architectures Writes software and utility software tools to create and test new models Assists the Application Delivery teams in support of operations Assists the Application Delivery team in the maintenance of existing models Provides work/cost estimates for client requests Prepares documentation for client review. Conducts reviews, discusses findings, risks and challenges and establishes courses of action\\n\\nIT Governance Follows all defined iT standards and processes and provides input for improvements to the appropriate process owners as needed Maintains a proper balance between business and operational risk Follows the defined project management standards and processes\\n\\nService Provider Management\\nTo the extent that this role interacts with service providers: Validates that Service Providers develop and manage respective aspects of a project plan, including schedules, deliverables, and appropriate metrics. Makes short term plans for the team to effectively utilize resources Monitors Service Provider outcomes Reviews estimates of work effort for client project provided by Service Providers for accuracy Facilitates timely resolution of service delivery problems and minimizes the impact to clients\\n\\nMANAGEMENT COMPETENCIES\\nLeadership Communication - Conveys information and ideas to others in a convincing and engaging manner through a variety of methods. Leading Through Vision and Values - Keeps the organization's vision and values at the forefront of employee decision making and action. Managing Change - Initiates and/or manages the change process and energizes it on an ongoing basis, taking steps to remove barriers or accelerate its pace; serves as role model for how to handle change by maintaining composure and performance level under pressure or when experiencing challenges. Problem Solving and Decision Making - Identifies and understands issues, problems, and opportunities; obtains and compares information from different sources to draw conclusions, develops and evaluates alternatives and solutions, solves problems, and chooses a course of action. Professional Demeanor - Exhibits behavioral styles that convey confidence and command respect from others; makes a good first impression and represents the company in alignment with its values. Strategy Development - Develops business plans by exploring and systematically evaluating opportunities with the greatest potential for producing positive results; ensures successful preparation and execution of business plans through effective planning, organizing, and on-going evaluation processes.\\nManaging Execution Building a Successful Team - Uses an effective interpersonal style to build a cohesive team; inspires and sustains team cohesion and engagement by focusing the team on its mission and importance to the organization. Strategy Execution – Ensures successful execution across of business plans designed to maximize customer satisfaction, profitability, and market share through effective planning, organizing, and on-going evaluation processes. Driving for Results - Sets high standards of performance for self and/or others; assumes responsibility for work objectives; initiates, focuses, and monitors the efforts of self and/or others toward the accomplishment goals; proactively takes action and goes beyond what is required.\\nBuilding Relationships Customer Relationships - Develops and sustains relationships based on an understanding of customer/stakeholder needs and actions consistent with the company’s service standards. Global Mindset - Supports employees and business partners with diverse styles, abilities, motivations, and/or cultural perspectives; utilizes differences to drive innovation, engagement and enhance business results; and ensures employees are given the opportunity to contribute to their full potential. Strategic Partnerships - Develops collaborative relationships with fellow employees and business partners by making them feel valued, appreciated, and included; explores partnership opportunities with other people in and outside the organization; influences and leverages corporate and continental shared services and/or discipline leaders (e.g., HR, Sales &amp; Marketing, Finance, Revenue Management) to achieve objectives; maintains effective external relations with government, business and industry in respective countries; performs effectively as a liaison between locations, disciplines, and corporate to ensure needed resources are received and corporate strategies are understood and executed.\\nGenerating Talent and Organizational Capability Developing Others - Supports the development of other’s skills and capabilities so that they can fulfill current or future job/role responsibilities more effectively. Organizational Capability - Evaluates and adapts the structure of assignments and work processes to best fit the needs and/or support the goals of an organizational unit.\\nLearning and Applying Professional Expertise Continuous Learning - Actively identifies new areas for learning; regularly creates and takes advantage of learning opportunities; uses newly gained knowledge and skill on the job and learns through their application. Technical Acumen - Understanding and utilizing professional skills and knowledge in a specific functional area to conduct and manage everyday business operations and generate innovative solutions to approach function-specific work challenges\\no Technical Intelligence: Knowledge and ability to define and apply appropriate technology to enhance business process\\no Development Methodologies: Knowledge of general stages of SDLC framework and the application tiers within the development space.\\no Information Security: Knowledge of the security considerations relevant within the development space, including industry best practices related to information security Business Acumen - Understands and utilizes business information to manage everyday operations and generate innovative solutions to approach business and administrative challenges. Basic Competencies - Fundamental competencies required for accomplishing basic work activities.\\no Basic Computer Skills - Using basic computer hardware and software (e.g., personal computers, word processing software, Internet browsers, etc.).\\no Mathematical Reasoning - The ability to add, subtract, multiply, or divide quickly, correctly, and in a way that allows one to solve work-related issues.\\no Oral Comprehension - The ability to listen to and understand information and ideas presented through spoken words and sentences.\\no Reading Comprehension - Understanding written sentences and paragraphs in work related documents.\\no Writing - Communicating effectively in writing as appropriate for the needs of the audience.\\n\\n\\nMarriott International is an equal opportunity employer committed to hiring a diverse workforce and sustaining an inclusive culture. Marriott International does not discriminate on the basis of disability, veteran status or any other basis protected under federal, state or local laws.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>GDIT is hiring IT Professionals for F- 35 Program-Secret Clearance Required</td>\n",
       "      <td>Arlington, VA 22201</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>VA</td>\n",
       "      <td>22201</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nActive Secret Security Clearance is Required\\nAdditional IT certifications may be required for specific roles\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelors Degree in Computer Science, Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\nDo you hold an active US government security clearance? Are you interested in work that gives you the opportunity to use your skills to solve complex problems? Would you like to join a team that encourages ingenuity and is mission driven? Would you like to join an organization that makes a difference for our warfighters and our citizens?\\nGDIT was recently awarded the Joint Strike Fighter (JSF) F-35 IT program support contract. We are providing knowledge-based, information assurance and cybersecurity IT services to the F-35 JSF Virtual Enterprise (JVE) network in support of the F-35 Lightning II Joint Program Office (JPO). Our services include program management, enterprise performance management, enterprise architecture, implementation of emerging capabilities and requirements, life cycle management, operations &amp; maintenance, enterprise data management, service desk support and IT training.\\nWe are building a team of dedicated professionals. We are seeking candidates for multiple roles. The positions include but are not limited to:\\nHelp Desk Manager\\nInfrastructure Lead Engineer\\nSoftware Integrator Lead\\nNetwork Engineer\\nHelp Desk Specialist\\nStorage/SAN Engineer\\nVoice/Data Engineer\\nSystems Administrators\\nSoftware Developers\\nInformation Assurance\\n#F35ProgramLandingPage\\nEducation\\nBachelors Degree in Computer Science, Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience.\\nQualifications\\nActive Secret Security Clearance is Required\\nAdditional IT certifications may be required for specific roles\\nFor more than 50 years, General Dynamics Information Technology has served as a trusted provider of information technology, systems engineering, training and professional services to customers across federal, state, and local governments, and in the commercial sector. Over 40,000 GDIT professionals deliver enterprise solutions, manage mission-critical IT programs and provide mission support services worldwide. GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Springfield, VA</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Experience with Amazon Web Services (AWS), Microsoft Azure, or MilCloud 2.0</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Experience with Amazon Web Services (AWS), Microsoft Azure, or MilCloud 2.0</td>\n",
       "      <td>None Found</td>\n",
       "      <td>This position requires a Top Secret security clearance and eligibility for access to Sensitive Compartmented Information (TS/SCI)\\n\\nDesired Experience:\\nAt least 5 years of experience with Big Data systems, including Hadoop and Cloudera\\nAt least 2 years of experience with the design, implementation, or consulting for applications deployed across multiple organizations or a technical environment\\nExperience with multiple operating systems, including Linux-, UNIX-, and Windows-based\\n- Experience running, enhancing, and significantly upgrading and modifying Hadoop- and Cloudera-based environments for support to a wide variety of querying approaches\\nExperience establishing continuous integration and continuous deployment processes for applications and environments\\nExperience with extract, transform, and load (ETL) processes\\nExperience with multiple database technologies\\n\\n\\nDesired Education: Bachelors of Art (BA) or Bachelors of Science (BS), preferably in a related field.\\n\\nRequired Certifications:Security+ or CISSP Certification required\\n\\nWork Description:\\nThe Data Engineer is responsible for the configuration and ingestion of structured, unstructured, and semi-structured data repositories. Their work is focused on turning these data repositories into effective resources that satisfy mission requirements and that support a data analytics and rapid development pipeline. They maintain all operational aspects of data transfers, accounting for the security posture of the underlying infrastructure and the systems and applications that are supported, and they monitor the health of the environment through a variety of health tracking capabilities.\\n\\nThe Data Engineer also automates configuration management using NiFi and other tools and they stay current on data extraction, transfer, and loading (ETL) technologies and services. They should be able to work under general guidance, demonstrate the initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.\\n\\nThe Data Engineer applies their specialized knowledge of data and leverages their expertise to structure and retrieve data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically. Lastly, they engage with multiple functional groups to understand client challenges, prototype new ideas and new technologies, help create solutions to drive innovation, and they design, implement, schedule, test, and deploy full features and components of solutions.\\n\\nAdditional Desired Qualifications:\\nExperience with Amazon Web Services (AWS), Microsoft Azure, or MilCloud 2.0\\nExperience applying DoD Security Technical Implementation Guides (STIGs) and automating that process\\nExperience with storage fundamentals, including CIFS and NFS and backup and disaster recovery processes a plus\\nExperience configuring and aggregating logs for data analysis using Splunk or ELK solutions\\nExperience maintaining data transfer systems, including NIFI\\nExperience with two or more programming languages, including C#, Java, .NET, or similar\\nKnowledge of FedRAMP, RMF, and the implications of C&amp;A and SA&amp;A in a DoD environment a plus\\nAny industry-recognized Cloud Certifications preferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Alexandria, VA</td>\n",
       "      <td>Alexandria</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>RII develops cutting-edge software for the government and military. We use agile development practices and user-centered design to create innovative software solutions for complex real-world problems. We're breaking through the big, slow status quo with transformative technology that fundamentally changes and improves the world.\\n\\nOur team is currently seeking high-quality contributors for a variety of positions in the Northern Virginia area. Our projects include developing complex, web client server applications; developing mobile device applications and prototypes; developing Big Data and machine learning solutions; and architecting and prototyping complex distributed command and control network applications.\\n\\nIf you are a sharp, experienced engineer with demonstrated capabilities in designing and implementing Big Data architectures with scalable data extract, transform, and load technologies, and in deploying advanced analytics (e.g., natural language processing, deep learning) leveraging these architectures, we want to hear from you. Joining RII not only provides unique challenges and opportunities, it also directly and positively impacts many of our Defense and Homeland Security end users.\\nWHAT YOU WILL BE DOING\\nEstablish, maintain, and enhance a Big Data analytics architecture in support of several customer projects\\nBe the engineering point-person for Big Data ingest, storage, and analytical capabilities and technologies\\nMaintain awareness of the current and emerging capabilities in Big Data and analytics technologies and how these apply to our customer challenges\\nDevelop captivating solutions by collaborating with customers and the broader RII development team\\nDocument use cases, solutions, &amp; recommendations for our customers\\nWHAT YOU HAVE DONE\\nBS in Computer Science, equivalent degree, or previous work experience\\nWorked with NoSQL databases and distributed indexing systems (Elasticsearch or SOLR)\\nDeveloped scalable ingest/ETL pipelines using distributed messaging systems (such as Kafka or AMQP solutions)\\nExperience in developing and deploying Java and Python-based software architectures\\nExperience in deploying advanced analytics capabilities (e.g., natural language processing, deep learning) in Big Data settings\\nExperience in Linux administration, and in runtime configuration management and automated deployment (using tools such as Puppet and Ansible)\\nExcellent general understanding of Linux operating systems, distributed systems, microservices, and database technologies\\nExcellent general understanding of ETL and data analytics platforms\\nKnowledge of cloud computing infrastructure such as: Amazon Web Services EC2 or C2S\\nExperience in understanding and decomposing system level requirements into discrete and measurable tasks\\nEVEN BETTER\\nMS or PhD in Computer Science, equivalent degree, or work experience\\nExperience with Agile Methodologies and supporting technologies enabled by Atlassian products\\nExperience with the BigTable family of columnar stores (HBase, Cassandra, Accumulo)\\nStrong knowledge of installation, configuration, and maintenance of cloud computing and Big Data infrastructure to include Hadoop, Accumulo, Mongo, Kafka, Spark, Elasticsearch, Puppet, Ansible, Lucene, and related technologies\\nExperience with Continuous Integration and Continuous Deployment concepts\\nDesigned, developed, and deployed machine learning based capabilities\\nExperience creating Big Data solutions using public, private, and hybrid cloud approaches\\nExperience with integration methodologies and tools for Big Data applications and services\\nExperience with data quality and data profiling tools\\nExperience with data analytics profiling and performance analysis\\nExperience with client-side development including JavaScript, HTML5 and Angular\\nExperience leading a team of developers\\nUS Citizenship required. Candidates must be clearable to Secret, desired TS/SCI. Active clearance not required to apply.\\n\\nResearch Innovations, Inc. is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national, origin, disability status, protected veteran status, or any other characteristic protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Chief Data Engineer</td>\n",
       "      <td>Arlington, VA</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Key Role:\\nDesign, implement, and manage data platforms, databases, and data delivery systems and transform them into solutions that enable greater data insights, analysis, and reporting. Apply expertise in Big Data technology implementation and operations to working with clients to design integrated data platforms, ecosystems, and solutions. Leverage expertise in structured and unstructured data, streaming and batch data processing, extract, transform, and load (ETL), data wrangling, data ingest, and data access. Apply comprehension of database design and implementation tools, including entity-relationship data modelling and SQL, distributed computing architectures, operating systems, storage technologies, memory management, and networking to creating structure and value out of complex and ambiguous technical challenges with little guidance.\\nBasic Qualifications:10+ years experience with data analysis, management, architecture or engineering7+years experience with leading a teamExperience with custom or structured ETL design, implementation, and maintenanceExperience with NoSQL and big table data stores, including Accumulo, HBase, MongoDB, and CassandraAbility to quickly learn technical concepts and communicate with multiple functional groupsSecret clearanceBA or BS degree\\nAdditional Qualifications:Experience with enterprise architecture (EA)Experience with data ingest and preparation tools such as NiFi, Kylo, or StreamsetsExperience with Big Data platforms such as Cloudera, Hortonworks, or data bricksExperience with batch and streaming frameworks, including Kafka, Storm, or FlinkExperience with traditional databases such as Oracle, MSSQL, PostGres or othersExperience with data visualization tools such as Tableau or Zoom DataExperience with data management tools such as Informatica, TalenD or othersExperience with multiple data modeling concepts, including XML and JSONPossession of excellent analytical and problem-solving skills\\nClearance:\\nApplicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required.\\nWe’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change.\\nSIG2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Full-Stack Data Engineer-TS/SCI</td>\n",
       "      <td>Reston, VA 20191</td>\n",
       "      <td>Reston</td>\n",
       "      <td>VA</td>\n",
       "      <td>20191</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>http://www-01.ibm.com/employment/us/benefits/\\nhttps://www-03.ibm.com/press/us/en/pressrelease/50744.wss</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Introduction\\nAs a Data Scientist at IBM, you will help transform our clients’ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it’s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.\\n\\nYour Role and Responsibilities\\nIBM Global Business Services (GBS) is a team of business, strategy and technology consultants enabling enterprises to make smarter decisions and providing unparalleled client and consumer experiences in cognitive, data analytics, cloud technology and mobile app development. IBM GBS empowers clients to digitally reinvent their business and get the competitive edge in the cognitive era in over 170 countries.\\n\\nBottom line? We outthink ordinary. Discover what you can do at IBM.\\n\\nWe are seeking a full-stack data engineer to join IBM in support of a growing Advanced Analytics effort for the US Navy. The Full-Stack Data Engineer will primarily provide React and Node support. There may be additional work in database design, data manipulations, tailored script / algorithm development, and data visualization. The role is in the Washington DC area (Reston and/or Arlington, Virginia).\\n\\nBENEFITS\\nHealth Insurance. Paid time off. Corporate Holidays. Sick leave. Family planning. Financial Guidance. Competitive 401K. Training and Learning. We continue to expand our benefits and programs, offering some of the best support, guidance and coverage for a diverse employee population.\\nhttp://www-01.ibm.com/employment/us/benefits/\\nhttps://www-03.ibm.com/press/us/en/pressrelease/50744.wss\\nCAREER GROWTH\\nOur goal is to be essential to the world, which starts with our people. Company wide we kicked off an internal talent strategy program called Go Organic. At our core, we are committed to believing and investing in our workforce through:\\nSkill development: helping our employees grow their foundational skills\\nFinding the dream job at IBM: navigating our company with the potential for many careers by channeling an employee’s strengths and career aspirations\\nDiversity of people: Diversity of thought driving collective innovation\\nIn 2015, Go Organic filled approximately 50% of our open positions with internal talent that were promoted into the role.\\n\\nCORPORATE CITIZENSHIP\\nWith an employee population of 375,000 in over 170 countries, amazingly we connect, collaborate, and care. IBMers drive a corporate culture of shared responsibility. We love grand challenges and everyday improvements for our company and for the world. We care about each other, our clients, and the communities we live, work, and play in!\\nhttp://www.ibm.com/ibm/responsibility/initiatives.html\\nhttp://www.ibm.com/ibm/responsibility/corporateservicecorps\\n\\nRequired Professional and Technical Expertise\\nUS Citizen with active TS//SCI clearance or show immediate eligibility according to JPAS security database\\n5+ years of React and Node development and maintenance.\\nDemonstrable skills in Node/JavaScript, SQL/NoSQL in a Windows or Linux desktop operating environment\\nData visualization skills using JavaScript (such as ChartJS or D3)\\nTechnical skills in MVC frameworks and Web API design\\nMust possess the ability to communicate clearly, concisely, and with technical accuracy in both oral and written modes.\\nMust be able to work effectively under time constraints and potentially changing priorities, while maintaining a high level of attention to detail.\\nMust be able to work in a collaborative, team environment, including on government site where availability of advanced development/database tools may be limited\\nBachelors Degree in Engineering, Mathematics, Operations Research, Computer Science, or related technical field\\n\\nPreferred Professional and Technical Expertise\\nMasters Degree, preferably in Engineering, Mathematics, Operations Research, Statistics, Computer Science, or related\\nExperience with React and Node in a military setting\\nExperience with Python and the ETL process\\nExperience with identifying and gathering research and analysis data from DOD organizations and systems\\nExperience with Navy research and analysis databases\\n\\nAbout Business Unit\\nIBM Services is a team of business, strategy and technology consultants that design, build, and run foundational systems and services that is the backbone of the world's economy. IBM Services partners with the world's leading companies in over 170 countries to build smarter businesses by reimagining and reinventing through technology, with its outcome-focused methodologies, industry-leading portfolio and world class research and operations expertise leading to results-driven innovation and enduring excellence.\\n\\nYour Life @ IBM\\nWhat matters to you when you’re looking for your next career challenge?\\n\\nMaybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities – where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust – where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.\\n\\nImpact. Inclusion. Infinite Experiences. Do your best work ever.\\n\\nAbout IBM\\nIBM’s greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.\\n\\nLocation Statement\\nFor additional information about location requirements, please discuss with the recruiter following submission of your application.\\n\\nBeing You @ IBM\\nIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Herndon, VA 20171</td>\n",
       "      <td>Herndon</td>\n",
       "      <td>VA</td>\n",
       "      <td>20171</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum Requirements: Bachelor’s Degree in Computer Science, IT or related field &amp; minimum 5-7 years of QA experience\\nMinimum experience of 5 years with relational databases such as Oracle, SQL Server, Sybase, RedShift\\nMinimum 3+ years of experience with ETL technologies\\n</td>\n",
       "      <td>Secure our Nation, Ignite your Future\\nThis position will report to the Group CTO and be part of the technology office staff. You will be supporting an internal big data analytics application that will be hosted on a cloud platform. You will have prior experience working in an agile/DevOps environment. You will have the opportunity to work with some of the newest technologies, transformative projects, and play a strong role in shaping the innovation agenda across the company.\\n\\nEssential Job Duties:\\nProvide database design, development and implementation support\\nParticipate in daily scrum meetings and support development team\\nWork with customers and team members in an Agile development environment\\nFollow Continuous Integration/Continuous Delivery (CI/CD) best practices for code build and deployments\\nDevelop PL/SQL, stored procedures, ETL scrips to support this analytics application\\nDocument database design and develop optimum data ingest techniques from multiple data sources\\nMinimum Requirements:\\nMinimum Requirements: Bachelor’s Degree in Computer Science, IT or related field &amp; minimum 5-7 years of QA experience\\nMinimum experience of 5 years with relational databases such as Oracle, SQL Server, Sybase, RedShift\\nMinimum 3+ years of experience with ETL technologies\\nAdditional skills:\\n\\nExperience using Jira, Git, Confluence, Knowledge and understanding of big data technologies like Hadoop, Hive, etc., Ability to handle stress and work well under pressure, Ability to use MS Office, Experience with cloud technologies, Analytical and Critical Thinking Skills, Interpersonal and People Skills, Leadership Skills, Listening Skills, Multitasking Ability, Oral and Written Communication Skills, Organizational Skills\\nManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.\\nIf you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.\\nIf you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Reston, VA</td>\n",
       "      <td>Reston</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n3+ years of experience with SAS programming in a business environment, particularly with data modeling, or in a “big data” context.\\n1 year of Java experience\\nWell qualified with SQL.\\nBS or MS in any quantitative field (computer science, systems engineering, mathematics, economics, statistics, etc.) or equivalent work experience.</td>\n",
       "      <td>Solid experience with SAS data processing, macro programming and running SQL queries within SAS (PROC SQL and SQL pass-through to Vertica.)\\nOur planned Data Lake will make use of new tools including AWS Glue, AWS Athena, Spark, etc. Knowledge of these tools or the interest and desire to learn them.\\nFamiliarity with Linux, GIT and AWS Console.\\nAble to implement complex algorithms which transform, cleanse, impute, and mash up data.\\nPassion for data processing, data modeling, data mining and tackling complex operations.\\nProfessional experience working in an Agile environment.\\nProfessional experience working with a source code version control system.\\nProficiency with Microsoft Excel.\\nEnglish fluency, both spoken and written. Able to discuss complex technical subjects with clarity and precision.</td>\n",
       "      <td>Maintaining, improving, and executing existing scripts written in SAS, SQL, and Python. Most of the codebase is currently in SAS but the future will contain more Java, and possibly Python or R.\\nDesigning new scripts using the above tools to ingest, cleanse, consolidate, analyze, and summarize the incoming data. You will also be implementing and tuning algorithms and business rules, quality-checking the data results, and working iteratively with evolving requirements.\\nMeeting delivery deadlines for new products, features, and enhancements.\\nImplementing and delivering large historical data solutions to new and existing customers.\\nCoding new applications for ad hoc reporting and for data research inquiries.\\nInvestigating issues with data quality and responding to stakeholders’ technical questions.\\nIdentifying opportunities to complement, enhance and/or optimize our data processing environment with new tools and techniques.\\nProvide on-call support on rotation basis, occasionally during afterhours and weekends.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer\\nReston, VA\\nWe seek an experienced Data Engineer with the skills, energy and business acumen to excel on our aviation Data Engineering Team. Our team performs data acquisition and ingestion, processing, and data delivery across a variety of products.\\nYour contribution will be critical in many areas such as cleansing, consolidating, and normalizing industry data, and then synthesizing valuable data sets to be served directly to clients or loaded into a data warehouse that supports our online analytics tools. You will support industry-leading Cirium products and help develop the next generation of Cirium Business Intelligence tools for the aviation industry.\\n\\nKey Accountabilities and Responsibilities:\\nMaintaining, improving, and executing existing scripts written in SAS, SQL, and Python. Most of the codebase is currently in SAS but the future will contain more Java, and possibly Python or R.\\nDesigning new scripts using the above tools to ingest, cleanse, consolidate, analyze, and summarize the incoming data. You will also be implementing and tuning algorithms and business rules, quality-checking the data results, and working iteratively with evolving requirements.\\nMeeting delivery deadlines for new products, features, and enhancements.\\nImplementing and delivering large historical data solutions to new and existing customers.\\nCoding new applications for ad hoc reporting and for data research inquiries.\\nInvestigating issues with data quality and responding to stakeholders’ technical questions.\\nIdentifying opportunities to complement, enhance and/or optimize our data processing environment with new tools and techniques.\\nProvide on-call support on rotation basis, occasionally during afterhours and weekends.\\nQualifications:\\n3+ years of experience with SAS programming in a business environment, particularly with data modeling, or in a “big data” context.\\n1 year of Java experience\\nWell qualified with SQL.\\nBS or MS in any quantitative field (computer science, systems engineering, mathematics, economics, statistics, etc.) or equivalent work experience.\\nKey Skills Required:\\nSolid experience with SAS data processing, macro programming and running SQL queries within SAS (PROC SQL and SQL pass-through to Vertica.)\\nOur planned Data Lake will make use of new tools including AWS Glue, AWS Athena, Spark, etc. Knowledge of these tools or the interest and desire to learn them.\\nFamiliarity with Linux, GIT and AWS Console.\\nAble to implement complex algorithms which transform, cleanse, impute, and mash up data.\\nPassion for data processing, data modeling, data mining and tackling complex operations.\\nProfessional experience working in an Agile environment.\\nProfessional experience working with a source code version control system.\\nProficiency with Microsoft Excel.\\nEnglish fluency, both spoken and written. Able to discuss complex technical subjects with clarity and precision.\\nOptional Skills Preferred:\\nKnowledge of advanced SAS programming techniques and efficiencies.\\nFamiliar with Linux command line and shell scripting.\\nKnowledge of passenger aviation data (ticket data, DOT data, schedules), QSI scoring models.\\nKnowledge of airline terminology and airline business metrics.\\nExperience with Kanban.\\nKnowledge of GIT source control, particularly from the Linux command line.\\nKnowledge of R and / or Python for data processing and machine learning.\\nPlease note: This is a regular, full-time position which requires working out of our office located in downtown Reston, VA.\\n\\nAbout Cirium\\n\\nCirium provides data services and end-to-end data solutions to customers serving the global travel industry. The company has established a leadership position as a provider of real-time global flight information, serving airlines and airports, travel agencies, developers, consumers, and more. The company is leveraging the platform and domain knowledge it has developed to expand into new data sets and new products that deliver value to the company’s core markets. We work in an interesting field, we have demonstrated success in what we do, and we are well positioned for future growth.\\n\\nOur Environment: We hire the best talent in the travel and technology industries. To support our talented team, we offer an extraordinary work environment that places trust and respect at the forefront of our company values. These values enable our employees to do their best work by creating an open, supportive environment that promotes creativity. We think it's a great place to do great work, and if you like the sound of this as well, we encourage you to consider this opening.\\n\\nCirium is a branded division of Reed Business Information (RBI). RBI provides information, analytics and data to business professionals worldwide. Our strong global products and services hold market-leading positions across a wide range of industry sectors including banking, petrochemicals and aviation where we help customers make key strategic decisions every day. RBI is part of RELX Group plc, a world-leading provider of information solutions for professional customers across industries.\\nhttp://www.reedbusiness.com\\n\\nRBI is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. If a qualified individual with a disability or disabled veteran needs a reasonable accommodation to use or access our online system, that individual should please contact 1.877.734.1938 or accommodations@relx.com.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Data Engineer Specialist</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a savvy Data Engineer to join Loan Advisor Business Intelligence (LA BI). Ideal candidate has extensive experience modeling and wrangling data (structured and unstructured), in-depth knowledge of database architecture, and enjoys optimizing data systems and building them from the ground up all in support of analytic and reporting applications. The Data Engineer will support our BI developers and data/business analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Knowledge of Hadoop based platform a must.\\nDuties:\\nPerform complex data analysis, including query optimizationAssemble large, complex data sets that meet functional / non-functional business requirementsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Develop and implement ETL processes, reports and queries in support of business analytics\\nQualifications:\\nAdvanced working SQL knowledge and experience working a variety of databases technologies (Oracle, MySQL, JDBC, NoSQL)Experience building and optimizing ‘big data (Hadoop)’ data pipelines, architectures and data sets.Experience shredding XML/JSON filesA successful history of manipulating, processing and extracting value from large disconnected datasets.Effective organization skills with attention to detailEffective oral and written communication skillsKnowledge of R, Python, or other ETL toolsKnowledge of Tableau and/or MicroStrategy a plus.Familiarity with Secondary Mortgage Market and/or Freddie Mac business and operations also a plus.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Data Engineer, Data and Services (University Hire - Summer 2020 Start)</td>\n",
       "      <td>Arlington, VA</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Who is Mastercard?\\nWe are the global technology company behind the world’s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless ®. We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.\\nJob Title\\nData Engineer, Data and Services (University Hire - Summer 2020 Start)\\n********APPLICATION INSTRUCTIONS: Please attach your resume, transcript, and cover letter with your application in the resume upload tab. All three documents must be submitted. *********\\n\\n\\nData Engineers are fundamental to the success of our clients; you will be the bridge between raw client data and Mastercard's software. You will be responsible for:\\n\\n\\nDesigning processes to extract, transform, and load (ETL) terabytes of client data into Mastercard's analytics platform using SQL and other technologies\\n\\nWorking across multiple client teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set.\\n\\nTackling big data problems across various industries, utilizing your creative thinking skills\\n\\n\\nMake an Impact as a Data Engineer\\n\\nStrategic problem solving with exceptional peers who are also passionate about data\\nCreative freedom to innovate with new technologies (such as Microsoft SQL Server &amp; Business Intelligence Tools) and to explore a variety of directions\\nFlexibility to work on many new and challenging projects across a diversity of industries\\nA dynamic environment where you will have an impact and make an immediate difference\\nAn immediate opportunity for increased responsibility, leadership, and professional growth\\nCommitted mentoring and training by an experienced and driven management team\\nCollaborate with other Mastercard departments and focus on internal development, during which you will develop new tools and processes that will be used across Mastercard\\n\\n\\nBring your passion and expertise\\n\\nUnderstanding of relational databases and ETL Processes (preferably Microsoft SQL Server)\\nDesire to work with data and help businesses make better data-driven decisions\\nExcellent written and verbal communication skills\\nHands-on experience with the ETL process, SQL, and SSIS\\nKnowledge of at least one programming language a plus (Powershell, .Net, Perl, Python, VB Script, C#)\\nStrong troubleshooting and problem solving capabilities\\nDemonstrated analytical/quantitative skills\\nBachelor's degree with an established history of academic success\\n\\nMastercard Worldwide is an Equal Employment Opportunity Employer and does not discriminate in employment on the basis of age, race, color, gender, national origin, disability, veteran status, or any other basis that is prohibited by applicable law.\\nMastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.\\nIf you require accommodations or assistance to complete the online application process, please contact reasonable.accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>McLean 2 (19052), United States of America, McLean, Virginia\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nData Engineer\\n\\nAs a Capital One Data Engineer, you’ll be part of a team that’s building new analytical and machine learning tools and frameworks to exploit advantages in the latest developments in cloud computing. You will participate in detailed technical design, development and implementation of applications used by our data scientists and business analysts to build and launch models, analyze data, and make million dollar decisions. We work closely with our users and are looking for people who are excited to iterate quickly on solutions and see the impact in days, not months.\\n\\nWho You Are\\n\\nYou yearn to be part of cutting edge, high profile projects and are motivated by delivering world-class solutions on an aggressive schedule\\n\\nSomeone who is not intimidated by challenges; thrives even under pressure; is passionate about their craft; and hyper focused on delivering exceptional results\\n\\nYou love to learn new technologies and mentor junior engineers to raise the bar on your team\\n\\nIt would be awesome if you have a robust portfolio on Github and/or open source contributions you are proud to share\\n\\nPassionate about intuitive and engaging user interfaces, as well as new/emerging concepts and techniques.\\n\\nThe Job\\n\\nCollaborating as part of a cross-functional Agile team to create and enhance software that enables state of the art data science and data analysis\\n\\nDeveloping and deploying data pipelines using Apache Spark and AWS Big Data stack\\n\\nDeveloping frameworks to accelerate the model development lifecycle\\n\\nUtilizing programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake\\n\\nLeveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment and Test Automation to enable the rapid delivery of working code utilizing tools like Jenkins, Terraform, Git and Docker\\n\\nPerforming unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance\\n\\nBasic Qualifications:\\nBachelor’s Degree\\n\\nAt least 2 years of Data Engineering experience\\n\\nAt least 2 years of experience using Java, Python, or Scala\\n\\nAt least 1 year of experience using Apache Spark\\n\\nPreferred Qualifications:\\nMaster’s Degree or PhD\\n\\n4+ years of experience using Java, Python, or Scala\\n\\n4+ years experience with Relational Database Systems and SQL\\n\\n4+ years of experience with PostgreSQL or Redshift\\n\\n4+ years of UNIX or Linux experience\\n\\n2+ years of experience with Cloud computing\\n\\n2+ years of experience with AWS\\n\\n2+ years of experience using Apache Spark\\n\\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Master, Data Engineering</td>\n",
       "      <td>McLean, VA</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>McLean 2 (19052), United States of America, McLean, Virginia\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nMaster, Data Engineering\\n\\nWe are looking for driven individuals to join our team of passionate Software and Data Engineers who will help us create Capital One’s next generation of data products and capabilities. Do you possess deep knowledge across technologies and have a strong background in software and data engineering?\\n\\nAs a Master, Data Engineer you will:\\nManage teams of Software and Data Engineers who will build data pipeline frameworks to automate high-volume and real-time data delivery for our Spark and streaming data hub\\nTransform complex analytical models in scalable, production-ready solutions\\nContinuously integrate and ship code into our cloud production environments\\nDevelop cloud-based applications from the ground up using a modern technology stack\\nWork directly with Product Owners and customers to deliver data products in a collaborative and agile environment\\n\\nResponsibilities include:\\nLead and develop sustainable data driven solutions with current new generation data technologies to drive our business and technology strategies\\nDesign robust systems with an eye on the long-term maintenance and support of the application\\nLeverage reusable code modules to solve problems across the team and organization\\nHandle multiple functions and roles for the projects and Agile teams\\nManage and mentor a team of data engineers (both full-time associates and/or third-party resources)\\nProviding business, application and technology consulting in feasibility discussions with technology team members and business partners\\nDefine, execute and continuously improve our internal software architecture processes\\nBe a technology thought leader and strategist\\n\\nWhat We Have:\\nFlexible work schedules\\nConvenient office locations\\nGenerous salary and merit-based pay incentives\\nA startup mindset with the wallet of a top 10 bank\\nMonthly innovation challenges dedicated to test driving cutting edge technologies\\nYour choice of equipment (MacBook/PC, iPhone/Android Device)\\n\\nBasic Qualifications:\\nBachelor's Degree\\nAt least 5 years of experience in Java or Python based software application development\\nAt least 3 years of people management experience\\nAt least 3 years of Agile experience\\nAt least 1 year of experience with a Cloud platform (AWS, Google, or Azure)\\n\\nPreferred Qualifications:\\n7+ years of experience with Data Engineering concepts and building and maintaining Big Data applications using open source software like Spark or Hadoop\\n7+ years of experience with Software Engineering\\nUnderstanding of Test-Driven Development concepts and supportive tools like Cucumber\\nUnderstanding of Object-Oriented and Functional programming concepts using languages like Python and Java\\nExperience with developing and deploying applications on Cloud, preferably on Amazon Web Services (AWS)\\n1+ years’ experience designing, developing, and implementing APIs\\nFamiliarity with Configuration management tools like Ansible or Chef\\nFamiliarity with Application Container concepts using tools like Docker or Kubernetes\\n\\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Business Intelligence / Data Engineer</td>\n",
       "      <td>Herndon, VA</td>\n",
       "      <td>Herndon</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s Degree in Computer Science, Information Systems, Mathematics, Statistics, or related field or equivalent experience5+ year experience with Data modeling, SQL, ETL , Data Warehousing and Datalakes5+ year experience in the design, creation, management, and business use of large datasets.This position requires the candidate selected be a U.S. citizen and must currently possess an active Top Secret security clearance. The position further requires, after start, the selected candidate obtain and maintain an active TS/SCI security clearance with polygraph and satisfy other security related requirements.\\n\\nAs a Business Intelligence / Data Engineer you will enable data-driven decision making within the Amazon Web Services (AWS) Data Center Infrastructure Operations organization.\\nThe Infrastructure Operations Team is responsible for planning, implementing, monitoring and continuously improving the global Amazon Data Center infrastructure. The team supports all aspects of the Data Center based organizations, including but not limited to: Safety, Security, maintenance, daily operations, logistics, engineering and equipment management.\\n\\nYou should be passionate about working with huge data sets and be someone who is able to bring data sets together to answer business questions and drive growth. You will have an opportunity to work with big data and emerging technologies while driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, metadata, reporting, and dash boarding.\\n\\nThe Business Intelligence Data Engineer will:\\nPrimarily support teams within the Infrastructure environment, and long term will have opportunities to support teams in the overall Amazon Web Services community.Build ETLs to ingest the data into the data warehouse and datalake, as well as end-user facing reporting applications.Develop, implement and maintain the metrics and reports to enable decision support systems for the organization. This includes working with other teams to develop those metrics from their services.Partner with business customers and development teams to define analytics requirements and then deliver flexible, scalable, end-to-end solutions.\\nThis position requires the candidate selected be a U.S. citizen and must currently possess an active Top Secret security clearance. The position further requires, after start, the selected candidate obtain and maintain an active TS/SCI security clearance with polygraph and satisfy other security related requirements.\\n\\nAbility to balance and prioritize multiple conflicting requirements with high attention to detail.Experience with writing SQL scriptsExperience with enterprise-class Business Intelligence tools such as Microstrategy, PowerBI, Tableau, Oracle BI, Penthao, etc.Experience communicating with business owners to understand their data and reporting requirements.Experience with data presentation skills to summarize key findings and communicate with both business and technical teams.Experience working in a Linux environmentExperience with scripting language such as Python, Perl, Ruby or JavascriptExperience with MPP databases such as RedshiftExperience with Datalake developmentKnowledge of predictive/advanced analytics and tools (such as R, SAS, Matlab)Knowledge of noSQL databases (such as DynamoDB, MongoDB)Knowledge in an enterprise class RDBMSKnowledge of AWS products and services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Big Data Engineer, Analytics</td>\n",
       "      <td>Chantilly, VA</td>\n",
       "      <td>Chantilly</td>\n",
       "      <td>VA</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>OVERVIEW\\nTechnology is constantly changing, and our adversaries are digitally “going dark” at a rate that is exceeding law enforcement’s ability to keep pace. Those charged with protecting the United States are not always able to access the evidence needed to prosecute crime and prevent terrorism. The Government has trusted in Peraton to provide the technical ability, tools, and resources to bring criminals to justice. In response to this challenge, we are seeking a talented Big Data Engineer with a focus in Analytics.\\nRESPONSIBILITIES\\nWhat you’ll do…\\nProvide entity resolution expertise in support of a large scale search and discovery application\\nWork closely with entity resolution subject matter experts, program engineers and stakeholders to integrate complex data sources into operation and develop new analytics and algorithms\\nSupport advanced data exploitation capabilities using Hadoop related technologies\\nEnhance the data processing infrastructure using open source and commercial technologies\\nCapture and define requirements for entity resolution features and functionality from a UI end-user perspective\\nAnalyze documentation and source data of new data feeds\\nDevelop scripts to ETL source data into destination format\\nProvide O&amp;M monitoring and resolution of production web services using monitoring tools including Ganglia, Jenkins, and other means\\nModify pipeline scripts to enhance functionality and support bug fixes\\nResolve bugs by modifying pipeline scripts and entity resolution explanation service scripts\\nCoordinate development efforts in an agile team\\nQUALIFICATIONS\\nYou’d be a great fit if…\\nYou’ve obtained a BS degree and have five (5) years of relevant experience. However, equivalent experience may be considered in lieu of degree.\\nYou have three (3) or more years of experience using Hadoop or other large-scale data warehouse technologies to support entity and relationship resolution and operations\\nYou have one (1) year of experience in Apache, Spark, Hive, Pig, ER Resolution Engines, Unix scripting, Novetta Entity Analytics, Waremen Pro or similar ER engines\\nYou have a current Top Secret security clearance with SCI eligibility and the ability to obtain a polygraph\\n\\nIt would be even better if you…\\nHave hands on experience with any of the following technologies:\\nSQL\\nLinux scripting\\nAWK, PERL, BASH or other scripting language\\nSOLR\\nJenkins configuration to perform O&amp;M operations\\nSpark\\n\\n What you’ll get…\\nAn immediately-vested 401(K) with employer matching\\nComprehensive medical, dental, and vision coverage\\nTuition assistance, financing, and refinancing\\nCompany-paid infertility treatments\\nCross-training and professional development opportunities\\nInfluence major initiatives\\nThis position requires the candidate to have a current Top Secret security clearance and the ability to obtain a polygraph. Candidate must possess SCI eligibility.\\nABOUT PERATON\\nAre you ready to join the next-generation of national security? Peraton is a fresh name in the industry with an established portfolio and legacy going back more than a century. We work differently than our peers – with agility, the freedom to innovate, an entrepreneurial spirit and a culture of responsibility. As part of the Peraton team, you’ll be part of our movement to build a great company, solve the most daunting challenges facing mankind today, to protect and promote freedom around the world, and to secure our future, for our families, our communities, our nation, and our way of life.\\nEEO STATEMENT\\nWe are an Equal Opportunity/Affirmative Action Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Arlington, VA 22201</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>VA</td>\n",
       "      <td>22201</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer\\nArlington, VA, US\\nAt Elder Research Inc., a recognized leader in data science and machine learning solutions, we pride ourselves in our ability to find creative, cutting edge solutions to real-world problems. We are looking for innovative and inquisitive self-starters who enjoy understanding a problem space and building fast, efficient, and tractable data infrastructure to deliver real value for our clients.\\n\\nAs a member of the Elder Research team, you will join a functional team of accomplished Data Scientists and Software Engineers that deliver custom analytic solutions. Some of your responsibilities will include: wrangling and fusing large and disparate data sets, assisting in the deployment of models and algorithms, automating the entire data pipeline, and communicating model results through user-focused data visualizations.\\n\\nJob Description\\nA Data Engineer supports robust and repeatable data manipulation, large scale infrastructure for data ingestion, and stunning data visualization for custom client applications.\\nEssential Functions:\\nÂ· Work collaboratively with data scientists, business consultants, and software engineers to create and deploy dynamic data applications that help our customers make meaningful business decisions.\\nÂ· Develop and deploy robust data pipelines and end-to-end systems\\nParticipate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance\\nÂ· Provide leadership and coordination for certain stages of the engineering lifecycle as needed\\nÂ· Perform other technical tasks as needed, including writing project reports, managing, implementing, and/or maintaining technical infrastructure, etc.\\nÂ· Ability and the willingness to tailor applications to a clientâ€™s business goals using an iterative methodology.\\nÂ· Ability to consider both long-term stability and scalability while taking a user-focused approach to development and deployment.\\nÂ· Communicate clearly both verbally and in writing to teammates and clients\\nÂ· Ability to work independently in a collaborative, dynamic, cross-functional environment\\nÂ· Travel to and work on-site at clients both local and non-local. Number of days at client site vary depending on project requirements.\\n\\nRequired Skills:\\nÂ· Bachelors or Masterâ€™s degree in Computer Science or related field, or equivalent experience\\nÂ· Excellent written and verbal communication skills\\nÂ· Ability to work with high-level mathematical concepts and associated code-form representations\\nÂ· Focus areas: data manipulation, big data architecture, data structures, database administration, cloud platforms and SaaS, development operations (devops), data visualization and user experience\\n\\nSelected Technologies: (A combination of experience with some of the following is required)\\n\\nDatabases:\\nSQL-based technologies (e.g. PostgreSQL and MySQL, Oracle)\\nNoSQL technologies (e.g. Cassandra, MongoDB, Graph Database)\\n\\nBig Data:\\nSpark/Databricks (RDD, Data Frames, GraphX)\\nHadoop (e.g. MapReduce, Hive and Pig)\\n\\nETL and Data Integration:\\nKettle/Spoon, Luigi, Jenkins, Airflow, Nifi\\n\\nIndexers/Search Engines:\\nElasticSearch, Solr\\n\\nCloud:\\nAWS, Azure, stack configuration and management\\n\\nDeployment:\\nDocker\\n\\nLanguages:\\nPython, Java, Scala, Familiarity with R\\n\\nO/S:\\nUNIX, Linux, Solaris, ssh, git\\n\\nDesired Skills\\nÂ· Data manipulation, SQL, relational databases, and/or NoSQL databases â€“ experience as a DBA is a huge plus\\nÂ· Cloud platform development and SaaS\\nÂ· DevOps â€“ infrastructure, continuous integration and automation, packaging and deployment\\nÂ· Consulting experience is a plus\\n\\nAbout Elder Research, Inc.\\nHeadquartered in Charlottesville, VA with offices in Arlington, VA, Baltimore, MD, and Raleigh, NC, Elder Research is a fast growing solutions and consulting firm specializing in predictive analytics. At Elder Research, youâ€™ll be part of a fun, friendly community. In keeping with our entrepreneurial spirit, we want candidates that are self-motivated with an innate curiosity and strong team work ethic. We work hard to provide the best value to our clients and allow each person to contribute their ideas and put their skills to use immediately.\\n\\nElder Research provides analytic solutions to hundreds of companies across numerous industries. Our team enjoys great variety in the type of work they do and exposure to a wide range of techniques and tools. If you are passionate about integrating data, technology, and analytics in a team-based environment to solve problems, then Elder Research may be a good fit for you.\\nElder Research, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.\\nElder Research is unable to sponsor H1B visas for this role</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Ashburn, VA 20147</td>\n",
       "      <td>Ashburn</td>\n",
       "      <td>VA</td>\n",
       "      <td>20147</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>What you’ll be doing...\\nVerizon’s Data Analytics, Insights and Enablement team works with some of the largest data sets collected from the largest, more reliable network. We are responsible for providing fast, clean, and relevant data for Verizon’s network and field organizations as well as key measurements and insights about performance to front line employees serving our customers. You will work side by side with Verizon team members and other partners to develop next generation technologies to ensure our networks are available when our customers need them through complex, scalable data platforms, with ever growing and interesting challenges.\\nDesigning, architecting and developing data analytics systems for various use cases including but not limited to network performance and field operations.\\nParticipating and contributing in engineering lifecycle including writing production code, building end-to-end machine learning solutions, conduct code reviews and working closely with infrastructure teams\\nBuilding scalable systems to process petabytes of wireline and wireless data to provide real-time insights into the health of Verizon networks.\\nBuilding batch and real-time data pipelines to ingest and transform data for model training/ testing; and building and deploy model inference pipeline.\\nWhat we’re looking for...\\nYou will need to have:\\nBachelor's degree or four or more years of work experience.\\nSix or more years of relevant work experience.\\nEven better if you have:\\nA degree in Computer Science, Engineering or related discipline.\\nSix or more years of architectural design experience, preferably focused on Hadoop, Open Source and Big Data.\\nSolid Software Development skills with proficiency in Java and Python.\\nComputer science fundamentals in object-oriented design, data structures and algorithms and complexity analysis.\\nExperience working in distributed computing with Apache Hadoop, Apache Spark, Apache Druid and data ingestion frameworks like Apache Gobblin, Logstash, open source data connectors, and real-time data streaming services like Apache Kafka, Apache Flink and or Apache Storm/Pulsar.\\nExperience with Machine learning tools like TensorFlow, scikit-learn, Pandas, Kera’s, etc.\\nStrong experience with Big data processing tools like Hadoop/Hive/HBase/Oozie/HDFS/Yarn.\\nExperience implementing and monitoring big data pipelines and working in a large, complex devops and CICD environment.\\nExperience in production scale software development with ML/AI use cases.\\nStrong background in basic machine learning techniques including Anomaly Detection, Time-series, Deep Learning, supervised and unsupervised learning.\\nExperience in building scalable solutions using advanced analytics and machine learning techniques.\\nWorking in an Agile-team environment, with other data engineers, data scientist, ML engineers, etc.\\nExperience in feature engineering for both ML/DL models, A/B testing, data management and model governance.\\nExperience in productionizing ML/DL models with containers using Docker and Kubernetes.\\nNetwork domain knowledge.\\nWillingness to travel.\\nWhen you join Verizon...\\nYou’ll have the power to go beyond – doing the work that’s transforming how people, businesses and things connect with each other. Not only do we provide the fastest and most reliable network for our customers, but we were first to 5G - a quantum leap in connectivity. Our connected solutions are making communities stronger and enabling energy efficiency. Here, you’ll have the ability to make an impact and create positive change. Whether you think in code, words, pictures or numbers, join our team of the best and brightest. We offer great pay, amazing benefits and opportunity to learn and grow in every role. Together we’ll go far.\\nEqual Employment Opportunity\\nWe're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>Washington</td>\n",
       "      <td>DC</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Senior Data Engineer responsibilities include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of our client's Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatment, etc .\\n\\nRequired Education/Experience:\\nBachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.\\n\\nPreferred:\\nExperience in Healthcare or related industryExperience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plusExperience productizing/automating predictive models that use R, SAS, Python, SPSS, etc.Continuous delivery and deployment automation for analytic solutions using tools like BambooFamiliarity with test driven development methodology for analytic solutionsAGILEAPI developmentData visualization and/or dashboard developmentDemonstrated ability to achieve stretch goals in a highly innovative and fast-paced environment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title                      Location  \\\n",
       "0    Big Data Engineer                      Annapolis Junction, MD 20701   \n",
       "1    Big Data Engineer INTELF8              Chantilly, VA                  \n",
       "2    Data Engineer                          Arlington, VA                  \n",
       "3    Google Technical Architect             Arlington, VA 22209            \n",
       "4    Technical Writer                       Crystal City, VA               \n",
       "..                ...                                    ...               \n",
       "167  Business Intelligence / Data Engineer  Herndon, VA                    \n",
       "168  Big Data Engineer, Analytics           Chantilly, VA                  \n",
       "169  Data Engineer                          Arlington, VA 22201            \n",
       "170  Big Data Engineer                      Ashburn, VA 20147              \n",
       "171  Senior Data Engineer                   Washington, DC                 \n",
       "\n",
       "                   City State         Zip     Country  \\\n",
       "0    Annapolis Junction  MD    20701       None Found   \n",
       "1    Chantilly           VA    None Found  None Found   \n",
       "2    Arlington           VA    None Found  None Found   \n",
       "3    Arlington           VA    22209       None Found   \n",
       "4    Crystal City        VA    None Found  None Found   \n",
       "..            ...        ..           ...         ...   \n",
       "167  Herndon             VA    None Found  None Found   \n",
       "168  Chantilly           VA    None Found  None Found   \n",
       "169  Arlington           VA    22201       None Found   \n",
       "170  Ashburn             VA    20147       None Found   \n",
       "171  Washington          DC    None Found  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Qualifications  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "1    Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark\\nDevelopment experience with Java, C++, Scala, Groovy, Python, and/or shell scripting\\nExperience with data warehousing tools and technologies\\nAbility to work within UNIX/Linux operating systems\\nAWS experience a plus\\nThis position requires U.S. Citizenship and an active TS/SCI security clearance                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "3    Minimum 5 years of Consulting or client service delivery experience on Google GCP\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "4    \\n3 or more years of experience in the Information Technology field\\nBachelor's degree\\nExcellent communications skills working with customers and engineers on a software development project\\nPrior experience:\\nWorking with development teams\\nWorking with users to create user stories and define requirements\\nDocumenting programs and processes\\nReporting status to customers and management\\nIdentifying and evaluating risks\\nTechnologies:\\nMS Office Suite\\nJIRA and Confluence, or a similar tracking and documentation system\\nSharePoint, or a similar documentation system\\nMust currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence (CI) polygraph examination\\n   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...   \n",
       "167  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "168  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "169  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "170  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "171  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "\n",
       "                                                                                                                                                                                                                                                        Skills  \\\n",
       "0    None Found                                                                                                                                                                                                                                                  \n",
       "1    None Found                                                                                                                                                                                                                                                  \n",
       "2    None Found                                                                                                                                                                                                                                                  \n",
       "3    DevOps on an GCP platform. Multi-cloud experience a plus.                                                                                                                                                                                                   \n",
       "4    \\nExperience working on information technology programs developing user interfaces and complex data processes\\nComfortable acting as a liaison between customers and a development team\\nAgile development certifications (scrum master, product owner)\\n   \n",
       "..                                                                                                                                                                                                                                                         ...   \n",
       "167  None Found                                                                                                                                                                                                                                                  \n",
       "168  None Found                                                                                                                                                                                                                                                  \n",
       "169  None Found                                                                                                                                                                                                                                                  \n",
       "170  None Found                                                                                                                                                                                                                                                  \n",
       "171  None Found                                                                                                                                                                                                                                                  \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Responsibilities  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1    Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark\\nDevelopment experience with Java, C++, Scala, Groovy, Python, and/or shell scripting\\nExperience with data warehousing tools and technologies\\nAbility to work within UNIX/Linux operating systems\\nAWS experience a plus\\nThis position requires U.S. Citizenship and an active TS/SCI security clearance                                                                                                                                                                                                          \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "4    \\nDetermine the needs of end users of technical documentation\\nStudy product samples and talk with product designers and developers\\nWork with technical staff to make products easier to use and thus need fewer instructions\\nOrganize and write supporting documents for products\\nUse photographs, drawings, diagrams, animation, and charts that increase users' understanding\\nSelect appropriate medium for message or audience, such as manuals or online videos\\nStandardize content across platforms and media\\nGather usability feedback from customers, designers, and manufacturers\\nRevise documents as new issues arise\\n   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ...   \n",
       "167  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "168  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "169  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "170  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "171  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Education  \\\n",
       "0    \\nA Bachelor’s degree in electrical engineering, computer engineering, mathematics or a related discipline may be substituted for four years of general experience.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "167  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "168  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "169  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "170  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "171  Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                            Requirement  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                           \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                           \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                           \n",
       "3    Proven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills   \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                           \n",
       "..          ...                                                                                                                                                                                                                                                                                                                           \n",
       "167  None Found                                                                                                                                                                                                                                                                                                                           \n",
       "168  None Found                                                                                                                                                                                                                                                                                                                           \n",
       "169  None Found                                                                                                                                                                                                                                                                                                                           \n",
       "170  None Found                                                                                                                                                                                                                                                                                                                           \n",
       "171  None Found                                                                                                                                                                                                                                                                                                                           \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                FullDescriptions  \n",
       "0    Imagine being able to shape the future of cyber… on a team that powers the recruitment of the most talented and motivated employees in the world through technological innovations. This is the reality of being part of AG Grace, Inc. We need leaders who want to discover, enhance, capture and counter cyber activity, who have a bias for action, and who have a deep desire to build….to make the previously impossible possible. Is that you?\\nYou’ll work on cutting edge technology and provide Solutions, Insights and Deliver professional services and solutions and that our customers have come to expect from AG Grace, Inc. You should be somebody who enjoys working on complex system software, is customer-centric, and feels strongly about building a software system that maximizes the value of cloud computing. As a developer on the team you’ll collaborate with sharp engineers to drive improvements to cyber analysis technology, design and develop new services and solutions, build and track metrics to ensure high quality of results.\\n\\nExperience\\nAt least five (5) years of general experience in software development/engineering, including requirements analysis, software development, installation, integration, evaluation, enhancement, maintenance, testing, and problem diagnosis/resolution.\\nAt least three (3) years of experience developing software with high level languages such as Java, C, C++.\\nAt least three (3) years of experience developing software in UNIX/Linux (Red Hat versions 3-5+) operating systems.\\nAt least three (3) years of experience in software integration and software testing, to include developing and implementing test plans and test scripts.\\nAt least two (2) years of experience with distributed scalable Big Data Store (NoSQL) such as HBase, Cloud Base/Accumulo, Big Table, etc., as well as two (2) years of experience with the Map Reduce programming model, the Hadoop Distributed File System (HDFS), and technologies such as Hadoop, Hive, Pig, etc\\nDemonstrated work experience with Serialization such as JSON and/or BSON\\nDemonstrated work experience with developing restful services, Ruby on Rails framework, LDAP protocol configuration management and cluster performance management (e.g. Nagios)\\nDemonstrated work experience developing solutions integrating and extending FOSS/COTS products.\\nDemonstrated technical writing skills and shall have generated technical documents in support of a software development project\\nDemonstrated work experience with Source Code Management {e.g. Git, Stash, or Subversion, etc.).\\nTS/SCI Full Scope Poly required\\nEducation\\nA Bachelor’s degree in electrical engineering, computer engineering, mathematics or a related discipline may be substituted for four years of general experience.\\nAG Grace, Inc. is dedicated to developing our nation's future and protecting our critical infrastructure resources. We provide a full range of IT services and solutions that help improve our client's ability to reduce risk, improve performance and provide mission assurance. We assist our customers in solving the problems of today and tomorrow. Come be a part of the Future as we help our customers execute their mission.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "1    Data Works is looking for senior Big Data Engineers able to lead the way in tackling the most difficult engineering challenges in Big Data systems\\nResponsibilities\\nData Works is seeking a Big Data Engineer with demonstrated experience in leading large scale data warehousing projects. A successful candidate will be strong in Map Reduce, Java, and possess an understanding of data science concepts such as machine learning and trend analysis. Candidate should also be familiar with indexing products such as Lucene and Elasticsearch. Relevant certifications considered but not required.\\nRequired Qualifications\\nExperience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark\\nDevelopment experience with Java, C++, Scala, Groovy, Python, and/or shell scripting\\nExperience with data warehousing tools and technologies\\nAbility to work within UNIX/Linux operating systems\\nAWS experience a plus\\nThis position requires U.S. Citizenship and an active TS/SCI security clearance\\nE3/Sentinel is an equal opportunity employer and Vietnam Era Veterans Readjustment Assistance Act (VEVRAA) federal contractor. All qualified applicants receive consideration for employment without regard to race, color, religion, gender, national origin, age, sexual orientation, gender identity, protected veteran status, or status as a qualified individual with a disability. E3/Sentinel hires and promotes individuals solely on the basis of their qualifications for the job to be filled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "2    The Software/Data Engineer who joins Sila will support a unique mission that leverages leading technology tools to facilitate the highly efficient integration of data and advanced analytics.\\nIn this role, you will architect an API Gateway and oversee application integration across the enterprise. Your results, as a Software/Data Engineer, will enable access to or submission of heterogeneous data, exposure of analytic services through standard interfaces, and integration with external partners.\\n\\nCollaborating with data scientists, data engineers, and client stakeholders, you will work together to define the requirements that drive the API strategy. Your existing, personal knowledge of distributed application architectures, to include APIs, web services, microservices, and asynchronous event protocols contributes substantially to this endeavor.\\n\\nThe successful candidate has the option to work on-site in Bethesda, MD or Reston, VA when not at Sila’s office in Arlington, VA.\\n\\nCandidates must be currently authorized to work in the United States without the need for employment-based visa sponsorship now or in the future.\\nYou will:\\nArchitect, build, and manage API’s supporting on premise and cloud platform environments such as AWS.\\nContribute to the conceptual and physical design of application integration using APIs and events\\nDevelop and test an API gateway that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale\\nDevelop and test runtime execution of APIs\\nPrototyping and demonstrate concepts when necessary (e.g. mock-client apps)\\nExperience\\nBachelor’s degree in Computer Science or Business Information Systems or equivalent educational or professional experience and/or qualifications with at least 6 years of experience in software development\\nMust possess a TS security clearance and be able to obtain a DoD TS/SCI clearance\\nAt least 1 year of experience in developing REST services using Java or Python or Node.js\\nExperience delivering APIs for external partners and integrating with external vendors\\nAble to implement processes and troubleshoot to continue to improve operational stability\\nExperience with GitHub or GitLab\\nUnderstanding of DevOps processes\\nStrong communication, interpersonal skills and problem-solving skills\\nDesired Experience\\nPrevious Agile SW team participation\\nUse of JIRA, Confluence, and Jenkins\\nElasticsearch and logging framework use\\nApache JMeter or Blazemeter use/knowledge\\nTwo (2) or more years of AWS Commercial and/or AWS GovCloud experience\\nHigh-volume applications tuning experience\\nKnowledgeable in security protocols such as SAML and OAuth\\nAbout Sila\\n\\nSila is a technology and management consulting firm that delivers solutions to the world’s leading corporations and Federal government agencies. Our solutions expertise lies in the areas of cybersecurity, risk management, data analytics, software engineering and integration, strategy and transformation, and digital creative services. We are a values-driven company with a culture that fosters collaboration, creativity, and social responsibility. Sila employees are part of a community of vibrant, high-performing contributors who push each other to achieve the highest standard of excellence. Our team members have extensive opportunities to discover their passions and shape their own career paths, and we continually invest in employees’ growth through innovative training, mentorship, and professional development programs. Staff are quickly immersed in clients’ business challenges, work closely with emerging technologies to develop impactful solutions to these challenges, and are exposed to a variety of industries and market offerings.\\n\\nWe are looking for full-time employees to become an integral part of our growing team. Sila is headquartered in Washington, D.C. and has offices in Chicago, IL; Seattle, WA; and Shelton, CT. Sila offers a range of great benefits including a comprehensive healthcare package, 401K with matching, paid time off, and paid company holidays, as well as other unique benefits that support our staff’s active work/life balance.\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.\\nBenefits\\nHealth & Wellness\\nComprehensive healthcare insurance options, flexible spending accounts, disability and life insurance\\n401k\\nRobust 401k plan with matching for your retirement savings\\nPaid Time Off\\nGenerous PTO allowance, and we actually take it!\\nProfessional Development\\nCore curriculum of training plus a variety of professional development and learning channels\\nSocial Events\\nPaint nite, races, mixology classes—we have something for everyone at our monthly social events\\nWork/Life Balance\\nWe understand and value the importance of life outside of work                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "3    Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Google Cloud Platform (GCP) Technical Architect Delivery is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would also be responsible for developing and delivering Google GCP cloud solutions to meet todays high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Google GCP Technical Architect is a highly performant GCP Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data soltuions on cloud. Using Google GCP public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications.\\n\\nRole & Responsibilities:Work with Sales and Bus Dev teams in providing Data and GCP Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nBasic Qualifications\\nMinimum 5 years of Consulting or client service delivery experience on Google GCP\\nMinimum 10 years of experience in big data, database and data warehouse architecture and delivery\\nBachelors degree or 12 years previous professional experience\\nAble to travel 100% (M-TH)\\nMinimum of 5 years of professional experience in 2 of the following areas:\\nSolution/technical architecture in the cloud\\nBig Data/analytics/information analysis/database management in the cloud\\nIoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using GCP services etc.:\\nData Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core\\nStreaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam\\nData Warehousing & Data Lake : BigQuery, Cloud Storage\\nAdvanced Analytics : Cloud ML engine, Google Data Studio, Tensorflow & Sheets\\n\\nFamiliarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nCertified GCP Solutions Architect - Associate\\nCertified GCP Solutions Architect – Professional (Nice to have)\\nCertified GCP Big Data Specialty (Nice to have)\\nCertified GCP AI/ML Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an GCP platform. Multi-cloud experience a plus.\\nExperience developing and deploying ETL solutions on GCP\\nStrong in Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience beyond GCP a plus - AWS and Azure\\n\\nProfessional Skill Requirements\\nProven ability to build, manage and foster a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\nExcellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.  \n",
       "4    Are you passionate about solving challenging problems?\\nDo you thrive being a critical part of an elite team of like-minded people?\\nHow would you like for your next career move to take you to the next level?\\n\\nIf any of this sounds appealing, look no further.\\n\\nJob Description:\\nWe are seeking a data engineer/analyst who is excited about analyzing and solving national security related problems.\\n\\nResponsibilities include:\\n\\nDetermine the needs of end users of technical documentation\\nStudy product samples and talk with product designers and developers\\nWork with technical staff to make products easier to use and thus need fewer instructions\\nOrganize and write supporting documents for products\\nUse photographs, drawings, diagrams, animation, and charts that increase users' understanding\\nSelect appropriate medium for message or audience, such as manuals or online videos\\nStandardize content across platforms and media\\nGather usability feedback from customers, designers, and manufacturers\\nRevise documents as new issues arise\\n\\nBasic Qualifications:\\n\\n3 or more years of experience in the Information Technology field\\nBachelor's degree\\nExcellent communications skills working with customers and engineers on a software development project\\nPrior experience:\\nWorking with development teams\\nWorking with users to create user stories and define requirements\\nDocumenting programs and processes\\nReporting status to customers and management\\nIdentifying and evaluating risks\\nTechnologies:\\nMS Office Suite\\nJIRA and Confluence, or a similar tracking and documentation system\\nSharePoint, or a similar documentation system\\nMust currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence (CI) polygraph examination\\n\\nDesired Skills:\\n\\nExperience working on information technology programs developing user interfaces and complex data processes\\nComfortable acting as a liaison between customers and a development team\\nAgile development certifications (scrum master, product owner)\\n\\nSo, what does Novetta do?\\n\\nWe focus on three core areas: Cyber, Entity, and Multi-Int Analytics. Our products are focused on processing and analyzing vast amounts of data in these core areas. Our services are focused on helping our customers move from complexity to clarity. At Novetta, we bridge the gap between what our customers think they can do and what they aspire to achieve.\\n\\nOur culture is shaped by a commitment to our Core Values:\\n\\nIntegrity: We hold ourselves accountable to the highest standards of integrity and ethics.\\nCustomer Mission Success: Customer mission success drives our daily efforts—we strive always to exceed customer expectations and focus on mission success beyond contractual commitments.\\nEmployee Focus: We value our employees and demonstrate our commitment to them by providing clear communications, outstanding benefits, career development, and opportunities to work on problems and technical challenges of national significance.\\nInnovation: We believe that innovation is critical to our success – that discovering new and more effective ways to achieve customer mission success is what makes us a great company.\\n\\nGET A REFERRAL BONUS FOR THE GREAT PEOPLE YOU KNOW!\\nWith our amazing referral program, you could be eligible to earn\\noutstanding rewards for referring qualified new hires to Novetta.\\n\\nNovetta is an equal opportunity/affirmative action employer.\\nAll qualified applicants will receive consideration for employment without regard to sex,\\ngender identity, sexual orientation, race, color, religion, national origin, disability,\\nprotected veteran status, age, or any other characteristic protected by law.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "167  Bachelor’s Degree in Computer Science, Information Systems, Mathematics, Statistics, or related field or equivalent experience5+ year experience with Data modeling, SQL, ETL , Data Warehousing and Datalakes5+ year experience in the design, creation, management, and business use of large datasets.This position requires the candidate selected be a U.S. citizen and must currently possess an active Top Secret security clearance. The position further requires, after start, the selected candidate obtain and maintain an active TS/SCI security clearance with polygraph and satisfy other security related requirements.\\n\\nAs a Business Intelligence / Data Engineer you will enable data-driven decision making within the Amazon Web Services (AWS) Data Center Infrastructure Operations organization.\\nThe Infrastructure Operations Team is responsible for planning, implementing, monitoring and continuously improving the global Amazon Data Center infrastructure. The team supports all aspects of the Data Center based organizations, including but not limited to: Safety, Security, maintenance, daily operations, logistics, engineering and equipment management.\\n\\nYou should be passionate about working with huge data sets and be someone who is able to bring data sets together to answer business questions and drive growth. You will have an opportunity to work with big data and emerging technologies while driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, metadata, reporting, and dash boarding.\\n\\nThe Business Intelligence Data Engineer will:\\nPrimarily support teams within the Infrastructure environment, and long term will have opportunities to support teams in the overall Amazon Web Services community.Build ETLs to ingest the data into the data warehouse and datalake, as well as end-user facing reporting applications.Develop, implement and maintain the metrics and reports to enable decision support systems for the organization. This includes working with other teams to develop those metrics from their services.Partner with business customers and development teams to define analytics requirements and then deliver flexible, scalable, end-to-end solutions.\\nThis position requires the candidate selected be a U.S. citizen and must currently possess an active Top Secret security clearance. The position further requires, after start, the selected candidate obtain and maintain an active TS/SCI security clearance with polygraph and satisfy other security related requirements.\\n\\nAbility to balance and prioritize multiple conflicting requirements with high attention to detail.Experience with writing SQL scriptsExperience with enterprise-class Business Intelligence tools such as Microstrategy, PowerBI, Tableau, Oracle BI, Penthao, etc.Experience communicating with business owners to understand their data and reporting requirements.Experience with data presentation skills to summarize key findings and communicate with both business and technical teams.Experience working in a Linux environmentExperience with scripting language such as Python, Perl, Ruby or JavascriptExperience with MPP databases such as RedshiftExperience with Datalake developmentKnowledge of predictive/advanced analytics and tools (such as R, SAS, Matlab)Knowledge of noSQL databases (such as DynamoDB, MongoDB)Knowledge in an enterprise class RDBMSKnowledge of AWS products and services                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "168  OVERVIEW\\nTechnology is constantly changing, and our adversaries are digitally “going dark” at a rate that is exceeding law enforcement’s ability to keep pace. Those charged with protecting the United States are not always able to access the evidence needed to prosecute crime and prevent terrorism. The Government has trusted in Peraton to provide the technical ability, tools, and resources to bring criminals to justice. In response to this challenge, we are seeking a talented Big Data Engineer with a focus in Analytics.\\nRESPONSIBILITIES\\nWhat you’ll do…\\nProvide entity resolution expertise in support of a large scale search and discovery application\\nWork closely with entity resolution subject matter experts, program engineers and stakeholders to integrate complex data sources into operation and develop new analytics and algorithms\\nSupport advanced data exploitation capabilities using Hadoop related technologies\\nEnhance the data processing infrastructure using open source and commercial technologies\\nCapture and define requirements for entity resolution features and functionality from a UI end-user perspective\\nAnalyze documentation and source data of new data feeds\\nDevelop scripts to ETL source data into destination format\\nProvide O&M monitoring and resolution of production web services using monitoring tools including Ganglia, Jenkins, and other means\\nModify pipeline scripts to enhance functionality and support bug fixes\\nResolve bugs by modifying pipeline scripts and entity resolution explanation service scripts\\nCoordinate development efforts in an agile team\\nQUALIFICATIONS\\nYou’d be a great fit if…\\nYou’ve obtained a BS degree and have five (5) years of relevant experience. However, equivalent experience may be considered in lieu of degree.\\nYou have three (3) or more years of experience using Hadoop or other large-scale data warehouse technologies to support entity and relationship resolution and operations\\nYou have one (1) year of experience in Apache, Spark, Hive, Pig, ER Resolution Engines, Unix scripting, Novetta Entity Analytics, Waremen Pro or similar ER engines\\nYou have a current Top Secret security clearance with SCI eligibility and the ability to obtain a polygraph\\n\\nIt would be even better if you…\\nHave hands on experience with any of the following technologies:\\nSQL\\nLinux scripting\\nAWK, PERL, BASH or other scripting language\\nSOLR\\nJenkins configuration to perform O&M operations\\nSpark\\n\\n What you’ll get…\\nAn immediately-vested 401(K) with employer matching\\nComprehensive medical, dental, and vision coverage\\nTuition assistance, financing, and refinancing\\nCompany-paid infertility treatments\\nCross-training and professional development opportunities\\nInfluence major initiatives\\nThis position requires the candidate to have a current Top Secret security clearance and the ability to obtain a polygraph. Candidate must possess SCI eligibility.\\nABOUT PERATON\\nAre you ready to join the next-generation of national security? Peraton is a fresh name in the industry with an established portfolio and legacy going back more than a century. We work differently than our peers – with agility, the freedom to innovate, an entrepreneurial spirit and a culture of responsibility. As part of the Peraton team, you’ll be part of our movement to build a great company, solve the most daunting challenges facing mankind today, to protect and promote freedom around the world, and to secure our future, for our families, our communities, our nation, and our way of life.\\nEEO STATEMENT\\nWe are an Equal Opportunity/Affirmative Action Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "169  Data Engineer\\nArlington, VA, US\\nAt Elder Research Inc., a recognized leader in data science and machine learning solutions, we pride ourselves in our ability to find creative, cutting edge solutions to real-world problems. We are looking for innovative and inquisitive self-starters who enjoy understanding a problem space and building fast, efficient, and tractable data infrastructure to deliver real value for our clients.\\n\\nAs a member of the Elder Research team, you will join a functional team of accomplished Data Scientists and Software Engineers that deliver custom analytic solutions. Some of your responsibilities will include: wrangling and fusing large and disparate data sets, assisting in the deployment of models and algorithms, automating the entire data pipeline, and communicating model results through user-focused data visualizations.\\n\\nJob Description\\nA Data Engineer supports robust and repeatable data manipulation, large scale infrastructure for data ingestion, and stunning data visualization for custom client applications.\\nEssential Functions:\\nÂ· Work collaboratively with data scientists, business consultants, and software engineers to create and deploy dynamic data applications that help our customers make meaningful business decisions.\\nÂ· Develop and deploy robust data pipelines and end-to-end systems\\nParticipate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance\\nÂ· Provide leadership and coordination for certain stages of the engineering lifecycle as needed\\nÂ· Perform other technical tasks as needed, including writing project reports, managing, implementing, and/or maintaining technical infrastructure, etc.\\nÂ· Ability and the willingness to tailor applications to a clientâ€™s business goals using an iterative methodology.\\nÂ· Ability to consider both long-term stability and scalability while taking a user-focused approach to development and deployment.\\nÂ· Communicate clearly both verbally and in writing to teammates and clients\\nÂ· Ability to work independently in a collaborative, dynamic, cross-functional environment\\nÂ· Travel to and work on-site at clients both local and non-local. Number of days at client site vary depending on project requirements.\\n\\nRequired Skills:\\nÂ· Bachelors or Masterâ€™s degree in Computer Science or related field, or equivalent experience\\nÂ· Excellent written and verbal communication skills\\nÂ· Ability to work with high-level mathematical concepts and associated code-form representations\\nÂ· Focus areas: data manipulation, big data architecture, data structures, database administration, cloud platforms and SaaS, development operations (devops), data visualization and user experience\\n\\nSelected Technologies: (A combination of experience with some of the following is required)\\n\\nDatabases:\\nSQL-based technologies (e.g. PostgreSQL and MySQL, Oracle)\\nNoSQL technologies (e.g. Cassandra, MongoDB, Graph Database)\\n\\nBig Data:\\nSpark/Databricks (RDD, Data Frames, GraphX)\\nHadoop (e.g. MapReduce, Hive and Pig)\\n\\nETL and Data Integration:\\nKettle/Spoon, Luigi, Jenkins, Airflow, Nifi\\n\\nIndexers/Search Engines:\\nElasticSearch, Solr\\n\\nCloud:\\nAWS, Azure, stack configuration and management\\n\\nDeployment:\\nDocker\\n\\nLanguages:\\nPython, Java, Scala, Familiarity with R\\n\\nO/S:\\nUNIX, Linux, Solaris, ssh, git\\n\\nDesired Skills\\nÂ· Data manipulation, SQL, relational databases, and/or NoSQL databases â€“ experience as a DBA is a huge plus\\nÂ· Cloud platform development and SaaS\\nÂ· DevOps â€“ infrastructure, continuous integration and automation, packaging and deployment\\nÂ· Consulting experience is a plus\\n\\nAbout Elder Research, Inc.\\nHeadquartered in Charlottesville, VA with offices in Arlington, VA, Baltimore, MD, and Raleigh, NC, Elder Research is a fast growing solutions and consulting firm specializing in predictive analytics. At Elder Research, youâ€™ll be part of a fun, friendly community. In keeping with our entrepreneurial spirit, we want candidates that are self-motivated with an innate curiosity and strong team work ethic. We work hard to provide the best value to our clients and allow each person to contribute their ideas and put their skills to use immediately.\\n\\nElder Research provides analytic solutions to hundreds of companies across numerous industries. Our team enjoys great variety in the type of work they do and exposure to a wide range of techniques and tools. If you are passionate about integrating data, technology, and analytics in a team-based environment to solve problems, then Elder Research may be a good fit for you.\\nElder Research, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.\\nElder Research is unable to sponsor H1B visas for this role                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "170  What you’ll be doing...\\nVerizon’s Data Analytics, Insights and Enablement team works with some of the largest data sets collected from the largest, more reliable network. We are responsible for providing fast, clean, and relevant data for Verizon’s network and field organizations as well as key measurements and insights about performance to front line employees serving our customers. You will work side by side with Verizon team members and other partners to develop next generation technologies to ensure our networks are available when our customers need them through complex, scalable data platforms, with ever growing and interesting challenges.\\nDesigning, architecting and developing data analytics systems for various use cases including but not limited to network performance and field operations.\\nParticipating and contributing in engineering lifecycle including writing production code, building end-to-end machine learning solutions, conduct code reviews and working closely with infrastructure teams\\nBuilding scalable systems to process petabytes of wireline and wireless data to provide real-time insights into the health of Verizon networks.\\nBuilding batch and real-time data pipelines to ingest and transform data for model training/ testing; and building and deploy model inference pipeline.\\nWhat we’re looking for...\\nYou will need to have:\\nBachelor's degree or four or more years of work experience.\\nSix or more years of relevant work experience.\\nEven better if you have:\\nA degree in Computer Science, Engineering or related discipline.\\nSix or more years of architectural design experience, preferably focused on Hadoop, Open Source and Big Data.\\nSolid Software Development skills with proficiency in Java and Python.\\nComputer science fundamentals in object-oriented design, data structures and algorithms and complexity analysis.\\nExperience working in distributed computing with Apache Hadoop, Apache Spark, Apache Druid and data ingestion frameworks like Apache Gobblin, Logstash, open source data connectors, and real-time data streaming services like Apache Kafka, Apache Flink and or Apache Storm/Pulsar.\\nExperience with Machine learning tools like TensorFlow, scikit-learn, Pandas, Kera’s, etc.\\nStrong experience with Big data processing tools like Hadoop/Hive/HBase/Oozie/HDFS/Yarn.\\nExperience implementing and monitoring big data pipelines and working in a large, complex devops and CICD environment.\\nExperience in production scale software development with ML/AI use cases.\\nStrong background in basic machine learning techniques including Anomaly Detection, Time-series, Deep Learning, supervised and unsupervised learning.\\nExperience in building scalable solutions using advanced analytics and machine learning techniques.\\nWorking in an Agile-team environment, with other data engineers, data scientist, ML engineers, etc.\\nExperience in feature engineering for both ML/DL models, A/B testing, data management and model governance.\\nExperience in productionizing ML/DL models with containers using Docker and Kubernetes.\\nNetwork domain knowledge.\\nWillingness to travel.\\nWhen you join Verizon...\\nYou’ll have the power to go beyond – doing the work that’s transforming how people, businesses and things connect with each other. Not only do we provide the fastest and most reliable network for our customers, but we were first to 5G - a quantum leap in connectivity. Our connected solutions are making communities stronger and enabling energy efficiency. Here, you’ll have the ability to make an impact and create positive change. Whether you think in code, words, pictures or numbers, join our team of the best and brightest. We offer great pay, amazing benefits and opportunity to learn and grow in every role. Together we’ll go far.\\nEqual Employment Opportunity\\nWe're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "171  The Senior Data Engineer responsibilities include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of our client's Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatment, etc .\\n\\nRequired Education/Experience:\\nBachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.\\n\\nPreferred:\\nExperience in Healthcare or related industryExperience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plusExperience productizing/automating predictive models that use R, SAS, Python, SPSS, etc.Continuous delivery and deployment automation for analytic solutions using tools like BambooFamiliarity with test driven development methodology for analytic solutionsAGILEAPI developmentData visualization and/or dashboard developmentDemonstrated ability to achieve stretch goals in a highly innovative and fast-paced environment                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "\n",
       "[172 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_WashingtonDC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
