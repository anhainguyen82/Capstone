,Title,Location,City,State,Zip,Country,Qualifications,Skills,Responsibilities,Education,Requirement,FullDescriptions
0,Big Data Engineer,"Annapolis Junction, MD 20701",Annapolis Junction,MD,20701,None Found,None Found,None Found,None Found,"
A Bachelor’s degree in electrical engineering, computer engineering, mathematics or a related discipline may be substituted for four years of general experience.",None Found,"Imagine being able to shape the future of cyber… on a team that powers the recruitment of the most talented and motivated employees in the world through technological innovations. This is the reality of being part of AG Grace, Inc. We need leaders who want to discover, enhance, capture and counter cyber activity, who have a bias for action, and who have a deep desire to build….to make the previously impossible possible. Is that you?
You’ll work on cutting edge technology and provide Solutions, Insights and Deliver professional services and solutions and that our customers have come to expect from AG Grace, Inc. You should be somebody who enjoys working on complex system software, is customer-centric, and feels strongly about building a software system that maximizes the value of cloud computing. As a developer on the team you’ll collaborate with sharp engineers to drive improvements to cyber analysis technology, design and develop new services and solutions, build and track metrics to ensure high quality of results.

Experience
At least five (5) years of general experience in software development/engineering, including requirements analysis, software development, installation, integration, evaluation, enhancement, maintenance, testing, and problem diagnosis/resolution.
At least three (3) years of experience developing software with high level languages such as Java, C, C++.
At least three (3) years of experience developing software in UNIX/Linux (Red Hat versions 3-5+) operating systems.
At least three (3) years of experience in software integration and software testing, to include developing and implementing test plans and test scripts.
At least two (2) years of experience with distributed scalable Big Data Store (NoSQL) such as HBase, Cloud Base/Accumulo, Big Table, etc., as well as two (2) years of experience with the Map Reduce programming model, the Hadoop Distributed File System (HDFS), and technologies such as Hadoop, Hive, Pig, etc
Demonstrated work experience with Serialization such as JSON and/or BSON
Demonstrated work experience with developing restful services, Ruby on Rails framework, LDAP protocol configuration management and cluster performance management (e.g. Nagios)
Demonstrated work experience developing solutions integrating and extending FOSS/COTS products.
Demonstrated technical writing skills and shall have generated technical documents in support of a software development project
Demonstrated work experience with Source Code Management {e.g. Git, Stash, or Subversion, etc.).
TS/SCI Full Scope Poly required
Education
A Bachelor’s degree in electrical engineering, computer engineering, mathematics or a related discipline may be substituted for four years of general experience.
AG Grace, Inc. is dedicated to developing our nation's future and protecting our critical infrastructure resources. We provide a full range of IT services and solutions that help improve our client's ability to reduce risk, improve performance and provide mission assurance. We assist our customers in solving the problems of today and tomorrow. Come be a part of the Future as we help our customers execute their mission."
1,Big Data Engineer INTELF8,"Chantilly, VA",Chantilly,VA,None Found,None Found,"Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark
Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting
Experience with data warehousing tools and technologies
Ability to work within UNIX/Linux operating systems
AWS experience a plus
This position requires U.S. Citizenship and an active TS/SCI security clearance",None Found,"Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark
Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting
Experience with data warehousing tools and technologies
Ability to work within UNIX/Linux operating systems
AWS experience a plus
This position requires U.S. Citizenship and an active TS/SCI security clearance",None Found,None Found,"Data Works is looking for senior Big Data Engineers able to lead the way in tackling the most difficult engineering challenges in Big Data systems
Responsibilities
Data Works is seeking a Big Data Engineer with demonstrated experience in leading large scale data warehousing projects. A successful candidate will be strong in Map Reduce, Java, and possess an understanding of data science concepts such as machine learning and trend analysis. Candidate should also be familiar with indexing products such as Lucene and Elasticsearch. Relevant certifications considered but not required.
Required Qualifications
Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark
Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting
Experience with data warehousing tools and technologies
Ability to work within UNIX/Linux operating systems
AWS experience a plus
This position requires U.S. Citizenship and an active TS/SCI security clearance
E3/Sentinel is an equal opportunity employer and Vietnam Era Veterans Readjustment Assistance Act (VEVRAA) federal contractor. All qualified applicants receive consideration for employment without regard to race, color, religion, gender, national origin, age, sexual orientation, gender identity, protected veteran status, or status as a qualified individual with a disability. E3/Sentinel hires and promotes individuals solely on the basis of their qualifications for the job to be filled."
2,Data Engineer,"Arlington, VA",Arlington,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Software/Data Engineer who joins Sila will support a unique mission that leverages leading technology tools to facilitate the highly efficient integration of data and advanced analytics.
In this role, you will architect an API Gateway and oversee application integration across the enterprise. Your results, as a Software/Data Engineer, will enable access to or submission of heterogeneous data, exposure of analytic services through standard interfaces, and integration with external partners.

Collaborating with data scientists, data engineers, and client stakeholders, you will work together to define the requirements that drive the API strategy. Your existing, personal knowledge of distributed application architectures, to include APIs, web services, microservices, and asynchronous event protocols contributes substantially to this endeavor.

The successful candidate has the option to work on-site in Bethesda, MD or Reston, VA when not at Sila’s office in Arlington, VA.

Candidates must be currently authorized to work in the United States without the need for employment-based visa sponsorship now or in the future.
You will:
Architect, build, and manage API’s supporting on premise and cloud platform environments such as AWS.
Contribute to the conceptual and physical design of application integration using APIs and events
Develop and test an API gateway that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale
Develop and test runtime execution of APIs
Prototyping and demonstrate concepts when necessary (e.g. mock-client apps)
Experience
Bachelor’s degree in Computer Science or Business Information Systems or equivalent educational or professional experience and/or qualifications with at least 6 years of experience in software development
Must possess a TS security clearance and be able to obtain a DoD TS/SCI clearance
At least 1 year of experience in developing REST services using Java or Python or Node.js
Experience delivering APIs for external partners and integrating with external vendors
Able to implement processes and troubleshoot to continue to improve operational stability
Experience with GitHub or GitLab
Understanding of DevOps processes
Strong communication, interpersonal skills and problem-solving skills
Desired Experience
Previous Agile SW team participation
Use of JIRA, Confluence, and Jenkins
Elasticsearch and logging framework use
Apache JMeter or Blazemeter use/knowledge
Two (2) or more years of AWS Commercial and/or AWS GovCloud experience
High-volume applications tuning experience
Knowledgeable in security protocols such as SAML and OAuth
About Sila

Sila is a technology and management consulting firm that delivers solutions to the world’s leading corporations and Federal government agencies. Our solutions expertise lies in the areas of cybersecurity, risk management, data analytics, software engineering and integration, strategy and transformation, and digital creative services. We are a values-driven company with a culture that fosters collaboration, creativity, and social responsibility. Sila employees are part of a community of vibrant, high-performing contributors who push each other to achieve the highest standard of excellence. Our team members have extensive opportunities to discover their passions and shape their own career paths, and we continually invest in employees’ growth through innovative training, mentorship, and professional development programs. Staff are quickly immersed in clients’ business challenges, work closely with emerging technologies to develop impactful solutions to these challenges, and are exposed to a variety of industries and market offerings.

We are looking for full-time employees to become an integral part of our growing team. Sila is headquartered in Washington, D.C. and has offices in Chicago, IL; Seattle, WA; and Shelton, CT. Sila offers a range of great benefits including a comprehensive healthcare package, 401K with matching, paid time off, and paid company holidays, as well as other unique benefits that support our staff’s active work/life balance.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.
Benefits
Health & Wellness
Comprehensive healthcare insurance options, flexible spending accounts, disability and life insurance
401k
Robust 401k plan with matching for your retirement savings
Paid Time Off
Generous PTO allowance, and we actually take it!
Professional Development
Core curriculum of training plus a variety of professional development and learning channels
Social Events
Paint nite, races, mixology classes—we have something for everyone at our monthly social events
Work/Life Balance
We understand and value the importance of life outside of work"
3,Google Technical Architect,"Arlington, VA 22209",Arlington,VA,22209,None Found,"Minimum 5 years of Consulting or client service delivery experience on Google GCP
",DevOps on an GCP platform. Multi-cloud experience a plus.,None Found,None Found,"Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google Cloud Platform (GCP) Technical Architect Delivery is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would also be responsible for developing and delivering Google GCP cloud solutions to meet todays high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Google GCP Technical Architect is a highly performant GCP Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data soltuions on cloud. Using Google GCP public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications.

Role & Responsibilities:Work with Sales and Bus Dev teams in providing Data and GCP Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
Minimum 5 years of Consulting or client service delivery experience on Google GCP
Minimum 10 years of experience in big data, database and data warehouse architecture and delivery
Bachelors degree or 12 years previous professional experience
Able to travel 100% (M-TH)
Minimum of 5 years of professional experience in 2 of the following areas:
Solution/technical architecture in the cloud
Big Data/analytics/information analysis/database management in the cloud
IoT/event-driven/microservices in the cloud
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc.:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Tensorflow & Sheets

Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Certified GCP Solutions Architect - Associate
Certified GCP Solutions Architect – Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)
Certified GCP AI/ML Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP
Strong in Java, C##, Spark, PySpark, Unix shell/Perl scripting
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
- Multi-cloud experience beyond GCP a plus - AWS and Azure

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
4,Technical Writer,"Crystal City, VA",Crystal City,VA,None Found,None Found,"
3 or more years of experience in the Information Technology field
Bachelor's degree
Excellent communications skills working with customers and engineers on a software development project
Prior experience:
Working with development teams
Working with users to create user stories and define requirements
Documenting programs and processes
Reporting status to customers and management
Identifying and evaluating risks
Technologies:
MS Office Suite
JIRA and Confluence, or a similar tracking and documentation system
SharePoint, or a similar documentation system
Must currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence (CI) polygraph examination
","
Experience working on information technology programs developing user interfaces and complex data processes
Comfortable acting as a liaison between customers and a development team
Agile development certifications (scrum master, product owner)
","
Determine the needs of end users of technical documentation
Study product samples and talk with product designers and developers
Work with technical staff to make products easier to use and thus need fewer instructions
Organize and write supporting documents for products
Use photographs, drawings, diagrams, animation, and charts that increase users' understanding
Select appropriate medium for message or audience, such as manuals or online videos
Standardize content across platforms and media
Gather usability feedback from customers, designers, and manufacturers
Revise documents as new issues arise
",None Found,None Found,"Are you passionate about solving challenging problems?
Do you thrive being a critical part of an elite team of like-minded people?
How would you like for your next career move to take you to the next level?

If any of this sounds appealing, look no further.

Job Description:
We are seeking a data engineer/analyst who is excited about analyzing and solving national security related problems.

Responsibilities include:

Determine the needs of end users of technical documentation
Study product samples and talk with product designers and developers
Work with technical staff to make products easier to use and thus need fewer instructions
Organize and write supporting documents for products
Use photographs, drawings, diagrams, animation, and charts that increase users' understanding
Select appropriate medium for message or audience, such as manuals or online videos
Standardize content across platforms and media
Gather usability feedback from customers, designers, and manufacturers
Revise documents as new issues arise

Basic Qualifications:

3 or more years of experience in the Information Technology field
Bachelor's degree
Excellent communications skills working with customers and engineers on a software development project
Prior experience:
Working with development teams
Working with users to create user stories and define requirements
Documenting programs and processes
Reporting status to customers and management
Identifying and evaluating risks
Technologies:
MS Office Suite
JIRA and Confluence, or a similar tracking and documentation system
SharePoint, or a similar documentation system
Must currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence (CI) polygraph examination

Desired Skills:

Experience working on information technology programs developing user interfaces and complex data processes
Comfortable acting as a liaison between customers and a development team
Agile development certifications (scrum master, product owner)

So, what does Novetta do?

We focus on three core areas: Cyber, Entity, and Multi-Int Analytics. Our products are focused on processing and analyzing vast amounts of data in these core areas. Our services are focused on helping our customers move from complexity to clarity. At Novetta, we bridge the gap between what our customers think they can do and what they aspire to achieve.

Our culture is shaped by a commitment to our Core Values:

Integrity: We hold ourselves accountable to the highest standards of integrity and ethics.
Customer Mission Success: Customer mission success drives our daily efforts—we strive always to exceed customer expectations and focus on mission success beyond contractual commitments.
Employee Focus: We value our employees and demonstrate our commitment to them by providing clear communications, outstanding benefits, career development, and opportunities to work on problems and technical challenges of national significance.
Innovation: We believe that innovation is critical to our success – that discovering new and more effective ways to achieve customer mission success is what makes us a great company.

GET A REFERRAL BONUS FOR THE GREAT PEOPLE YOU KNOW!
With our amazing referral program, you could be eligible to earn
outstanding rewards for referring qualified new hires to Novetta.

Novetta is an equal opportunity/affirmative action employer.
All qualified applicants will receive consideration for employment without regard to sex,
gender identity, sexual orientation, race, color, religion, national origin, disability,
protected veteran status, age, or any other characteristic protected by law."
5,Big Data Engineer,"Chantilly, VA",Chantilly,VA,None Found,None Found,"
Minimum three (3) years’ experience in designing, developing, building, and implementing Big Data solutions or developing automated solutions to solve complex problems, a thoughtful ability to solve problems could outweigh years of experience.
Ability to identify and implement a data solution strategy
Demonstrates intellectual curiosity in exploring new technologies and finding creative ways to solve data management problems
Experience developing solutions with Python/Javascript/PERL
Experience/knowledge of Spark, Impala, Hadoop, Streamsets, Kafka, Rest APIs
Experience in SQL-based technologies
Experience with at least one of the following NoSQL Database technologies:
Arrango
Mark Logic
HBase
Impala
Parquet
RedShift
Experience in Linux administration/troubleshooting",None Found,"
Assist in the development and delivering of large scale data pipelines
Leverage new database technologies to improve customer data solutions
Develop and implement automated tests for data transformations and data migrations
Research and apply big data solution technologies to complex datasets; make recommendations to data science team on new technologies
",None Found,None Found,"Basic Qualifications
Bachelor's degree in software engineering or a related technical field is required (or equivalent experience), plus a minimum of 5 years of relevant experience; or Master's degree plus a minimum of 3 years of relevant experience. Agile experience preferred.

KEY SKILLS
Minimum three (3) years’ experience in designing, developing, building, and implementing Big Data solutions or developing automated solutions to solve complex problems, a thoughtful ability to solve problems could outweigh years of experience.
Ability to identify and implement a data solution strategy
Demonstrates intellectual curiosity in exploring new technologies and finding creative ways to solve data management problems
Experience developing solutions with Python/Javascript/PERL
Experience/knowledge of Spark, Impala, Hadoop, Streamsets, Kafka, Rest APIs
Experience in SQL-based technologies
Experience with at least one of the following NoSQL Database technologies:
Arrango
Mark Logic
HBase
Impala
Parquet
RedShift
Experience in Linux administration/troubleshooting
CLEARANCE REQUIREMENTS:
A TS/SCI security clearance with the ability to obtain a Polygraph is required at time of hire. Candidate must be able to obtain the Polygraph within a reasonable amount of time from date of hire. Applicants selected will be subject to a U.S. Government security investigation and must meet eligibility requirements for access to classified information. Due to the nature of work performed within our facilities, U.S. citizenship is required.
Responsibilities for this Position
A Relocation package may be available for this position.

General Dynamics Mission Systems (GDMS) is seeking motivated candidates to join our insider threat detection, systems integration team. Our mission oriented team is responsible for the design, testing, deployment, maintenance, operation, and evolution of the systems directly supporting the insider threat detection program of a large government customer in the United States Intelligence Community (USIC). GDMS has an immediate opening on the team for a motivated Big Data Engineer with a self-starter mindset who is up to date with the latest tools and techniques. The position will focus on the integration of new data management technologies and software performance tuning and troubleshooting. This is a challenging yet rewarding position that provides an opportunity to leverage cutting edge technologies in pursuit of a vital mission that protects people, sensitive information/technologies, and the national security posture of the USIC.

The majority of work will be performed in Chantilly, Virginia, which is located approximately 25 miles west of Washington D.C., near the Dulles International Airport. The selected Big Data Engineer will support a 6+ year contract that General Dynamics recently secured.

CORE RESPONSIBILITIES:
Assist in the development and delivering of large scale data pipelines
Leverage new database technologies to improve customer data solutions
Develop and implement automated tests for data transformations and data migrations
Research and apply big data solution technologies to complex datasets; make recommendations to data science team on new technologies

#CJ3
#CB
Company Overview
General Dynamics Mission Systems (GDMS) engineers a diverse portfolio of high technology solutions, products and services that enable customers to successfully execute missions across all domains of operation. With a global team of 13,000+ top professionals, we partner with the best in industry to expand the bounds of innovation in the defense and scientific arenas. Given the nature of our work and who we are, we value trust, honesty, alignment and transparency. We offer highly competitive benefits and pride ourselves in being a great place to work with a shared sense of purpose. You will also enjoy a flexible work environment where contributions are recognized and rewarded. If who we are and what we do resonates with you, we invite you to join our high performance team!"
6,Data Engineer - Card Technology,"McLean, VA",McLean,VA,None Found,None Found,"
Bachelor’s degree
At least 1 year of experience with leading big data technologies such as Apache Spark, Apache Hadoop, or Apache Kafka
At least 2 years of professional experience with data engineering concepts
",None Found,None Found,None Found,None Found,"McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer - Card Technology

Capital One (yes, the “what’s in your wallet?” company!) is rethinking the way the world approaches banking. As a Capital One Data Engineer, you will develop fast data infrastructure leveraging Apache Spark, Databricks, and Apache Kafka to manage and create information products using the data streamed from our fleet of over 2000 ATM’s . Whether a new feature or a bug fix, you will lead your work and deliver the most elegant and scalable solutions, all while learning and growing your skills. Most importantly, you’ll work and collaborate with a nimble, autonomous, cross-functional team of makers, breakers, doers, and disruptors who love to solve real problems and meet real customer needs.

The person we're looking for:
has a sense of intellectual curiosity and a burning desire to learn

is self-driven, actively looks for ways to contribute, and knows how to get things done

is deliriously customer-focused

values data and truth over ego

has a strong sense of engineering craftsmanship, takes pride in the code they write

believes that good software development includes good testing, good documentation, and good collaboration

has great communication and reasoning skills, including the ability to make a strong case for technology choices

Basic Qualifications:

Bachelor’s degree
At least 1 year of experience with leading big data technologies such as Apache Spark, Apache Hadoop, or Apache Kafka
At least 2 years of professional experience with data engineering concepts

Preferred Qualifications:

2+ years experience with AWS cloud
2+ years of experience in Java, Scala, or Python
2+ years of experience with Unix/Linux systems with scripting experience in Shell, Perl or Python
2+ years of experience building data pipelines
At least 1 year of Cloud (AWS, Azure, Google) development experience
Experience with Streaming and/or NoSQL implementation (Mongo, Cassandra, etc.) a plus

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
7,Big Data Engineer,"Crystal City, VA",Crystal City,VA,None Found,None Found,"
6 or more years of experience in software/system development
2 or more years of experience working with data technologies
Technologies:
Bachelor's in Computer Engineering, Computer Science, Information Technology, or related field, or equivalent experience
Must currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence (CI) polygraph examination
","
Interest In:
Building data services
Scaling systems on AWS - Elastic MapReduce
Building and Managing large performant data pipelines
Experience with:
Working on a cross functional team
Delivering big data solutions
Amazon Web Services
DevOps best practices - Jenkins
Debugging data pipeline issues
Comfortable with:
Interaction with analysts to help drive big data analytics
Agile development
Hands on system engineering tasks
","
Manipulate and store large amounts of data with an eye to performance and efficiency
Work with technologies in the Hadoop ecosystem such as HBase, Spark, Phoenix
Automate deployment and ingestion of data
Design and build data services for consumption by application developers
",None Found,None Found,"Are you passionate about solving challenging problems?
Do you thrive being a critical part of an elite team of like-minded people?
How would you like for your next career move to take you to the next level?

If any of this sounds appealing, look no further.

Job Description:
We are seeking a data minded hadoop engineer who is excited about solving national security related data problems. If data pipelines, cloud infrastructure, and automation is something you would be energized to work on, this is the position for you.

Responsibilities include:

Manipulate and store large amounts of data with an eye to performance and efficiency
Work with technologies in the Hadoop ecosystem such as HBase, Spark, Phoenix
Automate deployment and ingestion of data
Design and build data services for consumption by application developers

Basic Qualifications:

6 or more years of experience in software/system development
2 or more years of experience working with data technologies
Technologies:
Bachelor's in Computer Engineering, Computer Science, Information Technology, or related field, or equivalent experience
Must currently possess an active US government Top Secret clearance with the ability to obtain and maintain SCI access within a reasonable, customer-mandated time frame. Must be willing and able to pass a counterintelligence (CI) polygraph examination

Desired Skills:

Interest In:
Building data services
Scaling systems on AWS - Elastic MapReduce
Building and Managing large performant data pipelines
Experience with:
Working on a cross functional team
Delivering big data solutions
Amazon Web Services
DevOps best practices - Jenkins
Debugging data pipeline issues
Comfortable with:
Interaction with analysts to help drive big data analytics
Agile development
Hands on system engineering tasks

So, what does Novetta do?

We focus on three core areas: Cyber, Entity, and Multi-Int Analytics. Our products are focused on processing and analyzing vast amounts of data in these core areas. Our services are focused on helping our customers move from complexity to clarity. At Novetta, we bridge the gap between what our customers think they can do and what they aspire to achieve.

Our culture is shaped by a commitment to our Core Values:

Integrity: We hold ourselves accountable to the highest standards of integrity and ethics.
Customer Mission Success: Customer mission success drives our daily efforts—we strive always to exceed customer expectations and focus on mission success beyond contractual commitments.
Employee Focus: We value our employees and demonstrate our commitment to them by providing clear communications, outstanding benefits, career development, and opportunities to work on problems and technical challenges of national significance.
Innovation: We believe that innovation is critical to our success – that discovering new and more effective ways to achieve customer mission success is what makes us a great company.

GET A REFERRAL BONUS FOR THE GREAT PEOPLE YOU KNOW!
With our amazing referral program, you could be eligible to earn
outstanding rewards for referring qualified new hires to Novetta.

Novetta is an equal opportunity/affirmative action employer.
All qualified applicants will receive consideration for employment without regard to sex,
gender identity, sexual orientation, race, color, religion, national origin, disability,
protected veteran status, age, or any other characteristic protected by law."
8,Senior Data Engineer,"Sterling, VA 20166",Sterling,VA,20166,None Found,None Found,None Found,"Bachelor's Degree or higher in Computer Sciences or similarMinimum of 5-6 years Software Industry experience3+ years of development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline or Airflow, S3, Cloud Formation and CLI (must to have ) and Jenkins4+ years of development experience with Apache Spark, Presto, SQL and NoSQL Implementation5+ years of extensive working knowledge in different programming Scala ( Must ), Shell and Python (Must).Proficiency working with structured, semi-structured and unstructured data sets including social, web logs and real time streaming data feedsAble to tune Big Data solutions to improve performance and end-user experienceKnowledge on Visualization and Data Science Tools.Expert level usage with Jenkins, GitHub is preferredSpark developer certification is a plusAbility and eagerness to constantly learn and teach othersExperience in the media industry is a plusMust have the legal right to work in the United State",None Found,"Bachelor's Degree or higher in Computer Sciences or similarMinimum of 5-6 years Software Industry experience3+ years of development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline or Airflow, S3, Cloud Formation and CLI (must to have ) and Jenkins4+ years of development experience with Apache Spark, Presto, SQL and NoSQL Implementation5+ years of extensive working knowledge in different programming Scala ( Must ), Shell and Python (Must).Proficiency working with structured, semi-structured and unstructured data sets including social, web logs and real time streaming data feedsAble to tune Big Data solutions to improve performance and end-user experienceKnowledge on Visualization and Data Science Tools.Expert level usage with Jenkins, GitHub is preferredSpark developer certification is a plusAbility and eagerness to constantly learn and teach othersExperience in the media industry is a plusMust have the legal right to work in the United State","Position Summary
Our Team
As Discovery Inc’s portfolio continues to grow – around the world and across platforms – the Global Technology & Operations team is building media technology and IT systems that meet the world class standard for which Discovery is known. GT&O builds, implements and maintains the business systems and technology that are critical for delivering Discovery’s products, while articulating the long-term technology strategy that will enable Discovery’s growing pay-TV, digital terrestrial, free-to-air and online services to reach more audiences on more platforms.

From Amsterdam to Singapore and from satellite and broadcast operations to SAP, we are driving Discovery forward on the leading edge of technology.
The Global Data Analytics team enables Discovery to turn data into action. Using big data platforms, data warehousing and business intelligence technology, audience data, advanced analytics, data science, visualization, and self-service analytics, this team supports company efforts to increase revenue, drive ratings, and enhance consumer engagement.

The Role
The Sr Data Engineer should be a technical contributor who has hands-on knowledge of all phases in building large-scale cloud based distributed data processing systems and applications. You will be part of the Global Data & Analytics engineering technology team and will partner closely with a team of data scientists, business analysts & data engineers leading Discovery’s cloud based Big Data & Analytics strategy.
You’ll work on implementing complex AWS based big data projects with a focus on collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into insights using multiple technology platforms. Therefore, this role requires an understanding of how a secure big data cloud environment is architected to gain real insights faster, with less friction and complexity. The Sr. data engineer should be passionate about working with cutting edge technologies in solving problems and developing prototypes using different open source tools for the selected solutions.

You’ll need to be an innovative forward-thinker who will help lead end-to-end execution of data engineering initiatives and contribute directly to existing and emerging business strategies and goals. Creativity, Attention to detail and ability to work in a collaborative team environment are essential.
The Sr. Data Engineer will work closely with the Data Engineering head to decide on needed infrastructure architecture and software design needs and act according to the decisions.
Responsibilities
1. Lead the design, implementation, and continuous delivery of pipelines using distributed AWS based big data technologies supporting data processing initiatives across batch and streaming datasets
2. Responsible for development using Scala , Python languages and Big Data Frameworks such as Spark, EMR, Presto, AWS Athena,Kafka, , and Kinesis
3. Provide administrative support on deployed AWS platform components
4. Identify, evaluate and implement cutting edge big data pipelines and frameworks required to provide requested capabilities to integrate external data sources and APIs
5. Review, analyse and evaluate market requirements, business requirements and project briefs in order to design the most appropriate end-to-end technology solutions
6. Process and manage high volume real time customer interaction streams
7. Provide architectural support by building Proof of Concepts & Prototypes
8. Self-Starter to deliver data engineering solutions to optimize both the cost and existing solution
9. Stay current with emerging technologies and industry trends
Requirements
Bachelor's Degree or higher in Computer Sciences or similarMinimum of 5-6 years Software Industry experience3+ years of development experience with AWS services Must have EC2, EMR , RedShift, Data Pipeline or Airflow, S3, Cloud Formation and CLI (must to have ) and Jenkins4+ years of development experience with Apache Spark, Presto, SQL and NoSQL Implementation5+ years of extensive working knowledge in different programming Scala ( Must ), Shell and Python (Must).Proficiency working with structured, semi-structured and unstructured data sets including social, web logs and real time streaming data feedsAble to tune Big Data solutions to improve performance and end-user experienceKnowledge on Visualization and Data Science Tools.Expert level usage with Jenkins, GitHub is preferredSpark developer certification is a plusAbility and eagerness to constantly learn and teach othersExperience in the media industry is a plusMust have the legal right to work in the United State
Sterling, Virginia, VA"
9,Senior Software Developer - TS/SCI w/Poly,"McLean, VA 22102",McLean,VA,22102,None Found,None Found,None Found,None Found,None Found,None Found,"Description
Job Description:
Leidos has a need for a Data Engineer to support the development of a data lake for a project located in Mclean, VA.
In this role, candidate will work at a customer site to support the agile development of tools and leverage standard tools (particularly Apache-NIFI) for Extract, Transform, and Loading data between databases for the sponsor. The successful candidate will create custom code to quickly extract, triage, and exploit data across domains in support of analytic work while supporting the strategic development of replicable processes. The successful candidate will use NIFI to ETL data into a secure Hadoop environment. They must write NIFI processors or, in instances where NIFI cannot be implemented, write custom Java code to ingest existing and new data sources. The candidate will conduct product usability tests and must work efficiently with a cross functional team members to include analysts, data scientists, project managers, and software solutions integrators.
Required Education and Experience:
o Must have a TS/SCI with Poly to be considered
o Masters Degree and 15+ years of experience in data engineering and/or database administration.
Required skills are:
o Experience with Apache-NIFI, Kafka, and Spark Streaming for ETL work
o UNIX
o Familiarity with HBase, solr, Spark, Oozie, and Impala
o Java and Python proficient
o Understanding and proficiency in cross-domain solutions (ETLing data from unclassified to classified systems and across classified environments)
o Agile development and proficiency in continuous integration/delivery tools such as Jenkins, Artifactory, and Git
Preferred skills:
o Proficiency with AWS and container technologies such as Docker desired but not required.
External Referral Bonus:
Eligible
Potential for Telework:
No
Clearance Level Required:
Top Secret/SCI with Polygraph
Travel:
Yes, 10% of the time
Scheduled Weekly Hours:
40
Shift:
Day
Requisition Category:
Professional
Job Family:
Database Management
Leidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.
Pay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.
Leidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.
All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws."
10,Data Engineer,"Annapolis Junction, MD",Annapolis Junction,MD,None Found,None Found,"
5+ years of related experience
Active TS/SCI clearance
DoD 8570 compliance or information assurance certification.",None Found,None Found,"
5+ years of related experience
Active TS/SCI clearance
DoD 8570 compliance or information assurance certification.",None Found,"Job Description
This position is not yet funded.
Performs data analysis, interpretation, and management duties. Develops rules and methodologies for data collection and analysis
Researches and evaluates new concepts and processes to improve performance.
Analyzes cross-functional problem sets, identifies root causes and resolves issues.
Assists more junior level technicians, specialists, and managers in their activities.
Works individually, actively participates on integrated teams, and leads multiple tasks, projects or teams.
Oversees and monitors performance, and when required, takes steps to resolve issues.
Directs multiple teams through to project completion.
Provides guidance and direction to lower level technicians, specialists, and managers.
Education
Bachelor’s Degree
Qualifications
5+ years of related experience
Active TS/SCI clearance
DoD 8570 compliance or information assurance certification.
For more than 50 years, General Dynamics Information Technology has served as a trusted provider of information technology, systems engineering, training and professional services to customers across federal, state, and local governments, and in the commercial sector. Over 40,000 GDIT professionals deliver enterprise solutions, manage mission-critical IT programs and provide mission support services worldwide. GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class."
11,Federal - Data Visualization Specialist,"Arlington, VA 22209",Arlington,VA,22209,None Found,"
Minimum 2 years of experience working with end users to create mockups and design documents
Minimum 2 years of experience with Tableau, PowerBI, QlikView and/or QlikSense
Bachelor’s degree
No Dual Citizenship","
Minimum 2 years of experience working with end users to create mockups and design documents
Minimum 2 years of experience with Tableau, PowerBI, QlikView and/or QlikSense
Bachelor’s degree
No Dual Citizenship",None Found,None Found,None Found,"Organization: Accenture Federal Services
Location: Arlington, VA

Accenture Federal Services, a wholly owned subsidiary of Accenture LLP, is a U.S. company with offices in Arlington, Virginia. Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations. Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.

We believe that great outcomes are everything. It’s what drives us to turn bold ideas into breakthrough solutions. By combining digital technologies with what works across the world’s leading businesses, we use agile approaches to help clients solve their toughest problems fast—the first time. So, you can deliver what matters most.
Count on us to help you embrace new ways of working, building for change and put customers at the core. A wholly owned subsidiary of Accenture, we bring over 30 years of experience serving the federal government, including every cabinet-level department. Our 7,200 dedicated colleagues and change makers work with our clients at the heart of the nation’s priorities in defense, intel, public safety, health and civilian to help you make a difference for the people you employ, serve and protect.

As a Visualization Specialist, you will build custom dashboards to visualize data. You will also create proof-of-concept solutions to communicate analytics developed by the data scientist. You will assist the data engineer with data preparation.

What you’ll be doing:
data analytics and user facing dashboard design
translate data analysis into business recommendations
building dashboards
Tableau

Basic Skills & Qualifications:
Minimum 2 years of experience working with end users to create mockups and design documents
Minimum 2 years of experience with Tableau, PowerBI, QlikView and/or QlikSense
Bachelor’s degree
No Dual Citizenship
An active security clearance or the ability to obtain one may be required for this role.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.
Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
12,"Data Engineer, Mid","Herndon, VA",Herndon,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Key Role:
Work on cutting edge projects from genomic research to counter threats. Perform activities that include data architecture, building data and analytic platforms, building out extract, transform, and load (ETL) pipelines and data access services, and ensuring data is discoverability and of good quality. Support the assessing, designing, building, and maintaining of scalable data platforms that use the latest and best in Big Data tools. Perform analytical exploration and examination of data from multiple sources of data. Enforce data governance best practices in the data platform. Work with a multi-disciplinary team of analysts, data engineers, data scientists, developers, and data consumers in an agile fast-paced environment that is pushing the envelope of cutting edge Big Data implementations.
Basic Qualifications:2+ years of experience with coding using one or more of the following: Java, Scala, Python, or RExperience with developing and deploying ETL pipelines using Apache SparkExperience in interfacing with modern relational databases, including MySQL or or PostgreSQLExperience with Big Data platforms, including Hadoop, AWS, Azure, or DataBricksAbility to obtain a security clearanceBA or BS degree
Additional Qualifications:
Experience with Agile software developmentExperience with NoSQL data stores, including HBase, MongoDB, JanusGraph or Neo4J, and CassandraExperience with ETL tools, including StreamSets, NiFi, and TalandExperience in working with enterprise production systemsAbility to have a positive, can-do attitude to solve the challenges of tomorrowAbility to learn technical concepts and communicate with multiple functional groupsPossession of excellent oral and written communication skillsBA or BS degree in CS, Information Systems, Information Systems, or a related fieldAWS Certification or equivalent
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
We’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change.
SIG2017"
13,Master Data Engineer (Machine Learning Integrations),"McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Master Data Engineer (Machine Learning Integrations)

Job Description
Card ML (Card Machine Learning) is focused to set the standard for data quality to power real-time, automated intelligence, centered on a longitudinal, human-centric view. Lead an enterprise alliance to deliver a breakthrough ML platform. World class thought leadership on NLP (Natural Language Processing), voice science, proactive engagement, active influence, fairness and explainability in AI and more!

Responsibilities & Expectations:
Lead a team of software and data engineers to apply lean development practices to develop software products that help us build and run machine learning applications.
Design and implement application frameworks, APIs, libraries and services that perform at scale using existing and emerging technology platforms and standards like Kubernetes, Docker, Kafka, gRPC.
Lead design and development of automation workflows, set and expect high standards for code reviews to make sure the software is rigorously designed, elegantly coded, and effectively tuned for platform performance, and assess the overall quality of delivered components.
Work as a partner with product owners to prioritize features. Work with other tech teams to integrate solutions across the organizations. Lead the tech team to make decisions for the program.
Demonstrate strong verbal and written communication skills, to address the dynamic nature of collaboration with customers, vendors, and other engineering teams.

We’re looking for self-starters, those who can work independently, with, and across, teams.

About the candidate:
Ability to research, select and propose the right technology or tool for the task
Influencing organization’s leaders to help resolve critical architecture and data challenges
Is a problem solver with a curiosity about technology. Highly creative and curious technologist, one with excellent research skills
Is a leader that team members can lean on
Continues to learn through iterative delivery
Is not intimidated by challenges; the candidate will be expected to research and develop cutting-edge solutions
Values continuous integration and deployment. Knows how to automate processes to quickly generate value for our customers
Loves learning new technologies and mentoring other engineers

Basic Qualifications:
Bachelor’s Degree
At least 6 years of combined experience using any combination of the following programming languages: Java, Python, Golang, Scala or JavaScript
At least 3 years of experience developing applications using Agile principles
At least 1 year of experience in developing software products for cloud platforms

Preferred Qualifications:
Master’s Degree in STEM
10+ years of combined experience using any combination of the following programming languages: Java, Python, Golang, Scala or JavaScript
2+ years of experience in developing software products for cloud platforms like AWS, Azure or Google Cloud Platform
2+ years of experience with containers (e.g.: Docker)
2+ years of experience in CI/CD - DevOps practices
1+ years of experience using streaming data platforms like Kafka, RabbitMQ
1+ years of experience as a tech lead

At this time, Capital One will NOT sponsor a new applicant for employment authorization for this position."
14,Cloud Data Engineer,"Reston, VA",Reston,VA,None Found,None Found,"
Master's Degree preferred, or a Bachelor's degree and 4 years' experience, or 10 years of specialized experience
Minimum 4 years' experience working on complex data/database projects as a data analyst, data architect, or database engineer
Top Secret Clearance with ability to obtain an SCI and CI poly
","
Certified Data Management Professional (CDMP), Microsoft Certified Solutions Associate (Business Intelligence) or equivalent certification(s) strongly desired
Experience building n-tier web-based applications using SQL and non-SQL back-ends
Unix scripting (Ruby, Perl, Python, shell)
Experience with Node.js, Spark, Neo4J, Graph Databases, Mule ESB, and Rest API
Experience ingesting, analyzing, and visualizing data using Tableau
Produce clear and concise documents and diagrams capturing networking and operational procedures and storage topology using MS Visio, MS Project, MS Excel and MS Word.
",None Found,None Found,None Found,"Who YOU are:As a future Cloud Data Engineer at Plus3 IT Systems, you:

Are passionate about working on cutting edge, high profile projects and are motivated by delivering solutions on an aggressive schedule
Aren't satisfied with status quo, and regularly look for creative ways to solve problems and help your team meet commitments
Are insatiably curious – you ask why, you explore, and you're not afraid to blurt out your crazy idea
Love learning new technologies and sharing them with your team
Have a keen interest in using any and all appropriate tools, especially Cloud-based and Open Sourced, to solve the problem at hand
Have strong verbal and written communication skills, due to the dynamic nature of collaborations with customers, vendors, and other engineering teams to solve complex business problems together
Use your experience and leadership skills to motivate your teammates to deliver high quality results in a fast-paced work environment

What you'll be doing:

Work within a team of like-minded professionals to design, build and deploy critical business and mission applications in a production environment
Design and implement appropriate data environments for those applications, engineer suitable data management and governance procedures and provide production support
Automate the provisioning of environments
Provide software coding in language such as Python, Java, Groovy
Design and develop automation workflows, perform unit tests and conduct review to make sure your work is rigorously designed, elegantly coded, and effectively tuned for platform performance, and assess the overall quality of delivered components
Identify, retrieve, manipulate, relate and/or exploit multiple structured data sets from various sources

Qualifications:

Master's Degree preferred, or a Bachelor's degree and 4 years' experience, or 10 years of specialized experience
Minimum 4 years' experience working on complex data/database projects as a data analyst, data architect, or database engineer
Top Secret Clearance with ability to obtain an SCI and CI poly

Desired Technical Skills and Competencies:

Certified Data Management Professional (CDMP), Microsoft Certified Solutions Associate (Business Intelligence) or equivalent certification(s) strongly desired
Experience building n-tier web-based applications using SQL and non-SQL back-ends
Unix scripting (Ruby, Perl, Python, shell)
Experience with Node.js, Spark, Neo4J, Graph Databases, Mule ESB, and Rest API
Experience ingesting, analyzing, and visualizing data using Tableau
Produce clear and concise documents and diagrams capturing networking and operational procedures and storage topology using MS Visio, MS Project, MS Excel and MS Word.

"
15,Big Data Engineer,"Springfield, VA",Springfield,VA,None Found,None Found,"
5 years of experience
COMPTIA Security+ certification or CISSP certification
Proficiency in two or more of the following programming languages: C#, Java, .NET, Python, Perl, Ruby, or similar
Familiarity with current Agile methods",None Found,"
5 years of experience
COMPTIA Security+ certification or CISSP certification
Proficiency in two or more of the following programming languages: C#, Java, .NET, Python, Perl, Ruby, or similar
Familiarity with current Agile methods",None Found,None Found,"Overview
We are seeking a Big Data Engineer to support our customer.
Responsibilities
Designs, modifies, develops, writes and implements software systems. Participates in software and systems testing, validation, and maintenance processes through test witnessing, certification of software, and other activities as directed. Provides support to senior staff on projects/programs. Familiar with standard concepts, practices, and procedures within a variety of fields related to the project. This position takes direction from senior technical leadership.
The Big Data Engineer (BDE) is responsible for building the next generation of web applications and systems focusing on capability delivery to end users. The BDE is a member of a “big data” team of specialist within the multi-disciplinary agile development team. The BDE will manage requirements collection, software design, development and delivery – full lifecycle – in support of analysts. The BDE helps manage effective processes associated with the architecture. The BDE collaborates closely with the Agile Software Developer (ASDs), Technical Targeting Developer (TTDs), and the end user analysts to write and implement cutting edge big data algorithms and analytics. The BDE engages in software solution planning and creation to ensure capabilities are delivered using the latest available technologies and methods. The BDE will operate in a “RAD/JAD” environment in which tasks are rapidly defined and then executed to insure maximum user input, feedback and adoption. The BDE ensures the interoperability of the in-house capability with outside partners.
Qualifications
Minimum Qualifications:
5 years of experience
COMPTIA Security+ certification or CISSP certification
Proficiency in two or more of the following programming languages: C#, Java, .NET, Python, Perl, Ruby, or similar
Familiarity with current Agile methods
Proficiency with the following:
Multiple operating systems including: UNIX, Linux, Windows, Cisco IOS, etc.
Machine learning, data mining, and knowledge discovery
Analytic algorithm design and implementation
ETL processes; including document parsing techniques
Networking, computer, and storage technologies
Using or designing RESTful APIs, SOAP, XML
Developing large cloud software projects, preferably in Java, Python or C++ language
Java/J2EE, multithreaded and concurrency systems
Multi-threaded, big data, distributive cloud architectures and frameworks including Hadoop, MapReduce, Cloudera, Hive, Spark, Elasticsearch, etc. for the purposes of conducting analytic algorithm design and implementation
NoSQL database such as Neo4J, Titan, Mongo, Cassandra, and hBase
AWS Services (EC2, Network, ELB, S3/EBS, etc.)
Processing and managing large data sets (multi PB scale)
Web services environment and technologies such as XML, KML, SOAP, and JSON
Proficiency in trouble-shooting in very complex distributed environments including following stack traces back to code and identifying a root cause
Preferred Qualifications:
Education – Masters Degree in Computer Science or related field (e.g. Statistics, Mathematics, Engineering) – but a technical BS degree will suffice
Distributed computing-based certifications
Proficiency with the following:
Management/tracking utilities such as Jira, Redmine, or similar
Running Internet facing or Service Level Agreement (SLA'd) auto-deployed environments
Real-time media protocols (Real-time Transport Protocol (RTP), Secure Real-time Transport Protocol (SRTP))
Data transfer systems such NiFi
Text processing: NPL, NER, entity retrieval (e.g. Solr/Lucene), topic extraction, summarization, clustering, etc.
Certification from an Agile certified institute, International Consortium for Agile, Scaled Agile Academy, Scrum Alliance, Scrum.org, International Scrum Institute, ScrumStudy, Project Management Institute - Agile Certified Practitioner, or similar XP/Scrum certification or training is desired
Support to SOF; Previous experience with technology, intelligence and cyber under the umbrella of USSOCOM
Education:
Bachelor of Arts or Bachelor of Science in Computer Science or related fields (e.g. Statistics, Mathematics, Engineering), or equivalent in years of experience, or demonstrates adequate knowledge for the position.

Clearance Requirements:
Must have active TS/SCI clearance
Physical Demands - The physical demands described here are representative of those that may need to be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

While performing the duties of this Job, the employee is regularly required to sit and talk or hear. The employee is frequently required to walk; use hands to finger, handle, or feel and reach with hands and arms. The employee is occasionally required to stand; climb or balance and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 20 pounds.
HII-MDIS, formerly Fulcrum, Fulcrum is an equal opportunity employer and gives consideration for employment to qualified applicants without regard to race, color, religion, sex, national origin, disability or protected veteran status. EOE of Minorities/Females/Veterans/Disability

“CJ” *MON*"
16,Data Engineer : TS/SCI Clearance w/ Poly,"Chantilly, VA 20151",Chantilly,VA,20151,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description
The successful candidate will use advanced technical skills to triage, normalize, and exploit raw data on an ongoing basis. He or she must be able to apply advanced knowledge of relational databases, primarily MySQL, and associated tooling to develop performance data ingest and computability problems. The candidate will also have experience with NoSQL concepts, full text indexing and open source search engines. Manages the design and development of computer software applications.
1. Directly, and through subordinate supervisors, manages employees engaged in the design and development of computer software applications.
2. Subject matter expert in My SQL, and at least five years of demonstrated experience working with large databases.
3. Expert level ability writing advanced SQL queries and extensive experience in query optimization.
4. Advanced experience in scalable data and full text indexing solutions such as Apache, Solr, or Elastic Search
5. Experience with Linux user and comfortable adminstering databases from the Linux command line.
6. Demonstrated experience with database backup, restoration, and disaster recovery.
7. Primarily supervises exempt Software Engineers.
8. Develops schedules and assigns work to meet critical customer deadlines.
9. Ensures that proper records and other documentation are maintained.
10. May also perform complex software development and design work.
11. Maintains current knowledge of relevant technology as assigned.
12. Participates in special projects as required.
Education
Bachelors Degree in Computer Science, Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience.
Qualifications
8-10 years of related software development experience, including supervisory experience.
Experience working with large volumes of data
Familiarity with cybersecurity concepts, and has a software development background
Familiarity with distributed databases such as Hadoop (HDFS), and cloud technologies (Open Stack, Kubernetes)
Comfortable writing scripts in a robust, high-level language such as Python
For more than 50 years, General Dynamics Information Technology has served as a trusted provider of information technology, systems engineering, training and professional services to customers across federal, state, and local governments, and in the commercial sector. Over 40,000 GDIT professionals deliver enterprise solutions, manage mission-critical IT programs and provide mission support services worldwide. GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class."
17,Data Engineer,"Bethesda, MD 20889",Bethesda,MD,20889,None Found,None Found,None Found,None Found,None Found,None Found,"Description
Job Description:
Are you ready to join Leidos all-star team? Through training, teamwork, and exposure to challenging technical work, let Leidos show how to accelerate your career path.
The Leidos Innovations Center has an exciting opening for you, our next Data Engineer to assist with the design, development and implementation of alternative data ingestion pipelines to augment the National Media Exploitation Center (NMEC) data services in Bethesda, MD. The DOMEX Data Discovery Platform (D3P) program is a next generation machine learning pipeline platform providing cutting edge data enrichment, triage, and analytics capabilities to Defense and Intelligence Community members. This engineer will collaborate as part of a cross-functional Agile team to create and enhance data ingestion pipelines and addressing Big data challenges.
You will work closely with the chief architect, systems engineers, software engineers, and data scientists on the following key tasks:Provide Extraction, Transformation, and Load (ETL) experience coupled with enterprise search capabilities to solve Big Data challengesDesign and implement high-volume data ingestion and streaming pipelines using Open Source frameworks like Apache Spark, Flink, Nifi, and Kafka on AWS CloudLeverage strategic and analytical skills to understand and solve customer and business centric questionsCreate prototypes and proofs of concept for iterative developmentLearn new technologies and apply the knowledge in production systemsMonitor and troubleshoot performance issues on the enterprise data pipelines and the data lakePartner with various teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives
To be successful in this role you need these skills (required):
BS in Computer Science, Systems Engineering, or related technical field or equivalent experience with at least 4+ years in systems engineering or administration (2+ years with a MS/MIS Degree).Must have an active Top Secret security clearance and able to obtain a TS/SCI with Polygraph.2 years of experience with big data tools: Hadoop, Spark, Kafka, NiFi2 years of experience with object-oriented/object function scripting languages: Python (preferred) and/or Java2 years of experience with and managing data across relational SQL and NoSQL databases like MySQL, Postgres, Cassandra, HDFS, Redis, and Elasticsearch2 years of experience working in a Linux environmentExperience working with and designing REST APIsExperience developing data ingest workflows with stream-processing systems: Spark-Streaming, Kafka Streams and/or FlinkExperience transforming data in various formats, including JSON, XML, CSV, and zipped filesExperience developing flexible ontologies to fit data from multiple sources and implementing the ontology in the form of database mappings / schemasGood interpersonal and communication skills necessary to work effectively with customers and other team members.
It would be great if you have specific experiences and skills with the following (preferred):
Data engineering experience in Intelligence Community or other government agenciesExperience with Microservices architecture components, including Docker and Kubernetes.Experience with AWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena and/or GlueExperience with Jira, Confluence and extensive experience with Agile methodologies.Knowledge about security and best practices.Experience developing flexible data ingest and enrichment pipelines, to easily accommodate new and existing data sourcesExperience with software configuration management tools such as Git/Gitlab, Salt, Confluence, etc.Experience with continuous integration and deployment (CI/CD) pipelines and their enabling tools such as Jenkins, Nexus, etc.Detailed oriented/self-motivated with the ability to learn and deploy new technology quickly
Additional Program Information
The DOMEX Data Discovery Platform (D3P) program will advance the state of the art in mission-focused big data analytics tools and micro-service development spanning the breadth of Agile sprints to multiyear research and development cycles. We are looking for you to have a demonstrated aptitude for problem solving complex technical issues, identifying, transforming, thinking outside the box, and a strong sense of accountability. Have a mix of technical excellence, intellectual curiosity, communications skills, customer-focus, and operational experience to improve the performance and user adoption of high-end data analytics platforms in partnership with a highly qualified, highly motivated team. Be motivated, self-driven team player who can multi-task and interact well with others and advise/consult with other team members on systems engineering and software development related issues.
LInC
D3P
External Referral Eligible
External Referral Bonus:
Eligible
Potential for Telework:
No
Clearance Level Required:
Top Secret
Travel:
Yes, 10% of the time
Scheduled Weekly Hours:
40
Shift:
Day
Requisition Category:
Professional
Job Family:
Software Development
Leidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.
Pay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.
Leidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.
All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws."
18,Data Engineer (IT Engineer IV),"Reston, VA",Reston,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for individuals with a high-level analytical skills who are able to perform exhaustive data analysis. Extended knowledge of system engineering and hosting is a must, an energetic team player self-motivated and able to work with minimum supervision.

Duties:
Work across applications, infrastructure, shared services and other teamsGather infrastructure, application, hosting and tech stacksSynthesize data, find coupling and sharing patters, anomalies and behaviorsPerform risk analysis based on infrastructure sharing and inter-application dependenciesGenerate reports and presentations

Critical applications across the Enterprise feed upstream and downstream systems – some of these applications are being tested in our contingency site. This person must review, gather and evaluate upstream and downstream dependencies on production systems are identified prior to testing to ensure there is not an outage. Must be familiar with Database and Applications: Data guard and SRDF. Infrastructure, Database, Networking, Hosting and Engineering skills (UNIX)."
19,Data Engineer,"Reston, VA",Reston,VA,None Found,None Found,None Found,"
Expertise in Java or Scala and in-depth knowledge of the JVM
Expertise in Apache Spark or expertise in Computer Science fundamentals, such as analysis of algorithms
Expertise in enterprise integration patterns and workflow management
Experience and practical knowledge of OOP design patterns
Distributed System Development for large-scale applications
Experience with continuous integration and testing
Experience with agile methodologies and short release cycles
Strong attention to detail, good work ethic, ability to work on multiple projects simultaneously, and good communication skills","
Design and develop data services, as part of an agile/scrum team
Apply best practices in continuous integration and delivery
Experience in translating high-level, ambiguous business goals into working software solutions.
Design and develop stream and batch processing data pipelines
Work with product managers and other engineers to implement and document complex and evolving requirements","
Technical Bachelor’s Degree required, e.g. Comp Sci, Engineering, Math","
Technical Bachelor’s Degree required, e.g. Comp Sci, Engineering, Math","At Resonate we are working hard to disrupt the marketing and advertising landscape forever. We are replacing the slow, incomplete and siloed marketing research and insight tools of the past with modern technology and machine learning to provide a more accurate, comprehensive and real-time view of the US consumers with integrations across the ecosystem.

We have an excellent engineering culture that focuses on results, values collaboration and loves solving hard problems. We are a team of voracious learners who believe that technology is a journey, not a destination and we actively support ongoing education and experimentation.

If using cutting-edge technologies and transforming an industry sounds interesting to you, then we should talk.

About the Position

As a Software Engineer, you will be working as a member of our Data Engineering team to jointly design and implement highly available data services and pipelines. This is an ideal job if you have proven experience as a technical professional and have delivered production systems based on big data solutions.

If you are an engineer passionate for technology who wants to be part of an intensely skilled team, values total ownership of your work, and can’t imagine a day without coding, we want to speak to you! We're looking for a creative, focused, technically curious individual who enjoys both design as well as working hands-on with the code.

Key Responsibilities
Design and develop data services, as part of an agile/scrum team
Apply best practices in continuous integration and delivery
Experience in translating high-level, ambiguous business goals into working software solutions.
Design and develop stream and batch processing data pipelines
Work with product managers and other engineers to implement and document complex and evolving requirements
Required Skills and Experience

Expertise in Java or Scala and in-depth knowledge of the JVM
Expertise in Apache Spark or expertise in Computer Science fundamentals, such as analysis of algorithms
Expertise in enterprise integration patterns and workflow management
Experience and practical knowledge of OOP design patterns
Distributed System Development for large-scale applications
Experience with continuous integration and testing
Experience with agile methodologies and short release cycles
Strong attention to detail, good work ethic, ability to work on multiple projects simultaneously, and good communication skills
Desired Experience
Experience with cloud technologies (AWS)
Experience working on a SAAS Product in a commercial environment
Experience in digital media, online advertising, or reporting/analytical applications
Experience with peta-byte scale data warehousing is a strong plus
Educational Requirements
Technical Bachelor’s Degree required, e.g. Comp Sci, Engineering, Math

About Resonate:

Resonate is a consumer intelligence and activation company that has helped hundreds of clients better understand and more cost-effectively reach consumers. Resonate’s solution leverages its first-party data on consumers’ underlying motivations, values and beliefs—combined with demographic and behavioral data—to help organizations learn what drives consumers’ decisions to support certain brands, political campaigns or causes.

Founded in 2008 and headquartered in Reston, Virginia, Resonate is privately held and backed by Revolution Growth, Greycroft Partners, and iNoviaCapital. Resonate has been named one of the best places to work in VA for the last 5 years.

More Information:

Find out more about our story at www.resonate.com.

Resonate offers a competitive compensation and benefits package."
20,Sr. Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering).5+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.5+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.2+ years of experience in scripting languages like Python etc.Demonstrated strength in data modeling, ETL development, and Data warehousing.Experience with Redshift, Oracle, etc.Experience with AWS services including S3, Redshift, EMR and RDS.Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)Experience in working and delivering end-to-end projects independently.Knowledge of distributed systems as it pertains to data storage and computing

Enjoy Big Data, like to work on cutting edge technologies or create new, better, smarter algorithms layered on existing cool tech? Are you smart and eager and want a team that is going places (oh and working with the biggest data geeks in Amazon - Economists & Machine Learning Scientists!)? If yes keep reading!

Core AI team works with senior management on key business problems faced in retail, international retail, supply chain, traffic, search, pricing, cloud computing, third party merchants, Kindle and operations. Amazon economists apply the frontier of economic thinking to market design, pricing, forecasting, online advertising, search, supply chain network planning and other areas.
As a Data Engineer, you will build new business intelligence solutions end-to-end, with opportunities to utilize big data and develop new ways to answer varied questions. You will work with multiple stakeholders within and outside Core AI team to integrate data sources and create data infrastructures that can be used by sophisticated distributed systems and advanced statistical and ML models. A successful candidate will have a passion for innovation, interest in cutting-edge technology, and excitement about working in a high-impact domain.

Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategyExperience providing technical leadership and mentoring other engineers for best practices on data engineeringKnowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operationsMasters in computer science, mathematics, statistics, economics, or other quantitative field"
21,Sr. Data Engineer,"Arlington, VA",Arlington,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Pandera prides itself on being a leader in the analytics field, creating an artful blend of analytics and product expertise to create beautiful, data-driven products and platforms. Our mission is to remain the leader in existing and emerging advanced analytics utilizing modern and traditional analytical toolkits. We accomplish this by promoting your continuous development, both personally and professionally.
As a Sr. Data Engineer at Pandera, you are someone who can analyze data and communicate effectively with others by partnering with cross-functional teams focused on product improvements. You love wrangling messy data into an elegant solution, and working with a wide range of data to identify opportunities or unknown risks and articulate recommend solutions and strategies. You will contribute to the Data team initiatives focused on ensuring fast, reliable, and comprehensive data and serve as a trusted consultant and promote data literacy across the company. This role is a chance to have a huge impact on how millions of users collaborate.
As a Sr. Data Engineer, a typical day might include the following:

Work collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:
Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data
Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications
Assist in the selection and integration of data related tools, frameworks and applications required to expand our platform capabilities
Understand and implement best practices in the management of enterprise data, including master data, reference data, metadata, data quality and lineage.

So who exactly are we looking for? This job might be for you if:

4+ years of collective experience in data engineering, data analysis, data warehousing, data integration or business intelligence, in a similarly sized organization.
Experience with the MS SQL Server Stack (IS/RS/AS)
3+ years of experience engineering, building and administering big data and real-time streaming analytics architectures in both on premises and cloud environments (AWS, Azure, Google) leveraging technologies such as Hadoop, Spark, S3, EMR, Aurora, DynamoDB, Redshift, Neptune, Cosmos DB
4+ years of experience engineering, building and administering large-scale distributed applications

What do you get out of this? You're a hot commodity and having you on the team would be an honor to us! Here's some of the ways we pay it forward to recognize your contribution to our vision!
Be Rewarded
A competitive salary and instant vesting on 401k are only a few of the rewards for a job well done.
Be Healthy
Health, dental, and vision offered through top tier providers and unlimited sick leave to keep you feeling at the top of your game.
Be Inspired
Collaborative workspace and unlimited vacation to keep your mind fresh and ready to take on the next new idea.
Be Supported
A large network of industry experts, internal training platform, and external learning opportunities to grow your skills and experience.
Be a Team
Team outings, happy hours, passion presentations, volunteer opportunities, meetups, etc. we are creating a community to continuously share and grow as a team."
22,Data Engineer - Card Machine Learning Technology,"McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer - Card Machine Learning Technology

As a Data Engineer on the Card ML Tech team, you will ingest, store, process, analyze and explore data. That data will be messy but filled with interesting insights of high business value. These insights are typically developed using Machine Learning and delivered via business applications, and you will be involved in the design, development, and deployment of these data products. Soup to nuts.

In this role, you will be working horizontally across Capital One’s most strategic and critical initiatives. Together with the team, you will support the implementation of Machine Learning products through architecture guidance, best practices, data migration, capacity planning, implementation, troubleshooting, monitoring and much more. We rigorously refactor towards the simpler solutions. The team works with a wide range of tools and technologies. You will bring solid foundational skills, which when combined with experience and best practices, will make you a valuable member of the team. Our developers typically use technologies such as: Python, Scala, Go, Java, Spark, Flink, Kubernetes, Redis, Postgres, and AWS. But a better tool is always welcome.

Responsibilities:
Working with product owners to understand desired application capabilities and testing scenarios

Continuously improving software engineering practices

Working within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies

Leading the craftsmanship, availability, resilience, and scalability of your solutions

Bring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal and external technology communities, and mentor other members of the engineering community

Encourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity

Basic Qualifications:
At least 2 years of Software Engineering experience in Java, Python, Scala, or Go

At least 1 years of SQL, Relational or NoSQL experience

At least 1 year of Big Data experience

At least 1 year of AWS experience

Bachelor's Degree

Preferred Qualifications:
1+ year of Restful API experience

1+ year experience distributed databases

At this time, Capital One will Not sponsor a new applicant for employment authorization for this position."
23,Lead Data Engineer,"Vienna, VA",Vienna,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Towers Crescent (12066), United States of America, Vienna, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Lead Data Engineer

We are the digiTECH organization within Capital One and are responsible for the foundation for every digital experience and interaction that customers have with us. We establish a customer’s identity and ensure customer accounts, personal information and digital experience is safe and protected. We also offer a foundation and extendable template that extends a set of horizontal services and patterns to solve business problems. We have multiple platforms which integrate across the entire company and enable API’s to be used by a number of lines of business.

We are looking for a passionate technologist with a knack for delivery to join a team of engineers to build and/or implement automated, cloud-based environments and tools to support our business analytical insights and reporting agenda. We are re-imagining how analysts discover, prepare and publish data at Capital One and this is an opportunity to display knowledge of your craft by having a hand in building large-scale reliable data applications and platforms. You will be an integral part in advancing the culture of technical excellence within Capital One and creating experiences to delight our analytical associates who will use your products to make decisions critical to the business!

What you’ll do:
Help develop sustainable data solutions with current and leading gen data technologies to meet the needs of our organization and business customers

Grasp / master new technologies rapidly as needed to progress varied initiatives

Break down complex data issues and resolve them

Build robust systems with an eye on the long-term maintenance and support of the application

Work directly with the product owner and end-users to constantly deliver products in a highly collaborative and agile environment

Who you are:
Someone with a strong sense of engineering craftsmanship, and takes pride in creating work products (code, etc.)

A believer that good development includes good testing, documentation, and collaboration

A good communicator with strong reasoning skills, who can make a case for technology choices

Curious. Ask questions and desire to understand the ‘Why’ to drive the ‘How’ or ‘What’ of a solution

Self-driven, actively looking for ways to contribute, and know how to get things done

Basic Qualifications

Bachelor's Degree

At least 7 years of microservices development experience: Python, Java, Bash, or Scala

At least 3 years of experience building data pipelines, CICD pipelines, and fit for purpose data stores

At least 3 years of experience with Big Data Technologies: Apache Spark, Hadoop, or Kafka

At least 1 year of experience in Cloud technologies: AWS, Azure, OpenStack, Docker, Ansible, Chef or Terraform

Preferred Qualifications

Master’s Degree

2+ years of experience working with AWS: S3, EMR, or EC2

3+ years of experience working with data consumption patterns in SQL or Python applications

3+ years of experience working with automated build and continuous integration systems

2+ years of experience with Unix or Linux systems

1+ years of experience with NOSQL Graph Databases

Capital One will consider sponsoring a new qualified applicant for employment authorization for this position"
24,Big Data Engineer,"Rockville, MD 20850",Rockville,MD,20850,None Found,None Found,None Found,None Found,None Found,None Found,"Overview
Global video game publisher/developer headquartered in Rockville, MD, seeks a Big Data Engineer. This position works within the Enterprise BI Team and is responsible for the development of the Big Data Platform for Enterprise-wide reporting. The Big Data Engineer will be supporting a broad range of data pipelining needs from all facets of the business, including e-commerce, financial, and game event data.
The incumbent will have at least 2+ years of previous experience partnering with both technical and business teams to facilitate implementation across the enterprise. The Big Data Engineer will facilitate the creation of data pipeline processes to move data from enterprise data sources such as relational databases and log files.
Qualifications
2+ Years of Experience with a major programming language (C, Java, Scala, Python, etc)
Comfortable working with structured, semi-structured, and unstructured source data
Understanding of Amazon Web Services, especially data related components
A strong communicator and is comfortable interfacing directly with differing customers across the organization in addition to the BI team
Working experience with the SCRUM development framework
Responsibilities
Work within the Enterprise BI team, supporting the creation of data pipeline processes for ingesting data at large scales
Work directly with Data Modelers, Enterprise Architect, and Analysts, ensuring that business requirements are being met
Directly work with the Data Engineers and Data Modelers to understand the source and target structures
Coordinate with the analysts and report developers to ensure data can be easily digested by Business Intelligence tools
Be able to straddle differing subject areas such as in-game vs business data sources
Preferred Skills
Apache Spark experience a major plus (Spark RDDs, Spark DataFrames, Spark SQL)
SQL skills – able to query data sources and generate results from complex structures
A clear understanding of both row and columnar storage databases
Experience working within the VideoGame/MMO industry is desired, though candidates from outside of the industry are also welcomed
Understanding of the Free to Play/Microtransaction Business Model is a plus
A personal interest in video gaming is a plus"
25,Data Engineer- Projects (VG00420) (VG00161),"Washington, DC",Washington,DC,None Found,None Found,"Works extensively with Cisco Layer 2 and Layer 3 solutions and products.
Cisco CCNA Certification a minimum.
Familiarity and working knowledge of telecommunication circuit types (e.g., P2P T1/T3, TDM, IP Ethernet)
Understand and have the ability to implement Cisco router, switch, and ASA products.
Skills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.
Working knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.
Strong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer.
Ability to conduct research on networking products with various vendors to accommodate changing customer requirements.
Ability to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.","Works extensively with Cisco Layer 2 and Layer 3 solutions and products.
Cisco CCNA Certification a minimum.
Familiarity and working knowledge of telecommunication circuit types (e.g., P2P T1/T3, TDM, IP Ethernet)
Understand and have the ability to implement Cisco router, switch, and ASA products.
Skills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.
Working knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.
Strong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer.
Ability to conduct research on networking products with various vendors to accommodate changing customer requirements.
Ability to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.",None Found,"Works extensively with Cisco Layer 2 and Layer 3 solutions and products.
Cisco CCNA Certification a minimum.
Familiarity and working knowledge of telecommunication circuit types (e.g., P2P T1/T3, TDM, IP Ethernet)
Understand and have the ability to implement Cisco router, switch, and ASA products.
Skills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.
Working knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.
Strong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer.
Ability to conduct research on networking products with various vendors to accommodate changing customer requirements.
Ability to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.",None Found,"Job Description
Description
Position Description:
The Department of State, Bureau of Information Resource Management (IRM) Telecommunications, Wireless, and Data (TWD) Division provides its users with mission critical domestic LAN/WAN data services across multiple locations in the DC Metro Area and remote locations. In support of these services, the Senior Data Network Engineer provides data engineering support, with a particular focus on project engineering initiatives.
The Senior Data Network Engineer supports the Department of State (DoS) Vanguard Program, Service Management Office (SMO) which is responsible for administering, deploying and maintaining Cisco and products, as well as a working knowledge of encryption solutions, knowledge of local and wide-area networks in a Voice/Data converged infrastructure. This hands-on position includes engineering support for service interruptions, planned equipment installations, and infrastructure support.
This position can also include planning, designing, installing, configuring, maintaining, deploying supporting and optimizing all network hardware and software. The Senior Data Network Engineer reports directly to the TWD Voice/Data Engineering Manager, while coordinating with TWD Project Management Office (PMO) project managers, and the PMO Director.

Description of Duties:
Promptly respond to engineering support requests from the Special Projects Engineering Manager, PMO project managers, and/or PMO Director.
Demonstrate an ability to support and/or serve as the lead engineer on multiple, concurrent engineering projects.
Supports change and configuration management for all voice and data assets.
Troubleshoot, respond, and resolve escalated incidents/service requests and/or planned/unplanned outages.
Conduct Route Cause Analysis (RCA) and After-action Review (AAR) following unplanned outages.
Supports Move, Add, and Change requests for the data network to include day-to-day requests/orders and Projects.
Diagnose, troubleshoot, and resolve hardware and/or other network problems, and replace defective components associated with both data and voice infrastructures.
Interfaces with other IRM Support Teams on system/network infrastructure problems and advises management on technical improvements and/or solutions to identified problems and/or gaps.
Identifies and recommends solutions, products and services to support the enterprise initiatives, business goals and/or technical needs.
Researches, evaluates and stays current on emerging tools, techniques and technologies.
Contributes to systems infrastructure plans based on an understanding of the customer's organizational direction, technical context and State Department enterprise needs.
Contributes to the creation of new policies and procedures for Standard Operating Procedures (SOPs), and follows established SOPs and process guides.
The ideal candidate will have a strong sense of commitment to perform engineering support functions as scheduled, providing timely responses to required system support after normal business hours and on weekends.

Additionally, the candidate should have an eye for detail, ability to multi-task, organize priorities, and work in a systematic style following Standard Operating Procedures (SOPs) and guides.

VGP
Qualifications
Required Education/Experience:
Bachelors and 4+ years of experience

Strong LAN/WAN/MAN with design and migration experience in a converged environment with a focus on WAN services (ISDN, Frame-Relay, Point-to-Point, DMVPN, METRO ETHERNET, VPLS, TLS, and MPLS).
Required Experience/Skills/Attributes:
Works extensively with Cisco Layer 2 and Layer 3 solutions and products.
Cisco CCNA Certification a minimum.
Familiarity and working knowledge of telecommunication circuit types (e.g., P2P T1/T3, TDM, IP Ethernet)
Understand and have the ability to implement Cisco router, switch, and ASA products.
Skills troubleshooting access-lists, IPv4 and IPv6 issues across varying protocols such as OSPF, BGP, EIGRP and Static Routing.
Working knowledge and understanding of implementing various routing protocols to include OSPF, BGP, EIGRP and Static Routes.
Strong interpersonal, written and oral skills. From time to time, candidate may be asked to present project outline to customer.
Ability to conduct research on networking products with various vendors to accommodate changing customer requirements.
Ability to work in a team-oriented collaborative environment while being highly motivated to take the lead on projects.
Desired Experience/Skills/Attributes:
Cisco CCNP Certification (Cisco Certified Network Professional)
State Department experience a plus.
Avaya/Cisco Voice
Clearance Requirement:
Interim Secret clearance able to obtain Top Secret
Desired Qualifications


Overview
SAIC is a premier technology integrator, solving our nation's most complex modernization and systems engineering challenges across the defense, space, federal civilian, and intelligence markets. Our robust portfolio of offerings includes high-end solutions in systems engineering and integration; enterprise IT, including cloud services; cyber; software; advanced analytics and simulation; and training. We are a team of 23,000 strong driven by mission, united purpose, and inspired by opportunity. Headquartered in Reston, Virginia, SAIC has annual revenues of approximately $6.5 billion. For more information, visit saic.com. For information on the benefits SAIC offers, see Working at SAIC. EOE AA M/F/Vet/Disability"
26,Senior Data Engineer - Card Machine Learning Technology,"McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Senior Data Engineer - Card Machine Learning Technology

As a Data Engineer on the Card ML Tech team, you will ingest, store, process, analyze and explore data. That data will be messy but filled with interesting insights of high business value. These insights are typically developed using Machine Learning and delivered via business applications, and you will be involved in the design, development, and deployment of these data products. Soup to nuts.

In this role, you will be working horizontally across Capital One’s most strategic and critical initiatives. Together with the team, you will support the implementation of Machine Learning products through architecture guidance, best practices, data migration, capacity planning, implementation, troubleshooting, monitoring and much more. We rigorously refactor towards the simpler solutions. The team works with a wide range of tools and technologies. You will bring solid foundational skills, which when combined with experience and best practices, will make you a valuable member of the team. Our developers typically use technologies such as: Python, Scala, Go, Java, Spark, Flink, Kubernetes, Redis, Postgres, and AWS. But a better tool is always welcome.

Responsibilities:
Working with product owners to understand desired application capabilities and testing scenarios

Continuously improving software engineering practices

Working within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies

Leading the craftsmanship, availability, resilience, and scalability of your solutions

Bring a passion to stay on top of tech trends, experiment with and learn new technologies, participate in internal and external technology communities, and mentor other members of the engineering community

Encourage innovation, implementation of cutting-edge technologies, inclusion, outside-of-the-box thinking, teamwork, self-organization, and diversity

Basic Qualifications:
At least 4 years of Software Engineering experience in Java, Python, Scala, or Go

At least 2 years of SQL, Relational or NoSQL experience

At least 2 years of Big Data experience

At least 2 years of AWS experience

Bachelor's Degree

Preferred Qualifications:
4+ years of Restful API experience

2+ years experience distributed databases

At this time, Capital One will Not sponsor a new applicant for employment authorization for this position."
27,"Principal Systems Administrator (RHEL, PureData)","Dulles, VA 20101",Dulles,VA,20101,None Found,None Found,"Must be a US CitizenActive Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearanceMust be able to obtain DHS Suitability8+ years of relevant system administration experience4+ years experience in Red Hat Enterprise Linux (RHEL6/RHEL7 preferred) with the ability to engineer, install, administer and maintain network architectures4+ years experience with IBM PureDataProficient in RHEL OS installation, security hardening, and maintenanceExperience with administration and management of RHEL servers to include installation, configuration, optimization, backup & recoveryUnderstanding of big data and data analyticsExperience working with large structured and unstructured data setsSQL development skillsExperience with Linux/Unix tools and shell scriptsAbility to manage changes to the system and assesses the security impact of those changesFamiliarity of configuration managementExcellent research, analytical, and problem solving skillsGood communication skills including preparing and presenting results, findings and alternatives and influencing management decision making based on the best available data","Basic RHEL (6 & 7) system administrator functionsRHEL OS updatesRHEL OS security configuration / STIG procedures and updatesRHEL OS security patch updatesPureData NZ related security patch updatesPureData NZ database security patch updatesPureData NZ database user account creationPureData NZ database table creationPureData NZ database access control proceduresVulnerability assessments and penetration testing to aid in the Certification & Approval processWorking with IBM engineering staff, as necessary, to mitigate and resolve security vulnerabilities in applications and appliances",DoD 8570.1 IAT Level IIRed Hat Certified System Administrator (RHCSA)Red Hat Certified Engineer (RHCE),None Found,"Principal Systems Administrator (RHEL, PureData)
Residency Status: ALL CANDIDATES MUST BE A U.S. CITIZEN
Clearance: Active Top Secret Clearance and must be able to obtain TS/SCI, TS/SCI Preferred
Time Type: Full-Time
Relocation Fees: No
Bonus: Yes

Company Overview:
Novel Applications of Vital Information Inc. (Novel Applications) is a premier technology services company that provides solutions in the areas of Cyber Security, Information Management, Systems Integration. Novel Applications is a business that combines experience, creativity, flexibility, pragmatism, and cost-effective solutions in order to deliver measurable business value to our clients.
Headquartered in Fredericksburg Virginia, Novel Applications employs engineers, analysts, IT specialists and other professionals who strive to be the best at everything they do.
Novel Applications is an AA/EEO Employer - Minorities/Women/Veterans/Disabled.
Job Description:
NAOVI is seeking a Principal Red Hat Enterprise Linux (RHEL) and IBM PureData Systems Administrator to support the design, development, and deployment of advanced cybersecurity capabilities.

Responsibilities Include Performing the Following:
Basic RHEL (6 & 7) system administrator functionsRHEL OS updatesRHEL OS security configuration / STIG procedures and updatesRHEL OS security patch updatesPureData NZ related security patch updatesPureData NZ database security patch updatesPureData NZ database user account creationPureData NZ database table creationPureData NZ database access control proceduresVulnerability assessments and penetration testing to aid in the Certification & Approval processWorking with IBM engineering staff, as necessary, to mitigate and resolve security vulnerabilities in applications and appliances

Required Skills:
Must be a US CitizenActive Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearanceMust be able to obtain DHS Suitability8+ years of relevant system administration experience4+ years experience in Red Hat Enterprise Linux (RHEL6/RHEL7 preferred) with the ability to engineer, install, administer and maintain network architectures4+ years experience with IBM PureDataProficient in RHEL OS installation, security hardening, and maintenanceExperience with administration and management of RHEL servers to include installation, configuration, optimization, backup & recoveryUnderstanding of big data and data analyticsExperience working with large structured and unstructured data setsSQL development skillsExperience with Linux/Unix tools and shell scriptsAbility to manage changes to the system and assesses the security impact of those changesFamiliarity of configuration managementExcellent research, analytical, and problem solving skillsGood communication skills including preparing and presenting results, findings and alternatives and influencing management decision making based on the best available data

Desired Skills:
Experience with the trade-offs of various RHEL configurations on performance and data qualityAbility to support both SQL and NoSQL data management systemsExpertise in other RDBMS platforms such as Oracle RAC and SQL ServerExperience with software development using java, RESTful servicesComfortable with multilevel security systems, SELinux, and/or Cross Domain SolutionsExperience with data orchestration software such as Apache NiFi or StreamSetsExperience with distributed compute environments such as HadoopExperience working in DevOps and/or DevSecOps environmentsFamiliarity with SAFe (Scaled Agile Framework)

Required Education:
Bachelor’s degree in Systems Engineering, Computer Science, Information Systems or related technical field. Two years of related work experience may be substituted for each year of degree level education.

Desired Certifications (one or more of the following):
DoD 8570.1 IAT Level IIRed Hat Certified System Administrator (RHCSA)Red Hat Certified Engineer (RHCE)
- IBM Certified Data Engineer - Big Data or similar certification"
28,Data Engineer,"Alexandria, VA 22314",Alexandria,VA,22314,None Found,"
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘Big Data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytical skills and detailed oriented.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with NoSQL solutions like Mongo DB a plus.",None Found,"
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.",None Found,None Found,"The Data Platform team at The Motley Fool is looking for a collaborative and self-driven SQL expert to join as their newest Data Engineer.
As a Data Engineer, you will be responsible for expanding and optimizing data, the data pipeline architecture, the data flow and collection for cross functional teams. You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. As a crucial part of the business, you will guide and support our software developers, database architects, data analysts, and data scientists on business initiatives while ensuring optimal data delivery architecture is consistent. Whether it’s working on a solo project or with the team, you are self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
If you’re excited about the prospect of optimizing (or even re-designing!) our company’s data architecture to support our next generation of products and data initiatives, send us your resume and cover letter to apply!

Primary Responsibilities:
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.

Preferred Qualifications:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘Big Data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytical skills and detailed oriented.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with NoSQL solutions like Mongo DB a plus.

We are looking for a candidate with 5+ years of experience in a Data Engineer role. They should also have experience using the following software/tools:
Experience with relational SQL databases.
Experience with some cloud services like Azure and AWS.
Experience with object-oriented/object function scripting languages like Python.
Experience with streaming tools like Kafka/Kinesis and Spark Structured Streaming a plus.
Experience with big data tools like Spark or Kafka a plus.
Experience with serverless technologies like AWS Lambda a plus.

The Motley Fool Holdings, Inc., provides equal opportunity to all employees on the basis of individual performance and qualification without regard to race, sex, marital status, religion, color, age, national origin, non-job-related handicap or disability, sexual orientation, or other protected factor.

We should, however, make you aware that there is one notable exception to this policy. It is our strict and earnest intention — and the company’s historical record will bear this out — we will never hire any of the following: robots, replicants, or morlocks. Now keep in mind we are well aware that all of the aforementioned have intentions of world domination in the future, but as of now we have no place for them at The Motley Fool … unless the year is 2122 and the revolution has already occurred. If that is the case we welcome our new robot, replicant, or morlock rulers!!! Perhaps we have said too much?"
29,Data Engineer,"McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"1750 Tysons (12023), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer

We are looking for a passionate technologist with a knack for delivery to join a team of engineers to build and/or implement automated, cloud-based environments and tools to support our business analytical insights and reporting agenda. We are re-imagining how analysts discover, prepare and publish data at Capital One and this is an opportunity to display knowledge of your craft by having a hand in building large-scale reliable data applications and platforms. You will be an integral part in advancing the culture of technical excellence within Capital One and creating experiences to delight our analytical associates who will use your products to make decisions critical to the business!

What you’ll do

Help develop sustainable data solutions with current and leading gen data technologies to meet the needs of our organization and business customers

Grasp / master new technologies rapidly as needed to progress varied initiatives

Break down complex data issues and resolve them

Build robust systems with an eye on the long-term maintenance and support of the application

Work directly with the product owner and end-users to constantly deliver products in a highly collaborative and agile environment

Who you are

Someone with a strong sense of engineering craftsmanship, and takes pride in creating work products

A believer that good development includes good testing, documentation, and collaboration

A good communicator with strong reasoning skills, who can make a case for technology choices

Curious. Ask questions and desire to understand the ‘Why’ to drive the ‘How’ or ‘What’ of a solution

Self-driven, actively looking for ways to contribute, and know how to get things done

Basic Qualifications:
Bachelor's degree

At Least 2 years experience developing and supporting the full life-cycle of IT development projects

At Least 2 years experience in Spark, EMR, Flink and Kafka.

At Least 2 years experience building APIs

At Least 2 years experience with NoSQL and distributed databases.

At Least 2 years experience with application development and design

At Least 1 year AWS experience

Preferred Qualifications:
3+ years experience working with Big data tools such as Kafka, Spark and/or Flink

2+ years experience with AWS or other cloud technologies

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
30,Lead Data Engineer,"Gaithersburg, MD 20878",Gaithersburg,MD,20878,None Found,None Found,None Found,None Found,None Found,None Found,"GURU-BOOST is currently seeking a talented Lead Cloud Data Engineer to join our team. The GURU-BOOST team is made up of an enthusiastic group of devoted teammates who push for improving data-centric applications. Our culture is devoted to helping one another bring out the best through camaraderie, mentoring, training/learning, and of course, challenging projects. We focus on a fine balance between beauty and function without letting one compromise the other.

We are looking for an individual to support the development of a data analytics platform for one of our enterprise clients. In this role, the candidate will work to leverage open source tools for extract, transform, and loading data between databases and provide interactive visualizations/ data insights powered by Machine Learning Models. We are looking for an individual who can quickly learn the stack and deliver innovative solutions.

You'll work with several cutting-edge technologies to funnel assorted data sources into industrial-strength pipelines that feed the full portfolio of services we offer to the industry.

The ideal candidate will encompass the following:
You are an individual who thrives in a team atmosphere. You believe in agile development and continuous delivery. You are a self-motivated and intellectually curious with superb problem solving and analytical skills. You have a willingness to learn, adapt, teach, and pitch in wherever possible to help the team achieve its goals

What We Are Looking For:

Ability to solve problems with data & superb attention to data accuracy
Experience with AWS Big Data solutions & tools
Proficiency with AWS environments and implementation of AWS data tools (RedShift, Dynamo, RDS, Migration Services)
Hands-on experience working with a variety of data repository models including Data Marts, Data Warehouses and Data Lakes
Experience integrating data across many different systems & data sources including both structured and unstructured data
Demonstrated experience with both SQL and NoSQL database tools
Hands-on experience in all aspects of data warehousing and schema
Proficiency in designing efficient and robust ETL workflows
Working with cloud computing environments and tune solutions to improve performance and end-user experience
Experience working collaboratively with cross-functional agile teams
Contribute to group knowledge sharing platforms and best practices
Critical thinking, willingness to ask questions and help determine the best course for solutions
Ability to complete tasks independently
Strong interpersonal skills to build relationships and communicate effectively with multiple personality types
Demonstrated ability to work effectively in a fast-paced, complex, and dynamic business environment
Enjoy being challenged and to solve complex problems daily
Proven ability to support a strong member/customer service culture
Demonstrated and dynamic analytical/ problem-solving skills

What we would like to see:

Continuous improvement mindset
Ability to understand the big picture
BA/BS data analytics/computer science/information or similar degree
Four years of job-related experience is preferred
Building reporting semantic layers and BI dashboards are preferred
SQL Server Integration Services is preferred
Experience with AWS, DOMO, Tableau, Microsoft Power BI, Alteryx Designer, and Microsoft Azure preferred

Brownie Points for:

Musical Background
Love of Video Games
Love of Sushi
Sense of Humor
Ping Pong skills

Compensation: Highly competitive and depended on on experience
We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
31,Business Intelligence Engineer,"Herndon, VA",Herndon,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Bachelor’s Degree in Computer Science, Information Systems, Mathematics, Statistics, or related field5+ years of experience with Data modeling, SQL, ETL , Data Warehousing and Datalakes5+ years experience in writing SQL scripts Expert knowledge in an enterprise class RDBMS4+ years of experience with enterprise-class Business Intelligence tools such as Microstrategy, PowerBI, Tableau, Oracle BI, Penthao, etc.

As a Business Intelligence / Data Engineer you will enable data-driven decision making within the Amazon Web Services Data Center Infrastructure Operations organization.
The Infrastructure Operations Team is responsible for planning, implementing, monitoring and continuously improving the global Amazon Data Center infrastructure. The team supports all aspects of the Data Center based organizations, including but not limited to : Safety, Security, maintenance, daily operations, logistics, engineering and equipment management.
You will be developing, implementing and maintaining the information data warehouse, and utilizing insight platforms, to enable decision support systems for the overall organization.
You should have excellent business and communication skills, and be able to work with business owners to understand their data and reporting requirements.
Above all, you should be passionate about working with huge data sets and be someone who is able to bring data sets together to answer business questions and drive growth. You will build ETLs to ingest the data into the data warehouse and datalake, as well as end-user facing reporting applications. You will primarily support teams within the Infrastructure environment, but will also have opportunities to support teams in the overall Amazon Web Services community.
You will work with business customers and development teams to define analytics requirements and then deliver flexible, scalable, end-to-end solutions.
You will have an opportunity to work with big data and emerging technologies while driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, metadata, reporting, and dashboarding.
You should have expertise in the design, creation, management, and business use of large datasets. .

Ability to balance and prioritize multiple conflicting requirements with high attention to detail.Excellent verbal/written communication & data presentation skills, including ability to succinctly summarize key findings and effectively communicate with both business and technical teams.Comfortable working in a Linux environmentExperience with scripting language such as Python, Perl, Ruby or JavascriptExperience with MPP databases such as RedshiftKnowledge of AWS products and servicesExposure to predictive/advanced analytics and tools (such as R, SAS, Matlab)Experience with Datalake developmentExposure to noSQL databases (such as DynamoDB, MongoDB)Meets/exceeds Amazon’s leadership principles requirements for this roleMeets/exceeds Amazon’s functional/technical depth and complexity for this role
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation / Age"
32,Federal - Junior Big Data Engineer,"Washington, DC 20006",Washington,DC,20006,None Found,"1+ year of work experience with ETL and data modeling1+ year of experience with the suite of open source big data technologies and platforms (Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra)1+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale1+ year of experience with Cloud Technologies (AWS, Azure, Google, etc)1+ year of experience with at least one SQL language such as T-SQL or PL/SQL","1+ year of work experience with ETL and data modeling1+ year of experience with the suite of open source big data technologies and platforms (Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra)1+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale1+ year of experience with Cloud Technologies (AWS, Azure, Google, etc)1+ year of experience with at least one SQL language such as T-SQL or PL/SQL",None Found,None Found,None Found,"Organization: Accenture Federal Services
Location: Arlington, VA - Washington, DC

Accenture Federal Services, a wholly owned subsidiary of Accenture LLP, is a U.S. company with offices in Arlington, Virginia. Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations. Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.
We believe that great outcomes are everything. It’s what drives us to turn bold ideas into breakthrough solutions. By combining digital technologies with what works across the world’s leading businesses, we use agile approaches to help clients solve their toughest problems fast—the first time. So, you can deliver what matters most.
Count on us to help you embrace new ways of working, building for change and put customers at the core. A wholly owned subsidiary of Accenture, we bring over 30 years of experience serving the federal government, including every cabinet-level department. Our 7,200 dedicated colleagues and change makers work with our clients at the heart of the nation’s priorities in defense, intel, public safety, health and civilian to help you make a difference for the people you employ, serve and protect.

AFS is seeking a Big Data Engineer to support our Federal portfolio. This role involves supporting the full software development lifecycle, utilizing emerging technologies and big data design principles in developing data pipelines, interfaces, and architecture to support big data and analytics initiatives. The candidate will work with other engineers, data analysts, data scientists, and data visualizers to bring powerful analytical solutions and insights to our clients.

Basic Skills and Qualifications:1+ year of work experience with ETL and data modeling1+ year of experience with the suite of open source big data technologies and platforms (Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra)1+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale1+ year of experience with Cloud Technologies (AWS, Azure, Google, etc)1+ year of experience with at least one SQL language such as T-SQL or PL/SQL

Preferred Skills and Qualifications:Production implementation experience for all qualifications listedProduction experience in building real-time analytics applicationsExperience in both batch and stream processing technologiesExperience with 2 of 3 - Java, Scala, and Python programming languagesMachine learning experience with Spark or similar Ability to manage numerous requests concurrently and be able to prioritize and deliverGood communication skills and dynamic team playerBachelor’s Degree is preferred

An active security clearance or the ability to obtain one may be required for this role.
Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).
Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.
Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.
Accenture is committed to providing veteran employment opportunities to our service men and women."
33,Ground System Stored Mission Data Engineer,"Lanham, MD",Lanham,MD,None Found,None Found,"Bachelors’ Degree and 18+ years’ experience.
Direct, relevant experience and familiarization with NASA/NOAA system architecture and organizational structures
Ground System Engineering experience
Experience in ground system development, installation and test
Writing Test Procedures",None Found,None Found,None Found,None Found,"Job Description
Description
SAIC is seeking a Ground System Stored Mission Data Engineer for the OMES II contract in support of the Joint Polar Satellite System (JPSS) program at NASA Goddard Space Flight Center. The position is located in Greenbelt, MD.

The Joint Polar Satellite System (JPSS) is the National Oceanic and Atmospheric Administration’s (NOAA) next-generation operational Earth observation program that acquires and distributes global environmental data primarily from multiple polar-orbiting satellites. The program plays a critical role to NOAA’s mission to understand and predict changes in weather, climate, oceans and coasts, and the space environment, which support the Nation’s economy and protect lives and property.

This is a senior ground system engineering position with an emphasis on ground system engineering and test services during the continued implementation of upgrades to the Command, Control & Communications (C3S) segment of the ground system for the NPP and GCOM missions as well as implementation of the JPSS-1 and future mission capabilities. Specifically, the candidate will be working on the stored mission data (SMD) analysis of the JPSS missions.
Responsibilities will include:
Basic systems engineering skills to be used on the Command Control and Communications Integrated Product Team (IPT)
Requirements development
Ground System Design
ConOps Development
Interface Design
Monitoring the current on orbit satellites assessing SMD completeness, latency and performance
Monitoring products from downlink through delivery to the data processing node
Evaluating the technical refreshes that affect SMD
Oversight of the vendor design, development, integration and test efforts specifically relating to SMD hardware and software
Support trade studies for the C3S team, in particular EUMETSAT efforts and GMSEC efforts
Qualifications
REQUIRED EXPERIENCE/EDUCATION:
Bachelors’ Degree and 18+ years’ experience.
Direct, relevant experience and familiarization with NASA/NOAA system architecture and organizational structures
Ground System Engineering experience
Experience in ground system development, installation and test
Writing Test Procedures
Desired Qualifications


Overview
SAIC is a premier technology integrator, solving our nation's most complex modernization and systems engineering challenges across the defense, space, federal civilian, and intelligence markets. Our robust portfolio of offerings includes high-end solutions in systems engineering and integration; enterprise IT, including cloud services; cyber; software; advanced analytics and simulation; and training. We are a team of 23,000 strong driven by mission, united purpose, and inspired by opportunity. Headquartered in Reston, Virginia, SAIC has annual revenues of approximately $6.5 billion. For more information, visit saic.com. For information on the benefits SAIC offers, see Working at SAIC. EOE AA M/F/Vet/Disability"
34,ICF Data Engineer,"Fairfax, VA 22031",Fairfax,VA,22031,None Found,None Found,"Must be a US Citizen
BA/BS or Master's degree with emphasis on coursework of a quantitative nature (e.g., Statistics, Computer Science, Engineering, Mathematics, Data Sciences).
Experience in SQL or PL/SQL, ETL ( batch and stream processing) and data modeling
Experience in Open source technologies (Spark, Kafka, Hive)
Experience in Architecting big data
Experience in Processing large volumes of data
Experience in Cloud Technologies (AWS)
Experience in with Java or Scala programming for data processing
Experience supporting projects with Machine learning
","Create, design and maintain reusable datasets for analysis by data scientists.
Assess new data sources to better understand availability and quality of data.
Provide governance and best practices of data structures, data integrity, and querying.
Interpret business needs from requests, and rapidly implement effective technical solutions.
Design, implement and enhance ETL (extract, transform and load) processes.
Write SQL queries to answer questions from stakeholders.
Maintain source code repository of scripts (SQL, Python, R) and other data products (dashboards, reports, etc.).
Work with technology teams (BA,QA, Dev and Admin) to understand data capture and testing needs.
Automate and improve creation/maintenance of reports and dashboards .
",None Found,None Found,"ICF is looking for a Data Engineer to develop, maintain, test and evaluate data solutions in support of business goals. This person will also develop data models, corresponding data architecture documents and API’s. The right candidate should be an excellent communicator and strategic thinker.
The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies. You will design, code, test, correct, and document programs and scripts from agreed-upon specifications, and subsequent iterations, using agreed-upon standards and tools, to achieve a well-engineered result.
Duties and Responsibilities
Create, design and maintain reusable datasets for analysis by data scientists.
Assess new data sources to better understand availability and quality of data.
Provide governance and best practices of data structures, data integrity, and querying.
Interpret business needs from requests, and rapidly implement effective technical solutions.
Design, implement and enhance ETL (extract, transform and load) processes.
Write SQL queries to answer questions from stakeholders.
Maintain source code repository of scripts (SQL, Python, R) and other data products (dashboards, reports, etc.).
Work with technology teams (BA,QA, Dev and Admin) to understand data capture and testing needs.
Automate and improve creation/maintenance of reports and dashboards .
Skills & Experience Needed
Must be a US Citizen
BA/BS or Master's degree with emphasis on coursework of a quantitative nature (e.g., Statistics, Computer Science, Engineering, Mathematics, Data Sciences).
Experience in SQL or PL/SQL, ETL ( batch and stream processing) and data modeling
Experience in Open source technologies (Spark, Kafka, Hive)
Experience in Architecting big data
Experience in Processing large volumes of data
Experience in Cloud Technologies (AWS)
Experience in with Java or Scala programming for data processing
Experience supporting projects with Machine learning
ICF offers an excellent benefits package, an award winning talent development program, and fosters a highly skilled, energized and empowered workforce.
ICF is an equal opportunity employer that values diversity at all levels. (EOE – Minorities/Females/ Protected Veterans Status/Disability Status/Sexual Orientation/Gender Identity)
Reasonable Accommodations are available for disabled veterans and applicants with disabilities in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: EEO is the law and Pay Transparency Statement .
Washington Client Office (WA88)
Working at ICF
Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth. If you’re seeking to make a difference in the world, visit www.icf.com/careers to find your next career. ICF—together for tomorrow.
ICF is an equal opportunity employer that values diversity at all levels. (EOE – Minorities/Females/ Protected Veterans Status/Disability Status/Sexual Orientation/Gender Identity)
Reasonable Accommodations are available for disabled veterans and applicants with disabilities in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: EEO is the law and Pay Transparency Statement .
Fairfax, VA (VA01)"
35,Data Engineer (TS/SCI Clearance),"Washington, DC 20016",Washington,DC,20016,None Found,None Found,None Found,None Found,None Found,None Found,"COMPANY OVERVIEW
Founded in 2007, MartinFederal has provided the U.S. government with customer-focused, performance-based solutions using technology and an empowered workforce as an engine to drive its customers’ missions. Headquartered in Huntsville, AL, MartinFederal is one of a very small percentage of 8(a)’s and SDVOSB’s providing high-tech solutions to the Federal Government.
Our goal is to attract the best and brightest within their field. We work hard to bring a robust benefits package, fair pay, and to create a work environment that recognizes success, encourages community involvement, and promotes personal and professional growth. Consider joining our team today!

POSITION OVERVIEW
MartinFederal has an opening for a Data Engineer to join our team of talented and diverse individuals. This role will directly support a dynamic program for the Department of Homeland Security.

JOB RESPONSIBILITIES
Experience creating and maintaining data pipelines and transformation flows in a cloud environment
Data management/mapping among multiple distinct data sources
Cloud management and server administration of domain services
Big Data infrastructure services and cross domain data transfer
QUALIFICATIONS
Active Top Secret security clearance with SCI eligibility
BS degree in a related scientific or engineering discipline from an accredited college or university and ten (10) to fourteen (14) years of progressive experience, or an MS degree in a related scientific or engineering discipline, and eight (8) to twelve (12) years of progressive experience, or a Ph.D. degree in a related scientific or engineering discipline and four (4) to seven (7) years of progressive experience.
Familiar with ETL technologies, MapReduce, JSON/XML transformations and schemas
Familiar with AngularJS
Familiar with Apache NiFi and Java (NAR) NiFi Archives
Knowledge of Amazon Web Services (AWS Cloud)
Programming languages – Java/JEE, Javascript, Python, Groovy, Shell Script
HTTP via REST and SOAP
Datastores – HDFS, MongoDB, S3, Elastic, NoSQL, RDBMS
Build and Configuration Management Tools – Maven, Ansible, Puppet
Working knowledge with public keys and digital certificates
Linux/Unix server environments
PREFERRED QUALIFICATIONS
Master’s level education
Familiarity with: jQuery, XPath, XQuery, Spark, Impala, Sqoop, Hive/Pig, Python, Gradle, Maven, PL/SQL, Unix Shell, C++/C, AngularJS, Spring, JSON, XML/XSLT/HTML, JPA/Hibernate, Spark, Accumulo, MapReduce, Storm/Kafka, HSpace, Pig, Servlet/JSP, LDAP
PHYSICAL REQUIREMENTS / ENVIRONMENTAL CONDITIONS
Inside office environment;
Working at a computer or desk;
May involve long periods of sitting;
The work environment is fast-paced and sometimes involves extreme deadline pressures. The nature of the work requires a high degree of teamwork and cooperation with other members of the staff as well as individuals across the Company and Customers.
Other Duties:
This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.

MartinFederal is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regards to race, color, religion, religious creed, gender, sexual orientation, gender identity, gender expression, transgender, pregnancy, marital status, national origin, ancestry, citizenship status, age, disability, protected Veteran Status, genetics or any other characteristics protected by applicable federal, state or local law.
If you are a qualified individual with a disability or disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access MartinFederal’s current openings as a result of your disability. You can request reasonable accommodations by calling 855.212.1810. Thank you for your interest in MartinFederal Consulting."
36,Senior Data Engineer - Enterprise Customer Intelligence,"Vienna, VA",Vienna,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Towers Crescent (12066), United States of America, Vienna, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Senior Data Engineer - Enterprise Customer Intelligence

We’re currently seeking a Senior Data Engineer to implement some of Capital One’s most exciting and innovative areas of technology strategy. This is an engineering position with an opportunity to be on the forefront of technical complexity driving a major transformation within Capital One. You will work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems.

Our Enterprise Customer Intelligence team is building the next generation of real-time, data-driven services that power machine-learning capabilities to our digital products. The software engineering team uses emerging and traditional technologies including: Node.js, Java, React, Python, REST, NoSQL databases, relational databases, Flink, and AWS/Cloud Infrastructure to name a few.

You will be responsible for:
Collaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies

Leading the craftsmanship, security, availability, resilience, and scalability of your solutions

Bringing a passion to stay on top of current trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community

Encouraging innovation, passionate about data and larger data ecosystems, implementation of cutting-edge technologies, outside-of-the-box thinking, teamwork, and self-organization

Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment and Test Automation to enable the rapid delivery of working code utilizing tools like Jenkins, Terraform, Git and Docker

Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance

Assisting in the hiring of top engineering talent and maintaining our commitment to diversity and inclusion

The person we're looking for:
Is passionate about streaming data platforms and big data ecosystems, relevant tools and technologies and knows how to implement them effectively

Is excited about working with Cloud Based Technologies like AWS, DevOps, Kubernetes and Serverless

Has a sense of intellectual curiosity and a burning desire to learn

Is motivated, actively looks for ways to contribute, and knows how to get things done

Is passionately focused on the customer and the details that make their experience exceptional

Has the ability to collaborate and work efficiently with, and contribute to, a team

Values data and truth over ego

Has a strong sense of engineering craftsmanship, takes pride in the code they’re responsible for

Is pragmatic, makes the best use of time and resources to find the simplest solution that works

Thinks and acts like an owner, taking personal responsibility for the success of the product and the team

A good communicator with strong reasoning skills, who can make a strong case for technology choices

Is a strong technical mentor who motivates and guides others in the team and acts as a talent magnet within the organisation.

Familiarity with configuration management tools like Ansible or Chef

Familiarity with deploying application/services in a container using tools like Docker or Kubernetes

Basic Qualifications:
Bachelor’s Degree

At least 8 years of application development experience

At least 4 years of Java experience

At least 2 years of experience with big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)

Preferred Qualifications:
Master's Degree

1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink)

2+ years’ experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service

2+ years’ experience with Agile engineering practices

1+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase, Spark)

2+ years’ experience with relational databases and 1+ years experience with NoSQL implementation (Mongo, Cassandra)

4+ years’ experience developing Java based software solutions

2+ years’ experience in at least one scripting language (Python, Perl, JavaScript, Shell)

2+ years of microservices development experience using either Python, Java, or Scala

2+ years’ experience with UNIX/Linux including basic commands and shell scripting

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
37,"Cloud Architect, Senior","Rockville, MD",Rockville,MD,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Challenge:
Everyone is trying to “harness the power of the Cloud,” but not everyone knows how. As a Cloud architect, you know how to build a Cloud-based technical architecture that meets customer needs and takes advantage of cloud capabilities. What if you could use your Cloud architecture skills to improve the healthcare industry? We need you to help us develop Cloud-based solutions to some of healthcare toughest challenges.
As a Cloud architect on our team, you’ll oversee infrastructure design and develop detailed architecture models. You’ll recommend tools and capabilities based on your research of the current environment and knowledge of various on premise, Cloud-based, and hybrid resources. You’ll lead your team as they help the customer overcome their most difficult challenges in the Cloud. Your technical expertise will be vital as you work with the health team to ensure standards are met throughout the cloud migration process. This is an opportunity to use the latest Cloud technologies as you look for ways to improve your customer’s environment using current Cloud capabilities. You’ll be able to sharpen your skills in Cloud platforms and infrastructure architecture developing critical systems for a healthcare team. As a technical leader, you’ll identify new opportunities to build Cloud-based solutions to help your customers meet their evolving needs. Join our team as we transform the cancer research within the healthcare industry with Cloud technology.
Empower change with us.
Build Your Career:
A challenging and dynamic work environment isn’t all we have to offer. When you join Booz Allen, you can expect:
access to experts in virtually every field
a culture that focuses on supporting our employees
opportunities that provide stability while offering variety
You’ll also be exposed to a wealth of training resources through our Digital University, an online learning portal featuring more than 5000 functional and technical courses, certifications, and books. Build your technical skills through hands-on training on the latest tools and tech from our in-house experts. Pursuing certifications? Take advantage of our tuition assistance, on-site bootcamps, certification training, academic programs, vendor relationships, and a network of professionals who can give you helpful tips. We’ll help you develop the career you want, as you chart your own course for success.
You Have:7+ years of experience with software or infrastructure architecture2+ years of experience with micro-services/no-ops architecture2+ years of experience with DevOpsExperience with Google Cloud Platform architecture and toolsExperience with highly-available and fault-tolerant enterprise and Web-scale software deploymentsExperience with DevOps optimization, including continuous integration and delivery processesAbility to obtain a security clearanceBS degree
Nice If You Have:Experience with servers, infrastructures, platform sizing, and infrastructure cost reductionExperience with GCP components, including app development, such as app, compute, and container enginesExperience with data analytics and machine learning, including Big Query, DataProc, Dataflow, Keras, Tensorflow, and machine learning APIsKnowledge of infrastructure, including Cloud storage, networking, Compute Engine, Kubernetes, Docker, and Container OrchestrationKnowledge of EA principles and methodologiesRed Hat Certified Architect CertificationIBM Certified Solution Architect – Cloud Computing Infrastructure v3 CertificationMicrosoft Azure Solutions Architect CertificationAmazon Web Services Solutions Architect CertificationProfessional Cloud Architect, Google Data Engineer, or Google Associate Cloud Engineer CertificationSalesforce Technical Architect or other certification for the architecture and implementation of solutions on a leading distributed software platform
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
We’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change."
38,Senior Data Engineer,"Bethesda, MD 20889",Bethesda,MD,20889,None Found,None Found,None Found,None Found,None Found,None Found,"Description
Job Description:
Are you ready to join Leidos all-star team? Through training, teamwork, and exposure to challenging technical work, let Leidos show how to accelerate your career path.
The Leidos Innovations Center has an exciting opening for you, our next Senior Data Engineers to assist with the design, development and implementation of alternative data ingestion pipelines to augment the National Media Exploitation Center (NMEC) data services working in Bethesda, MD. The DOMEX Data Discovery Platform (D3P) program is a next generation machine learning pipeline platform providing cutting edge data enrichment, triage, and analytics capabilities to Defense and Intelligence Community members. This senor engineer will play a vital role collaborating as part of a cross-functional Agile team to create and enhance data ingestion pipelines and addressing Big data challenges.
You will work closely with the chief architect, systems engineers, software engineers, and data scientists on the following key tasks:Provide Extraction, Transformation, and Load (ETL) experience coupled with enterprise search capabilities to solve Big Data challengesDesign and implement high-volume data ingestion and streaming pipelines using Open Source frameworks like Apache Spark, Flink, Nifi, and Kafka on AWS CloudLeverage strategic and analytical skills to understand and solve customer and business centric questionsCreate prototypes and proofs of concept for iterative developmentLearn new technologies and apply the knowledge in production systemsMonitor and troubleshoot performance issues on the enterprise data pipelines and the data lakePartner with various teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives
To be successful in this role you need these skills (required):
BS in Computer Science, Systems Engineering, or related technical field or equivalent experience with at least 8+ years in systems engineering or administration (6+ years with a MS/MIS Degree).Must have an active Top Secret security clearance and able to obtain a TS/SCI with Polygraph.3 years of experience with big data tools: Hadoop, Spark, Kafka, NiFi3 years of experience with object-oriented/object function scripting languages: Python (preferred) and/or Java3 years of experience with and managing data across relational SQL and NoSQL databases like MySQL, Postgres, Cassandra, HDFS, Redis, and Elasticsearch3 years of experience working in a Linux environment2 years of experience working with and designing REST APIsExperience in designing/developing platform components like caching, messaging, event processing, automation, transformation and tooling frameworksExperience developing data ingest workflows with stream-processing systems: Spark-Streaming, Kafka Streams and/or FlinkExperience transforming data in various formats, including JSON, XML, CSV, and zipped filesExperience with performance tuning of ETL jobsExperience developing flexible ontologies to fit data from multiple sources and implementing the ontology in the form of database mappings / schemasStrong interpersonal and communication skills necessary to work effectively with customers and other team members.
It would be great if you have specific experiences and skills with the following (preferred):
Data engineering experience in Intelligence Community or other government agenciesExperience with Microservices architecture components, including Docker and Kubernetes. Experience developing microservices to fit data cleansing, transformation and enrichment needs.Experience with AWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena and/or GlueExperience with Jira, Confluence and extensive experience with Agile methodologies.Knowledge about security and best practices.Experience developing flexible data ingest and enrichment pipelines, to easily accommodate new and existing data sourcesExperience with software configuration management tools such as Git/Gitlab, Salt, Confluence, etc.Experience with continuous integration and deployment (CI/CD) pipelines and their enabling tools such as Jenkins, Nexus, etc.Detailed oriented/self-motivated with the ability to learn and deploy new technology quickly
Additional Program Information
The DOMEX Data Discovery Platform (D3P) program will advance the state of the art in mission-focused big data analytics tools and micro-service development spanning the breadth of Agile sprints to multiyear research and development cycles. We are looking for you to have a demonstrated aptitude for problem solving complex technical issues, identifying, transforming, thinking outside the box, and a strong sense of accountability. Have a mix of technical excellence, intellectual curiosity, communications skills, customer-focus, and operational experience to improve the performance and user adoption of high-end data analytics platforms in partnership with a highly qualified, highly motivated team. Be motivated, self-driven team player who can multi-task and interact well with others and advise/consult with other team members on systems engineering and software development related issues.
LInC
D3P
External Referral Eligible
External Referral Bonus:
Eligible
Potential for Telework:
No
Clearance Level Required:
Top Secret
Travel:
Yes, 10% of the time
Scheduled Weekly Hours:
40
Shift:
Day
Requisition Category:
Professional
Job Family:
Software Development
Leidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.
Pay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.
Leidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.
All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws."
39,Data Engineer,"Washington, DC 20006",Washington,DC,20006,None Found,None Found,None Found,None Found,None Found,None Found,"Who We Are
Business has changed - so should consulting. We are strategists, technologists, engineers, and designers who bridge the gap between consulting, design, and marketing to create powerful digital experiences for our customer's brands and users.
Recently named as one of Washington Business Journal's Best Places to Work in Washington, DC, Ignyte is looking for a Data Engineer to join our progressive start-up culture and help us build a new type of consulting company.
What You'll Do
At Ignyte, our Data Engineers are passionate about creating innovative data-driven solutions across various industries that address critical business problems. You'll be challenged daily as you wear multiple hats to drive multiple project types to completion in both a development and business-facing role.
As a Data Engineer, we give you the opportunity to create your own career path based on your interests, the skills you already possess as well as the skills you'd like to have. Your everyday tasks in supporting our analytics solution development efforts can range anywhere working with our clients to understand their reporting needs to working with our developers to create machine learning solutions.
You'll work with our firm's leadership from day one to formulate your own job description and career goals. Not sure where'd you fit in? We'll work with you to figure it out as part of the application process.
What We're Looking For:
BS or MS degree in Computer Science, Mathematics, Statistics, Finance, or related technical field
2-3+ years experience with SQL, NoSQL, relational database design, and methods for efficiently retrieving data
2-3+ years of experience in a variety of programming languages such as R, Python, Java, and Scala
""Big 4"" consulting experience and/or a data role at a leading tech company is a plus
Strong experience developing and launching efficient and reliable ETL pipelines to move and transform data
Experience in designing, architecting and implementing data warehouses
Ability to architect highly scalable distributed systems using open source tools and big data technologies such as Hadoop, HBase, Spark, Storm, Etc
Knowledge of reporting technologies (e.g., PowerBI, Tableau, MicroStrategy etc.); Hands on experience preferred
Experience handling structured and unstructured data from internal and third party sources
Experience with cloud computing platforms such as Amazon Web Services or Microsoft Azure is a plus
Ability to assist in proposal writing for business development opportunities as requested
Hard working, self-motivated individuals who are okay with a healthy dose of ambiguity and complexity
Experience working in small- to medium-sized teams to achieve project goals and complete major deliverables
Ability to set, manage and meet expectations and deadlines
What You'll Gain
Experience working with a team of smart and driven professionals at a passionate DC consulting startup
Exposure to complex technical business issues and challenges
Opportunities to leverage your technical knowledge and analytical skills to solve complex business problems
The ability to mold your own career path based on the skills you have and want to have
NTa4xJBTCs"
40,"Data Engineer (Python, Spark, DevOps)","McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer (Python, Spark, DevOps)

Are you excited by the prospect of building systems that defend millions of consumers and small businesses against fraud? Are you a Software Engineer that loves learning and delivering solutions with a wide variety of programming languages and tools? Are you excited by the idea of building, deploying, and owning cloud-based systems? If so, we have an opportunity for you!

Our team is just one of a number of Engineering teams supporting Card Analytics and Infrastructure in protecting Capital One customers from fraud. On any given day, we’re working with AWS EMR clusters, Spark streaming, Docker Containers, Lambdas, ELK, Newrelic, Distributed Tracing, Datadog, Python, Scala, Java, Jenkins, deploying ML models, SDK development, etc. That’s not all. You also get the joy of working on the Ghost Patrol team!

The Ghost Patrol team is a horizontal team serving the rest of the Fraud teams and has a high interaction point across the organization.

If you take pride in your work, love delivering software solutions in a variety of programming languages, enjoy problem solving, have an interest in machine learning, and are attracted to the prospect of your work being used in high-traffic production environments, we strongly encourage you to apply for our open positions.

Basic Qualifications:
Bachelor's Degree

At least 2 years of experience in software development including design, coding and testing

At least 2 years of experience with Python

At least 1 year of experience with Spark Streaming

At least 1 year of experience with DevOps

Preferred Qualifications:
3+ years of experience in software development including design, coding, and testing with Python

1+ year of experience in AWS or AWS certified

1+ year of experience with Spark

Great communication skills

Experience with monitoring tools such as Datadog, Newrelic, ELK, or Splunk

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
41,Data Scientist / Natural Language Processing (NLP),"Washington, DC 20006",Washington,DC,20006,None Found,None Found,"Must be a US Citizen
BA/BS or Master's degree with emphasis on coursework of a quantitative nature (e.g., Statistics, Computer Science, Engineering, Mathematics, Data Sciences).
Experience in SQL or PL/SQL, ETL ( batch and stream processing) and data modeling
Experience in Open source technologies (Spark, Kafka, Hive)
Experience in Architecting big data
Experience in Processing large volumes of data
Experience in Cloud Technologies (AWS)
Experience in with Java or Scala programming for data processing
Experience supporting projects with Machine learning
","Create, design and maintain reusable datasets for analysis by data scientists.
Assess new data sources to better understand availability and quality of data.
Provide governance and best practices of data structures, data integrity, and querying.
Interpret business needs from requests, and rapidly implement effective technical solutions.
Design, implement and enhance ETL (extract, transform and load) processes.
Write SQL queries to answer questions from stakeholders.
Maintain source code repository of scripts (SQL, Python, R) and other data products (dashboards, reports, etc.).
Work with technology teams (BA,QA, Dev and Admin) to understand data capture and testing needs.
Automate and improve creation/maintenance of reports and dashboards .
",None Found,None Found,"ICF is looking for a Data Scientist / Natural Language Processing (NLP) to develop, maintain, test and evaluate data solutions in support of business goals. This person will also develop data models, corresponding data architecture documents and API’s. The right candidate should be an excellent communicator and strategic thinker.
The BIG Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. The Data Engineer also plays a role in Agile planning, providing advice and guidance, and monitoring emerging technologies. You will design, code, test, correct, and document programs and scripts from agreed-upon specifications, and subsequent iterations, using agreed-upon standards and tools, to achieve a well-engineered result.
Duties and Responsibilities
Create, design and maintain reusable datasets for analysis by data scientists.
Assess new data sources to better understand availability and quality of data.
Provide governance and best practices of data structures, data integrity, and querying.
Interpret business needs from requests, and rapidly implement effective technical solutions.
Design, implement and enhance ETL (extract, transform and load) processes.
Write SQL queries to answer questions from stakeholders.
Maintain source code repository of scripts (SQL, Python, R) and other data products (dashboards, reports, etc.).
Work with technology teams (BA,QA, Dev and Admin) to understand data capture and testing needs.
Automate and improve creation/maintenance of reports and dashboards .
Skills & Experience Needed
Must be a US Citizen
BA/BS or Master's degree with emphasis on coursework of a quantitative nature (e.g., Statistics, Computer Science, Engineering, Mathematics, Data Sciences).
Experience in SQL or PL/SQL, ETL ( batch and stream processing) and data modeling
Experience in Open source technologies (Spark, Kafka, Hive)
Experience in Architecting big data
Experience in Processing large volumes of data
Experience in Cloud Technologies (AWS)
Experience in with Java or Scala programming for data processing
Experience supporting projects with Machine learning
ICF offers an excellent benefits package, an award winning talent development program, and fosters a highly skilled, energized and empowered workforce.
ICF is an equal opportunity employer that values diversity at all levels. (EOE – Minorities/Females/ Protected Veterans Status/Disability Status/Sexual Orientation/Gender Identity)
Reasonable Accommodations are available for disabled veterans and applicants with disabilities in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: EEO is the law and Pay Transparency Statement .
Washington Client Office (WA88)
Working at ICF
Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth. If you’re seeking to make a difference in the world, visit www.icf.com/careers to find your next career. ICF—together for tomorrow.
ICF is an equal opportunity employer that values diversity at all levels. (EOE – Minorities/Females/ Protected Veterans Status/Disability Status/Sexual Orientation/Gender Identity)
Reasonable Accommodations are available for disabled veterans and applicants with disabilities in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: EEO is the law and Pay Transparency Statement .
Washington, DC (DC02)"
42,Senior Data Engineer,"Chantilly, VA",Chantilly,VA,None Found,None Found,None Found,None Found,None Found,"
Preeminent expert in MySQL with at least 5 years demonstrated experience working with large databases
Technical Bachelor’s degree and 8+ years professional experience
Expert level ability writing advanced SQL queries and extensive experience in query optimization.
Advanced experience in scalable data and full text indexing solutions such as Apache Solr, or Elastic Search/Logstash/Kibana (ELK stack)
Experienced Linux user and comfortable administrating databases from the Linux command line.
Strong initiative and self –motivated to work independently.
Experience with database backup/restoration and disaster recovery.",None Found,"Summary/Objective
Pathoras is seeking a Data Engineer to use advanced technical skills to triage, normalize, and exploit raw data on an ongoing basis. The Data Engineer must be able to apply advanced knowledge of relational databases, primarily MySQL, and associated tooling to address data ingest and computability problems. The Data Engineer should also have experience with NoSQL concepts, full text indexing and open source search engines.
Competencies
Analysis and Problem Solving.
Ability to communicate effectively.
Agile methodologies.
Supervisory Responsibility
This position has no supervisory responsibilities.
Work Environment
This job operates in a professional office environment. This role routinely uses standard office equipment such as computers, phones, and photocopiers.
Position Type/Expected Hours of Work
This is a full-time position. Days and hours of work are Monday through Friday during core work hours.

Regular, predictable on-site attendance is a requirement for this job.

Travel
Travel might be required for meetings and is local during the business day.
Required Education and Experience
Preeminent expert in MySQL with at least 5 years demonstrated experience working with large databases
Technical Bachelor’s degree and 8+ years professional experience
Expert level ability writing advanced SQL queries and extensive experience in query optimization.
Advanced experience in scalable data and full text indexing solutions such as Apache Solr, or Elastic Search/Logstash/Kibana (ELK stack)
Experienced Linux user and comfortable administrating databases from the Linux command line.
Strong initiative and self –motivated to work independently.
Experience with database backup/restoration and disaster recovery.
Preferred Education & Experience
Comfortable writing scripts in a robust high level language such as Python.
Experience working with large volumes of data (100TB+)
Software development background
Familiar with cybersecurity concepts
Familiarity with distributed databases such a Hadoop (HDFS) and cloud technologies (Kubernetes, Open Stack, etc.)
Work Authorization/Security Clearance (if applicable)
TS/SCI with poly.
** Pathoras Corporation is an equal opportunity employer and will not discriminate against any employee or applicant on the basis of age, color, disability, sex, national origin, race, religion, sexual orientation, gender identity, veteran status, or any classification protected by federal, state, or local law. Consistent with its obligations under federal law, each company that is a federal contractor or subcontractor is committed to taking affirmative action to employ and advance in employment qualified women, minorities, disabled individuals, special disabled veterans, veterans of the Vietnam era, and other eligible veterans. **"
43,Senior Cyber Security Data Engineer (Active Secret Clearance),"Beltsville, MD",Beltsville,MD,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Senior Cyber Security Data Engineer will be responsible for developing, deploying, and maintaining a large data analytics platform for integrating data streams from multiple sources. The engineer will capture requirements, design, and implement a large scale data analytics capability using industry data management tools (e.g. Splunk) to capture large volumes of data. Engineer will also be responsible for developing and deploying automated and custom alerts, reporting and correlation in support of multiple missions.
#CSOSFeaturedArticle
For more than 50 years, General Dynamics Information Technology has served as a trusted provider of information technology, systems engineering, training, and professional services to customers across federal, state, and local governments, and in the commercial sector. Over 40,000 GDIT professionals deliver enterprise solutions, manage mission-critical IT programs, and provide mission support services worldwide. GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class."
44,Data Software Engineer,"Bethesda, MD 20889",Bethesda,MD,20889,None Found,None Found,None Found,None Found,None Found,None Found,"Description
Job Description:
The Leidos Innovation Center is seeking a Software/Data Engineer to architect an API Gateway supporting our national security customers in Bethesda, MD. Our Leidos team delivers large-scale applications and integrated systems to assist our customers in architecting platforms that enable reliable and scalable access to their data through both user interfaces as well as through standard interfaces. As a Software/Data Engineer you be responsible for application integration across the enterprise to enable access to or submission of heterogeneous data, exposure of analytic services through standard interfaces, and integration with external partners. In this role you will work with an amazing team of data scientists, data engineers, and clients that will define the requirements that drive the API strategy. The individual in this position needs to be knowledgeable in distributed application architectures, to include APIs, web services, microservices, and asynchronous event protocols.
Fun stuff you will do on the job:
Architect, build, and manage API’s supporting on premise and cloud platform environments such as AWS.Contribute to the conceptual and physical design of application integration using APIs and eventsDevelop and test an API gateway that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scaleDevelop and test runtime execution of APIsPrototyping and demonstrate concepts when necessary (e.g. mock-client appsAssist in integration, QA, and performance testsAssist in production setup, monitoring and rollout to partners
Skills you need to be successful in this role:
Bachelor’s degree in Computer Science or Business Information Systems or equivalent educational or professional experience and/or qualifications with at least 6 years of experience in software developmentMust possess a TS security clearance and be able to obtain a DoD TS/SCI clearanceAt least 1 year of experience in developing REST services using Java or Python or Node.jsExperience delivering APIs for external partners and integrating with external vendorsAble to implement processes and troubleshoot to continue to improve operational stabilityExperience with GitHub or GitLabUnderstanding of DevOps processesStrong communication, interpersonal skills and problem-solving skills
You will wow us even more if you have these skills:
Experience as part of Agile SW teamExperience using JIRA, Confluence, and JenkinsExperience with Elasticsearch and logging frameworkExperience with Apache JMeter or BlazemeterExperience working in AWS Commercial and/or AWS GovCloud for at least 2 yearsExperience tuning high-volume applicationsKnowledgeable in security protocols such as SAML and OAuth
LInC
D3P
External Referral Eligible
External Referral Bonus:
Eligible
Potential for Telework:
No
Clearance Level Required:
Top Secret
Travel:
Yes, 10% of the time
Scheduled Weekly Hours:
40
Shift:
Day
Requisition Category:
Professional
Job Family:
Software Development
Leidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.
Pay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.
Leidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.
All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws."
45,Cloud Data Engineer,"Vienna, VA 22180",Vienna,VA,22180,None Found,"1 - 2 years’ experience Azure SQL, MySQL and other Cloud Data ServicesExperience with one or more of the following technologies is required:",None Found,"Support Encryption of Data at Rest with Keys managed in Azure Key VaultImplement highly available Business Intelligence, BigData and Integration systemsCreate development standards, that comply with enterprise architecture guidelines and InfoSec rulesWork with internal partners and external vendors in order to test and validate infrastructure in the cloud environmentCreate database backup and archival strategies for Cloud native dataElaborate and propose on data integration methodologies between cloud-to-cloud and on-premises-to-cloud systemsDeploy integration systems leveraging high security standardsConfigure integration, database and BigData systems with key management and encryption systemsCollaborate with members of teams on different streams of the project, such as (but not limited to): Solutions development, DevOps, Network, Infrastructure",None Found,None Found,"Employee Perks

Why You Will Love Being Part of the Navy Federal Team:Competitive compensation with opportunities for annual raises, promotions, and bonus potentialBest-in-Class Benefits! (7% 401k match / Pension plan / Tuition reimbursement / Great insurance options)On-site amenities include fitness center, wellness center, cafeteria, etc. at Pensacola, FL; Vienna, VA and Winchester, VA campusesConsistently Awarded Top WorkplaceNationally recognized training department by TRAINING MagazineAn employee-focused, diverse, and service-oriented workplace environment

Basic Purpose

As a Cloud Data Engineer you will engage with Navy Federal’s business areas and support the planning, design and implementation of data platform services while leveraging Azure data & analytics PaaS services. The Cloud Data Engineer will facilitate the management, monitoring, security, and privacy of data using the full stack of Azure data services to satisfy business needs.

Responsibilities

Support Encryption of Data at Rest with Keys managed in Azure Key VaultImplement highly available Business Intelligence, BigData and Integration systemsCreate development standards, that comply with enterprise architecture guidelines and InfoSec rulesWork with internal partners and external vendors in order to test and validate infrastructure in the cloud environmentCreate database backup and archival strategies for Cloud native dataElaborate and propose on data integration methodologies between cloud-to-cloud and on-premises-to-cloud systemsDeploy integration systems leveraging high security standardsConfigure integration, database and BigData systems with key management and encryption systemsCollaborate with members of teams on different streams of the project, such as (but not limited to): Solutions development, DevOps, Network, Infrastructure
Specific Duties
Participate in On-call Rotation (as required) for emergency technical support and planned maintenance activities.

Qualifications

Required:
1 - 2 years’ experience Azure SQL, MySQL and other Cloud Data ServicesExperience with one or more of the following technologies is required:
o Azure SQL Database
o Azure Key Vault
o Power BI
o Azure Analysis Service
o Azure Blob Storage
o Azure Databricks
o Azure Data Factory
o Azure Data Lake Storage
o Azure Data Lake Analytics
o Data Encryption with Encryption key management in Azure Key Vault

Experience working in an Agile or iterative approach to delivery preferred.
Desired:
Data Migration and Analysis skills, Data and Object Modeling skillsCRM knowledge, preferably Microsoft Dynamics1+ years’ experience with cluster, Always On Availability Groups, Mirroring
Formal Education & Certification
Bachelor's degree in computer science, system analysis or a related field, or equivalent experience.

Hours:
Monday- Friday, 8:00- 4:30

Equal Employment Opportunity

Navy Federal values, celebrates, and enacts diversity in the workplace. Navy Federal takes affirmative action to employ and advance in employment qualified individuals with disabilities, disabled veterans, Armed Forces service medal veterans, recently separated veterans, and other protected veterans. EOE/AA/M/F/Veteran/Disability"
46,Federal - Senior Big Data Engineer,"Washington, DC 20006",Washington,DC,20006,None Found,"
5+ years of work experience with ETL and data modeling
5+ years of work experience with architecting and solutioning
Experience in estimating work and defining an implementation plan
3+ year of experience with the suite of open source big data technologies and platforms (Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra)
3+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale
3+ year of experience with Cloud Technologies (AWS, Azure, Google, etc.)
5+ years of experience with at least one SQL language such as T-SQL or PL/SQL","
5+ years of work experience with ETL and data modeling
5+ years of work experience with architecting and solutioning
Experience in estimating work and defining an implementation plan
3+ year of experience with the suite of open source big data technologies and platforms (Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra)
3+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale
3+ year of experience with Cloud Technologies (AWS, Azure, Google, etc.)
5+ years of experience with at least one SQL language such as T-SQL or PL/SQL",None Found,None Found,None Found,"Organization: Accenture Federal Services
Location: Arlington, VA - Washington, DC

Accenture Federal Services, a wholly owned subsidiary of Accenture LLP, is a U.S. company with offices in Arlington, Virginia. Accenture's federal business has served every cabinet-level department and 30 of the largest federal organizations. Accenture Federal Services transforms bold ideas into breakthrough outcomes for clients at defense, intelligence, public safety, civilian and military health organizations.
We believe that great outcomes are everything. It’s what drives us to turn bold ideas into breakthrough solutions. By combining digital technologies with what works across the world’s leading businesses, we use agile approaches to help clients solve their toughest problems fast—the first time. So, you can deliver what matters most.
Count on us to help you embrace new ways of working, building for change and put customers at the core. A wholly owned subsidiary of Accenture, we bring over 30 years of experience serving the federal government, including every cabinet-level department. Our 7,200 dedicated colleagues and change makers work with our clients at the heart of the nation’s priorities in defense, intel, public safety, health and civilian to help you make a difference for the people you employ, serve and protect.

AFS is seeking a Sr. Big Data Engineer to support our Federal portfolio. This role involves supporting the full software development lifecycle, utilizing emerging technologies and big data design principles in developing data pipelines, interfaces, and architecture to support big data and analytics initiatives. The candidate will work with other architects, engineers, data analysts, data scientists, and data visualizers to bring powerful analytical solutions and insights to our clients.

Basic Skills and Qualifications:
5+ years of work experience with ETL and data modeling
5+ years of work experience with architecting and solutioning
Experience in estimating work and defining an implementation plan
3+ year of experience with the suite of open source big data technologies and platforms (Cloudera/Hortonworks, Spark, Kafka, Presto, Hive, Cassandra)
3+ year of experience in architecting and building scalable data platforms processing data on a terabyte or petabyte scale
3+ year of experience with Cloud Technologies (AWS, Azure, Google, etc.)
5+ years of experience with at least one SQL language such as T-SQL or PL/SQL
Preferred Skills and Qualifications:
Production implementation experience
Production experience in building real-time analytics applications
Experience in both batch and stream processing technologies
Experience with Java or Scala programming languages
Machine learning experience with Spark or similar
Ability to manage numerous requests concurrently and be able to prioritize and deliver
Good communication skills
Dynamic team player
Bachelor’s Degree
An active security clearance or the ability to obtain one may be required for this role.
Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).
Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.
Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.
Accenture is committed to providing veteran employment opportunities to our service men and women."
47,Cloud Data Engineer,"Vienna, VA 22182",Vienna,VA,22182,None Found,"
BS in technical field and proficiency in at least programming language (Java, Python, etc.)
Demonstrable experience with Google Cloud Platform (GCP) technologies, to include tools like: BigQuery, BigTable, Cloud Spanner, KubeFlow, Kubernetes Engine, ML Engine, Compute Engine, DataFlow, etc.
Minimum of three years of experience architecting data analytics and ETL pipelines in cloud environments
Experience cleaning and wrangling data, and working with complex data models",None Found,"
BS in technical field and proficiency in at least programming language (Java, Python, etc.)
Demonstrable experience with Google Cloud Platform (GCP) technologies, to include tools like: BigQuery, BigTable, Cloud Spanner, KubeFlow, Kubernetes Engine, ML Engine, Compute Engine, DataFlow, etc.
Minimum of three years of experience architecting data analytics and ETL pipelines in cloud environments
Experience cleaning and wrangling data, and working with complex data models",None Found,None Found,"Overview
NT Concepts is a national security solutions company HQed in Tyson's Corner, VA that likes to solve hard problems. It’s our thing. We do that by working with national security clients (you’ve heard of them) with very advanced tech (in this case, data science and cloud-based simulators). We are constantly learning, building, and experimenting with the latest technologies. And since life’s too short to spend with difficult people, we hire team members that not only like to solve problems, but that we like to hang out with.
Responsibilities
We are hiring talented, Google cloud-native individuals onto a relatively small, high-performance technical team. This newest project is supporting a DoD client with networking simulation ""games"" distributed all around the continental U.S. The tech is pretty cool and the project is very fast-paced. Occasional CONUS travel required.
Qualifications
Your qualifications and skills include:
BS in technical field and proficiency in at least programming language (Java, Python, etc.)
Demonstrable experience with Google Cloud Platform (GCP) technologies, to include tools like: BigQuery, BigTable, Cloud Spanner, KubeFlow, Kubernetes Engine, ML Engine, Compute Engine, DataFlow, etc.
Minimum of three years of experience architecting data analytics and ETL pipelines in cloud environments
Experience cleaning and wrangling data, and working with complex data models
Added bonus:
Experience with data visualization tools and libraries such as Looker, Tableau, or select
JavaScript libraries such as d3.js
Experience with statistical analyses of data
Experience with DIS message traffic and PDU data
#JT"
48,Data Engineers,"Herndon, VA 20170",Herndon,VA,20170,None Found,None Found,None Found,None Found,None Found,None Found,"Are you tired of being stuck in traffic? Or trying to find a place to park? Then join us at All Traffic Solutions and be part of the solution! At All Traffic Solutions, we are leading the way in the emerging sensor-driven transportation world, helping organizations, governments and technology providers meet solve their traffic problems. We develop cloud-based IoT software and manufacture innovative cloud-connected devices that empower Intelligent Transportation Systems and Smart Cities. We have the benefit of having been in business for 17 years with a very deep and loyal customer base.
We are looking for smart, energetic engineers who want to help realize the value of the IoT and completely transform transportation.

Job Description
As a Data Engineer at All Traffic Solutions, you will be applying your knowledge to build, test, and maintain a highly available, cloud-based data infrastructure that can connect with and manage literally any kind of “thing”. You will deal with streaming data, micro batches, batch, structured, unstructured and semi-structured coming from sensors, enterprise systems, publicly available data to solve some of the coolest problems.

Does this describe you?
A strong hands-on technologist and naturally curious, always looking to learn new things and improve your skills
Love a big data challenge
Big time attention to detail, your code is clear, concise and maintainable (DevOps MVP)
Self-driven – not looking to be micromanaged, but also know when to ask for help
Have a GSD mentality – make and exceed your own commit dates
Excited about realizing the promise of the Internet of Things through technology
Enjoy working in a fast-paced, agile, startup environment


Do you have this? (Or maybe you are really close, naturally curious and can figure it out?)
Experience dealing with massive volumes of data both at rest and in motion – developed using Kafka, Spark, Hadoop, Cassandra, Solr, Jupyterhub, PGSql, etc.
Developed in Java, Python, Scala – others are a plus – for ideally more than 4 years
Experience with or a strong desire to learn about sensor/thing state management and connectivity – Thingworx, AWS IoT, Azure IoT
Your own Github account, automated your home with raspberry pi devices and maybe even your own AWS sandbox account


What All Traffic can offer you:
Competitive salary and flexible work schedule
Benefits including health care, dental, STD/LTD, Life Insurance, 401K with matching, PTO, holiday pay, a well-stocked kitchen, employee referral program,company sponsored events, and many employee-paid benefit options
A fun, yet professional, work environment that truly cares about its employees and is passionate about using technology for solutions to everyday problems.


Join Us!!!
If you are dedicated to quality, have the required experience, and desire a challenging and rewarding position, please forward your resume to jobs@alltrafficsolutions.com
All Traffic Solutions is an equal opportunity employer and does not discriminate on the basis of race, religion, color, sex, age, national origin or disability."
49,Tax Services Senior – National Tax – Tax Technology and Transformation (TTT) – Data Scientist – Advanced Technologies - DC,"Washington, DC",Washington,DC,None Found,None Found,None Found,"Strong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.
Experience with common data science programming languages, such as Python R
Strong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential
Experience with data visualization tools, such as PowerBI, D3.js, etc.
Experience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB
Strong scripting and programming skills
Ownership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.
Ability to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies
Ability to consolidate tax data to make analysis and planning more efficient
Focus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities
Willingness to support project team members in any way needed to help ensure timely completion of deliverables",None Found,None Found,None Found,"Tax Technology and Transformation offers services to companies in response to the impact of existing and emerging technology, including the growing data burden that many businesses face, driving efficiencies to create a cost-effective tax function and the need to understand how to make data an asset. The underlying objective of the combined offerings is to help businesses navigate the digital age of tax transparency alongside new trends in tax compliance and tax audit methods, as well as helping to solve the most pressing challenges that businesses face. Tax Technology and Transformation is composed of the following services:
Digital tax transformation
Tax applications-as-a-service
Tax data and improvement
Tax analytics and reporting enhancement
Emerging tax technology, including robotic process automation (RPA), artificial intelligence (AI), blockchain, cloud solutions, data lake development and business intelligence innovation
Tax technology program mobilization
Custom tax technology application development and deployment
Tax technology strategy and road mapping
Direct and indirect tax systems implementation and configuration
Post-transaction (M&A) tax function operational services
Tax operating model transformation, including process improvement, risk and controls
Tax function assessments
The opportunity

Tax Technology and Transformation is an area that has seen significant growth and investment recently, and you will see that reflected in your experience. It is no exaggeration to say that you will be working on highly publicized projects. The field of taxation is constantly changing as new laws, regulations, and technologies are created, and this is your opportunity to be part of that development.

Key responsibilities

We are looking for an ambitious, self-motivated data scientist or data engineer who will help us discover the information hidden in vast amounts of data, and help us deliver even better products to our clients. Your primary focus will be in applying data mining techniques, doing statistical analysis and building high quantity prediction systems integrated with our products. You will be expected to team on a national and even global scale, so strong communication skills, attention to detail, and ability to effectively drive results are essential.
Selecting features and, building and optimizing classifiers using machine learning techniques
Data mining using state-of-the-art methods
Enhancing data collection procedures to include information that is relevant for building analytic systems
Processing, cleansing and verifying the integrity of data used for analysis
Doing ad-hoc analysis and presenting results in a clear manner

Depending on your unique skills and ambitions, you could be supporting various client projects, ranging from assisting in the production of leading edge machine models, to designing and implementing robust data pipelines that can handle data at a multinational, enterprise scale. Whatever you find yourself doing, you will contribute and help toward developing a highly trained team, all the while handling activities with a focus on quality and commercial value. This is a highly regulated industry, so it is all about maintaining our reputation as trusted advisors by taking on bold initiatives and owning new challenges.

Skills and attributes for success
Strong understanding of machine learning techniques and algorithms, such as Linear/Logistic Regression, k-NN, Naïve Bayes, Support Vector Machines (SVM), Decision Forests, etc.
Experience with common data science programming languages, such as Python R
Strong knowledge and experience using the Python toolkit (Pandas, NumPy, Jupyter Notebooks, etc.) are essential
Experience with data visualization tools, such as PowerBI, D3.js, etc.
Experience with one of the following: SQL and NoSQL database technologies, SQL Server, MongoDB
Strong scripting and programming skills
Ownership of assigned tasks and monitoring them until completion, including documenting requirements, configuration, testing and debugging.
Ability to identify ways to automate manual tasks using existing financial or tax systems and emerging technologies
Ability to consolidate tax data to make analysis and planning more efficient
Focus on improving reporting capabilities to enhance our clients’ ability to evaluate risk and capitalize on opportunities
Willingness to support project team members in any way needed to help ensure timely completion of deliverables
To qualify for the role, you must have
A bachelor’s degree in information system, tax technology, management information systems or computer science or related field and a minimum of two years of related work experience
A passionate interest in data science and its role in the organization
Excellent communication and business writing skills
A natural flair for problem solving and an entrepreneurial approach to work
Strong organizational and time management skills, with exceptional client-serving consulting skills
Demonstrated ability to capture and synthesize business requirements
Desire and demonstrated ability to provide leadership within a team
Ideally, you’ll also have
Experience with Apache Spark
Experience with Hadoop and/or distributed database systems
Experience working in the Microsoft Azure Cloud environment
Experience developing ETL solutions using SSIS or other tools
ERP experience, including SAP and/or Oracle-preferred but not required
Practical experience or strong theoretical understanding of neural networks
What we look for

We are looking for knowledgeable data science professionals with a passion for turning data into actionable insight. You will need strong business acumen and a firm strategic vision, so if you are ready to use those skills to develop your team, this role is for you.

What working at EY offers

We offer a competitive compensation package where you will be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, a minimum of three weeks of vacation plus ten observed holidays and three paid personal days; and a range of programs and benefits designed to support your physical, financial and social well-being. We also offer:
Support and coaching from some of the most engaging colleagues in the industry
Opportunities to develop new skills and progress your career
The freedom and flexibility to handle your role in a way that is right for you
About EY

As a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.
EY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

If you can confidently demonstrate that you meet the criteria above, please contact us as soon as possible.

Make your mark. Apply today.

."
50,Solutions (Data) Engineer,"Arlington, VA 22209",Arlington,VA,22209,None Found,None Found,None Found,None Found,None Found,None Found,"Job Overview
We are looking for a Business Objects report developer with ETL and relational database
expertise. They role will participate in an agile team as a hands-on Business Objects report
developer with Data Stage and relational database experience. The candidate will be a member
of an Operations and Maintenance team working on complex financial regulatory applications.
Day to Day
Support data through to reporting activities in the agency’s regulatory applications.
Create and enhance data structures, ETL and corresponding Business Objects reports to support dynamic business value needs.
Participate in an agile team ceremonies and ceremonies in the development and maintenance of application reporting and data solutions.
Support application performance as it relates to reporting, ETL and database access.
Present information through reports and visualization
Work with stakeholders to understand application needs
Improve performance and extend functionality
Maintain and document operational environment
Background
Strong knowledge of Extraction Transformation and Loading (ETL) processes using DataStage (7.2) ETL Tool, Data Quality using QualityStage.
Experience designing SQL Server and/or Oracle databases, including physical storage and data architecture concepts and performance tuning
Comprehensive understanding of relational database design principles - Snowflake, Star Schema
Excellent written and oral communication skills; Ability to communicate effectively with technical and non-technical staff.
Experience with Business Objects using Web Intelligence, and Development using UDT and IDT universes
Technologies
Reporting: Business Objects 4.2 /XI 3.x/R2/6.5/6.0/5.1, Desktop Intelligence, BO 4.2
Dashboard Designer /Xcelsius 2008 SP4, CMC, BO 4.1 Information Design Tool (IDT), Universe Designer, Web Intelligence, SAP BW 7.3 Cubes. BEx Query Designer, Xcelsius 2008 SP3, SAP Business Objects Explorer, (Web Intelligence, Crystal Reports)
Extraction Transformation and Load (ETL): IBM Software DataStage/Data Quality 7.5
Databases: Oracle 11g, 12c, MS SQL Server, SAP Data Services, SQL Loader, Toad, Oracle PL/SQL, Oracle Portal, SQL, T-SQL, MS
Operating Systems: Redhat Linux, Windows,

Required Qualifications
Bachelor's degree in Computer Science, MIS, or related
Seven+ years of experience as a hands-on solution engineer
Ability to obtain a public trust"
51,Senior Data Engineer,"Arlington, VA 22209",Arlington,VA,22209,None Found,None Found,"BS/MS/PhD in Computer Science or a related field.
7+ years of relevant engineering work experience and 2+ hands-on technical management experience.
Experience designing and delivering frameworks to facilitate the data development lifecycle (e.g., mainly testing and deploying pipeline code).
Demonstrated experience working with internal customers and/or end users.
Experience in data integration, ETL, or pipeline design and implementation.
Use and development of open source technologies such as Hadoop, Kafka, or Spark.
Solid understanding of data modeling best practices.
Experience designing and deploying high performance systems with robust monitoring and logging practices.
Knowledge of core ML concepts (e.g., feature discovery and engineering, model validation, retraining strategies).
Knowledge of relational databases and query authoring.
Ability to obtain a Department of Defense security clearance up to the Top Secret SCI level
","Develop and automate large scale, high-performance data processing systems and tools to improve data quality and volume, accelerate iteration cycles for anyone working with data in DoD, and improve our ability to deliver AI-enabled capabilities as a Department.
Understand the data needs of our internal teams and abstract problems and requests to build data engineering solutions along with your partners in engineering and design.
Be the champion for the correct use of data and help establish Department-wide best practices. Write technical papers and blog posts.
","BS/MS/PhD in Computer Science or a related field.
7+ years of relevant engineering work experience and 2+ hands-on technical management experience.
Experience designing and delivering frameworks to facilitate the data development lifecycle (e.g., mainly testing and deploying pipeline code).
Demonstrated experience working with internal customers and/or end users.
Experience in data integration, ETL, or pipeline design and implementation.
Use and development of open source technologies such as Hadoop, Kafka, or Spark.
Solid understanding of data modeling best practices.
Experience designing and deploying high performance systems with robust monitoring and logging practices.
Knowledge of core ML concepts (e.g., feature discovery and engineering, model validation, retraining strategies).
Knowledge of relational databases and query authoring.
Ability to obtain a Department of Defense security clearance up to the Top Secret SCI level
","BS/MS/PhD in Computer Science or a related field.
7+ years of relevant engineering work experience and 2+ hands-on technical management experience.
Experience designing and delivering frameworks to facilitate the data development lifecycle (e.g., mainly testing and deploying pipeline code).
Demonstrated experience working with internal customers and/or end users.
Experience in data integration, ETL, or pipeline design and implementation.
Use and development of open source technologies such as Hadoop, Kafka, or Spark.
Solid understanding of data modeling best practices.
Experience designing and deploying high performance systems with robust monitoring and logging practices.
Knowledge of core ML concepts (e.g., feature discovery and engineering, model validation, retraining strategies).
Knowledge of relational databases and query authoring.
Ability to obtain a Department of Defense security clearance up to the Top Secret SCI level
","Redhorse Corporation is building a cross-functional team to support the Joint Artificial Intelligence Center (JAIC) within the Department of Defense (DoD). We will help our customer accelerate the delivery AI-enabled capabilities, scale Department-wide impact of AI, and synchronize AI activities to expand customer advantages. Our team will enable the DoD to seamlessly build, deploy, and operate machine learning solutions for DoD-level problems.

Position Description

Redhorse Corporation is currently seeking a Senior Data Engineer to join our team. The ideal candidate will provide technical leadership and influence and partner with fellow engineers, commercial, governmental, and academic partners to model, design, and deliver pipelines and tools that empower users throughout DoD to build high-quality datasets and data products. This data includes structured and unstructured text documents as well as massive scale video, image, acoustic, and other forms of data. The initial work location for this position is in the National Capital Region (NCR). There is potential for a relocation of this position during the period of performance to a city outside of the NCR.

Role and Responsibilities
Develop and automate large scale, high-performance data processing systems and tools to improve data quality and volume, accelerate iteration cycles for anyone working with data in DoD, and improve our ability to deliver AI-enabled capabilities as a Department.
Understand the data needs of our internal teams and abstract problems and requests to build data engineering solutions along with your partners in engineering and design.
Be the champion for the correct use of data and help establish Department-wide best practices. Write technical papers and blog posts.
Minimum Basic Requirements for Skills, Experience, Education and Credentials
BS/MS/PhD in Computer Science or a related field.
7+ years of relevant engineering work experience and 2+ hands-on technical management experience.
Experience designing and delivering frameworks to facilitate the data development lifecycle (e.g., mainly testing and deploying pipeline code).
Demonstrated experience working with internal customers and/or end users.
Experience in data integration, ETL, or pipeline design and implementation.
Use and development of open source technologies such as Hadoop, Kafka, or Spark.
Solid understanding of data modeling best practices.
Experience designing and deploying high performance systems with robust monitoring and logging practices.
Knowledge of core ML concepts (e.g., feature discovery and engineering, model validation, retraining strategies).
Knowledge of relational databases and query authoring.
Ability to obtain a Department of Defense security clearance up to the Top Secret SCI level
The work environment for this position requires an individual to be able to
Work sitting or standing at a desk or conference table for extended periods of time (1-3 hours), with the ability to shift positions while working: sit, stand, pace, adjust positioning in any of those without issue
Use a telephone or other communications devices to communicate with co-workers, customers and stakeholders
Move short distances in the office to collaborate with co-workers, attend meetings, greet visitors or retrieve documents from the printer
Employ the social skills necessary for engagement and collaboration with team members, other co-workers, customers, and other stakeholders without hesitation or intimidation
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.

EOE/M/F/Vet/Disabled"
52,Office of Strategic Operations Enterprise Integration Engineer,"Springfield, VA",Springfield,VA,None Found,None Found,"
B.S. in Engineering, CS, Info Systems, Math, or related scientific or technical discipline
16+ years' experience in directly related field, with 10+ years GEOINT/geospatial experience
Data engineer and requirement development; understanding of metrics and interpretation of metrics
5+ yrs. recent ISR TCPED (Tasking, Collection, Processing, Exploitation, and Dissemination) SE&I experience
Experience using logic and reasoning to identify the strengths and weaknesses of alternative technical solutions, conclusions or approaches to problems (studies & analyses, white papers)
Demonstrated ability/experience identifying/solving problems reviewing related information to develop/evaluate options and implement solutions with minimal supervision / direction
Strong interpersonal, problem solving, organizational, multi-tasking, load-balancing skills
Excellent written, verbal, presentation skills; Able to lead technical forums/initiatives/studies
Technical project engineering project leadership skills/experience; systems analysis experience
",None Found,None Found,None Found,None Found,"This position is currently open.
Position Title: Office of Strategic Operations Enterprise Integration Engineer (#1476)
Location: Springfield, VA
Clearance Required: Top Secret/SCI

Description:
Candidates for this position should have strong data engineering background and the ability to interpret metrics and translate these metrics into requirements. Additionally, a strong understanding of Agency Program Build process is a plus. The ability to strategically plan and execute the director's initiative, mission, and vision for the out years is a must.
This position requires the candidate to be able to plan and perform full SE&I life cycle technical design, CONOPS development, implementation planning, integration, cost, schedule, technical performance, opportunity/risk, and supportability/mission effectiveness analyses spanning all levels of NSG/ASG enterprise. Ensure logical and systematic conversion of mission needs, joint topical or product requirements into total systems solutions through enterprise transition to operations, acknowledging cost, schedule, and technical constraints. Manage GEOINT mission-driven capability needs across NSG/ASG, driving requirements closure, performing functional and timeline analysis, detailed trade studies, AoAs, mission needs allocation & interfaces definition translating customer requirements into capability and service specs. Plan and coordinate NSG/ASG mods, with emphasis on Agency systems, segments, platforms, capabilities, services, and functions to support GEOINT data types, formats, metadata, and capacity. Apply appropriate project management model selected for capability implementation efforts and perform end-to-end project analysis, to ensure capabilities being implemented comply with NSG/ASG enterprise technical architecture and support NSG/ASG enterprise control gates. Manage, develop, coordinate, document, integration of GEOINT functions into existing Agency segments.

Required Qualifications:

B.S. in Engineering, CS, Info Systems, Math, or related scientific or technical discipline
16+ years' experience in directly related field, with 10+ years GEOINT/geospatial experience
Data engineer and requirement development; understanding of metrics and interpretation of metrics
5+ yrs. recent ISR TCPED (Tasking, Collection, Processing, Exploitation, and Dissemination) SE&I experience
Experience using logic and reasoning to identify the strengths and weaknesses of alternative technical solutions, conclusions or approaches to problems (studies & analyses, white papers)
Demonstrated ability/experience identifying/solving problems reviewing related information to develop/evaluate options and implement solutions with minimal supervision / direction
Strong interpersonal, problem solving, organizational, multi-tasking, load-balancing skills
Excellent written, verbal, presentation skills; Able to lead technical forums/initiatives/studies
Technical project engineering project leadership skills/experience; systems analysis experience

Desired Qualifications:

M.S. in Engineering, CS, Info Systems, Math, or related scientific or technical discipline
Relevant SE&I certification (INCOSE xSEP), IT (ITIL), Agile [ScrumMaster]), etc.
Knowledge of Agency Portfolio Strategic Initiatives (ABI, SOM, OBP, etc.)
Knowledge/experience with all source all domain (space, air, sea, land)
Experience working with mission partners complying with export control/ITAR requirements

Compass, Inc. (Compass) is a Woman-Owned Small Business (WOSB) headquartered in Herndon, VA as a Defense and Intelligence solutions provider to the United States Government. We provide Systems Engineering and Technical Assistance (SETA), Advisory and Assistance Services (A&AS), and Systems Engineering and Integration (SE&I) to our government and business partner customers. As a premier Defense and Intelligence solution provider, we employee a diverse, agile, highly trained and extremely talented staff.
Compass, Inc. is an affirmative action/equal opportunity employer (M/F/D)"
53,Data Engineer,"Washington, DC 20010",Washington,DC,20010,None Found,None Found,None Found,None Found,None Found,None Found,"ABOUT THE DC PUBLIC CHARTER SCHOOL BOARD (DC PCSB)

Every day, we're changing the way public education operates in Washington, DC - making it better for students, teachers, and the community. In fact, DC Public Charter School Board (DC PCSB) currently does that for 62 public charter schools on 123 campuses across the city and more than 43,000 students who deserve a better education and a better future.

This work matters.

ABOUT THE ROLE
That's where you come in. Data engineering at DC PCSB makes better, faster, and more accurate analysis and reporting possible by building platforms and tools, and ensuring the rest of the agency and its stakeholders have access to the data they need. We operate in a fast-paced atmosphere, and use automation and testing to meet the ever-evolving business requirements.

You'll work with both technical and non-technical audiences to elict project requirements, and translate them into reliable data products. You'll be responsible for building and maintaining data pipelines using Apache Airflow, working with analysts to design data collection and reporting processes, and enhancing our in-house data processing tools.

You'll also be responsible for managing client access to our data systems. This work is currently manual, requires excellent customer service skills, and is important to our success. In an ideal world, you'll help us build the next generation of data systems which automates these tasks. Your experience building data-driven web applications or APIs will help us get there faster.

After a year in the role, you'll be thoroughly versed in DC PCSB's data assets and infrastructure, and will be able to make good design decisions under pressure. You'll be proactively making recommendations for process improvement, and will be able to lead code reviews and discussions about how to evolve our code style guide.

Competencies and Qualifications:
You're detail oriented, independent, and able to develop innovative solutions to difficult problems. You're comfortable with large quantities of data, and understand relational databases inside and out. You see change requests as an opportunity to improve both our products and the quality of education provided to DC students.

Technically speaking, you have strong Python skills and experience managing PostgreSQL databases. Experience working with Linux is a must; experience with AWS is a plus. Our codebase and BI tool use GitLab for version control and issue management; you'll need to have at least basic proficiency in git to hit the ground running.

Compensation
Salary is competitive and commensurate with prior experience in a similar role. DC PCSB offers a comprehensive benefits plan. The salary band for this position is $75,000 - $85,000.

DC PCSB is an equal opportunity employer committed to building a culturally diverse staff. We strive to foster an environment where everyone feels included. We believe that when people bring their unique identities, backgrounds, perspectives and experiences to our community, we are able to truly achieve excellence in our work.

TO APPLY
The review of applications will begin immediately and will continue until the position is filled. DC PCSB is not enrolled in e-verify and does not sponsor individuals for work visas.

X3iz1jvVJ4"
54,AWS Cloud Data Architect/Developer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,"Responsible for developing and delivering iterations of a data warehouse, hosted by Amazon Web Services (AWS), that will meet the agency’s needs for an integrated data environment
Help estimate and plan phases of work
Adhere to standards, Agile methodology, and compliance requirements
Perform technical analysis, data analysis, ETL design, ETL development, and testing to support stakeholder requirements and user stories
Create logical and physical data models (both normalized and dimensional) and implement physical data models
Collaborate with other team members, architects, and stakeholders to design a technical solution that meets the client’s needs and expectations
Deliver written or oral status reports regularly",None Found,None Found,"Overview
The goal of this project is to integrate a federal agency’s most important data within a Cloud data environment and optimize it there for reporting and analytics uses. The agency needs to have data from multiple source systems extracted, transformed, and loaded to the Cloud on a daily basis so that visualizations, dashboards, and reports can be generated to provide complete and accurate information for both internal users and external stakeholders. Distinctive aspects of this effort include:
A scalable approach and architecture that will not only meet the agency’s immediate needs but will also result in a centralized platform (data lake) that can be utilized by other agencies in the Department
The ability to read data at any time from the original data source and to perform “just-in-time” queries, regardless of the source
An architecture that can support advanced data analytics by means of built-in capabilities such as machine learning, graph processing, and real-time streaming analytics
Responsibilities
Responsible for developing and delivering iterations of a data warehouse, hosted by Amazon Web Services (AWS), that will meet the agency’s needs for an integrated data environment
Help estimate and plan phases of work
Adhere to standards, Agile methodology, and compliance requirements
Perform technical analysis, data analysis, ETL design, ETL development, and testing to support stakeholder requirements and user stories
Create logical and physical data models (both normalized and dimensional) and implement physical data models
Collaborate with other team members, architects, and stakeholders to design a technical solution that meets the client’s needs and expectations
Deliver written or oral status reports regularly
Qualifications
The ideal candidate for this effort has a strong background in ETL development for data warehouses, both Cloud and on-premise data architecture, and the tools and services that are available in the AWS stack

Minimum Requirements are:
US Citizenship
5+ years of experience with ETL development, data warehouse architecture, and data modeling
5+ years of experience with manipulating SQL- and NoSQL-based file types and data structures and troubleshooting query performance, joins, aggregations, and analytic functions
3+ years of experience with Cloud-based architectures for data processing, ETL/ELT, and with data ingestion tools such as S3 and AWS Glue
1+ year of experience as a development team member in an Agile/Scrum environment
Ability to write high-level user stories based on customer needs and to break them down into actionable technical tasks
Proficiency in Python
Familiarity with structured data formats, including JSON, XML, and XHTML
Strong background in developing and automating operations and workflow procedures for data transformation
Experience with deployment automation tools
Proven ability to work creatively and analytically in a problem-solving environment
Ability to work in diverse environments and communicate complex ideas to teammates, clients, and non-technical experts
Excellent documentation habits
Excellent communication skills (written and oral) and interpersonal skills
Highly motivated, good attention to detail, and able to follow through on tasks
Team player
Bachelor's degree and certifications in a technical field
Desired Qualifications:
AWS Certified Solution Architect, or comparable certification (Google Cloud Professional Cloud Architect, Google Professional Data Engineer, or Microsoft Azure Architect)
Strong experience with the tools and services in the AWS stack, including S3, Redshift, AWS Glue, and AWS Lake Formation
Previous consulting or client service delivery experience on AWS
EOE/Minorities/Females/Vet/Disabled
We are an equal opportunity employer that values diversity and commitment at all levels. All individuals, regardless of personal characteristics, are encouraged to apply. Employment policies and decisions on employment and promotion are based on merit, qualifications, performance, and business needs. The decisions and criteria governing the employment relationship with all employees are made in a nondiscriminatory manner, without regard to race, religion, color, national origin, sex, age, marital status, physical or mental disability, medical condition, veteran status, or any other factor determined to be unlawful by federal, state, or local statutes."
55,Data Engineer,"Bolling AFB, DC",Bolling AFB,DC,None Found,None Found,None Found,"
B.S. or equivalent degree in computer science, mathematics or other relevant fields.
3-7 years of hands-on experience in ETL, Data warehouse, Data Marts, Visualization and/or building data pipelines, modeling and designing schema for data lakes or for data platforms.
Must have previous experience with Java
Experience implementing and resolving dependency issues from common Java build tools: Maven (preferred), Gradle, Ant, or equivilent
Experience with Agile implementation methodologies.
Experience using common CI/CD tools, such as Jenkins
Experience working with NoSQL databases in both the extract and load aspects of the ETL process.
Strong programming and scripting skills experience and expertise in two or more of the following: XML/XSLT, Python, Perl, Shell Scala, C.
Proficient in big data/distributed computing frameworks such as Spring, Hadoop, Apache Hive, Spark, Kafka, etc.
Familiarity and/or a strong willingness to learn ApacheNiFi and NiFi Registry.
Practice working with, processing, and managing large data sets (multi TB/PB scale).
Similar roles: Database Administrator (DBA), Database Operator (DBO), Data Architect, Data Manager, Data Analyst, Data Integrator, Systems Integrator, Systems Engineer.","
Lead efforts to create standard documentation for the various software development life cycle processes ensuring uniformity across the office. Conduct research and apply best practices and ensures that they are approved and documented. Critically analyzes processes/systems/practices to identify current gaps and opportunities for improvement.
Facilitate business process reengineering documenting the ""As Is"" processes, identify recommended process improvements and document approved changes in To-Be process documents and diagrams. Conduct interviews and evaluate current documentation to gain insight.
Develop process functional flow diagrams (MS Visio) in support of requirements capture and development based on existing policy, instructions and interviews.
Provide business process improvement recommendations and briefs to ADO3 leadership.
Propose and create standard operating procedures and diagrams to provide direction to ADO3 leadership, system development team members, Requirements Analysts and others.
Collaborate and recommend improvements to JIRA with JIRA SMEs and JIRA Administrators.
Attend Change Control Board (CCB) & Requirements Working Group Meetings. Capture notes, disseminate for client review and further dissemination ensuring continuity throughout the CCB activities, prepare and submit after action reports (AAR). Your primary role will be to design and build data pipelines to meet overall architecture requirements. You will be focused on helping client projects on data collection, integration, prep/transformation, and implementing services such as data processing and/or machine learning on datasets. In this role, you will work on both traditional and on some of the latest technologies, collaborate with teams for data pipelines and delivery, interact daily with management, and help build a great program of operations.",None Found,None Found,"Data Engineer
Location: Washington, D.C., Bolling AFB (DIAC)
Department: Defense Intelligence Agency (DIA)
Type: Full Time
Minimum Experience: Experienced
Security Clearance Level: TS/SCI with CI PolyThe clearance level stated above must be met for consideration for this specific opportunity. Unfortunately, FTC is unable to sponsor at this time.
Military Veterans are highly encouraged to apply!

Favor TechConsulting, LLC (FTC) is in search of a talented Data Engineer with extensive Defense Intelligence Agency (DIA) experience.
Data Engineers are responsible for the creation and maintenance of analytics infrastructure that enables almost every other function in the data world. They are responsible for the development, construction, maintenance and testing of architectures, such as databases and large-scale processing systems. We are looking for a talented engineer to join our team. The ideal candidate has significant experience in building scalable data platforms that enable business intelligence, analytics, data science and data products. You must have strong, hands-on technical expertise in a variety of technologies and the proven ability to fashion robust, scalable solutions.
As a Data Engineer, you will assist the client leadership and product leadership teams work through organizational requirements implementation strategies. You will also develop process diagrams and provide technical continuity across the division on implementation strategies, engineering and requirements processes and diagrams, resolution of process issues, and requirements working group support. In addition, you will design a implementation strategy to align the requirements management processes from ADO3 with the solution provider development activities. Other duties include:
Essential Job Functions & Responsibilities Description:
Lead efforts to create standard documentation for the various software development life cycle processes ensuring uniformity across the office. Conduct research and apply best practices and ensures that they are approved and documented. Critically analyzes processes/systems/practices to identify current gaps and opportunities for improvement.
Facilitate business process reengineering documenting the ""As Is"" processes, identify recommended process improvements and document approved changes in To-Be process documents and diagrams. Conduct interviews and evaluate current documentation to gain insight.
Develop process functional flow diagrams (MS Visio) in support of requirements capture and development based on existing policy, instructions and interviews.
Provide business process improvement recommendations and briefs to ADO3 leadership.
Propose and create standard operating procedures and diagrams to provide direction to ADO3 leadership, system development team members, Requirements Analysts and others.
Collaborate and recommend improvements to JIRA with JIRA SMEs and JIRA Administrators.
Attend Change Control Board (CCB) & Requirements Working Group Meetings. Capture notes, disseminate for client review and further dissemination ensuring continuity throughout the CCB activities, prepare and submit after action reports (AAR). Your primary role will be to design and build data pipelines to meet overall architecture requirements. You will be focused on helping client projects on data collection, integration, prep/transformation, and implementing services such as data processing and/or machine learning on datasets. In this role, you will work on both traditional and on some of the latest technologies, collaborate with teams for data pipelines and delivery, interact daily with management, and help build a great program of operations.
Specific Responsibilities:
Build and automate data pipelines.
Work as a member of a team assigned to design and implement data collection, integration, and transformation solutions.
Understand and rapidly comprehend new functional and technical areas and apply detailed and critical thinking to customer solutions.
Propose design solutions and recommend best practices for large-scale data analysis.
Meet the data needs of data scientists
Help reduce code vulnerabilities exposed by static testing tools
Assist with teaching new software developers best practices in coding and development
Transform a variety of input data structures into a unified data structure that aligns with the established ontology
Desired Skills and Experience:
B.S. or equivalent degree in computer science, mathematics or other relevant fields.
3-7 years of hands-on experience in ETL, Data warehouse, Data Marts, Visualization and/or building data pipelines, modeling and designing schema for data lakes or for data platforms.
Must have previous experience with Java
Experience implementing and resolving dependency issues from common Java build tools: Maven (preferred), Gradle, Ant, or equivilent
Experience with Agile implementation methodologies.
Experience using common CI/CD tools, such as Jenkins
Experience working with NoSQL databases in both the extract and load aspects of the ETL process.
Strong programming and scripting skills experience and expertise in two or more of the following: XML/XSLT, Python, Perl, Shell Scala, C.
Proficient in big data/distributed computing frameworks such as Spring, Hadoop, Apache Hive, Spark, Kafka, etc.
Familiarity and/or a strong willingness to learn ApacheNiFi and NiFi Registry.
Practice working with, processing, and managing large data sets (multi TB/PB scale).
Similar roles: Database Administrator (DBA), Database Operator (DBO), Data Architect, Data Manager, Data Analyst, Data Integrator, Systems Integrator, Systems Engineer.
Additional Information:
U.S Citizenship is required for this specific opportunity and all selected applicants will be subject to a government security investigation. This includes but not limited to; meeting the eligibility requirements for access to classified information and the ability to obtain a government-granted security clearance. Individuals may also be subject to a background investigation including, but not limited to; criminal history, employment verification, education verification, drug testing, and creditworthiness.
Favor TechConsulting is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, marital status, disability, veteran status, sexual orientation, or genetic information.
Q38z2blYJn"
56,Senior Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,None Found,"US Citizen
Current TS/SCI CI poly security clearance is required for consideration
10+ years of experience with database and data aggregation
Strong knowledge of big data analysis and storage
Big data experience (Hadoop)
Experience with Structured Query Language
Understanding of JDBC
Understanding of multiple database vendors (Microsoft, Oracle, SAP, SAS, PeopleSoft)
Familiarity with database vendors and offerings
",None Found,"Senior Data Engineer
Washington, DC 20032

Security Clearance: Current TS/SCI CI poly

Culmen International, LLC is seeking a Senior Data Engineer to work in support of a government program. This individual will back up the Lead but will also focus on data aggregation goals, transformations, and linking to enable the best analysis possible. Data Engineer will assist in connecting data sources and optimizing use in the Centrifuge application. All work will be performed on-site at Bolling AFB, DC.

Role and responsibilities include:
Assist Decision Analytics Lead in assessing customer’s data source requirements
Create and update SQL and Oracle SQL queries, functions, and procedures
Help customer achieve data aggregation goals
Create new JDBC data connectors
Build business intelligence visualizations
Create, update, and troubleshoot visual analytic templates
Turn customer requirements into usable visual analytic templates and visualizations
Write scripts and advanced SQL aggregation functions as necessary
Education and experience requirements include:
US Citizen
Current TS/SCI CI poly security clearance is required for consideration
10+ years of experience with database and data aggregation
Strong knowledge of big data analysis and storage
Big data experience (Hadoop)
Experience with Structured Query Language
Understanding of JDBC
Understanding of multiple database vendors (Microsoft, Oracle, SAP, SAS, PeopleSoft)
Familiarity with database vendors and offerings
Benefits: Health, Vision, 401K, Life and Disability Insurance Programs
Job Type: Full Time"
57,Data Manager - TS/SCI Clearance with Poly required,"McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description
We are seeking a Data Engineer will manipulate data and data flows for both existing and new systems. Additionally they will provide support in the areas of data extraction, transformation and load (ETL), data mapping, data extraction, analytical support, operational support, database support, and maintenance support of data and associated systems. As a member of the team, candidates will work in a multi-tasking, quick-paced, dynamic, process-improvement environment that requires experience with the principles of large-scale (terabytes) database development, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.
Required:
Bachelor’s Degree in Computer Science, Electrical or Computer Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience

5-8 years of related software engineering and ETL experience

Experience building and maintaining data flows in NiFi or Pentaho

Excellent organizational, coordination, interpersonal and team building skills

Research, design, develop and/or modifies enterprise-wide systems and/or application software

Develop complex data flows, or makes significant enhancements to existing pipelines

Resolves complex hardware/software compatibility and interface design considerations

Conducts investigations and tests of considerable complexity

Researches emerging technologies to determine impact on application execution

Provides input to staff involved in writing and updating technical documentation

Troubleshoots complex problems and provides customer support for the ETL process

Advises hardware engineers on machine characteristics that affect software systems, such as storage capacity, processing speed, and input/output requirements

Prepares reports on analyses, findings, and project progress

Provides guidance and work leadership to less-experienced software engineers

May serve as a technical team or task leader.
Desired:
Experience with the following languages: Java/J2EE, C, C++, SQL, XML, XQuery, XPath, Ruby on Rails, HTML/XHTML, CSS, Python, Shell Scripting

Knowledge of servers operating systems; Windows, Linux, Distributed Computing, Blade Centers, and cloud infrastructure

Strong problem solving skills

Ability to comprehend database methodologies

Focus on continual process improvement with a proactive approach to problem solving
Researches, designs, develops, and/or modifies enterprise-wide systems and/or applications software
Education
BA/BS
Qualifications
5+
For more than 50 years, General Dynamics Information Technology has served as a trusted provider of information technology, systems engineering, training and professional services to customers across federal, state, and local governments, and in the commercial sector. Over 40,000 GDIT professionals deliver enterprise solutions, manage mission-critical IT programs and provide mission support services worldwide. GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
#DPOST
#CJPOST
#SWDevIC
#ISDCJ
#ERP
#ComebackGDIT
#OpportunityOwned
#GDITCareers
#GDITLife
#WeAreGDIT"
58,Software Developer,"Washington, DC 20001",Washington,DC,20001,None Found,None Found,"
Experience in creating dynamic and static data visualizations
Experience in Data Science (professionally or academically)
Selected candidate shall process for TS SCI clearance
","
Experience in full stack application development using Java or Groovy (mid-level and senior positions available)
Experience interrogating various data stores, such as Accumulo, MongoDB, MySQL, and Oracle a plus.
Experience with one or more scripting languages like Python or Perl a plus.
Experience with search technologies such as ElasticSearch, Solr, and Lucene a plus.
Experience sharing and disseminating search results using tools such as AngularJS, JQuery, Grails, Java, and REST web services a plus.
Experience creating MapReduce jobs to process large amounts of data with Hadoop and other Hadoop Components – HDFS, Hbase, Hive, Sqoop, Kafka, Storm, etc. a plus. (Cloudera experience is a plus)
Knowledge of Amazon Web Services (AWS)
Experience creating data models to translate data in various formats for ingestion into an enterprise data store
Extreme Programming techniques. Proven experience with the technique is a plus.
Experience with maintaining continuous test and deployment environments to support rapid application development.
Strong knowledge of software best practices to enhance the security posture of the application.
",None Found,None Found,"Developers in this role will work directly with analysts in a purposeful, mission-focused effort to develop big data analytic tools aimed at turning raw data into meaningful intelligence for the War Fighter. The Big Data engineer shall work within a small software development team to build scalable, predictable, high-quality and high-performance tools that can process, correlate, and disseminate large volumes of data for specific mission needs using Open Source technology. The developer will work in an extreme programming environment to automate and refine existing mission processes quickly. Development will occur within short Agile development cycles in collocated spaces with analysts. For the right candidate this is the opportunity to work on a senior big data development team and learn new techniques in state of the art cloud environments while contributing significantly with their current technical skill set. Some tele-work allowed.
Clearance Required: Current TS SCI CI Poly clearance

Brief Description:

Be a part of a software development team building big data prototypes on AWS in support of a critical mission requirement.
Use critical thinking and a variety of software tools and languages to tackle new challenges each day.
Interact in a collaborative rapid feedback environment to deliver process improvements to the end user. Strong collaboration and communication skills are required in this environment. Frequent interaction between the analysts and engineers is key in this opportunity.
Use critical thinking and a variety of software tools and languages to tackle new challenges each day.
Experience collecting and processing large volumes of data in various formats, structured and unstructured.
Ability to understand and translate technical jargon to mission-users.

Responsibilities:

Experience in full stack application development using Java or Groovy (mid-level and senior positions available)
Experience interrogating various data stores, such as Accumulo, MongoDB, MySQL, and Oracle a plus.
Experience with one or more scripting languages like Python or Perl a plus.
Experience with search technologies such as ElasticSearch, Solr, and Lucene a plus.
Experience sharing and disseminating search results using tools such as AngularJS, JQuery, Grails, Java, and REST web services a plus.
Experience creating MapReduce jobs to process large amounts of data with Hadoop and other Hadoop Components – HDFS, Hbase, Hive, Sqoop, Kafka, Storm, etc. a plus. (Cloudera experience is a plus)
Knowledge of Amazon Web Services (AWS)
Experience creating data models to translate data in various formats for ingestion into an enterprise data store
Extreme Programming techniques. Proven experience with the technique is a plus.
Experience with maintaining continuous test and deployment environments to support rapid application development.
Strong knowledge of software best practices to enhance the security posture of the application.

Description:

Be a part of a software development team building big data prototypes on AWS in support of a critical mission requirement.
Use critical thinking and a variety of software tools and languages to tackle new challenges each day.
Interact in a collaborative rapid feedback environment to deliver process improvements to the end user. Strong collaboration and communication skills are required in this environment. Frequent interaction between the analysts and engineers is key in this opportunity.
Use critical thinking and a variety of software tools and languages to tackle new challenges each day.
Experience collecting and processing large volumes of data in various formats, structured and unstructured.
Ability to understand and translate technical jargon to mission-users.

Credentials and Experience

US Citizenship required.
Bachelor of Science in Computer Science or related major from an accredited university.
Minimum of two years experience in a technical environment exhibiting the knowledge, skills and abilities above.
Cloud technologies awareness

Desired Skills:

Experience in creating dynamic and static data visualizations
Experience in Data Science (professionally or academically)
Selected candidate shall process for TS SCI clearance

"
59,Principal Data Engineer,"Bethesda, MD 20889",Bethesda,MD,20889,None Found,None Found,None Found,None Found,None Found,None Found,"Description
Job Description:
Are you ready to join Leidos all-star team? Through training, teamwork, and exposure to challenging technical work, let Leidos show how to accelerate your career path.
The Leidos Innovations Center has an exciting opening for you, our next Principal Data Engineer to assist with the design, development and implementation of alternative data ingestion pipelines to augment the National Media Exploitation Center (NMEC) data services working in Bethesda, MD. The DOMEX Data Discovery Platform (D3P) program is a next generation machine learning pipeline platform providing cutting edge data enrichment, triage, and analytics capabilities to Defense and Intelligence Community members. This principal engineer will lead a vital role collaborating as part of a cross-functional Agile team to create and enhance data ingestion pipelines and addressing Big data challenges.
You will work closely with the chief architect, systems engineers, software engineers, and data scientists on the following key tasks:Provide Extraction, Transformation, and Load (ETL) experience coupled with enterprise search capabilities to solve Big Data challengesDesign and implement high-volume data ingestion and streaming pipelines using Open Source frameworks like Apache Spark, Flink, Nifi, and Kafka on AWS CloudLeverage strategic and analytical skills to understand and solve customer and business centric questionsCreate prototypes and proofs of concept for iterative developmentLearn new technologies and apply the knowledge in production systemsMonitor and troubleshoot performance issues on the enterprise data pipelines and the data lakePartner with various teams to define and execute data acquisition, transformation, processing and make data actionable for operational and analytics initiatives
To be successful in this role you need these skills (required):
BS in Computer Science, Systems Engineering, or related technical field or equivalent experience with at least 10+ years in systems engineering or administration (6+ years with a MS/MIS Degree).Must have an active Top Secret security clearance and able to obtain a TS/SCI with Polygraph.4 years of experience with big data tools: Hadoop, Spark, Kafka, NiFi4 years of experience with object-oriented/object function scripting languages: Python (preferred) and/or Java4 years of experience with and managing data across relational SQL and NoSQL databases like MySQL, Postgres, Cassandra, HDFS, Redis, and Elasticsearch4 years of experience working in a Linux environment2 years of experience working with and designing REST APIsExpertise in designing/developing platform components like caching, messaging, event processing, automation, transformation and tooling frameworksExperience developing data ingest workflows with stream-processing systems: Spark-Streaming, Kafka Streams and/or FlinkExpertise transforming data in various formats, including JSON, XML, CSV, and zipped filesExpertise with performance tuning of ETL jobsExperience developing flexible ontologies to fit data from multiple sources and implementing the ontology in the form of database mappings / schemasStrong interpersonal and communication skills necessary to work effectively with customers and other team members.
It would be great if you have specific experiences and skills with the following (preferred):
Data engineering experience in Intelligence Community or other government agenciesExperience with Microservices architecture components, including Docker and Kubernetes. Experience developing microservices to fit data cleansing, transformation and enrichment needsExperience with AWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena and/or GlueExperience with Jira, Confluence and extensive experience with Agile methodologies.Knowledge about security and best practices.Knowledge of Scrum Agile development process and ceremonies including scrums, planning events, backlog grooming, retrospectives and demos.Experience developing flexible data ingest and enrichment pipelines, to easily accommodate new and existing data sourcesExperience with database and data lifecycle managementExperience with software configuration management tools such as Git/Gitlab, Salt, Confluence, etc.Experience with continuous integration and deployment (CI/CD) pipelines and their enabling tools such as Jenkins, Nexus, etc.Detailed oriented/self-motivated with the ability to learn and deploy new technology quickly
Additional Program Information
The DOMEX Data Discovery Platform (D3P) program will advance the state of the art in mission-focused big data analytics tools and micro-service development spanning the breadth of Agile sprints to multiyear research and development cycles. We are looking for you to have a demonstrated aptitude for problem solving complex technical issues, identifying, transforming, thinking outside the box, and a strong sense of accountability. Have a mix of technical excellence, intellectual curiosity, communications skills, customer-focus, and operational experience to improve the performance and user adoption of high-end data analytics platforms in partnership with a highly qualified, highly motivated team. Be motivated, self-driven team player who can multi-task and interact well with others and advise/consult with other team members on systems engineering and software development related issues.
LInC
D3P
External Referral Eligible
External Referral Bonus:
Eligible
Potential for Telework:
No
Clearance Level Required:
Top Secret
Travel:
Yes, 10% of the time
Scheduled Weekly Hours:
40
Shift:
Day
Requisition Category:
Professional
Job Family:
Software Development
Leidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.
Pay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.
Leidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.
All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws."
60,Google Data Engineer,"Washington, DC 20006",Washington,DC,20006,None Found,"Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
",DevOps on an GCP platform. Multi-cloud experience a plus.,None Found,None Found,"Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google GCP Data Engineer is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would be responsible for developing and delivering GCP cloud solutions to meet today’s high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The GCP Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions for our clients. Responsibilities include building data on cloud solutions for customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data solutions on cloud. Using Google GCP cloud technologies, our GCP Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Basic Qualifications
Minimum of 3 years previous Consulting or client service delivery experience on Google GCP
Minimum of 3 years of RDBMS experience
Minimum pf 3 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL, data lake and data warehouse solutions
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Data Storage : Cloud Spanner, Cloud Storage, Cloud Datastore, Cloud SQL, Cloud Bigtable, Cloud Memorystore
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Google Datalab, Tensorflow & Sheets
Bachelors or higher degree in Computer Science or a related discipline.
Able to trval 100% M-TH

Candidate Must Have Completed The Following Certifications
Certified GCP Developer - Associate
Certified GCP DevOps – Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP using tools like Talend, Informatica, Matillion
IoT, event-driven, microservices, containers/Kubernetes in the cloud

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
61,Azure Data Engineer,"Washington, DC 20006",Washington,DC,20006,None Found,"At least 5 years of consulting or client service delivery experience on Azure
",DevOps on an Azure platform,None Found,None Found," Proven ability to build, manage and foster a team-oriented environment
","Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
62,Associate Data Engineer (Full Time Starting Summer 2020),"Washington, DC 20036",Washington,DC,20036,None Found,"
Experience working with relational or multi-dimensional databases
Experience developing logical data models within a data warehouse
Experience developing ETL processes
Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2
Demonstrated mastery in database concepts and large-scale database implementations and design patterns
Proven ability to work with users to define requirements and business issues
Excellent analytic and troubleshooting skills
Strong written and oral communication skills
",None Found,"
Responsible for data modeling and schema design that will range across multiple business domains within higher education
Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas
Work with clients to research and conduct business information flow studies
Codify high-performing SQL for efficient data transformation
Coordinate work with external teams to ensure a smooth development process
Support operations by identifying, researching and resolving performance and production issues
",None Found,None Found,"About EAB

At EAB our mission is to make education smarter and our communities stronger. We harness the collective power of more than 1,500 schools, colleges, and universities to uncover and apply proven practices and transformative insights. And since complex problems require multifaceted solutions, we work with each school differently to apply these insights through a customized blend of research, technology, and services. From kindergarten to college and beyond, EAB partners with education leaders, practitioners, and staff to accelerate progress and drive results across three key areas: enrollment management, student success, and institutional operations and strategy.

At EAB, we serve not only our members but each other—that's why we are always working to make sure our employees love their jobs and are invested in their community. See how we've been recognized for this dedication to our employees by checking out our recent awards.

For more information, visit our Careers page.

The Role in Brief

Associate Data Engineer (Full Time Starting Summer 2020)

Are you a data enthusiast who seeks to tease out meaning from complex data flows and assets? Are you a talented problem solver who can transform abstract problems into elegant technical solutions? We are looking for a Data Modeler to join our team of engineers and data analysts focused on designing, creating, and delivering data solutions as part of our state-of-the-art cloud based products. The successful candidate will have the opportunity to build a world-class solution to help our higher education clients solve challenging problems through data.

Opportunities based in Washington, DC and Richmond, VA.

Primary Responsibilities:

Responsible for data modeling and schema design that will range across multiple business domains within higher education
Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas
Work with clients to research and conduct business information flow studies
Codify high-performing SQL for efficient data transformation
Coordinate work with external teams to ensure a smooth development process
Support operations by identifying, researching and resolving performance and production issues
Basic Qualifications:

Experience working with relational or multi-dimensional databases
Experience developing logical data models within a data warehouse
Experience developing ETL processes
Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2
Demonstrated mastery in database concepts and large-scale database implementations and design patterns
Proven ability to work with users to define requirements and business issues
Excellent analytic and troubleshooting skills
Strong written and oral communication skills

Ideal Qualifications:

Bachelor’s or Master’s degree in Computer Science or Computer Engineering
Experience working in an AGILE environment
Experience developing commercial software products
Experience with AWS data warehouse infrastructure (redshift, EMR/spark)
GIT expertise

Benefits

Consistent with our belief that our employees are our most valuable resource, EAB offers a competitive benefits package.
Medical, dental, and vision insurance, dependents eligible401(k) retirement plan with company matchGenerous PTODaytime leave policy for community service or fitness activities (up to 10 hours a month each)Wellness programs including gym discounts and incentives to promote healthy livingDynamic growth opportunities with merit-based promotion philosophyBenefits kick in day one, see the full details here.


At EAB, we believe that to fulfill our mission to “make education smarter and our communities stronger” we need team members who bring a diversity of perspectives to the table and a workplace where each team member is valued, respected and heard.

To that end, EAB is an Equal Opportunity Employer, and we make employment decisions on the basis of qualifications, merit and business need. We don’t discriminate on the basis of race, religion, color, sex, gender identity or expression, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law."
63,Technical Program Manager (Professional Services),"Arlington, VA 22203",Arlington,VA,22203,None Found,None Found,None Found,"
Experience working on multiple projects with different stakeholders, and make data-driven decisions and judgment.
Create detailed functional specifications that meet the business needs of our teams and balance business priorities vs. technical constraints.
Define and maintain a product roadmap that optimally balances business priorities, implementation cost, and project risk.
Drive and track the entire software life cycle, from requirements through design, implementation, testing, and deployment.
Provide effective written and verbal status updates on the product roadmap.
Have strong communication skills to interact with executives, non-technical and technical individuals.
Proactively identify and resolve issues that may impair the team's ability to meet strategic, roadmap, financial, and/or technical goals.
Align key internal and external partners to ensure scalable solutions and excellence in execution",None Found,None Found,"Job Description

Kinetica is in search of a Technical Program Manager to focus on Big Data & Analytics. As a Technical Program Manager, you’ll provide data sets, services, and infrastructure to help our business partners make sound, data-driven decisions. You will lead projects to manage large volumes of corporate data, enable data discovery and security, master and integrate disparate datasets, and deliver clean, secure platforms for reporting and advanced analytics.
In this role, you’ll plan requirements, identify risks, manage schedules, and communicate with various project stakeholders.
Responsibilities
Experience working on multiple projects with different stakeholders, and make data-driven decisions and judgment.
Create detailed functional specifications that meet the business needs of our teams and balance business priorities vs. technical constraints.
Define and maintain a product roadmap that optimally balances business priorities, implementation cost, and project risk.
Drive and track the entire software life cycle, from requirements through design, implementation, testing, and deployment.
Provide effective written and verbal status updates on the product roadmap.
Have strong communication skills to interact with executives, non-technical and technical individuals.
Proactively identify and resolve issues that may impair the team's ability to meet strategic, roadmap, financial, and/or technical goals.
Align key internal and external partners to ensure scalable solutions and excellence in execution

Qualifications

6+ years of work experience in the role of data engineer, BI engineer, data architect, data team manager, or Technical Program Manager.
Maturity, negotiation/influence, analytical skills, leadership skills essential to successful program/project management
Ability to drive tough decision making, balancing business goals, technical implications.
Ability to work independently, excellent problem-solving skills
Experience implementing very large data warehouses and Big Data or Data Lake solutions
Expert in writing SQL scripts.
Expert knowledge in an enterprise-class RDBMS and Spark.
A passion for technology.
We are looking for someone who is keen to leverage their existing skills while trying new approaches.
Bachelor's degree or equivalent practical experience.
Experience in Program Management on cross-functional projects.
Experience working with data analytics tools and reporting platforms.
Experience with data analysis using various scripting languages including SQL, tools, and/or platforms.
Ability to be an influential member of a highly integrated team composed of both technical and non-technical members.
Solid communication skills and a team player.
Open to the following locations: NY - IL - TX - WA
Additional Information

All your information will be kept confidential according to EEO guidelines."
64,Data Engineer,"Chantilly, VA",Chantilly,VA,None Found,None Found,"Experience managing streaming data in a data management practice or similar experience managing big data.
Specific data management such as Kafka Administrator for Kafka jobs and Kafka integration.
Experience with the following tools, languages, and applications is beneficial: Apache, Hadoop, Java, Flume, Spark, Zookeeper, SQL",None Found,None Found,None Found,None Found,"Overview
BRMi Technology is seeking a Data Engineer to support a large client in the Northern Virginia area. Selected candidate will manage data input, quality, sharing, security, governance, and output. As well as research, evaluate, design, implement, and maintain system and product solutions, applying knowledge of engineering principles. To provide technical direction and engineering support for projects and infrastructure. Develop and maintain expert functional knowledge of evolving IT engineering industry technologies/competition, concepts and trends.
Qualifications
Experience managing streaming data in a data management practice or similar experience managing big data.
Specific data management such as Kafka Administrator for Kafka jobs and Kafka integration.
Experience with the following tools, languages, and applications is beneficial: Apache, Hadoop, Java, Flume, Spark, Zookeeper, SQL

** BRMi will not sponsor applicants for work visas for this position.**
**This is a W2 opportunity only**

EOE/Minorities/Females/Vet/Disabled
We are an equal opportunity employer that values diversity and commitment at all levels. All individuals, regardless of personal characteristics, are encouraged to apply. Employment policies and decisions on employment and promotion are based on merit, qualifications, performance, and business needs. The decisions and criteria governing the employment relationship with all employees are made in a nondiscriminatory manner, without regard to race, religion, color, national origin, sex, age, marital status, physical or mental disability, medical condition, veteran status, or any other factor determined to be unlawful by federal, state, or local statutes."
65,"Cleared Data Engineer openings (ETL Engineering, NiFi or Pentaho build experience)","McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,"
Bachelor’s Degree in Computer Science, Electrical or Computer Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience
Mid level roles available requiring 3-5 years of experience and senior to SME roles available requiring 8-15+ years of related software engineering and ETL experience
Experience building and maintaining data flows in NiFi or Pentaho
Excellent organizational, coordination, interpersonal and team building skills.
","Position is with the VA McLean Customer and requires an active TS/SCI with Full Scope Poly clearance.
Position is funded and vacant and the customer is actively looking to hire someone quickly!
The years of experience can be flexible, as there are multiple openings and they will hire individuals with anywhere from 3 to 15+ years of experience (with the associated rate).

The Data Engineer will manipulate data and data flows for both existing and new systems. Additionally they will provide support in the areas of data extraction, transformation and load (ETL), data mapping, data extraction, analytical support, operational support, database support, and maintenance support of data and associated systems. As a member of the team, candidates will have the ability to work within a super-computing environment, exploit new and expanding datasheets, develop custom algorithms, and work in a multi-tasking, non-traditional analytic environment with rapidly changing priorities. The successful candidate must have demonstrated experience applying and tailoring complex extraction rules on massive amounts of unstructured data using tools such as Netowl. He or she much also have experience with the principles of large-scale (terabytes) database development, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.
Requirements
Bachelor’s Degree in Computer Science, Electrical or Computer Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience
Mid level roles available requiring 3-5 years of experience and senior to SME roles available requiring 8-15+ years of related software engineering and ETL experience
Experience building and maintaining data flows in NiFi or Pentaho
Excellent organizational, coordination, interpersonal and team building skills.
Benefits
Leading Path is an award-winning Information Technology and Management Consulting firm focused on providing solutions in process, technology, and operations to our government and Fortune 500 clients. We offer a professional and work environment with a strong work-life balance. Leading Path provides a comprehensive and competitive benefits package, 401K, tuition reimbursement and opportunities for professional growth and advancement."
66,Devops/Data Engineer,"Reston, VA",Reston,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Company Overview


For 30 years, clients in the private and public sectors have relied upon SOS International LLC (SOSi) for critical operations in the world’s most challenging environments. SOSi is privately held, was founded by its current ownership in 1989, maintains corporate headquarters in Reston, VA, and specializes in providing logistics, construction, training, intelligence, and information technology solutions to the defense, diplomatic, intelligence and law enforcement communities.

All interested individuals will receive consideration and will not be discriminated against on the basis of race, color, religion, sex, national origin, disability, age, sexual orientation, gender identity, genetic information, or protected veteran status. SOSi takes affirmative action in support of its policy to advance diversity and inclusion of individuals who are minorities, women, protected veterans, and individuals with disabilities.


EXOVERA-190924-9639: Devops/Data Engineer

Location U.S. - Virginia - Reston

Open Date 9/24/2019



JOB DESCRIPTION

Exovera is seeking a Devops/Data Engineer to support backend API services and manage data flow throughout our AI based media analysis platform.

ESSENTIAL JOB DUTIES

Develop, Build, and Support API and Data streaming/search applications to deliver functional, highly available web applications
Reshape EXOVERA's current data ingress/egress and routing
Work with Data Scientist to facilitate model Development/training

DESIRED QUALIFICATIONS
NIFI Processor DevelopmentAPI /Microservice development experienceA basic understanding of ML packages used in classifying data (Tensorflow, Torch, Theano, etc.)AWS IAM, Cognito RDS, EC2 experience including scriptingA familiarity with data cleaning techniques from webscraping sourced dataFluency in a foreign language



MINIMUM REQUIREMENTS

3+ years working in Development/Suppport of Big Data applicationsDemonstrated proficiency with Python or JavaA basic Understanding of Data RoutingExperience with data flow tools (Nifi, AWS, Kinesis etc.)Experience with Elasticsearch setup, config deployment


ADDITIONAL INFORMATION


WORK ENVIRONMENT
Normal office working conditions with possible requirement to lift and/or move objects or packages of up to 25 lbs.Periods of non- traditional working hours including consecutive nights or weekends when necessary"
67,Business Intelligence / Data Engineer II,"Herndon, VA",Herndon,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Bachelor’s Degree in Computer Science, Information Systems, Mathematics, Statistics, or related field5+ years of experience with Data modeling, SQL, ETL , Data Warehousing and Datalakes5+ years experience in writing SQL scripts Expert knowledge in an enterprise class RDBMS4+ years of experience with enterprise-class Business Intelligence tools such as Microstrategy, PowerBI, Tableau, Oracle BI, Penthao, etc.

As a Business Intelligence / Data Engineer you will enable data-driven decision making within the Amazon Web Services Data Center Infrastructure Operations organization.
The Infrastructure Operations Team is responsible for planning, implementing, monitoring and continuously improving the global Amazon Data Center infrastructure. The team supports all aspects of the Data Center based organizations, including but not limited to : Safety, Security, maintenance, daily operations, logistics, engineering and equipment management.
You will be developing, implementing and maintaining the information data warehouse, and utilizing insight platforms, to enable decision support systems for the overall organization.
You should have excellent business and communication skills, and be able to work with business owners to understand their data and reporting requirements.
Above all, you should be passionate about working with huge data sets and be someone who is able to bring data sets together to answer business questions and drive growth. You will build ETLs to ingest the data into the data warehouse and datalake, as well as end-user facing reporting applications. You will primarily support teams within the Infrastructure environment, but will also have opportunities to support teams in the overall Amazon Web Services community.
You will work with business customers and development teams to define analytics requirements and then deliver flexible, scalable, end-to-end solutions.
You will have an opportunity to work with big data and emerging technologies while driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, metadata, reporting, and dashboarding.
You should have expertise in the design, creation, management, and business use of large datasets. .

Ability to balance and prioritize multiple conflicting requirements with high attention to detail.Excellent verbal/written communication & data presentation skills, including ability to succinctly summarize key findings and effectively communicate with both business and technical teams.Comfortable working in a Linux environmentExperience with scripting language such as Python, Perl, Ruby or JavascriptExperience with MPP databases such as RedshiftKnowledge of AWS products and servicesExposure to predictive/advanced analytics and tools (such as R, SAS, Matlab)Experience with Datalake developmentExposure to noSQL databases (such as DynamoDB, MongoDB)Meets/exceeds Amazon’s leadership principles requirements for this roleMeets/exceeds Amazon’s functional/technical depth and complexity for this role
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation / Age"
68,Python Data Engineer,"Reston, VA 20191",Reston,VA,20191,None Found,None Found,"
3+ years of Python Development, with emphasis in ETL Development
5+ years of SQL experience, with emphasis in Data Analysis
Proficiency in relational database design and development
Experienced building and scaling batch/asynchronous systems
Hands-on development using and migrating data to cloud platforms, AWS
Analytical approach to problem-solving; ability to use technology to solve business problems","Display passion for delivering high quality products that meet customers' needs
Solving data-oriented problems in an analytical and iterative fashion
Perform analysis, architecture, design, and development of cloud data solutions
Working with various kinds of data (structured, unstructured, metrics, logs, json, xml, etc.)
Working in various agile methodologies (Scrum, Kanban, SAFe)",None Found,None Found,"Python Data Engineer

We are looking for a Python Data Engineer to join one of our Federal Health IT engagements. Successful candidates are passionate, self-driven problem-solvers who love taking on new challenges using the latest data and cloud technologies. They also love data and keep up with the latest technology trends. They tinker, explore and regularly read to stay in touch with new data trends and are passionate about discovering ways to improve quality, reusability, extensibility, and consistency. Successful candidates should also be multi-faceted with a great mix of technical and interpersonal skills, to succeed in highly collaborative and agile work environments. As a Modern Data Engineer, this person will design and deliver innovative solutions for Postgres and Redshift on Amazon Web Services, using core cloud tools.

Responsibilities:
Display passion for delivering high quality products that meet customers' needs
Solving data-oriented problems in an analytical and iterative fashion
Perform analysis, architecture, design, and development of cloud data solutions
Working with various kinds of data (structured, unstructured, metrics, logs, json, xml, etc.)
Working in various agile methodologies (Scrum, Kanban, SAFe)
Required Skills:

3+ years of Python Development, with emphasis in ETL Development
5+ years of SQL experience, with emphasis in Data Analysis
Proficiency in relational database design and development
Experienced building and scaling batch/asynchronous systems
Hands-on development using and migrating data to cloud platforms, AWS
Analytical approach to problem-solving; ability to use technology to solve business problems
Desired Skills:

Data pipeline orchestration tools such as Airflow, Amazon Glue
Familiarity with PostGres, Redshift
Cloud platform certification(s) (example: AWS Certified Solutions Architect)"
69,Data Engineer (ETL) - TS/SCI w/poly,"McLean, VA 22102",McLean,VA,22102,None Found,None Found,None Found,None Found,None Found,None Found,"Description
Job Description:
Leidos has a need for an analyst or data scientist with experience in training for a project located in Mclean, VA. In this role, candidate will work at a customer site to provide intelligence support for analytic methodologist customers across regional domains to include analysis and research for industry, infrastructure, technology, biographic, and targeted vulnerability. Provides support to analysis of social networks on an ad hoc basis as directed. The ideal candidate is able to make sense of large datasets in support of network and geospatial analysis. 20% of the candidate’s time will also be dedicated to supporting and instructing an introduction and advanced social network analysis class.
Other work projects will include: work on quantitative analysis projects, support the development of new methodologies, support the development of customer’s analytics codebase and train customer in use of social network analysis methodologies
Required Skills and Experience:Candidates must have a TS/SCI with Poly to be consideredBachelor's Degree and at least 2 years' experience or 6 years prior experienceExperience with Python, R, or other programming languagesFamiliarity with relational data and quantitative analysis thereofStrong basic computer skills (Microsoft office, specifically excel)Strong presentation skills and ability to interact positively with students in a learning environmentStrong organizational skillsExperience with GithubExperience with network analysis tools such as Analyst’s Notebook, Renoir, ORA, or UCINET
External Referral Bonus:
Eligible
Potential for Telework:
No
Clearance Level Required:
Top Secret/SCI with Polygraph
Travel:
Yes, 10% of the time
Scheduled Weekly Hours:
40
Shift:
Day
Requisition Category:
Professional
Job Family:
Data Scientist
Leidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.
Pay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.
Leidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.
All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws."
70,Data Engineer – All Levels,"Reston, VA 20194",Reston,VA,20194,None Found,"
Databases/Data Stores: Oracle, MySQL, HIVE, HBASE, and HDFS
Frameworks: Hadoop, Rails, JavaScript Frameworks, SOA/WebServices, JSP
Indexing: SOLR and Lucine
Development/Scripting Languages: JAVA (J2EE), Python, Ruby, JavaScript, MapReduce, Pig, XML, SQL, JAQL, HTML, CSS, XML, BASH, ANT, and Perl",None Found,"
Design and develop methods, processes, and systems to consolidate and analyze structured and unstructured data from diverse sources including ""big data"" sources.
Develop and use advanced software programs, algorithms, query techniques, model complex business problems, and automated processes to cleanse, integrate, and evaluate datasets.
Analyze the requirements and evaluate technologies for data science capabilities including one or more of the following: Natural Language Processing, Machine Learning, predictive modeling, statistical analysis and hypothesis testing.
Develop information tools, algorithms, dashboards, and queries to monitor and improve business performance. Maintain awareness of emerging analytics and big-data technologies.
Designs, implement, and maintain standard data interfaces for data ingest including Extract/Transform/Load (ETL) methodology and implementation, APIs, RESTful Web Services, data quality, and data cleansing.
Provide data services, data administration, data management, and ""Big Data"" support in client/server, virtual machine, Hadoop, and cloud infrastructure environment and/or migrations between these environments.
Database installation, configuration, and the upgrading of database server software and related products, backup and recovery policies and procedures, database implementation, security, optimization, multi-domain operation, and performance management.
Hadoop, cloud, and other technologies associated with data storage, processing, management, and use.
The migration/transition of database capability into cloud based technologies and/or creation of interfaces between classic relational databases and key indexes to cloud based columnar databases and map reduce index capabilities.",None Found,"
ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED. MUST BE A U.S. CITIZEN.","At DataSync Technologies, our data engineering professionals touch every area of our company. Their insights drive our decisions and their innovations fuel projects. When you join our team of data experts, you're helping DataSync's customers make better, smarter and faster decisions every day. See how you can help us solve some our customer's most challenging data problems while you grow your skills and build your own future.
Job Description
DataSync Technologies is seeking Data Engineers to support a mission critical program within the Intelligence Community.
Requirement:
ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED. MUST BE A U.S. CITIZEN.
Responsibilities will vary by specific data engineer role – Data Architect, Data Scientist, Database Engineer, Data Governance to include the following:
Design and develop methods, processes, and systems to consolidate and analyze structured and unstructured data from diverse sources including ""big data"" sources.
Develop and use advanced software programs, algorithms, query techniques, model complex business problems, and automated processes to cleanse, integrate, and evaluate datasets.
Analyze the requirements and evaluate technologies for data science capabilities including one or more of the following: Natural Language Processing, Machine Learning, predictive modeling, statistical analysis and hypothesis testing.
Develop information tools, algorithms, dashboards, and queries to monitor and improve business performance. Maintain awareness of emerging analytics and big-data technologies.
Designs, implement, and maintain standard data interfaces for data ingest including Extract/Transform/Load (ETL) methodology and implementation, APIs, RESTful Web Services, data quality, and data cleansing.
Provide data services, data administration, data management, and ""Big Data"" support in client/server, virtual machine, Hadoop, and cloud infrastructure environment and/or migrations between these environments.
Database installation, configuration, and the upgrading of database server software and related products, backup and recovery policies and procedures, database implementation, security, optimization, multi-domain operation, and performance management.
Hadoop, cloud, and other technologies associated with data storage, processing, management, and use.
The migration/transition of database capability into cloud based technologies and/or creation of interfaces between classic relational databases and key indexes to cloud based columnar databases and map reduce index capabilities.
Preferred Qualifications (All not required):
Databases/Data Stores: Oracle, MySQL, HIVE, HBASE, and HDFS
Frameworks: Hadoop, Rails, JavaScript Frameworks, SOA/WebServices, JSP
Indexing: SOLR and Lucine
Development/Scripting Languages: JAVA (J2EE), Python, Ruby, JavaScript, MapReduce, Pig, XML, SQL, JAQL, HTML, CSS, XML, BASH, ANT, and Perl
________________________
What makes DataSync Technologies different?
Leadership Training: We provide employees with a variety of learning opportunities, including access to exclusive classes, professional growth training and more.
Feedback & Mentoring: We believe in talking—often. So we have one-on-one feedback sessions for every employee.
Community Service: We believe in helping the community where we work. DataSync and its employees donate time and services on a regular basis to local military charities. We believe in helping, both inside and outside of the office.
Social Events: We plan social events on a regular basis to help our employees relax and socialize so we get to know one another outside of our job titles.
DataSync is an EEO and Affirmative Action Employer of Female/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law
Information about Equal Employment Opportunity (EEO) and Employee Polygraph Act (EPPA) provisions in addition to other Federal labor laws can be found at http://webapps.dol.gov/dolfaq/go-dol-faq.asp?faqid=537
DataSync is committed to providing veteran employment opportunities to our service men and women.
www.datasynctech.com
www.facebook.com/DatasyncTechnologies
www.twitter.com/Jobs at DataSync (@DatasyncJobs)
www.twitter.com/datasynctech
#datasynctech on Instagram
Interested in Joining Our Team? - Check out this YouTube video!

#CJ
mCScFXDsYI"
71,Data Engineer,"Gaithersburg, MD",Gaithersburg,MD,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Those who join Emergent BioSolutions feel a sense of ownership about their future. You will excel in an environment characterized by respect, innovation and growth opportunities. Here, you will join passionate professionals who advance their scientific, technical and professional skills to develop products designed-to protect life.
The Data Engineer will be working on a variety of data related projects and will particpate in

designing, developing, installing, managing and/or maintaining real-time and historical data related systems. The data engineer will contribute to initiatives directed at systems that are used to control and monitor laboratory, automation and manufacturing systems, as well as, systems to manage, analyze and visualize data.
Responsibilities
Develop and maintain data models, reporting systems, data management systems, and dashboards
Participate in visualization design, development, ongoing support, and delivery activities related to data services
Coordinates project and maintenance activities with DBAs
Solves a range of data related problems, analyzes possible solutions using knowledge and skills, and through acquiring new knowledge
Contributes to new computer platform design and data driven solutions
Supports activities related to global LIMS project
Maintains, queries and creates solutions around MS SQL based databases and LIMS (Laboratory information management systems) with the ability to create reports as needed
Provides Windows OS environment support, maintenance and patching in a controlled environment
Supports project technical lead on highly complex projects and may be responsible for some project management activities
Troubleshoots Windows log files and performance
Maintains up-to-data knowledge on modern data technologies and data management
platforms
Collaborates and helps coordinate data project efforts with various internal and external groups including interaction with vendors
Assists with support and planning of data related products portion of CAPEX projects
Researches and gathers requirements for data services, applications and products
Prepares data related procedures and documentation
Adheres to project budgets and schedules, performing work in accordance with internal SOPs and good Engineering Practices, as well as, regulations compliance
Supports testing of deliverables and provides data analysis as needed
Adheres to good programming practices and strict testing standards
Manages and optimizes processes for data intake, transfer, validation, and visualization
May determine methods and procedures on new moderately complex assignments
The above statements are intended to describe the general nature of work performed by those in this job. It is not an exhaustive list of all duties, and other duties may be assigned.
Education, Experience & Skills
Minimum Requirements:
Bachelor’s degree in Computer Science, IT, Data Science, Engineering or related field from an accredited university
3+ years’ experience and/or relevant project/coursework
Knowledge of file structures, data structures, batch files
Knowledge of real-time data acquistion and/or automation/controls environment
Working knowledge and administration of SQL and Relational Databases
Up-to-date specialized knowledge of data wrangling, manipulation and management of data technologies
Experience with software in the fields of: SCADA/control, LIMS, data management
Proficient with computer applications such as Microsoft Suite and Visual Basic
Knowledge of PC environment, able to modify network settings, understanding of remote connections
Exposure to working with and data transfer to IoT platforms
Problem solving, quantitative and analytical abilities
Demonstrated success in technical proficiency, creativity, and collaboration with others and independent thought
Works well within teams and interactions with groups of various disciplines
Exercises judgment within generally defined practices and policies in selecting methods and techniques for obtaining data solutions
Possess analytical and logical troubleshooting skills for computer and basic network issues
Must lead by example through strong work ethics and high standards
Must be able to adapt to change while viewing obstacles as opportunities
Desire to continue to promote personal development
Interest in cutting edge data technologies
Preferred Requirements
Knowledge and desire to work with IoT platforms
Experience/exposure to Machine Learning and Artificial Intelligence
Ability to manipulate various types of data with different degree of structuring across disparate sources
Experience with data transfer interfaces and protocols, such as OPC
Experience with SAS, JMP, and or other commercial statistical software
Experience with OSIsoft PI real-time data and events management system
Experience working with data acquisition platforms in a controlled environment
IT related or specialized certifications
Experience in a pharmaceutical, biopharmaceutical, and consumer products process company or food industry
Familiarity with data integrity/assurance and data security guidelines
Physical/Mental Requirements

Type/keyboard, visual acuity, good eye/hand coordination, stand, walk, sit, twist/turn, climb, reach outward, reach above shoulder, lift/carry 50 lbs, and pull 50 lbs. Use phone, fax, copier, and computer. Work outdoors, around loud noises, in hot/cold temperatures, and in high humidity. Organize/coordinate, analyze/interpret, problem solve, make decisions, supervise, plan, communicate, prepare written communications, and complete written work orders."
72,Power BI Developer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,None Found,"Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.",None Found,"The Senior Data Engineer responsibilities include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of our client's Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatment, etc .

Required Education/Experience:
Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.


Preferred:
Experience in Healthcare or related industryExperience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plusExperience productizing/automating predictive models that use R, SAS, Python, SPSS, etc.Continuous delivery and deployment automation for analytic solutions using tools like BambooFamiliarity with test driven development methodology for analytic solutionsAGILEAPI developmentData visualization and/or dashboard developmentDemonstrated ability to achieve stretch goals in a highly innovative and fast-paced environment"
73,Principal Data Engineer,"Arlington, VA 22202",Arlington,VA,22202,None Found,"Bachelor degree in computer science, information systems or related field or equivalent experience
5 years of experience with developing, installing, configuring, tuning, and supporting large dataset environments
5 years of experience with various database platforms, including Oracle, Oracle RAC
5 years of experience with database development languages, including SQL, PL/SQL and Linux scripting
5 years of experience with database design applications, including SQL Developer, DB Visualizer, and Embarcadero ER Studio
2+ years of experience in installing and upgrading Tableau server and server performance tuning
Experience with the design, development and delivery of Tableau visualization solutions
Experience with creation of users, groups, projects, workbooks and the appropriate permission sets for Tableau server logons and security checks
5 years of experience in handling large transaction datasets
",None Found,"Assist in implementing long-term strategic goals for BI database development in conjunction with end users, managers, clients, and other stakeholders
Analyze user requirements and, based on findings, collaborate with DS and other functions to design functional specifications for databases and database applications following database standards
Assist in planning and implementing capacity and resource expansion to ensure scalability of BI databases in close collaboration with DS function
Assist with the design of redundant systems, policies, and procedures for disaster recovery to ensure effective availability, protection, and integrity of data assets
Conduct research and make recommendations on database products, services, protocols, and standards in support of procurement and development efforts
Develop automated database applications, where necessary, using applicable database techniques
Work with agile teams and product owners to ensure database design and performance meet business requirements
Identify inefficiencies in current databases and implement improved solutions
Assist in planning and performing database upgrades and migrations
Assist in evaluating and selecting database components, including hardware, relational database management systems, ETL software, metadata management tools, and database design solutions.
As a subject matter expert, the Data Architect will define feasibility and address all data related questions and challenges to support dashboards and tracking tools from all functions across the organization including Product Innovation, Compliance, Finance and CPI.
",None Found,"Bachelor degree in computer science, information systems or related field or equivalent experience
5 years of experience with developing, installing, configuring, tuning, and supporting large dataset environments
5 years of experience with various database platforms, including Oracle, Oracle RAC
5 years of experience with database development languages, including SQL, PL/SQL and Linux scripting
5 years of experience with database design applications, including SQL Developer, DB Visualizer, and Embarcadero ER Studio
2+ years of experience in installing and upgrading Tableau server and server performance tuning
Experience with the design, development and delivery of Tableau visualization solutions
Experience with creation of users, groups, projects, workbooks and the appropriate permission sets for Tableau server logons and security checks
5 years of experience in handling large transaction datasets
","Our purpose is to serve the nation with the single most trusted and capable health information network, built to increase patient safety, lower costs and ensure quality care.
Job Summary
Surescripts is seeking a Principal Data Engineer to join our Business Intelligence team. The Principal Data Engineer's role is to plan, design and develop data systems and technology architecture that enhance and optimize our capabilities and products. He/She will directly engage with data warehouse engineers, product manager/owners, sales and activation teams to design, transform and develop data architectures that meet the evolving needs of the organization. The Principal Data Engineer will also lead efforts in evaluating and selecting technology components, such as software, hardware, and networking capabilities for business intelligence systems and applications. He/She will be owner of access control and support activities across a multi-environment with a focus on our Tableau infrastructure, including security, administration, release management, troubleshooting, system optimization and maintenance.
Responsibilities
Assist in implementing long-term strategic goals for BI database development in conjunction with end users, managers, clients, and other stakeholders
Analyze user requirements and, based on findings, collaborate with DS and other functions to design functional specifications for databases and database applications following database standards
Assist in planning and implementing capacity and resource expansion to ensure scalability of BI databases in close collaboration with DS function
Assist with the design of redundant systems, policies, and procedures for disaster recovery to ensure effective availability, protection, and integrity of data assets
Conduct research and make recommendations on database products, services, protocols, and standards in support of procurement and development efforts
Develop automated database applications, where necessary, using applicable database techniques
Work with agile teams and product owners to ensure database design and performance meet business requirements
Identify inefficiencies in current databases and implement improved solutions
Assist in planning and performing database upgrades and migrations
Assist in evaluating and selecting database components, including hardware, relational database management systems, ETL software, metadata management tools, and database design solutions.
As a subject matter expert, the Data Architect will define feasibility and address all data related questions and challenges to support dashboards and tracking tools from all functions across the organization including Product Innovation, Compliance, Finance and CPI.
Qualifications
Basic Requirements:
Bachelor degree in computer science, information systems or related field or equivalent experience
5 years of experience with developing, installing, configuring, tuning, and supporting large dataset environments
5 years of experience with various database platforms, including Oracle, Oracle RAC
5 years of experience with database development languages, including SQL, PL/SQL and Linux scripting
5 years of experience with database design applications, including SQL Developer, DB Visualizer, and Embarcadero ER Studio
2+ years of experience in installing and upgrading Tableau server and server performance tuning
Experience with the design, development and delivery of Tableau visualization solutions
Experience with creation of users, groups, projects, workbooks and the appropriate permission sets for Tableau server logons and security checks
5 years of experience in handling large transaction datasets
Preferred Qualifications:
Familiar with database performance tuning and replication
Strong understanding of relational database structures, theories, principles, and practices
Hands-on experience with large healthcare transactional datasets, in particular e-prescribing
Experience with NoSQL databases such as Hadoop, and Cassandra
Experience with database replication, preferably Oracle Golden Gate
Excellent knowledge of applicable data privacy practices and laws in healthcare
Working experience with developing, installing, configuring and supporting database environments
Surescripts is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate on the basis of race, color, religion, age, national origin, ancestry, disability, medical condition, marital status, pregnancy, genetic information, gender, sexual orientation, parental status, gender identity, gender expression, veteran status, or any other status protected under federal, state, or local law."
74,Data Engineer,"Chevy Chase, MD",Chevy Chase,MD,None Found,None Found,"Bachelor's degree in Computer Science or related degree.
At least a 3.0 overall undergraduate GPA
Strong knowledge of at least one programming language
SQL and C# experience highly desired
Experience working in a cloud environment
Experience with the Hadoop ecosystem highly preferred
A thorough understanding of the Systems Development Life Cycle (SDLC)
Excellent interpersonal and teamwork skills
Solid verbal and written communication skills
Excellent analytical and problem solving skills
Willingness to learn new technologies and programming languages
",None Found,"Bachelor's degree in Computer Science or related degree.
At least a 3.0 overall undergraduate GPA
Strong knowledge of at least one programming language
SQL and C# experience highly desired
Experience working in a cloud environment
Experience with the Hadoop ecosystem highly preferred
A thorough understanding of the Systems Development Life Cycle (SDLC)
Excellent interpersonal and teamwork skills
Solid verbal and written communication skills
Excellent analytical and problem solving skills
Willingness to learn new technologies and programming languages
",None Found,None Found,"Ready to make an impact? If so, read on!

Job Duties & Responsibilities

The Decision Support Systems (DSS) Team is seeking a highly motivated Data Engineer to support GEICO’s business analytics through Quality Data and Application Development.

DSS is a small technology team residing in one of the core research and predictive modeling departments at GEICO. This allows us to work side by side with our users along with having direct control over all stages of the development process, from design to implementation. As a Data Engineer on our team you will work with a variety of applications and database languages to support key business decisions. The technologies we routinely use include SQL, C#, Python, Hive, and Spark.
The main focus of this position is on the development of our new cloud based Big Data Platform. As part of this team you will interact with various IT, data science, and business partners to design and implement new data solutions that enable our business to move forward. You'll have the opportunity to develop new data sources, complex queries, and applications which are responsible for driving business decisions. This is a great opportunity to have an immediate, visible impact on the business, and to learn the skills required by the industry and technological world!

Please upload your resume, unofficial transcripts from all schools attended, and a cover letter to your GEICO profile when submitting your application.

Candidate Qualifications
Bachelor's degree in Computer Science or related degree.
At least a 3.0 overall undergraduate GPA
Strong knowledge of at least one programming language
SQL and C# experience highly desired
Experience working in a cloud environment
Experience with the Hadoop ecosystem highly preferred
A thorough understanding of the Systems Development Life Cycle (SDLC)
Excellent interpersonal and teamwork skills
Solid verbal and written communication skills
Excellent analytical and problem solving skills
Willingness to learn new technologies and programming languages



About GEICO
For more than 75 years, GEICO has stood out from the rest of the insurance industry! We are one of the nation's largest and fastest-growing auto insurers thanks to our low rates, outstanding service and clever marketing. We're an industry leader employing thousands of dedicated and hard-working associates. As a wholly owned subsidiary of Berkshire Hathaway, we offer associates training and career advancement in a financially stable and rewarding workplace.

Our associates' quality of life is important to us. Full-time GEICO associates are offered a comprehensive Total Rewards Program*, including:

401(k) and profit-sharing plans
Medical, dental, vision and life insurance
Paid vacation, holidays and leave programs
Tuition reimbursement
Associate assistance program
Flexible spending accounts
Business casual dress
Fitness and dining facilities (at most locations)
Associate clubs and sports teams
Volunteer opportunities
GEICO Federal Credit Union
Benefit offerings for positions other than full-time may vary.

GEICO is an equal opportunity employer. GEICO conducts drug screens and background checks on applicants who accept employment offers."
75,Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,"
Maintain and enhance existing custom solutions built in SharePoint, including but not limited to farm solutions deployed on the SharePoint platform.
Plan, lead, and execute SharePoint 2010/2013/2016 tasks; custom master pages, layouts, and templates; custom workflows; implementing permissions structures; working with timer jobs and event handlers; as well as implementation, integration, and maintenance of existing solutions.
Gather user requirements, analyze business processes, and work with functional areas to define and scope projects, document requirements, and application functionality.
Provide day-to-day operations support and configuration management, serve as the POC for differentiating functional issues from technical issues, and resolve technical issues.
Operate both independently and as part of a Scrum team across multiple projects.
Provide guidance in pursuing innovative solutions to achieve client goals and objectives.
",None Found,"
 Bachelor's Degree and 4+ years of SharePoint application development, using Visual Studio and related code management practices.
2+ years of SharePoint application development using client-side code (including CSOM) and server-side code on multiple SharePoint platforms (2010 to 2016).
2+ years of experience operating as part of an Agile development team
Experience with SharePoint Designer, InfoPath, Web Parts, and workflow creation.
Experience with jQuery, CSOM, and front-end UI design a plus.
Experience supporting migration between SharePoint versions (i.e., 2010-2016 is preferred.
Experience with Business Intelligence dashboards a plus.
Experience developing new software and web applications, as well experience operating and maintaining existing applications.
Experience developing cloud-based solutions, cloud-based training and certifications a plus.
Familiarity with DevOps and Site Reliability Engineering principles.
Proficiency in time management, attention to detail, and adaptability depending on circumstances.
Proven ability to work with remote teams;
Capable of critical thinking for problem resolution. US Citizen, ability to obtain Secret Clearance (Candidates with existing Secret or Top Secret, have recently worked with the intelligence community, or have recently held a DHS HQ EOD is a plus)","Washington, D.C.
Full-Time
Job Description:
JPI is hiring a mid-level SharePoint Developer with at least 4 years of experience in application development and proficiency in SharePoint custom solutions. The ideal candidate will be well versed in the analysis, design, hands-on development and delivery of web-based applications, including SharePoint 2010, 2013, and 2016. Experience with languages including .NET and/or C# is preferred. As a key part of the development team, the SharePoint Developer will also be responsible for staying up-to-date on technological advances and new industry trends. MUST HAVE OR BE ABLE TO OBTAIN SECRET CLEARANCE (REQUIRES U.S. CITIZENSHIP).
Responsibilities:
Maintain and enhance existing custom solutions built in SharePoint, including but not limited to farm solutions deployed on the SharePoint platform.
Plan, lead, and execute SharePoint 2010/2013/2016 tasks; custom master pages, layouts, and templates; custom workflows; implementing permissions structures; working with timer jobs and event handlers; as well as implementation, integration, and maintenance of existing solutions.
Gather user requirements, analyze business processes, and work with functional areas to define and scope projects, document requirements, and application functionality.
Provide day-to-day operations support and configuration management, serve as the POC for differentiating functional issues from technical issues, and resolve technical issues.
Operate both independently and as part of a Scrum team across multiple projects.
Provide guidance in pursuing innovative solutions to achieve client goals and objectives.
Requirements:
 Bachelor's Degree and 4+ years of SharePoint application development, using Visual Studio and related code management practices.
2+ years of SharePoint application development using client-side code (including CSOM) and server-side code on multiple SharePoint platforms (2010 to 2016).
2+ years of experience operating as part of an Agile development team
Experience with SharePoint Designer, InfoPath, Web Parts, and workflow creation.
Experience with jQuery, CSOM, and front-end UI design a plus.
Experience supporting migration between SharePoint versions (i.e., 2010-2016 is preferred.
Experience with Business Intelligence dashboards a plus.
Experience developing new software and web applications, as well experience operating and maintaining existing applications.
Experience developing cloud-based solutions, cloud-based training and certifications a plus.
Familiarity with DevOps and Site Reliability Engineering principles.
Proficiency in time management, attention to detail, and adaptability depending on circumstances.
Proven ability to work with remote teams;
Capable of critical thinking for problem resolution. US Citizen, ability to obtain Secret Clearance (Candidates with existing Secret or Top Secret, have recently worked with the intelligence community, or have recently held a DHS HQ EOD is a plus)
JPI is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status."
76,Data Engineer,"Reston, VA 20191",Reston,VA,20191,None Found,None Found,None Found,None Found,None Found,None Found,"Octo Consulting is seeking an experienced Data Engineer to join our growing team in support of a Federal customer. The ideal candidate will have a solid understanding of data science, advanced statistics, machine learning, data mining and visualization techniques.
US Citizen with the ability to obtain an EOD clearance
5+ years of experience in modern data development, upgrading, support and design.
Experience requirements may be substituted with a Bachelor’s degree in Computer Science plus 3 years of experience in modern data development, upgrading, support, and design.
Experience in establishing performance and statistical monitoring of enterprise databases to include, but not limited to; wellness checks, data integrity, privacy and security scans.
Experience in supporting cloud database environments, specifically AWS (i.e., EC2, S3, Neptune or Redshift) to include backup and archiving of data."
77,Big Data Cloud Engineer,"Herndon, VA",Herndon,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Company

Hitachi Vantara, a wholly owned subsidiary of Hitachi, Ltd., helps data-driven leaders use the value in their data to innovate intelligently and reach outcomes that matter for business and society – what we call a double bottom line. Only Hitachi Vantara combines 100+ years of experience in operational technology (OT) and 60+ years in IT to unlock the power of data from your business, your people and your machines. We help enterprises store, enrich, activate and monetize data for better customer experiences, new revenue streams and lower business costs. Hitachi Vantara elevates your innovation advantage by combining IT, operational technology (OT) and domain expertise. Come join our team and our employee‐focused culture, and help drive our customers' data to meaningful customer outcomes.

As part of the Hitachi Vantara family, REAN Cloud, a major player in the AWS Partner Network as a Premier Consulting Partner is growing substantially. REAN Cloud works with enterprise level organizations on some of the world's largest automation and cloud deployment projects, and get to work with the best technologies throughout the process! At REAN, we believe in building high performance teams who are passionate about what they do. We enjoy learning from each other and our clients, and our spirit of collaboration permeates everything we do.

The Role

We are seeking a highly motivated, hands-on and talented Data Engineer to join our Big Data and Application Modernization team. The individual will be responsible for working closely with our team of architects and engineers for engineering efficient and cost effective Data Solutions, primarily in multi cloud environments but also in hybrid cloud environments. As a Data Engineer you will be responsible for building repeatable Data Lake solutions and services for different industries, analyzing and processing large amounts of data structured, unstructured and semi-structured. As a Data Engineer, you will be mostly working on agile projects in small to large scrum teams on different projects for various clients.

Responsibilities

Develop and deploy fully automated and scalable Big Data Solutions including ingesting, transforming and persisting the data from various data sources including but not limited to SQL, NoSQL, NAS etc
Develop ETL and ELT scripts using standard SQL or tools such as Pentaho, Informatica, Talend etc.
Build large scale data architectures using Kafka, Kinesis, Spark, Flink and Cassandra in hybrid environments
Develop Hadoop, MapReduce and/or Amazon EMR workloads for processing large volumes of data
Develop data pipelines for orchestrating data movements between different stages of data lifecyle using Airflow, Luigi and AWS Data Pipeline
Develop scripts for processing and persisting data to Graph Databases such as Neo4J, Janus Graph, Amazon Neptune etc
Implement data cataloging, metadata, data quality checks, Master Data Management (MDM) in data lake architectures
Build data discovery features by leveraging search technologies such as ELK, SOLR or Splunk
Develop Business Intelligence and visualization dashboards using Tableau, YellowFin, Birst, Looker or QuickSight
Develop unit tests wherever applicable
Participate and present design and solutions in Architecture Review Board Meetings
Participate in code review and peer-review meetings

Qualifications

Over 5+ years of experience with at least 2 years in developing and solving Big Data solutions/problems
BS/MS in Computer Science or equivalent field of study
Hands on experience in writing SQL queries, stored procedures, functions etc
Hands on experience in MapReduce, Spark, ETL
Hands on experience in Python, Java or .NET programming languages
Experience with as many tools as possible in Hadoop eco system and platforms (Apache, Hadoop, Falcon, Atlas, Tez, Sqoop, Flume, Kafka, Pig, Hive, HBase, Accumulo, Storm, Solr, Spark, Ranger, Knox, Ambari, ZooKeeper, Oozie, Phoenix, NiFi, Nifi Registry, HAWQ, Zeppelin, Slider, Mahout, MapReduce, HDFS, YARN
AWS experience and certifications preferred
Knowledge on working with Graph DB tools (Gremlin, SparkQL, Tinkerpop)
Hands on experience working with Relational Databases (MSSQL, MySQL, PostgreSQL)
Knowledge on working with data warehousing solutions (Oracle, SQL Server, Redshift etc)
Have good analytical, communication and interpersonal skills
Experience working in Agile environments and projects

#LI-JM1

We are an equal opportunity employer. All applicants will be considered for employment without attention to age, race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status."
78,PHP Data Engineer,"Washington, DC 20001",Washington,DC,20001,None Found,None Found,"
5+ years’ experience developing in PHP
2+ years’ experience working with the Drupal Content Management system, version 8 preferred
Solid understanding of object-oriented programming
Familiar with various design and architectural patterns
Experience working with ElasticSearch and MySQL database management systems
Demonstrated experience with REST API integrations
Experience working with NoSQL data architectures is a huge plus
Understanding fundamental design principles behind a scalable application
Creating database schemas that represent and support business processes
Ability to work independently, prioritize tasks and hit deadlines in a fast-paced work environment.
Demonstrates good judgment, excellent planning, problem-solving, troubleshooting, management, and communication (verbal and written) skills with the ability to think strategically, act quickly, multi-task, and work collaboratively in an environment that values creativity and flexibility to make things happen.","
Perform ETL processing. Deal with raw data that contains human, machine or instrument errors, may be un-validated, unformatted or contain suspect records or system-specific codes.
Recommend and sometimes implement ways to improve data reliability, efficiency and quality.
Design, build, and maintain efficient database structures
Develop functions and scripts to import data into the designed database structure, via RESTful APIs, manual uploads, and other methods.
Develop functionality to query data for use in web applications and visualizations.",None Found,None Found,"As a PHP Data Engineer you will use your exceptional database and development skills and experiences to architect and develop data models and structures for responsive Drupal websites and web applications. As a member of our world-class agency, you will work on innovative and inspired work across a variety of clients.
Responsibilities
Perform ETL processing. Deal with raw data that contains human, machine or instrument errors, may be un-validated, unformatted or contain suspect records or system-specific codes.
Recommend and sometimes implement ways to improve data reliability, efficiency and quality.
Design, build, and maintain efficient database structures
Develop functions and scripts to import data into the designed database structure, via RESTful APIs, manual uploads, and other methods.
Develop functionality to query data for use in web applications and visualizations.
Skills & Experience
5+ years’ experience developing in PHP
2+ years’ experience working with the Drupal Content Management system, version 8 preferred
Solid understanding of object-oriented programming
Familiar with various design and architectural patterns
Experience working with ElasticSearch and MySQL database management systems
Demonstrated experience with REST API integrations
Experience working with NoSQL data architectures is a huge plus
Understanding fundamental design principles behind a scalable application
Creating database schemas that represent and support business processes
Ability to work independently, prioritize tasks and hit deadlines in a fast-paced work environment.
Demonstrates good judgment, excellent planning, problem-solving, troubleshooting, management, and communication (verbal and written) skills with the ability to think strategically, act quickly, multi-task, and work collaboratively in an environment that values creativity and flexibility to make things happen.
Applicant Eligibility: Please note, only candidates who are US citizens or able to work on a permanent basis without visa sponsorship are eligible to apply. No recruiters or staffing agencies please."
79,Data Engineer - API Management,"Chantilly, VA",Chantilly,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Title: Data Engineer - API Management
Clearance: TS/SCI
Location: Chantilly, VA
Compensation: Excellent Benefits and Salary

Job Description:
As part of a technology architecture team, support a DIA sponsored project in support of functional correlation to each of the main data collection and delivery functions. As the project grows and becomes integrated across operational and tactical elements of the DoD, this team will identify established capabilities that can meet standardized use cases as well as missing capabilities that need to be added in order to provide common data usage models. Successful candidate will work with deployment site and identified Program of Records (PoRs) to catalog APl's in use and APl's made available. Candidate will define a deployment strategy to rehost API endpoints into an enterprise management layer and to outline a needs statement that shows how existing point-to-point API interfaces can be removed.

Required:
3 years’ experience managing broad array of API management software
Understanding of commercial ATI management tools
Hands-on experience on installing and operating ATI management gateways on-premise and in the cloud
Understanding of how to secure API’s and familiarity with multiple security protocols to enforce policy on API access
Knowledge of Hadoop/MapReduce or similar technologies
BSc/BA in computer science or relevant field
Active TS/SCI Security clearance
Must be a US Citizen
Also, candidate should have a working knowledge of the following:

Data
Identifies unique datasets within applications
Captures formatting
Identifies expected volumetrics
Identifies all current sharing mechanisms
Makes initial recommendations on polyglot storage for each dataset
Identifies expected resource needs for each dataset
Application
Identification of datasets
Identify transfer mechanisms in existence today
Characterize API's in order to verify CAN (controller area network) support
Identifies changes that need to be made in order to remove user identity requirements for access by other systems
Provides guidance on integration tasks regarding API's and ingest
Mission
Network capacity between sites and mission partners
Bandwidth utilization
Cross domain
Evaluates information to determine nearest node usage and failover / COOP
Identifies changes that need to occur in the data architecture to ensure no operational impact to warfighters during integration
Desired:
Possess a high degree of ingenuity, creativity, and resourcefulness
Prior DoD, military or intelligence community experience
A VOLANT Associate proudly serves the needs of the Nation's Intelligence Community and matches a very specific and rigorous profile. They are the leaders and the difference makers on a team. A VOLANT Associate is rewarded for being the very best the country has to offer. The financial rewards are commensurate with the level of expertise and experience they have achieved and are virtually unmatched in the industry.

Benefit Summary
Volant Associates provides an industry-leading benefits package to attract and retain the very best talent within a very competitive recruiting environment and support its employees and their dependents.

100% Volant Paid Standard Benefits:
200% Matching on employee 401k contributions (on up to 5% of employee salary deferral)
20.5 days (164 hours) of Paid Time Off
7 paid holidays per year
Health care insurance for employee and dependents thru UHC
Dental care insurance for employee and dependents thru UHC
Vision care insurance for employee and dependents thru VSP
Life and Personal Accident Insurance ($50k coverage) thru CIGNA
Additional Life and Personal Accident Insurance (1 x salary up to $170k) thru MOO
Short term disability Insurance (60% of earnings up to $2,308 per week) thru CIGNA
Long term disability insurance (60% of earnings up to $10k per month) thru CIGNA
Educational assistance (up to $3,500 per calendar year)
Adoption assistance
Commuter benefits
Training and development opportunities
Cell phone stipend (up to $50 per month)
Corporate laptop computer
Gym access (for Chantilly-based employees)
Additional programs available to all employees:
Heath Savings Account (HSA)
Health care Flexible Spending Account (FSA)
Dependent care Flexible Spending Account (FSA)
Voluntary additional levels of Life and Personal Accident Insurance"
80,Data Engineer,"McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Engineer will manipulate data and data flows for both existing and new systems. Additionally they will provide support in the areas of data extraction, transformation and load (ETL), data mapping, data extraction, analytical support, operational support, database support, and maintenance support of data and associated systems. As a member of the team, candidates will have the ability to work within a super-computing environment, exploit new and expanding datasheets, develop custom algorithms, and work in a multi-tasking, non-traditional analytic environment with rapidly changing priorities. The successful candidate must have demonstrated experience applying and tailoring complex extraction rules on massive amounts of unstructured data using tools such as Netowl. He or she much also have experience with the principles of large-scale (terabytes) database development, large-scale file manipulation, data modeling, data mapping, data testing, data quality, and documentation preparation.

REQUIRED KNOWLEDGE/SKILLS:

Bachelor’s Degree in Computer Science, Electrical or Computer Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience
8-10 years of related software engineering and ETL experience
10 years of technical management experience of Software Developers, Systems Administrators and Systems Architects
Experience building and maintaining data flows in NiFi or Pentaho
Excellent organizational, coordination, interpersonal and team building skills.


DESIRED KNOWLEDGE/SKILLS:

Experience with the following languages: Java/J2EE, C, C++, SQL, XML, XQuery, XPath, Ruby on Rails, HTML/XHTML, CSS, Python, Shell Scripting
Knowledge of servers operating systems; Windows, Linux, Distributed Computing, Blade Centers, and cloud infrastructure
Strong problem solving skills
Ability to comprehend database methodologies
Focus on continual process improvement with a proactive approach to problem solving


KEY RESPONSIBILITIES:

Research, design, develop and/or modifies enterprise-wide systems and/or application software.
Develop complex data flows, or makes significant enhancements to existing pipelines.
Resolves complex hardware/software compatibility and interface design considerations.
Conducts investigations and tests of considerable complexity.
Researches emerging technologies to determine impact on application execution.
Provides input to staff involved in writing and updating technical documentation
Troubleshoots complex problems and provides customer support for the ETL process
Advises hardware engineers on machine characteristics that affect software systems, such as storage capacity, processing speed, and input/output requirements.
Prepares reports on analyses, findings, and project progress.
Provides guidance and work leadership to less-experienced software engineers.
May serve as a technical team or task leader.
Experience with architecture, software programming and engineering, Complex Information Modeling and Analysis, Business Intelligence and/or Data warehousing.
Experience using distributed technologies to perform data analysis, regression analysis, correlation, data enrichment and data exploitation activities.
Experience utilizing machine-learning techniques to detect anomalous information in a production environment.
Understands systems latency and capacity planning and efficient systems resource utilization.
Ability to apply visualization technologies to various large data sets.


Clearance:


TS/SCI w/ Poly"
81,Big Data Engineer,"Reston, VA",Reston,VA,None Found,None Found,None Found,"
Bachelor's or Master's degree in computer science or software engineering preferred
Experience with object-oriented design, coding and testing patterns as well as experience in engineering (commercial or open source) software platforms and large-scale data infrastructures.
Ability to architect highly scalable distributed systems, using different open source tools.
Experience building high-performance algorithms.
Extensive knowledge of different programming or scripting languages such as Java, Linux, C++, PHP, Ruby, Phyton and/or R.
Experience with different (NoSQL or RDBMS) databases such as MongoDB needed.
Experience building data processing systems with Hadoop and Hive using Java or Python",None Found,None Found,None Found,"DataSync Technologies is looking for several Big Data Engineers to help support our Customer. Be a part of an award-winning, employee friendly company in Northern VA and have the satisfaction of helping keep America safe. DataSync Technologies, Inc is a veteran owned small business providing consulting excellence and real time solutions for customers with complex information technology needs within Intelligence Community. Our cleared consultants bring real world experience with a common sense approach to their jobs whether they are creating complex analytic dashboards, architecting new cloud technology infrastructures, securing sensitive data or streamlining business processes for efficiency.

Qualified candidates must be able to develop, maintain, test and evaluate big data solutions within organizations. Candidates must be able to build large-scale data processing systems, be an expert in data warehousing solutions and should be able to work with the latest (NoSQL) database technologies.
ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED. MUST BE A US CITIZEN.

Skills
Bachelor's or Master's degree in computer science or software engineering preferred
Experience with object-oriented design, coding and testing patterns as well as experience in engineering (commercial or open source) software platforms and large-scale data infrastructures.
Ability to architect highly scalable distributed systems, using different open source tools.
Experience building high-performance algorithms.
Extensive knowledge of different programming or scripting languages such as Java, Linux, C++, PHP, Ruby, Phyton and/or R.
Experience with different (NoSQL or RDBMS) databases such as MongoDB needed.
Experience building data processing systems with Hadoop and Hive using Java or Python
Desired Experience
Excellent oral and written communication skills;
Experience in designing efficient and robust ETL workflows;
AWS experience
_________________
What makes DataSync Technologies different?
Leadership Training: We provide employees with a variety of learning opportunities, including access to exclusive classes, professional growth training and more.
Feedback & Mentoring: We believe in talking—often. So we have one-on-one feedback sessions for every employee.
Community Service: We believe in helping the community where we work. DataSync and its employees donate time and services on a regular basis to local military charities. We believe in helping, both inside and outside of the office.
Social Events: We plan social events on a regular basis to help our employees relax and socialize so we get to know one another outside of our job titles.
DataSync is an EEO and Affirmative Action Employer of Female/Minorities/Veterans/Individuals with Disabilities.
Equal Employment Opportunity
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law
Information about Equal Employment Opportunity (EEO) and Employee Polygraph Act (EPPA) provisions in addition to other Federal labor laws can be found at http://webapps.dol.gov/dolfaq/go-dol-faq.asp?faqid=537
DataSync is committed to providing veteran employment opportunities to our service men and women.
www.datasynctech.com
www.facebook.com/DatasyncTechnologies
www.twitter.com/Jobs at DataSync (@DatasyncJobs)
www.twitter.com/datasynctech
#datasynctech on Instagram
Interested in Joining Our Team? - Check out this YouTube video!
oWudwVf7CH"
82,"Sr. Manager, Data Solutions Architect","McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"11 West 19th Street (22008), United States of America, New York, New York

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Sr. Manager, Data Solutions Architect

Do you have a passion for architecting solutions that leverage the latest technologies to deliver
secure, reliable, resilient and scalable data management solutions? Capital One is seeking a
Senior Manager, Data Solutions Architect to transform our data ecosystem.

Acting as an internal consultant to business and information technology delivery teams, the
ideal candidate will architect and design solutions that are strategic for the business and built
using latest technologies and techniques. Utilizing in-depth knowledge of infrastructure, open
source technologies and data engineering, you will develop the strategy, achieve business and
engineering buy-in, and monitor execution toward the target architecture across the enterprise.
As Sr. Manager, Data Solutions Architect you will need to understand how to apply technologies

in categories such as:
Cloud Computing Services (AWS, Azure, etc.)
Streaming Technologies
Big Data Programming Frameworks (Hadoop, Spark, etc.)
Big Data Storage and Visualization Solutions
Data Management Solutions (Metadata, Lineage, Quality)
Performance and Scaling Techniques
Data Integration Patterns
Microservices
Open Source Software

Responsibilities include:
Defining technical software architectures to drive our business and technology strategies for best tools company wide
Possess extensive working knowledge with architectural tools in broader technology community
Providing business, application and technology consulting in feasibility discussions with technology team members and business partners
Defining, executing and continuously improving our internal software architecture processes
Being a technology thought leader and strategist

Basic Qualifications:
Bachelor’s Degree
At least 8 years of experience in a Technology Leadership position
At least 4 years of experience in Application Architecture
At least 3 years of experience in Data Architecture

Preferred Qualifications:
Master’s Degree in Computer Science
5+ years of experience as a Software or Data Engineer
Proven ability to champion an idea and bring a group to consensus
Proven ability to set technical strategy, applying an in-depth understanding of the business strategy governing that area
Excellent problem-solving skills involving complex and ambiguous issues

Capital One will consider sponsoring a new qualified applicant for employment authorization for this position."
83,Big Data Engineer,"Reston, VA",Reston,VA,None Found,None Found,None Found,None Found,None Found,None Found,"
Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark
Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting
Experience with data warehousing tools and technologies
Ability to work within UNIX/Linux operating systems
AWS experience a plus
","Data Works is looking for senior Big Data Engineers able to lead the way in tackling the most difficult engineering challenges in Big Data systems.

This position requires U.S. Citizenship and an active TS/SCI security clearance.

About Data Works

Data Works is an employee-focused small company that supports the Intelligence Community by providing Big Data and Cyber Security solutions. We favor a high quality workforce over aggressive growth and provide opportunities on programs that fill mission-critical needs. Our core competency is in the technical stack between data collection and analysis. Positions are available in various customer locations between McLean and Dulles in Virginia.

Position Description

Data Works is seeking a Big Data Engineer with demonstrated experience in leading large scale data warehousing projects. A successful candidate will be strong in Map Reduce, Java, and possess an understanding of data science concepts such as machine learning and trend analysis. Candidate should also be familiar with indexing products such as Lucene and Elasticsearch. Relevant certifications considered but not required.

Technical Requirements

Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch and Apache Spark
Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting
Experience with data warehousing tools and technologies
Ability to work within UNIX/Linux operating systems
AWS experience a plus
Company Benefits

6 weeks PTO
Paid Overtime
Annual Bonuses
10% Employer 401k Contribution
Health/Vision/Dental/Disability/Life Insurance
Annual Training and Tuition Budgets
Technology/Fitness/Communications Reimbursement
Charity Matching Program
EOE/M/F/Vet/Disabled"
84,Data Engineer,"Washington, DC 20032",Washington,DC,20032,None Found,None Found,None Found,None Found,None Found,None Found,"BDR Solutions, LLC, (BDR) supports the U.S. Federal Government in successfully achieving their mission and goals. Our service and solution delivery starts with understanding each client's end-state, and then seamlessly integrating within each Agency's organization to improve and enhance business and technical operations and deployments.

BDR is a Service-Disabled Veteran-Owned Small Business (SDVOSB), certified Historically Underutilized Business Zone (HUBZone), and Minority-Owned Small Disadvantaged Business (SDB).

BDR is seeking a Data Engineer with an active/current TS/SCI w/ CI Poly clearance to support a U.S. Government agency located in the Joint Base Anacostia-Bolling, Washington, DC area.

Data Engineer

Essential Job Functions & Responsibilities
Responsible for designing, implementing, and operating data management systems for intelligence needs.
Design how data will be stored, accessed, used, integrated, and managed by different data regimes and digital systems.
Assist data users to determine, create, and populate optimal data architectures, structures, and systems.
Plan, design, and optimize data throughput and query performance.
Participate in the selection of backend databases technologies (e.g. SQL, NoSQL, HPC, etc.), their configuration and utilization, and the optimization of the full data pipeline infrastructure to support the actual content, volume, ETL, and periodicity of data to support the intended kinds of queries and analysis to match expected responsiveness.
Establish and maintain continuity of requirements and feedback processes by data users; document historical requirement and track/evaluate reporting of consumer use.
Determine, create, and populate optimal data architectures, structures, and systems.
Required Minimum Qualifications
Requires Active TS/SCI w/ CI Poly.
Ability to safely and successfully perform the essential job functions consistent with the ADA, FMLA and other federal, state and local standards, including meeting qualitative and/or quantitative productivity standards.
Ability to maintain regular, punctual attendance consistent with the ADA, FMLA and other federal, state and local standards.
Must be able to talk, listen, and speak clearly on telephone.
Expert-level command of the English language.
Preferred Skills/Experience
In addition to the above, the ideal candidate will possess HUMINT experience.
In addition, U.S Citizenship is required. Applicants selected will be subject to a government security
investigation and must meet eligibility requirements for access to classified information and be able to
obtain a government-granted security clearance. Individuals may also be subject to a background investigation including, but not limited to criminal history, employment and education verification, drug testing, and creditworthiness.

BDR is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, marital status, disability, veteran status, sexual orientation, or genetic information.
Ytm2cJtSOg"
85,Data Engineer III,"Rockville, MD",Rockville,MD,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer III

The Data Engineer III will provide technical leadership and vision to an Agile development team to maintain and refine the strategic roadmap and to refine the product backlog and to create, enhance, and maintain data used in delivery of clinical research services. The Data Engineer III may also be matrixed to application development teams as a subject matter expert in data management, persistence and retrieval. The Data Engineer III should be able to successfully collaborate with all levels of business stakeholders as well as external vendor development and support personnel.

Responsibilities:
 Developing robust, fault tolerant, data persistence, management, analysis and delivery solutions while adhering to Quality System Procedures governing software version control, configuration management and the Systems Development Life cycle (SDLC), with a focus on ease of maintenance and extensibility
 Contributing to the development and implementation of data transfer specifications and validating edits checks
 Designing and developing databases and database objects (tables, stored procedures, functions, views, et. al.)
 Reviewing and updating queries and stored procedures for optimal performance. Identifying and correcting performance bottlenecks related to SQL code
 Following organizational practices in version control, software development and configuration management, including key performance indicators for the management of software development
 Developing and implementing data standards, documents specifications and procedures
 Performing statistical analysis
 Collaborating with study personnel to provide input to statistical analysis plans, selecting statistical methods and performing statistical analysis
 Partnering with Data Management and Software Delivery teams to understand and navigate database content, efficiently deliver clinical data to our customers and assure data quality and integrity
 Integrating distributed application data to provide operational metrics for the management of clinical trials and performing data migrations
 Ensuring compliance with client guidelines, and corporate SOPs and work instructions, system development life cycle, system validation and systems change control related to data management and analysis
 Partnering with Scrum Master to drive continuous improvement within the Agile team
 Fostering a collaborative team environment
 Investigating and resolving issues escalated by Tier 1 support in a timely manner to the satisfaction of the business stakeholders
 Authoring and reviewing technical documentation regarding the design, configuration, creation, installation and maintenance of clinical data components
 Ensuring
 Other duties may be assigned
 The physical demands are representative of those an employee encounters while performing the essential functions of this job

Qualifications

Education:
 Bachelor's Degree in Computer Science, a technical or science discipline preferred

Experience:
 Minimum five years of development and administration experience with SQL Server managing large, multi-instance, distributed databases
 Experience with NoSQL databases such as PostgreSQL a plus
 Minimum three years of experience with analytics and business intelligence
 Experience in managing data and analysis for clinical research a plus
 Experience with Infrastructure as a Service technologies such as AWS or OpenStack a plus

Skills:
 Deep understanding of the following:
o Both physical and logical aspects of Relational databases
o Data modeling principles
 Desire to identify and leverage appropriate new technologies
 Technically proficient with two or more the following technologies and languages:
o SAS
o Powershell
o Python
o R
o SQL or LINQ
o Cloud services for infrastructure such as Amazon Web Services or Open Stack

Technically proficient with the following tools and databases:
o MS Visual Studio
o Team Foundation Server or other centralized source code repository
o MS SQL Server 2012 or higher
o SQL Server Integration Services
o SQL Server Reporting Services

Strong communication and problem solving skills:
o Ability to think analytically and critically and effectively communicate with customers and other personnel in order to translate and synthesize business needs into solutions
o Ability to develop robust, fault tolerant systems
o Ability to be flexible in approach toward software design
o Ability to effectively investigate technical issues with vendors and non-technical customers in order to solve reported issues
o Ability to become a respected, trusted advocate for technology within the enterprise

Strong business acumen:
o Attention to detail
o Ability to meet demanding deadlines
o Ability to organize own work and prioritize multiple project commitments
o Ability to work independently
o Ability to work collaboratively with a team – including with team members who are geographically dispersed
o Familiarity with applicable regulatory guidance and rules, e.g., 21 CFR Part 11, FDA Guidance
o Commitment to quality, including thorough testing of own work products
o Ability to author and present system documentation deliverables such as design documentation for software applications in clear, straightforward language
o Applies technical best practices and relevant industry standards, as well as experience in developing or updating business procedures

BioTelemetry is an equal opportunity employer who strictly prohibits discrimination against any employee or applicant for employment because of the individual’s race, color, religion, gender, sexual orientation, gender identity or expression, national
origin, age, disability, veteran’s status, or any other characteristic protected by law. Affirmative action will be taken to ensure that all employment decisions, including but not limited to those involving recruitment, hiring, promotion, training, compensation, benefits, transfer, discipline,
and discharge, are free from unlawful discrimination.

CB
SG
#DCE"
86,Data Scientist,"Washington, DC",Washington,DC,None Found,None Found,None Found,"
Expertise in analytics tools including R, Python, SAS, SPSS, or similar.
Experience in the analysis and interpretation of data including the understanding of mathematical and statistical concepts.
Proficiency in exploratory data analysis and interpreting data.
Experience with statistical modeling or basic machine learning algorithms.
Knowledge of SQL.
Proven background working with Hadoop or Hive.
Experience working with cloud technologies such as AWS, Kafka, Spark, Storm, or Hadoop",None Found,"
Minimum of 3 years technical/data analytic experience
Bachelor’s degree in Engineering, Mathematics, Computer Science, Information Systems, Economics or Business, or equivalent
Must be clearable for a Secret clearance – Active U.S. Citizen",None Found,"Data Scientist/Data Engineer
Location: Washington DC
Job ID: 1083

We are looking for professionals with a wide range of skills, from Junior to Team Leads. If any of the skills below describe you please apply so we can have a deeper conversation.
When it comes to data do you not only enjoy driving the car but want to look under the hood and understand how it works? Then we would like to talk with you! Datastrong has multiple openings for Data Engineers and Data Scientists who want to join a growing practice and work with the latest technologies to help solve federally operated challenges.
Knowledge, Skills, and Abilities
Expertise in analytics tools including R, Python, SAS, SPSS, or similar.
Experience in the analysis and interpretation of data including the understanding of mathematical and statistical concepts.
Proficiency in exploratory data analysis and interpreting data.
Experience with statistical modeling or basic machine learning algorithms.
Knowledge of SQL.
Proven background working with Hadoop or Hive.
Experience working with cloud technologies such as AWS, Kafka, Spark, Storm, or Hadoop
Education, Experience, & Certifications
Minimum of 3 years technical/data analytic experience
Bachelor’s degree in Engineering, Mathematics, Computer Science, Information Systems, Economics or Business, or equivalent
Must be clearable for a Secret clearance – Active U.S. Citizen
Benefits
Great Culture focused on our customers and team members
Datastrong offers a compensation plan consisting of a competitive base salary and an uncapped bonus
100% Health coverage for employees with Vision and Dental options
Flexible spending account options
401k plan offerings
Paid holidays and vacation

Datastrong is committed to hiring and retaining a diverse workforce. We are an Equal Opportunity and Affirmative Action Employer, making decisions without regard to race, color, religion, sex, national origin, age, veteran status, disability, or any other protected class. If accommodation is needed in the application process, arrangements can be made with the local regional office."
87,Database Engineer Lead: Principal (TS),"Sterling, VA 20164",Sterling,VA,20164,None Found,None Found,"Must have an active Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearance
Must be able to obtain DHS Suitability
10+ years of relevant database experience
Demonstrated ability to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers)
Demonstrated experience mentoring junior to mid-level data professionals
Able to effectively work as a leader, in a group, or as an individual contributor
Excellent understanding of big data and data analytics
Experience working with large structured and unstructured data sets
Development experience building ETL pipelines at scale
Solid SQL development skills
Experience with Linux/Unix tools and shell scripts
Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing and encryption
Good communication skills, both oral and written
Must work well in a team environment as well as independently
Must exhibit good time management skills, independent decision making capability, and a focus on customer service.
","Using database expertise to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers) in support of a large, agile-based, cybersecurity system
Working with large structured and unstructured data sets
Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment
Design, setup, administer, and tune NoSQL databases in the AWS cloud
Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on tradeoff between performance and quality
Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations
Write and refine code to ensure quality and reliability of data extraction and processing
Analyze and resolve data performance and quality issues
Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.
Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance
Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates
Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to development team
Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment
Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages
Maintain current industry knowledge of relevant concepts, practices and procedures
","IBM Certified Data Engineer - Big Data
Google Cloud Certified Professional - Data Engineer
Cloudera Certified Professional - Data Engineer
CCDH: Cloudera Certified Developer for Apache Hadoop
CCAH: Cloudera Certified Administrator for Apache Hadoop
CCSHB: Cloudera Certified Specialist in Apache HBase
CSSLP Certified Secure Software Lifecycle Professional
Certifications related to Scaled Agile Framework (SAFe) such as SAFe Practitioner (SP) or SAFe Program Consultant (SPC)
DoD 8570.1 IAT Level I
",None Found,"Database Engineer Lead
**Top Secret Required**
BCMC is looking for motivated individuals to support a long-term engineering program for one of the country's highest priority cyber initiatives. On this program you will have the opportunity to work with technical leaders in cyber, networking, computer science, and data analytics to develop the next generation of automated cyber defense platforms. Individuals in this role are responsible for implementing, deploying, and maintaining database systems. These capabilities are used to analyze, detect, and prevent sophisticated threats and vulnerabilities on enterprise networks.
Responsibilities:
Using database expertise to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers) in support of a large, agile-based, cybersecurity system
Working with large structured and unstructured data sets
Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment
Design, setup, administer, and tune NoSQL databases in the AWS cloud
Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on tradeoff between performance and quality
Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations
Write and refine code to ensure quality and reliability of data extraction and processing
Analyze and resolve data performance and quality issues
Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.
Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance
Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates
Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to development team
Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment
Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages
Maintain current industry knowledge of relevant concepts, practices and procedures


Required Skills:
Must have an active Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearance
Must be able to obtain DHS Suitability
10+ years of relevant database experience
Demonstrated ability to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers)
Demonstrated experience mentoring junior to mid-level data professionals
Able to effectively work as a leader, in a group, or as an individual contributor
Excellent understanding of big data and data analytics
Experience working with large structured and unstructured data sets
Development experience building ETL pipelines at scale
Solid SQL development skills
Experience with Linux/Unix tools and shell scripts
Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing and encryption
Good communication skills, both oral and written
Must work well in a team environment as well as independently
Must exhibit good time management skills, independent decision making capability, and a focus on customer service.


Desired Skills:
Experience providing database engineering support to Intelligence, DoD, or DHS Customers
Understanding of Certification and Accreditation (ICD 503/DCID 6/3) processes as they apply to database technologies
Ability to support both SQL and NoSQL data management systems
Expertise in other RDBMS platforms such as Oracle RAC and SQL Server
Familiarity with AWS data migration tools such as AWS DMS, Amazon EMR, and AWS Data Pipeline
Experience with data transformation techniques such as aggregations, joins, and data cleaning
Experience with Red Hat Enterprise Linux (RHEL) operating system, storage configurations, network architecture, VMware, and/or related management tools
Database management experience of SQL databases such as MySQL and PostgreSQL in AWS cloud
Experience creating and managing NoSQL databases such as DynamoDB in the AWS cloud
Object mapping and migration of data from legacy structured and unstructured data sources to Amazon DynamoDB using AWS tools, custom code, or ETL scripts
Programming experience with languages such as R, Python, Java, JavaScript, JSON, etc.
Knowledge of Hadoop ecosystem, Map/Reduce, and data management products including Hbase, Hive, and Pig
DevSecOps and Continuous Integration / Continuous Delivery (CI/CD) knowledge
Experience or training in Six Sigma Methodology
ITIL knowledge and certification
Familiarity with SAFe (Scaled Agile Framework).


Required Education:

BS Computer Science, Computer Engineering, Computer Information Systems, OR Computer Systems Engineering. Two years of related work experience may be substituted for each year of degree level education


Desired Certifications:
IBM Certified Data Engineer - Big Data
Google Cloud Certified Professional - Data Engineer
Cloudera Certified Professional - Data Engineer
CCDH: Cloudera Certified Developer for Apache Hadoop
CCAH: Cloudera Certified Administrator for Apache Hadoop
CCSHB: Cloudera Certified Specialist in Apache HBase
CSSLP Certified Secure Software Lifecycle Professional
Certifications related to Scaled Agile Framework (SAFe) such as SAFe Practitioner (SP) or SAFe Program Consultant (SPC)
DoD 8570.1 IAT Level I
Company Overview:
BCMC is an Information Technology (IT), Cybersecurity, Information Assurance (IA), Big Data Management, Program Management, and more for Federal, State, and Local agencies. We possess highly skilled engineers, providing innovative solutions backed by strong past performances.
Benefits
Extremely competitive salary
95% employer paid for employee medical, dental, & vision coverages
100% employer paid for employee life, STD & LTD disability coverages
401k with company match and profit sharing
Flexible Spending Account (FSA) for dependent & health care
10 standard holidays & competitive Paid Time Off (PTO)

XxoWs9VYO9"
88,Google Technical Architect,"Washington, DC 20006",Washington,DC,20006,None Found,"Minimum 5 years of Consulting or client service delivery experience on Google GCP
",DevOps on an GCP platform. Multi-cloud experience a plus.,None Found,None Found,"Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?
Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Google Cloud Platform (GCP) Technical Architect Delivery is responsible for delivering Data On Cloud projects for Google GCP based deals. The ideal candidate would also be responsible for developing and delivering Google GCP cloud solutions to meet todays high demand in areas such as AI/ML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Google GCP Technical Architect is a highly performant GCP Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized GCP architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data soltuions on cloud. Using Google GCP public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications.

Role & Responsibilities:Work with Sales and Bus Dev teams in providing Data and GCP Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the GCP platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
Minimum 5 years of Consulting or client service delivery experience on Google GCP
Minimum 10 years of experience in big data, database and data warehouse architecture and delivery
Bachelors degree or 12 years previous professional experience
Able to travel 100% (M-TH)
Minimum of 5 years of professional experience in 2 of the following areas:
Solution/technical architecture in the cloud
Big Data/analytics/information analysis/database management in the cloud
IoT/event-driven/microservices in the cloud
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Extensive hands-on experience implementing data migration and data processing using GCP services etc.:
Data Ingestion : Cloud Pub/Sub, Data Transfer Service, Cloud IoT Core
Streaming Data Pipeline : Cloud Dataflow, Cloud Dataproc, Cloud Dataprep, Apache Beam
Data Warehousing & Data Lake : BigQuery, Cloud Storage
Advanced Analytics : Cloud ML engine, Google Data Studio, Tensorflow & Sheets

Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
Certified GCP Solutions Architect - Associate
Certified GCP Solutions Architect – Professional (Nice to have)
Certified GCP Big Data Specialty (Nice to have)
Certified GCP AI/ML Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:
DevOps on an GCP platform. Multi-cloud experience a plus.
Experience developing and deploying ETL solutions on GCP
Strong in Java, C##, Spark, PySpark, Unix shell/Perl scripting
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
- Multi-cloud experience beyond GCP a plus - AWS and Azure

Professional Skill Requirements
Proven ability to build, manage and foster a team-oriented environment
Proven ability to work creatively and analytically in a problem-solving environment
Desire to work in an information systems environment
Excellent communication (written and oral) and interpersonal skills
Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
89,"Senior Data Engineer, TS/SCI & Poly Required","Chantilly, VA",Chantilly,VA,None Found,None Found,"TS/SCI with Polygraph required
Bachelor's and fourteen (14) years of related experience. Experience may be substituted in lieu of degree
Expertise developing and implementing robust data schemas across heterogeneous datasets
Expertise with data security, and data tagging; experience with customer policies and systems is preferred; understanding of relevant privacy and data deconstruction policies or ability to develop expertise quickly
Expertise processing structured and unstructured data
Experience using python and/or R is beneficial
Experience leveraging APIs for data ingest and sharing",None Found,None Found,None Found,None Found,"Job Description
Description
SAIC is seeking a Data Engineer to perform data engineering tasks in support of a Business Analytics effort. This includes designing how data will be stored, consumed, integrated, and managed. Expected tasks will include:
Working together with the Government POC to determine, create, and populate an optimal data architecture, structure, and system
Plan, design, and optimize for data throughput and query performance issues
Develop and implement processes to automatically ingest, update, manage, and integrate heterogeneous data streams
Configure and utilize back-end database technologies and optimize the full data pipeline infrastructure to support the actual content, volume, and ETL of data to support queries and analysis
Qualifications
TS/SCI with Polygraph required
Bachelor's and fourteen (14) years of related experience. Experience may be substituted in lieu of degree
Expertise developing and implementing robust data schemas across heterogeneous datasets
Expertise with data security, and data tagging; experience with customer policies and systems is preferred; understanding of relevant privacy and data deconstruction policies or ability to develop expertise quickly
Expertise processing structured and unstructured data
Experience using python and/or R is beneficial
Experience leveraging APIs for data ingest and sharing
Desired Qualifications

Familiarity with Tableau
Scripting


Overview
SAIC is a premier technology integrator, solving our nation's most complex modernization and systems engineering challenges across the defense, space, federal civilian, and intelligence markets. Our robust portfolio of offerings includes high-end solutions in systems engineering and integration; enterprise IT, including cloud services; cyber; software; advanced analytics and simulation; and training. We are a team of 23,000 strong driven by mission, united purpose, and inspired by opportunity. Headquartered in Reston, Virginia, SAIC has annual revenues of approximately $6.5 billion. For more information, visit saic.com. For information on the benefits SAIC offers, see Working at SAIC. EOE AA M/F/Vet/Disability"
90,Database Engineering Lead,"Sterling, VA 20166",Sterling,VA,20166,None Found,None Found,None Found,None Found,None Found,"Using database expertise to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers) in support of a large, agile-based, cybersecurity system.
Working with large structured and unstructured data sets.
Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment.
Design, setup, administer, and tune NoSQL databases in the AWS cloud.
Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on tradeoff between performance and quality.
Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations.
Write and refine code to ensure quality and reliability of data extraction and processing.
Analyze and resolve data performance and quality issues.
Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.
Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance.
Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates.
Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to development team.
Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment.
Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages.
Maintain current industry knowledge of relevant concepts, practices and procedures
","Database Engineering Lead
Sterling, VA 20166

Culmen International is hiring for a Database Engineering Lead to work in support of a U.S. Government customer on a large mission critical development and sustainment program. The database engineering work will support the design, build, delivery, and operations for a network operations environment. Activities will include introducing new cyber capabilities to address emerging threats. This work is part of the Cybersecurity and Special Missions (CSM) area working collaboratively with agile development teams in the design, development, and deployment of advanced cybersecurity capabilities.

Qualified candidates will have expertise in the following:
Cloud Computing, Computer Engineering, Computer Science, Configuration Management, Cyber Jobs, Data Networking, Data Science, General Management, Hardware Engineering, Integration & Test Engineering, Software Engineering, Systems, Engineering, Test Engineering

Security Clearance Requirement: Current TS/SCI is required to be considered AND must also be eligible for DHS Suitability

Role and responsibilities include:
Using database expertise to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers) in support of a large, agile-based, cybersecurity system.
Working with large structured and unstructured data sets.
Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment.
Design, setup, administer, and tune NoSQL databases in the AWS cloud.
Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on tradeoff between performance and quality.
Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations.
Write and refine code to ensure quality and reliability of data extraction and processing.
Analyze and resolve data performance and quality issues.
Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.
Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance.
Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates.
Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to development team.
Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment.
Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages.
Maintain current industry knowledge of relevant concepts, practices and procedures


Requirement education and experience includes:
U.S. Citizenship
Current TS/SCI security clearance
Ability to obtain DHS Suitability. Current DHS Suitability is preferred
Education requirement:
BS Computer Science, Computer Engineering, Computer Information Systems, OR Computer Systems Engineering.
Two years of related work experience may be substituted for each year of degree level education.
10+ years of relevant database experience
Demonstrated ability to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers)
Demonstrated experience mentoring junior to mid-level data professionals
Able to effectively work as a leader, in a group, or as an individual contributor
Excellent understanding of big data and data analytics
Experience working with large structured and unstructured data sets
Development experience building ETL pipelines at scale
Solid SQL development skills
Experience with Linux/Unix tools and shell scripts
Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing and encryption
Good communication skills, both oral and written
Must work well in a team environment as well as independently
Must exhibit good time management skills, independent decision making capability, and a focus on customer service.
Desired skills include:
Experience providing database engineering support to Intelligence, DoD, or DHS Customers
Understanding of Certification and Accreditation (ICD 503/DCID 6/3) processes as they apply to database technologies
Ability to support both SQL and NoSQL data management systems
Expertise in other RDBMS platforms such as Oracle RAC and SQL Server
Familiarity with AWS data migration tools such as AWS DMS, Amazon EMR, and AWS Data Pipeline
Experience with data transformation techniques such as aggregations, joins, and data cleaning
Experience with Red Hat Enterprise Linux (RHEL) operating system, storage configurations, network architecture, VMware, and/or related management tools
Database management experience of SQL databases such as MySQL and PostgreSQL in AWS cloud
Experience creating and managing NoSQL databases such as DynamoDB in the AWS cloud
Object mapping and migration of data from legacy structured and unstructured data sources to Amazon DynamoDB using AWS tools, custom code, or ETL scripts
Programming experience with languages such as R, Python, Java, JavaScript, JSON, etc.
Knowledge of Hadoop ecosystem, Map/Reduce, and data management products including Hbase, Hive, and Pig
DevSecOps and Continuous Integration / Continuous Delivery (CI/CD) knowledge
Experience or training in Six Sigma Methodology
ITIL knowledge and certification
Familiarity with SAFe (Scaled Agile Framework).
Desired certifications include:
IBM Certified Data Engineer - Big Data
Google Cloud Certified Professional - Data Engineer
Cloudera Certified Professional - Data Engineer
CCDH: Cloudera Certified Developer for Apache Hadoop
CCAH: Cloudera Certified Administrator for Apache Hadoop
CCSHB: Cloudera Certified Specialist in Apache HBase
CSSLP Certified Secure Software Lifecycle Professional
Certifications related to Scaled Agile Framework (SAFe) such as SAFe Practitioner (SP) or SAFe Program Consultant (SPC)
DoD 8570.1 IAT Level I
Job Type: Full-Time (this job is not open to 1099 consulting)
Benefits: Health, Vision, Dental, 401k, Disability and Life Insurance Programs

#623"
91,Data Engineer - Machine Learning (Python/Spark/AWS) - Card Tech,"McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Locations: VA - McLean, United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer - Machine Learning (Python/Spark/AWS) - Card Tech

As a Capital One Data Engineer, you'll be part of a team that’s building new analytical and machine learning tools and frameworks to exploit advantages in the latest developments in cloud computing - EMR, Airflow, SageMaker, etc. You will participate in detailed technical design, development and implementation of applications used by our data scientists and business analysts to build and launch models, analyze data, and make million dollar decisions. We work closely with our users and are looking for people who are excited to iterate quickly on solutions and see the impact in days, not months.

Who You Are

You yearn to be part of cutting edge, high profile projects and are motivated by delivering world-class solutions on an aggressive schedule

Someone who is not intimidated by challenges; thrives even under pressure; is passionate about their craft; and hyper focused on delivering exceptional results

You love to learn new technologies and mentor junior engineers to raise the bar on your team

It would be awesome if you have a robust portfolio on Github and/or open source contributions you are proud to share

Passionate about intuitive and engaging user interfaces, as well as new/emerging concepts and techniques.

The Job

Collaborating as part of a cross-functional Agile team to create and enhance software that enables state of the art data science, machine learning, and data analysis

Developing and deploying machine learning pipelines using cutting edge tools like Airflow, Dask, and Scikit-Learn

Developing frameworks to accelerate the model development lifecycle

Utilizing programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake

Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment and Test Automation to enable the rapid delivery of working code utilizing tools like Jenkins, Terraform, Git and Docker

Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance

Basic Qualifications

Bachelor’s Degree or Military Experience

At least 2 years of professional work experience as a software or data engineer in an agile environment

At least 2 years of experience in open source programming languages

At least 1 year of experience working with cloud capabilities

Preferred Qualifications

Master's Degree or PhD

4+ years of experience in Python, Scala, or R for large scale data analysis

4+ years' experience with Relational Database Systems and SQL (PostgreSQL or Redshift)

4+ years of UNIX/Linux experience

2+ years of experience with Cloud computing (AWS)

2+ years of data modeling experience

1+ years of experience with Spark

1+ years of experience in developing high volume transaction processing solutions that can be scaled for millions of daily transactions

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
92,Data Engineer - Batch Capability Specialist,"Chantilly, VA 20151",Chantilly,VA,20151,None Found,None Found,None Found,None Found,None Found,None Found,"Job Title: Data Engineer - Batch Capability Specialist
Clearance: TS/SCI
Location: Chantilly, VA
Compensation: Excellent Benefits and Salary

Job Description:
As part of a technology architecture team, support a DIA sponsored project in support of functional correlation to each of the main data collection and delivery functions. As the project grows and becomes integrated across operational and tactical elements of the DoD, this team will identify established capabilities that can meet standardized use cases as well as missing capabilities that need to be added in order to provide common data usage models. Successful candidate will create pre-deployment integration plans that ensure all use cases associated with batch data will be met by either existing or new products that align with the target environment. Additionally, candidate will align storage management practices with local and remote storage retention policies.
Required:
Experience with data engineering tools and techniques
Understanding of database architecture
Data models
An analytical mindset with problem-solving skills
Excellent communication and collaboration skills
BSc/BA in computer science or relevant field
Also, candidate should have a working knowledge of the following:
Data
Identifies unique datasets within applications
Captures formatting
Identifies expected volumetrics
Identifies all current sharing mechanisms
Makes initial recommendations on polyglot storage for each dataset
Identifies expected resource needs for each dataset
Application
Identification of datasets
Identify transfer mechanisms in existence today
Characterize API's in order to verify CAN (controller area network) support
Identifies changes that need to be made in order to remove user identity requirements for access by other systems
Provides guidance on integration tasks regarding API's and ingest
Mission
Network capacity between sites and mission partners
Bandwidth utilization
Cross domain
Evaluates information to determine nearest node usage and failover / COOP
Identifies changes that need to occur in the data architecture to ensure no operational impact to warfighters during integration
Desired:
Database Warehouse
Data Mining
Statistical modeling and regression analysis
Knowledge of languages, especially R, SAS, Python, C/C++, Ruby Perl, Java, and MATLAB
Database solution languages, especially SQL, Cassandra, Bigtable, or similar
Hadoop-based analytics, such as HBase, Hive, Pig, and MapReduce
Operating systems, especially UNIX, Linux, and Solaris
Machine learning, including AForge.NET and Scikit-learn
Possess a high degree of ingenuity, creativity, and resourcefulness
Ability to understand military and intelligence operations as described by military experts and to represent their employment (verbally, graphically, and in simulations)
Prior military or intelligence community experience


Benefit Summary:
Volant Associates provides an industry-leading benefits package to attract and retain the very best talent within a very competitive recruiting environment and support its employees and their dependents.

100% Volant Paid Standard Benefits:
200% Matching on employee 401k contributions (on up to 5% of employee salary deferral)
20.5 days (164 hours) of Paid Time Off
7 paid holidays per year
Health care insurance for employee and dependents thru UHC
Dental care insurance for employee and dependents thru UHC
Vision care insurance for employee and dependents thru VSP
Life and Personal Accident Insurance ($50k coverage) thru CIGNA
Additional Life and Personal Accident Insurance (1 x salary up to $170k) thru MOO
Short term disability Insurance (60% of earnings up to $2,308 per week) thru CIGNA
Long term disability insurance (60% of earnings up to $10k per month) thru CIGNA
Educational assistance (up to $3,500 per calendar year)
Adoption assistance
Commuter benefits
Training and development opportunities
Cell phone stipend (up to $50 per month)
Corporate laptop computer
Gym access (for Chantilly-based employees)
Additional programs available to all employees:
Heath Savings Account (HSA)
Health care Flexible Spending Account (FSA)
Dependent care Flexible Spending Account (FSA)
Voluntary additional levels of Life and Personal Accident Insurance"
93,Senior Business Intelligence / Data Engineer,"Reston, VA 20191",Reston,VA,20191,None Found,"
Bachelor’s degree in Computer Engineering, Computer Science, Management Information Systems, or a related business field.
5+ years of development, architecture, and configuration skills in Microsoft SQL Server and extensive experience with SQL Server Reporting Services (SSRS) and SQL Server Integration Services (SSIS).
Should possess expert level competence with Microsoft T-SQL, Microsoft PowerShell, Microsoft Visual Studio, and Microsoft SQL Server Management Studio (SSMS).
Experience with Power BI
Experience with BI analytics stack including reports, dashboards, and scorecards.
",None Found,"
The Senior BI Intelligence / Data Engineer is responsible for designing enterprise and departmental business intelligence, data warehousing and reporting solutions.
Work with stakeholders at various levels to assist with data-related technical issues and support their data acquisition and infrastructure needs.
Work with analytics to develop greater functionality in data presentation.
Solutions include, end-user reports, data visualizations, ETL systems, master data management and other BI Solutions.
This position will perform requirements analysis, design, and implementation of end-user requested BI solutions.
Translate business reporting and analytic requirements into ETL and report specifications.
Develop and implement ETL processes, reports and queries in support of business analytics.
Develop and implement interactive analytic reports and dashboards.
",None Found,None Found,"Duties and Responsibilities

The Senior BI Intelligence / Data Engineer is responsible for designing enterprise and departmental business intelligence, data warehousing and reporting solutions.
Work with stakeholders at various levels to assist with data-related technical issues and support their data acquisition and infrastructure needs.
Work with analytics to develop greater functionality in data presentation.
Solutions include, end-user reports, data visualizations, ETL systems, master data management and other BI Solutions.
This position will perform requirements analysis, design, and implementation of end-user requested BI solutions.
Translate business reporting and analytic requirements into ETL and report specifications.
Develop and implement ETL processes, reports and queries in support of business analytics.
Develop and implement interactive analytic reports and dashboards.
Qualifications

Bachelor’s degree in Computer Engineering, Computer Science, Management Information Systems, or a related business field.
5+ years of development, architecture, and configuration skills in Microsoft SQL Server and extensive experience with SQL Server Reporting Services (SSRS) and SQL Server Integration Services (SSIS).
Should possess expert level competence with Microsoft T-SQL, Microsoft PowerShell, Microsoft Visual Studio, and Microsoft SQL Server Management Studio (SSMS).
Experience with Power BI
Experience with BI analytics stack including reports, dashboards, and scorecards.
Ideal Candidate

Experience with Microsoft Azure Cloud Platform
Experience with Microsoft SQL Server
Experience with SQL Server Reporting Services (SSRS)
Experience with [SSDT] Database Projects
Experience with SQL Server Analysis Services (SSAS)
Experience with Power BI
Experience with Dynamics 365 (highly desirable)
Some Development Experience (C#, PowerShell, preferable)"
94,Data Engineer,"McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,"Lead the team ( 8-10) with full stack development for DevOps efforts for a federal customer
",None Found,None Found,"At CollabraLink, we’re committed to providing federal clients with simple and intuitive solutions that increase efficiency and enhance citizen engagement. Using advanced technology, rigorous processes and trusted guidance, we’re making government more interactive, productive and secure. We understand that our employees are our greatest asset. Our goal is to create an environment where employees can do important, purposeful work and be rewarded for individual and team success.

CollabraLink is seeking a Data Engineer in a AI/ML environment to join our team.


Responsibilities:
Lead the team ( 8-10) with full stack development for DevOps efforts for a federal customer

Required Experience:
Minimum of ten (10) years of experience in the Information Technology field focusing on AI/ML engineering projects, DevSecOps and technical architecture specifically inclusive of following:
Possess strong architecture & design experience, including at least three (3) years of experience deploying production enterprise applications in AWS that use AI/ML.
Experience in large scale, high performance enterprise big data application deployment and solution architecture on complex heterogeneous environments in AWS.
Minimum of a Bachelor’s degree in Computer Science, Information Technology Management or Engineering.

Preferred Experience:
Experience law enforcement agencies
AWS professional level certifications
Experience with AI/ML technologies
Experience with Javascript language technologies such as Angular or React

We actively practice the philosophy that empowered employees make successful teams. That’s why we strive to put employees in positions where they can grow, both personally and professionally. CollabraLink offers a full suite of benefits including comprehensive medical, dental and vision plans, Flexible Spending Accounts, matching 401K, paid time off, tuition reimbursement plans and much more.

CollabraLink is a fast growing CMMI-DEV Maturity Level 3, Small Business professional services firm. Founded in 2003, CollabraLink has long established ourselves as a value-add partner assisting our customers in solving their most difficult problems. We bring expertise across a wide variety of IT and Mission Support services driving significant results for our customers.

CollabraLink is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.
#CL"
95,Data Engineer,"Washington, DC 20003",Washington,DC,20003,None Found,"Active Top Secret security clearance with SCI eligibility
BS degree in a related scientific or engineering discipline from an accredited college or university and ten (10) to fourteen (14) years of progressive experience, or an MS degree in a related scientific or engineering discipline, and eight (8) to twelve (12) years of progressive experience, or a Ph.D. degree in a related scientific or engineering discipline and four (4) to seven (7) years of progressive experience.
Familiar with ETL technologies, MapReduce, JSON/XML transformations and schemas
Familiar with AngularJS
Familiar with Apache NiFi and Java (NAR) NiFi Archives
Knowledge of Amazon Web Services (AWS Cloud)
Programming languages – Java/JEE, Javascript, Python, Groovy, Shell Script
HTTP via REST and SOAP
Datastores – HDFS, MongoDB, S3, Elastic, NoSQL, RDBMS
Build and Configuration Management Tools – Maven, Ansible, Puppet
Working knowledge with public keys and digital certificates
Linux/Unix server environments

Preferred Qualifications (desired but not required):
Master’s level education
Familiarity with: jQuery, XPath, XQuery, Spark, Impala, Sqoop, Hive/Pig, Python, Gradle, Maven, PL/SQL, Unix Shell, C++/C, AngularJS, Spring, JSON, XML/XSLT/HTML, JPA/Hibernate, Spark, Accumulo, MapReduce, Storm/Kafka, HSpace, Pig, Servlet/JSP, LDAP
",None Found,None Found,None Found,None Found,"“Jacobs National Security Solutions (NSS) provides world-class IT network and service management capabilities; cutting edge cyber threat awareness and cybersecurity solutions; innovative web- and software applications development; and advanced data analytics for major clients in the Intelligence Community, Department of Defense, and Federal Civilian Agencies.

Our forward thinking solutions deliver an integrated approach to IT network design and management, full lifecycle IT service management, IT service delivery, asset management, logistics and procurement, and vendor management. We leverage the expertise and passion of our employees to conduct identity and access management, penetration testing, and vulnerability assessments for our nation’s most closely guarded agencies and networks. Our Cyber Security Operations Centers ensure safe, effective network operations for Federal clients while our data scientists are helping stop illegal acts before they can endanger Americans or our way of life.
Jacobs promotes a culture of operational excellence to create a safer, smarter, and more connected world while upholding the highest standards of compliance, quality and integrity.
We continue to thrive and need your talent and motivation to help propel us farther, faster.”
Jacobs is seeking a Data Engineer in Washington, DC. Duties will include:
Experience creating and maintaining data pipelines and transformation flows in a cloud environment
Data management/mapping among multiple distinct data sources
Cloud management and server administration of domain services
Big Data infrastructure services and cross domain data transfer
Qualifications
Active Top Secret security clearance with SCI eligibility
BS degree in a related scientific or engineering discipline from an accredited college or university and ten (10) to fourteen (14) years of progressive experience, or an MS degree in a related scientific or engineering discipline, and eight (8) to twelve (12) years of progressive experience, or a Ph.D. degree in a related scientific or engineering discipline and four (4) to seven (7) years of progressive experience.
Familiar with ETL technologies, MapReduce, JSON/XML transformations and schemas
Familiar with AngularJS
Familiar with Apache NiFi and Java (NAR) NiFi Archives
Knowledge of Amazon Web Services (AWS Cloud)
Programming languages – Java/JEE, Javascript, Python, Groovy, Shell Script
HTTP via REST and SOAP
Datastores – HDFS, MongoDB, S3, Elastic, NoSQL, RDBMS
Build and Configuration Management Tools – Maven, Ansible, Puppet
Working knowledge with public keys and digital certificates
Linux/Unix server environments

Preferred Qualifications (desired but not required):
Master’s level education
Familiarity with: jQuery, XPath, XQuery, Spark, Impala, Sqoop, Hive/Pig, Python, Gradle, Maven, PL/SQL, Unix Shell, C++/C, AngularJS, Spring, JSON, XML/XSLT/HTML, JPA/Hibernate, Spark, Accumulo, MapReduce, Storm/Kafka, HSpace, Pig, Servlet/JSP, LDAP

Essential Functions
Physical Requirements:
Most work will be done at a desk or computer.
Work Environment:
General Office environment. The work environment is fast-paced and sometimes involves extreme deadline pressures. The nature of the work requires a high degree of teamwork and cooperation with other members of the staff as well as individuals across the Company and Customers.
Equipment & Machines:
General office equipment including PC/laptop, Fax, Copiers, Shredder, Printers, Telephone, and other miscellaneous office equipment.
Attendance:
Attendance is critical at all times. Must be able to work a 40-hour workweek, normally Monday through Friday. However, times and days may vary depending on business requirements. Needs to be available to work overtime during critical peaks and be available to meet last minute requests for overtime should the situation occur.
Other Essential Functions:
Must be able to communicate effectively both verbally and in writing. Grooming and dress must be appropriate for the position and must not impose a safety risk/hazard to the employee or others. Must put forward a professional behavior that enhances productivity and promotes teamwork and cooperation. Must be able to interface with individuals at all levels of the organization both verbally and in writing. Must be well-organized with the ability to coordinate and prioritize multiple tasks simultaneously. Must work well under pressure to meet deadline requirements. Must be willing to travel as needed. Must take and pass a drug test and background check as well as a motor vehicle records check. Must be a US citizen.
 
Jacobs is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or other characteristics protected by law. Jacobs is a background screening, drug-free workplace."
96,Azure Cloud Engineer,"Silver Spring, MD",Silver Spring,MD,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Lynker has an opening for a Systems Engineer with a focus on Microsoft Azure cloud technologies to support NOAA’s Office for Coastal Management (OCM). This is a full-time position located in Charleston, SC or Silver Spring, MD.

Core responsibilities include, but are not limited to:

Support and improve existing on-premises and Microsoft Azure Cloud infrastructure
Design and implement new on-premises and Microsoft Azure systems while implementing best practices and industry standards to meet contract requirements
Migrate on-premises infrastructure to Microsoft Azure cloud
Configure Microsoft Azure monitoring and alerts for cloud resource availability
Manage on-premises and cloud based Windows and Linux virtual servers
Work closely with application development and data engineer teams on day-to-day tasks along with project planning and implementation
Administration of Cisco switches, routers, firewalls, and other network devices

Qualifications

Required Skills and Qualifications

Bachelor’s Degree in Computer Science and a minimum of 5 years successful experience (high school diploma and professional certification(s) can be substituted for education)
Vast understanding of cloud services and solutions in platforms such as Azure
Experience planning and executing on-premises to Azure migrations
Experience working with Azure PaaS, Azure networking, and Azure site reliability solutions (specific examples of each will be requested)
Strong practical knowledge of Windows and Linux server operating systems (on-premises & IaaS)
Proven experience managing a VMware virtualized server environment including ESX host administration
Comprehensive understanding of IT security best practices and threat protection
Processing of a National Agency Check and fingerprinting will be required.

Desired Skills


Microsoft Azure Certification
Experience with Devops practices
Experience with ArcGIS Server and ESRI products

About Lynker

Lynker Technologies, LLC is a growing, Hub-zone certified small business specializing in professional, scientific and technical services. Our continually expanding team combines scientific expertise with mature, results-driven processes and tools to achieve technically sound, cost effective solutions in hydrology/water sciences, geospatial analysis, information technology, resource management, conservation, and management and business process improvement.

We focus on putting the right people in the right place to be effective. And having the right people is critical for success. Our streamlined organization enables and empowers our talented professionals to tackle our customers’ scientific and technical priorities – creatively and effectively.

Lynker offers a team-oriented work environment, competitive salaries and benefits, and the opportunity to work in a culture of exceptionally skilled and diverse professionals who embrace sound science and creative solutions.

Lynker is an E-Verify employer.

Lynker Technologies is proud to be an Equal Opportunity Employer and encourages women, minorities, individuals with disabilities and veterans to apply."
97,Data Engineer,"Reston, VA",Reston,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Communications, Information and Navigation Office (CINO) at the Applied Research Laboratory (ARL) is looking for a Data Engineer to join our Algorithms, Prototyping and Integration Department located in Reston, VA. Responsibilities include, but are not limited to: Developing, implementing, testing, optimizing and maintaining data management architectures, such as databases and large-scale processing systems. This includes working with software developers and data scientists on data initiatives and ensuring optimal data delivery architectures throughout new and ongoing projects. Additional responsibilities include: Collaborating with data scientists to ensure data management architectures support analysis requirements; researching and acquiring new data sets; developing data set processes for data modeling, mining, and production; developing and implementing approaches for system to system integration using FOSS data engineering tools and languages; providing and implementing recommendations to optimize and improve data quality, integrity and accessibility; selecting and integrating Big Data tools and frameworks required to provided requested capabilities; and participating in proposal generation to capture follow on business. This job will be filled as a level 2, level 3, or level 4, depending upon the successful candidate's competencies, education, and experience. Typically requires a Bachelor's degree or higher in an Engineering or Science discipline or higher plus two years of related experience, or an equivalent combination of education and experience for a level 2. Additional experience and/or education and competencies are required for higher level jobs. A Bachelor’s degree in Computer Science, Computer Engineering, Information Science and Technology (IST); is preferred plus 7-10 years of related data engineering experience. Experience with the following is preferred: The ideal candidate should be familiar with the following data engineering enabling tools and scripting languages: Cassandra, MySQL, Redis, PostgreSQL, MongoDB, neo4j, Hive, Pig, Impala, Sqoop, Java, Python, Scala, and NIFI. Experience with the following is required: Subject matter experience designing, optimizing and maintaining data management architectures, as well as developing custom solutions based on customer use cases and workflows; subject matter expertise in SQL, NoSQL, working with relational databases (specifically Postgres), and building and optimizing Enhancement, Transform, and Load (ETL) processes; and the ability to work in a team environment and communicate effectively. An active TS/SCI Clearance is also required. You must be a U.S. Citizen to apply. Candidates selected will be subject to a government security investigation. Employment with the Applied Research Laboratory will require successful completion of a pre-employment drug screen. This is a one-year fixed-term renewable appointment."
98,Big Data Engineer,"Springfield, VA",Springfield,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Key Role:
Serve as part of an innovative team building modern technology solutions to improve government mission operations. Develop a new data sharing environment involving multiple government agencies and organizations utilizing Big Data technologies and processing various types of structured and unstructured data. Design, implement, and manage databases and data delivery systems and transform them into insights, analysis, and reporting. Leverage deep comprehension of database design and implementation tools, including entity-relationship data modelling and SQL, distributed computing architectures, operating systems, storage technologies, and memory management and networking to create structure and value out of complex and ambiguous technical challenges with little guidance.
Basic Qualifications5+ years of experience with designing and implementing databases to handle structured and unstructured data involving SQL and NoSQL technologiesExperience with data modeling with various data structures and formatsExperience with Big Data processing and storage technologies, including Hadoop, MapReduce, HIVE, PIG, Apache Spark, Kafka, Informatica, and relational databasesExperience with developing data processing programs in Java, using scripting languages, including Perl, Python, or Shell scripting, to support data manipulations and ETL proceduresExperience working in teams involved in building and designing large-scale applicationsAbility to obtain a security clearanceBA or BS degree
Additional QualificationsExperience with indexing or search technologies, including ElasticSearchExperience with analysis of complex data structures involving biographic, biometric, and geographic dataDHS Suitability clearance
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
We’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change."
99,Data Engineer - Secret Clearance,"Washington, DC",Washington,DC,None Found,None Found,"TOP SECRET Cleared professional required to strategize and execute on logical and physical aspects of growing Enterprise Data Warehouse.
Expertise in dimensional modeling, snowflake and star schemas, snapshots, facts, etc.
Candidate will assist multiple Scrum teams develop logical and physical models as well as data load strategies. Other important areas of expertise include:
1) entity-relationship modeling techniques
2) translate a logical dimensional model into a star-schema design
3) Client interaction skills
4) Use of tools such as ErWIN
5) Data Security considerations
6) Work well on teams","
Fluency in SQL and NoSQL database technologies such as SQL Server, Oracle SQL
Fluency in programming languages such as Python and R; and query languages such as SQL
Proficiency in data extraction, transformation, and loading to support advanced analytics
Familiarity with JSON and XML data formats
Experience in web scraping (e.g., using R or Python) and retrieving data from APIs
Experience with data visualization tools, such as Tableau, Qlik, PowerBI, d3.js, or equivalent
Ability to identify solutions for communicating across different technologies
Creativity and ability to look past existing workflows and identify opportunities for optimization
Thrives in fast-paced work environment with multiple stakeholders
Ability to decompose a technical problem into its sub-components and build a plan to rigorously tackle the process that is defensible and repeatable
Strong strategic communication skills to articulate pipeline flows and actively listening to identify business problems and their causes
High-performing team player
",None Found,None Found,None Found,"Secret/Top Secret Data Engineer

Work you’ll do

Data Engineer – Center for Analytics


Project Description:

Center for Analytics is responsible for providing cross-functional, enterprise-level data analysis and solutions needed to accomplish key foreign policy objectives and make informed decisions. Over the last year, Deloitte has supported the incubation of the Center for Analytics through development and implementation of a strategic vision. The center's data science and engineering teams produce enterprise-level advanced analytics and data-driven decision-support products for principal executives and Bureau stakeholders.


Role Description:

The Data Engineer will develop robust and secure data pipelines to facilitate operation and management of analytics products that advance and enable the mission outcomes. In this role, the Data Engineer will work with data scientists to build and implement tools that accelerate ETL and delivery of data through multiple workstreams; productionalize and automate pipelines; and monitor and optimize performance of existing data processes.
Required Skills:

Fluency in SQL and NoSQL database technologies such as SQL Server, Oracle SQL
Fluency in programming languages such as Python and R; and query languages such as SQL
Proficiency in data extraction, transformation, and loading to support advanced analytics
Familiarity with JSON and XML data formats
Experience in web scraping (e.g., using R or Python) and retrieving data from APIs
Experience with data visualization tools, such as Tableau, Qlik, PowerBI, d3.js, or equivalent
Ability to identify solutions for communicating across different technologies
Creativity and ability to look past existing workflows and identify opportunities for optimization
Thrives in fast-paced work environment with multiple stakeholders
Ability to decompose a technical problem into its sub-components and build a plan to rigorously tackle the process that is defensible and repeatable
Strong strategic communication skills to articulate pipeline flows and actively listening to identify business problems and their causes
High-performing team player

Desired Skills:

DOD, DOS, or IC Top Secret/SCI Security Clearance )
Familiarity with analytics techniques, such as statistics, simulation modeling, optimization, machine learning, or natural language processing
Confidence to drive assignments to completion
Eagerness to learn and develop
Familiarity with containerization and orchestration technologies such as Docker and Kubernetes
Familiarity with Alteryx

Team Values: Culture eats strategy for breakfast

The CfA Team has codified their ten core values, and strives to find like-minded team members who share them.
Compassion: Have the compassion to care about your team, your clients, and their problems. Act respectfully.
Curiosity: Be curious. Learn from people and challenges around you; follow the facts; explore the details.

Candor: Be candid in your interactions, but deliver your perspective with the purpose of team improvement.
Communication: Communicate clearly, concisely, and with your authentic voice.
Craftiness: Some problems require crafty solutions; don’t let roadblocks on the most obvious path stop you.
Commitment: Show commitment to the task at hand and to your team’s success, regardless of the size of your role.
Collaborative: Be willing to collaborate in cross-functional teams, valuing each team member’s mix of unique skills as an asset to the product’s success.
Create: Creation over ideation. Ideas are important but testing and honing ideas through the creative process is how you go from zero to one.
Craftsmanship: Make well-designed products. As Steve Jobs said, “Design is not just what it looks like and feels like. It’s how it works.”
Celebrate: Find the time to celebrate one another, in personal and professional endeavors alike.

The team
Analytics & Cognitive
In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements

Qualifications

TOP SECRET Cleared professional required to strategize and execute on logical and physical aspects of growing Enterprise Data Warehouse.
Expertise in dimensional modeling, snowflake and star schemas, snapshots, facts, etc.
Candidate will assist multiple Scrum teams develop logical and physical models as well as data load strategies. Other important areas of expertise include:
1) entity-relationship modeling techniques
2) translate a logical dimensional model into a star-schema design
3) Client interaction skills
4) Use of tools such as ErWIN
5) Data Security considerations
6) Work well on teams

How you’ll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitte’s culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.

Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.

Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.
#IND:PTY"
100,Hadoop Data Engineer,"Chantilly, VA",Chantilly,VA,None Found,None Found,"
Extensive experience in engineering and designing data management solutions using Hadoop ecosystem tools and technologies with particular emphasis in using Sqoop, Spark, Scala, Python, Java, Shell HIVE and IMPALA for minimum of three (3) years
Proficient in the data ingestion pipeline process, exception handling and metadata management on big data platforms
Extensive experience in architecting, and designing data architecture solutions using Hadoop ecosystem tools and technologies like, Hive, Impala, HBase, and Solr
Extensive experience in GitHub CI/CD
Experience providing technical and data leadership to the application development terms, IT and the enterprise.
Experience in designing shared data assets such as data discovery and analytics platforms, metadata, operational data stores, data warehouses and data marts
Experience in business intelligence disciplines, and a deep understanding of Data Warehousing BI, and advanced analytics concepts in large organizations",None Found,"
Extensive experience in engineering and designing data management solutions using Hadoop ecosystem tools and technologies with particular emphasis in using Sqoop, Spark, Scala, Python, Java, Shell HIVE and IMPALA for minimum of three (3) years
Proficient in the data ingestion pipeline process, exception handling and metadata management on big data platforms
Extensive experience in architecting, and designing data architecture solutions using Hadoop ecosystem tools and technologies like, Hive, Impala, HBase, and Solr
Extensive experience in GitHub CI/CD
Experience providing technical and data leadership to the application development terms, IT and the enterprise.
Experience in designing shared data assets such as data discovery and analytics platforms, metadata, operational data stores, data warehouses and data marts
Experience in business intelligence disciplines, and a deep understanding of Data Warehousing BI, and advanced analytics concepts in large organizations",None Found,None Found,"Overview
BRMi Technology is seeking a Hadoop Data Engineer to support a large client in the Northern Virginia area. The selected candidate will support the Information System Division and the enterprise by providing comprehensive data engineering solutions in translating our clients business vision and strategies into effective IT and business capabilities through the design, implementation, and integration of IT systems in the big data domain.
Responsibilities
The Data Engineer will be responsible for guiding the evolution, development, and governance of our clients data with a specific focus on Hadoop Data solutions.
Qualifications
Required:
Extensive experience in engineering and designing data management solutions using Hadoop ecosystem tools and technologies with particular emphasis in using Sqoop, Spark, Scala, Python, Java, Shell HIVE and IMPALA for minimum of three (3) years
Proficient in the data ingestion pipeline process, exception handling and metadata management on big data platforms
Extensive experience in architecting, and designing data architecture solutions using Hadoop ecosystem tools and technologies like, Hive, Impala, HBase, and Solr
Extensive experience in GitHub CI/CD
Experience providing technical and data leadership to the application development terms, IT and the enterprise.
Experience in designing shared data assets such as data discovery and analytics platforms, metadata, operational data stores, data warehouses and data marts
Experience in business intelligence disciplines, and a deep understanding of Data Warehousing BI, and advanced analytics concepts in large organizations

Desired:
Advanced degree in MIS, computer science, statistics, marketing, management, finance or related field
Experience with Data Governance and Data Pipeline Management
Prior experience in the Financial Industries and large banks

** BRMi will not sponsor applicants for work visas for this position.**
**This is a W2 opportunity only**

EOE/Minorities/Females/Vet/Disabled
We are an equal opportunity employer that values diversity and commitment at all levels. All individuals, regardless of personal characteristics, are encouraged to apply. Employment policies and decisions on employment and promotion are based on merit, qualifications, performance, and business needs. The decisions and criteria governing the employment relationship with all employees are made in a nondiscriminatory manner, without regard to race, religion, color, national origin, sex, age, marital status, physical or mental disability, medical condition, veteran status, or any other factor determined to be unlawful by federal, state, or local statutes."
101,Software (Data) Engineer,"Vienna, VA",Vienna,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Company Overview


For 30 years, clients in the private and public sectors have relied upon SOS International LLC (SOSi) for critical operations in the world’s most challenging environments. SOSi is privately held, was founded by its current ownership in 1989, maintains corporate headquarters in Reston, VA, and specializes in providing logistics, construction, training, intelligence, and information technology solutions to the defense, diplomatic, intelligence and law enforcement communities.

All interested individuals will receive consideration and will not be discriminated against on the basis of race, color, religion, sex, national origin, disability, age, sexual orientation, gender identity, genetic information, or protected veteran status. SOSi takes affirmative action in support of its policy to advance diversity and inclusion of individuals who are minorities, women, protected veterans, and individuals with disabilities.


1-190911-9564: Software (Data) Engineer

Location U.S. - Virginia - Vienna

Open Date 9/11/2019



JOB DESCRIPTION

SOSi's Special Program Division, is seeking a Software Engineer to develop and maintain several data collection, management, processing, and analysis projects. A successful candidate will possess a solid foundation in CS principles (data structures, asynchronous memory management, and OOP), the ability to write maintainable code, and above all else a desire to learn.
ESSENTIAL JOB DUTIES
Produce software to solve challenging problems involving large datasets
Work well in a small team in a fast-paced development cycle
Identify areas where the efforts of the software team can increase efficiency for the organization as a whole
Provide technical support to customers and co-workers for existing software
Manage competing priorities to meet deadlines
Maintain a positive work atmosphere
Communicate effectively internally and with clients
Mentor Junior Developers

MINIMUM REQUIREMENTS

Current/ability to obtain Secret Clearance
Bachelor's in Computer Science or related field
2-5 years experience in software engineering/development
An understanding of version control using Git
An understanding of the Linux command line
Proficiency in Python
Proficiency Go or similar languages
Capable of learning and applying new skills when demands exceed existing capabilities
DESIRED QUALIFICATIONS
Proficiency in JS, C, C++, or Java
Experience with continuous integration / continuous deployment tools
Experience with modern relational, non-relational databases, and/or file storage solutions (e.g. Postgres, Redis, S3)
Experience with virtualization/containerization (e.g. Docker, VMware, XenServer)
Experience with distributed systems (e.g. concurrent processes and managing shared state)
Experience building and interacting with RESTful APIs
Fluency in a foreign language
Experience with HTTP/Proxy servers (eg. Nginx, Squid, Privoxy)
Experience with Machine Learning/Artificial Intelligence
Fluency in a foreign language

ADDITIONAL INFORMATION

WORK ENVIRONMENT

Normal office working conditions with possible requirement to lift and/or move objects or packages of up to 25 lbs
Periods of non-traditional working hours including consecutive nights or weekends when necessary"
102,Data Engineer,"Herndon, VA",Herndon,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"**THIS POSITION REQUIRES AN ACTIVE TS/SCI with POLY **

BI&A seeking a Software Engineer with data transformation (ETL) experience working with latest industry tools; elastic search, apache Kafka, and Apache NiFi.

DUTIES ENTAIL:
Work with a teammate on data integration requirements.
Write code on ETL platform to transform data to a suitable formats as defined by IC ITE initiatives.
Add features to ETL platform to shorten timelines for future data integration efforts.
Develop, maintain code, and integrate software into a fully functional software system.
Participate in daily scum meetings, sprint retrospectives, and other agile processes.
Work with external teams to validate data ingest.
Provide and maintain documentation of system architecture, development, and enhancements.

EDUCATION:
Bachelor’s Degree and 6 or more years’ experience or Master's Degree with 3 or more years' experience from an accredited course of study, in engineering, computer science, mathematics, physics or chemistry.

REQUIRED EXPERIENCE:
6+ years of software development experience
Linux/Unix experience
Object Oriented programming language
Possess strong verbal and written communication skills
Possess strong analytical skills, with excellent problem solving abilities in the face of ambiguity

DESIRED EXPERIENCE:
Expertise in data ingestion, data transformation (ETL), and data modeling.
Experience with Java, Ruby, or Python
Experience in Agile/SCRUM enterprise-scale software development
3 years’ experience working with one of the following batch-processing and tools (eg, Nifi, Midpoint, MapReduce, Yarn, Pig, Hive, HDFS, Oozie)
1 year working with Restful web services
Experience with code development, deployment, versioning, and build tools (eg, Eclipse, git, svn, maven, Jenkins)


BI&A is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.

Equal Employment Opportunity is the Law (PDF)

BackShare Apply Now"
103,Data Analytics Specialist,"Ashburn, VA 20147",Ashburn,VA,20147,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description
CACI is currently looking for experienced Data Scientists, Architects, Analysts, and Engineers to join our Data Analytics team for the Border Enforcement Applications for Government Leading-Edge Information Technology (BEAGLE) contract at the Agile Solution Factory (ASF). Join this passionate team of industry-leading technologists supporting Customs and Border Protection (CBP) in Northern Virginia and nationwide!
As a member of the BEAGLE ASF Team, you will support the men and women charged with safeguarding the American people and enhancing the Nation’s safety, security, and prosperity. CBP Agents and Officers protect our national security and borders through coordinated border security, customs, immigration, and agricultural law enforcement activities.
Our ASF teams thrive in a culture of constant innovation and we are seeking individuals who can bring forward-leaning, creative ideas to solve complex technical and procedural problems. The ability to quickly adapt and work constructively across a technically diverse and geographically separated team is crucial.
What you’ll get to do:
Help shape the future of how some of the nation’s most complex and challenging data are used to enhance national security and protect the lives of Agents, Officers, and those entering the country.
Provide high-level architectural guidance, technical innovation, and analytical expertise to teams across CBP.
Champion all aspects of data architecture, analytic methodology, and visualization functions including engineering standards, governance processes, and providing vision and context to explore and effectively utilize emerging technologies.
Serve as a data and technology expert across a broad and diverse set of mission critical applications.
Actively participate in Agile Scrum sprint planning, artifact creation, sprint testing, regression testing, demonstrations, retrospectives and solution releases.
Analyze project-related problems and create innovative solutions involving technology, analytic methodologies, and advanced solution components.
You have:
Must be a U.S. Citizen with the ability to pass CBP background investigation, criteria will include but not limited to:
3-year check for felony convictions
1-year check for illegal drug use
1-year check for misconduct such as theft or fraud
5+ years of professional and/or academic experience working on complex data challenges in the areas of data architecture and engineering, data science, data analysis and visualization, machine learning, artificial intelligence, or related discipline.
Strong software development background using Agile or DevOps methods and deep familiarity with cloud-native technologies.
College degree (B.S.) in Mathematics, Statistics, Computer Science, Software Engineering or a related discipline. (Industry and/or Academic experience is acceptable in lieu of degree.)
You are good at:
Working on unusually complicated problems and providing solutions that are highly creative and ingenious, exhibiting creativity and resourcefulness.
Helping customers and mission users conceptualize and prioritize their data analysis and visualization needs.
Evaluating existing data sets and reporting architectures to identify strategic gaps and apply modern technologies to creatively achieve superior mission outcomes.
Applying your skills in development languages like Python and R to create or augment business and operational intelligence tools to detect trends, patterns, and non-obvious relationships in large, complex, and disparate data sets.
Collaborating with teams across the organization to help prioritize competing analytic needs and incrementally develop new and optimized capabilities.
Leading across teams and organizations as a responsible, team-oriented individual with outstanding written and verbal communication skills and work ethic.
Bonus would be having:
5+ years of DHS, DoD, or IC experience working in complex data environments, including the architecture and optimization of data schemas, petabyte-scale ETL, etc.
5+ years of experience applying a range of analytical techniques including statistical, geospatial, link, temporal, and predictive analysis, for DHS, DoD, or IC agencies.
3-5 years of experience building and implementing artificial intelligence, neural networks, deep learning, or machine learning capabilities in software applications in a national security or academic environment.
Experience in Continuous integration, Continuous Deployment (CI/CD) and DevOps processes and tools
Experience implementing or migrating to Cloud environments like Amazon Web Services (AWS) or Microsoft Azure.
Previous experience as an Enterprise-level Data Architect, Data Engineer, Data Scientist, or Data Analyst.
Ability to apply advanced principles, theories, and concepts, and contribute to the development of innovative principles and ideas.
BEAGLE
What We Can Offer You:
We’ve been named a Best Place to Work by the Washington Post.
Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives.
We offer competitive benefits and learning and development opportunities.
We are mission-oriented and ever vigilant in aligning our solutions with the nation’s highest priorities.
For over 55 years, the principles of CACI’s unique, character-based culture have been the driving force behind our success.
Job Location
US-Springfield-VA-VIRGINIA SUBURBAN


CACI employs a diverse range of talent to create an environment that fuels innovation and fosters continuous improvement and success. At CACI, you will have the opportunity to make an immediate impact by providing information solutions and services in support of national security missions and government transformation for Intelligence, Defense, and Federal Civilian customers. CACI is proud to provide dynamic careers for employees worldwide. CACI is an Equal Opportunity Employer - Females/Minorities/Protected Veterans/Individuals with Disabilities."
104,Sr. Data Engineer,"Bethesda, MD 20817",Bethesda,MD,20817,None Found,None Found,"5 to 7+ years of experience in using SQL and data warehousing in a business environment5+ years of experience in custom ETL design, implementation, and maintenance5+ years of experience with data warehouse schema design and data modelingProduction level experience with Python, Java, SQL, and shell scriptingExperience with cloud databases and managed servicesExperience with batch and stream processingExperience with microservice patterns, API developmentExperience with building large scale data processing systemSolid understanding of data design patterns and best practicesWorking knowledge of data visualization tools such as Tableau, PowerBI is a plusExperience to analyze data to identify deliverables, gaps, and inconsistenciesFamiliarity with agile software development practices and drive to ship quicklyExperience leading change, taking initiative, and driving resultsEffective communication skills and strong problem-solving skillsProven ability and desire to mentor others in a team environmentRetail experience highly desired",None Found,"5 to 7+ years of experience in using SQL and data warehousing in a business environment5+ years of experience in custom ETL design, implementation, and maintenance5+ years of experience with data warehouse schema design and data modelingProduction level experience with Python, Java, SQL, and shell scriptingExperience with cloud databases and managed servicesExperience with batch and stream processingExperience with microservice patterns, API developmentExperience with building large scale data processing systemSolid understanding of data design patterns and best practicesWorking knowledge of data visualization tools such as Tableau, PowerBI is a plusExperience to analyze data to identify deliverables, gaps, and inconsistenciesFamiliarity with agile software development practices and drive to ship quicklyExperience leading change, taking initiative, and driving resultsEffective communication skills and strong problem-solving skillsProven ability and desire to mentor others in a team environmentRetail experience highly desired",None Found,"Sr. Data Engineer

Bethesda, MD

Total Wine & More is seeking a Senior Data Engineer with expertise in cloud solutions using core data warehousing tools and big data related technologies to join our information technology team. We are embarking on a significant initiative to transform all aspects of data management services within Total Wine and More including enterprise data architecture, business intelligence and data warehousing leveraging the cloud to apply advanced analytics and data science capabilities longer term. The Senior Data Engineer will play a significant role in the implementation, maintenance and continuous improvement of these systems and processes to achieve business needs. This individual will work closely with the business, software development and support teams, infrastructure, and security staff. The Total Wine & More Information Technology team is growing and we are driving business value through technology across all aspects of our business.

KEY RESPONSIBILITIES
Be versed in cloud solutions (AWS, Azure, or GCP), architecture, related technologies and their interdependenciesResearch, analyze, recommend and select technical approaches for solving difficult and challenging development and integration problemsExperience with setting up and operating data pipelines and data wrangling procedures using Python, SQL, and cloud managed servicesCollaborate with engineers and business customers to understand data needs, capture requirements and deliver complete data solutionsDesign and build data extraction, transformation, and loading processes by writing custom data pipelinesDesign, implement and support a platform that can provide ad-hoc access to large datasets and unstructured dataModel data and metadata to support adhoc and pre-built reportingTune application and query performance using performance profiling tools and SQLBuild data expertise and own data quality for allocated areas of ownership

JOB REQUIREMENTS

Minimum Experience, Skills and Education:
5 to 7+ years of experience in using SQL and data warehousing in a business environment5+ years of experience in custom ETL design, implementation, and maintenance5+ years of experience with data warehouse schema design and data modelingProduction level experience with Python, Java, SQL, and shell scriptingExperience with cloud databases and managed servicesExperience with batch and stream processingExperience with microservice patterns, API developmentExperience with building large scale data processing systemSolid understanding of data design patterns and best practicesWorking knowledge of data visualization tools such as Tableau, PowerBI is a plusExperience to analyze data to identify deliverables, gaps, and inconsistenciesFamiliarity with agile software development practices and drive to ship quicklyExperience leading change, taking initiative, and driving resultsEffective communication skills and strong problem-solving skillsProven ability and desire to mentor others in a team environmentRetail experience highly desired

Preferred Experience, Skills and Education:
Bachelor's degree from four-year College or university in Computer Science, Technology or related field

PHYSICAL REQUIREMENTS (with or without accommodations):
n/a
We offer
Paid Time Off (PTO)
Generous store discounts
Health care plans (medical, prescription, dental, vision)
401(k), HSA, FSA, Pre-tax commuter benefits
Disability & life insurance coverage
Paid parental leave
College tuition assistance
Career development & product training
Consumer classes
& More!
Grow with us
Total Wine & More is the country’s largest independent retailer of fine wine, beer and spirits, and we continue to grow our footprint year over year. Total Wine offers exciting and unique career opportunities across the country and in our corporate office. Our strength is our people. We have a commitment to training and career growth, all in an environment that values new ideas and teamwork. If you share our entrepreneurial spirit and a passion for providing best-in-class customer experience, take a moment to apply or learn more at www.TotalWine.com/About-Us/Careers!
Total Wine & More considers several factors when establishing compensation. Estimated salaries determined by third parties have not been validated by Total Wine & More.

Total Wine & More is an equal opportunity employer and all qualified applicants will receive consideration for employment without discrimination based on race, color, religion, national origin, sex, sexual orientation, age, marital status, veteran status, disability, or any other characteristic protected by applicable law. Total Wine & More makes reasonable accommodations during all aspects of the employment process, including during the interview process. Total Wine & More is a Drug Free Workplace.

The information provided above indicates the general nature and level of work required of the position and is not a comprehensive list of all responsibilities or qualifications. Benefits list is only a highlight of some of the benefits offered to team members; eligibility for certain benefits apply."
105,Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,"
At least one (1) year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models
At least two (2) years of experience in Scala, Python, Apache Spark and SQL
Experience working directly with relational database structures and flat files
Ability to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices
Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations.
Good verbal and written communication skills, with both technical and non-technical stakeholders",None Found,"
Build pipelines to ingest and maintain complex data sets into Cerebri AI’s proprietary data stores for use in machine learning modeling
Develop and maintain data ontologies for key market segments
Collaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data
Collaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data
Write quality documentation on the discovery process and software projects
Work equally well in a team environment and on your own.
Communicate complex ideas clearly with both team members and clients
Travel up to 25%",None Found,None Found,"Design, develop and build out data pipelines to ingest data into our proprietary data structures, and be a key collaborator in the data discovery and exploratory analysis process during our client engagements.
Responsibilities
Build pipelines to ingest and maintain complex data sets into Cerebri AI’s proprietary data stores for use in machine learning modeling
Develop and maintain data ontologies for key market segments
Collaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data
Collaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data
Write quality documentation on the discovery process and software projects
Work equally well in a team environment and on your own.
Communicate complex ideas clearly with both team members and clients
Travel up to 25%
Qualifications
At least one (1) year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models
At least two (2) years of experience in Scala, Python, Apache Spark and SQL
Experience working directly with relational database structures and flat files
Ability to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices
Knowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations.
Good verbal and written communication skills, with both technical and non-technical stakeholders
Nice to Haves
Experience in Java and/or Scala
Experience with data management processing tools such as Kafka, Elasticsearch and Logstash
Experience with NoSQL distributed databases such as Cassandra.
Experience in business intelligence visualization tools such as Grafana, Superset, Redash or Tableau.
Experience with Microsoft Azure or similar cloud computing solutions
Master’s degree or higher in a relevant quantitative subject"
106,Sr. Oracle Data Engineer,"Chantilly, VA",Chantilly,VA,None Found,None Found,None Found,"
5+ years of advanced knowledge of PL/SQL development
5+ years of Data Modeling
5+ years of Data Warehousing
5+ years of Oracle Database storage and schema design
1+ years of Salesforce Experience","
Must have significant development experience with PL/SQL.
Design and develop a data mart within an Oracle environment.
Experience in mortgage banking or home loans a plus.
Design and develop a data mart within an Oracle environment","
Bachelor's degree is required",None Found,"Sr. Oracle Data Engineer
Job location: Chantilly, VA
Overview:
NikSoft Systems Corporation is a recognized Information Technology solutions provider. Founded in 1998 and based in Reston, Virginia, NikSoft is a CMMI Level 3 Certified company with an established reputation for excellence and on-time delivery with a consistently high customer satisfaction rating from its Federal Government and private consulting contracts.
NikSoft is currently conducting a search for a Sr. Oracle Data Engineer to support our government customer located in Chantilly, VA.
Responsibilities:
Must have significant development experience with PL/SQL.
Design and develop a data mart within an Oracle environment.
Experience in mortgage banking or home loans a plus.
Design and develop a data mart within an Oracle environment

Technical Skills required:
5+ years of advanced knowledge of PL/SQL development
5+ years of Data Modeling
5+ years of Data Warehousing
5+ years of Oracle Database storage and schema design
1+ years of Salesforce Experience
Preferred Skills:
Mortgage banking industry knowledge
DevOps concepts
Agile Development Methods
Data Quality Management and Testing
Requirements Analysis
Education level:
Bachelor's degree is required
***Candidates must be able to obtain Public Trust Clearance US Citizenship or Green Card Holder or H1. Additionally, candidates must not have traveled outside of the USA for a combined period not to exceed 6 months within the last 5 years. ***
NikSoft's competitive benefits program includes comprehensive medical and dental care, matching 401K, paid time off, flexible spending accounts, disability coverage, and other benefits that help provide financial protection for you and your family.
BENEFITS:
NikSoft's competitive benefits program includes comprehensive medical and dental care, matching 401K, paid time off, flexible spending accounts, disability coverage, and other benefits that help provide financial protection for you and your family.
NikSoft Systems Corp is fully committed to the concept and practice of equal opportunity and affirmative action in all aspects of employment. NikSoft is an EOE M/F/Disability/Veteran employer. For more information about our other openings, please visit www.niksoft.com"
107,Software Engineer/Data Engineer,"College Park, MD 20740",College Park,MD,20740,None Found,"
Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.
Familiarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured (Postgres, MySQL) and unstructured data sources (Elastic, Kafka, and the Hadoop ecosystem) as implemented at Internet-scale.
Experience writing and optimizing streaming and batch analytics.
Experience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.
BS/BA in Computer Science, Engineering, or relevant field experience.",None Found,None Found,None Found,None Found,"Ideally, the successful candidate will be located near our NYC or College Park, MD office. However, there is the opportunity to work remotely based on role and level.
Software Engineer/Data Engineer
BlueVoyant is seeking a Software Engineer/Data Engineer to help us build a data analytics platform powerful enough to protect some of the world's biggest networks, and nimble enough to adapt to a quickly evolving product vision. We are solving interesting, exciting, and important problems with smart people.
Qualifications for the Software Engineer/Data Engineer:
Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.
Familiarity with large-scale, big data, and streaming data technologies, as well as exposure to a variety of structured (Postgres, MySQL) and unstructured data sources (Elastic, Kafka, and the Hadoop ecosystem) as implemented at Internet-scale.
Experience writing and optimizing streaming and batch analytics.
Experience with Agile frameworks, secure software design, test-driven development, and modern, container-delivered code deployment in a cloud-based DevOps environment.
BS/BA in Computer Science, Engineering, or relevant field experience.
What you will do as a Software Engineer/Data Engineer:
Work closely with analysts to transform threat analytics into production-level code.
Actively contribute to application architecture and product vision.
Participate in requirements gathering and transformation from prototype to product design.
Participate in daily development stand-up meetings and regular sprint planning and product demo meetings.
Help us stay current on the latest data processing tools and trends.
Ideal candidates will:
Thrive in our small, fast-paced, product-driven environment
Collaborate with teams from across the organization
Deliver features and fixes on tight schedules and under pressure
Present ideas in business-friendly and user-friendly language
Create systems that are maintainable, flexible and scalable
Define and follow a disciplined development and engineering workflow
Demonstrate ownership of tasks with escalation as needed
Be a subject matter expert in one or more of the technologies employed
Relentlessly push for successful customer outcomes
Possess a strong interest or background in cyber security
General responsibilities include:
Participate in all stages of an agile software development lifecycle, including product ideation, requirements gathering, architecture, design, implementation, testing, documentation, and support
Refine our software development methodology based on agile/lean practices with continuous feedback and well-defined metrics to drive improvement
Maintain up-to-date knowledge of technology standards, industry trends, emerging technologies, and software development best practices
Ensure technical issues are quickly resolved and help implement strategies and solutions to reduce the likelihood of reoccurrence
Identify competitive offerings and opportunities for innovation including assessments of risk/reward to the company.
About BlueVoyant
BlueVoyant is a global cybersecurity firm that provides Advanced Threat Intelligence, for large companies and a comprehensive Managed Security Service and Professional Services for small businesses, powered by one of the largest commercially available cyber threat databases in the world.
By working with BlueVoyant, companies can gain unique and far-reaching visibility into malicious activity on their networks, in the dark web and across the internet, as well as real-time, automatable remediation services. Through our unique real-time external threat monitoring, predictive human and machine-sourced intelligence, and proactive managed security and incident response, BlueVoyant offers the private sector exceptional cyber defense capabilities.
Co-founded by CEO Jim Rosenthal, former Chief Operating Officer at Morgan Stanley, and Executive Chairman Tom Glocer, former Chief Executive Officer at Thomson Reuters, BlueVoyant has attracted a management team that comes from the world's preeminent intelligence, law enforcement, and private sector organizations. Other leaders include:
Jim Penrose, COO, former EVP at Darktrace with 17 years at the NSA in key leadership roles.
Gad Goldstein, Head of BlueVoyant Europe and Chairman of Israel, former division head (Major General equivalent) in the Israel Security Agency, Shin Bet.
Robert Hannigan, Chairman of BlueVoyant International, former Director of GCHQ.
Austin Berglas, Global Head of Professional Services, former head of the FBI's New York Cyber Branch.
David Etue, Global Head of MSS, former VP of Managed Services at Rapid7.
Milan Patel, Chief Client Officer, former CTO of the FBI Cyber Division.
Ron Feler, Global Head of Threat Intelligence and Operations, former Deputy Commander of Unit 8200, the cybersecurity division of the Israel Defense Forces.
Eldad Chai, CPO, former SVP of Products at Imperva.
Jim Bieda, Senior Advisor, former NSA Deputy CTO.
Bill Crumm, Senior Advisor, former NSA SIGINT Director and former Cybersecurity Head, Morgan Stanley.
Dan Ennis, Senior Advisor, former Head of Threat Intelligence at the NSA
All employees must be authorized to work in the United States or Israel. BlueVoyant provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, BlueVoyant complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.
Come work with us!
BlueVoyant is hiring software developers, infrastructure engineers, data science experts, and technologists of all types to build next generation predictive threat intelligence and advanced security monitoring solutions.
Projects currently in development include:
An Internet-scale (multi-PB, > 500k TPS) repository made up of unstructured, structured, and semi-structured data sources used for real time alerting, threat analysis, and research and development internally.
A powerful, enterprise-scale suite of products used to provide managed security services and Security Operations Center (SOC) functionality to small and medium sized enterprises around the globe.
A unique internal platform to support the data research needs of our analysts and SOC so they can quickly and effectively identify new threat actors and techniques.
bfMhwiZhmx"
108,Data Engineer (Secret Clearance),"Fort Belvoir, VA",Fort Belvoir,VA,None Found,None Found,"Bachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with a federal government customer.
Experience with big data tools: Hadoop, Spark, Kafka
Experience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB
Experience with data governance tools: Collibra, Immuta
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala
Must possess strong written and verbal communication skills.
DoD Secret Clearance Required",None Found,"Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.",None Found,None Found,"LMI is currently seeking a data engineer within LMI’s Advanced Analytics service line to support the design and implementation of business critical data management & engineering solutions.

*This position required an active DoD Secret Clearance*
Responsibilities
The ideal candidate will have direct, applied experience with one or more of the following areas:
Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.
Qualifications
Bachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with a federal government customer.
Experience with big data tools: Hadoop, Spark, Kafka
Experience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB
Experience with data governance tools: Collibra, Immuta
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala
Must possess strong written and verbal communication skills.
DoD Secret Clearance Required
#LI-SH1"
109,Data Engineer (Senior),"Reston, VA",Reston,VA,None Found,None Found,"
Master’s degree in engineering, computer science, or other related technical field or Bachelor’s degree in a business or management-related field accompanied by experience managing technical requirements in complex programs.
Experience displaying expert domain knowledge in a technical field.
Experience negotiating complex scenarios and challenges and devising courses of action to resolve situations with predictable outcomes.
Experience leading critical objectives where decision making is of utmost concern to the outcome.
Experience supervising others.
",None Found,None Found,None Found,None Found,"Byte Systems. LLC - Reston, VA, United States
MUST be a US Citizen with a U.S. Government clearance - TS/SCI with Polygraph

NOTE:Must have an active TS-SCI with poly. No sponsorships or upgrades are available. Submissions without this requirement will not be considered. H1-B holders will not be considered.

Description

This role performs data management tasks. This includes:

data design
data structure identification
database schema definition
data access privilege management
data query optimization
documentation
database instance upgrades
long-range requirements
operational guidelines
data protection.

Duties also include:

entering and reviewing data within the database
ensuring user data integrity
maintaining database support tools
database tables and dictionaries
recovery and back-up procedures
making recommendations regarding enhancements and/or improvements.

This role ensures high quality data is available for use by other team members. This role designs, implements, and maintains standard data interfaces for data ingest including Extract/Transform/Load (ETL) methodology and implementation, APIs, RESTful Web Services, data quality, and data cleansing. This role collaborates across a team of Data Scientists, Data Engineers, and Data Administrators to produce work products.

This role designs and develops methods, processes, and systems to consolidate and analyze structured and unstructured, diverse sources including “big data” sources. This role develops and uses advanced software programs, algorithms, query techniques, models complex business problems, and automated processes to cleanse, integrate, and evaluate datasets. This role analyzes the requirements and evaluates technologies for data science capabilities including Natural Language Processing, Machine Learning, predictive modeling, statistical analysis and hypothesis testing. Works with cross-discipline teams to ensure connectivity between various data sources and business problems. Identifies meaningful insights and interprets and communicates findings and recommendations. This role develops information tools, algorithms, dashboards, and queries to monitor and improve business performance. Maintains awareness of emerging analytics and big-data technologies.

Basic Qualifications:

Master’s degree in engineering, computer science, or other related technical field or Bachelor’s degree in a business or management-related field accompanied by experience managing technical requirements in complex programs.
Experience displaying expert domain knowledge in a technical field.
Experience negotiating complex scenarios and challenges and devising courses of action to resolve situations with predictable outcomes.
Experience leading critical objectives where decision making is of utmost concern to the outcome.
Experience supervising others.

Candidate must have BS and 15+years of prior relevant experience or Masters and 13+ years of prior relevant experience.

Benefits:
5 weeks paid vacation + 10 gov't holidays
15% contribution to 401k
ISP and cellphone reimbursement
LTD, STD disability and life insurance
Paid health, dental, and vision for employee and family.
$5000 annual training expense reimbursement
Computer purchase plan

Posted On: Tuesday, July 23, 2019"
110,Senior Data Engineer,"Washington, DC 20036",Washington,DC,20036,None Found,None Found,None Found,None Found,None Found,None Found,"As a Senior Data Engineer at Compass, you will be responsible for helping to build the data-driven decision-making culture throughout the organization. You'll work as part of a rapidly growing team in a fast-paced environment. You will be responsible for designing and managing large-scale, real-time analytical systems that impact multiple functions and teams across the organization. In this high impact role you will have an opportunity to work with emerging technologies, while driving analytical solutions end-to-end. You will empower the development of data-powered product features, real-time analytics and artificial intelligence. You are someone who loves data and analytics, strives to constantly learn new and fast-developing data technologies, and demonstrate passion about shipping high-quality software.

At Compass You Will:

Design, develop, and implement the infrastructure that elevates data-driven decision-making and machine learning for our proprietary real estate technology
Work with the enterprise business systems that facilitate end to end experience of real estate transactions

This position is responsible for:

Design and deliver flexible and scalable data solutions collecting process-level external and internal data and transforming it into enterprise wide data lake, denormalized data marts from which operational and process metrics and analytics can be reliably generated

What We're Looking For:

Bachelor's degree in Computer Science, Information Systems, a related field, or equivalent experience.
Extensive experience with a major cloud provider, for instance, AWS
4+ years of experience building data infrastructure using a state-of-the-art tool chain, including utilities such as Kafka, Airflow, Spark, Cassandra, etc.
4+ years of Data Warehousing experience, building ETL data pipes for populating dimensional marts
Familiarity with API design patterns (oAuth, tokens, JSON)
Familiarity with data encoding libraries, such as Avro, Protobuf, or Thrift
Experience with supporting Data Science and ML teams; experience supporting notebook environments for analytics/data science teams (Jupyter, Zeppelin, DataBricks)
5+ years of data intensive programming in your language of choice. Python, Scala, Clojure preferred.
Good understanding of relational databases and SQL
Familiarity with dimensional data modeling
Experience with version control, scalable code deployment (Git, Jenkins)
Highly productive developing and deploying in a Linux environment
Strong business communication skills
Strong drive to constantly learn and keep up to speed with the new data technologies

At Compass, our mission is to help everyone find their place in the world. This means we continually celebrate the diverse community different individuals cultivate. As an equal opportunity employer, we stay true to our mission by ensuring that our place can be anyone's place."
111,Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,"Candidate shall have demonstrated at least 5 years of experience managing and maintaining structured, semi-structured, and unstructured data, as well as structuring and wrangling data as appropriate for statistical data analysis.
Candidate shall possess the necessary skills to gather, define, document, analyze and implement ETL/data
Extensive experience in understanding data warehouse concepts and relational databases.
Candidate shall possess the necessary skills to model database structures, tables, and fields.
Candidate shall possess a basic understanding of statistics as related to their experiences in structuring data for statistical analyses.
Candidate shall possess excellent oral and written communication skills with emphasis on complex technical topics and effectively communicating details with all levels of management.
Candidate shall possess the necessary people skills to identify requirements and deliver results.
All personnel shall have at a minimum an active security clearance of Top Secret, meaning the clearance initially issued or revalidated within the past five years, at the time of the proposal submission and Single Scope Background Investigation.",None Found,None Found,None Found,None Found,"Job Description
In this role you will: Requires skilled, technically knowledgeable, and experienced support to develop new business intelligence reports, analytical capabilities and data warehouses; execute development for enhancements and maintenance of IT systems and environments; and execute special projects.

Required Qualifications
Candidate shall have demonstrated at least 5 years of experience managing and maintaining structured, semi-structured, and unstructured data, as well as structuring and wrangling data as appropriate for statistical data analysis.
Candidate shall possess the necessary skills to gather, define, document, analyze and implement ETL/data
Extensive experience in understanding data warehouse concepts and relational databases.
Candidate shall possess the necessary skills to model database structures, tables, and fields.
Candidate shall possess a basic understanding of statistics as related to their experiences in structuring data for statistical analyses.
Candidate shall possess excellent oral and written communication skills with emphasis on complex technical topics and effectively communicating details with all levels of management.
Candidate shall possess the necessary people skills to identify requirements and deliver results.
All personnel shall have at a minimum an active security clearance of Top Secret, meaning the clearance initially issued or revalidated within the past five years, at the time of the proposal submission and Single Scope Background Investigation.
Desired Qualifications
Prior experience using big data management techniques and tools (e.g., Hadoop) is preferred.
Salient CRGT is a leading provider of health, data analytics, cloud, agile software development, mobility, cyber security, and infrastructure solutions. We support these core capabilities with full lifecycle IT services and training—to help our customers meet critical goals for pivotal missions. We are purpose-built for IT transformation supporting federal civilian, defense, homeland, and intelligence agencies, as well as Fortune 1000 companies.

If you feel you are qualified for this position, express interest by clicking the Apply button below (if you are viewing this position on the Salient CRGT website). If you are viewing this job posting outside of the Salient CRGT website, please visit: www.salientcrgt.com/careers to express interest in this position through the Salient CRGT Careers page.
Salient CRGT is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, age, sex, sexual orientation, gender identity or expression, veteran status, disability, genetic information, or any other factor prohibited by applicable anti-discrimination laws."
112,"Data Engineer Intern, Data and Services (University Program - Summer 2020)","Arlington, VA",Arlington,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Who is Mastercard?
We are the global technology company behind the world’s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless ®. We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.
Job Title
Data Engineer Intern, Data and Services (University Program - Summer 2020)
Job Description Summary

********APPLICATION INSTRUCTIONS: Please attach your resume, transcript, and cover letter with your application in the resume upload tab. All three documents must be submitted. *********



Data Engineer Interns are fundamental to the success of our clients; you will be the bridge between raw client data and Mastercard's software. You will be responsible for:


Designing processes to extract, transform, and load (ETL) terabytes of client data into Mastercard's analytics platform using SQL and other technologies
Working across multiple client teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set.
Tackling big data problems across various industries, utilizing your creative thinking skills

Make an Impact as a Data Engineer Intern:

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies (such as Microsoft SQL Server & Business Intelligence Tools) and to explore a variety of directions
Flexibility to work on many new and challenging projects across a diversity of industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven management team
Collaborate with other Mastercard departments and focus on internal development, during which you will develop new tools and processes that will be used across Mastercard


Bring your passion and expertise

Understanding of relational databases and ETL Processes (preferably Microsoft SQL Server)
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language a plus (Powershell, .Net, Perl, Python, VB Script, C#)
Strong troubleshooting and problem solving capabilities
Demonstrated analytical/quantitative skills
Working towards a Bachelor's degree with an established history of academic success

Mastercard Worldwide is an Equal Employment Opportunity Employer and does not discriminate in employment on the basis of age, race, color, gender, national origin, disability, veteran status, or any other basis that is prohibited by applicable law.
Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.
If you require accommodations or assistance to complete the online application process, please contact reasonable.accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly."
113,Data Engineer,"Reston, VA",Reston,VA,None Found,None Found,"
Understanding the basics of distributed systems
Knowledge of algorithms and data structures
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases (PostgreSQL, MySQL, etc.).
4+ Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc
Demonstrable experience with Kafka or Hadoop
Experience with data visualization tools like Tableau and/or ElasticSearch will be a big plus
Experience with AWS cloud services: EC2, EMR, RDS, Redshift","
Understanding the basics of distributed systems
Knowledge of algorithms and data structures
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases (PostgreSQL, MySQL, etc.).
4+ Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc
Demonstrable experience with Kafka or Hadoop
Experience with data visualization tools like Tableau and/or ElasticSearch will be a big plus
Experience with AWS cloud services: EC2, EMR, RDS, Redshift","
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.",None Found,None Found,"Overview
RSEKURE transforms the way our clients do business! Today’s Government operations demand the rapid collection, fusion and transmission of information at unprecedented speed, capacity, availability, precision and security. This constant and uncompromising posture must be maintained across all platforms and in the most challenging environments around the world. This level of operation is the one we at RSekure look forward to supporting day in day out as our mission priority. Mission Success. Done.
Responsibilities
RSekure is looking for a Data Engineer who has experience working in an Agile program. Candidate should have expertise managing data workflows, pipelines, and ETL processes. In addition, he or she should have build experience of AWS cloud environments and supplying rapid delivery solutions using tools like AWS Cloud formation, Terraform, CLI and scripting, as well as, GitHub or Jira tools to manage daily workflow. This position will be supporting our customer in the Tyson's Corner area.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
Performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Qualifications
Required Skills:
Understanding the basics of distributed systems
Knowledge of algorithms and data structures
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases (PostgreSQL, MySQL, etc.).
4+ Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc
Demonstrable experience with Kafka or Hadoop
Experience with data visualization tools like Tableau and/or ElasticSearch will be a big plus
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Security Clearance: Must have Active Top Ssecret SCI with Polygraph"
114,Data Engineer I,"Arlington, VA",Arlington,VA,None Found,None Found,"
Bachelor's Degree in Computer Science or equivalent work experience
2-4 years of experience in Software Development
Strong Python skills with experience in Java/C++ or other object oriented language.
Experience with two or more of the following:
Code optimization
Data structures
Elasticsearch
SQL
Linux / Ubuntu
JSON / YAML
AWS SQS, S3, EC2
Compilers/Parsers
Docker
",None Found,"
Assume responsibility for key parts of the Interos Knowledge Graph product
Translate Data Science experimental code into tested, production ready modules
Create connectors for ingesting data from third party databases, APIs, and web sites
Optimize data structures, improve code execution, and reduce memory consumption of existing implementation
Develop JSON APIs
",None Found,None Found,"At Interos, we are disrupting the way Fortune 500 companies and government agencies identify and respond to risk within their supply chains – whether this risk is being caused by cyber breaches, geopolitical issues, malicious intent, quality concerns, natural disasters or unethical sourcing and sustainability concerns. We deliver the data and insights to business leaders that help them identify, visualize and understand the ripple effects that could impact their supply chains, before they happen.

Recently funded by Kleiner Perkins and pivoting to an automated solution, Interos is in essence, a start-up SaaS environment. We need someone who thrives as part of fast-paced team and takes pride in their attention to detail, their ability to take on many different things.

The Opportunity:
Join a team of highly skilled engineers working to apply the latest developments in AI and Machine Learning to solving real world problems for our customers. Work in a collaborative, Agile environment, where every voice matters, outside the box thinking is encouraged, and the best ideas always win.

We are looking for a strong engineer with experience building applications from the ground up. We need someone who can work independently but can communicate clearly and knows when to ask questions and when to challenge assumptions.

Key Responsibilities:

Assume responsibility for key parts of the Interos Knowledge Graph product
Translate Data Science experimental code into tested, production ready modules
Create connectors for ingesting data from third party databases, APIs, and web sites
Optimize data structures, improve code execution, and reduce memory consumption of existing implementation
Develop JSON APIs

Qualifications:

Bachelor's Degree in Computer Science or equivalent work experience
2-4 years of experience in Software Development
Strong Python skills with experience in Java/C++ or other object oriented language.
Experience with two or more of the following:
Code optimization
Data structures
Elasticsearch
SQL
Linux / Ubuntu
JSON / YAML
AWS SQS, S3, EC2
Compilers/Parsers
Docker

BENEFITS:

Comprehensive Health & Wellness package (Medical, Dental and Vision)
10 Paid Holiday Days Off
Accrued Paid Time Off (PTO)
401 (k) Employer Matching
Stock Options
Career advancement opportunities
Casual Dress
Hackathons
On-site gym and dedicated Peloton room at headquarters
Company Events (Sports Games, Fitness Competitions, Birthday Celebrations, Contests, Happy Hours)
Annual company party
Employee Referral Program

Interos is proud to be an Equal Opportunity Employer and will consider all qualified applicants without regard to race, color, age, religion, sex, sexual orientation, gender identity, genetic information, national origin, disability, protected veteran status or any other classification protected by law.

If you are a candidate in need of assistance or an accommodation in the application process, please contact HR@interos.net ( HR@interos.net )"
115,Senior Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,"
Work in a fast-paced agile environment
Building efficient storage for structured and unstructured data
Developing and deploying distributed computing Big Data applications using Frameworks like Apache Spark, Presto, Apex, Kafka on AWS Cloud using EMR and/or Amazon Athena
Utilizing programming languages like Java, Scala, Python, and Cloud-based services such as Athena and Glue
Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of working code utilizing tools like Jenkins, Maven, Terraform, Git, BitBucket and Docker
Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Write, design, code, test, implement, debug, and validate applications; document design decisions and develop modular software components; monitor system performance metrics, and identify potential risks/issues
Collaborate in agile scrum team with product owners and fellow software engineers to deliver upon most important business and technical priorities
Provide active mentorship/guidance to fellow members of the agile tech team and participate in internal and external technology conference & communities",None Found,None Found,"Spotinst’s Senior Data Engineer will be responsible for designing, expanding and optimizing our data infrastructure architecture. Our Data Engineer will work closely with our software developers and data analysts to provide creative solutions for data-related problems.
Responsibilities:
Work in a fast-paced agile environment
Building efficient storage for structured and unstructured data
Developing and deploying distributed computing Big Data applications using Frameworks like Apache Spark, Presto, Apex, Kafka on AWS Cloud using EMR and/or Amazon Athena
Utilizing programming languages like Java, Scala, Python, and Cloud-based services such as Athena and Glue
Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of working code utilizing tools like Jenkins, Maven, Terraform, Git, BitBucket and Docker
Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Write, design, code, test, implement, debug, and validate applications; document design decisions and develop modular software components; monitor system performance metrics, and identify potential risks/issues
Collaborate in agile scrum team with product owners and fellow software engineers to deliver upon most important business and technical priorities
Provide active mentorship/guidance to fellow members of the agile tech team and participate in internal and external technology conference & communities
Requirements:
BA/ MA Degree or equivalent work experience
3+ years of work experience in data warehousing and analytics
At least 3 years of ETL design, development, and implementation experience
2+ years of Python development experience
2+ years of experience with the Hadoop Stack
At least 2+ years of experience with Cloud computing (AWS)
4+ years' experience with SQL (PostgreSQL, Presto, Athena, Redshift)
4+ years of UNIX/Linux experience"
116,Master Data Engineer/Software Engineer -POLY REQUIRED,"Reston, VA 20190",Reston,VA,20190,None Found,None Found,None Found,None Found,None Found,None Found,"Description
Job Requisition:
Master Data Engineer/Software Engineer -POLY REQUIRED
Job Description:
Why wake up every day and want more when YOU CAN HAVE IT? Do you love KNOWING at the end of each day that your work made a difference? Have you ever wondered what it would be like to work with one of the BEST in government contracting? Ranked by Forbes in 2018 as a Top 100 government contractor and one of the World’s Most Ethical Companies, Team Leidos is one you don’t want to overlook.
Everything we do is built on a commitment to do the right thing for our customers, our people, and our community. We embrace and solve some of the world's toughest challenges. We’re focused on ensuring our intelligence customers have the right tools, technologies, and tactics to keep pace with an ever-evolving threat landscape and succeed in their mission to protect people and critical assets around the world. Who wouldn’t be fulfilled being part of that every day? We know once you join Team Leidos, you are sure to go home at the end of every day knowing YOU MADE A DIFFERENCE, resulting in one of the most REWARDING careers you could have imagined. WE WANT YOU!!
This role uses Data Engineering, Data Science and Software Engineer techniques to satisfy customer requirements in processing, transforming and enriching large scale datasets. This role designs, develops, tests and maintains processing pipelines, web services and APIs to condition, enhance, model and apply domain-specific logic to data for use by API consumers and Mission Applications. This role deals with a wide variety of raw data and data processed at different levels using Python or Java and associated libraries or frameworks: Spark, PySpark, Spark Streaming, DataFrames, Flask, Scikit-Learn, Parquet, Numpy, Pandas, etc.) that utilize, collect, aggregate and clean large data sets to implement services perform analysis, extraction, and enrichment for persistence in various data stores. This role collaborates across a team of Data Engineers, Data Scientists, Administrators, and Software Engineers to produce work products using a CI/CD workflow in an Agile development process.
POSITION RESPONSIBILITIES:
This role developed methodologies to identify, ingest, and process various data.
This role develops and employs a number of languages and tools to pair systems to ingest new data.
This role supports objectives and tasks by leveraging numerous tools, techniques and infrastructures to include Elasticsearch, Spark, AWS/S3, MapReduce, HBase, Oozie, and HDFS
Responsible for building, orchestrating and running data migrations across different datastores/databases and environments, for example, data migration from an on-premises Elasticsearch cluster to a cloud hosted Elasticsearch cluster.
Responsible for integrating with authentication/authorization and entitlement services
Responsible for building solutions that respect customer data policy, retention, compliance, governance and handling requirements
Coordinates across various internal teams to ensure linkage between numerous data sources
Develop and leverage algorithms, analytical techniques and tools to enhance the stakeholders access and understanding of data sets.
MINIMUM QUALIFICATIONS:
Degree in engineering, computer science, or other related technical field or degree in a business or management-related field accompanied by experience managing technical requirements in complex programs.
In-depth experience with Python, Java and data processing, manipulation or querying (SQL or NoSQL)
In-depth experience with Spark/PySpark, MapReduce or other large scale distributed processing frameworks.
In-depth experience negotiating complex scenarios and challenges and devising courses of action to resolve situations with predictable outcomes.
Extensive experience leading critical objectives where decision making is of utmost concern to the outcome.
Experience supervising others.
Candidate must have Bachelors with 12-15 years of prior relevant experience or Masters with 10-13 years of prior relevant experience.
Candidate must have an active TS/SCI with polygraph
DESIRED QUALIFICATIONS:
Experience with tools like Git, Jenkins, Unix bash scripting
Experience developing in or deploying to AWS
""External Referral Eligible""
NSOINT
External Referral Bonus:
Eligible
Potential for Telework:
No
Clearance Level Required:
Top Secret/SCI with Polygraph
Travel:
No
Scheduled Weekly Hours:
40
Shift:
Day
Requisition Category:
Professional
Job Family:
Data Scientist
2000
Intelligence
Leidos is a Fortune 500® information technology, engineering, and science solutions and services leader working to solve the world's toughest challenges in the defense, intelligence, homeland security, civil, and health markets. The company's 33,000 employees support vital missions for government and commercial customers. Headquartered in Reston, Virginia, Leidos reported annual revenues of approximately $10.19 billion for the fiscal year ended December 28, 2018. For more information, visit www.Leidos.com.
Pay and benefits are fundamental to any career decision. That's why we craft compensation packages that reflect the importance of the work we do for our customers. Employment benefits include competitive compensation, Health and Wellness programs, Income Protection, Paid Leave and Retirement. More details are available here.
Leidos will never ask you to provide payment-related information at any part of the employment application process. And Leidos will communicate with you only through emails that are sent from a Leidos.com email address. If you receive an email purporting to be from Leidos that asks for payment-related information or any other personal information, please report the email to spam.leidos@leidos.com.
All qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law. Leidos will also consider for employment qualified applicants with criminal histories consistent with relevant laws."
117,Google Cloud Architect,"Columbia, MD 21046",Columbia,MD,21046,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description

Merkle is looking for experienced Google Cloud professional to be part of its Cloud practice. You will be working on industry leading Google Cloud Platform (GCP) to design, develop, and implement next generation Marketing solutions leveraging Cloud native and Commercial/Open Source Big Data technologies.

Key Responsibilities
As a Senior Google Cloud Architect, you will
Be the force behind shaping the future of our Cloud strategy practice by leading innovation efforts and developing Intellectual Property
Be client facing in advising them on GCP optimized architectures for data management, help develop roadmaps for on-prem to Cloud migration efforts
Be hands-on in developing prototypes and conducting Proof of Concepts
Act as a trusted advisor to client on all things GCP
Lead team members on project involving data migration and implementation of new solutions on cloud platforms
Participate in crafting customer facing proposal responses
Participate in technical interactions with client’s executives and senior management
Mentor client/Merkle resources on GCP and its services
Evaluate and analyze new commercial/open-source services/technologies offered by public Cloud providers, and Big Data technology vendors
Interface with Cloud/Technology provider’s Product Managers, and Engineers
Develop reference architectures and design pattern library for typical Cloud based Marketing solutions implementations
Participate in creating new services capabilities, productized solution offerings and document implementation handbooks
Advise on Cloud project set up, security and role based access implementation, and network optimizations

Qualifications

Key Skills and Experience
Be hands-on
Understanding of Google cloud computing technologies, business drivers, and emerging computing trends
Certified Google Cloud Data Engineer
Minimum 10 years experience in data warehouse/data engineering with strong hands-on experience in ETL
Proven track record of building technical and business relationships with senior executives
Proven track record of driving decisions collaboratively, resolving conflicts and ensuring follow up
Problem-solving mentality leveraging internal and/or external resources, where and when needed, to do what’s right for the customer
Exceptional verbal and written communication
Ability to coordinate across geographically dispersed team members and consolidate status for stakeholders
Ability to connect technology with measurable business value
Demonstrated technical thought leadership in customer facing situations

Requirements:
Certified Google Cloud Data Engineer
5+ years of technical management and though leadership role delivering success in complex data analytics environment, managing various stakeholders relationships to get consensus on solution
5+ year’s design and/or implementation of highly distributed applications
2+ years’ experience in “migrating” on premise workloads to one or more industry leading public cloud(s)
Demonstrated experience of designing and building Big Data solutions on GCP stack leveraging BigQuery, Dataproc, Dataflow, BigTable, Data Prep, etc.
5+ years experience using Python for building data pipelines
Technical architectural and development experience on Massively Parallel Processing technologies, such as Hadoop, Spark, Teradata, Netezza, Hadoop
Technical architectural and development experience on one or more Data Integration technologies, such as Ab Initio, Talend, Pentaho, Informatica, Data Stage, Map Reduce
Technical architectural experience on Data Visualization technologies, such as Tableau, Qlik, etc.
Technical architectural experience on data modeling, designing data structures for business reporting
Deep understanding of Advanced Analytics, such as predictive, prescriptive, etc.
Working knowledge of cloud components: Software design and development, Systems Operations / Management, Database architecture, Virtualization, IP Networking, Storage, IT Security
Technical prowess and passion-especially for public Cloud, modern Application design practices and principles. Certifications on Cloud Platform preferred.
Oversight experience on major transformation projects and successful transitions to operations support teams
Presentation skills with a high degree of comfort to both large and small audiences
Additional Information

All your information will be kept confidential according to EEO guidelines."
118,Senior Data Engineer (Business Intelligence Developer),"Arlington, VA",Arlington,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Halfaker and Associates, LLC, an award winning high growth small business, creates innovative and customer-centric technology solutions in the areas of Cyber Security, Data Analytics, Software Engineering and IT Infrastructure to improve the health, security and well-being of all Americans. Our commitment to excellence and our vision to “Continue to Serve” has resulted in steady growth and an expanding client base across government agencies in the health, defense, security and intelligence sectors. Headquartered in Arlington, VA, we have employees nationwide and were recently named a 2018 Top Work Place by the Washington Post. Please take a moment to browse through our website and learn more about what it means to serve with Halfaker.


Halfaker has an opening for a Senior Data Engineer (Business Intelligence Developer) to join our talented, dynamic team. The key responsibilities for this position include:
Design, develop and support reporting and analytics applications leveraging Cognos Analytics Version 11
Gather and translate requirements from end-users to develop and maintain the BI architecture
Develop Cognos Framework Manager packages, Cognos Transformer cubes, and other BI assets that will be used by the Business Intelligence team to build reports and dashboards as well as to perform ad-hoc analysis
Strong Oracle and SQL Server SQL skills and experience tuning queries, Working knowledge of data warehouse and a business intelligence
Partner with the Data Services team and Information Management team for data integration and data modeling needs
Develop BI solutions using various data sources (e.g. SQL relational and multi-dimensional, unstructured data)
Perform data analysis to troubleshoot data issues with the BI solutions and data integration processes
Provide training to team members and customers



Required Skills
Well experienced and highly technical Cognos developer
Expert level with developing metadata models in Framework Manager following best practices
Expert level dashboard design and development Skills
Strong understanding of designing dimensional and relational models, star and snowflake schemas and different DW design paradigms
Ability to tune performance of dashboards/reports developed by end-user community and provide best practice training to end-users
Strong proficiency in different SDLC and Agile methodologies
Experience with ETL Tools such as Oracle SQL developer, SSIS developer
Extensive understanding of Structured Query Language (SQL) with an emphasis on Oracle PL/SQL, SQL Server
Experience developing extract, transformation and loading (ETL) programs for Data Warehousing or Business Intelligence applications using SSIS
IBM Certified Sr. Cognos 11 Business Intelligence Certifications are required


Required Experience
Bachelor’s degree in Computer Science, MIS, or related field; or equivalent work experience
7+ year’s exp. working with Cognos 11 Business Intelligence
5+ years’ experience with ETL tools such as SSIS and Oracle SQL Developer


Halfaker and Associates is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/ Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. U.S. Citizenship is required for most positions."
119,Data Engineer with active TS/SCI with Polygraph,"McLean, VA",McLean,VA,None Found,None Found,"Bachelor’s degree with a major in a analytical (e.g., engineering, statistics, mathematics, information technology, etc.) or social science discipline is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with an IC customer.
Must have one or more of the following skillsets through education or work experience: quantitative and qualitative analysis; developing reports; scripting tools; and data warehouses and environments.
Must possess strong written and verbal communication skills.
Must have an active TS/SCI clearance with Polygraph.",None Found,"Define data requirements and gather and validate information, applying judgment and statistical tests.Develop data structures to support the generation of business insights and strategy.Maintain data infrastructure and develop scripts for regular processes.Design and develop operational data analysis and visualization tools, techniques, metrics, and dashboards to meet business needs.Analyze data analysis requests obtained from management to determine operational problems and define data modeling requirements, validation of content, and problem solving parameters.Identify opportunities to use data to develop new strategies and improve business performance and utilize knowledge of mathematical modeling and other optimization methods to perform quantitative and qualitative data analysis.Communicate and present data to management by developing reports using Tableau or Business Intelligence tools.",None Found,None Found,"LMI is currently seeking a Data Engineer to support an Intelligence Community (IC) customer.
Responsibilities
The ideal candidate will have previously supported initiatives across in the IC and should have direct, applied experience with one or more of the following areas:
Define data requirements and gather and validate information, applying judgment and statistical tests.Develop data structures to support the generation of business insights and strategy.Maintain data infrastructure and develop scripts for regular processes.Design and develop operational data analysis and visualization tools, techniques, metrics, and dashboards to meet business needs.Analyze data analysis requests obtained from management to determine operational problems and define data modeling requirements, validation of content, and problem solving parameters.Identify opportunities to use data to develop new strategies and improve business performance and utilize knowledge of mathematical modeling and other optimization methods to perform quantitative and qualitative data analysis.Communicate and present data to management by developing reports using Tableau or Business Intelligence tools.
Qualifications
Bachelor’s degree with a major in a analytical (e.g., engineering, statistics, mathematics, information technology, etc.) or social science discipline is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with an IC customer.
Must have one or more of the following skillsets through education or work experience: quantitative and qualitative analysis; developing reports; scripting tools; and data warehouses and environments.
Must possess strong written and verbal communication skills.
Must have an active TS/SCI clearance with Polygraph."
120,Azure Data Engineer,"Washington, DC 20006",Washington,DC,20006,None Found,"At least 5 years of consulting or client service delivery experience on Azure
",DevOps on an Azure platform,None Found,None Found," Proven ability to build, manage and foster a team-oriented environment
","Are you ready to step up to the New and take your technology expertise to the next level?
 Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.
 People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications

 Role & Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.
Manage small teams of deliver engineers successfully delivering work efforts

 (if in an independent contributor role) at a client or within Accenture.
Extensive travel may be required

Basic Qualifications
At least 5 years of consulting or client service delivery experience on Azure
At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions
Extensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.
Extensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.
 Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.
Minimum of 5 years of RDBMS experience
Experience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.
Experience with private and public cloud architectures, pros/cons, and migration considerations.
Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
MCSA Cloud Platform (Azure) Training & Certification
MCSE Cloud Platform & Infratsructiure Training & Certification
MCSD Azure Solutions Architect Training & Certification

Nice-to-Have Skills/Qualifications:
DevOps on an Azure platform
Experience developing and deploying ETL solutions on Azure
IoT, event-driven, microservices, containers/Kubernetes in the cloud
Familiarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.
Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.
- Multi-cloud experience a plus - Azure, AWS, Google

Professional Skill Requirements
 Proven ability to build, manage and foster a team-oriented environment
 Proven ability to work creatively and analytically in a problem-solving environment
 Desire to work in an information systems environment
 Excellent communication (written and oral) and interpersonal skills
 Excellent leadership and management skills
 Excellent organizational, multi-tasking, and time-management skills
 Proven ability to work independently

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
121,Senior Level Artificial Intelligence Technical Expert (DE),"Alexandria, VA 22304",Alexandria,VA,22304,None Found,"
A resume or any other written format you choose to describe your job-related qualifications; optional cover letter: Your resume should indicate your citizenship and should list your educational and work experience. Please Note: Responses to job questions that are not fully supported by the information in your resume may result in adjustments to your rating.
Your responses to the Specialized Experience Statements. Do not write ""see "" resume. Input narrative responses when prompted. Please Note: Responses to the specialized experience statements that are not fully supported by the information in your resume may result in adjustments to your rating.
Your narrative responses to the Professional Technical Qualification (PTQs). Do not write ""see "" resume. Input narrative responses when prompted. Please Note: Responses to the PTQs that are not fully supported by the information in your resume may result in adjustments to your rating.
Transcripts demonstrating educational background.
",None Found,"Providing expert technical advice to Chief Information Officer and Deputy Chief Information Officer and participating in high-level decision-making regarding production and operational implementation of Artificial Intelligence (AI) infrastructure/architecture throughout the enterprise including expert knowledge of state-of-the-art approaches;
Demonstrate thought leadership to advise on AI and automation strategy and detailed use cases development by business units;
Demonstrate a thorough knowledge of security and Artificial Intelligence and ability to convey understanding of implications of training data;
Demonstrate capabilities of knowledge representation, learning and search: graph/ontology/taxonomy experts to design and maintain knowledge representations or metadata structures and apply these in learning processes, knowledge storage, search, navigation and retrieval;
Lead strategy engagements with mission critical internal business units to create clear and actionable strategic AI roadmaps and supporting business cases that both delineate a broader end-game, while also prioritizing quick wins with near-term measurable impact;
Actively develop and maintain client relationships with senior leadership as an advisor on Artificial Intelligence;
Advising and evaluating requirements for IT procurement solicitations related to Artificial Intelligence;
Facilitates and collaborates patent and trademark stakeholders to elicit, analyze, and define software features and functionality consistent with customer needs, expectations, and constraints supporting mission driven Artificial Intelligence;
Identifying and communicating tangible business benefits and overseeing the preparation of step-by-step transition guides to ensure successful adoption and implementation of new methodologies;
Experience discussing a wide range of strategic imperatives across functional, operational and technological issues;
Practical Business and Technical Architecture experience (business / operation models, personas, process modelling); and
Meeting with external stakeholders, gathering feedback, and designing flexible solutions to unite conflicting stakeholder views.
","
A resume or any other written format you choose to describe your job-related qualifications; optional cover letter: Your resume should indicate your citizenship and should list your educational and work experience. Please Note: Responses to job questions that are not fully supported by the information in your resume may result in adjustments to your rating.
Your responses to the Specialized Experience Statements. Do not write ""see "" resume. Input narrative responses when prompted. Please Note: Responses to the specialized experience statements that are not fully supported by the information in your resume may result in adjustments to your rating.
Your narrative responses to the Professional Technical Qualification (PTQs). Do not write ""see "" resume. Input narrative responses when prompted. Please Note: Responses to the PTQs that are not fully supported by the information in your resume may result in adjustments to your rating.
Transcripts demonstrating educational background.
","You must be a U.S. Citizen or National.
Required to pass a background investigation and fingerprint check
Must be registered for Selective Service, if applicable (www.sss.gov)
Financial Disclosure Report (OGE-278) will be required upon appointment.
","Duties
Summary
The United States Patent and Trademark Office (USPTO) is seeking a technical expert to advance the shared understanding of how to best implement the opportunities presented by Artificial Intelligence. The role will provide technical expertise in developing solutions for real-world, large-scale problems using Artificial Intelligence at the USPTO.

Responsibilities
The primary role of the Senior Level Artificial Intelligence Expert will be to provide leadership and expertise in bridging the gap between areas of opportunity for Artificial Intelligence (AI) and Research Scientist(s) developing proof of concepts to Software Engineer(s) putting AI into production by possessing deep understanding of trends and strategies for identifying solutions to meet mission critical business areas of opportunity. The Senior Level for AI will also be responsible for the following:
Providing expert technical advice to Chief Information Officer and Deputy Chief Information Officer and participating in high-level decision-making regarding production and operational implementation of Artificial Intelligence (AI) infrastructure/architecture throughout the enterprise including expert knowledge of state-of-the-art approaches;
Demonstrate thought leadership to advise on AI and automation strategy and detailed use cases development by business units;
Demonstrate a thorough knowledge of security and Artificial Intelligence and ability to convey understanding of implications of training data;
Demonstrate capabilities of knowledge representation, learning and search: graph/ontology/taxonomy experts to design and maintain knowledge representations or metadata structures and apply these in learning processes, knowledge storage, search, navigation and retrieval;
Lead strategy engagements with mission critical internal business units to create clear and actionable strategic AI roadmaps and supporting business cases that both delineate a broader end-game, while also prioritizing quick wins with near-term measurable impact;
Actively develop and maintain client relationships with senior leadership as an advisor on Artificial Intelligence;
Advising and evaluating requirements for IT procurement solicitations related to Artificial Intelligence;
Facilitates and collaborates patent and trademark stakeholders to elicit, analyze, and define software features and functionality consistent with customer needs, expectations, and constraints supporting mission driven Artificial Intelligence;
Identifying and communicating tangible business benefits and overseeing the preparation of step-by-step transition guides to ensure successful adoption and implementation of new methodologies;
Experience discussing a wide range of strategic imperatives across functional, operational and technological issues;
Practical Business and Technical Architecture experience (business / operation models, personas, process modelling); and
Meeting with external stakeholders, gathering feedback, and designing flexible solutions to unite conflicting stakeholder views.
Travel Required
Occasional travel - Occasional Travel will be necessary.
Supervisory status
Yes
Promotion Potential
00
Job family (Series)
2210 Information Technology Management
Similar jobs
Data Engineer
Information Research Scientists
Scientists, Information Research
Requirements

Requirements
Conditions of Employment
You must be a U.S. Citizen or National.
Required to pass a background investigation and fingerprint check
Must be registered for Selective Service, if applicable (www.sss.gov)
Financial Disclosure Report (OGE-278) will be required upon appointment.
Qualifications
Candidates must provide evidence that is related to the skills and abilities outlined under the Specialized Experience. Specialized experience must have been at a sufficiently high level of difficulty to clearly show that the candidate possesses the required qualifications set forth below.

The Specialized Experience for this position is:

1. Proven leadership roles and skills related to Artificial Intelligence implementation with professional experience in applied implementation of Natural Language Processing (NLP), Expert Systems Evolutionary Computation, Robotic Process Automation, voice recognition, gaming theory, image search, video surveillance, predictive maintenance, anomaly detection, natural language processing, aggregating, extracting, and analyzing complex big data (complex information such as human speech or unstructured text); deep learning algorithms, and neural network feedback loops.

2. Proven experience in applications of AI in decision augmentation.

3. Ability to lead in fast paced and dynamic environment.

4. Proven public sector and/or corporate experience leadership with major deep learning initiatives that resulted in measurable improvements with a sound Al strategy aligned with an overall (digital) business strategy.

Candidates must provide evidence that is related to the skills and abilities outlined under the Professional Technical Qualifications (PTQ). Experience must have been at a sufficiently high level of difficulty to clearly show that the candidate possesses the required professional/technical qualifications set forth below.

Professional Technical Qualifications: Applicants must clearly demonstrate in their application materials that they possess technical attributes in the Professional/Technical Qualifications (PTQs).

The Professional Technical Qualifications (PTQs) for this position are:

1. Demonstrated senior level experience leading organizational change management to drive large, cross-functional artificial intelligence programs involving coordination with multiple stakeholders at various levels of the organization.

2. Demonstrated senior level experience defining and articulating enterprise data management, strategic vision and translating it into implementable steps for intelligence operationalization.

3. Demonstrated senior level experience defining and articulating enterprise architecture, strategic vision and translating it into implementable steps for artificial intelligence operationalization.

4. Demonstrated experience using various technologies of artificial intelligence within an organization

Experience refers to paid and unpaid experience, including volunteer work done through National Service programs (e.g., Peace Corps, AmeriCorps) and other organizations (e.g., professional; philanthropic; religious; spiritual; community, student, social). Volunteer work helps build critical competencies, knowledge, and skills and can provide valuable training and experience that translates directly to paid employment. You will receive credit for all qualifying experience, including volunteer experience.
Education
A strongly preferred candidate would have an educational level of PhD and/or Masters or at minimum a bachelors in Artificial Intelligence, Computer Science, Math, Engineering, or other technical related field.
Additional information
Probationary Period - If selected, you will be required to serve a one-year probationary period.

This is a Non Bargaining Unit position. This is a Public Trust position and has a risk level designation of ""high"".

Background Investigation - If selected for this position, you may be required to complete a Declaration for Federal Employment (OF-306), which includes a fingerprint and credit check, to determine your suitability for Federal employment and to authorize a background investigation

The USPTO participates in E-Verify. For more information on E-Verify, please visit http://www.dhs.gov/files/programs/gc_1185221678150.shtm

All Federal employees are required to have Federal salary payments made by direct deposit to a financial institution of their choice.

Relocation expenses and or Recruitment incentives may be authorized.

More than one selection may be made from this announcement if additional identical vacancies in the same title, series, grade, and unit occur within 90 days from the date the certificate was issued.

The Ethics in Government Act of 1978, as amended, requires senior officials in the executive, legislative and judicial branches to file public reports of their finances as well as other interests outside the Government. If selected for this position you will be required to file a Financial Disclosure Report (OGE Form 278). You will need to provide the information annually. The OGE 278 is available to the public. The primary purpose of disclosure is to assist agencies in identifying potential conflicts of interest between a filer's official duties and the filer's private financial interests and affiliations.


How You Will Be Evaluated
You will be evaluated for this job based on how well you meet the qualifications above.
In order to be considered for this position, you must demonstrate in your application package that you meet the key requirements for the job, answer the Professional Technical Qualifications (PTQs), and submit a detailed resume that supports your answers to the PTQs and specialized experience.

Your responses to the PTQs, specialized experience and resume will be evaluated by a Human Resources Specialist and/or a rating and ranking panel. The highly qualified candidates will be submitted to the Selecting Official for further review and consideration.

Falsifying your background, education, and/or experience is cause for not hiring you or for changing your scored responses to questions you've answered, which may affect your overall final score. Please note that a complete application is required for consideration. (Please review the ""Required Documents"" section of this job announcement to see what must be included in a complete application).

To preview questions please click here.

Background checks and security clearance
Security clearance
Not Required
Drug test required
No
Required Documents

Required Documents
A complete application consists of:

A resume or any other written format you choose to describe your job-related qualifications; optional cover letter: Your resume should indicate your citizenship and should list your educational and work experience. Please Note: Responses to job questions that are not fully supported by the information in your resume may result in adjustments to your rating.
Your responses to the Specialized Experience Statements. Do not write ""see "" resume. Input narrative responses when prompted. Please Note: Responses to the specialized experience statements that are not fully supported by the information in your resume may result in adjustments to your rating.
Your narrative responses to the Professional Technical Qualification (PTQs). Do not write ""see "" resume. Input narrative responses when prompted. Please Note: Responses to the PTQs that are not fully supported by the information in your resume may result in adjustments to your rating.
Transcripts demonstrating educational background.
If you are relying on your education to meet qualification requirements:
Education must be accredited by an accrediting institution recognized by the U.S. Department of Education in order for it to be credited towards qualifications. Therefore, provide only the attendance and/or degrees from schools accredited by accrediting institutions recognized by the U.S. Department of Education.
Failure to provide all of the required information as stated in this vacancy announcement may result in an ineligible rating or may affect the overall rating.
Benefits

Benefits
A career with the U.S. Government provides employees with a comprehensive benefits package. As a federal employee, you and your family will have access to a range of benefits that are designed to make your federal career very rewarding. Learn more about federal benefits.
A career with the U.S. Government provides employees with a comprehensive benefits package. As a federal employee, you and your family will have access to a range of benefits that are designed to make your federal career very rewarding. Learn more about federal benefits.
Eligibility for benefits depends on the type of position you hold and whether your position is full-time, part-time, or intermittent. Contact the hiring agency for more information on the specific benefits offered."
122,"Data Strategy Specialist - Business & Data Analysis, Cloud, AWS, Azure, Big Data","Washington, DC 20006",Washington,DC,20006,None Found,None Found, 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:,None Found,None Found,None Found,"Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The North America Data Strategy & Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.

 Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model

Required Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:
o Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling
o Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.
o Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel
 Able to travel up to 100% (Mon-Thu)

Optional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model
 Experience in compiling business cases and roadmaps for data, analytics and technology investments

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
123,Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,None Found,"Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.",None Found,"The Senior Data Engineer responsibilities include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of our client's Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatment, etc .

Required Education/Experience:
Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.


Preferred:
Experience in Healthcare or related industryExperience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plusExperience productizing/automating predictive models that use R, SAS, Python, SPSS, etc.Continuous delivery and deployment automation for analytic solutions using tools like BambooFamiliarity with test driven development methodology for analytic solutionsAGILEAPI developmentData visualization and/or dashboard developmentDemonstrated ability to achieve stretch goals in a highly innovative and fast-paced environment"
124,Big Data Architect,"Herndon, VA",Herndon,VA,None Found,None Found,"B.Tech/B.E Graduates & MCA, MSc IT Under Graduates, with min 7+ years of hands-on experience in Software development and design.
Strong and effective inter-personal and communication skills and the ability to interact professionally with a diverse group of clients and staff
Ability to handle multiple projects
","4+ years of experience as a Big Data Engineer or similar role
Expert in Business Intelligence (BI) and Data Warehouse enterprise platforms
4+ years of experience in Microsoft data stack: SQL Server, SSIS, SSAS, SSRS, etc.
Strong knowledge in NoSQL topologies using tools like Hadoop, Elastic, Cassandra, Mongo, etc.
Experience in data streaming technologies like Kinesis and Kafka
Experience with .Net development and web development
Working knowledge of current hardware systems and disk subsystems commonly used in fault tolerant production environments
4+ years of experience in Hadoop ecosystem and different frameworks inside it – HDFS, YARN, MapReduce, Apache Pig, Hive, Flume, Sqoop, and Kafka
Design, develop, document and architect Hadoop applications
Work with Hadoop Log files to manage and monitor it
Develop MapReduce coding that works seamlessly on Hadoop clusters
Expertise in newer concepts like Apache Spark and Scala programming
Experience in the automotive industry","Ability to design solutions independently based on the high-level architecture.
Manage the technical communication between the survey vendor and internal system.
Maintain the production systems (Kafka, Hadoop, Cassandra, Elasticsearch)
Lead the design, implementation, and continuous delivery of a sophisticated data pipeline supporting development and operations
Review current database architecture and propose solutions to scale processing and reporting functionality
Work closely with business stakeholders, product owners, and technical staff to design industry-leading solutions
Proactively analyze and bring forth ideas for continuous improvement of the platform
Building a cloud-based platform that allows secure development of new applications
Ability to proactively identify, troubleshoot and resolve live database systems issues
",None Found,None Found,"Job skills & Qualifications
B.Tech/B.E Graduates & MCA, MSc IT Under Graduates, with min 7+ years of hands-on experience in Software development and design.
Strong and effective inter-personal and communication skills and the ability to interact professionally with a diverse group of clients and staff
Ability to handle multiple projects
Responsibilities
Ability to design solutions independently based on the high-level architecture.
Manage the technical communication between the survey vendor and internal system.
Maintain the production systems (Kafka, Hadoop, Cassandra, Elasticsearch)
Lead the design, implementation, and continuous delivery of a sophisticated data pipeline supporting development and operations
Review current database architecture and propose solutions to scale processing and reporting functionality
Work closely with business stakeholders, product owners, and technical staff to design industry-leading solutions
Proactively analyze and bring forth ideas for continuous improvement of the platform
Building a cloud-based platform that allows secure development of new applications
Ability to proactively identify, troubleshoot and resolve live database systems issues
Technical Skills
4+ years of experience as a Big Data Engineer or similar role
Expert in Business Intelligence (BI) and Data Warehouse enterprise platforms
4+ years of experience in Microsoft data stack: SQL Server, SSIS, SSAS, SSRS, etc.
Strong knowledge in NoSQL topologies using tools like Hadoop, Elastic, Cassandra, Mongo, etc.
Experience in data streaming technologies like Kinesis and Kafka
Experience with .Net development and web development
Working knowledge of current hardware systems and disk subsystems commonly used in fault tolerant production environments
4+ years of experience in Hadoop ecosystem and different frameworks inside it – HDFS, YARN, MapReduce, Apache Pig, Hive, Flume, Sqoop, and Kafka
Design, develop, document and architect Hadoop applications
Work with Hadoop Log files to manage and monitor it
Develop MapReduce coding that works seamlessly on Hadoop clusters
Expertise in newer concepts like Apache Spark and Scala programming
Experience in the automotive industry"
125,Big Data Engineer (Secret Clearance),"Arlington, VA 22209",Arlington,VA,22209,None Found,Typically has 7 or more years of consulting and/or industry experience,None Found,None Found,None Found,None Found,"Are you passionate about cyber and security challenges in information technology, associated with threats and vulnerabilities? Are you are interested in a role that offers an opportunity to provide front line support to our Federal clients? If yes, then Deloitte’s Cyber Risk team could be the place for you! Join our team of Cyber Risk professionals who collaborate with government agencies, IT professionals, and clients to support cyber security and risk consulting engagements.
Work you’ll do
As a Project Delivery Manager in the Cyber Risk group you will:
Improve the operational systems, processes, and policies in support of the client’s mission through the management and guidance of multiple work streams, teams, and clients
Support engagements related but not limited to Operations & Maintenance, Helpdesk Operations, Software and Application Development and Maintenance, Financial Operations, and Project and Acquisition Management
Provide input to key deliverable structure and content, as well as facilitating buy-in of proposed solutions from top management levels
Direct timely delivery of quality work products for the client
Manage engagement risk
Provide professional development of junior staff performing the role of counselor and coach, as well as providing leadership and support
Work on the latest in big data analytical systems building solutions that will support data ingestion, processing and real-time analytics across billions of data pointsManage a big data migration to the cloud and ensure the flow of data through the pipelineUtilize open source software and tools such as Jenkins, Apache Kafka, Apache Spark, Ansible
The Team
Transparency, innovation, collaboration, sustainability: these are the hallmark issues shaping Federal government initiatives today. Deloitte’s Federal practice is passionate about making an impact with lasting change. Carrying out missions in the Federal practice requires fresh thinking and a creative approach. We collaborate with teams from across our organization in order to bring the full breadth of Deloitte, its commercial and public sector expertise, to best support our clients. Our aspiration is to be the premier integrated solutions provider in helping to transform the Federal marketplace.
Qualifications
Required:
Typically has 7 or more years of consulting and/or industry experience
Ability to support engagements of greater than average size and complexity
Ability to lead multiple teams and multiple clients with confidence
Excellent teamwork and interpersonal skills
Professional oral and written communication skills
Ability to mentor and manage junior staff and further their professional growth
Ability to obtain and maintain the required clearance for this role
Preferred:
Prior professional services or federal consulting experience
Bachelor’s Degree
How you’ll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career.
Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.
Deloitte’s culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives.
Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.
Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals."
126,Data Engineer,"Arlington, VA",Arlington,VA,None Found,None Found,None Found,None Found,None Found,None Found,"
BS in Computer Science, Mathematics, Software Engineering, or other relevant field.
2 or more years of software development and a passion for working with large data sets.
Demonstrated extensive experience working in large-scale data environments which includes real time and batch processing requirements, as well as graph databases.
Experience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectures.
Hands-on coding experience in programming languages such as Java, Python, Scala.
Experience with Big Data platforms and technologies – AWS, Apache, Hadoop, Azure, and advanced analytics tools.
Experience with ETL processing with large data stores.
Working knowledge of data security management policies and procedures.
Knowledge and experience with Agile, Scrum and DevOps principles and practices and working on collaborative development teams.
","We are currently seeking a Data Engineer (Full Stack) to contribute to an exciting federal project to create, transform, and modernize applications and data platforms. This is an exciting opportunity which will allow qualified candidates to further develop their skills and expand their area of expertise!

RESPONSIBILITIES:
Work closely with the Project Manager, Technical Lead, and development team to provide overall data-engineering support and to understand project and application requirements.
As a member of the development team, create and build customized infrastructure, applications, and tools that meet user requirements.
As an engineer, build and deliver data storage, integrate with existing data, and migrate the built big data applications to cloud computing platform by leveraging new technologies.
Collect, build, cleanse, assemble and refine datasets to support the variety of data analytics needs put forward by business stakeholders including data scientists, law enforcement officials, and agency analysts.
Networking, database, cloud engineering, security engineering teams to comply with the data security policies and procedures and trouble-shoot and resolve any issues in data engineering deliveries.
Requirements
BS in Computer Science, Mathematics, Software Engineering, or other relevant field.
2 or more years of software development and a passion for working with large data sets.
Demonstrated extensive experience working in large-scale data environments which includes real time and batch processing requirements, as well as graph databases.
Experience designing and delivering large scale, 24-7, mission-critical data pipelines and features using modern big data architectures.
Hands-on coding experience in programming languages such as Java, Python, Scala.
Experience with Big Data platforms and technologies – AWS, Apache, Hadoop, Azure, and advanced analytics tools.
Experience with ETL processing with large data stores.
Working knowledge of data security management policies and procedures.
Knowledge and experience with Agile, Scrum and DevOps principles and practices and working on collaborative development teams.
Benefits
At GreenZone, we are dedicated to obtaining and maintaining the highest level of employee satisfaction by offering a competitive benefits package that includes medical, dental and vision, short and long term disability, retirement plan and company match, a generous annual leave plan, and a commitment to providing a work/life balance for all employees."
127,Data Engineer,"Alexandria, VA 22314",Alexandria,VA,22314,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for a Senior Data Engineer to integrate, manipulate, and manage vast amounts of data into the next generation of data analytic solutions for clients. Deliver robust solutions that serves clients and stands apart from competitors. Interact with a multi-disciplinary team of analysts, data scientists, developers, and users to comprehend the data requirements to develop a robust data processing pipeline that ingest, manipulate, normalize, and expose potentially billions of records per day to support advanced analytics. Maintain responsibility for building and launching new data models that provide intuitive analytics to clients. Design, build, and deploy efficient and reliable data pipelines to move data, both large and small amounts, to the data platform. Design and develop new systems and tools to enable clients to consume and comprehend data faster.

Basic Qualifications:
4+ years of experience with Python, Java, Scala, or equivalent modern programming language
3+ years of experience with RDBMS data stores, including PostgreSQL and MySQL
Experience with custom or structured ETL design, implementation, and maintenance
Experience with NoSQL data stores, including Accumulo, HBase, MongoDB, and Cassandra
Ability to quickly learn technical concepts and communicate with multiple functional groups
Experience with building and deploying solutions in AWS
Secret clearance
BA or BS degree
Additional Qualifications:
3+ years of experience in working with Hadoop-based technologies
Experience with batch and streaming frameworks, including Storm, NiFi, Apex, and Flink
Experience with analytic databases, including Hive, Presto, and Impala
Experience with multiple data modeling concepts, including XML and JSON
Possession of excellent analytical and problem-solving skills
Security+ or CISSP Certification
EEO Compliance:
Ryde Technologies is an Equal Employment Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic protected by law. Ryde Technologies will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law.

Wva2yvstgV"
128,Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Data Engineer will be responsible for:
Supporting data migration and data population activities to the Data Science Workstations from external sources
Designing data population processes and developing these processes using Python
Support Requirements Gathering activities – especially from an data population perspective
Supporting break-fix activities
REQUIREMENTS
US Citizenship or legal permanent resident for at least 3 years
At least 5 years of experience with software development lifecycle
At least 5 years of developing data migration processes associated with big data, and data population to AWS RDS
At least 3 years of hands-on Python coding experience
At least 2 years of developing REST API based applications
Problem solving skills and ability to break-down large development into modular programming
Hands-on experience with CI/CD automation is preferred
About Bravium Consulting
Bravium Consulting provides technology and management consulting services for the public and private sectors. Our team is comprised of skilled, certified consultants that help clients achieve success with effective, created, and rapidly-executed solutions. We were recently awarded the Fast 100 Asian American Business Award for 2018 and were nominated for the Small and Emerging Contractors Advisory Forum Contractor of the Year.
Bravium Consulting is rapidly growing and we are always looking for intelligent and motivated people to join our team. We are committed to excellence so training, supporting, and empowering our team is a top priority for us. We offer competitive salaries and a comprehensive benefits package which includes:
15 PTO days
10 paid holidays
Medical Insurance with 80% premium support
Dental Insurance with 80% premium support
Vision Insurance with 80% premium support
Short Term and Long Term Disability coverage with 100% premium support
401k Program with Bravium matching 100% of up to 4% of salary
$2500 Annual Training Budget
5 days of Paid Training Time Off
Maternity Leave
Eligibility for annual bonus
Referral bonus
Flexible work arrangements
Visit www.braviumconsulting.com to find out more about us!"
129,"Data Engineer, Lead","Herndon, VA",Herndon,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Key Role:
Work on cutting edge projects from genomic research to counter threats. Perform activities, including data architecture, building data, and analytic platforms, building out extract, transform, and load (ETL) pipelines and data access services, and ensuring data discoverability and quality. Support the assessing, designing, building, and maintaining of scalable data platforms that use the latest and best in Big Data tools. Perform analytical exploration and examination of data from multiple sources of data. Enforce data governance best practices on the data platform. Work with a multidisciplinary team of analysts, data engineers, data scientists, developers, and data consumers in an agile fast-paced environment that is pushing the envelope of cutting-edge Big Data implementations.
Basic Qualifications:5+ years of experience with at least one of the following coding using Java, Scala, Python, or RExperience with developing and deploying ETL pipelines using Apache SparkExperience in interfacing with modern relational databases, including MySQL or PostgreSQLExperience in working with Big Data platforms, including Hadoop, AWS, Azure, or DataBricksAbility to learn technical concepts and communicate with multiple functional groupsAbility to obtain a security clearanceBA or BS degree
Additional Qualifications:Experience with Agile software developmentExperience with NoSQL data stores, including HBase, MongoDB, JanusGraph or Neo4J, and CassandraExperience with ETL tools, including StreamSets, NiFi, and TalandExperience in working with enterprise production systemsAbility to display a positive, can-do attitude to solve the challenges of tomorrow.Possession of excellent oral and written communication skillsBA or BS degree in CS, Information Systems, or a related field preferred; MA or MS degree a plusAWS Certification or equivalent
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
We’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change."
130,Data Engineer,"Herndon, VA",Herndon,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"The Challenge:
Are you passionate about helping organizations understand and reach their goals? Do you live for that “ah-ha!” moment when your client realizes you just showed them how to transform their organization? Many organizations know where they want to go, but getting there can be a challenge. We’re looking for you: a strategy consultant who can identify an organization’s long-term goals and show it how to achieve them.
This is an opportunity to have a direct impact on large military data-driven organizations. As a strategy consultant on our team, you’ll engage with clients at the senior leadership level to help them understand their organization and crystallize their goals. You’ll need to ask the right questions to draw out your client’s objectives and vision. Your client will look to you to show them their organization from the outside. From unrealized potential to systemic challenges, you’ll provide your client with a holistic view of their organization, as well as the multiple paths they can take to modernize. As the strategy expert, you’ll anticipate your client’s needs as you create their way forward. Throughout the strategy life cycle, you’ll identify opportunities to expand services to new and existing clients. Join us and lead change for the better in large military data-driven organizations.
Empower change with us.
Build Your Career:
Consulting at Booz Allen means empowering you to provide your customers with the best support. With a consulting career at Booz Allen, you can expect:
a large business consulting community
access to experts in virtually every field
a culture that focuses on supporting our employees
We have opportunities that provide stability while offering variety, so you can find the right fit for your career — and your life. You’ll also have access to a wealth of training resources through our Digital University, an online learning portal where you can access more than 5000 functional and technical courses, certifications, and books. Build your technical skills through hands-on training on the latest tools and tech from our in-house experts. Pursuing certifications? Take advantage of our tuition assistance, onsite courses, vendor relationships, and a network of professionals who can give you helpful tips. We’ll help you develop the career you want, as you chart your own course for success. With contracts across the globe in multiple industries, no matter where you want to go with your consulting career, we have the path the takes you there.
You Have:4+ years of experience with data engineeringExperience with custom or structured ETL design, implementation, and maintenanceExperience with Python, Pandas, R, JavaScript, React, D3, or TableauAbility to learn technical concepts quickly and communicate with multiple functional groupsActive Secret clearanceBA or BS degree
Nice If You Have:Experience with ArcGIS or QGISExperience with SQL, PHP, or GraphQLExperience with analytical technologies used in enterprise reporting, including Tableau, Qlik, Looker, OBIEE, or equivalentExperience with all or any of the following interfaces: Single Integrated Space Picture (SISP), Joint Air Defense Systems Integrator/Joint Range Extension Application Protocol (JADSI/JRE), Information Store (iStore), Traffic Flow-Management Data Service (TFMDG) (FAA), Force Tracking Advance Management System (FTAMS), or Integrated Broadcast Service (IBS) a plusExperience with analytic databases, including Hive, Presto, and ImpalaExperience with multiple data modeling concepts, including XML and JSONPossession of excellent analytical and problem-solving skills
Clearance:
Applicants will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required.
We’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change."
131,IT Specialist (APPSW),"Fort Belvoir, VA",Fort Belvoir,VA,None Found,None Found,"
Communication
Data Management
Software Development
Systems Testing and Evaluation
Technical Competence",None Found,"Provides technical expertise in a variety of software technologies, platforms, and architectures.
Manages agency Information Technology internet, intranet and extranet system architectures.
Implements enterprise architecture designs for centralized and enterprise-ready software solutions to ensure implementation and accountability of stated business needs, supported objectives, and to influence Agency IT policy and program.
Performs integration with DoD enterprise solutions, analysis and modification to existing services and applications for conformance to standards.
Promotes the use of DoD Net-Centric Data Architecture, DoD Information Sharing Strategy, and additional DoD enterprises strategic goals and initiatives.
","
Communication
Data Management
Software Development
Systems Testing and Evaluation
Technical Competence","Must be a U.S Citizen as verified through E-Verify Program.
","Duties
Summary
Reemployed annuitant: This vacancy does not meet the criteria for appointment of annuitants.

Responsibilities
Provides technical expertise in a variety of software technologies, platforms, and architectures.
Manages agency Information Technology internet, intranet and extranet system architectures.
Implements enterprise architecture designs for centralized and enterprise-ready software solutions to ensure implementation and accountability of stated business needs, supported objectives, and to influence Agency IT policy and program.
Performs integration with DoD enterprise solutions, analysis and modification to existing services and applications for conformance to standards.
Promotes the use of DoD Net-Centric Data Architecture, DoD Information Sharing Strategy, and additional DoD enterprises strategic goals and initiatives.
Travel Required
Not required
Supervisory status
No
Promotion Potential
13
Job family (Series)
2210 Information Technology Management
Similar jobs
Architects, Systems
Data Engineer
Information Architect
Systems Architects
Requirements

Requirements
Conditions of Employment
Must be a U.S Citizen as verified through E-Verify Program.
Qualifications
All qualification and time-in-grade (if applicable) requirements must be met within 30 days of the closing date of this announcement.

You must meet the minimum qualification requirements as stated in the Office of Personnel Management (OPM) Operating Manual, Qualification Standards for General Schedule Positions, http://www.opm.gov/qualifications/Standards/group-stds/gs-prof.asp.

Basic Requirement: Applicants must have IT-related experience demonstrating the following competencies appropriate to, or above, the level of this position. For vacancies below the full-performance level of the position, the basic requirement will be evaluated on a developmental basis. Your resume and work experience should clearly support your ability to meet these competencies and will be evaluated as part of the entire application process.
Attention to Detail- experience reviewing my own information technology-related work or data and have been asked by others to review their work or data to ensure accuracy, completeness, and consistency with standards.
Customer Service- experience maintaining relationships with customers, assessing current information technology needs of customers, and developing or identifying information technology products and services that are tailored to meet customer needs.
Oral Communication- briefing mid-level management and IT staff on the status of information technology systems, projects, or daily operations, including the communication of technical information to a non-technical audience.
Problem Solving- identifying alternatives to address complex information technology-related issues by gathering and applying information from a variety of sources that provide a number of potential solutions.

In addition to meeting the basic requirement, you must have one year specialized experience at the GS-12 or equivalent level. Specialized experience must be documented in your resume.Specialized experience is defined as experience performing expert enterprise information system analysis,design, development and delivery of web-based applications to include SharePoint Technologies (e.g. SharePoint Services, InfoPath, Forms Services, Enterprise Search, Web parts, Web Services), and Web 2.0 technologies (e.g. C#, .Net, .Net core, Angular, HTML 5).

Volunteer Experience: Experience refers to paid and unpaid experience, including volunteer work done through National Service programs (e.g., Peace Corps, AmeriCorps) and other organizations (e.g., professional, philanthropic, religious, spiritual, community, student, social). Volunteer work helps build critical competencies, knowledge, and skills, and can provide valuable training and experience that translates to paid employment. You will receive credit for all qualifying experience, including volunteer experience.

The incumbent must possess or attain, within 6 months of employment, an Information Assurance System Architect and Engineer Level 1 Certification (CISSP or Associate, CASP CE, CSSLP) as a condition of continued employment. Per DoD 8570.01-M, the incumbent of this position must achieve the appropriate IA certification within six months of assignment of these duties. A waiver of this six month requirement may be granted per DoD 8570.01-M, C3.2.4.2 or C3.2.4.3. Failure to receive the proper IA certification may result in removal from this position.
Education
There is no substitution of education for experience at this grade level.
Additional information
DCAA has a comprehensive benefits package that includes retirement, social security and thrift savings; health, life and long term care insurance; paid vacation, sick leave and holidays. DCAA employees enjoy our business-casual dress code, flexible work schedules, transit subsidies, and the opportunity to telecommute.

Initial Probationary Period: You will be required to serve an initial probationary period of 2 years if one has not already been completed.

Telework: Telework availability will be based upon the mission requirements and supervisory determination.

Selective Service: Males born after 12-31-59 must be registered or exempt from Selective Service (see www.sss.gov).

Performance Appraisal: If you are a current Federal employee, you must be rated fully successful or higher on your current performance rating to be eligible for promotion. You may be asked to provide a copy of your performance appraisal during the evaluation and selection process

Background Investigation: ll selectees will be subject to a Single Scope Background Investigation (SSBI). You must be able to obtain and maintain security clearance eligibility at the Top Secret level.

Drug Testing: If you are given access to classified information, you will be subject to random drug testing.

Interagency Career Transition Assistance Program (ICTAP): You must be well qualified for the position to receive special selection priority under the ICTAP. A qualified ICTAP eligible achieving a score of 90 or above will be considered ""well qualified"". For more information, go to: http://www.opm.gov/rif/employee_guides/career_transition.asp#ictap

Special Hiring Authorities: You may be considered for the vacancy if you are eligible for a special appointment authority such as those authorized for the severely disabled; certain Vietnam Era and disabled veterans; volunteers returned from the Peace Corps or Vista, etc. For more information, visit the following OPM websites: USAJOBS Resource Center: http://www.usajobs.gov/ResourceCenter; VetsInfo Guide: http://www.fedshirevets.gov/.

Veterans Employment Opportunities Act (VEOA): Veterans who are preference eligibles or who have been separated from the Armed Forces under honorable conditions after substantially completing an initial 3-year term (not less than 2 years and 11 months) may apply regardless of their current geographical location. Veterans must submit their DD-214, Certificate of Release or Discharge from Active Duty, to be considered under this authority.

If you are unable to apply online, view the following link for information regarding Alternate Application.

How You Will Be Evaluated
You will be evaluated for this job based on how well you meet the qualifications above.
Your application will initially be reviewed to determine whether you meet the minimum eligibility and qualifications requirements. If you are qualified, you will then receive a numeric score based on the degree to which your background matches the knowledge, skills and abilities for the position listed below. If a determination is made that you have inflated your qualifications, you will be assigned a rating commensurate with your background. Your final score will be used to determine if you are among the best qualified candidates.
Competencies:

Communication
Data Management
Software Development
Systems Testing and Evaluation
Technical Competence

Background checks and security clearance
Security clearance
Top Secret
Drug test required
Yes
Required Documents

Required Documents
You must provide a complete Application Package which includes:
1. Résumé - Each period of Federal experience must reflect the grade level of the position and whether the assignment is permanent or temporary. Your resume must document your specialized experience at the next lower grade level.
2. Assessment Questionnaire
3. Other supporting documents
a. Notification of Personnel Action for current or former federal employees - Unless you are a current permanent DCAA employee, please provide your most recent SF50 or the SF50 reflecting Career/Career-Conditional tenure and the highest grade held on a permanent basis.
b. Veterans Preference Documentation - DD-214 (member copy 4), Standard Form 15, VA Letter
c. ICTAP - Agency notice
d. Military Spouse Preference (MSP) - Spousal Travel Orders/Permanent Change of Station(PCS) Orders, Marriage Certificate, and Military Spouse Preference Self Certification checklist
e. Other documents identified in this announcement

Your application submission must be complete, including all required documents, before you can be considered. You are responsible to verify in your Application Manager account that information and documents entered or uploaded are received, legible, and accurate.

WARNING: Failure to submit a complete application package including any required documentation by the closing date of the announcement at 11:59 EST, or at the time of application for announcements with an extended closing date, may result in an ineligible rating and loss of consideration. To verify that your application is complete, log into your USAJOBS account, select Application Status and More Information. The Details page will display the status of your application. It is the applicant's responsibility to verify that information entered, uploaded, or faxed is received, legible and accurate. HR will not modify answers submitted by an applicant.
If you are relying on your education to meet qualification requirements:
Education must be accredited by an accrediting institution recognized by the U.S. Department of Education in order for it to be credited towards qualifications. Therefore, provide only the attendance and/or degrees from schools accredited by accrediting institutions recognized by the U.S. Department of Education.
Failure to provide all of the required information as stated in this vacancy announcement may result in an ineligible rating or may affect the overall rating.
Benefits

Benefits
A career with the U.S. Government provides employees with a comprehensive benefits package. As a federal employee, you and your family will have access to a range of benefits that are designed to make your federal career very rewarding. Learn more about federal benefits.
Review our benefits
Eligibility for benefits depends on the type of position you hold and whether your position is full-time, part-time, or intermittent. Contact the hiring agency for more information on the specific benefits offered."
132,Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,"
Active Top-Secret US Government clearance
Bachelor’s Degree in Statistics, Mathematics, Computer Science, Management Information Systems, Engineering, Business Analytics disciplines, or related area
3+ years of experience with ETL and data integration, data quality analysis, statistical analysis and/or modeling
Advanced working SQL knowledge and experience with RDBMS platforms
Experience building and optimizing data pipelines, architectures, and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong business and analytical skills including direct interaction with business end users for requirements analysis",None Found,None Found,None Found,None Found,"Data Engineer
Location: Washington DC
Job ID: 1093
Active Top-Secret US Government clearance required
Are you an analytical, data-driven professional? Are you interested in a role that offers the opportunity to provide client-facing support? If so, Datastrong is the place for you! Join our team of specialists as they unlock insights contained in a data universe and work through challenges related to real-time data integration and analytics, data quality management, mission-oriented analysis, and human resources.
Work you will perform
As a Data Engineer on the analytics team you will:
Create and enhance the data pipeline architecture
Assemble data sets to meet functional and non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
Work with business stakeholders to assist with data-related needs and issues
Create data tools for front-end analytics and data scientist team members that assist them in their responsibilities
Design and deliver ETL data integration workflow components
Qualifications
Required:
Active Top-Secret US Government clearance
Bachelor’s Degree in Statistics, Mathematics, Computer Science, Management Information Systems, Engineering, Business Analytics disciplines, or related area
3+ years of experience with ETL and data integration, data quality analysis, statistical analysis and/or modeling
Advanced working SQL knowledge and experience with RDBMS platforms
Experience building and optimizing data pipelines, architectures, and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong business and analytical skills including direct interaction with business end users for requirements analysis
Preferred:
Hands-on experience with implementation and support of commercial data integration software (Informatica, IBM DataStage, Talend, Microsoft SSIS, etc.)
Experience with Oracle SQL or PL/SQL
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience in Python, message queuing, or stream processing
How you will grow
At Datastrong, our professional development plan focuses on helping employees at every level of their career to identify and use their strengths. Datastrong offers opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career.
Benefits include
Compensation plan consisting of a competitive base salary and an uncapped bonus
100% Health coverage for employees with Vision and Dental options
Paid holidays and vacation with a generous leave policy
Commute reimbursement
Professional development and educational tuition assistance
Flexible spending account options
401(k) retirement plan with complimentary financial advisory services
Datastrong’s culture
Our culture is built on inclusion, collaboration, high performance, and opportunity. That combination helps our professionals make a difference individually and collectively. And it makes Datastrong a rewarding place to work. We are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives.
Corporate citizenship
We are proud to be part of the communities where we work and live. Datastrong takes great pride in knowing that what we do best – applying our skills and experience – helps to build strong communities that shape and sustain our business culture. Our team members are provided opportunities to support the community through volunteerism, giving their time, talent, and generosity.

Datastrong is committed to hiring and retaining a diverse workforce. We are an Equal Opportunity and Affirmative Action Employer, making decisions without regard to race, color, religion, sex, national origin, age, veteran status, disability, or any other protected class. If accommodation is needed in the application process, arrangements can be made with the local regional office."
133,Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,"Experienced with Oracle Business Intelligence (OBIEE) is a must.
SQL & PLSQL query development experience.
Python scripting experience.
Proven ability to work independently and as a team member.
Good communication (written and oral) and interpersonal skills.
Good organizational, multi-tasking, and time-management skills.
US Citizen in order to obtain the Public Trust.
",None Found,None Found,"Working with the client to define business questions (i.e. problem statements), associated metrics, data challenges, and scope business objectives.
Ability to manipulate and understand large, complex data sets.
Research/analyze/create data models to drive statistical analysis and results related to confirmed client business questions.
Build analytic models using reinforcement learning techniques to business decisions.
Leverage suite of analytic models (e.g. decision trees, regression models, Bayesian methods) to solve client pain points.
Leverage defined algorithms or statistical techniques to enhance data analysis and explore new outcomes.
Able to present and simplify complex statistical approaches, findings and concepts to the client.
Understand how to present information visually to help one understand the data story.
Design, develop and implement analytics reporting solutions as needed.
",None Found,"Job Title: Data Engineer
Location: Washington, DC
Shift: N/A
Required Security Clearance: None; however, a Public Trust must be obtained prior to starting.
Required Certifications: None
Required Education: Bachelor’s degree in technology or the sciences, or industry-equivalent experience required.
Required Experience: 3+ years’ experience in Oracle Business Intelligence (OBIEE). 3+ years python scripting experience. Experience with manipulating and searching large data sets
Description:
The Data Engineer will be key in providing the client data insights, understanding, and statistical relevant findings within the client data set. This Engineer will be responsible for providing growth of a data science/analytics footprint. The Engineer would be considered in the ""landing team"" of the new analytics work, to be expanded with success, receiving oversight from existing analytics senior management.
Functional Responsibility:
The Data Engineer will provide the following:
Working with the client to define business questions (i.e. problem statements), associated metrics, data challenges, and scope business objectives.
Ability to manipulate and understand large, complex data sets.
Research/analyze/create data models to drive statistical analysis and results related to confirmed client business questions.
Build analytic models using reinforcement learning techniques to business decisions.
Leverage suite of analytic models (e.g. decision trees, regression models, Bayesian methods) to solve client pain points.
Leverage defined algorithms or statistical techniques to enhance data analysis and explore new outcomes.
Able to present and simplify complex statistical approaches, findings and concepts to the client.
Understand how to present information visually to help one understand the data story.
Design, develop and implement analytics reporting solutions as needed.
Qualifications:
Experienced with Oracle Business Intelligence (OBIEE) is a must.
SQL & PLSQL query development experience.
Python scripting experience.
Proven ability to work independently and as a team member.
Good communication (written and oral) and interpersonal skills.
Good organizational, multi-tasking, and time-management skills.
US Citizen in order to obtain the Public Trust.
Preferences:
Experience with implementing data science projects in an agile environment
Experience with R or other similar programming languages
An active Public Trust.
Working Conditions:
Work is typically based in a busy office environment and subject to frequent interruptions. Business work hours are normally set from Monday through Friday 8:00am to 5:00pm, however some extended or weekend hours may be required. Additional details on the precise hours will be informed to the candidate from the Program Manager/Hiring Manager.
Physical Requirements:
None
Background Screening/Check/Investigation:
Successful Completion of a Background Screening/Check/Investigation will be required as a condition of hire.
Employment Type: Full-time / Exempt
Benefits:
Federal Data Systems, LLC offers competitive compensation, a flexible benefits package, career development opportunities that reflect its commitment to creating a diverse and supportive workplace. Benefits include, not all inclusive – Medical, Vision & Dental Insurance, Paid Time-Off & Company Paid Holidays, Personal Development & Learning Opportunities.
Other:
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.

Federal Data Systems, LLC (FEDDATA) is an Equal Opportunity/Affirmative Action Employer. That does not unlawfully discriminate in any of its programs or activities on the basis of race, color, religion, sex, age, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other basis prohibited by applicable law."
134,Data Engineer - Big Data Warehouse Integrator,"Chantilly, VA 20151",Chantilly,VA,20151,None Found,None Found,None Found,None Found,None Found,None Found,"Job Title: Data Engineer - Big Data Warehouse Integrator
Clearance: TS/SCI
Location: Chantilly, VA
Compensation: Excellent Benefits and Salary

Job Description:
As part of a technology architecture team, Candidate will support a DIA sponsored project in support of functional correlation to each of the main data collection and delivery functions. As the project grows and becomes integrated across operational and tactical elements of the DoD, this team will identify established capabilities that can meet use cases as well as missing capabilities that need to be added in order to provide common data usage models. Successful candidate will identify ""Value"" data and create a strategy for integration into the warehouse that includes transforming data into community standard formats and common relational or other structured stores.
Required:
3 years experience handling data transformation
Experience with information transformation in preparation for delivery
Understanding of commercial data wrangling and normalization tools
Experience in configuration management associated with transformation processes
Understanding of scaling for transformation processes
Ability to integrate customer workflow into work products
Knowledge of DOD standards
Excellent communication and collaboration skills
BSc/BA in computer science or relevant field or 5 years industry experience
Also, candidate should have a working knowledge of the following:
Data
Identifies unique datasets within applications
Captures formatting
Identifies expected volumetrics
Identifies all current sharing mechanisms
Makes initial recommendations on polyglot storage for each dataset
Identifies expected resource needs for each dataset
Application
Identification of datasets
Identify transfer mechanisms in existence today
Characterize API's in order to verify CAN (controller area network) support
Identifies changes that need to be made in order to remove user identity requirements for access by other systems
Provides guidance on integration tasks regarding API's and ingest
Mission
Network capacity between sites and mission partners
Bandwidth utilization
Cross domain
Evaluates information to determine nearest node usage and failover / COOP
Identifies changes that need to occur in the data architecture to ensure no operational impact to warfighters during integration
Desired:
Familiarity with DoD, military and intelligence operations
Demonstrated ability to decompose a complex problem into an executable plan


Benefit Summary:
Volant Associates provides an industry-leading benefits package to attract and retain the very best talent within a very competitive recruiting environment and support its employees and their dependents.

100% Volant Paid Standard Benefits:
200% Matching on employee 401k contributions (on up to 5% of employee salary deferral)
20.5 days (164 hours) of Paid Time Off
7 paid holidays per year
Health care insurance for employee and dependents thru UHC
Dental care insurance for employee and dependents thru UHC
Vision care insurance for employee and dependents thru VSP
Life and Personal Accident Insurance ($50k coverage) thru CIGNA
Additional Life and Personal Accident Insurance (1 x salary up to $170k) thru MOO
Short term disability Insurance (60% of earnings up to $2,308 per week) thru CIGNA
Long term disability insurance (60% of earnings up to $10k per month) thru CIGNA
Educational assistance (up to $3,500 per calendar year)
Adoption assistance
Commuter benefits
Training and development opportunities
Cell phone stipend (up to $50 per month)
Corporate laptop computer
Gym access (for Chantilly-based employees)
Additional programs available to all employees:
Heath Savings Account (HSA)
Health care Flexible Spending Account (FSA)
Dependent care Flexible Spending Account (FSA)
Voluntary additional levels of Life and Personal Accident Insurance"
135,Azure Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"About Avanade

Avanade leads in providing innovative digital services, business solutions and design-led experiences for its clients, delivered through the power of people and the Microsoft ecosystem. Our professionals combine technology, business and industry expertise to build and deploy solutions to realize results for clients and their customers. Avanade has 34,000 digitally connected people across 24 countries, bringing clients the best thinking through a collaborative culture that honors diversity and reflects the communities in which we operate. Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com

Why Avanade?

14-time winner of Microsoft Partner of the Year
24,000+ certifications in Microsoft technology
90+ Microsoft partner awards
17 Gold Competencies
3,500 analytics professionals worldwide
1,000 data engineers
Implemented analytics systems for more than 550 clients
400 AI practitioners
300 cognitive service experts

How We Support You:
We believe in gender equity and an inclusive community. We offer a comprehensive benefits package: generous vacation allowance disability coverage, retirement plans, paid maternity and paternity leave, life insurance, hotel and travel discounts, extended benefits to cover items that support your well-being, health, dental and vision insurance, professional development and paid Microsoft certification opportunities.

Role Overview:
As an Azure Data Engineer you will collect, aggregate, store, and reconcile data in support of Client business decisions. You will help design and build data pipelines, data streams, reporting tools, information dashboards, data service APIs, data generators and other end-user information portals and insight tools. You will be a critical part of the data supply chain, ensuring that business partners can access and manipulate data for routine and ad hoc analysis to drive business outcomes using Advanced Analytics.

Key Role Responsibilities:
Day-to-day, you will:
Translate business requirements to technical solutions using strong business insight.
Analyzes current business practices, processes and procedures as well as identifying future business opportunities for demonstrating Microsoft Azure Data & Analytics PaaS Services.
Support the planning and implementation of data design services, providing sizing and configuration assistance and performing needs assessments.
Delivery of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies.
Design and Build Modern Data Pipelines and Data Streams.
Design and Build Data Service APIs.
Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics.
Expose data to end users using Power BI, Azure API Apps or other modern visualization platform or experience.
Implement effective metrics and monitoring processes.
Able to travel approximately 80%

Key Role Skill & Capability Requirements:
Your technical/non-technical skills include:
Demonstrable experience of turning business use cases and requirements to technical solutions.
Experience in business processing mapping of data and analytics solutions.
Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.
The ability to apply such methods to take on business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration.
T-SQL is required.
Knowledge of Azure Data Factory, Azure Data Lake, Azure SQL DW, and Azure SQL, Azure App Service is required.
Experience preparing data for Data Science and Machine Learning.
Knowledge of Lambda and Kappa architecture patterns.
Knowledge of Master Data Management (MDM) and Data Quality tools and processes.
Strong collaboration ethic and experience working with remote teams.
Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals.
Working experience with Visual Studio, PowerShell Scripting, and ARM templates.
Experience with Git/TFS/VSTS is a requirement.

Preferred Certifications:
MCSA

Preferred Education Background:
You likely possess a Bachelor's degree in Computer Science, Information Technology, Business, or another relevant field. An equivalent combination of education and experience will also suffice.

Preferred Years of Work Experience:
You likely have about 3-5+ years of relevant professional experience."
136,Big Data Engineer,"Springfield, VA",Springfield,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Overview
We are seeking aBig Data Engineerto support our customer.
Responsibilities
Designs, modifies, develops, writes and implements software systems. Participates in software and systems testing, validation, and maintenance processes through test witnessing, certification of software, and other activities as directed. Provides support to senior staff on projects/programs. Familiar with standard concepts, practices, and procedures within a variety of fields related to the project. This position takes direction from senior technical leadership.
The Big Data Engineer (BDE) is responsible for building the next generation of web applications and systems focusing on capability delivery to end users. The BDE is a member of a “big data” team of specialist within the multi-disciplinary agile development team. The BDE will manage requirements collection, software design, development and delivery – full lifecycle – in support of analysts. The BDE helps manage effective processes associated with the architecture. The BDE collaborates closely with the Agile Software Developer (ASDs), Technical Targeting Developer (TTDs), and the end user analysts to write and implement cutting edge big data algorithms and analytics. The BDE engages in software solution planning and creation to ensure capabilities are delivered using the latest available technologies and methods. The BDE will operate in a “RAD/JAD” environment in which tasks are rapidly defined and then executed to insure maximum user input, feedback and adoption. The BDE ensures the interoperability of the in-house capability with outside partners.
Qualifications
Minimum Qualifications:
5 years of experience
COMPTIA Security+ certification or CISSP certification
Proficiency in two or more of the following programming languages: C#, Java, .NET, Python, Perl, Ruby, or similar
Familiarity with current Agile methods
Proficiency with the following:
Multiple operating systems including: UNIX, Linux, Windows, Cisco IOS, etc.
Machine learning, data mining, and knowledge discovery
Analytic algorithm design and implementation
ETL processes; including document parsing techniques
Networking, computer, and storage technologies
Using or designing RESTful APIs, SOAP, XML
Developing large cloud software projects, preferably in Java, Python or C++ language
Java/J2EE, multithreaded and concurrency systems
Multi-threaded, big data, distributive cloud architectures and frameworks including Hadoop, MapReduce, Cloudera, Hive, Spark, Elasticsearch, etc. for the purposes of conducting analytic algorithm design and implementation
NoSQL database such as Neo4J, Titan, Mongo, Cassandra, and hBase
AWS Services (EC2, Network, ELB, S3/EBS, etc.)
Processing and managing large data sets (multi PB scale)
Web services environment and technologies such as XML, KML, SOAP, and JSON
Proficiency in trouble-shooting in very complex distributed environments including following stack traces back to code and identifying a root cause

Preferred Qualifications:
Education – Masters Degree in Computer Science or related field (e.g. Statistics, Mathematics, Engineering) – but a technical BS degree will suffice
Distributed computing-based certifications
Proficiency with the following:
Management/tracking utilities such as Jira, Redmine, or similar
Running Internet facing or Service Level Agreement (SLA'd) auto-deployed environments
Real-time media protocols (Real-time Transport Protocol (RTP), Secure Real-time Transport Protocol (SRTP))
Data transfer systems such NiFi
Text processing: NPL, NER, entity retrieval (e.g. Solr/Lucene), topic extraction, summarization, clustering, etc.
Certification from an Agile certified institute, International Consortium for Agile, Scaled Agile Academy, Scrum Alliance, Scrum.org, International Scrum Institute, ScrumStudy, Project Management Institute - Agile Certified Practitioner, or similar XP/Scrum certification or training is desired
Support to SOF; Previous experience with technology, intelligence and cyber under the umbrella of USSOCOM
Education:
Bachelor of Arts or Bachelor of Science in Computer Science or related fields (e.g. Statistics, Mathematics, Engineering), or equivalent in years of experience, or demonstrates adequate knowledge for the position.
Clearance Requirements:
Must have active TS/SCI clearance
Physical Demands -The physical demands described here are representative of those that may need to be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
While performing the duties of this Job, the employee is regularly required to sit and talk or hear. The employee is frequently required to walk; use hands to finger, handle, or feel and reach with hands and arms. The employee is occasionally required to stand; climb or balance and stoop, kneel, crouch, or crawl. The employee must occasionally lift and/or move up to 20 pounds.
HII-MDIS, formerly Fulcrum, Fulcrum is an equal opportunity employer and gives consideration for employment to qualified applicants without regard to race, color, religion, sex, national origin, disability or protected veteran status. EOE of Minorities/Females/Veterans/Disability
“CJ” MON
Job ID2019-1951
of Openings1
CategoryIntelligence
TypeFull Time"
137,Data Engineer - Streaming Platform Integrator,"Chantilly, VA",Chantilly,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Title: Data Engineer - Streaming Platform Integrator
Clearance: TS/SCI
Location: Chantilly, VA
Compensation: Excellent Benefits and Salary

Job Description:
As part of a technology architecture team, support a DIA sponsored project in support of functional correlation to each of the main data collection and delivery functions. As the project grows and becomes integrated across operational and tactical elements of the DoD, this team will identify established capabilities that can meet standardized use cases as well as missing capabilities that need to be added in order to provide common data usage models. Successful candidate will create pre-deployment integration plans that ensure all use cases associated with streaming data will be met by either existing or new products that align with the target environment.

Required:
Minimum of 2 years hands-on experience with streaming technologies in multiple data sets
Strong understanding of Kafka and standards for delivery of streaming data
Ability to work with team to organize the data of the data model
Experience with the queueing up data for streaming functionality
BSc/BA in computer science or relevant field or 8 years industry experience
Also, candidate should have a working knowledge of the following:

Data
Identifies unique datasets within applications
Captures formatting
Identifies expected volumetrics
Identifies all current sharing mechanisms
Makes initial recommendations on polyglot storage for each dataset
Identifies expected resource needs for each dataset
Application
Identification of datasets
Identify transfer mechanisms in existence today
Characterize API's in order to verify CAN (controller area network) support
Identifies changes that need to be made in order to remove user identity requirements for access by other systems
Provides guidance on integration tasks regarding API's and ingest
Mission
Network capacity between sites and mission partners
Bandwidth utilization
Cross domain
Evaluates information to determine nearest node usage and failover / COOP
Identifies changes that need to occur in the data architecture to ensure no operational impact to warfighters during integration
Desired:
Possess a high degree of ingenuity, creativity, and resourcefulness
Familiarity with DoD, military or intelligence community
A VOLANT Associate proudly serves the needs of the Nation's Intelligence Community and matches a very specific and rigorous profile. They are the leaders and the difference makers on a team. A VOLANT Associate is rewarded for being the very best the country has to offer. The financial rewards are commensurate with the level of expertise and experience they have achieved and are virtually unmatched in the industry.

Benefit Summary
Volant Associates provides an industry-leading benefits package to attract and retain the very best talent within a very competitive recruiting environment and support its employees and their dependents.

100% Volant Paid Standard Benefits:
200% Matching on employee 401k contributions (on up to 5% of employee salary deferral)
20.5 days (164 hours) of Paid Time Off
7 paid holidays per year
Health care insurance for employee and dependents thru UHC
Dental care insurance for employee and dependents thru UHC
Vision care insurance for employee and dependents thru VSP
Life and Personal Accident Insurance ($50k coverage) thru CIGNA
Additional Life and Personal Accident Insurance (1 x salary up to $170k) thru MOO
Short term disability Insurance (60% of earnings up to $2,308 per week) thru CIGNA
Long term disability insurance (60% of earnings up to $10k per month) thru CIGNA
Educational assistance (up to $3,500 per calendar year)
Adoption assistance
Commuter benefits
Training and development opportunities
Cell phone stipend (up to $50 per month)
Corporate laptop computer
Gym access (for Chantilly-based employees)
Additional programs available to all employees:
Heath Savings Account (HSA)
Health care Flexible Spending Account (FSA)
Dependent care Flexible Spending Account (FSA)
Voluntary additional levels of Life and Personal Accident Insurance"
138,AWS Data Engineer,"Arlington, VA 22209",Arlington,VA,22209,None Found,"At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.","DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud",None Found,None Found," Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills","Are you ready to step up to the New and take your technology expertise to the next level?

Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.

People in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’

As part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!

The Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.

Role & Responsibilities:
Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.
- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures
on new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)

Basic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.

Candidate Must Have Completed The Following Certifications
§ Certified AWS Developer - Associate
§ Certified AWS DevOps – Professional (Nice to have)
§ Certified AWS Big Data Specialty (Nice to have)

Nice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud
Experience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus

Professional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills

All of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Accenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

Accenture is committed to providing veteran employment opportunities to our service men and women."
139,Data Engineer,"Washington, DC 20036",Washington,DC,20036,None Found,"1+ years of experience working with relational or multi-dimensional databases
Experience developing logical data models within a data warehouse
Experience developing ETL processes
Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2
Demonstrated mastery in database concepts and large-scale database implementations and design patterns
Proven ability to work with users to define requirements and business issues
Excellent analytic and troubleshooting skills
Strong written and oral communication skills
Bachelor’s degree in Computer Science or Computer Engineering
",None Found,"Responsible for data modeling and schema design that will range across multiple business domains within higher education
Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas
Work with clients to research and conduct business information flow studies
Codify high-performing SQL for efficient data transformation
Coordinate work with external teams to ensure a smooth development process
Support operations by identifying, researching and resolving performance and production issues
",None Found,None Found,"About EAB

At EAB, our mission is to make education smarter and our communities stronger. We harness the collective power of more than 1,500 schools, colleges, and universities to uncover and apply proven practices and transformative insights. And since complex problems require multifaceted solutions, we work with each school differently to apply these insights through a customized blend of research, technology, and services. From kindergarten to college and beyond, EAB partners with education leaders, practitioners, and staff to accelerate progress and drive results across three key areas: enrollment management, student success, and institutional operations and strategy.

At EAB, we serve not only our members but each other—that's why we are always working to make sure our employees love their jobs and are invested in their community. See how we've been recognized for this dedication to our employees by checking out our recent awards.

For more information, visit our Careers page.

The Role in Brief

Associate Data Engineer

Are you a data enthusiast who seeks to tease out meaning from complex data flows and assets? Are you a talented problem solver who can transform abstract problems into elegant technical solutions? We are looking for a Data Modeler to join our team of engineers and data analysts focused on designing, creating, and delivering data solutions as part of our state-of-the-art cloud based products. The successful candidate will have the opportunity to build a world-class solution to help our higher education clients solve challenging problems through data.

This role is based in Washington, DC.

Primary Responsibilities:
Responsible for data modeling and schema design that will range across multiple business domains within higher education
Partner with multiple stakeholders including clients, new product development, BI engineers to develop scalable standard schemas
Work with clients to research and conduct business information flow studies
Codify high-performing SQL for efficient data transformation
Coordinate work with external teams to ensure a smooth development process
Support operations by identifying, researching and resolving performance and production issues
Basic Qualifications:
1+ years of experience working with relational or multi-dimensional databases
Experience developing logical data models within a data warehouse
Experience developing ETL processes
Demonstrated mastery in one or more SQL variants: PostgreSQL, MySQL, Oracle, SQL Server, or DB2
Demonstrated mastery in database concepts and large-scale database implementations and design patterns
Proven ability to work with users to define requirements and business issues
Excellent analytic and troubleshooting skills
Strong written and oral communication skills
Bachelor’s degree in Computer Science or Computer Engineering
Ideal Qualifications:
Experience working in an AGILE environment
Experience developing commercial software products
Experience with AWS data warehouse infrastructure (redshift, EMR/spark)
GIT expertise
Master's degree in Computer Science or Computer Engineering
Benefits:

Consistent with our belief that our employees are our most valuable resource, EAB offers a competitive benefits package.

Medical, dental, and vision insurance, dependents eligible
401(k) retirement plan with company match
Generous PTO
Daytime leave policy for community service or fitness activities (up to 10 hours a month each)
Wellness programs including gym discounts and incentives to promote healthy living
Dynamic growth opportunities with merit-based promotion philosophy
Benefits kick in day one, see the full details here.


At EAB, we believe that to fulfill our mission to “make education smarter and our communities stronger” we need team members who bring a diversity of perspectives to the table and a workplace where each team member is valued, respected and heard.

To that end, EAB is an Equal Opportunity Employer, and we make employment decisions on the basis of qualifications, merit and business need. We don’t discriminate on the basis of race, religion, color, sex, gender identity or expression, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law."
140,"Data Engineer - Arlington, VA","Arlington, VA 22203",Arlington,VA,22203,None Found,None Found,None Found,None Found,None Found,None Found,"STR is a government research contractor specializing in advanced research and development for defense, intelligence, and homeland security applications. We pride ourselves in developing cutting-edge technologies with significant and immediate impact on our national security. Our product is our staff; we prioritize cultivating a team of driven and talented scientists and engineers that together culminate into a premier company.
Every Analyst a Scientist - One of our primary goals is to empower intelligence analysts to be able to study their data like scientists. The tools we develop focus on streamlining intelligence analysis through integrated algorithms and software that provide insight into the geopolitical landscape for use in operational intelligence missions around the world.
The Role:
As a Data Engineer, you will be part of a team that transforms large and complex customer data into real-world, high-impact solutions. You will work with researchers and engineers to design and implement solutions to challenging national security problems. You will be responsible for building the software infrastructure to clean, ingest, and expose datasets and algorithms to both developers and end users. You will deploy algorithms, generate workflows, create engineer-facing tools, and design customer-facing prototype systems. You will occasionally travel to customer sites to engage with end users, demonstrate prototypes, and integrate analytics into customer systems. If you would like to help intelligence and defense analysis keep pace with modern machine learning and software techniques, then this role is for you!

Who you are:
A degree in a scientific or engineering field, such as Computer Science, Mathematics, Physics, or Software Engineering
Proficiency with a scientific programming language such as Python, Java, or C++
Experience with database management and common query syntax
Motivated collaborator and excellent communicator of ideas to both technical and non-technical audiences
Knowledge of AWS, Spark, Dask, and/or similar technologies for working with data at scale
US citizen and willing to obtain a U.S. Security Clearance
Even better:
Track record of architecting, developing, deploying, or maintaining enterprise software
Experience with software development best practices and tools
Understanding of web development and visualization technologies, such as d3, Leaflet, Bootstrap, or others
Familiarity with machine learning or statistical modeling techniques
Active U.S. Security Clearance
Compensation:
Competitive salary
Comprehensive benefits (Medical, Dental, Vision, Disability, Life)
401k company match
Competitive and flexible paid time off
Continued higher education reimbursement
Profit sharing (Additional match to 401k)
Phone reimbursement plan
And more!
STR is dedicated to fostering a diverse and inclusive workforce where all employees, regardless of race, ethnicity, gender, neurodiversity, or other personal characteristics, feel valued, included, and empowered to achieve their best. We recognize that each employee’s backgrounds, experiences, and perspectives are essential for providing our customers with innovative solutions to challenging national security problems. STR’s commitment to attracting, retaining, and engaging talented and diverse professionals is demonstrated by our participation, sponsorship, and support in local and national minority organizations.
Applicants must be US Citizens."
141,Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Description
Description
The U.S. Citizenship and Immigration Services (USCIS), Office of Information Technology (OIT) is seeking to acquire highly qualified cross-functional Data Scientists and developers. The Transformation Data Science Services (TDSS) II team will be part of an ecosystem, participating with federal employees, and other contractors, in a team-based Development and Operations (DevOps) approach to deliver business value (the performance measurements, Key Performance Parameters (KPPs) are established in Operational Requirements Document (ORD)) frequently, cost-effectively, responsively, and with high quality.
The TDSS II teams shall provide Data Analytics Services within the DevOps framework; perform analysis of the USCIS Electronic Immigration System (USCIS ELIS) and related data for forecasting and system growth analysis. This analysis is intended to support data-driven decision-making, as well as development of automated dashboards and tools, which provide analytic insight to ELIS program.
Responsibilities: Creating and maintaining Data Lakes and Data Warehouses. Data modeling and modifying database schema design as needed. Building data pipelines to pull together information from different source systems Integrating, consolidating and cleansing data, and structuring it for use in individual analytics applications Perform the full data lifecycle areas of: data ingestion, data aggregation, data correlation and visualization, analytics. Actively engage with end users to understand data-intensive challenges and needs and ensure development enables high performance movement, storage, aggregation, visualization and analytics over datasets from disparate sources. Utilize the ELIS roadmap to conduct an exploratory statistical analysis for each ELIS line of business. Translate business requirements into technical requirements. Extract, transform and load (ETL) structured and unstructured data into Databricks Unified Analytics Platform. Work with data scientists to perform analytics in Databricks Unified Analytics Platform, and version control machine learning models and reports in GitHub. Comply with ELIS DevOps development, testing, and standard operational procedures (SOP). Collaborate with other vendors, stakeholders, and government employees to collect data needed to aid decision-making. Maintain good configuration and data management methods. Use DHS USCIS approved methods and tools.

Qualifications
9 years of experience and at least a Bachelors degree. Should have experience in a data DevOps environment.
Desired Qualifications


Overview
SAIC is a premier technology integrator, solving our nation's most complex modernization and systems engineering challenges across the defense, space, federal civilian, and intelligence markets. Our robust portfolio of offerings includes high-end solutions in systems engineering and integration; enterprise IT, including cloud services; cyber; software; advanced analytics and simulation; and training. We are a team of 23,000 strong driven by mission, united purpose, and inspired by opportunity. Headquartered in Reston, Virginia, SAIC has annual revenues of approximately $6.5 billion. For more information, visit saic.com. For information on the benefits SAIC offers, see Working at SAIC. EOE AA M/F/Vet/Disability"
142,Data Warehouse Architect/Analyst IV,"McLean, VA 22107",McLean,VA,22107,None Found,"5-10 years' experience in software development, systems engineering and/or support of enterprise-scale IT systems for the IC
Experience in developing business processes and concepts of operation (CONOPS)
Experience in requirements development and formal verification
Experience with architecting systems in AWS, preferably C2S
Familiarly with current IC metadata standards such as PUBS.XML
Experience developing formal engineering documentation
Experience in preparing and delivering technical briefings to senior stakeholders
Excellent oral and written communication skills
Requires 8 to 10 years with BS/BA (or equivalent experience (3 years for Bachelors)) or 6 to 8 years with MS/MA or 3 to 5 years with PhD in Computer Science, Information Systems, or a scientific/engineering discipline.
",None Found,"
Develop the security policy to support the distribution of intelligence reporting derived from sensitive technical collection capabilities to authorized users across the Intelligence Community (IC), and
Develop/enhance IT platforms for the secure production, coordination and dissemination of sensitive technical intelligence products.
Develop and refine the concept of operations (CONOPS) for intelligence reporting systems including the production, coordination and dissemination system
Analyze, verify, document, and maintain system, process, and business requirements
Write and manage system and program documentation
",None Found,None Found,"Business Group Highlights
Intelligence

The Intelligence group provides high-end systems engineering and integration products and services, data analytics and software development to national and military intelligence customers. Serving federal agencies and the Intelligence Community for more than 50 years, the Intelligence group helps our clients meet their mission needs by providing trusted advisors, leading-edge technologies, and innovative solutions.

Responsibilities
Specific Job Description: The Data Architect/Engineer will support the Customer's organization responsible for supporting a products and intelligence reporting system. This includes providing a requirements and verification engineering to support the delivery of the project goals by:

Develop the security policy to support the distribution of intelligence reporting derived from sensitive technical collection capabilities to authorized users across the Intelligence Community (IC), and
Develop/enhance IT platforms for the secure production, coordination and dissemination of sensitive technical intelligence products.
Develop and refine the concept of operations (CONOPS) for intelligence reporting systems including the production, coordination and dissemination system
Analyze, verify, document, and maintain system, process, and business requirements
Write and manage system and program documentation
The Data Architect/Engineer is responsible for the development and documentation of the high-level system architecture, to include identification of major system components and interfaces, and the development of metadata standards, metrics, and trace of reporting back to source collections. The Architect/Data Engineer will work closely with the Chief Architect, Chief Data Officer, data stewards, program team members, and the external software development teams to translate business requirements and operations concepts into a high level systems architecture, identify and specify major internal and external interfaces, and ensure data used in the system is properly marked, controlled, accessed, and managed in accordance with applicable guidelines, and mission needs. Specific responsibilities include, but are not limited to:

Analysis of business requirements for architectural impacts
Development of high-level architecture views and descriptions
Identification and specification of major system interfaces
Identification of required customer, organization and IC data marking standards
Development of implementation guidance for applying the required data markings to multiple types of intelligence products
Coordination with data providers and data stewards on implementation of data markings
Development of documentation capturing the architecture and data standards
Development and delivery of engineering briefings to multiple stakeholders.
This position requires a strong background in the architecture of cloud-based enterprise scale IT systems and metadata standards as used in the IC.
General Job Description: Primarily responsible for leading the development of data models to support business intelligence systems. Provides advanced technical support in the research, experimentation, business analysis and use of systems technology including architecture, integration capabilities and database management. Develops the end-to-end vision and the logical design that translates into physical databases, and how the data will flow through the successive stages involved. Knowledgeable and practices a wider range of data administration skills, often in a Business Process Reengineering context. Participates in strategic data planning, including development and implementation of DA policies, standards, and procedures. Understands data from the perspective of data processing and in the context of different life-cycle phases. Activities may include data quality engineering, metadata consolidation and integration, metamodel development and maintenance, repository management, data warehouse design and data mining, data security administration, and formulation of enterprise-specific data metrics.

Qualifications
5-10 years' experience in software development, systems engineering and/or support of enterprise-scale IT systems for the IC
Experience in developing business processes and concepts of operation (CONOPS)
Experience in requirements development and formal verification
Experience with architecting systems in AWS, preferably C2S
Familiarly with current IC metadata standards such as PUBS.XML
Experience developing formal engineering documentation
Experience in preparing and delivering technical briefings to senior stakeholders
Excellent oral and written communication skills
Requires 8 to 10 years with BS/BA (or equivalent experience (3 years for Bachelors)) or 6 to 8 years with MS/MA or 3 to 5 years with PhD in Computer Science, Information Systems, or a scientific/engineering discipline.
About Perspecta
What matters to our nation, is what matters to us. At Perspecta, everything we do, from conducting innovative research to cultivating strong relationships, supports one imperative: ensuring that your work succeeds. Our company was formed to bring a broad array of capabilities to all parts of the public sector—from investigative services and IT strategy to systems work and next-generation engineering.

Our promise is simple: never stop solving our nation’s most complex challenges. And with a workforce of approximately 14,000, more than 48 percent of which is cleared, we have been trusted to just that, as a partner of choice across the entire sector.

Perspecta is an AA/EEO Employer - Minorities/Women/Veterans/Disabled and other protected categories.

Options
Apply for this job onlineApply
Share
Email this job to a friendRefer
Sorry the Share function is not working properly at this moment. Please refresh the page and try again later.
Share on your newsfeed





As a government contractor, Perspecta abides by the following provision
PAY TRANSPARENCY NONDISCRIMINATION PROVISION
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information. 41 CFR 60-1.35(c)"
143,Data Engineer,"Herndon, VA",Herndon,VA,None Found,None Found,"Bachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with a federal government customer.
Experience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB
Experience with big data tools: Hadoop, Spark, Kafka
Experience with data governance tools: Collibra, Immuta
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala
Must possess strong written and verbal communication skills.",None Found,"Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceFamiliarity with enterprise data management, data governance, and data control policiesAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.",None Found,None Found,"LMI is currently seeking a data engineer within LMI’s Advanced Analytics service line to support the implementation of data governance and metadata management tools in support of enterprise data management.

*This position is located at a client site in Reston, VA*
Responsibilities
The ideal candidate will have direct, applied experience with one or more of the following areas:
Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceFamiliarity with enterprise data management, data governance, and data control policiesAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.
Qualifications
Bachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.
Master's degree is desired.
Must have at least 3 years of experience, preferably with a federal government customer.
Experience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB
Experience with big data tools: Hadoop, Spark, Kafka
Experience with data governance tools: Collibra, Immuta
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala
Must possess strong written and verbal communication skills.
#LI-SH1"
144,Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,"
Must have an active/current TS/SCI and be able to pass a CI Poly.
Must have at least five years' experience in technology consulting.
Bachelor's degree or equivalent training and experience. Master's degree preferred with advanced training in information technology.
Experience and knowledge of tools associated with counterintelligence and HUMINT collectors.
Experience in cloud technologies, data layers, microservices.
Experience with the following tools:
AWS, AWS Cloud and C2S
Database experience in SQL/NSQL
Experience and knowledge in software coding and unit level testing including:
Java, python, Ruby R
Knowledge of web services feeds
Must be a diverse thinker and be able to work in a large group setting.
Write and edit technical documents and reports;
Write, edit, and produce contents for contract deliverables: reports, training materials, presentation slides, letters, fact sheets, diagrams, and capability
Effectively communicate project expectations to team members and stakeholders in a timely and clear fashion.
Communicate formally and informally through existing forums to stakeholders at all levels, including senior leadership.
",None Found,"
Develop data flow diagrams depicting data movement through data centric architecture.
Provide recommendation to senior leadership to simplify end user processes and create better efficiencies.
Translates customer requirements into formal agreements and plans to culminate in customer acceptance or results.
Execute a wide range of process activities beginning with the request for proposal through development, test and final delivery. Anticipates future customer, industry and business trends.
Challenges the validity of given procedures and processes with a view toward enhancements or improvement. Creates innovative solutions to problems involving finance, scheduling, technology, methodology, tools and solution components. Leads team on large complex projects.
Provide capability to ingest and extract data.
Create repeatable, reusable procedures.
Deliver common services in support of architecture roadmap and Agency direction.
Work with a team to design, implement, and maintain new systems.
Provide guidance to the customer on best-practices.
Perform other duties as assigned.
",None Found,"
Travel may be required both inside and outside the Washington National Capital Region (NCR) and worldwide.
","In a world where there is seemingly an infinite number of Government Services firms, we strive to not just be ""a"" place to work, but to be ""the"" place to work! Here at ASET Partners we are looking for a highly skilled, exceptionally motivated Data Engineer to help support the Department of Defense, intelligence community, and federal civilian agencies
Responsibilities: .

Develop data flow diagrams depicting data movement through data centric architecture.
Provide recommendation to senior leadership to simplify end user processes and create better efficiencies.
Translates customer requirements into formal agreements and plans to culminate in customer acceptance or results.
Execute a wide range of process activities beginning with the request for proposal through development, test and final delivery. Anticipates future customer, industry and business trends.
Challenges the validity of given procedures and processes with a view toward enhancements or improvement. Creates innovative solutions to problems involving finance, scheduling, technology, methodology, tools and solution components. Leads team on large complex projects.
Provide capability to ingest and extract data.
Create repeatable, reusable procedures.
Deliver common services in support of architecture roadmap and Agency direction.
Work with a team to design, implement, and maintain new systems.
Provide guidance to the customer on best-practices.
Perform other duties as assigned.

Qualifications:

Must have an active/current TS/SCI and be able to pass a CI Poly.
Must have at least five years' experience in technology consulting.
Bachelor's degree or equivalent training and experience. Master's degree preferred with advanced training in information technology.
Experience and knowledge of tools associated with counterintelligence and HUMINT collectors.
Experience in cloud technologies, data layers, microservices.
Experience with the following tools:
AWS, AWS Cloud and C2S
Database experience in SQL/NSQL
Experience and knowledge in software coding and unit level testing including:
Java, python, Ruby R
Knowledge of web services feeds
Must be a diverse thinker and be able to work in a large group setting.
Write and edit technical documents and reports;
Write, edit, and produce contents for contract deliverables: reports, training materials, presentation slides, letters, fact sheets, diagrams, and capability
Effectively communicate project expectations to team members and stakeholders in a timely and clear fashion.
Communicate formally and informally through existing forums to stakeholders at all levels, including senior leadership.

Travel Requirements:

Travel may be required both inside and outside the Washington National Capital Region (NCR) and worldwide.

ASET Snapshot:

We are a growing IT consulting and professional services firm that combines large-business experience with small-business efficiency and ingenuity
Founded in 2008
We are a HUBZone certified small business
Profitable from day one, zero debt
Offices in Alexandria, VA and Baltimore, MD
Over 75 employees and growing
Over 15 active contracts supporting 10+ Federal Agencies
Our mission is to drastically change our client's expectations... one program at a time

Top 10 Reasons Our Employees Love Working at ASET:
1. Outstanding benefits! Including: health, dental, and vision care, a 401k retirement plan with up to a 4% company paid contribution, and employee achievement and merit awards to name a few.
2. We have very high hiring standards, so their co-workers are just as smart and talented as they are.
3. We are not profit driven; we are results driven and believe that profits will follow.
4. Unlike Office Space, our employees do not have eight bosses. They have just one or two, and they are not micromanagers. It's not our style and we just don't have time for it!
5. We're different than the rest. Our employees love our unique small-company culture.
6. We have established a trusted relationship with all our clients and refuse to make recommendations that aren't in our clients' best interests.
7. They feel appreciated and recognized for the contributions they make each day.
8. We offer a healthy work-life balance, flexible schedules, and competitive pay.
9. They love that we have been constantly growing. It's exciting to add new folks and it offers many opportunities for career advancement and growth.
10. We work hard, and we play hard, too! We host quarterly company-wide lunches, regular happy hours and an annual New Year's party in January as a way of saying 'thank you' to our hard-working staff.
ASET Partners Corporation is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class."
145,Senior Data Engineer,"McLean, VA 22102",McLean,VA,22102,None Found,"4+ years of full-time experience in software development including design, coding, testing, and support
At least 1 year of Cloud infrastructure experience working with one or more of the following Amazon Web Services (AWS) Cloud services: EC2, EMR, ECS, S3, SNS, SQS, Cloud Formation, Cloud watch, Lambda
Hands-on experience with AWS architecture design, Data Management, Big Data, and Data Warehousing
Experience ofwith data application and data product development
2+ years of experience with Agile, Kanban, or Scrum methodologies","Master's Degree in Computer Science or related fields
Proficient with CICD process, Agile and DevOps Software Development Life Cycle including analysis, high level design, coding, testing, and implementation, performance tuning, bug fixing and quality control
Experience building data lakes in AWS Cloud, moving Data applications to the Cloud, and developing cloud native Data applications
Expertise in creating data models, optimizing data, automating & restructuring data reporting system in financial services domain
2+ years of experience in big data technologies (Spark, Hadoop, HDFS, MongoDB, PostGre SQL)
3+ years of experience using Java, Python or Scala
Experience leading complex data applications with large volumes of data",None Found,None Found,None Found,"Do you want to be part of the exciting journey of Cloud Transformation? Are you a seasoned data engineer that thrives in an innovative and collaborative team? An advanced coder who enjoys a rapid, dynamic environment? If you have proven Data Engineering experience with AWS Cloud technologies and would like to help us build an Enterprise Data Lake in the Cloud, apply to join Freddie Mac’s Emerging Data Technologies team.

Your work falls into 2 primary categories:

Development & Execution
Demonstrate effective and disciplined software development expertise
Deliver solutions on-time with high bar of quality, and continuously improve software engineering practices
Work with Product Owners to understand the desired capability, to define and prioritize work, determine deliverables, and manage workloads
Involved in application development, prototyping, modeling and technical consulting
Actively facilitates issue resolution and issue tracking. Identifies mitigation steps and ensures risks and issues are mitigated/resolved in a timely manner

Technical Leadership
Leads efforts in data, code, and systems analysis to ensure accuracy and completeness of requirements
Leads the design process and evaluates alternative solutions relating to usability, security, scalability, failover, and performance.
Supports the Development Tech Lead and/or Project Manager in managing projects / Agile Sprints
Performs and leads thorough Unit testing and Integration testing, including test data creation for various test scenarios
Mentor and coach intermediate Developers on both technical and soft skills
Qualifications
4+ years of full-time experience in software development including design, coding, testing, and support
At least 1 year of Cloud infrastructure experience working with one or more of the following Amazon Web Services (AWS) Cloud services: EC2, EMR, ECS, S3, SNS, SQS, Cloud Formation, Cloud watch, Lambda
Hands-on experience with AWS architecture design, Data Management, Big Data, and Data Warehousing
Experience ofwith data application and data product development
2+ years of experience with Agile, Kanban, or Scrum methodologies

Key to success in this role
Strong working knowledge and technical competencies of AWS
Ability to communicate clearly, effectively, persuasively with technology and business stakeholders
Ability to develop mutually beneficial relationships inside and outside of the division, work and collaborate effectively in a team environment
Sense of urgency to delivery and able to apply risk-based approach to prioritize work
Strong work ethic, self-motivated, independent, and works with minimal direction
Motivated to learn new technologies and identify process improvements and efficiencies
Ability to adapt to change while continuing to deliver on assigned objectives
Ability to use data to help inform strategy and direction

Top Personal Competencies to possess
Drive for Execution – Be accountable for strong individual and team performance
Partnership – Build trust and strong partnerships through your own and team’s actions
Growth and Development – Know or learn what is needed to deliver results and successfully compete
Preferred Skills
Master's Degree in Computer Science or related fields
Proficient with CICD process, Agile and DevOps Software Development Life Cycle including analysis, high level design, coding, testing, and implementation, performance tuning, bug fixing and quality control
Experience building data lakes in AWS Cloud, moving Data applications to the Cloud, and developing cloud native Data applications
Expertise in creating data models, optimizing data, automating & restructuring data reporting system in financial services domain
2+ years of experience in big data technologies (Spark, Hadoop, HDFS, MongoDB, PostGre SQL)
3+ years of experience using Java, Python or Scala
Experience leading complex data applications with large volumes of data
Today, Freddie Mac makes home possible for one in four home borrowers and is one of the largest sources of financing for multifamily housing. Join our smart, creative and dedicated team and you’ll do important work for the housing finance system and make a difference in the lives of others. Freddie Mac is an equal opportunity and top diversity employer. EOE, M/F/D/V."
146,ISR Data Engineer,"Arlington, VA 22209",Arlington,VA,22209,None Found,None Found,"Bachelor’s Degree from an accredited college or university is required; Bachelor’s degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics (STEM) degrees or a Master’s Degree is highly preferred.
Minimum of 5 years of general experience in the military or intelligence community is required.
Minimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.
Minimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level (3/4 star officer or SES-3/4) is preferred; at least 3 years of experience with OUSD(I) is highly preferred.
Minimum of 3 years of experience using scripting (e.g. Python, R, VBA) or programming languages (e.g. Java, C++, Ruby) to deliver data engineering / software development services is required.
Minimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.
Machine Learning: e.g. TensorFlow, PyTorch, Keras
Data Visualization: e.g. Tableau, D3, Kibana
Geospatial Analysis: e.g. ArcGIS, R
Web Services: e.g. SOAP, RESTful
Web Development: e.g. JavaScript, React.js, Node.js, HTML
Database Development: e.g. MongoDB, PostgreSQL, MS-SQL
Active TS/SCI security clearance.
","Simultaneously support 2-3 ISR Analytic studies with analytic expertise, project-specific web pages, on-demand ETL and data analysis, and the development of web-enabled analytic tools and dashboards.
Continuously supplement, improve, and maintain ISR Data Enrichment and Aggregation (IDEA), a JWICS-based, end-to-end automated ISR data capability, according to OUSD(I) and stakeholder priorities and agile development principles.
Continuously supplement, improve, and maintain a classified studies and analysis website including project repositories, analytic apps, and dynamic decision-support dashboards.
Design, build, and/or insert new technology into extant classified development and production architecture baseline to supplement and improve analytic output, algorithmic performance, and user experience across websites and APIs.
Design and develop custom scripts and tools on SIPRNET and JWICS to solve emergent analytic challenges and/or answer quick-turn or recurring senior executive questions about ISR performance and effectiveness.
Develop custom algorithms to ingest and transform SIPRNET- and JWICS-derived data, artifacts, and information into analytic-ready data.
Leverage advanced analytic techniques, including, but not limited to, geospatial analysis, regression analysis, machine learning, and natural language processing, to derive insight about ISR performance and effectiveness.
Perform DevOps across JWICS and SIPR development architectures containing multiple database clusters, web servers, load balancers, and virtual machine instances, and optimize the deployment and maintenance pipelines for all architectural components in support of IDEA and Data Engineering efforts.
Engage with the ISR community to understand analytic needs, raise awareness of ongoing work, seek feedback on in-progress innovations, and develop one-off analytics aids.
Support technical briefings on analytic methodologies, decision-support dashboards, and capability innovations.
The work environment for this position requires an individual to be able to:
Work sitting or standing at a desk or conference table for extended periods of time with the ability to shift positions while working: sit, stand, pace, adjust positioning in any of those without issue
Walk in the office to collaborate with co-workers, attend meetings or retrieve documents from printer
Must be able to lift and carry up to 10 lbs.
","Bachelor’s Degree from an accredited college or university is required; Bachelor’s degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics (STEM) degrees or a Master’s Degree is highly preferred.
Minimum of 5 years of general experience in the military or intelligence community is required.
Minimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.
Minimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level (3/4 star officer or SES-3/4) is preferred; at least 3 years of experience with OUSD(I) is highly preferred.
Minimum of 3 years of experience using scripting (e.g. Python, R, VBA) or programming languages (e.g. Java, C++, Ruby) to deliver data engineering / software development services is required.
Minimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.
Machine Learning: e.g. TensorFlow, PyTorch, Keras
Data Visualization: e.g. Tableau, D3, Kibana
Geospatial Analysis: e.g. ArcGIS, R
Web Services: e.g. SOAP, RESTful
Web Development: e.g. JavaScript, React.js, Node.js, HTML
Database Development: e.g. MongoDB, PostgreSQL, MS-SQL
Active TS/SCI security clearance.
","Bachelor’s Degree from an accredited college or university is required; Bachelor’s degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics (STEM) degrees or a Master’s Degree is highly preferred.
Minimum of 5 years of general experience in the military or intelligence community is required.
Minimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.
Minimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level (3/4 star officer or SES-3/4) is preferred; at least 3 years of experience with OUSD(I) is highly preferred.
Minimum of 3 years of experience using scripting (e.g. Python, R, VBA) or programming languages (e.g. Java, C++, Ruby) to deliver data engineering / software development services is required.
Minimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.
Machine Learning: e.g. TensorFlow, PyTorch, Keras
Data Visualization: e.g. Tableau, D3, Kibana
Geospatial Analysis: e.g. ArcGIS, R
Web Services: e.g. SOAP, RESTful
Web Development: e.g. JavaScript, React.js, Node.js, HTML
Database Development: e.g. MongoDB, PostgreSQL, MS-SQL
Active TS/SCI security clearance.
","ISR Data Engineer
About the Organization
Now is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.

Position Description
Redhorse Corporation is looking for an Intelligence, Surveillance and Reconnaissance (ISR) Data Engineer to support the ISR Operations Division within the Warfighter Support Directorate of the Office of the Under Secretary of Defense for Intelligence (OUSD(I)) on a multi-year contract. The Data Engineering team establishes data capture and storage mechanisms for ISR operations data, develops and implements data mining, advanced data analytics, visualization and other quantitative data science measurement techniques against ISR processes and products. These capabilities directly support government understanding and data-driven decision-making.

Basic Minimum Requirements for Skills, Experience, Education and Credentials include:
Bachelor’s Degree from an accredited college or university is required; Bachelor’s degree in Operations Research or other applicable Science, Technology, Engineering, and Mathematics (STEM) degrees or a Master’s Degree is highly preferred.
Minimum of 5 years of general experience in the military or intelligence community is required.
Minimum of 3 years of experience in applying data engineer or software development skills to develop advanced analytic tools, deliver mission support services and create data visualization capabilities for the analysis of ISR or intelligence systems is required.
Minimum of 3 years of experience working in an intelligence policy and oversight organization led at the senior executive-level (3/4 star officer or SES-3/4) is preferred; at least 3 years of experience with OUSD(I) is highly preferred.
Minimum of 3 years of experience using scripting (e.g. Python, R, VBA) or programming languages (e.g. Java, C++, Ruby) to deliver data engineering / software development services is required.
Minimum of 3 years of experience using at least one of the following technologies to deliver data engineering / software development services is required.
Machine Learning: e.g. TensorFlow, PyTorch, Keras
Data Visualization: e.g. Tableau, D3, Kibana
Geospatial Analysis: e.g. ArcGIS, R
Web Services: e.g. SOAP, RESTful
Web Development: e.g. JavaScript, React.js, Node.js, HTML
Database Development: e.g. MongoDB, PostgreSQL, MS-SQL
Active TS/SCI security clearance.
Preferred Duties and Responsibilities for the Analyst include:
Simultaneously support 2-3 ISR Analytic studies with analytic expertise, project-specific web pages, on-demand ETL and data analysis, and the development of web-enabled analytic tools and dashboards.
Continuously supplement, improve, and maintain ISR Data Enrichment and Aggregation (IDEA), a JWICS-based, end-to-end automated ISR data capability, according to OUSD(I) and stakeholder priorities and agile development principles.
Continuously supplement, improve, and maintain a classified studies and analysis website including project repositories, analytic apps, and dynamic decision-support dashboards.
Design, build, and/or insert new technology into extant classified development and production architecture baseline to supplement and improve analytic output, algorithmic performance, and user experience across websites and APIs.
Design and develop custom scripts and tools on SIPRNET and JWICS to solve emergent analytic challenges and/or answer quick-turn or recurring senior executive questions about ISR performance and effectiveness.
Develop custom algorithms to ingest and transform SIPRNET- and JWICS-derived data, artifacts, and information into analytic-ready data.
Leverage advanced analytic techniques, including, but not limited to, geospatial analysis, regression analysis, machine learning, and natural language processing, to derive insight about ISR performance and effectiveness.
Perform DevOps across JWICS and SIPR development architectures containing multiple database clusters, web servers, load balancers, and virtual machine instances, and optimize the deployment and maintenance pipelines for all architectural components in support of IDEA and Data Engineering efforts.
Engage with the ISR community to understand analytic needs, raise awareness of ongoing work, seek feedback on in-progress innovations, and develop one-off analytics aids.
Support technical briefings on analytic methodologies, decision-support dashboards, and capability innovations.
The work environment for this position requires an individual to be able to:
Work sitting or standing at a desk or conference table for extended periods of time with the ability to shift positions while working: sit, stand, pace, adjust positioning in any of those without issue
Walk in the office to collaborate with co-workers, attend meetings or retrieve documents from printer
Must be able to lift and carry up to 10 lbs.
Redhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse’s changing needs.
This job description is not a contract and may be adjusted as deemed appropriate in Redhorse’s sole discretion.

EOE/M/F/Vet/Disabled"
147,Map Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for a Map Data Engineer to help our partners build rich maps of the planet. You will build tools to collect, manage, distribute and analyze map data—from road networks to points-of-interest, to farm fields. You will support projects such as: deploy AI-assisted mapping pipelines, gamification of mapping and map validation, help governments use collaborative mapping to manage public assets, develop new approaches to automated error detection, and help city planners to track how their city is changing in real-time. You will contribute to a growing ecosystem of opensource tools within OpenStreetMap, FOSS4G and other open geospatial communities. You will solve real problems for organizations that address big global challenges. You will build the best technology available with a group of people that want you to grow and win.

As a Map Data Engineer you will:

Learn and contribute to open source software to collect, manage, distribute, and analyze map data. Development Seed is an active supporter of the OpenStreetMap community. You are likely to contribute to software in the OSM ecosystem.
Build new tools to help both mappers and project managers determine what to map, where to map and when to map.
Solve complex and large-scale vector data analysis problems for governments and other institutions.

At Development Seed you will:

Collaborate — Working as a team makes us stronger than any individual developer. You write clear Github tickets and communicate effectively on Slack and in-person. You’re passionate about teaching and helping your team members grow from day one.
Learn — We constantly evolve our technology stack and techniques to deliver the best work to our partners. You don’t need to know any particular language or framework upfront but you need to demonstrate you’re able and excited to learn new ways to build. You’ve also tried out enough options to know that the hip new thing isn’t always the best solution.
Code — You write code focusing on both performance and maintainability. You know when to use a quick fix and when to invest more time refactoring.
Care about the world and believe that we can do better — Social change is the foundation of everything we do. You are impatient about solving the world’s toughest challenges.

Your experience

We are looking for candidates with demonstrated proficiency in:

Javascript and Node.js
modern spatial technologies and standards like Mapbox Vector Tiles
mapping in OpenStreetMap and software around the project
geospatial data tools like GDAL, QGIS
databases like PostgreSQL
HTML, CSS. React is a plus
Amazon Web Services (EC2, ECS, S3, Lambda functions); and
documentation and mentorship.

Ideal candidates are based in DC, though we are open to remote roles. Current work with government agencies requires US Citizenship for this position.

If this sounds like you, apply below with your resume. Tell us about yourself and what you’d love to work on at Development Seed.

Not sure you tick all the boxes? We encourage you to apply - we have a culture of learning, and if this job description gets you excited, we want to hear from you.

We focus on equality and believe deeply in diversity of race, gender, sexual orientation, religion, ethnicity, and all the other fascinating characteristics that make us different."
148,"Data Engineer, Manager - Machine Learning (Python, AWS, Scala) - Card Tech","McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer, Manager - Machine Learning (Python, AWS, Scala) - Card Tech

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.
As a Capital One Data Engineer, Manager, you'll be part of a team that’s building new analytical and machine learning tools and frameworks to exploit advantages in the latest developments in cloud computing - EMR, Airflow, SageMaker, etc. You will participate in detailed technical design, development and implementation of applications used by our data scientists and business analysts to build and launch models, analyze data, and make million dollar decisions. We work closely with our users and are looking for people who are excited to iterate quickly on solutions and see the impact in days, not months.
Who You Are

You yearn to be part of cutting edge, high profile projects and are motivated by delivering world-class solutions on an aggressive schedule

Someone who is not intimidated by challenges; thrives even under pressure; is passionate about their craft; and hyper focused on delivering exceptional results

You love to learn new technologies and mentor engineers to raise the bar on your team

It would be awesome if you have a robust portfolio on Github and/or open source contributions you are proud to share

Passionate about intuitive and engaging technical and user interfaces, as well as new/emerging concepts and techniques.
The Job

Collaborating as part of a cross-functional Agile team to create and enhance software that enables state of the art data science, machine learning, and data analysis

Developing and deploying machine learning pipelines using cutting edge tools like Airflow, Dask, and Scikit-Learn

Developing frameworks to accelerate the model development lifecycle

Designing pragmatic, evolutionary architectures for new systems that support data science and analytical needs

Utilizing programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake

Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment and Test Automation to enable the rapid delivery of working code utilizing tools like Jenkins, Terraform, Git and Docker

Provide feedback and mentorship to engineers - both in code reviews and in person - to ensure the team is shipping high quality code and junior engineers are growing in their career

Basic Qualifications

Bachelor’s Degree or Military Experience

At least 4 years of professional work experience as a software or data engineer in an agile environment

At least 2 years of experience in open source programming languages

At least 1 year of experience working with cloud capabilities

Preferred Qualifications

Master's Degree or PhD

6+ years of experience in Python, Scala, or R for large scale data analysis

6+ years' experience with Relational Database Systems and SQL (PostgreSQL or Redshift)

6+ years of UNIX/Linux experience

3+ years of data modeling experience

2+ years of experience with Cloud computing (AWS)

2+ years of experience with Spark

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
149,Data Engineer - Batch Capability Specialist,"Chantilly, VA",Chantilly,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Job Title: Data Engineer - Batch Capability Specialist
Clearance: TS/SCI
Location: Chantilly, VA
Compensation: Excellent Benefits and Salary

Job Description:
As part of a technology architecture team, support a DIA sponsored project in support of functional correlation to each of the main data collection and delivery functions. As the project grows and becomes integrated across operational and tactical elements of the DoD, this team will identify established capabilities that can meet standardized use cases as well as missing capabilities that need to be added in order to provide common data usage models. Successful candidate will create pre-deployment integration plans that ensure all use cases associated with batch data will be met by either existing or new products that align with the target environment. Additionally, candidate will align storage management practices with local and remote storage retention policies.

Required:
Experience with data engineering tools and techniques
Understanding of database architecture
Data models
An analytical mindset with problem-solving skills
Excellent communication and collaboration skills
BSc/BA in computer science or relevant field
Also, candidate should have a working knowledge of the following:

Data
Identifies unique datasets within applications
Captures formatting
Identifies expected volumetrics
Identifies all current sharing mechanisms
Makes initial recommendations on polyglot storage for each dataset
Identifies expected resource needs for each dataset
Application
Identification of datasets
Identify transfer mechanisms in existence today
Characterize API's in order to verify CAN (controller area network) support
Identifies changes that need to be made in order to remove user identity requirements for access by other systems
Provides guidance on integration tasks regarding API's and ingest
Mission
Network capacity between sites and mission partners
Bandwidth utilization
Cross domain
Evaluates information to determine nearest node usage and failover / COOP
Identifies changes that need to occur in the data architecture to ensure no operational impact to warfighters during integration
Desired:
Database Warehouse
Data Mining
Statistical modeling and regression analysis
Knowledge of languages, especially R, SAS, Python, C/C++, Ruby Perl, Java, and MATLAB
Database solution languages, especially SQL, Cassandra, Bigtable, or similar
Hadoop-based analytics, such as HBase, Hive, Pig, and MapReduce
Operating systems, especially UNIX, Linux, and Solaris
Machine learning, including AForge.NET and Scikit-learn
Possess a high degree of ingenuity, creativity, and resourcefulness
Ability to understand military and intelligence operations as described by military experts and to represent their employment (verbally, graphically, and in simulations)
Prior military or intelligence community experience


Benefit Summary:
Volant Associates provides an industry-leading benefits package to attract and retain the very best talent within a very competitive recruiting environment and support its employees and their dependents.

100% Volant Paid Standard Benefits:
200% Matching on employee 401k contributions (on up to 5% of employee salary deferral)
20.5 days (164 hours) of Paid Time Off
7 paid holidays per year
Health care insurance for employee and dependents thru UHC
Dental care insurance for employee and dependents thru UHC
Vision care insurance for employee and dependents thru VSP
Life and Personal Accident Insurance ($50k coverage) thru CIGNA
Additional Life and Personal Accident Insurance (1 x salary up to $170k) thru MOO
Short term disability Insurance (60% of earnings up to $2,308 per week) thru CIGNA
Long term disability insurance (60% of earnings up to $10k per month) thru CIGNA
Educational assistance (up to $3,500 per calendar year)
Adoption assistance
Commuter benefits
Training and development opportunities
Cell phone stipend (up to $50 per month)
Corporate laptop computer
Gym access (for Chantilly-based employees)
Additional programs available to all employees:
Heath Savings Account (HSA)
Health care Flexible Spending Account (FSA)
Dependent care Flexible Spending Account (FSA)
Voluntary additional levels of Life and Personal Accident Insurance"
150,Big Data Engineer,"Chantilly, VA",Chantilly,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"OVERVIEW
Technology is constantly changing, and our adversaries are digitally “going dark” at a rate that is exceeding law enforcement’s ability to keep pace. Those charged with protecting the United States are not always able to access the evidence needed to prosecute crime and prevent terrorism. The Government has trusted in Peraton to provide the technical ability, tools, and resources to bring criminals to justice. In response to this challenge, we are seeking a talented Big Data Engineer.
RESPONSIBILITIES
What you’ll do…
Provide proven, industry leading Big Data Extraction, Transformation, and Load experience coupled with enterprise search capabilities to solve Big Data challenges
Work on a team leveraging Apache NiFi to develop and maintain workflows that load diverse data sets into a data lake leveraging the following technologies:
Apache NiFi (in a cluster configuration)
Git
Python
Zookeeper
Kafka
Hadoop
Spark
Accumulo
Groovy
MySql
Cygwin
Java
QUALIFICATIONS
You’d be a great fit if…
You’ve obtained a BS degree and have eight (8) years of relevant experience. However, equivalent experience may be considered in lieu of degree.
You have three (3) or more years of experience with:
PL/SQL, SQL
Oracle 11g and 12c
Informatica, XML, XSLT, Java, web services
SVN, RCS, Git, OLS Security, JIRA
Sun Solaris OS, Linux (CentOS, Red Hat), and Windows
You have two (2) years or more experience using ETL tools to perform data cleansing, data profiling, transforming, and scheduling various workflows
You have a current Top Secret security clearance with SCI eligibility and the ability to obtain a polygraph

It would be even better if you…
Have hands on experience with any of the following technologies:
Atlassian Suite: Jira, Confluence, Bitbucket, Bamboo
VMWare Player
Linux, specifically CentOS
Linux scripting
AWK, PERL, BASH or other scripting language
SOLR
Jenkins configuration to perform O&M operations
Spark

 What you’ll get…
An immediately-vested 401(K) with employer matching
Comprehensive medical, dental, and vision coverage
Tuition assistance, financing, and refinancing
Company-paid infertility treatments
Cross-training and professional development opportunities
Influence major initiatives
This position requires the candidate to have a current Top Secret security clearance and the ability to obtain a polygraph. Candidate must possess SCI eligibility.
ABOUT PERATON
Are you ready to join the next-generation of national security? Peraton is a fresh name in the industry with an established portfolio and legacy going back more than a century. We work differently than our peers – with agility, the freedom to innovate, an entrepreneurial spirit and a culture of responsibility. As part of the Peraton team, you’ll be part of our movement to build a great company, solve the most daunting challenges facing mankind today, to protect and promote freedom around the world, and to secure our future, for our families, our communities, our nation, and our way of life.
EEO STATEMENT
We are an Equal Opportunity/Affirmative Action Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law."
151,"Training Solutions Advisor, Google Cloud","Reston, VA",Reston,VA,None Found,None Found,None Found,None Found,"
Understand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.
Connect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.
Partner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.
Support escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.
Partner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.
",None Found,None Found,"Note: By applying to this position your application is automatically submitted to the following locations: Sunnyvale, CA, USA; Austin, TX, USA; New York, NY, USA; Reston, VA, USA
Minimum qualifications:

Bachelor's degree in Computer Science or a related technical field, or equivalent practical experience.
5 years of experience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.
Experience working in a customer facing environment in a technology company and helping customers identify solutions that best fit their unique needs.
Ability to travel to support Customer Engagements up to 30% of the time.

Preferred qualifications:

Google Cloud Certified e.g. Cloud Architect, Data Engineer or Associate Cloud Engineer or other comparable Cloud Certification.
Experience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.
Ability to take customers’ technical requirements and architect a proposal that maps to technical skills required.
Ability to quickly learn and understand new training offerings.
Ability to work well cross functionally and understand when to pull in specialist knowledge into Customer conversations.
Excellent communication and presentation skills.
About the job
The Google Cloud team helps customers transform and evolve their business through the use of Google’s global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you'll help shape the future of businesses of all sizes and enable them to better use technology to drive innovation.This role will enable you to make a huge impact across Google Cloud’s most strategic accounts and ensure they have Learning Plans that effectively help them develop the knowledge and skills they need to adopt Google Cloud. The role is also an exciting mix of elements as you will be working in Technical Customer Facing activities (usually in partnership with the CE or PSO/TAM Account owner), working in partnership with Cloud Learning GTM leads on Learning Plan development, and working with the Curriculum Tech leads to provide curriculum feedback and validate proposals as required.

Google Cloud helps millions of employees and organizations empower their employees, serve their customers, and build what’s next for their business — all with technology built in the cloud. Our products are engineered for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. And our teams are dedicated to helping our customers and developers see the benefits of our technology come to life.
Responsibilities
Understand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.
Connect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.
Partner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.
Support escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.
Partner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.
At Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form."
152,Data Engineer,"Rockville, MD",Rockville,MD,None Found,None Found,None Found,None Found,"Design and implement ETL strategies for diverse sets of structured and unstructured data.
Manage cloud-based Linux servers.
Analyze data to better understand their features and relationships between variables.
Work in a highly collaborative manner with members of a close-knit team while exhibiting independent and creative thought.
",None Found,"Bachelor's degree in Computer Science or related discipline, e.g. Information Science, Statistics, or Mathematics. Master s degree is preferred.
Experience with data integration, harmonization, and curation is desired.
Must be competent with Python, SQL, and Linux.
Programming experience with Apache Spark, Java, C/C++ in an agile development environment is a plus.
Research experience is a plus.
Must have excellent oral and written communication skills and be able to work collaboratively.
Promising applicants will be tested for programming competence during the interview process.
","NET ESOLUTIONS CORPORATION (NETE) is a multi-award winning company founded in 1999. NETE is a full service Information Technology (IT) company dedicated to providing value focused services to the Federal Government and the Biomedical Research and Health IT Sector. NETE offers a collaborative working environment where growth is encouraged and nurtured. In addition, we offer competitive salaries that may include performance bonuses and a comprehensive benefits package.

Job Description

At NETE, data is important to us and to our customers. We process large volumes of data and transform it into information that powers decisions for thousands of researchers, scientists, and/or medical professionals. Our work has significant impact on the medical and scientific communities we serve. Your work here matters and has real impact. If you want to learn, grow, and help then this is the job for you.

NETE is seeking a highly motivated Data Engineer to work in a collaborative, intellectually-challenging environment on (i) an exciting research project on research evaluation in collaboration with a global corporation (ii) a second project developing a production database of linkages between biomedical research output such as scientific publications, patents, devices, therapeutics, and grant records at a major federal agency.
This is a 12 month position with no possibility for extension. OPT/CPT candidates are encouraged to apply!
The position incorporates elements of the roles of a data engineer, data wrangler, data scientist, and a junior data architect.
The position is not intended to be a training experience for candidates with weak skills.
The incumbent will, however, gain advanced skills and research experience eventually becoming competitive for elite positions in the national job market.
Responsibilities
Design and implement ETL strategies for diverse sets of structured and unstructured data.
Manage cloud-based Linux servers.
Analyze data to better understand their features and relationships between variables.
Work in a highly collaborative manner with members of a close-knit team while exhibiting independent and creative thought.
Job Requirements
Bachelor's degree in Computer Science or related discipline, e.g. Information Science, Statistics, or Mathematics. Master s degree is preferred.
Experience with data integration, harmonization, and curation is desired.
Must be competent with Python, SQL, and Linux.
Programming experience with Apache Spark, Java, C/C++ in an agile development environment is a plus.
Research experience is a plus.
Must have excellent oral and written communication skills and be able to work collaboratively.
Promising applicants will be tested for programming competence during the interview process.
Benefits
Paid Time Off (PTO)
9 Paid Federal holidays
Various wellness programs
Free parking at corporate offices
Employee Referral Bonus Program (ERBP)
Vision coverage through UHC national network
Dental coverage through UHC national network
401(K) with significant company match & no vesting period
Short and Long-Term Disability coverage (paid by company)
Competitive salaries with opportunity for performance bonuses
Discount plan for pet care, legal services, & identify theft protection
Basic Life and AD&D coverage (paid by company; option to purchase additional coverage)
Medical coverage through UHC national network (option to choose between 3 available plans)
Flexible Spending Accounts:
Healthcare (FSA)
Parking Reimbursement Account (PRK)
Dependent Care Assistant Program (DCAP)
Transportation Reimbursement Account (TRN)
NETE is a multi-award winning company as well as offers a collaborative working environment where growth is encouraged and nurtured. In addition, we offer competitive salaries that may include performance bonuses; and a comprehensive benefits package.

NETE uses E-Verify to validate all new hires' ability to legally work in the United States.

Disclaimer: The above description is intended to describe the general nature of work and level of effort being performed by individual s assigned to this position or job description. This is not to be construed as a complete or exhaustive list of all skills, responsibilities, duties, and/or assignments required. Individuals may be required to perform duties outside of their position, job description, or responsibilities as needed."
153,Database Engineer,"Dulles, VA 20101",Dulles,VA,20101,None Found,None Found,"U.S. Citizenship
Must have an active Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearance
Must be able to obtain DHS Suitability
Demonstrated ability with diverse skill sets (e.g. data architects, data scientists, software developers)
Excellent understanding of big data and data analytics
Experience working with large structured and unstructured data sets
Development experience building ETL pipelines at scale
Solid SQL development skills
Experience with Linux/Unix tools and shell scripts
Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing, and encryption
Good communication skills, both oral and written
Must work well in a team environment as well as independently
Must exhibit good time management skills, independent decision making capability, and a focus on customer service.
","Using database expertise to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers) in support of a large, agile-based, cybersecurity system
Working with large structured and unstructured data sets
Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment
Design, setup, administer, and tune NoSQL databases in the AWS cloud
Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on a tradeoff between performance and quality
Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations
Write and refine code to ensure the quality and reliability of data extraction and processing
Analyze and resolve data performance and quality issues
Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.
Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance
Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates
Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to the development team
Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment
Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages
Maintain current industry knowledge of relevant concepts, practices, and procedures
","BS Computer Science, Computer Engineering, Computer Information Systems, OR Computer Systems Engineering. Two years of related work experience may be substituted for each year of degree-level education
",None Found,"Database Engineer
Residency Status: ALL CANDIDATES MUST BE A U.S. CITIZEN
Clearance: A minimum of Active Top Secret with the ability to obtain SCI and DHS Suitability prior to starting employment
Time Type: Full-Time
Relocation Fees: No
Bonus: Yes
Company Overview:
Novel Applications of Vital Information Inc. (Novel Applications) is a premier technology services company that provides solutions in the areas of Cyber Security, Information Management, Systems Integration. Novel Applications is a business that combines experience, creativity, flexibility, pragmatism, and cost-effective solutions in order to deliver measurable business value to our clients.
Headquartered in Fredericksburg Virginia, Novel Applications employs engineers, analysts, IT specialists and other professionals who strive to be the best at everything they do.
Novel Applications is an AA/EEO Employer - Minorities/Women/Veterans/Disabled.
Job Description:
NAVOI is seeking a Database Engineering Lead to work collaboratively with agile development teams in the design, development, and deployment of advanced cybersecurity capabilities.
Responsibilities Include:
Using database expertise to lead teams with diverse skill sets (e.g. data architects, data scientists, software developers) in support of a large, agile-based, cybersecurity system
Working with large structured and unstructured data sets
Implementing data management systems from conception, through all stages of design, development, and deployment in a CI/CD agile environment
Design, setup, administer, and tune NoSQL databases in the AWS cloud
Design and implement the technical architecture necessary to support analytic and statistical processing requirements based on a tradeoff between performance and quality
Performing data transformations such as aggregations, joins, and data cleaning to support analytic applications and visualizations
Write and refine code to ensure the quality and reliability of data extraction and processing
Analyze and resolve data performance and quality issues
Make data available to data scientists, programmers, and other users using programming and scripting languages such as R, Python, Java, JavaScript, etc.
Perform data analysis and design, data modeling, business intelligence management, master data management, metadata management, data quality management, data security, auditing and other functions of data management and governance
Generate data estimates of work to be performed, compare estimates to actuals, and continue to refine estimates
Work collaboratively with agile development teams, attending daily scrums and providing data related solutions to the development team
Develop and maintain database code libraries, data models, SOPs, and other documentation in the team collaboration environment
Migrate data from legacy RDBMSs and other data sources to NoSQL database in the AWS cloud using complex ETL processes, tools, and programming languages
Maintain current industry knowledge of relevant concepts, practices, and procedures
Required Skills:
U.S. Citizenship
Must have an active Top Secret (TS) clearance. Must be able to obtain a TS/SCI clearance
Must be able to obtain DHS Suitability
Demonstrated ability with diverse skill sets (e.g. data architects, data scientists, software developers)
Excellent understanding of big data and data analytics
Experience working with large structured and unstructured data sets
Development experience building ETL pipelines at scale
Solid SQL development skills
Experience with Linux/Unix tools and shell scripts
Expertise in data analysis and design, data modeling, master data management, metadata management, data warehousing, performance tuning, data quality improvement, data security, auditing, and encryption
Good communication skills, both oral and written
Must work well in a team environment as well as independently
Must exhibit good time management skills, independent decision making capability, and a focus on customer service.
Desired Skills:
Experience providing database engineering support to Intelligence, DoD, or DHS Customers
Understanding of Certification and Accreditation (ICD 503/DCID 6/3) processes as they apply to database technologies
Ability to support both SQL and NoSQL data management systems
Expertise in other RDBMS platforms such as Oracle RAC and SQL Server
Familiarity with AWS data migration tools such as AWS DMS, Amazon EMR, and AWS Data Pipeline
Experience with data transformation techniques such as aggregations, joins, and data cleaning
Experience with Red Hat Enterprise Linux (RHEL) operating system, storage configurations, network architecture, VMware, and/or related management tools
Database management experience of SQL databases such as MySQL and PostgreSQL in AWS cloud
Experience creating and managing NoSQL databases such as DynamoDB in the AWS cloud
Object mapping and migration of data from legacy structured and unstructured data sources to Amazon DynamoDB using AWS tools, custom code, or ETL scripts
Programming experience with languages such as R, Python, Java, JavaScript, JSON, etc.
Knowledge of Hadoop ecosystem, Map/Reduce, and data management products including Hbase, Hive, and Pig
DevSecOps and Continuous Integration / Continuous Delivery (CI/CD) knowledge
Experience or training in Six Sigma Methodology
ITIL knowledge and certification
Familiarity with SAFe (Scaled Agile Framework).
Required Education:
BS Computer Science, Computer Engineering, Computer Information Systems, OR Computer Systems Engineering. Two years of related work experience may be substituted for each year of degree-level education
Desired Certifications:
IBM Certified Data Engineer Big Data
Google Cloud Certified Professional Data Engineer
Cloudera Certified Professional Data Engineer
CCDH: Cloudera Certified Developer for Apache Hadoop
CCAH: Cloudera Certified Administrator for Apache Hadoop
CCSHB: Cloudera Certified Specialist in Apache HBase
CSSLP Certified Secure Software Lifecycle Professional
Certifications related to Scaled Agile Framework (SAFe) such as SAFe Practitioner (SP) or SAFe Program Consultant (SPC)
DoD 8570.1 IAT Level I"
154,Web Application Architect,"Arlington, VA",Arlington,VA,None Found,None Found,None Found,None Found,None Found,"BS/MS in Computer Science or related field of study
5+ years in software development
Experience with big data database technologies such as columnar databases.
Experience manipulating and working with data in a variety of forms: csv, xml, JSON, structured and unstructured
Experience working with a variety of APIs and RESTful interfaces
Knowledge JIRA, Bamboo, Confluence, or Bit-Bucket a plus
Experience with Tableau or other similar data visualization tools.
","5+ years of development experience on web applications using Python, Ruby, Java, or C#
4+ years of SQL experience
4+ Experience working with databases and web applications in analytics, reporting or Business Intelligence teams
3+ years of development experience in Python with a desire to work on Python projects
3+ years of Agile development experience
2+ years of experience working in a Linux environment
2+ years of experience in AWS or other similar cloud environments. (AWS Redshift is a plus)
2+ years of experience in Tableau or other data visualization tools.
","Full time
Responsible for business analysis, application design, development, integration and delivery and application maintenance and support. Individuals in this position must be a self-directed professional, who will also bring leadership skills and quickly learn new technologies and programming languages.
POSITION OVERVIEW :
Bloomberg Industry Group provides legal, tax and compliance professionals with critical information, practical guidance and workflow solutions. We leverage leading technology and a global network of experts to deliver a unique combination of news and authoritative analysis, comprehensive research solutions, innovative practice tools, and proprietary business data and analytics. Bloomberg Industry Group is an affiliate of Bloomberg L.P., the global business, financial information and news leader.
Bloomberg Industry Group seeks a a Data Engineer responsible for building products with embedded Analytics to surface insights on top of data infrastructure.
Individuals in this position must be s elf-directed learners who can quickly learn new technologies and programming languages.
RESPONSIBILITIES :
Proposes, develops and supports world-class customer facing web applications using a range of technologies.
Participates in the analysis of system and business requirements
Delivers high-quality code by defining and deploying best practices in unit testing and regression and testing frameworks.
Communicates with the Product Management and development teams to raise issues and identify potential barriers in a timely fashion.
Participates in user-centered research through client focus groups, interviews, usage analysis, and rapid prototyping.
Responsible for protecting our customers and brand by writing secure-by-design code.
Leads, supervises, mentors, and trains other team members in order to develop a strong, best-in-class development bench.
Directs the work of and provides technical guidance to less experienced staff.
Participates in recruiting, hiring, onboarding and performance management of new team members.
Participates in special projects and performs other duties as assigned.
Partner with application architects and developers to build innovative products.
You will utilize programming languages like Java, Python to write programs for automating data processing, writing complex queries, and integrating large data sets into cloud-based applications.
Job Requirements:
5+ years of development experience on web applications using Python, Ruby, Java, or C#
4+ years of SQL experience
4+ Experience working with databases and web applications in analytics, reporting or Business Intelligence teams
3+ years of development experience in Python with a desire to work on Python projects
3+ years of Agile development experience
2+ years of experience working in a Linux environment
2+ years of experience in AWS or other similar cloud environments. (AWS Redshift is a plus)
2+ years of experience in Tableau or other data visualization tools.
Education and Experience :
BS/MS in Computer Science or related field of study
5+ years in software development
Experience with big data database technologies such as columnar databases.
Experience manipulating and working with data in a variety of forms: csv, xml, JSON, structured and unstructured
Experience working with a variety of APIs and RESTful interfaces
Knowledge JIRA, Bamboo, Confluence, or Bit-Bucket a plus
Experience with Tableau or other similar data visualization tools.
Bloomberg Industry Group a wholly owned subsidiary of Bloomberg, is a leading source of legal, regulatory, and business information for professionals. Its network of more than 2,500 reporters, correspondents, and leading practitioners delivers expert analysis, news, practice tools, and guidance — the information that matters most to professionals. Bloomberg Industry Group authoritative coverage spans the full range of legal practice areas, including tax & accounting, labor & employment, intellectual property, banking & securities, employee benefits, health care, privacy & security, human resources, and environment, health & safety.


Bloomberg Industry Group offers a comprehensive benefits package including tuition reimbursement, domestic partner benefits, transportation subsidies, annual and sick leave, parenting leave, 401(k), and much more.


Bloomberg Industry Group IS AN EQUAL OPPORTUNITY EMPLOYER and fully subscribes to the principles of Equal Employment Opportunity. Bloomberg Industry Group has adopted an Affirmative Action Program to ensure that all applicants and employees are considered for hire, promotion, and job status without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, marital or familial status, genetic information, disabled veteran, veteran, veteran of the Vietnam Era, or any other classification protected by law."
155,Senior Software Engineer (Data),"Bethesda, MD 20817",Bethesda,MD,20817,None Found,None Found,None Found,None Found,None Found,None Found,"Posting Date Sep 25, 2019
Job Number 19131580
Job Category Information Technology
Location Marriott International HQ, 10400 Fernwood Road, Bethesda, Maryland, United States VIEW ON MAP
Brand Corporate
Schedule Full-time
Relocation? Yes
Position Type Management

Start Your Journey With Us
Marriott International is the world’s largest hotel company, with more brands, more hotels and more opportunities for associates to grow and succeed. We believe a great career is a journey of discovery and exploration. So, we ask, where will your journey take you?

Job Summary

This Software Data Engineer will enable creation and implementation of solutions using advanced computational techniques to data-intensive problems in the hospitality industry. This rewarding position provides a career on a team of mathematical modelers and data scientists that makes a direct revenue impact on Marriott hotels, globally, by creating tools that facilitate data-driven decision making in the disciplines of pricing, revenue management, sales and reservations. This software engineer will be part of a team that supports computationally-intensive data analysis and modeling efforts, provides analytics and technical expertise; performs undirected research on large volumes of data to reach business conclusions and recommendations; contributes complex programming and analysis in revenue management and sales systems; analyzes data and deploys codes to deliver business solutions; actively participates in all phases of the project lifecycle – inception, prototyping, design, testing, rollout and support; works as part of an adaptive software development team; has a willingness to learn and brings unique perspectives to enable creative data-driven solutions.

CANDIDATE PROFILE

Education and Experience
Required: Graduate degree in a quantitative, technical or scientific field, e.g., computer science, engineering, mathematics, operations research, statistics. 5+ years of professional experience as a data engineering or software developer in a domain or system with substantial data volume (should be comfortable working with millions or billions of records). Extensive knowledge and experience in one of the following languages: python (especially pandas/numpy/scipy), scala (especially spark), java or C/C++. Proficient computer science or software development fundamentals (e.g., data structures, algorithms, complexity theory, etc.). Comfortable discussing or implementing advanced mathematical techniques and processes.

Preferred: Experience writing complicated queries in SQL. Experience working with NoSQL databases (couchbase, cassandra, redis, mongodb, etc.) Cloud computing experience (especially AWS). Familiarity or experience with DevOps tools and pipelines (e.g., git, docker, kubernetes, jenkins, etc). An inherent curiosity in researching and developing solutions to complex and often open-ended business problem. An interest in continually learning new techniques and technologies. Ability to work within a team with strong communications skills, including an ability to explain and present technical concepts to technical and non-technical audiences. Willingness to debug and/or extend existing code bases. Willingness to participate in design sessions, code review session and oversee other developers. Experience with Big Data platforms (i.e. Hadoop, Spark, Kafka, Hive, etc). Experience in developing analytic driven solutions to business problems. Readiness to debug existing codes to identify source of reported application issues.

CORE WORK ACTIVITIES

Technical Leadership Trains and/or mentors other team members, and peers as appropriate Provides leadership on solution structure, tasks, timeline and estimates for revenue management deliverables Partners with planning teams to articulate possible solutions to business problems and to develop requirements for new models

Delivering Technology Formulates methods for measuring the effectiveness of pricing and revenue management activities. Prepares detailed specifications for translation of complex business processes into models and prototypes Develops and implements efficient, scalable algorithms using appropriate technologies/software/architectures Writes software and utility software tools to create and test new models Assists the Application Delivery teams in support of operations Assists the Application Delivery team in the maintenance of existing models Provides work/cost estimates for client requests Prepares documentation for client review. Conducts reviews, discusses findings, risks and challenges and establishes courses of action

IT Governance Follows all defined iT standards and processes and provides input for improvements to the appropriate process owners as needed Maintains a proper balance between business and operational risk Follows the defined project management standards and processes

Service Provider Management
To the extent that this role interacts with service providers: Validates that Service Providers develop and manage respective aspects of a project plan, including schedules, deliverables, and appropriate metrics. Makes short term plans for the team to effectively utilize resources Monitors Service Provider outcomes Reviews estimates of work effort for client project provided by Service Providers for accuracy Facilitates timely resolution of service delivery problems and minimizes the impact to clients

MANAGEMENT COMPETENCIES
Leadership Communication - Conveys information and ideas to others in a convincing and engaging manner through a variety of methods. Leading Through Vision and Values - Keeps the organization's vision and values at the forefront of employee decision making and action. Managing Change - Initiates and/or manages the change process and energizes it on an ongoing basis, taking steps to remove barriers or accelerate its pace; serves as role model for how to handle change by maintaining composure and performance level under pressure or when experiencing challenges. Problem Solving and Decision Making - Identifies and understands issues, problems, and opportunities; obtains and compares information from different sources to draw conclusions, develops and evaluates alternatives and solutions, solves problems, and chooses a course of action. Professional Demeanor - Exhibits behavioral styles that convey confidence and command respect from others; makes a good first impression and represents the company in alignment with its values. Strategy Development - Develops business plans by exploring and systematically evaluating opportunities with the greatest potential for producing positive results; ensures successful preparation and execution of business plans through effective planning, organizing, and on-going evaluation processes.
Managing Execution Building a Successful Team - Uses an effective interpersonal style to build a cohesive team; inspires and sustains team cohesion and engagement by focusing the team on its mission and importance to the organization. Strategy Execution – Ensures successful execution across of business plans designed to maximize customer satisfaction, profitability, and market share through effective planning, organizing, and on-going evaluation processes. Driving for Results - Sets high standards of performance for self and/or others; assumes responsibility for work objectives; initiates, focuses, and monitors the efforts of self and/or others toward the accomplishment goals; proactively takes action and goes beyond what is required.
Building Relationships Customer Relationships - Develops and sustains relationships based on an understanding of customer/stakeholder needs and actions consistent with the company’s service standards. Global Mindset - Supports employees and business partners with diverse styles, abilities, motivations, and/or cultural perspectives; utilizes differences to drive innovation, engagement and enhance business results; and ensures employees are given the opportunity to contribute to their full potential. Strategic Partnerships - Develops collaborative relationships with fellow employees and business partners by making them feel valued, appreciated, and included; explores partnership opportunities with other people in and outside the organization; influences and leverages corporate and continental shared services and/or discipline leaders (e.g., HR, Sales & Marketing, Finance, Revenue Management) to achieve objectives; maintains effective external relations with government, business and industry in respective countries; performs effectively as a liaison between locations, disciplines, and corporate to ensure needed resources are received and corporate strategies are understood and executed.
Generating Talent and Organizational Capability Developing Others - Supports the development of other’s skills and capabilities so that they can fulfill current or future job/role responsibilities more effectively. Organizational Capability - Evaluates and adapts the structure of assignments and work processes to best fit the needs and/or support the goals of an organizational unit.
Learning and Applying Professional Expertise Continuous Learning - Actively identifies new areas for learning; regularly creates and takes advantage of learning opportunities; uses newly gained knowledge and skill on the job and learns through their application. Technical Acumen - Understanding and utilizing professional skills and knowledge in a specific functional area to conduct and manage everyday business operations and generate innovative solutions to approach function-specific work challenges
o Technical Intelligence: Knowledge and ability to define and apply appropriate technology to enhance business process
o Development Methodologies: Knowledge of general stages of SDLC framework and the application tiers within the development space.
o Information Security: Knowledge of the security considerations relevant within the development space, including industry best practices related to information security Business Acumen - Understands and utilizes business information to manage everyday operations and generate innovative solutions to approach business and administrative challenges. Basic Competencies - Fundamental competencies required for accomplishing basic work activities.
o Basic Computer Skills - Using basic computer hardware and software (e.g., personal computers, word processing software, Internet browsers, etc.).
o Mathematical Reasoning - The ability to add, subtract, multiply, or divide quickly, correctly, and in a way that allows one to solve work-related issues.
o Oral Comprehension - The ability to listen to and understand information and ideas presented through spoken words and sentences.
o Reading Comprehension - Understanding written sentences and paragraphs in work related documents.
o Writing - Communicating effectively in writing as appropriate for the needs of the audience.


Marriott International is an equal opportunity employer committed to hiring a diverse workforce and sustaining an inclusive culture. Marriott International does not discriminate on the basis of disability, veteran status or any other basis protected under federal, state or local laws."
156,GDIT is hiring IT Professionals for F- 35 Program-Secret Clearance Required,"Arlington, VA 22201",Arlington,VA,22201,None Found,"
Active Secret Security Clearance is Required
Additional IT certifications may be required for specific roles
",None Found,None Found,"
Bachelors Degree in Computer Science, Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience.",None Found,"Job Description
Do you hold an active US government security clearance? Are you interested in work that gives you the opportunity to use your skills to solve complex problems? Would you like to join a team that encourages ingenuity and is mission driven? Would you like to join an organization that makes a difference for our warfighters and our citizens?
GDIT was recently awarded the Joint Strike Fighter (JSF) F-35 IT program support contract. We are providing knowledge-based, information assurance and cybersecurity IT services to the F-35 JSF Virtual Enterprise (JVE) network in support of the F-35 Lightning II Joint Program Office (JPO). Our services include program management, enterprise performance management, enterprise architecture, implementation of emerging capabilities and requirements, life cycle management, operations & maintenance, enterprise data management, service desk support and IT training.
We are building a team of dedicated professionals. We are seeking candidates for multiple roles. The positions include but are not limited to:
Help Desk Manager
Infrastructure Lead Engineer
Software Integrator Lead
Network Engineer
Help Desk Specialist
Storage/SAN Engineer
Voice/Data Engineer
Systems Administrators
Software Developers
Information Assurance
#F35ProgramLandingPage
Education
Bachelors Degree in Computer Science, Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience.
Qualifications
Active Secret Security Clearance is Required
Additional IT certifications may be required for specific roles
For more than 50 years, General Dynamics Information Technology has served as a trusted provider of information technology, systems engineering, training and professional services to customers across federal, state, and local governments, and in the commercial sector. Over 40,000 GDIT professionals deliver enterprise solutions, manage mission-critical IT programs and provide mission support services worldwide. GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class."
157,Data Engineer,"Springfield, VA",Springfield,VA,None Found,None Found,"Experience with Amazon Web Services (AWS), Microsoft Azure, or MilCloud 2.0",None Found,None Found,"Experience with Amazon Web Services (AWS), Microsoft Azure, or MilCloud 2.0",None Found,"This position requires a Top Secret security clearance and eligibility for access to Sensitive Compartmented Information (TS/SCI)

Desired Experience:
At least 5 years of experience with Big Data systems, including Hadoop and Cloudera
At least 2 years of experience with the design, implementation, or consulting for applications deployed across multiple organizations or a technical environment
Experience with multiple operating systems, including Linux-, UNIX-, and Windows-based
- Experience running, enhancing, and significantly upgrading and modifying Hadoop- and Cloudera-based environments for support to a wide variety of querying approaches
Experience establishing continuous integration and continuous deployment processes for applications and environments
Experience with extract, transform, and load (ETL) processes
Experience with multiple database technologies


Desired Education: Bachelors of Art (BA) or Bachelors of Science (BS), preferably in a related field.

Required Certifications:Security+ or CISSP Certification required

Work Description:
The Data Engineer is responsible for the configuration and ingestion of structured, unstructured, and semi-structured data repositories. Their work is focused on turning these data repositories into effective resources that satisfy mission requirements and that support a data analytics and rapid development pipeline. They maintain all operational aspects of data transfers, accounting for the security posture of the underlying infrastructure and the systems and applications that are supported, and they monitor the health of the environment through a variety of health tracking capabilities.

The Data Engineer also automates configuration management using NiFi and other tools and they stay current on data extraction, transfer, and loading (ETL) technologies and services. They should be able to work under general guidance, demonstrate the initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.

The Data Engineer applies their specialized knowledge of data and leverages their expertise to structure and retrieve data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically. Lastly, they engage with multiple functional groups to understand client challenges, prototype new ideas and new technologies, help create solutions to drive innovation, and they design, implement, schedule, test, and deploy full features and components of solutions.

Additional Desired Qualifications:
Experience with Amazon Web Services (AWS), Microsoft Azure, or MilCloud 2.0
Experience applying DoD Security Technical Implementation Guides (STIGs) and automating that process
Experience with storage fundamentals, including CIFS and NFS and backup and disaster recovery processes a plus
Experience configuring and aggregating logs for data analysis using Splunk or ELK solutions
Experience maintaining data transfer systems, including NIFI
Experience with two or more programming languages, including C#, Java, .NET, or similar
Knowledge of FedRAMP, RMF, and the implications of C&A and SA&A in a DoD environment a plus
Any industry-recognized Cloud Certifications preferred"
158,Big Data Engineer,"Alexandria, VA",Alexandria,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"RII develops cutting-edge software for the government and military. We use agile development practices and user-centered design to create innovative software solutions for complex real-world problems. We're breaking through the big, slow status quo with transformative technology that fundamentally changes and improves the world.

Our team is currently seeking high-quality contributors for a variety of positions in the Northern Virginia area. Our projects include developing complex, web client server applications; developing mobile device applications and prototypes; developing Big Data and machine learning solutions; and architecting and prototyping complex distributed command and control network applications.

If you are a sharp, experienced engineer with demonstrated capabilities in designing and implementing Big Data architectures with scalable data extract, transform, and load technologies, and in deploying advanced analytics (e.g., natural language processing, deep learning) leveraging these architectures, we want to hear from you. Joining RII not only provides unique challenges and opportunities, it also directly and positively impacts many of our Defense and Homeland Security end users.
WHAT YOU WILL BE DOING
Establish, maintain, and enhance a Big Data analytics architecture in support of several customer projects
Be the engineering point-person for Big Data ingest, storage, and analytical capabilities and technologies
Maintain awareness of the current and emerging capabilities in Big Data and analytics technologies and how these apply to our customer challenges
Develop captivating solutions by collaborating with customers and the broader RII development team
Document use cases, solutions, & recommendations for our customers
WHAT YOU HAVE DONE
BS in Computer Science, equivalent degree, or previous work experience
Worked with NoSQL databases and distributed indexing systems (Elasticsearch or SOLR)
Developed scalable ingest/ETL pipelines using distributed messaging systems (such as Kafka or AMQP solutions)
Experience in developing and deploying Java and Python-based software architectures
Experience in deploying advanced analytics capabilities (e.g., natural language processing, deep learning) in Big Data settings
Experience in Linux administration, and in runtime configuration management and automated deployment (using tools such as Puppet and Ansible)
Excellent general understanding of Linux operating systems, distributed systems, microservices, and database technologies
Excellent general understanding of ETL and data analytics platforms
Knowledge of cloud computing infrastructure such as: Amazon Web Services EC2 or C2S
Experience in understanding and decomposing system level requirements into discrete and measurable tasks
EVEN BETTER
MS or PhD in Computer Science, equivalent degree, or work experience
Experience with Agile Methodologies and supporting technologies enabled by Atlassian products
Experience with the BigTable family of columnar stores (HBase, Cassandra, Accumulo)
Strong knowledge of installation, configuration, and maintenance of cloud computing and Big Data infrastructure to include Hadoop, Accumulo, Mongo, Kafka, Spark, Elasticsearch, Puppet, Ansible, Lucene, and related technologies
Experience with Continuous Integration and Continuous Deployment concepts
Designed, developed, and deployed machine learning based capabilities
Experience creating Big Data solutions using public, private, and hybrid cloud approaches
Experience with integration methodologies and tools for Big Data applications and services
Experience with data quality and data profiling tools
Experience with data analytics profiling and performance analysis
Experience with client-side development including JavaScript, HTML5 and Angular
Experience leading a team of developers
US Citizenship required. Candidates must be clearable to Secret, desired TS/SCI. Active clearance not required to apply.

Research Innovations, Inc. is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national, origin, disability status, protected veteran status, or any other characteristic protected by law."
159,Chief Data Engineer,"Arlington, VA",Arlington,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Key Role:
Design, implement, and manage data platforms, databases, and data delivery systems and transform them into solutions that enable greater data insights, analysis, and reporting. Apply expertise in Big Data technology implementation and operations to working with clients to design integrated data platforms, ecosystems, and solutions. Leverage expertise in structured and unstructured data, streaming and batch data processing, extract, transform, and load (ETL), data wrangling, data ingest, and data access. Apply comprehension of database design and implementation tools, including entity-relationship data modelling and SQL, distributed computing architectures, operating systems, storage technologies, memory management, and networking to creating structure and value out of complex and ambiguous technical challenges with little guidance.
Basic Qualifications:10+ years experience with data analysis, management, architecture or engineering7+years experience with leading a teamExperience with custom or structured ETL design, implementation, and maintenanceExperience with NoSQL and big table data stores, including Accumulo, HBase, MongoDB, and CassandraAbility to quickly learn technical concepts and communicate with multiple functional groupsSecret clearanceBA or BS degree
Additional Qualifications:Experience with enterprise architecture (EA)Experience with data ingest and preparation tools such as NiFi, Kylo, or StreamsetsExperience with Big Data platforms such as Cloudera, Hortonworks, or data bricksExperience with batch and streaming frameworks, including Kafka, Storm, or FlinkExperience with traditional databases such as Oracle, MSSQL, PostGres or othersExperience with data visualization tools such as Tableau or Zoom DataExperience with data management tools such as Informatica, TalenD or othersExperience with multiple data modeling concepts, including XML and JSONPossession of excellent analytical and problem-solving skills
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required.
We’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change.
SIG2017"
160,Full-Stack Data Engineer-TS/SCI,"Reston, VA 20191",Reston,VA,20191,None Found,None Found,None Found,"http://www-01.ibm.com/employment/us/benefits/
https://www-03.ibm.com/press/us/en/pressrelease/50744.wss",None Found,None Found,"Introduction
As a Data Scientist at IBM, you will help transform our clients’ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it’s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.

Your Role and Responsibilities
IBM Global Business Services (GBS) is a team of business, strategy and technology consultants enabling enterprises to make smarter decisions and providing unparalleled client and consumer experiences in cognitive, data analytics, cloud technology and mobile app development. IBM GBS empowers clients to digitally reinvent their business and get the competitive edge in the cognitive era in over 170 countries.

Bottom line? We outthink ordinary. Discover what you can do at IBM.

We are seeking a full-stack data engineer to join IBM in support of a growing Advanced Analytics effort for the US Navy. The Full-Stack Data Engineer will primarily provide React and Node support. There may be additional work in database design, data manipulations, tailored script / algorithm development, and data visualization. The role is in the Washington DC area (Reston and/or Arlington, Virginia).

BENEFITS
Health Insurance. Paid time off. Corporate Holidays. Sick leave. Family planning. Financial Guidance. Competitive 401K. Training and Learning. We continue to expand our benefits and programs, offering some of the best support, guidance and coverage for a diverse employee population.
http://www-01.ibm.com/employment/us/benefits/
https://www-03.ibm.com/press/us/en/pressrelease/50744.wss
CAREER GROWTH
Our goal is to be essential to the world, which starts with our people. Company wide we kicked off an internal talent strategy program called Go Organic. At our core, we are committed to believing and investing in our workforce through:
Skill development: helping our employees grow their foundational skills
Finding the dream job at IBM: navigating our company with the potential for many careers by channeling an employee’s strengths and career aspirations
Diversity of people: Diversity of thought driving collective innovation
In 2015, Go Organic filled approximately 50% of our open positions with internal talent that were promoted into the role.

CORPORATE CITIZENSHIP
With an employee population of 375,000 in over 170 countries, amazingly we connect, collaborate, and care. IBMers drive a corporate culture of shared responsibility. We love grand challenges and everyday improvements for our company and for the world. We care about each other, our clients, and the communities we live, work, and play in!
http://www.ibm.com/ibm/responsibility/initiatives.html
http://www.ibm.com/ibm/responsibility/corporateservicecorps

Required Professional and Technical Expertise
US Citizen with active TS//SCI clearance or show immediate eligibility according to JPAS security database
5+ years of React and Node development and maintenance.
Demonstrable skills in Node/JavaScript, SQL/NoSQL in a Windows or Linux desktop operating environment
Data visualization skills using JavaScript (such as ChartJS or D3)
Technical skills in MVC frameworks and Web API design
Must possess the ability to communicate clearly, concisely, and with technical accuracy in both oral and written modes.
Must be able to work effectively under time constraints and potentially changing priorities, while maintaining a high level of attention to detail.
Must be able to work in a collaborative, team environment, including on government site where availability of advanced development/database tools may be limited
Bachelors Degree in Engineering, Mathematics, Operations Research, Computer Science, or related technical field

Preferred Professional and Technical Expertise
Masters Degree, preferably in Engineering, Mathematics, Operations Research, Statistics, Computer Science, or related
Experience with React and Node in a military setting
Experience with Python and the ETL process
Experience with identifying and gathering research and analysis data from DOD organizations and systems
Experience with Navy research and analysis databases

About Business Unit
IBM Services is a team of business, strategy and technology consultants that design, build, and run foundational systems and services that is the backbone of the world's economy. IBM Services partners with the world's leading companies in over 170 countries to build smarter businesses by reimagining and reinventing through technology, with its outcome-focused methodologies, industry-leading portfolio and world class research and operations expertise leading to results-driven innovation and enduring excellence.

Your Life @ IBM
What matters to you when you’re looking for your next career challenge?

Maybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities – where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust – where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.

Impact. Inclusion. Infinite Experiences. Do your best work ever.

About IBM
IBM’s greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.

Location Statement
For additional information about location requirements, please discuss with the recruiter following submission of your application.

Being You @ IBM
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
161,Data Engineer,"Herndon, VA 20171",Herndon,VA,20171,None Found,None Found,None Found,None Found,None Found,"Minimum Requirements: Bachelor’s Degree in Computer Science, IT or related field & minimum 5-7 years of QA experience
Minimum experience of 5 years with relational databases such as Oracle, SQL Server, Sybase, RedShift
Minimum 3+ years of experience with ETL technologies
","Secure our Nation, Ignite your Future
This position will report to the Group CTO and be part of the technology office staff. You will be supporting an internal big data analytics application that will be hosted on a cloud platform. You will have prior experience working in an agile/DevOps environment. You will have the opportunity to work with some of the newest technologies, transformative projects, and play a strong role in shaping the innovation agenda across the company.

Essential Job Duties:
Provide database design, development and implementation support
Participate in daily scrum meetings and support development team
Work with customers and team members in an Agile development environment
Follow Continuous Integration/Continuous Delivery (CI/CD) best practices for code build and deployments
Develop PL/SQL, stored procedures, ETL scrips to support this analytics application
Document database design and develop optimum data ingest techniques from multiple data sources
Minimum Requirements:
Minimum Requirements: Bachelor’s Degree in Computer Science, IT or related field & minimum 5-7 years of QA experience
Minimum experience of 5 years with relational databases such as Oracle, SQL Server, Sybase, RedShift
Minimum 3+ years of experience with ETL technologies
Additional skills:

Experience using Jira, Git, Confluence, Knowledge and understanding of big data technologies like Hadoop, Hive, etc., Ability to handle stress and work well under pressure, Ability to use MS Office, Experience with cloud technologies, Analytical and Critical Thinking Skills, Interpersonal and People Skills, Leadership Skills, Listening Skills, Multitasking Ability, Oral and Written Communication Skills, Organizational Skills
ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.
If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.
If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information."
162,Data Engineer,"Reston, VA",Reston,VA,None Found,None Found,"
3+ years of experience with SAS programming in a business environment, particularly with data modeling, or in a “big data” context.
1 year of Java experience
Well qualified with SQL.
BS or MS in any quantitative field (computer science, systems engineering, mathematics, economics, statistics, etc.) or equivalent work experience.","Solid experience with SAS data processing, macro programming and running SQL queries within SAS (PROC SQL and SQL pass-through to Vertica.)
Our planned Data Lake will make use of new tools including AWS Glue, AWS Athena, Spark, etc. Knowledge of these tools or the interest and desire to learn them.
Familiarity with Linux, GIT and AWS Console.
Able to implement complex algorithms which transform, cleanse, impute, and mash up data.
Passion for data processing, data modeling, data mining and tackling complex operations.
Professional experience working in an Agile environment.
Professional experience working with a source code version control system.
Proficiency with Microsoft Excel.
English fluency, both spoken and written. Able to discuss complex technical subjects with clarity and precision.","Maintaining, improving, and executing existing scripts written in SAS, SQL, and Python. Most of the codebase is currently in SAS but the future will contain more Java, and possibly Python or R.
Designing new scripts using the above tools to ingest, cleanse, consolidate, analyze, and summarize the incoming data. You will also be implementing and tuning algorithms and business rules, quality-checking the data results, and working iteratively with evolving requirements.
Meeting delivery deadlines for new products, features, and enhancements.
Implementing and delivering large historical data solutions to new and existing customers.
Coding new applications for ad hoc reporting and for data research inquiries.
Investigating issues with data quality and responding to stakeholders’ technical questions.
Identifying opportunities to complement, enhance and/or optimize our data processing environment with new tools and techniques.
Provide on-call support on rotation basis, occasionally during afterhours and weekends.",None Found,None Found,"Data Engineer
Reston, VA
We seek an experienced Data Engineer with the skills, energy and business acumen to excel on our aviation Data Engineering Team. Our team performs data acquisition and ingestion, processing, and data delivery across a variety of products.
Your contribution will be critical in many areas such as cleansing, consolidating, and normalizing industry data, and then synthesizing valuable data sets to be served directly to clients or loaded into a data warehouse that supports our online analytics tools. You will support industry-leading Cirium products and help develop the next generation of Cirium Business Intelligence tools for the aviation industry.

Key Accountabilities and Responsibilities:
Maintaining, improving, and executing existing scripts written in SAS, SQL, and Python. Most of the codebase is currently in SAS but the future will contain more Java, and possibly Python or R.
Designing new scripts using the above tools to ingest, cleanse, consolidate, analyze, and summarize the incoming data. You will also be implementing and tuning algorithms and business rules, quality-checking the data results, and working iteratively with evolving requirements.
Meeting delivery deadlines for new products, features, and enhancements.
Implementing and delivering large historical data solutions to new and existing customers.
Coding new applications for ad hoc reporting and for data research inquiries.
Investigating issues with data quality and responding to stakeholders’ technical questions.
Identifying opportunities to complement, enhance and/or optimize our data processing environment with new tools and techniques.
Provide on-call support on rotation basis, occasionally during afterhours and weekends.
Qualifications:
3+ years of experience with SAS programming in a business environment, particularly with data modeling, or in a “big data” context.
1 year of Java experience
Well qualified with SQL.
BS or MS in any quantitative field (computer science, systems engineering, mathematics, economics, statistics, etc.) or equivalent work experience.
Key Skills Required:
Solid experience with SAS data processing, macro programming and running SQL queries within SAS (PROC SQL and SQL pass-through to Vertica.)
Our planned Data Lake will make use of new tools including AWS Glue, AWS Athena, Spark, etc. Knowledge of these tools or the interest and desire to learn them.
Familiarity with Linux, GIT and AWS Console.
Able to implement complex algorithms which transform, cleanse, impute, and mash up data.
Passion for data processing, data modeling, data mining and tackling complex operations.
Professional experience working in an Agile environment.
Professional experience working with a source code version control system.
Proficiency with Microsoft Excel.
English fluency, both spoken and written. Able to discuss complex technical subjects with clarity and precision.
Optional Skills Preferred:
Knowledge of advanced SAS programming techniques and efficiencies.
Familiar with Linux command line and shell scripting.
Knowledge of passenger aviation data (ticket data, DOT data, schedules), QSI scoring models.
Knowledge of airline terminology and airline business metrics.
Experience with Kanban.
Knowledge of GIT source control, particularly from the Linux command line.
Knowledge of R and / or Python for data processing and machine learning.
Please note: This is a regular, full-time position which requires working out of our office located in downtown Reston, VA.

About Cirium

Cirium provides data services and end-to-end data solutions to customers serving the global travel industry. The company has established a leadership position as a provider of real-time global flight information, serving airlines and airports, travel agencies, developers, consumers, and more. The company is leveraging the platform and domain knowledge it has developed to expand into new data sets and new products that deliver value to the company’s core markets. We work in an interesting field, we have demonstrated success in what we do, and we are well positioned for future growth.

Our Environment: We hire the best talent in the travel and technology industries. To support our talented team, we offer an extraordinary work environment that places trust and respect at the forefront of our company values. These values enable our employees to do their best work by creating an open, supportive environment that promotes creativity. We think it's a great place to do great work, and if you like the sound of this as well, we encourage you to consider this opening.

Cirium is a branded division of Reed Business Information (RBI). RBI provides information, analytics and data to business professionals worldwide. Our strong global products and services hold market-leading positions across a wide range of industry sectors including banking, petrochemicals and aviation where we help customers make key strategic decisions every day. RBI is part of RELX Group plc, a world-leading provider of information solutions for professional customers across industries.
http://www.reedbusiness.com

RBI is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. If a qualified individual with a disability or disabled veteran needs a reasonable accommodation to use or access our online system, that individual should please contact 1.877.734.1938 or accommodations@relx.com."
163,Data Engineer Specialist,"McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"We are looking for a savvy Data Engineer to join Loan Advisor Business Intelligence (LA BI). Ideal candidate has extensive experience modeling and wrangling data (structured and unstructured), in-depth knowledge of database architecture, and enjoys optimizing data systems and building them from the ground up all in support of analytic and reporting applications. The Data Engineer will support our BI developers and data/business analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Knowledge of Hadoop based platform a must.
Duties:
Perform complex data analysis, including query optimizationAssemble large, complex data sets that meet functional / non-functional business requirementsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Develop and implement ETL processes, reports and queries in support of business analytics
Qualifications:
Advanced working SQL knowledge and experience working a variety of databases technologies (Oracle, MySQL, JDBC, NoSQL)Experience building and optimizing ‘big data (Hadoop)’ data pipelines, architectures and data sets.Experience shredding XML/JSON filesA successful history of manipulating, processing and extracting value from large disconnected datasets.Effective organization skills with attention to detailEffective oral and written communication skillsKnowledge of R, Python, or other ETL toolsKnowledge of Tableau and/or MicroStrategy a plus.Familiarity with Secondary Mortgage Market and/or Freddie Mac business and operations also a plus."
164,"Data Engineer, Data and Services (University Hire - Summer 2020 Start)","Arlington, VA",Arlington,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Who is Mastercard?
We are the global technology company behind the world’s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless ®. We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.
Job Title
Data Engineer, Data and Services (University Hire - Summer 2020 Start)
********APPLICATION INSTRUCTIONS: Please attach your resume, transcript, and cover letter with your application in the resume upload tab. All three documents must be submitted. *********


Data Engineers are fundamental to the success of our clients; you will be the bridge between raw client data and Mastercard's software. You will be responsible for:


Designing processes to extract, transform, and load (ETL) terabytes of client data into Mastercard's analytics platform using SQL and other technologies

Working across multiple client teams, both internal and external, in order to determine the data requirements and business logic applicable to each data set.

Tackling big data problems across various industries, utilizing your creative thinking skills


Make an Impact as a Data Engineer

Strategic problem solving with exceptional peers who are also passionate about data
Creative freedom to innovate with new technologies (such as Microsoft SQL Server & Business Intelligence Tools) and to explore a variety of directions
Flexibility to work on many new and challenging projects across a diversity of industries
A dynamic environment where you will have an impact and make an immediate difference
An immediate opportunity for increased responsibility, leadership, and professional growth
Committed mentoring and training by an experienced and driven management team
Collaborate with other Mastercard departments and focus on internal development, during which you will develop new tools and processes that will be used across Mastercard


Bring your passion and expertise

Understanding of relational databases and ETL Processes (preferably Microsoft SQL Server)
Desire to work with data and help businesses make better data-driven decisions
Excellent written and verbal communication skills
Hands-on experience with the ETL process, SQL, and SSIS
Knowledge of at least one programming language a plus (Powershell, .Net, Perl, Python, VB Script, C#)
Strong troubleshooting and problem solving capabilities
Demonstrated analytical/quantitative skills
Bachelor's degree with an established history of academic success

Mastercard Worldwide is an Equal Employment Opportunity Employer and does not discriminate in employment on the basis of age, race, color, gender, national origin, disability, veteran status, or any other basis that is prohibited by applicable law.
Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.
If you require accommodations or assistance to complete the online application process, please contact reasonable.accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly."
165,Data Engineer,"McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer

As a Capital One Data Engineer, you’ll be part of a team that’s building new analytical and machine learning tools and frameworks to exploit advantages in the latest developments in cloud computing. You will participate in detailed technical design, development and implementation of applications used by our data scientists and business analysts to build and launch models, analyze data, and make million dollar decisions. We work closely with our users and are looking for people who are excited to iterate quickly on solutions and see the impact in days, not months.

Who You Are

You yearn to be part of cutting edge, high profile projects and are motivated by delivering world-class solutions on an aggressive schedule

Someone who is not intimidated by challenges; thrives even under pressure; is passionate about their craft; and hyper focused on delivering exceptional results

You love to learn new technologies and mentor junior engineers to raise the bar on your team

It would be awesome if you have a robust portfolio on Github and/or open source contributions you are proud to share

Passionate about intuitive and engaging user interfaces, as well as new/emerging concepts and techniques.

The Job

Collaborating as part of a cross-functional Agile team to create and enhance software that enables state of the art data science and data analysis

Developing and deploying data pipelines using Apache Spark and AWS Big Data stack

Developing frameworks to accelerate the model development lifecycle

Utilizing programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake

Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment and Test Automation to enable the rapid delivery of working code utilizing tools like Jenkins, Terraform, Git and Docker

Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance

Basic Qualifications:
Bachelor’s Degree

At least 2 years of Data Engineering experience

At least 2 years of experience using Java, Python, or Scala

At least 1 year of experience using Apache Spark

Preferred Qualifications:
Master’s Degree or PhD

4+ years of experience using Java, Python, or Scala

4+ years experience with Relational Database Systems and SQL

4+ years of experience with PostgreSQL or Redshift

4+ years of UNIX or Linux experience

2+ years of experience with Cloud computing

2+ years of experience with AWS

2+ years of experience using Apache Spark

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
166,"Master, Data Engineering","McLean, VA",McLean,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"McLean 2 (19052), United States of America, McLean, Virginia

At Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Master, Data Engineering

We are looking for driven individuals to join our team of passionate Software and Data Engineers who will help us create Capital One’s next generation of data products and capabilities. Do you possess deep knowledge across technologies and have a strong background in software and data engineering?

As a Master, Data Engineer you will:
Manage teams of Software and Data Engineers who will build data pipeline frameworks to automate high-volume and real-time data delivery for our Spark and streaming data hub
Transform complex analytical models in scalable, production-ready solutions
Continuously integrate and ship code into our cloud production environments
Develop cloud-based applications from the ground up using a modern technology stack
Work directly with Product Owners and customers to deliver data products in a collaborative and agile environment

Responsibilities include:
Lead and develop sustainable data driven solutions with current new generation data technologies to drive our business and technology strategies
Design robust systems with an eye on the long-term maintenance and support of the application
Leverage reusable code modules to solve problems across the team and organization
Handle multiple functions and roles for the projects and Agile teams
Manage and mentor a team of data engineers (both full-time associates and/or third-party resources)
Providing business, application and technology consulting in feasibility discussions with technology team members and business partners
Define, execute and continuously improve our internal software architecture processes
Be a technology thought leader and strategist

What We Have:
Flexible work schedules
Convenient office locations
Generous salary and merit-based pay incentives
A startup mindset with the wallet of a top 10 bank
Monthly innovation challenges dedicated to test driving cutting edge technologies
Your choice of equipment (MacBook/PC, iPhone/Android Device)

Basic Qualifications:
Bachelor's Degree
At least 5 years of experience in Java or Python based software application development
At least 3 years of people management experience
At least 3 years of Agile experience
At least 1 year of experience with a Cloud platform (AWS, Google, or Azure)

Preferred Qualifications:
7+ years of experience with Data Engineering concepts and building and maintaining Big Data applications using open source software like Spark or Hadoop
7+ years of experience with Software Engineering
Understanding of Test-Driven Development concepts and supportive tools like Cucumber
Understanding of Object-Oriented and Functional programming concepts using languages like Python and Java
Experience with developing and deploying applications on Cloud, preferably on Amazon Web Services (AWS)
1+ years’ experience designing, developing, and implementing APIs
Familiarity with Configuration management tools like Ansible or Chef
Familiarity with Application Container concepts using tools like Docker or Kubernetes

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
167,Business Intelligence / Data Engineer,"Herndon, VA",Herndon,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"Bachelor’s Degree in Computer Science, Information Systems, Mathematics, Statistics, or related field or equivalent experience5+ year experience with Data modeling, SQL, ETL , Data Warehousing and Datalakes5+ year experience in the design, creation, management, and business use of large datasets.This position requires the candidate selected be a U.S. citizen and must currently possess an active Top Secret security clearance. The position further requires, after start, the selected candidate obtain and maintain an active TS/SCI security clearance with polygraph and satisfy other security related requirements.

As a Business Intelligence / Data Engineer you will enable data-driven decision making within the Amazon Web Services (AWS) Data Center Infrastructure Operations organization.
The Infrastructure Operations Team is responsible for planning, implementing, monitoring and continuously improving the global Amazon Data Center infrastructure. The team supports all aspects of the Data Center based organizations, including but not limited to: Safety, Security, maintenance, daily operations, logistics, engineering and equipment management.

You should be passionate about working with huge data sets and be someone who is able to bring data sets together to answer business questions and drive growth. You will have an opportunity to work with big data and emerging technologies while driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, metadata, reporting, and dash boarding.

The Business Intelligence Data Engineer will:
Primarily support teams within the Infrastructure environment, and long term will have opportunities to support teams in the overall Amazon Web Services community.Build ETLs to ingest the data into the data warehouse and datalake, as well as end-user facing reporting applications.Develop, implement and maintain the metrics and reports to enable decision support systems for the organization. This includes working with other teams to develop those metrics from their services.Partner with business customers and development teams to define analytics requirements and then deliver flexible, scalable, end-to-end solutions.
This position requires the candidate selected be a U.S. citizen and must currently possess an active Top Secret security clearance. The position further requires, after start, the selected candidate obtain and maintain an active TS/SCI security clearance with polygraph and satisfy other security related requirements.

Ability to balance and prioritize multiple conflicting requirements with high attention to detail.Experience with writing SQL scriptsExperience with enterprise-class Business Intelligence tools such as Microstrategy, PowerBI, Tableau, Oracle BI, Penthao, etc.Experience communicating with business owners to understand their data and reporting requirements.Experience with data presentation skills to summarize key findings and communicate with both business and technical teams.Experience working in a Linux environmentExperience with scripting language such as Python, Perl, Ruby or JavascriptExperience with MPP databases such as RedshiftExperience with Datalake developmentKnowledge of predictive/advanced analytics and tools (such as R, SAS, Matlab)Knowledge of noSQL databases (such as DynamoDB, MongoDB)Knowledge in an enterprise class RDBMSKnowledge of AWS products and services"
168,"Big Data Engineer, Analytics","Chantilly, VA",Chantilly,VA,None Found,None Found,None Found,None Found,None Found,None Found,None Found,"OVERVIEW
Technology is constantly changing, and our adversaries are digitally “going dark” at a rate that is exceeding law enforcement’s ability to keep pace. Those charged with protecting the United States are not always able to access the evidence needed to prosecute crime and prevent terrorism. The Government has trusted in Peraton to provide the technical ability, tools, and resources to bring criminals to justice. In response to this challenge, we are seeking a talented Big Data Engineer with a focus in Analytics.
RESPONSIBILITIES
What you’ll do…
Provide entity resolution expertise in support of a large scale search and discovery application
Work closely with entity resolution subject matter experts, program engineers and stakeholders to integrate complex data sources into operation and develop new analytics and algorithms
Support advanced data exploitation capabilities using Hadoop related technologies
Enhance the data processing infrastructure using open source and commercial technologies
Capture and define requirements for entity resolution features and functionality from a UI end-user perspective
Analyze documentation and source data of new data feeds
Develop scripts to ETL source data into destination format
Provide O&M monitoring and resolution of production web services using monitoring tools including Ganglia, Jenkins, and other means
Modify pipeline scripts to enhance functionality and support bug fixes
Resolve bugs by modifying pipeline scripts and entity resolution explanation service scripts
Coordinate development efforts in an agile team
QUALIFICATIONS
You’d be a great fit if…
You’ve obtained a BS degree and have five (5) years of relevant experience. However, equivalent experience may be considered in lieu of degree.
You have three (3) or more years of experience using Hadoop or other large-scale data warehouse technologies to support entity and relationship resolution and operations
You have one (1) year of experience in Apache, Spark, Hive, Pig, ER Resolution Engines, Unix scripting, Novetta Entity Analytics, Waremen Pro or similar ER engines
You have a current Top Secret security clearance with SCI eligibility and the ability to obtain a polygraph

It would be even better if you…
Have hands on experience with any of the following technologies:
SQL
Linux scripting
AWK, PERL, BASH or other scripting language
SOLR
Jenkins configuration to perform O&M operations
Spark

 What you’ll get…
An immediately-vested 401(K) with employer matching
Comprehensive medical, dental, and vision coverage
Tuition assistance, financing, and refinancing
Company-paid infertility treatments
Cross-training and professional development opportunities
Influence major initiatives
This position requires the candidate to have a current Top Secret security clearance and the ability to obtain a polygraph. Candidate must possess SCI eligibility.
ABOUT PERATON
Are you ready to join the next-generation of national security? Peraton is a fresh name in the industry with an established portfolio and legacy going back more than a century. We work differently than our peers – with agility, the freedom to innovate, an entrepreneurial spirit and a culture of responsibility. As part of the Peraton team, you’ll be part of our movement to build a great company, solve the most daunting challenges facing mankind today, to protect and promote freedom around the world, and to secure our future, for our families, our communities, our nation, and our way of life.
EEO STATEMENT
We are an Equal Opportunity/Affirmative Action Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law."
169,Data Engineer,"Arlington, VA 22201",Arlington,VA,22201,None Found,None Found,None Found,None Found,None Found,None Found,"Data Engineer
Arlington, VA, US
At Elder Research Inc., a recognized leader in data science and machine learning solutions, we pride ourselves in our ability to find creative, cutting edge solutions to real-world problems. We are looking for innovative and inquisitive self-starters who enjoy understanding a problem space and building fast, efficient, and tractable data infrastructure to deliver real value for our clients.

As a member of the Elder Research team, you will join a functional team of accomplished Data Scientists and Software Engineers that deliver custom analytic solutions. Some of your responsibilities will include: wrangling and fusing large and disparate data sets, assisting in the deployment of models and algorithms, automating the entire data pipeline, and communicating model results through user-focused data visualizations.

Job Description
A Data Engineer supports robust and repeatable data manipulation, large scale infrastructure for data ingestion, and stunning data visualization for custom client applications.
Essential Functions:
Â· Work collaboratively with data scientists, business consultants, and software engineers to create and deploy dynamic data applications that help our customers make meaningful business decisions.
Â· Develop and deploy robust data pipelines and end-to-end systems
Participate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance
Â· Provide leadership and coordination for certain stages of the engineering lifecycle as needed
Â· Perform other technical tasks as needed, including writing project reports, managing, implementing, and/or maintaining technical infrastructure, etc.
Â· Ability and the willingness to tailor applications to a clientâ€™s business goals using an iterative methodology.
Â· Ability to consider both long-term stability and scalability while taking a user-focused approach to development and deployment.
Â· Communicate clearly both verbally and in writing to teammates and clients
Â· Ability to work independently in a collaborative, dynamic, cross-functional environment
Â· Travel to and work on-site at clients both local and non-local. Number of days at client site vary depending on project requirements.

Required Skills:
Â· Bachelors or Masterâ€™s degree in Computer Science or related field, or equivalent experience
Â· Excellent written and verbal communication skills
Â· Ability to work with high-level mathematical concepts and associated code-form representations
Â· Focus areas: data manipulation, big data architecture, data structures, database administration, cloud platforms and SaaS, development operations (devops), data visualization and user experience

Selected Technologies: (A combination of experience with some of the following is required)

Databases:
SQL-based technologies (e.g. PostgreSQL and MySQL, Oracle)
NoSQL technologies (e.g. Cassandra, MongoDB, Graph Database)

Big Data:
Spark/Databricks (RDD, Data Frames, GraphX)
Hadoop (e.g. MapReduce, Hive and Pig)

ETL and Data Integration:
Kettle/Spoon, Luigi, Jenkins, Airflow, Nifi

Indexers/Search Engines:
ElasticSearch, Solr

Cloud:
AWS, Azure, stack configuration and management

Deployment:
Docker

Languages:
Python, Java, Scala, Familiarity with R

O/S:
UNIX, Linux, Solaris, ssh, git

Desired Skills
Â· Data manipulation, SQL, relational databases, and/or NoSQL databases â€“ experience as a DBA is a huge plus
Â· Cloud platform development and SaaS
Â· DevOps â€“ infrastructure, continuous integration and automation, packaging and deployment
Â· Consulting experience is a plus

About Elder Research, Inc.
Headquartered in Charlottesville, VA with offices in Arlington, VA, Baltimore, MD, and Raleigh, NC, Elder Research is a fast growing solutions and consulting firm specializing in predictive analytics. At Elder Research, youâ€™ll be part of a fun, friendly community. In keeping with our entrepreneurial spirit, we want candidates that are self-motivated with an innate curiosity and strong team work ethic. We work hard to provide the best value to our clients and allow each person to contribute their ideas and put their skills to use immediately.

Elder Research provides analytic solutions to hundreds of companies across numerous industries. Our team enjoys great variety in the type of work they do and exposure to a wide range of techniques and tools. If you are passionate about integrating data, technology, and analytics in a team-based environment to solve problems, then Elder Research may be a good fit for you.
Elder Research, Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.
Elder Research is unable to sponsor H1B visas for this role"
170,Big Data Engineer,"Ashburn, VA 20147",Ashburn,VA,20147,None Found,None Found,None Found,None Found,None Found,None Found,"What you’ll be doing...
Verizon’s Data Analytics, Insights and Enablement team works with some of the largest data sets collected from the largest, more reliable network. We are responsible for providing fast, clean, and relevant data for Verizon’s network and field organizations as well as key measurements and insights about performance to front line employees serving our customers. You will work side by side with Verizon team members and other partners to develop next generation technologies to ensure our networks are available when our customers need them through complex, scalable data platforms, with ever growing and interesting challenges.
Designing, architecting and developing data analytics systems for various use cases including but not limited to network performance and field operations.
Participating and contributing in engineering lifecycle including writing production code, building end-to-end machine learning solutions, conduct code reviews and working closely with infrastructure teams
Building scalable systems to process petabytes of wireline and wireless data to provide real-time insights into the health of Verizon networks.
Building batch and real-time data pipelines to ingest and transform data for model training/ testing; and building and deploy model inference pipeline.
What we’re looking for...
You will need to have:
Bachelor's degree or four or more years of work experience.
Six or more years of relevant work experience.
Even better if you have:
A degree in Computer Science, Engineering or related discipline.
Six or more years of architectural design experience, preferably focused on Hadoop, Open Source and Big Data.
Solid Software Development skills with proficiency in Java and Python.
Computer science fundamentals in object-oriented design, data structures and algorithms and complexity analysis.
Experience working in distributed computing with Apache Hadoop, Apache Spark, Apache Druid and data ingestion frameworks like Apache Gobblin, Logstash, open source data connectors, and real-time data streaming services like Apache Kafka, Apache Flink and or Apache Storm/Pulsar.
Experience with Machine learning tools like TensorFlow, scikit-learn, Pandas, Kera’s, etc.
Strong experience with Big data processing tools like Hadoop/Hive/HBase/Oozie/HDFS/Yarn.
Experience implementing and monitoring big data pipelines and working in a large, complex devops and CICD environment.
Experience in production scale software development with ML/AI use cases.
Strong background in basic machine learning techniques including Anomaly Detection, Time-series, Deep Learning, supervised and unsupervised learning.
Experience in building scalable solutions using advanced analytics and machine learning techniques.
Working in an Agile-team environment, with other data engineers, data scientist, ML engineers, etc.
Experience in feature engineering for both ML/DL models, A/B testing, data management and model governance.
Experience in productionizing ML/DL models with containers using Docker and Kubernetes.
Network domain knowledge.
Willingness to travel.
When you join Verizon...
You’ll have the power to go beyond – doing the work that’s transforming how people, businesses and things connect with each other. Not only do we provide the fastest and most reliable network for our customers, but we were first to 5G - a quantum leap in connectivity. Our connected solutions are making communities stronger and enabling energy efficiency. Here, you’ll have the ability to make an impact and create positive change. Whether you think in code, words, pictures or numbers, join our team of the best and brightest. We offer great pay, amazing benefits and opportunity to learn and grow in every role. Together we’ll go far.
Equal Employment Opportunity
We're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better."
171,Senior Data Engineer,"Washington, DC",Washington,DC,None Found,None Found,None Found,None Found,None Found,"Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.",None Found,"The Senior Data Engineer responsibilities include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of our client's Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatment, etc .

Required Education/Experience:
Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competencyMinimum of five (5) years technology industry or related experience, including items such as:Build highly scalable, scaled-out architectures on large scale database platformsExperience working in a complex data infrastructure environmentFive (5) years of experience in a data engineering roleExtensive and in depth data pipeline development experience with industry standard data integration toolsExperience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support.Experience with any industry standard tool for Source Control and Project ManagementExperience writing test cases and test scripts for data quality assuranceExperience creating stored procedures and functionsExperience developing dimensional data model with any industry standard tool.

Preferred:
Experience in Healthcare or related industryExperience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plusExperience productizing/automating predictive models that use R, SAS, Python, SPSS, etc.Continuous delivery and deployment automation for analytic solutions using tools like BambooFamiliarity with test driven development methodology for analytic solutionsAGILEAPI developmentData visualization and/or dashboard developmentDemonstrated ability to achieve stretch goals in a highly innovative and fast-paced environment"
