{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=New+York%2C+NY&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>-------------\\nAbout Teampay\\n-------------\\n\\nTeampay is the first purchasing software built for fast-growing, technology-enabled businesses. The way companies spend money has changed, rendering expense management tools obsolete. Our products empower businesses to request, approve and track spending in real-time.\\n\\nEvery day, more and more finance teams rely on Teampay to scale company purchasing. Led by an experienced team with prior exits, Teampay has raised double-digit millions of capital from prominent venture investors including CrossCut Ventures, Tribe Capital, Precursor Ventures, and CoVenture.\\n\\nWe're an agile team building software that's revolutionizing how companies spend their money. Joining Teampay at this early stage is an opportunity to grow your skill set, build a company, and get paid to do it.\\n\\n--------------\\nAbout the role\\n--------------\\n\\nAs a data engineer at Teampay, you will architect a data infrastructure that is stable and scalable to support data analytics, reporting, and visualization. Your input and contribution will have a direct impact on our data strategy and technology roadmap.\\n\\nWhat you'll do...\\n\\n\\nCombine and analyze data from various sources to help drive business insights\\nWork with various team and executive stakeholders to define mission-critical metrics and key performance indicators\\nDevelop and maintain a scalable data infrastructure\\nDevelop tools supporting self-service data pipeline management\\nWork closely with teams on design and implementation of data solutions\\nOwn and provide BI development tools for internal use\\n\\nCommon Candidate Qualifications include…\\n\\n\\n3+ years functional experience in a data engineering or business intelligence role\\nProficient in data modeling and systems design skills\\nProficient in SQL\\nProficient in at least one scripting language (ideally Python)\\nExperience building and maintaining robust ETL pipelines\\nExperience with version control systems\\nExperience with BI platforms\\n\\nNice to Have...\\n\\n\\nExperience with AWS\\nMachine learning experience\\nYou enjoy telling a compelling story with data\\n\\nYou would be the first Data Engineer in a seed stage, fast-growing company. As a core member of the engineering team, you will have a strong impact on the future of Teampay. This is an unusual opportunity to join a business with traction at the ground floor.\\n\\n----------------------\\nApply at Teampay if...\\n----------------------\\n\\nYou're a builder. You're passionate about crafting things that matter. You're curious and agile in thought and action. You value authenticity and possess a strong work ethic. You're empathetic and look forward to learning from people unlike yourself. You want to make an impact with a strong team. You look for challenges that force you to grow. You rarely miss a detail and always learn from your mistakes. You have diverse interests outside of work, but are ready to pitch in and be responsive when the pressure is on.\\n\\n-----------\\nInterested?\\n-----------\\n\\nYou can learn more about the product at www.teampay.co ( http://www.teampay.co/ ) and if you'd like to apply, please include a cover note and tell us about something you started, whether it's a club, a team, business or blog.\\n\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Engineer – Datasets</td>\n",
       "      <td>New York, NY 10011</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10011</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data and information has become an invaluable asset for technology companies that focus on innovation and are dedicated to providing a great user experience. Spotify is taking this concept to heart and driving a “Data as a Product” initiative for its own data consumers within the company. The volume and breadth of data at Spotify is staggering – billions of records of streamed music, app interactions, artist information and user behavior trends flow through our platform on a daily basis. The Vivaldi squad and it’s Tribe are at the center of this initiative. We design and implement new ways and strategies to empower the Spotify community of data scientists, ML, researchers, product designers, fraud investigators, business analysts and the CEO himself.\\n\\nWhat you will do\\nUnderstanding what fuels many of Spotify’s product features such as Discover Weekly, Daily Mix, Podcast offerings, holiday campaigns and others\\nWorking hand-in-hand with the data science community to understand various user or content trends that influence product changes and customer acquisition strategies\\nGetting hands-on experience with Google Cloud Platform and technology/languages such as BigQuery, Scala, Scio, Luigi, Styx and Docker\\nCollaboration on a global scale; our squad offers ongoing opportunities to work in Stockholm with other engineering colleagues\\nCross departmental exposure and flexibility to engage with many teams in the company\\nGaining technical expertise in building a data platform at scale to solve business, product and technical use cases\\nWorking in a supportive team that offers engineers the flexibility to be creative and chase interesting ideas\\nWork closely with cross-functional teams of data and backend engineers, analysts, user researchers, product managers and designers\\nCommunicate insights and recommendations to key stakeholders, engineering and product partners\\n… and of course, having fun! Being passionate about what you do also means celebrating milestones within the team and the tribe!\\nWho you are\\nAn BS/MS in CS or any other relevant fields of study\\nAt least 3 years of work experience in the Data Engineering and Big Data field\\nStrong analytical and problem solving ability\\nCoding skills for analytics and data engineering/manipulation (Scala, Java and Python)\\nStrong communication and data presentation skills (such as Tableau, PowerPoint, Qlik, etc.)\\nExperience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Google’s Cloud Platform\\nYou are capable of tackling very loosely defined problems and thrive when working on a team which has autonomy in their day to day decisions\\nYou are a communicative person that values building strong relationships with colleagues and multiple stakeholders, and have the ability to explain complex topics in simple terms\\nIdeally you have experience working in a large scale, global consumer product company, in an engineering or insights role</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Director of Data Science</td>\n",
       "      <td>New York, NY 10005</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10005</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>DIRECTOR OF DATA SCIENCE\\n\\nWho We Are\\n\\nMurmuration transforms how political campaigns, advocates, and organizers identify, engage, and mobilize people and communities. Our focus is on driving change and accelerating progress toward a future where every child in America has the opportunity to benefit from a high-quality public education. Our partners include the leading practitioners and funders of efforts to ensure access for all children to a high-quality public education.\\n\\nWhat We Do\\n\\nAt Murmuration, we provide our partners with the skills, knowledge, and tools they need to drive sustained political change—at all levels of government and aspects of civic life. Through the use of predictive intelligence and easy-to-use tools, Murmuration’s partners make informed decisions about who they need to reach, what they need to say, and how to achieve and sustain impact. Every application of this work, as well as shared knowledge and best practices, further improves our collective ability to organize communities and electoral campaigns.\\n\\nMurmuration offers a fully featured political campaign platform that allows our partners to execute large scale electoral, advocacy and organizing efforts across the United States. The m{insights toolset includes a web platform that sits on top of an aggregated dataset from a wide variety of sources, including publicly available data, consumer data, voter-file data, and membership data furnished by Murmuration’s partners. Partners are also offered sophisticated targeting and outreach tools to help activate key audiences and expand their base of support. Murmuration’s data science team also provides deep analysis on our partners’ work, helping craft experiments and polls, building predictive models and measuring impact.\\n\\nAbout the Position\\n\\nThe Director of Data Science will lead a team of analysts and data scientists charged with managing a suite of analytics products that will position our partners for greater impact. The team’s current main projects include: building machine-learning models to predict voter opinions and behavior, running national scale polls for our sector and partner-specific polls on request, and constructing large scale randomized control trials to determine the effectiveness of various outreach methodologies in local and state elections. The Director will also have the remit and be encouraged to expand the scope of analytic offerings for our partners.\\n\\nThe Analytics team is a highly collaborative, friendly, and hard-working group, and we are looking for a Director that embodies those values. The Director of Data Science will report to the Chief Data Scientist and work in close tandem with the Director of Data Management and our Lead Data Engineer.\\n\\nThe Director will:\\n\\nBe an excellent personnel manager, with a track record of providing constructive feedback to direct reports and of helping employees develop their skills and careers\\nHelp manage and coordinate Murmuration’s day-to-day analytics projects, including managing the resources of the Analytics team and planning for scalable growth in order to provide continued high-quality analytics and data science to our partners\\nInteract with our partner organizations to understand their analytic needs, in order to better determine the course of our R&amp;D work\\nCollaborate with the Chief Data Scientist, to define future research priorities and R&amp;D efforts that streamline the production of our existing analytics projects\\nLiaise with the VP of Partnerships to determine project delivery timelines and determine the staffing needs and assignments for partner engagements\\nIdentify staffing needs and collaborate with HR to hire team members as we grow\\nProvide input and recommendations on the future composition and remit of the Analytics team\\nCandidate Profile\\n\\nMurmuration attracts employees with distinctive and diverse backgrounds and accomplishments. Integrity, creativity, flexibility, and drive are key attributes of competitive candidates.\\n\\nThe ideal candidate will have most of the following qualities:\\n\\nExperience managing an analytics team, with a track record of achieving project goals and meeting deadlines while fostering a healthy team environment\\nFamiliarity with modern analytics techniques in statistics and machine learning; it is ideal but not required to be an experienced data scientist, however experience managing data scientists is required\\nStrong problem solving skills as well as the ability to manage several tasks/projects concurrently and prioritize work effectively\\nBe an exceptional team player, with strong interpersonal skills\\nStrong communication skills to interact effectively with various internal and external stakeholders to develop analytics strategy and deepen our partnerships\\n\\nLocation, Compensation and Benefits\\n\\nThe Director of Data Science is a full-time, salaried position based onsite in New York City, with a comprehensive benefits package. Salary for this position is commensurate with experience.\\n\\nAn Equal-Opportunity Employer with a Commitment to Diversity\\n\\nMurmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>New York, NY 10017</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10017</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Senior Data Engineer\\nNew York, NY\\nAre you an experienced Data Engineer who designs and implements innovative solutions that leverage modern tools and technologies?\\nDo you share our passion for enabling positive change within healthcare and helping patients with chronic conditions like diabetes?\\nIf so, you could be a perfect fit for our team of like-minded professionals who share a common mission and passion for helping others and a desire to build a great company. Come and work with us to build our next generation healthcare platform!\\nCecilia Health is a high-growth, venture-backed healthcare company based in New York City. We partner with pharmaceutical &amp; device companies, payers and ACOs to deliver personalized, technology-enabled coaching to improve treatment, adherence and health outcomes for people living with diabetes and other chronic conditions. Cecelia Health is a high-energy, results-oriented work place that believes our success, as well as the success of our customers and patients, relies primarily on a fantastic team with the passion, drive and skills to change the face of chronic condition management.\\nWe are hiring a Senior Data Engineer in New York City. This role will report to our Chief Technology Officer and join a strong technology team that is continuously innovating our current solutions that allow clinicians to efficiently serve an increasing volume of patients.\\nWHO YOU ARE\\nYou have data in your DNA and at least 5 years of relevant work experience in data engineering requiring application of analytic skills to integrate data into business operations. You are an expert in using modern technology stacks to build intelligent ETL and data processing pipelines. You've worked in a variety of data wrangling roles and have strong knowledge of data manipulation using advanced SQL and other tools. You have a solid understanding of engineering best practices, as well as familiarity with scaling DevOps and engineering initiatives. You want to help develop software platforms that have real-world impact on people with chronic diseases like diabetes.\\nPOSITION RESPONSIBILITIES\\nDefine and lead data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage\\nApply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification\\nImprove data sharing, increase data repurposing and improve efficiency associated with data management efforts\\nBuild best practices to support tracking chain of custody of data so it can be easily traced back to the source for accuracy and consistency\\nResponsible for the design, development, and documentation of data models and ETL processes that will feed various databases including a data warehouse/lake and various analytics/data science components\\nSupport and enhance existing applications and automation processes on a daily basis using a combination of SSIS, C#, .NET Core, Python, PowerShell, SQL Server, and Snowflake databases\\nEnsure production service levels, performance quality, and resolution of data load failures\\nManage multiple projects independently\\nEvaluate and recommend tools, technologies and processes to ensure the highest quality product platform\\nQUALIFICATIONS\\nRequired Experience\\nBachelor's degree in information science, computer science, engineering or a similar area, with 5+ years of related experience, or comparable real-world development experience\\nMaster Data Management experience including data consolidation, linkage, federation and dissemination\\nStrong knowledge of Python for Data Engineering\\nAdvanced SQL experience (Nested Queries, Complex Joins, Analytic Functions, Time Series)\\nExperienced working in Agile/Dev Operations environment with continuous integration and continuous deployment and application lifecycle management\\nStrong communication skills\\nDesired Experience\\nData integration, application development and secure information management in healthcare, life sciences, or clinical research\\nExperience with scalable, enterprise-level development on virtualized (AWS, Azure, GCP) infrastructure\\nExperience with a cloud-based data warehouse such as Snowflake or Redshift\\nFamiliarity with analytics and business intelligence tools (Tableau, Power BI, Cognos).\\nExperience with Real Time processing and Big Data tools (Hive, Spark, Hadoop, HDFS, Kafka, Lucene, Glue, Airflow…)\\nD9aAMpL07D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Software Engineer - Data</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMaster's or bachelor's degree in Computer Science\\nAt least 10 years of hands-on software development experience in Python, Golang, Java, C++ or Scala\\nStrong Object Oriented Programming skills\\nDeep knowledge in data structures, algorithms, and software design\\nExperience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources\\nHighly proficient with relational and non-relational data storages\\nStrong verbal and written communication skills</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nWork with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs\\nAdheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time\\nAble to define new architectures and improve existing ones\\nCan be the central focus for code reviews, architecture discussion and bug fixing\\nDemonstrates code and product ownership in production\\nSupport the business teams and product managers in data extracts and data analysis\\nPerforms as a true agile team leader and exhibits competencies in all layers of the application stack\\nDemonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Lucid is a market research platform that provides access to authentic, first-party data in over 90 countries. Our products and services enable anyone, in any industry, to ask questions of targeted audiences and find the answers they need – fast. These answers can be used to uncover consumer motivations, increase revenue, and measure the impact of digital advertising. Founded in 2010, Lucid is headquartered in New Orleans, LA with offices in Dallas, New York, London, Sydney, Singapore, Gurgaon, Prague, and Hamburg.\\n\\n\\nApply for this Job\\n\\nThe Opportunity\\n\\nThe Senior Data Engineer will be a key part of our development team and work closely with development team peers, Product Management team, and business and other support teams.\\nResponsibilities\\nWork with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs\\nAdheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time\\nAble to define new architectures and improve existing ones\\nCan be the central focus for code reviews, architecture discussion and bug fixing\\nDemonstrates code and product ownership in production\\nSupport the business teams and product managers in data extracts and data analysis\\nPerforms as a true agile team leader and exhibits competencies in all layers of the application stack\\nDemonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration\\nQualifications\\nMaster's or bachelor's degree in Computer Science\\nAt least 10 years of hands-on software development experience in Python, Golang, Java, C++ or Scala\\nStrong Object Oriented Programming skills\\nDeep knowledge in data structures, algorithms, and software design\\nExperience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources\\nHighly proficient with relational and non-relational data storages\\nStrong verbal and written communication skills\\nPreferred Qualifications\\nAWS experience with S3, RDS, Redshift, EMR, Kinesis, Lambda, Elastic Beanstalk and Elasticsearch\\nExperience with Spark, Hadoop and Hive on EMR and non-managed. Can build and run a basic cluster\\nExperience developing ETLs and running job schedulers (e.g. Airflow)\\nExperience with PostgreSQL\\nExperience with NoSQL data sources including Cassandra\\nExperience with Kafka, Redis\\nExperience creating Data Lakes\\n\\n\\nAt Lucid we foster a collaborative and inspiring workplace. We pride ourselves in doing this by recruiting, hiring and retaining diverse, passionate, and forward-thinking talent. Lucid is committed to and encourages an inclusive environment and we are dedicated to providing equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please let us know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Engineer (Scala/Spark)</td>\n",
       "      <td>New York, NY 10010</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10010</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>dv01 is the world's first end-to-end data management, reporting and analytics platform offering loan level transparency and insight into lending markets, making them more efficient for institutional investors and safer for the world. In a nutshell, we're doing our part to prevent a repeat of 2008.\\n\\nAs the technological hub between lenders and capital markets, dv01 provides all parties with unprecedented data transparency, insight, and analytics. dv01 has integrated data from 16 marketplace lending platforms, including LendingClub, Prosper and SoFi and multiple mortgage servicers. To date, dv01 has provided reporting and analytics on $105 billion of online lending and mortgage loans and $35 billion of securitization coverage.\\n\\nTo get a better idea of what a year at dv01 looks like, check out our 2018 Year in Review page here: https://dv01.co/2018yearinreview/ ( https://dv01.co/2018yearinreview/ ). If that looks like fun to you, get in touch because we'd love to hear from you.\\n\\nYOU WILL:\\n---------\\n\\nOwn our data platform. You will own the infrastructure for the data platform that powers all of dv01's customer offerings. This includes an Apache Spark cluster, SQL Server databases, scheduling/monitoring tools, API crawlers, SFTP servers, and ad-hoc analysis services such as Apache Zeppelin and RStudio—all hosted on the cloud.\\n\\nWork extensively with open source technology. You'll work heavily within the Spark ecosystem, as well as explore new open source technologies to solve customer needs. The skills you develop here will serve you well beyond dv01.\\n\\nInteract with a diverse team. You will collaborate closely with other teams at dv01 to expand and improve the capabilities of our data platform. This includes working closely with the structuring team to resolve scaling or functionality pain points in data processing, the modeling team to build out a modern data science platform, and the frontend product engineering team to scale database queries to ship new products.\\n\\nGain knowledge of the financial industry. You'll have an exclusive view into the system that enables Americans to afford houses, cars, and college. You will learn about the participants, terminology, and mathematics behind this sector of the financial markets.\\n\\nQUALIFICATIONS:\\n---------------\\n\\nExperienced with all aspects of working with big data. You have 2+ years of professional data engineering experience working with large data sets and are intimately familiar with the construction of scalable ETL pipelines, intricacies of accurate data processing, and infrastructure involved with ensuring the reliability of hundreds of daily processes.\\n\\nA Spark Enthusiast. You have at least one year of professional programming experience with Apache Spark and a strong understanding of Spark internals and the operational complexities of managing a Spark cluster. You have engaged with the Spark community on mailing lists or on Github and may have contributed source code to the project.\\n\\nA well-rounded engineer. You have 3+ years of professional programming experience with Scala or Java and a deep appreciation for engineering fundamentals. You understand the importance of writing tests, designing systems for long term maintainability, and evaluating both sides of common engineering considerations.\\n\\nEnthusiastic about expanding your financial knowledge. You are excited to learn more about the intricacies of the financial system. You are adept at learning on the job and unafraid to dive into technical finance books.\\n\\nUndergraduate or graduate degree in Finance, Math, or Engineering. Note that we're not anti dropouts if you're a superstar.\\n\\nPerks and Benefits:\\nAlmost 100% Paid Benefits (medical/dental/vision)\\nAnnual Professional Development Stipend\\nMonthly Commuter Budget\\nDaily Lunch and Dinner Allowance\\nFree Premium Equinox or ClassPass Membership\\nUnlimited PTO and Remote days\\nCasual, collaborative culture\\nCompany Outings (Happy Hours, Team Yoga, Book Club, etc.)\\n\\ndv01 is an equal opportunity employer and all qualified applicants and employees will receive consideration for employment opportunities without regard to race, color, religion, creed, sex, sexual orientation, gender identity or expression, age, national origin or ancestry, citizenship, veteran status, membership in the uniformed services, disability, genetic information or any other basis protected by applicable law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY 10022</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10022</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBS/MS in Computer Science or a related technical field\\nSeeking candidates with 1+ years of experience in:\\nArchitecting, building, and maintaining end-to-end, high-throughput data systems and their supporting services\\nDesigning data systems that are secure, testable, and modular, particularly in Python, as well as their support infrastructure (shell scripts, job schedulers, message queues, etc.)\\nDesigning efficient data structures and database schemas\\nWorking with distributed systems architecture\\nIncorporating data processing and workflow management tools into pipeline design (AWS EMR, Airflow, Kafka, etc.)\\nUsing profiling tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements\\nDeveloping for continuous integration and automated deployments\\nUtilizing a variety of data stores, including data warehouses (ideally Redshift), RDBMSes (ideally MySQL), in-memory caches (ideally Aerospike and Redis), and searchable document DBs (ideally Elasticseach)\\nWrangling large-scale data sets</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nShip high-quality, well-tested, secure, and maintainable code\\nDesign, develop, and maintain data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions\\nManage automated unit and integration test suites\\nWork collaboratively and communicate effectively with a small, motivated team of engineers and product managers\\nExperiment with and recommend new technologies that simplify or improve PromoteIQ's stack\\nParticipate in an on-call rotation and work occasional off-hours</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>PromoteIQ delivers intelligent vendor marketing solutions built for the next generation of e-commerce. Our solutions help retailers implement, automate, and scale brand-funded marketing programs on e-commerce sites. PromoteIQ is a New York City-based technology company that works with the US's largest e-commerce retailers. Learn more about us at https://www.promoteiq.com.\\n\\nWho we’re looking for\\nAt PromoteIQ, data plays an integral role in our product, and software engineers on our data engineering team build the pipelines that power reporting and analytics for our e-commerce promotions platform. The infrastructure and applications that you'll build on the data engineering team will have broad and critical reach in powering real-time auction decisions, becoming multipliers on our revenues, and forecasting supply and demand for our customers.\\nResponsibilities\\nShip high-quality, well-tested, secure, and maintainable code\\nDesign, develop, and maintain data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions\\nManage automated unit and integration test suites\\nWork collaboratively and communicate effectively with a small, motivated team of engineers and product managers\\nExperiment with and recommend new technologies that simplify or improve PromoteIQ's stack\\nParticipate in an on-call rotation and work occasional off-hours\\nQualifications\\nBS/MS in Computer Science or a related technical field\\nSeeking candidates with 1+ years of experience in:\\nArchitecting, building, and maintaining end-to-end, high-throughput data systems and their supporting services\\nDesigning data systems that are secure, testable, and modular, particularly in Python, as well as their support infrastructure (shell scripts, job schedulers, message queues, etc.)\\nDesigning efficient data structures and database schemas\\nWorking with distributed systems architecture\\nIncorporating data processing and workflow management tools into pipeline design (AWS EMR, Airflow, Kafka, etc.)\\nUsing profiling tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements\\nDeveloping for continuous integration and automated deployments\\nUtilizing a variety of data stores, including data warehouses (ideally Redshift), RDBMSes (ideally MySQL), in-memory caches (ideally Aerospike and Redis), and searchable document DBs (ideally Elasticseach)\\nWrangling large-scale data sets\\n\\n#MicrosoftAdvertising #PromoteIQ\\n\\nMicrosoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.\\n\\nBenefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Us\\n--------\\n\\nNS1's mission is to manage the world's application traffic. We are the market leader in DNS and traffic management software and services, and our customers include the biggest properties and largest enterprises on the internet, such as Salesforce, LinkedIn, Squarespace, Pandora, Imgur, Yelp, Dropbox, and many more. Our modern DNS technologies enable optimized application delivery, couple via our APIs into the tooling and processes of today's DevOps organizations, and deliver reliability and performance at global scale. We operate a worldwide, highly tuned Managed DNS network, and also deliver our technologies to customers as single-tenant software deployments. We solve incredibly challenging problems on behalf of our customers, in the most mission critical parts of their stack.\\n\\nThe Role\\n--------\\n\\nNS1 is looking to hire an experienced data engineer (or backend developer interested in specializing in data) to join our growing data science team. As an engineer embedded within the data team, you will be responsible for architecting and deploying our big data analytics infrastructure, creating ETL pipelines and helping the team test and deploy machine learning models. We are seeking someone who can leverage existing technical strengths in distributed systems, immerse themselves in our platform quickly, and teach themselves any systems that are new to them.\\n\\nYour Skills\\n-----------\\n\\nData engineers come from all backgrounds. Even if you do not meet all the requirements, send us a note telling us why you are interested. In general, you would be a strong candidate if you are familiar with more than a few of the following:\\n\\n\\nPython, Go, Scala, C++, Node.js\\nExtensive experience of databases and data modeling\\nData pipelining, ETL, statistics\\nAWS: Kinesis, DynamoDB, Lambda, SQS\\nSpark, Hadoop, MapReduce\\nExperience with docker\\nExcellent UNIX and networking knowledge\\nStrong Linux systems programming skills\\nKnowledge of distributed systems\\nAbility to design with modern internet infrastructure operation in mind, including metrics, logging, automation, etc\\n\\nOur technology stack at NS1, and the many systems you'll have an opportunity to work with here:\\n-----------------------------------------------------------------------------------------------------\\n\\n\\nOur globally distributed platform is comprised of many subsystems including:\\nCustom built DNS software that's deployed on physical hardware and an anycasted network that spans nearly 30 facilities globally\\nREST API, and Portal\\nDeployment automation, CI/CD, unit/integration testing\\nMonitoring, metrics collection and alerting\\nTraffic load balancing, filtering, and DDoS mitigation tools\\nMessaging, persistent DB and caching systems\\nOther technologies and integrations include:\\nLinux, Ansible, Docker &amp; other container platforms\\nBGP, BPF/IPTables, SDN, packet analysis\\nMongoDB, Redis, RabbitMQ, SQL\\nPython (Twisted), Bash, C, C++14, React, Redux, D3\\nHadoop/HDFS/OpenTSDB, Grafana, Bosun\\nIntegrations with third party SaaS, APIs, and libraries, various Open Source projects including REST API clients and integrations\\n\\nWorking @ NS1\\n-------------\\n\\nWe're a fast-growing, well-funded startup based in the heart of New York City's Financial District with offices and team members around the world. Working at NS1, you'll come to understand our team is unique, both in and out of the workplace. We have PhDs, musicians, artists, and athletes working side by side, dedicated to delivering first class products. We're hardworking, but we're also a compassionate group. We understand that outside of NS1 is a world that places demands on our time. Our leadership team is dedicated to open and honest communication and we continuously strive to foster a culture of transparency, flexibility, and creativity.\\n\\nWe offer:\\n\\ncompetitive compensation (salary and stock options)\\nmedical, dental, and vision\\ncommuter benefits\\n401k\\nflexible hours and time off\\nchoice of workstation\\n\\nNS1 is an equal opportunity employer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Engineer - Product</td>\n",
       "      <td>New York, NY 10176</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10176</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer - Product\\nREF#: 35132\\nCBS BUSINESS UNIT: Showtime\\nJOB TYPE: Full-Time Staff\\nJOB SCHEDULE: Full-Time\\nJOB LOCATION: New York, NY\\nDESCRIPTION:\\nThe Showtime Product team is looking for a curious and creative Data Engineer to help us pursue answers to our increasingly interesting and complex business questions and empower our team to incorporate data-driven features and machine learning into our products, which include our standalone service SHOWTIME and our TV Everywhere service, Showtime Anytime.\\nThe big data platform at Showtime is relatively new, but is now being used across the company for critical functions and features like: recommendations, analyzing customer experience, understanding programming consumption’s effect on subscriber lifetime, building churn/retention prediction models, and more. In this role, you will work our dedicated Product Analytics team, the Showtime Research and Data Strategy teams, the CRM team and our in-house engineering team, and you will architect and enable technologies, systems and workflows that enable our analysts and data scientists to focus more on algorithms and analyses than on the associated engineering.\\nIdeal candidates will be innovative, self-motivated, a quick study, and willing to develop new skills while constantly improving existing abilities.\\nKey Technologies:\\nJava, Scala, Groovy, Spark, AWS, AWS/EMR, Spring, Mongo, Git, Redis, Bamboo, JIRA etc\\nResponsibilities\\nDevelop understanding of key business, product and user questions.\\nCollaborate with other Engineering team members to develop, test and support data-related initiatives. Work with other departments to understand their data needs.\\nEvolve data-driven feature prototypes into production features that scale; streamline feature engineering, so that the underlying data is efficiently extracted.\\nBuild flexible data pipelines that we can rapidly evolve as our needs change and capabilities grow.\\nDevelop and enhance data warehouse in AWS S3.\\nEmploy data mining, segmentation, and other analytical techniques to capture important trends in our user base.\\nQUALIFICATIONS:\\n3+ years of relevant experience in a comparable data engineering role\\nExpert-level knowledge of SQL/Spark SQL\\nExperience in pursuing and applying data-backed decisions, such as recommendations, trends etc. to make the core product better\\nYou like to dive-deep on data analysis or technical issues to come up with effective solutions and are comfortable summarizing key insights graphically when needed\\nYou believe in writing code that is easy to understand, test and maintain\\nYou enjoy a workplace that values autonomy, applauds ideas and a enjoys a sense of humor\\nABOUT US:\\nSHOWTIME continues to make its mark across the cultural landscape with one of the most successful programming lineups in television. The SHOWTIME programming slate features original series including Emmy® nominated limited series ESCAPE AT DANNEMORA, BILLIONS, HOMELAND, SHAMELESS, THE CHI, RAY DONOVAN, THE AFFAIR, KIDDING, BLACK MONDAY, THE LOUDEST VOICE, CITY ON A HILL and ON BECOMING A GOD IN CENTRAL FLORIDA. SHOWTIME continues to raise the bar with fresh content including upcoming series THE L WORD: GENERATION Q, BACK TO LIFE, THE GOOD LORD BIRD and WORK IN PROGRESS. The network’s eclectic, brand-defining programming is further distinguished by the captivating offerings of SHOWTIME Documentary Films, including docuseries THE CIRCUS: INSIDE THE WILDEST POLITICAL SHOW ON EARTH, Emmy-nominated WU-TANG CLAN: OF MICS AND MEN and THE FOURTH ESTATE and upcoming documentary films THE KINGMAKER and READY FOR WAR. SHOWTIME Sports continues to dominate with its flagship franchise SHOWTIME CHAMPIONSHIP BOXING® and the Emmy Award-winning series INSIDE THE NFL. SHOWTIME is currently available to subscribers via cable, DBS and telco providers, and as a stand-alone streaming service through Amazon, Apple®, Google, LG Smart TVs, Oculus Go, Roku®, Samsung and Xbox One. Consumers can also subscribe to SHOWTIME via Amazon’s Prime Video Channels, DirecTV Now, FuboTV, Hulu, Sling TV, Sony PlayStation™ Vue and YouTube TV. The network’s authentication service, SHOWTIME ANYTIME, is available at no additional cost to SHOWTIME customers who subscribe to the network through participating providers. Subscribers can also watch on their computers at [1] www.showtime.com and [2] www.showtimeanytime.com.\\nReferences\\nVisible links\\nhttp://www.showtime.com\\nhttp://www.showtimeanytime.com\\nEEO STATEMENT:\\nEqual Opportunity Employer Minorities/Women/Veterans/Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience with data dictionary concepts, data mapping, and data architecture.\\nKnowledge and experience with a wide range of tools and IT technologies.\\nProven ability to research and learn tools, hardware, and languages quickly.\\nUnderstanding of data analysis strategies and concepts (e.g. business intelligence, time series analysis, project management).\\nExcellent interpersonal, organizational, and communication skills required.\\nMust be a self-starter with the ability to work independently and manage multiple long-term projects.\\nStrong attention to detail and high concern for data accuracy.\\nAbility to interact with all levels of staff, with a high regard for confidentiality and diplomacy.\\nAbility to work efficiently to meet deadlines.\\nDependable team player who works collaboratively and cooperatively with staff in a team-oriented environment.\\nAbility to multi-task in a fast-paced environment, prioritize among competing needs and respond quickly to requests for information.\\nAbility to follow directions and apply proper policies, procedures and guidelines.\\nResourcefulness, initiative, and good judgment essential.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDevelop expertise in the DANY data model.\\nDevelop data dictionaries and best practices for data definitions and use.\\nCoordinate with network administrative staff to develop, implement, and maintain a secure and agile sandbox environment.\\nIdentify tools and procedures to effectively use data.\\nResearch and promote options for quick development.\\nDocument standards for security and maintainability.\\nSupervise developers hired for special projects.\\nInteract with Enterprise projects development team to coordinate data collection, standards, and use.\\nPerform related tasks as assigned.\\n</td>\n",
       "      <td>\\nBachelor's degree in Information Technology or Computer Science/Engineering, Machine Learning or a related field required.\\nMaster's degree preferred.\\n</td>\n",
       "      <td>\\nBachelor's degree in Information Technology or Computer Science/Engineering, Machine Learning or a related field required.\\nMaster's degree preferred.\\n</td>\n",
       "      <td>Position Summary:\\nThe New York County District Attorney's Office (DANY) has an opening for a Senior Data Engineer in its Information Technology (IT) Department. DANY's IT Department provides agency-wide IT solutions for investigations, prosecution support, and case management. The Senior Data Engineer will work with key data sets used in the prosecution process and other criminal justice initiatives. The Senior Data Engineer will also be a key force in the development of tools and appropriate environment for the effective and progressive use of the data to further criminal prosecutions and justice initiatives.\\n\\nResponsibilities include but are not limited to:\\n\\nDevelop expertise in the DANY data model.\\nDevelop data dictionaries and best practices for data definitions and use.\\nCoordinate with network administrative staff to develop, implement, and maintain a secure and agile sandbox environment.\\nIdentify tools and procedures to effectively use data.\\nResearch and promote options for quick development.\\nDocument standards for security and maintainability.\\nSupervise developers hired for special projects.\\nInteract with Enterprise projects development team to coordinate data collection, standards, and use.\\nPerform related tasks as assigned.\\n\\nQualifications:\\n\\nExperience with data dictionary concepts, data mapping, and data architecture.\\nKnowledge and experience with a wide range of tools and IT technologies.\\nProven ability to research and learn tools, hardware, and languages quickly.\\nUnderstanding of data analysis strategies and concepts (e.g. business intelligence, time series analysis, project management).\\nExcellent interpersonal, organizational, and communication skills required.\\nMust be a self-starter with the ability to work independently and manage multiple long-term projects.\\nStrong attention to detail and high concern for data accuracy.\\nAbility to interact with all levels of staff, with a high regard for confidentiality and diplomacy.\\nAbility to work efficiently to meet deadlines.\\nDependable team player who works collaboratively and cooperatively with staff in a team-oriented environment.\\nAbility to multi-task in a fast-paced environment, prioritize among competing needs and respond quickly to requests for information.\\nAbility to follow directions and apply proper policies, procedures and guidelines.\\nResourcefulness, initiative, and good judgment essential.\\n\\nEducational Requirements:\\n\\nBachelor's degree in Information Technology or Computer Science/Engineering, Machine Learning or a related field required.\\nMaster's degree preferred.\\n\\nCommitment:\\n\\nOne (1) year commitment to hiring department.\\n\\nThe New York County District Attorney's Office is an Equal Opportunity Employer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bioinformatics Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Description\\nElysium is seeking a bioinformatics data engineer. In this position, you will be an essential member of our team and support new product development. You will work with multidisciplinary teams to build complex epigenomic data analysis pipelines and cloud infrastructure.\\n\\nResponsibilities\\n\\nYou will:\\n\\nDevelop and maintain programmatic tools and workflows to automate the analysis of biological data\\nDesign and develop databases and interfaces for the management of biological data\\nProcess, integrate, and analyze biological datasets to reveal insights that enable testable hypotheses\\nImplement quality control measures and benchmarking methods into our bioinformatics solutions\\nStay up-to-date with available bioinformatics tools through literature review and computational experiments\\nContribute to a team of translational researchers committed to improving health\\nPlace a priority on getting results with an emphasis on high quality outcomes\\nDisplay a willingness to challenge the status quo and take risks\\nPossess a strong sense of urgency that drives performance beyond expectation\\n\\nRequirements\\n\\nYou have:\\n\\nMS or PhD in Bioinformatics, Computational Biology, Computer Science, or a related field\\n2+ years of experience (preferably industry-based) processing, managing, and integrating biological datasets\\n2+ years of programming experience in a production environment\\nProficiency with R and Bioconductor packages, and one or more programming languages (Python, Perl, etc)\\nFamiliarity with cloud services and cloud management systems\\nExcellent collaboration and communication skills, both verbal and written\\n\\nAbout Elysium\\nElysium is building the world's first direct-to-consumer health science company, developing advanced natural products that go beyond nutritional and dietary supplements to address cellular health at the most fundamental levels. Using new technologies, we can now impact wellness at an unprecedented scale - from cellular and metabolic processes to entire systems and pathways. We are growing our operational team to work with over 30 scientists and medical pioneers, as well as several Nobel Laureates, to bring scientific breakthroughs in health directly to the home.\\n\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Engineer Lead</td>\n",
       "      <td>New York, NY 10001</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10001</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Responsibilities include but not limited:\\nManage technical delivery related to Repo, Collateral &amp; Margin platforms for Business initiatives and Front-to-Back programs.\\nFormulate and execute strategy to mitigate/eliminate the End-of-Life technical stack (components and dependencies).\\nManage technical buildouts for critical margin services workflows, straight-through processing &amp; collateral valuations.\\nConsolidate &amp; standardize technical components supporting business processes (e.g. Margin Issuance, Disputes and Settlements).\\nTechnical lead with a strong focus on delivery – impact analysis, design, hands-on development, and implementation.\\n Machine learning/AI/NLP\\nExpectation/ Responsibilities:\\nAbility to source, aggregate, mine and analyze data from structured and unstructured sources.\\nDefine, prototype PoC, develop, validate and deploy solutions based on Machine Learning models\\nDevelop processes and tools to monitor and analyze model performance and data accuracy.\\nGood understanding of Natural Language Processing, Knowledge graph, named entity analysis\\nThorough understanding of statistical concepts (distribution, regression techniques, and statistical tests)\\n Qualifications:\\nStrong programming ability with major NLP toolkits – Open NLP/ Stanford etc.\\nStrong programming skills with Python &amp; packages- Scipy, Scikit-learn, NLTK, Flask etc.\\nExperience visualizing/presenting data – Pyplot, Tableau/ Power BI.\\nExperience with distributed data/computing tools: Map/Reduce, Hadoop, &amp; Spark.\\nStrong experience working with Sybase &amp; Oracle and NOSQL Data sources (Mongo).\\nStrong experience processing information in multiple formats Excel, CSV, XML, Fixed Length, JSON etc.\\nExperience working with projects based on machine learning and statistics.\\nStrong experience in machine learning models Logistic Regression, Linear Regression, Random Forest, etc.\\nExperience working with cloud services –AWS and Azure- Dynamo, S3, Cosmos, ML Studio.\\nExposure to RPA with BluePrism is nice to have.\\nExperience in real-time distributed messaging, multi-threaded trading, streaming data and elastic search is nice to have.\\nBachelor's in Computer Science or Engineering Field\\nJava Stack\\nOther Qualifications:\\nStrong background and hands on experience with server side Java Stack development in Micro services architecture\\nExperience with HTML5 and Java-script technologies (ExtJs/Angular, React)\\nStrong focus on design and application architecture with hands on development.\\nSolid experience in real-time distributed messaging and multi-threaded trading environment.\\nStrong experience with Web services development and REST framework with XML/JSON as the exchange mechanism.\\nStrong experience with Object oriented design and programming skills.\\nC# / .NET experience\\nExperience with Big Data frameworks Apache Spark, Hadoop, Integration frameworks and messaging systems (ex. IBM MQ, JMS) or memory grids (ex. MongoDB, Redis).\\nExperience with Financial Services technology environment.\\nExperience with Repo Trading &amp; Margin Services functions.\\nAbility to pick up new technologies independently.\\nAbility to work under pressure in a fast paced environment.\\nExperience in DevOps.\\nDelivery focused with proven ability to work under aggressive deadlines.\\nStrong controls/ compliance culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>PromoteIQ delivers intelligent vendor marketing solutions built for the next generation of e-commerce. Our solutions help retailers implement, automate, and scale brand-funded marketing programs on e-commerce sites. PromoteIQ is a New York City-based technology company that works with the US's largest e-commerce retailers. Learn more about us at https://www.promoteiq.com.\\n\\nWho we're looking for\\n\\nAt PromoteIQ, data plays an integral role in our product, and software engineers on our data engineering team build the pipelines that power reporting and analytics for our e-commerce promotions platform. The infrastructure and applications that you'll build on the data engineering team will have broad and critical reach in powering real-time auction decisions, becoming multipliers on our revenues, and forecasting supply and demand for our customers.\\n\\nResponsibilities\\n\\n\\nShip high-quality, well-tested, secure, and maintainable code\\nDesign, develop, and maintain data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions\\nManage automated unit and integration test suites\\nWork collaboratively and communicate effectively with a small, motivated team of engineers and product managers\\nExperiment with and recommend new technologies that simplify or improve PromoteIQ's stack\\nParticipate in an on-call rotation and work occasional off-hours\\n\\nQualifications\\n\\n\\nBS/MS in Computer Science or a related technical field\\nSeeking candidates with 4+ years of experience in:\\nArchitecting, building, and maintaining end-to-end, high-throughput data systems and their supporting services\\nDesigning data systems that are secure, testable, and modular, particularly in Python, as well as their support infrastructure (shell scripts, job schedulers, message queues, etc.)\\nDesigning efficient data structures and database schemas\\nWorking with distributed systems architecture\\nIncorporating data processing and workflow management tools into pipeline design (AWS EMR, Airflow, Kafka, etc.)\\nUsing profiling tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements\\nDeveloping for continuous integration and automated deployments\\nUtilizing a variety of data stores, including data warehouses (ideally Redshift), RDBMSes (ideally MySQL), in-memory caches (ideally Aerospike and Redis), and searchable document DBs (ideally Elasticseach)\\nWrangling large-scale data sets\\n\\n--\\n\\nHow We Interview &amp; Hire\\nOur interview process begins with a quick phone call to assess fit with the role and company - and help you decide if PromoteIQ is the right place for you. Then we do several rounds of in-person interviews, where you'll meet the team and participate in a number of interactive interviews. Throughout this process you'll have ample opportunity to ask questions, get to know us as a company, and learn more about our product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Senior Sales Specialist - Azure Data &amp; AI</td>\n",
       "      <td>New York, NY 10022</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10022</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>You will work with partners and others at Microsoft, as well as use our core tools, targeted account lists to identify and engage prioritized customers. You will be required to be disciplined in business-management, adaptable to a culture of accountability and build a strong and active business network.You will be the key technical leader, trusted advisor and influencer in shaping customer decisions to buy and adopt Microsoft Data &amp; AI solutions. You will own winning the technical decision at customers for sales opportunities and usage scenarios, through tailoring your message, bringing ideas to customers, engaging with them to show our technology differentiation, and guiding them in decision making. You will lead presentations and solution demonstrations to explain and prove to our largest customers the capabilities of Microsoft's Data &amp; AI solutions, and how we can make their businesses more successful.You will be influencing the Microsoft Data &amp; AI go to market strategies by providing feedback to sales, marketing, and engineering on current and future product requirements and sales blockers. You will be recognized for sharing, learning and driving work that results in business impact for customers, partners and Microsoft. We encourage thought leadership and we encourage all our employees to continuously maintain and enhance their technical, sales, professional skills and competitive readiness. You will therefore be required to attain and maintain required certifications</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As an Azure Data &amp; AI Specialist you will be a senior solution sales leader within our enterprise sales organization working with our most important customers. You will lead a virtual team of technical, partner and consulting resources to advance the sales process and achieve SQL Server and Azure Data Services (including our Data Platform, AI and IoT services) revenue and consumption in your assigned accounts. You will be a trusted advisor and a Data &amp; AI subject matter expert. You will help customers evaluate their applications, recommend solutions that meet their requirements and demonstrate these solutions to win the technical decision. You will need to support customers to remove roadblocks to deployment and drive customer satisfaction. You will help our customers take advantage of our unique hybrid data platform to realize the value of digital transformation.\\n\\nResponsibilities\\nPrimary accountabilities for this role include:You will work with partners and others at Microsoft, as well as use our core tools, targeted account lists to identify and engage prioritized customers. You will be required to be disciplined in business-management, adaptable to a culture of accountability and build a strong and active business network.You will be the key technical leader, trusted advisor and influencer in shaping customer decisions to buy and adopt Microsoft Data &amp; AI solutions. You will own winning the technical decision at customers for sales opportunities and usage scenarios, through tailoring your message, bringing ideas to customers, engaging with them to show our technology differentiation, and guiding them in decision making. You will lead presentations and solution demonstrations to explain and prove to our largest customers the capabilities of Microsoft's Data &amp; AI solutions, and how we can make their businesses more successful.You will be influencing the Microsoft Data &amp; AI go to market strategies by providing feedback to sales, marketing, and engineering on current and future product requirements and sales blockers. You will be recognized for sharing, learning and driving work that results in business impact for customers, partners and Microsoft. We encourage thought leadership and we encourage all our employees to continuously maintain and enhance their technical, sales, professional skills and competitive readiness. You will therefore be required to attain and maintain required certifications\\nQualifications\\nProfessional\\nExperienced. 8+ years’ experience selling business solutions to large/global enterprise customers with a focus on data platform technologies preferred\\nAccount Management. Effective account management: planning, opportunity qualification and creation, stakeholder and executive communication, needs analysis, value engineering, services/partner engagement, opportunity management, pipeline management, large dollar licensing and deal negotiation required\\nExecutive Presence. Experience and expertise selling to senior business decision makers by aligning &amp; reinforcing the value of the solution to the customer’s overall business pain and/or strategic opportunities and decision criteria.\\nProblem Solver. Ability to solve customer problems through cloud technologies required\\nCollaborative. Orchestrate and influence virtual teams to pursue sales opportunities and lead v-teams through influence\\n\\nTechnical\\nAzure Platform. Understanding of Microsoft server products and/or complementing solutions. The position requires the ability to articulate and demonstrate the business value of Microsoft's solutions and have a firm understanding of Microsoft's strategies and products relative to major Microsoft competitors required\\nLeadership. Experience leading large cloud deals especially those involving Data Platform modernization and migration, AI and related required\\nCompetitive Landscape. Knowledge of enterprise software solutions and platform competitor landscape\\nPartners. Understanding of partner ecosystems and the ability to leverage partner solutions to solve customer needs.\\nCertifications. Azure Data Engineer or Azure AI Engineer preferred\\n\\nEducation\\nBachelor’s Degree required, MBA preferred, or equivalent experience.\\n\\nMicrosoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.\\n\\nBenefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Analytics Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n2+ years experience working with data warehouses, data visualization and data analysis\\nExperience implementing best-practices for data modelling, especially with regards to dimensional modelling for business intelligence. Experience with DBT is a plus.\\nExperience with SQL-first, self-service visualization tools such as Mode Analytics, Tableau, Looker\\nHigh level of comfort with SQL\\nProficiency in Python and its scientific libraries (pandas, scipy etc.)\\nExperience with git and git-based workflows\\nYou understand that analytics and business intelligence should mimic, but not necessarily mirror the software development lifecycle\\nYou’re at least as comfortable with ambiguity as you are with a clearly defined problem\\nClear written and verbal communication skills, for both technical and non-technical audiences.\\nEmbodiment of our core company values of motivation, positivity, curiosity, humility and integrity, and buy-in for our company mission of eliminating bias in hiring.\\n</td>\n",
       "      <td>Using neuroscience-based assessments and machine learning algorithms, pymetrics is reinventing the recruiting industry by matching candidates to jobs and companies where they are most likely to succeed. We are leading the charge in an evolving industry, and growing our amazing team to support the mission of using data to unleash one's full potential and remove bias from hiring.\\nAre you a hardworking analytics or data engineer who has a curiosity for solving problems and a propensity for exploring data? Do you love understanding people, behavior and believe in making the hiring-process more equitable for everyone?\\nAs our next analytics engineer, you will work within our fast-paced Data Insights &amp; Analytics team to develop and deliver incredible visualization products aimed at making the output of our predictive models both intuitive and actionable.\\nIf this speaks to you and you want to help make hiring more predictive and less biased, we hope you apply!\\nWhat you’ll do includes:\\nBuild lasting, scalable solutions to surface data for our suite of external-facing visualization products\\nOwn the analytics layer of our data ecosystem\\nSupport efforts driving machine learning interpretability\\nChampion automation\\nInform product direction using your understanding of the capabilities of our rich datasets\\nExplore novel use cases for our current tools and future vendors to support both internal and external users\\nGuide adoption of new vendors, tools and technologies to enrich our analytics stack\\nHelp internal teams define the right metrics to answer the questions at hand\\nEnsure the right teams and the right people have access to the right data\\n\\nRequirements\\n\\n2+ years experience working with data warehouses, data visualization and data analysis\\nExperience implementing best-practices for data modelling, especially with regards to dimensional modelling for business intelligence. Experience with DBT is a plus.\\nExperience with SQL-first, self-service visualization tools such as Mode Analytics, Tableau, Looker\\nHigh level of comfort with SQL\\nProficiency in Python and its scientific libraries (pandas, scipy etc.)\\nExperience with git and git-based workflows\\nYou understand that analytics and business intelligence should mimic, but not necessarily mirror the software development lifecycle\\nYou’re at least as comfortable with ambiguity as you are with a clearly defined problem\\nClear written and verbal communication skills, for both technical and non-technical audiences.\\nEmbodiment of our core company values of motivation, positivity, curiosity, humility and integrity, and buy-in for our company mission of eliminating bias in hiring.\\nBenefits\\n\\nHealth care plan (medical, dental &amp; vision)\\nFlexible paid time off\\nFamily leave (maternity, paternity)\\n401k\\nTeam budget for training &amp; development\\nStock option plan\\nCommuter transportation reimbursement\\nDog-friendly workplace\\nFun, diverse and intellectually eager coworkers\\nFinally, we are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Help us Build the Future of Money\\nGemini Trust Company, LLC (Gemini) is a licensed digital asset exchange and custodian. We built the Gemini platform so customers can buy, sell, and store digital assets (e.g., Bitcoin, Ethereum, and Zcash) in a regulated, secure, and compliant manner.\\n\\n\\nDigital assets and blockchain technology have the power to transform the world for good. This truth, along with our core values, form the bedrock of our company and culture. At Gemini, no job is too small and no project too big as we endeavor to build the future of money. We are a mission-driven, team-based, inclusive, and determined community of thought leaders who invest in each other and the long game. Join us in our mission!\\n\\nTHE DEPARTMENT: DATA ENGINEERING\\n\\nTHE ROLE: DATA ENGINEER\\n\\nAs a member of our data engineering team, you’ll shape the way we approach data at Gemini by using your engineering, analytical and communication skills to work with teams across the business. You know how to ask the right questions and are passionate about using data to support and drive informed business decisions. You are ready to roll up your sleeves and are excited to take on challenging opportunities and projects. You’ll help to build data infrastructure at Gemini for cross-functional teams to leverage the data to improve operational efficiency, products and achieve KPIs. Communicating your insights with leaders across the organization is paramount to success.\\n\\nRESPONSIBILITIES:\\n\\nAssist in designing and implementation of best-in-class Data Infrastructure and Business Intelligence solutions\\nResearch new tools and technologies to build new and improve existing processes\\nParticipate in design discussions and meetings\\nDesign, automate, build, and launch scalable, efficient and reliable data pipelines in production\\nBuild data integrations to integrate different data sources into the Data Lake\\nAssist in building real-time data streaming and reporting solutions\\nSchedule, monitor and maintain data pipelines using a workflow management tool\\nDesign, build and enhance dimensional models for Data Warehouse and Data Marts\\nCreate test plans, test scripts and build data reconciliation processes\\nTune SQL queries, reports and ETL/ELT processes\\nPartner with engineers, project managers, and analysts to deliver insights to the business\\nPerform root cause analysis and resolve production and data issues\\n\\n\\nMINIMUM QUALIFICATIONS:\\n\\n4+ years experience in data engineering and data warehouse technologies\\n4+ years experience in custom ETL design, implementation and maintenance\\n2+ years experience working with AWS cloud technologies\\n2+ experience working with Airflow or a similar workflow management tool\\nSkilled in programming languages Python and/or Java\\nExperience building real-time data streaming solutions using Kafka and/or other tools/technologies\\nExperience with schema design and dimensional data modeling\\nExperience with one or more MPP databases(Redshift, Bigquery, Snowflake, etc)\\nAdvanced SQL skills is a must\\nExperienced in working collaboratively across different teams and departments\\nStrong technical and business communication\\n\\n\\nPREFERRED QUALIFICATIONS:\\n\\nKafka, Spark, HDFS, Cloud computing experience is a plus\\nExperience building and integrating web analytics solutions\\nExperience with one or more ETL tools(dbt, Informatica, Pentaho, SSIS, Alooma, etc)\\nExperience with continuous integration and deployment\\nKnowledge and experience of financial markets, banking or exchanges\\nIt Pays to Work Here\\n\\nWe take a holistic approach to compensation at Gemini, which includes:\\n\\nCompetitive base salaries across all departments\\nOwnership in the company via profit sharing units\\nAmazing benefits, 401k match contribution, and flexible hours\\nSnacks, Perks, Wellness Outings &amp; Events\\n\\n\\nGemini is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you have a disability or special need that requires accommodation, please let us know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Big-Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description:\\nApplicant should be at least 8 years of experience in the relevant field (At least 5 years in Bigdata). Must be a hands-on Bigdata developer.\\nShould have experience in building ETL/ELT pipeline in data technologies like Hadoop, spark, hive, presto, data bricks.\\nShould have understanding of data warehousing concepts.\\nShould be able to troubleshoot API integration code.\\nShould be familiar with Github and other source control tools.\\nKnowledge of Amazon EC2 would be a plus.\\nKnowledge of Python, PYspark or any other programming language is a plus.\\nRed shift or snowflake knowledge is a plus\\nknowledge of any reporting tool is a plus.\\nApplicant’s responsibility would be to interface with Business intelligence team and help them build data pipeline to support existing BI platform and data products\\n\\nWhat's in it for you?\\n\\nExcellent benefits plan: medical, dental, vision, life, FSA, &amp; PTO\\nRoll over vacation days\\nCommuter benefits\\nExcellent growth and advancement opportunities\\nCertification reimbursement\\nRewards and recognition programs\\nInnovative and collaborative company culture\\n\\n“LTI values diversity and inclusion and is committed to the principles of Equal Employment Opportunity EOE/Minority/Female/Veteran/Disabled/Sexual Orientation/Gender Identity.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Marquee - Data Engineer</td>\n",
       "      <td>New York, NY 10282</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10282</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>MORE ABOUT THIS JOB\\nThe Marquee team at Goldman Sachs is responsible for delivering digital products to our institutional client base. We design and build scalable web platforms that provide access to Goldman Sachs content, portfolio analytics, risk, and execution services. These tools help to transform and simplify client experiences while generating new revenue streams and business models for a leader in global financial markets. Marquee is a product driven team, composed of talented and passionate product managers, designers, and engineers working to change the expectation of institutional finance.\\n\\nAs an engineer within Marquee, you will be working in a close knit team at the forefront of shaping our client's experience using leading open source technologies. Teams develop in an agile environment that is flexible to client request which have need for us to prototype and create high quality tested code.\\nRESPONSIBILITIES AND QUALIFICATIONS\\nRESPONSIBILITIES\\nDesign and lead development of a scalable data warehouse containing measurements of client interactions with the Marquee digital platforms via web, mobile and API.\\nDevelop a real-time metrics ingestion pipeline using Kafka.\\nDevelop tools that allow product managers and business sponsors to gain the data insights they need to drive product decisions and strategic direction. This may include either static or dynamic reporting.\\n\\nWHO WE LOOK FOR\\nData engineers with the following qualities:\\ncommitment to understanding the metrics that drive our business\\npassion for clean, consistent data models\\nhealthy paranoia about data integrity and quality\\n\\nBASIC QUALIFICATIONS &amp; PREFERRED QUALIFICATIONS\\n3+ years working with large data warehouses\\nDemonstrated ability to deliver using some of the following: Python, SQL, Java, Kafka\\nExperience with digital client engagement analytics\\nExperience integrating data with tools like Qlikview, Tableau, Jupyter Notebooks\\nAWS experience preferred\\n\\nWHY MARQUEE\\nWork on some of the most complex technical and design challenges in technology and finance\\nLearn from the foremost experts in finance, technology, and math who are diverse in their academic, ethnic, and social backgrounds\\nBenefit from ongoing training, development, and mentoring to advance in your career\\nABOUT GOLDMAN SACHS\\nThe Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.\\n\\nÂ© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Strategy Specialist - Business &amp; Data Analysis, Cloud, AWS, Azure, Big Data</td>\n",
       "      <td>New York, NY 10011</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10011</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe North America Data Strategy &amp; Architecture capability is part of the Data Business Group (DBG) within Accenture Technology. This team provides advisory services to clients that create an architecture blueprint and an execution roadmap to rotate to “Data in the New” and become intelligent data driven enterprises.\\n\\n Connect business vision and current state problems with data, analytics and technology solutions and architectural patterns Interview business stakeholders to understand their vision and challenges Understand and document current state pain points including limitations caused by existing data, analytics and technology gaps Identify and detail business ‘use cases’, or ways that stakeholders would like to drive business value (e.g. increase revenue, decrease expenses, increase efficiency) through data and analytics Aggregate use cases into business consumption patterns detailing the data and technology designs that would support the execution of multiple use cases Ensure alignment between the client’s business needs of the future state with data and technology architecture, operating model and governance recommendations Synthesize business needs with enabling target state recommendations into a vision that client executives, department heads, business and technical resources can understand and align around Develop an execution roadmap detailing a strategic journey from current state to realization of the future state vision with incremental release of technical and operational features and business value Analyze business case for execution against the strategy, including the collection of business case inputs (costs, value drivers) as well as the calculation of return on investment Present data strategy to clients and gain buy in Participate in defining data governance strategy and operating model\\n\\nRequired Skills 3+ years with business responsibilities (Business Analyst or Project Lead) of data and analytics solutions spanning:\\no Data Management solutions with capabilities, such as Data Ingestion, Data Curation, Metadata and Catalog, Data Security, Data Modeling, Data Wrangling\\no Data Warehousing / BI / Reporting solutions that generate business value using platforms and technologies such as Hadoop, Teradata, Netezza, Greenplum, MapReduce, Spark, etc.\\no Data Science, AI / ML, Advanced Analytic solutions that meet business problems 3+ years of consulting experience, interviewing business stakeholders and developing relationships within client organizations Strong communication, presentation, written and facilitation skills Superior critical thinking, analytical and problem-solving skills Ability to interface with client at any level, executive to engineer Competent in leveraging Microsoft Office tools, specifically PowerPoint, Word, and Excel\\n Able to travel up to 100% (Mon-Thu)\\n\\nOptional Skills (Plus): Industry knowledge in Life Sciences, Financial Services or Healthcare Experience in data governance and operating model\\n Experience in compiling business cases and roadmaps for data, analytics and technology investments\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Engineer, Analytics</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>OVERVIEW:\\nGroupM Data &amp; Analytics Services (DAS) is a full-service analytics, data, business, and marketing consultancy. We maximize our clients’ and agencies’ marketing ROI by providing leading edge data management tools, advanced analytics, and consulting to monitor, evaluate, and optimize media investments.\\n\\nThe Data and Analytics organization in GroupM is responsible for managing and analyzing massive amounts of data as well as supporting the whole organization in delivering value from data. We focus on supporting operating our global technology stack with four key layers of Data Marketplace, Data Management, Analytics Workbench and Application Marketplace. These products are hosted on a variety of infrastructure options including but not limited to local servers, public and cloud-based solutions that enable a suit of data-driven media services.\\n YOUR IMPACT:\\nSupport development and deployment of GroupM products and services across multiple cloud environments\\nCollaborate with a cross-functional team of client leads, application developers, operations engineers and architects to translate complex product requirements into technical specs and design requirements\\nOptimize performance and cost efficiency of cloud based processes across multiple cloud environments (AWS, Azure, GCP)\\nMaintain a high degree of knowledge in cloud data architecture and ETL best practices especially across Google Cloud Platform products and services\\nAct as a consultant and subject matter expert for internal stakeholders in GroupM Data &amp; Analytics, GroupM Engineering and agency data science and tech leads\\nDevelop and deploy automated scripts in BigQuery and other Google cloud services to be used by other teams to increase productivity\\nFacilitate architectural discussions and initiatives to ensure cloud-based products are optimally deployed with maximum availability of design features\\nFormulate and execute robust UAT protocols to identify and address latent errors in product functions\\nDesign, build and deploy ETL and data management processes with reliable error/exception handling and rollback framework\\nProvide production support for data load jobs and develop customized query to generate automatic periodic reports\\nBuild applications writing SQL scripts to manipulate data and / or writing specific instructions for an off-shore programmer to write the scripts\\n YOUR QUALIFICATIONS:\\nEssential:\\nBachelor’s degree in Computer Science, Engineering, Mathematics or other technical field is highly preferred\\n5+ years of overall experience with Data Engineering\\nMinimum 2 years of hands-on experience with Google Cloud Platform\\nGood communication and written skills\\nStrong expertise with Data Architecture fundamentals, database design and programming, ETL and custom query development\\nExperience of the following GCP Services: Cloud Storage, DataProc, Dataflow, CloudSQL, BigQuery\\nExperience with building data pipes landing large files into GCP for processing, developing/ cleansing data for AI/ML purposes\\nExperience using the Linux Command Line, especially in conjunction with GCP\\nUse of Data Profiling Tools, ETL and Data Management Tools\\nData warehousing/data modeling experience, with strong understanding of semantic and physical data models\\nKnowledge of Agile methodology\\nExperience with the full development life cycle of an application stack - from architecture through test and deployment.\\nAble to deliver a broad range of data engagements in areas such as Data Architecture, Data Integration, Data Analytics, Data Lineage, Data Governance, Data Quality and BI Reporting\\nUnderstanding of data analysis techniques and how they can be applied in the marketing context beneficial\\nExperience with source code management systems\\nDesired:\\nPrior knowledge of advertising ecosystem, understanding of marketing metrics, and analytical products offered as a service is preferred\\nABOUT GROUPM:\\nGroupM is the leading global media investment management operation serving as the parent company to WPP media agencies including Mindshare, MEC, MediaCom, and Maxus, each global operations in their own right with leading market positions. GroupM’s primary purpose is to maximize performance of WPP’s media agencies by operating as leader and collaborator in trading, content creation, sports, digital, finance, proprietary tool development and other business-critical capabilities. GroupM’s focus is to deliver unrivaled marketplace advantage to its clients, stakeholders and people. Discover more about GroupM at www.groupm.com.\\n\\nTake A Virtual Office Tour: https://roundme.com/tour/368027\\nGroupM and all its affiliates embrace and celebrate diversity, inclusivity, and equal opportunity. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. We are a worldwide media agency network that represents global clients. The more inclusive we are, the more great work we can create together.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Senior Engineer, Data Engineering</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s degree in Computer Science or related field\\n3+ years of software development experience, as a developer\\nFluency in Scala and/or Java programming languages\\nStrong OO &amp; FP design patterns, data structure, and algorithm design skills\\nExtensive experience developing Apache Spark applications\\n2+ years of experience with both relational database design (SQL), non-relational (NoSQL) databases, big data, real-time technologies\\nFamiliar with various cloud data sources and architectures such as AWS/S3, HDFS, Kafka\\nExperience with software containerization, such as Docker\\nExperience developing and/or consuming web interfaces (REST API) and associated skills (HTTP, web services)\\nSelf-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nServe as a senior data engineer for AdSmart products.\\nParticipate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and leadership\\nDevelop and code the software components that are core to Audience Studio, under the leadership of the VP/Chief Architecture\\nSupport product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.\\nAnalyze and report results and adjust the overall engineering strategy accordingly with engineering leadership</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n Interested candidate must submit a resume/CV through www.nbcunicareers.com to be considered\\nMust be willing to work in New York, NY</td>\n",
       "      <td>As a member of the Data Engineering Team, the Senior, Data Engineer, AdSmart will be directly responsible for the design, management, and development of parts of the software platform and products for NBCUniversal’s AdSmart. NBCUniversal’s AdSmart products will enable NBCUniversal to better understand it's brand’s audiences such as NBC News, Bravo, The Tonight Show, Saturday Night Live, and USA Network as well as audiences that cross brands. The goal is to ensure we know who is watching what, where and when. In turn, enabling NBCUniversal’s sales teams to properly align our audiences with the market advertisements that can benefit them the most.\\n\\nYou’re a big thinker who can analyze and evangelize a long-range opportunity, architect a groundbreaking solution, and roll-up your sleeves to get code out the door when needed. You are data-driven and analytical. You understand the concept of a value proposition and evaluation criteria, and you know how to align them with low-level milestones to get the work done. You can apply domain knowledge from one technical subject, in order to quickly ramp and deliver on a new one. You know how to learn from failure until you succeed, and you are able to articulate and quantify the reasons for your decisions.\\n\\nYou will be part of the AdSmart Data Engineering team, participating in the data architecture that will drive both current and future data management initiatives within NBCUniversal’s AdSmart group.\\n\\nResponsibilities:\\nServe as a senior data engineer for AdSmart products.\\nParticipate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and leadership\\nDevelop and code the software components that are core to Audience Studio, under the leadership of the VP/Chief Architecture\\nSupport product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.\\nAnalyze and report results and adjust the overall engineering strategy accordingly with engineering leadership\\n\\nQualifications:\\nBachelor’s degree in Computer Science or related field\\n3+ years of software development experience, as a developer\\nFluency in Scala and/or Java programming languages\\nStrong OO &amp; FP design patterns, data structure, and algorithm design skills\\nExtensive experience developing Apache Spark applications\\n2+ years of experience with both relational database design (SQL), non-relational (NoSQL) databases, big data, real-time technologies\\nFamiliar with various cloud data sources and architectures such as AWS/S3, HDFS, Kafka\\nExperience with software containerization, such as Docker\\nExperience developing and/or consuming web interfaces (REST API) and associated skills (HTTP, web services)\\nSelf-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment\\n\\nAdditional Job Requirements:\\n Interested candidate must submit a resume/CV through www.nbcunicareers.com to be considered\\nMust be willing to work in New York, NY\\n\\nDesired Requirements:\\nExperience as a development manager (with direct authority over development staff)\\nExperience with Cluster Management and Container Orchestration technologies such as Mesos, Kubernetes, Hadoop/Yarn\\nExperience with Apache Kafka or similar streaming technologies\\nExperience with digital advertising technologies.\\nAble and eager to learn new technologies\\nAble to easily transition between high-level strategy and day-to-day implementation\\nExcellent teamwork and collaboration skills\\nResults-oriented, high energy, self-motivated\\nSub-BusinessTechnology\\nCareer Level\\nExperienced\\nCityNew York\\nState/Province\\nNew York\\nCountryUnited States\\nAbout Us\\nAt NBCUniversal, we believe in the talent of our people. It’s our passion and commitment to excellence that drives NBCU’s vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It’s what makes us uniquely NBCU. Here you can create the extraordinary. Join us.\\nNotices\\nNBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Engineer - Data</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMaster's or bachelor's degree in Computer Science\\nAt least 5 years of hands-on software development experience in Python, Golang, Java, C++ or Scala\\nStrong Object Oriented Programming skills\\nDeep knowledge in data structures, algorithms, and software design\\nExperience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources\\nHighly proficient with relational and non-relational data storages\\nStrong verbal and written communication skills</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Work with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs\\nAdheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time\\nCollaborates with code reviews, architecture discussion and bug fixing\\nDemonstrates code and product ownership in production\\nSupport the business teams and product managers in data extracts and data analysis\\nPerforms as a true agile team member and exhibits competencies in all layers of the application stack. Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Lucid is a market research platform that provides access to authentic, first-party data in over 90 countries. Our products and services enable anyone, in any industry, to ask questions of targeted audiences and find the answers they need – fast. These answers can be used to uncover consumer motivations, increase revenue, and measure the impact of digital advertising. Founded in 2010, Lucid is headquartered in New Orleans, LA with offices in Dallas, New York, London, Sydney, Singapore, Gurgaon, Prague, and Hamburg.\\n\\n\\nApply for this Job\\n\\nThe Opportunity\\n\\nThe Data Engineer will be part of our development team and work closely with development team peers, Product Management team, and business and other support teams.\\nResponsibilities\\nWork with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs\\nAdheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time\\nCollaborates with code reviews, architecture discussion and bug fixing\\nDemonstrates code and product ownership in production\\nSupport the business teams and product managers in data extracts and data analysis\\nPerforms as a true agile team member and exhibits competencies in all layers of the application stack. Demonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration\\nQualifications\\nMaster's or bachelor's degree in Computer Science\\nAt least 5 years of hands-on software development experience in Python, Golang, Java, C++ or Scala\\nStrong Object Oriented Programming skills\\nDeep knowledge in data structures, algorithms, and software design\\nExperience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources\\nHighly proficient with relational and non-relational data storages\\nStrong verbal and written communication skills\\nPreferred Qualifications\\nAWS experience with S3, RDS, Redshift, EMR, Kinesis, Lambda, Elastic Beanstalk and Elasticsearch\\nExperience with Spark, Hadoop and Hive on EMR and non-managed. Can build and run a basic cluster\\nExperience developing ETLs and running job schedulers (e.g. Airflow)\\nExperience with PostgreSQL\\nExperience with NoSQL data sources including Cassandra\\nExperience with Kafka, Redis\\nExperience creating Data Lakes\\n\\n\\nAt Lucid we foster a collaborative and inspiring workplace. We pride ourselves in doing this by recruiting, hiring and retaining diverse, passionate, and forward-thinking talent. Lucid is committed to and encourages an inclusive environment and we are dedicated to providing equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please let us know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AI Data Scientist / Data Engineer - Experienced Associate</td>\n",
       "      <td>New York, NY 10017</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10017</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExploring new analytical technologies and evaluating their technical and commercial viability quickly;\\nWorking in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;\\nTesting and rejecting hypotheses around data processing and machine learning model building;\\nDemonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;\\nBuilding machine learning pipelines that ingest, clean data, and make predictions;\\nStaying abreast of new AI research from leading labs by reading papers and experimenting with code;\\nDeveloping innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;\\nDemonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;\\nApplying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);\\nUnderstanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;\\nBuilding machine learning models and systems, interpreting their output, and communicating the results;\\nMoving models from development to production is a plus; and,\\nConducting research in a lab and publishing work is a plus.</td>\n",
       "      <td>\\nExploring new analytical technologies and evaluating their technical and commercial viability quickly;\\nWorking in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;\\nTesting and rejecting hypotheses around data processing and machine learning model building;\\nDemonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;\\nBuilding machine learning pipelines that ingest, clean data, and make predictions;\\nStaying abreast of new AI research from leading labs by reading papers and experimenting with code;\\nDeveloping innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;\\nDemonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;\\nApplying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);\\nUnderstanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;\\nBuilding machine learning models and systems, interpreting their output, and communicating the results;\\nMoving models from development to production is a plus; and,\\nConducting research in a lab and publishing work is a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExploring new analytical technologies and evaluating their technical and commercial viability quickly;\\nWorking in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;\\nTesting and rejecting hypotheses around data processing and machine learning model building;\\nDemonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;\\nBuilding machine learning pipelines that ingest, clean data, and make predictions;\\nStaying abreast of new AI research from leading labs by reading papers and experimenting with code;\\nDeveloping innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;\\nDemonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;\\nApplying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);\\nUnderstanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;\\nBuilding machine learning models and systems, interpreting their output, and communicating the results;\\nMoving models from development to production is a plus; and,\\nConducting research in a lab and publishing work is a plus.</td>\n",
       "      <td>PwC Labs is focused on standardizing, automating, delivering tools and processes and exploring emerging technologies that drive efficiency and enable our people to reimagine the possible. Process improvement, transformation, effective use of innovative technology and data &amp; analytics, and leveraging alternative delivery solutions are key areas of focus to drive additional value for our firm. The AI Lab focuses on implementing solutions that impact efficiency and effectiveness of our technology functions. Process improvement, transformation, effective use of technology and data &amp; analytics, and leveraging alternative delivery are key areas to drive value and continue to be recognized as the leading professional services firm. AI Lab is focused on identifying and prioritizing emerging technologies to get the most out of our investments.\\n\\nTo really stand out and make us ?t for the future in a constantly changing world, each and every one of us at PwC needs to be an authentic and inclusive leader, at all grades/levels and in all lines of service. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.\\n\\nAs an Associate, you’ll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:\\n\\nInvite and provide evidence-based feedback in a timely and constructive manner.Share and collaborate effectively with others.Work with existing processes/systems whilst making constructive suggestions for improvements.Validate data and analysis for accuracy and relevance.Follow risk management and compliance procedures.Keep up-to-date with technical developments for business area.\\n- Communicate confidently in a clear, concise and articulate manner - verbally and in written form.\\nSeek opportunities to learn about other cultures and other parts of the business across the Network of PwC firms.Uphold the firm’s code of ethics and business conduct.\\n\\nOur team is capability centric, focusing on AI and machine learning techniques that are broadly applicable across all industries. We work with a variety of data mediums including text, audio, imagery, sensory, and structured data. Our work involves the use of supervised/unsupervised machine learning algorithms, traditional statistical models, deep neural networks, terabyte scale data, and simulation modelling. Our work is having a tremendous impact on how PwC &amp; our clients do business.\\nJob Requirements and Preferences:\\n\\nBasic Qualifications:\\n\\nMinimum Degree Required:\\nBachelor Degree\\n\\nMinimum Years of Experience:\\n1 year(s)\\n\\nPreferred Qualifications:\\n\\nPreferred Fields of Study:\\nComputer and Information Science, Computer Engineering, Computer and Information Science &amp; Accounting, Economics, Economics and Finance, Economics and Finance &amp; Technology, Engineering, Mathematics, Mathematical Statistics, Statistics\\n\\nPreferred Knowledge/Skills:\\nDemonstrates some knowledge and/or a proven record of success in the following areas:\\nExploring new analytical technologies and evaluating their technical and commercial viability quickly;\\nWorking in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;\\nTesting and rejecting hypotheses around data processing and machine learning model building;\\nDemonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;\\nBuilding machine learning pipelines that ingest, clean data, and make predictions;\\nStaying abreast of new AI research from leading labs by reading papers and experimenting with code;\\nDeveloping innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;\\nDemonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;\\nApplying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);\\nUnderstanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;\\nBuilding machine learning models and systems, interpreting their output, and communicating the results;\\nMoving models from development to production is a plus; and,\\nConducting research in a lab and publishing work is a plus.\\nDemonstrates some abilities and/or a proven record of success learning and applying new skills quickly, including the following areas and technologies:\\nProgramming: Python, R, Java, JavaScript, C++, Unix;\\nHardware: sensors, robotics, GPU enabled machine learning, FPGAs, Raspberry Pis, etc.;\\nData Storage Technologies: SQL, NoSQL, Hadoop, cloud-based databases such as GCP BigQuery, and different storage formats (e.g. Parquet, etc.);\\nData Processing Tools: Python (Numpy, Pandas, etc.), Spark, cloud-based solutions such as GCP DataFlow;\\nMachine Learning Libraries: Python (scikit-learn, genism, etc.), TensorFlow, Keras, PyTorch, Spark MLlib;\\nVisualization: Python (Matplotlib, Seaborn, bokeh, etc.), JavaScript (d3); and,\\nProductionization and containerization technologies: GitHub, Flask, Docker, Kubernetes.\\nAll qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>New York, NY 10011</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10011</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nCommercial experience leading on client-facing projects, including working in close-knit teams\\n5+ years of experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)\\n5+ years of experience working on projects within the cloud ideally AWS or Azure\\n5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent\\nExperience with open source tools like Apache Airflow and Griffin\\nExperience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform\\nData Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift\\nExperience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models\\nExperience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma\\nExperience building automated data quality and testing into data pipelines\\nExperience with AI, NLP, Machine Learning, etc. is a plus\\nStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R\\nExperience working on lively projects and a consulting setting, often working on different and multiple projects at the same time\\nExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.\\nA deep personal motivation to always produce outstanding work for your clients and colleagues\\nExcel in team collaboration and working with others from diverse skill-sets and backgrounds</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary:\\n\\nYou have experience with client projects and in handling vast amounts of data – working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Data Science team. Lead on projects from a data engineering perspective, working with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches\\nPlan and execute secure, good practice data integration strategies and approaches\\nAcquire, ingest, and process data from multiple sources and systems into Big Data platforms\\nCreate and manage data environments in the Cloud\\nCollaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models\\nHave a strong understanding of Information Security principles to ensure compliant handling and management of client data\\nThis is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science\\nQualifications:\\nCommercial experience leading on client-facing projects, including working in close-knit teams\\n5+ years of experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)\\n5+ years of experience working on projects within the cloud ideally AWS or Azure\\n5+ years of experience working with streaming architectures and patterns like Kafka, Kinesis, Flink, or Confluent\\nExperience with open source tools like Apache Airflow and Griffin\\nExperience with DevOps and DataOps patterns and tools like Jenkins, Kubernetes, Docker, and Terraform\\nData Warehousing experience with cloud products like Snowflake, Azure DW, or Redshift\\nExperience building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models\\nExperience with one or more ETL/ELT tools like Talend, Matillion, FiveTran, or Alooma\\nExperience building automated data quality and testing into data pipelines\\nExperience with AI, NLP, Machine Learning, etc. is a plus\\nStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R\\nExperience working on lively projects and a consulting setting, often working on different and multiple projects at the same time\\nExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.\\nA deep personal motivation to always produce outstanding work for your clients and colleagues\\nExcel in team collaboration and working with others from diverse skill-sets and backgrounds\\nCervello is a dynamic technology company that is focused on business analytics and planning. We take an innovative approach to making complex solutions simple so our clients can focus on running their businesses. Our services and applications enable our clients to gain the benefits of a world-class analytics and planning capability without the headaches.\\n\\nM4ahUGavPU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>• Hands-on experience in Azure Data Factory, Informatica and good understanding on data obfuscation or data masking techniques • Design, construct, install, test and maintain highly scalable data management systems • Build automated data delivery pipelines and services to integrate data • Build and deliver cloud-based deployment and monitoring capabilities consistent with DevOps models • Develop solutions in agile environment for the overall data domain • Deep experience with developing SQL • Must have Deep experience developing with MS SQL • Must be able to do ETL (SSIS, Azure Data Factory, Informatica) • Understand Data Security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY 10018</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10018</td>\n",
       "      <td>None Found</td>\n",
       "      <td>THE PERSON\\nHands on research and development\\nFind innovative solutions to difficult and unstructured problems that will support and expand our core business\\nUse analytical rigor to analyze large amounts of data, help extract actionable insights using data analysis, feature engineering, optimization tools and machine learning techniques\\nDevelop and maintain scalable and reliable data pipelines for AI/ML processes\\nParticipate in all aspects of Teach For America’s agile software development cycle\\nOther duties (research, presentations, communicating with other teams)\\n\\nTHE MUST HAVES\\nPrior Experience:\\nIntermediate or higher experience working with machine learning tools in Python (TensorFlow, Keras, fast.ai, etc)\\nExperience working with data solutions (data ingestion, preprocessing, analysis, predictive analytics)\\nExperience with Reporting and Advanced Analytics Solutions\\nIntermediate or higher experience working with Java (Spring, REST, JMS) and SQL\\nComfortable working in a Unix environment\\nExperience with version control, containerization technologies\\nExperience working with Continuous Integration / Automation architectures\\nExperience interfacing and working with engineering teams throughout the product development lifecycle (leading projects and/or other developers)\\nExperience partnering with other teams to test and roll out cognitive &amp; predictive analytics solutions\\nOptional but desired:\\nExperience developing applications for and deploying applications in cloud environments\\nExperience with big data and associated technologies (Hadoop, Databricks, Azure Machine Learning etc.)\\nExperience with statistical modeling in R\\nSkills:\\nBe able to communicate in a clear and effective manner with both technical and non-technical audiences\\nBe interested in staying up-to-date with recent advances in machine learning and predictive analytics\\nBe detail oriented, able to work under pressure and effectively manage competing priorities\\nEducation:\\nAt least a four-year degree in Computer Science, BSEE, MIS or a related field, or equivalent experience\\nWork Demands:\\nThis position is located on site in our New York National Office\\nLimited travel may be required\\n\\nTHE TEAM\\nOur team loves to collaborate. We partner with every other team in the organization to create world-class technology solutions that staff and corps members use to more effectively and efficiently get all kids access to educational opportunity. Our team works very hard, but we also have a lot of fun. We enjoy game nights, quarterly trivia outings, and themed potlucks where we get together to eat and explore each other's cultures and favorite recipes.\\n\\nTHE PERKS\\nBy joining staff, you join a network of individuals committed to pursuing equity for all students and developing themselves as professionals in the process. We as an organization value the longevity of our employees and offer a comprehensive and competitive benefits plan. The salary for this position is also competitive and depends on your prior work experience. Please be advised, you will have an opportunity to discuss salary in more detail after you begin the application process.\\n\\nWE ARE DEEPLY COMMITTED TO DIVERSITY, EQUITY &amp; INCLUSIVENESS\\nTeach For America encourages individuals of all ethnic, racial, and socioeconomic backgrounds to apply for this position. We are committed to maximizing the diversity of our organization, as we want to engage all those who can contribute to this effort.\\n\\nTeach For America is committed to providing equal employment opportunities to all qualified individuals and does not discriminate on the basis of race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, parental status, genetic information or characteristics (or those of a family member) or any other basis prohibited by applicable law.\\n\\nThis job description reflects Teach For America's assignment of essential functions and qualifications of the role. Nothing in this herein restricts management's right to assign, reassign or eliminate duties and responsibilities to this role at any time.\\n\\nNEXT STEPS\\nInterested in this position? Apply now! Scroll down to the bottom of the page to find the link to the online application. If you still have questions regarding the role, feel free to contact our recruitment team at staffing@teachforamerica.org or visit www.teachforamerica.org/about-us/careers.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>TEAM: Information Technology\\nREPORTS TO: MD, AI &amp; Cognitive Services\\nLOCATION: New York, NY\\n\\nTHE ROLE\\nAre you passionate about Artificial Intelligence/Machine Learning, have coding experience and are looking for more progression &amp; autonomy in your career? We are seeking an AI/ML Engineer to collaborate with some of tech’s and analytics’ sharpest minds to solve the firm’s ever-changing but exciting challenges and explore unique technical solutions. Working in tech here at Teach for America means that you’ll always be presented with a variety of new possibilities as you continue to enhance your skills and contribute to our mission.\\n\\nTHE ORGANIZATION\\nThere are more than 16 million children growing up in poverty in the U.S., and less than 10 percent of them will graduate from college. These statistics are not a reflection of our children’s potential; we know that children growing up in poverty can and do achieve at the highest levels. Rather, these statistics reflect the systemic lack of access and opportunity for children in low-income communities.\\n\\nTeach For America’s (TFA) mission is to find, develop, and support a diverse network of leaders committed to expanding opportunity for children from classrooms, schools, and every sector and field that shapes the broader systems in which schools operate. We are seeking individuals who align with our mission, core values and commitment to Diversity Equity &amp; Inclusiveness and are ready to join us in this global movement.\\n\\n\\nQualifications:\\nTHE PERSON\\nHands on research and development\\nFind innovative solutions to difficult and unstructured problems that will support and expand our core business\\nUse analytical rigor to analyze large amounts of data, help extract actionable insights using data analysis, feature engineering, optimization tools and machine learning techniques\\nDevelop and maintain scalable and reliable data pipelines for AI/ML processes\\nParticipate in all aspects of Teach For America’s agile software development cycle\\nOther duties (research, presentations, communicating with other teams)\\n\\nTHE MUST HAVES\\nPrior Experience:\\nIntermediate or higher experience working with machine learning tools in Python (TensorFlow, Keras, fast.ai, etc)\\nExperience working with data solutions (data ingestion, preprocessing, analysis, predictive analytics)\\nExperience with Reporting and Advanced Analytics Solutions\\nIntermediate or higher experience working with Java (Spring, REST, JMS) and SQL\\nComfortable working in a Unix environment\\nExperience with version control, containerization technologies\\nExperience working with Continuous Integration / Automation architectures\\nExperience interfacing and working with engineering teams throughout the product development lifecycle (leading projects and/or other developers)\\nExperience partnering with other teams to test and roll out cognitive &amp; predictive analytics solutions\\nOptional but desired:\\nExperience developing applications for and deploying applications in cloud environments\\nExperience with big data and associated technologies (Hadoop, Databricks, Azure Machine Learning etc.)\\nExperience with statistical modeling in R\\nSkills:\\nBe able to communicate in a clear and effective manner with both technical and non-technical audiences\\nBe interested in staying up-to-date with recent advances in machine learning and predictive analytics\\nBe detail oriented, able to work under pressure and effectively manage competing priorities\\nEducation:\\nAt least a four-year degree in Computer Science, BSEE, MIS or a related field, or equivalent experience\\nWork Demands:\\nThis position is located on site in our New York National Office\\nLimited travel may be required\\n\\nTHE TEAM\\nOur team loves to collaborate. We partner with every other team in the organization to create world-class technology solutions that staff and corps members use to more effectively and efficiently get all kids access to educational opportunity. Our team works very hard, but we also have a lot of fun. We enjoy game nights, quarterly trivia outings, and themed potlucks where we get together to eat and explore each other's cultures and favorite recipes.\\n\\nTHE PERKS\\nBy joining staff, you join a network of individuals committed to pursuing equity for all students and developing themselves as professionals in the process. We as an organization value the longevity of our employees and offer a comprehensive and competitive benefits plan. The salary for this position is also competitive and depends on your prior work experience. Please be advised, you will have an opportunity to discuss salary in more detail after you begin the application process.\\n\\nWE ARE DEEPLY COMMITTED TO DIVERSITY, EQUITY &amp; INCLUSIVENESS\\nTeach For America encourages individuals of all ethnic, racial, and socioeconomic backgrounds to apply for this position. We are committed to maximizing the diversity of our organization, as we want to engage all those who can contribute to this effort.\\n\\nTeach For America is committed to providing equal employment opportunities to all qualified individuals and does not discriminate on the basis of race, color, ethnicity, religion, sex, gender, gender identity and expression, sexual orientation, national origin, disability, age, marital status, veteran status, pregnancy, parental status, genetic information or characteristics (or those of a family member) or any other basis prohibited by applicable law.\\n\\nThis job description reflects Teach For America's assignment of essential functions and qualifications of the role. Nothing in this herein restricts management's right to assign, reassign or eliminate duties and responsibilities to this role at any time.\\n\\nNEXT STEPS\\nInterested in this position? Apply now! Scroll down to the bottom of the page to find the link to the online application. If you still have questions regarding the role, feel free to contact our recruitment team at staffing@teachforamerica.org or visit www.teachforamerica.org/about-us/careers.\\n\\nShare||</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY 10036</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10036</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Short Description\\nAbout Capgemini\\n\\nA global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50-year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion (about $15.6 billion USD at 2018 average rate).\\nVisit us at www.capgemini.com. People matter, results count.\\n\\nTitle: Python Developer\\n\\nExpertise in converting SAS code to Python\\nMAIN Expertise in adding efficiency to models\\nKnowledge of writing a modular code\\nAdvanced Python R not necessary o\\nVery strong test driven programming skills o\\nStrong object oriented programming knowledge o\\nSoftware integration API design and integration\\nDB connection performance tuning exception\\nHandling Advanced Spark ETL PySpark API rather than Scala SQL Hive Presto AWS packages boto troposphere to be able to script automated full workflows pipelines\\n\\nCapgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.\\nThis is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.\\n\\nClick the following link for more information on your rights as an Applicant: http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nManage competing priorities across the company\\nMaintain and automate reporting infrastructure\\nManage the design and architecture of our Data Warehouse\\nCreate Scripts to automate and manage ETL processes and Dependencies\\nAdvise on the design of our application DB, machine learning components, and our data infrastructure.\\nCleaning and restructuring datasets\\nManaging and optimizing reporting systems</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nAt least two years experience as a Data Engineer, or related Software or DevOps experience\\nFluent in Python and SQL\\nExperience with Data Warehouses and Schema Design\\nStrong communication skills and experience communicating across business units\\nExperience working with Data Scientists and Data Analysts\\nAbility to create fast solutions to problems introduced in a changing environment with iteration towards optimal solutions\\nMachine Learning experience a plus\\nAnalytical and BI skills a plus\\nPostgres and RedShift experience is a plus</td>\n",
       "      <td>Who We Are:\\nOcrolus is a Series B venture-backed FinTech company that uses Artificial Intelligence and crowdsourcing to automate financial review processes. The Company transforms e-statements, scans, and cell phone images, regardless of quality, into 99+% accurate digital data. By replacing tedious, imperfect human audits with sharp, AI-driven analyses, Ocrolus modernizes financial assessments in lending and a variety of other industries.\\n\\n\\n\\n\\n\\nWe are seeking a candidate with proven experience working as a Data Engineer, Full Stack Software Engineer with a Data focus, or similar role. The ideal candidate is comfortable in DevOps and Software Engineering.\\nResponsibilities\\nManage competing priorities across the company\\nMaintain and automate reporting infrastructure\\nManage the design and architecture of our Data Warehouse\\nCreate Scripts to automate and manage ETL processes and Dependencies\\nAdvise on the design of our application DB, machine learning components, and our data infrastructure.\\nCleaning and restructuring datasets\\nManaging and optimizing reporting systems\\nRequirements\\nAt least two years experience as a Data Engineer, or related Software or DevOps experience\\nFluent in Python and SQL\\nExperience with Data Warehouses and Schema Design\\nStrong communication skills and experience communicating across business units\\nExperience working with Data Scientists and Data Analysts\\nAbility to create fast solutions to problems introduced in a changing environment with iteration towards optimal solutions\\nMachine Learning experience a plus\\nAnalytical and BI skills a plus\\nPostgres and RedShift experience is a plus\\n\\nWe’re a young and rapidly growing FinTech company - if you have ever wanted to jump on a rocket ship as it’s taking off, now is your chance!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GCP Data Engineer</td>\n",
       "      <td>New York, NY 10001</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10001</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\n5+ years of experience consulting in Data Engineering or Data Warehousing\\nHands-on experience with Google Cloud Platform\\nExperience leading data warehousing, data ingestion, and data profiling activities\\nAdvanced SQL &amp; Python skills\\nHands-on experience with Google cloud platform technologies: Google Cloud Platform Pub/Sub, Cloud Functions, DataFlow, DataProc (Hadoop, Spark, Hive), Cloud Machine Learning, Cloud Data Store and BigTable, BigQuery, DataLab, and DataStudio\\nMigrating Data Pipelines to Google Cloud Platform (GCP)\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBuild and Deploy Data Pipelines on Google Cloud to enable AI &amp; ML capabilities.\\nDrive the development of cloud-based and hybrid data warehouses &amp; business intelligence platforms\\nBuild Data Pipelines to ingest structured and Unstructured Data.\\nGain hands-on experience with new data platforms and programming languages\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Position: GCP Data Engineer\\nLocation: New York, United States\\nRemuneration: $ 115.00 per hour\\nWho is hiring?\\nOne of our Clients in Austin, TX will be hiring for a GCP Data Engineer to assist in building a recommendation software platform utilizing GCP as the main platform for their Data Visualization solution. This role will work closely with their Cloud Architect and Software Development teams thorughout the engagement and may require up to 25% - 50% travel.\\nWhat will you be doing?\\n\\nResponsibilities will include:\\n\\nBuild and Deploy Data Pipelines on Google Cloud to enable AI &amp; ML capabilities.\\nDrive the development of cloud-based and hybrid data warehouses &amp; business intelligence platforms\\nBuild Data Pipelines to ingest structured and Unstructured Data.\\nGain hands-on experience with new data platforms and programming languages\\n\\nQualifications for This Role:\\n\\n5+ years of experience consulting in Data Engineering or Data Warehousing\\nHands-on experience with Google Cloud Platform\\nExperience leading data warehousing, data ingestion, and data profiling activities\\nAdvanced SQL &amp; Python skills\\nHands-on experience with Google cloud platform technologies: Google Cloud Platform Pub/Sub, Cloud Functions, DataFlow, DataProc (Hadoop, Spark, Hive), Cloud Machine Learning, Cloud Data Store and BigTable, BigQuery, DataLab, and DataStudio\\nMigrating Data Pipelines to Google Cloud Platform (GCP)\\n\\nWhy you shouldn’t miss this opportunity?\\nAs a GCP Data Engineer through Third Republic (Recruitment Agency), you will work in teams to deliver innovative solutions on Google Cloud using core cloud data warehouse tools like Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you will be working with some of the most forward-thinking organizations in the advertisement space.\\nData Science(Data Engineer), Google Cloud Platform (GCP), Data Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Opportunity Overview\\nOVERALL SUMMARY\\nThe Business Technology Services (BTS) team within the Media Technology &amp; Information Services (MTIS) group at HBO is responsible for the build and support of vital technology solutions supporting the scheduling, acquisition, processing, and distribution of HBO content. The team is a fundamental part of HBO’s innovation and successful Technology team in New York City, Santa Monica and Seattle. The team partners with various business units to enable the services required to deliver HBO’s premium content to domestic and global platforms as well as third party partners by bringing together platform, software, data and business intelligence services. The team is looking to add a Senior Data Engineer to be part of a team dedicated to breaking the norm and pushing the limits of continuous improvement and innovation. The individual will be involved in detailed technical design, development and implementation of applications using Amazon AWS platform. Working within an agile environment, the Senior Data Engineer will provide input into architectural design decisions, develop code to meet business needs and ensure the applications we build are meeting high standards of quality and supportability. The individual will have the opportunity to develop his/her technical knowledge and skills while evolving the overall technical maturity of the team. This position will be based in Seattle.\\nPRIMARY RESPONSIBILITIES\\nPartner with architects and business leaders to design and build robust services using streaming and batch data.\\nKey contributor in building identity services that will enable HBO to share profiles across the organization in support of marketing and analytics.\\nWork independently and part of teams that will ingest data from a variety of source types including viewership, behavioral, attribution, content metadata etc.\\nStructure and munge ingested data in support of various use cases including analytics, marketing execution and cross divisional data sharing.\\nREQUIREMENTS\\nBachelor’s degree in Computer Science, Computer Engineering, or equivalent\\n2-3 years of experience in the AWS Cloud environment.\\n3-5 years of experience using Python and SQL.\\n4-6 years of experience working with various database methodologies such relational, columnar, NoSQL.\\nExperience with developing and maintaining production data pipelines\\nNice to have: 1 year of experience working on Snowflake Cloud Datawarehouse on AWS.\\nAbout Us\\nIt's HBOSM\\nAmerica's most successful premium television company, Home Box Office delivers two 24-hour pay television services—HBO®and Cinemax®. HBO continues to take advantage of the latest technological innovations with advancements that include the availability of HBO programming online though HBO GOSM and MAX GOSM, as well as HBO On Demand® and Cinemax On Demand® in HD. Just as HBO is a company noted for its commitment to excellence in the products and services it delivers to consumers, it makes the extra effort to create a work environment in which fairness, equity, trust, and individual responsibility are valued. HBO is committed to retaining and recruiting skilled and motivated employees, placing a priority on qualified team players who contribute to the diversity of their workforce. HBO offers competitive benefits to include medical, dental, vision, a matched 401(k) plan, flexible spending, a commuter benefit program and tuition reimbursement.\\nHBO is an equal employment opportunity employer. HBO does not discriminate against any applicant or employee based on race, color, religion, national origin, gender, age, sexual orientation, gender identity or expression, marital status, mental or physical disability, and genetic information, or any other basis protected by applicable law. HBO also prohibits harassment of applicants or employees based on any of these protected categories.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Blink\\n-----------\\n\\nBlink Health is fixing how broken, opaque, and unfair healthcare is. We are a New York based, mission driven, well-funded healthcare technology company. We're changing healthcare through technology and transparency. With our proprietary technology, everyone now has access to one, low negotiated price on over 15,000 medications. But there is more work to do.\\n\\nAbout The Team\\n--------------\\n\\nBlink Engineering strives to build trusted, highly observable, data-driven products to bring affordable, accessible healthcare to all Americans. We understand healthcare is the most complex system most of us will ever fix. We believe in solving this complexity through the use of simple, well-known technologies. We are a highly collaborative team that believes in owning outcomes over owning code and putting patients at the center of everything we do.\\n\\nThe Blink Health Data Engineering and Analytics team is a small team responsible for building infrastructure, frameworks and tooling to enable data-driven decisions; building and maintaining our data warehouse for security and scale. This role is central to building and executing on a robust and forward-looking data strategy for the company, and the successful candidate blends top-tier software engineering expertise with the ability to look ahead at what we need to build for the future.\\n\\nAbout the Role\\n--------------\\n\\nAs the senior software engineer for data, you will be a thought leader within the data engineering team that is designing and building our next generation of data tools and frameworks, in addition to developing and maintaining data products and infrastructure. You will proactively assess production DW support trends to determine and implement short- and long-term solutions, and be able to design for data integrity, reliability, and performance. You will set a high bar for clean and correct code, setting code standards, and performing peer code and architecture reviews.\\n\\nRequired Experience\\n\\n\\nYou have 6+ years hands-on experience and demonstrated strength with:\\nPython, building data pipelines, and managing data at multiple companies.\\nWriting complex, highly-optimized SQL queries across large data sets.\\nBuilding and maintaining robust and scalable data integration (ETL) pipelines using SQL, EMR, Python and Spark.\\nDesigning and maintaining columnar databases (e.g., Redshift, Snowflake)\\nDistributed data processing (Hadoop, Spark, Hive)\\nETL with batch (Data Pipeline) and streaming (Kinesis, Airflow)\\nIntegration and design for Business Intelligence tools (e.g., Looker, QuickSight)\\nCreating scalable data models for analytics.\\nYou have experience designing and refactoring large enterprise data warehouses and associated ETLs, with continuous improvement examples for automation and simplification across all aspects of the DW environment, inclusive of both engineering and business reporting.\\nExperience owning features from design through delivery along with ongoing support.\\nProven success with communicating effectively across diverse disciplines (including product engineering, infrastructure, analytics, data science, finance, marketing, customer support, etc.) to collect requirements and describe data engineering strategy and decisions.\\nExperience providing clear data engineering technical leadership, mentoring, and best practices for data management and quality within and across teams.\\nUndergraduate or graduate degree in Computer Science\\n\\nDesired Experience\\n\\n\\nHealthcare-relevant company experience as part of the required experience above, with demonstrated industry knowledge of handling sensitive information.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Data Engineer - CIMD - Marcus by Goldman Sachs Engineering</td>\n",
       "      <td>New York, NY 10282</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10282</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>MORE ABOUT THIS JOB\\nConsumer and Investment Management (CIMD)\\nThe Consumer and Investment Management Division includes Goldman Sachs Asset Management (GSAM), Private Wealth Management (PWM) and our Consumer business (Marcus by Goldman Sachs). We provide asset management, wealth management and banking expertise to consumers and institutions around the world. CIMD partners with various teams across the firm to help individuals and institutions navigate changing markets and take control of their financial lives.\\n\\nConsumer\\nConsumer, externally known as Marcus by Goldman Sachs, is comprised of the firm’s digitally-led consumer businesses, which include our deposits and lending businesses. It also includes our personal financial management app, Clarity Money. Consumer combines the strength and heritage of a 150-year-old financial institution with the agility and entrepreneurial spirit of a tech start-up. Through the use of insights and intuitive design, we provide customers with powerful tools that are grounded in value, transparency and simplicity to help them make smarter decisions about their money.\\nRESPONSIBILITIES AND QUALIFICATIONS\\nHOW YOU WILL FULFILL YOUR POTENTIAL\\nDesign and develop data ingest and transform processes\\nDevelop data visualizations using BI tools and web-based technologies\\nWork as part of a global team using Agile software methodologies\\nPartner with Marcus risk, product, acquisition and servicing teams\\nUse Marcus data to drive change throughout the Marcus business\\n\\nSKILLS AND EXPERIENCE WE ARE LOOKING FOR\\nBachelor’s degree or equivalent required\\nMinimum 3 years of relevant professional experience\\nProficient at Python, Spark and the Hadoop ecosystem\\nExperience with SQL and relational databases\\nSelf-starter, motivated, and good communication skills Strong sense of ownership and driven to manage tasks to completion\\nABOUT GOLDMAN SACHS\\nThe Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.\\n\\n© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Data Platform Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Knotch:\\n-------------\\n\\nKnotch is the Content Intelligence Platform that enables communications and marketing teams to measure, understand, and optimize content ROI across their digital efforts. We work closely with the following verticals: financial (including JP Morgan Chase &amp; Co., Capital One, Citi, TD, Discover, Ally, Bank of America and Synchrony), automotive (including Ford), telecommunications (including AT&amp;T and Sprint), retail (including Walmart), technology (including Salesforce, HP Inc, HP Enterprise, Citrix, and AWS), and consulting (including Deloitte and PwC.) We are currently expanding quickly into the insurance, fashion and health verticals.\\n\\nAt Knotch, we:\\n\\nEnable brands to become better content creators through real-time and independent measurement &amp; optimization across on and off property content investments.\\nActivate content to move customers to high value actions.\\nTurn content into a first party audience data source that flows into the rest of the downstream marketing platforms (CRM, DMP, CDP etc).\\n\\nWe're based in SoHo, NYC and we're proud to have been named to both Inc.'s Best Places to Work ( https://www.inc.com/best-workplaces-2019.html ) and Built In NYC's Best Places to Work ( https://www.builtinnyc.com/companies/best-places-to-work-nyc-2019 ) lists two years in a row!\\n\\nEngineering at Knotch:\\n----------------------\\n\\nEngineering is the cornerstone of our organization and we work hard everyday to build the most impactful products as possible. We love to experiment, find a deep joy in product iteration, achieve stability with thoughtful architecture and testing all while monitoring our performance and progress at every step.\\n\\nKnotch's founding mission has always been to improve the advertising and marketing industries in a lasting and meaningful way. Transparency through data is our ethos and something every member of our company takes seriously. We are looking for highly motivated engineers who are passionate about data and who are eager to transform an industry to join us on our journey.\\n\\nData Platform Engineering\\n-------------------------\\n\\nAt Knotch, our data platform is vital to providing key intelligence and insights to our clients. As a result, our data platform is the most important element when it comes to successfully scaling our products and technologies. As our platform evolves, agility with stability are critical to avoid bottlenecks in our products and to ultimately increase our value. We're looking for an experienced data engineer who deeply values clever platform architecture and who emphasizes speed and stability to join our growing team.\\n\\nWhat You'll Do at Knotch\\n------------------------\\n\\n\\nDesign and implement resilient backend architectures that process gigabytes and beyond\\nWrite software for backend services using Python, Scala, and other languages\\nWork directly with our Data Science team facilitate new understandings and insights from our data\\nWork directly with our full-stack engineering team to expose value from our data\\n\\nWhat We Want From You\\n---------------------\\n\\n\\n3+ years of data platform engineering experience\\nWorking experience with AWS services including Kinesis, Lambda, S3, and RedShift\\nExperience with various AWS offerings such as EC2, ECS, RDS, and ElastiCache\\nExperience with EMR, Scala, Ruby, Postgres, and Redis\\nExperience utilizing Python to work with Data Science teams to productize models and applications\\n\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Data Engineer Full-time New York, NY</td>\n",
       "      <td>New York, NY 10001</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10001</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are in search of a Data Engineer who loves to take huge chunks of data, transform it to facilitate analytics and customer understanding. In this role you will work with business teams to translate operational requirements into secure tech solutions.\\n\\nOur stack currently includes AWS, Spark, Redshift, Postgres, Pentaho, Airflow and Tableau.We constantly evaluate the latest technologies and we have the freedom to choose the right ones to solve the task.\\nThis role will become an integral part of TIDAL and will develop secure BI solutions. Those solutions will help bring to light actionable insights to improve business decisions and provide better experiences for our customers.\\nAs a Data Engineer at TIDAL you will:\\n\\nDevelop ETL jobs using Spark, Alteryx, or other ETL tools\\nServe as a liaison between the Analytics team and Integration teams to serve data in a way that fulfills analytics requirements\\nWork closely with other teams such as Analytics, Finance, Product, Retention, and Growth to gather specifications and provide them with relevant prepared data\\nAutomate report distribution to partners in the music industry, tech integrations, and carriers\\nBuild tech integrations between 3rd party tracking systems and internal databases\\nRespond to ad hoc data requests and troubleshoot data issues\\nMaintain and enforce role based security to ensure that data is secure and only accessed by approved employees\\nWork closely with stakeholders on the marketing team to enhance and maintain data flows, reporting, and event / user tracking on Adjust, Braze, and other platforms in the marketing tech stack\\n\\n\\nWhat you need to succeed in this role:\\n\\n4 Year Bachelor of Science with a concentration in Computer Science, Engineering, or Data Science preferred\\nExperience with Python or Scala\\nExperience using BI and ETL tools\\nKnowledge of SQL programming and development of data warehouses\\nAbility to understand complex business logic\\nKnowledge of Spark is an advantage\\n\\n\\nWho you are:\\nYou know how to work with high volume heterogeneous data, preferably with distributed systems such as AWS Redshift Spectrum and RDS.\\nYou are knowledgeable about data modeling, data access, and data storage techniques.\\nYou appreciate agile software processes, data-driven development, reliability, and responsible experimentation.\\nYou understand the value of partnership within teams\\nYou are prepared to have frequent business trips to Oslo\\n\\n\\nDoes this sound like you?\\n\\nApply today and become a new Data Engineer at TIDAL.\\n\\nWhat we offer:\\n\\nTechnology: You will work with the latest technology and collaborate with skilled teams\\nOur office: We are located in the middle of Times Square, NYC.\\nVisit Scandinavia: Since you will be part of our BI team in Oslo, you will have the opportunity to travel to Norway on frequent business trips.\\n\\nSocial: We host intimate concerts in the office and we have a Christmas party.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Three or more years of experience working with *nix-based, open source data processing tools\\nFluent in Python, SQL, Spark, Hadoop, AirFlow and/or similar technologies/toolsets\\nTwo or more years of experience developing production ETL applications\\nFour or more years of experience in software development\\nThree or more years of experience with SQL\\nCurious, informed and opinionated about data processing technologies\\nExperience with structured and unstructured data storage and modeling\\nDeep understanding of database and filesystem storage/access\\nExperience with various data engineering architecture patterns\\nInterest in Data Science and Data Analysis\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Building python-based ETL jobs\\nLight web application development of purpose-built internal tools\\nTechnical guidance in support of our project management team when defining the scope of data integration projects\\nContribute to designs of new components in modeling and data pipelines\\nRemain current on emerging open source data processing projects and tools\\nMust have experience with AWS services, Redshift data bases and HIPAA compliant architecture models.\\n</td>\n",
       "      <td>BS or MS in Computer Science\\nExperience writing production Python\\nImplementation experience with Airflow, Python, Spark etc.\\nExperience with healthcare data formats (x12 EDI, HL7, etc)\\nExperience implementing stream processing pipelines (Spark, etc)\\nMapReduce/Hadoop ecosystem experience (Hive, HDFS/S3, Presto)\\nExperience with source control technology like Git\\nExperience with testing frameworks (unit and end-to-end)\\nFamiliar with the usage of Continuous Integration/Continuous Deployment frameworks in AWS like Jenkins, CircleCI, Code deploy and code commit\\nUnderstanding of Docker implementation in AWS\\nUnderstanding of AWS serverless services like Lambda/API gateway\\nGood to have: AWS elastic beanstalk, AWS cloud formation\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>VillageCare – Redefining Wellness\\n112 Charles St., New York, NY\\nVillageCareMax\\nJob Title: Data Engineer\\n\\nRoles &amp; Responsibilities:\\nThe VillageCare Data Engineer will be responsible for helping to build ETL processes for Village Care’s Data Warehouse in an AWS cloud environment. The Data Engineer will build ETL pipelines, get analysis tools working properly, and stand up core data processing components in Village Care’s cloud-based data processing environment.\\nBuilding python-based ETL jobs\\nLight web application development of purpose-built internal tools\\nTechnical guidance in support of our project management team when defining the scope of data integration projects\\nContribute to designs of new components in modeling and data pipelines\\nRemain current on emerging open source data processing projects and tools\\nMust have experience with AWS services, Redshift data bases and HIPAA compliant architecture models.\\nQualifications:\\nThree or more years of experience working with *nix-based, open source data processing tools\\nFluent in Python, SQL, Spark, Hadoop, AirFlow and/or similar technologies/toolsets\\nTwo or more years of experience developing production ETL applications\\nFour or more years of experience in software development\\nThree or more years of experience with SQL\\nCurious, informed and opinionated about data processing technologies\\nExperience with structured and unstructured data storage and modeling\\nDeep understanding of database and filesystem storage/access\\nExperience with various data engineering architecture patterns\\nInterest in Data Science and Data Analysis\\nPreferred Education and Experience\\nBS or MS in Computer Science\\nExperience writing production Python\\nImplementation experience with Airflow, Python, Spark etc.\\nExperience with healthcare data formats (x12 EDI, HL7, etc)\\nExperience implementing stream processing pipelines (Spark, etc)\\nMapReduce/Hadoop ecosystem experience (Hive, HDFS/S3, Presto)\\nExperience with source control technology like Git\\nExperience with testing frameworks (unit and end-to-end)\\nFamiliar with the usage of Continuous Integration/Continuous Deployment frameworks in AWS like Jenkins, CircleCI, Code deploy and code commit\\nUnderstanding of Docker implementation in AWS\\nUnderstanding of AWS serverless services like Lambda/API gateway\\nGood to have: AWS elastic beanstalk, AWS cloud formation\\n\\nIntegrity\\nYou are a team member who serves as a positive example and reflection of why others trust the intentions of VillageCare by:\\nBeing honest and trustworthy\\nMeeting your commitments and obligations\\nAcknowledging your role in actions or events with unsatisfactory outcomes\\nCustomer Focus/Cultural Awareness\\nYou are a team member who understands the importance of strong customer service internally and externally and you demonstrate this by identifying customer needs and expectations, and responding to them in a timely and effective manner. You are consistently customer focused by:\\nDemonstrating an awareness of the needs of individuals through recognizing multiple levels of connections\\nAnticipates and prevents delays or other things that can adversely affect the customer.\\nKeeping customers informed about the status of pending actions and inquires\\nFlexibility/Agility\\nYou are a team member who adjusts quickly and effectively to changing conditions and demands. You understand that change is a necessary and an inevitable aspect of organizational life as well as an opportunity to learn new things. As such, you are flexible and agile by:\\nMaintaining a positive view of potentially stressful situations\\nAccepting and adapting to organizational or departmental changes\\nViewing change as opportunities for VillageCare to grow in a direction that better serves our clients and our employees\\nResult Oriented/Innovative Thinking\\nYou are a team member who consistently looks for new and innovative approaches that will improve efficiency in your role. You champion new ideas and build upon existing processes by:\\nUsing data/fact-based information to make decisions relevant your role\\nUnderstands that obstacles will occur and refuses to use them as an excuse for not achieving results\\nBEVital\\nYou are a team member that consistently supports VillageCare’s larger organizational culture by displaying a commitment to the three cultural drivers that make VillageCare and our employees vital to the healthcare space by:\\nExceeding expectations in both internal and external customer service areas\\nUsing data and key information to inform decisions pertinent to your role (where applicable)\\nUtilizing relationships, tools and positivity to enhance organizational performance through communication and collaborative team work\\nVillageCare is committed to superior outcomes in quality health care. Do you share a common commitment to* patient care, customer service and passion *for individuals’ well-being ?\\nApply now!\\nVillageCare:\\nWith over 25,000 people served in 2017, VillageCare’s mission is to promote healing, better health and well-being to the fullest extent possible.\\nVillageCare began in 1977 as a project by community volunteers to rescue and reorganize a for-profit nursing home slated for closure. It has become a much larger organization that provides post-acute care, community-based services and managed long-term care. As a result of this history, VillageCare has become a valued resource for the people we serve, their caregivers and other provider organizations with which we partner.\\nVillageCare is committed to the tenets of diversity and workforce that are strengthened by the inclusion of and respect for our differences. We offer our employees a highly competitive compensation and benefits package, a 403(b) retirement plan, and much more.\\nVillageCare is an equal opportunity employer. We promote recognition and respect for individual and cultural differences, and we work to make our employees feel valued and appreciated, whatever their race, gender, background, or sexual orientation.\\n* EOE Minorities/Women/Disabled/Veterans*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nWe’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.\\nThe desire to learn and code in Scala\\nExperience in working in an Agile environment\\nExpert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.\\nA strong coding background in either Java, Python or Scala\\n</td>\n",
       "      <td>Founded in 2016 with only a handful of individuals, Quantexa purpose was built that through a greater understanding of context, better decisions can be made. 3 years, 6 locations and 180+ employees later we still believe that today. Working within industries such as Finance, Insurance, Energy and Government, we connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.\\n\\nOur success is driven by the talent of our staff and our commitment to quality. We are looking for Data Engineers to join us in tackling some of the industry’s most challenging problems.\\nWhat does a Data Engineer role at Quantexa look like?\\nIn order to be a successful data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.\\nBeing Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service.\\nWe want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.\\nRequirements\\nWhat do I need to have?\\nWe’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.\\nThe desire to learn and code in Scala\\nExperience in working in an Agile environment\\nExpert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.\\nA strong coding background in either Java, Python or Scala\\nExperience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.\\nPassion and drive to grow within one of the UK’s fastest growing Start-ups\\nBenefits\\nWhy join Quantexa?\\n\\nWe know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying “thank you” for all your hard work!\\nCompetitive Salary\\nCompany Bonus\\nExcellent private healthcare, Dental and Optic coverage, Life assurance, LTD and STD coverage\\n401k where we’ll match up to 5%\\nOnline training customized to your personal preferences\\nGenerous annual leave\\nAmazing working environment - Ranging from regular social events, free beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY 10013</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10013</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Lab49 has an exciting opportunity for a Data Engineer to work with our clients to create next generation data and analytics solutions using the latest techniques and technologies. This role is responsible for understanding and solving the technical problems of each engagement, including identifying sources of relevant data, transforming and scrubbing to create data pipelines, doing feature engineering in collaboration with data scientists, and creating scalable solutions that can work in production.\\n\\nThe role will require a combination of strong technical skills and problem-solving capabilities, with an understanding of the underlying business concepts and meaning of the data. The Data Engineer is central to the success of every engagement, making sense of the data so that it can be analyzed and modeled to create insights.\\n\\nCandidate Profile\\n\\nThe successful candidate will be a professional with prior experience working on data and analytics initiatives. This person will be passionate about getting to the bottom of the problems underlying data and finding creative and unique solutions.\\n\\nThe successful candidate will have the ability to use programmatic tools and techniques to quickly understand different data sources, transform and combine them into reliable pipelines of clean data, help data scientists engineer features from the data, and scale the resulting solutions into production-quality systems.\\n\\nRequired Attributes\\n3+ years of experience working in a hands-on capacity on data and analytics initiatives in the financial services industry\\nAbility to quickly learn financial services concepts to successfully understand the meaning of data and analytics related to the business problem being addressed\\nStrong software development experience with languages such as Python, R, Scala, including strong proficiency with data manipulation libraries like pandas\\nPrevious experience and clear proficiency on the command-line using tools like Bash and integrated scripting\\nPrevious experience using a variety of tools for data exploration, analysis, and presentation, including advanced use of Microsoft Excel\\nSignificant experience with relational databases and advanced use of SQL\\nSufficient mathematical and statistical literacy to work with business stakeholders, quantitative analysts, and data scientists\\nGood interpersonal skills, with an ability to interact with teammates and clients to successfully understand and solve problems\\nEffective communication skills, with an ability to clearly present ideas and data, including making use of visualization techniques\\nPreferred, but not required, attributes\\nExperience using non-RDBMS technologies, e.g. graph databases, columnar stores, document databases, OLAP cubes, etc.\\nExperience working in a variety of cloud and PaaS environments, e.g. Azure, AWS, Databricks, Domino\\nExperience with a variety of big data frameworks, e.g. Spark, Hadoop, Cassandra\\nExperience doing basic modeling and analytics using different frameworks, e.g. TensorFlow, scikit-learn, Keras, Torch\\nAbout Lab49\\n\\nLab49 is a global strategy, design and technology consulting company specializing in capital markets and finance. We help our clients realize rapid transformational change at an industrial scale assisting those companies with their most key strategic business programs and technology investments.\\n\\nLab49’s Data Practice partners with clients to help understand the potential of their data assets and create and implement compelling products to realize that potential. We take a consultative and domain-led approach to understanding our clients’ businesses, and apply advanced data science and machine learning techniques to create next generation solutions using cutting edge technologies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3+ years of Software Engineering experience\\n2+ years experience work with real-time/streaming data\\nExperience with a RDBMS (e.g. MySQL, PostgreSQL)\\nExperience with real time data streaming tools (e.g. Apache Kafka, AWS Kinesis)\\nExperience working with an OLAP or Time Series Databases (e.g. Druid)\\nExperience with a data processing solution. (e.g. AWS Athena, Apache Spark)\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Who We Are\\nFairygodboss is an early-stage start-up based in New York. Our mission is to improve the workplace for women by creating transparency. We do this by creating a safe, anonymous and supportive place for women to leave job reviews and insight into compensation, benefits and culture. Our platform reaches over 3 million users every month and is sponsored by over 80 top employers. We are looking to double our technical team in 2019.\\nWhat You Will Be Doing\\n\\nWe are currently rebuilding our platform and looking to bring on an experienced data engineer to:\\nDevelop personalization and recommendation engines used to tailor unique experiences and results on a user-by-user basis for each of our various projects.\\nBuild out and support our Real-time and Batch processing pipelines.\\nConfigure and monitor our event logs and analytical data stores.\\nConfigure and maintain our BI tools and reporting libraries.\\n\\nQualifications\\nThe ideal candidate should have experience with real-time user data processing and storage. The candidate will also have experience working with stakeholders to identify important metrics that can then be incorporated into our APIs for both internal and external use.\\nRequired:\\n\\n3+ years of Software Engineering experience\\n2+ years experience work with real-time/streaming data\\nExperience with a RDBMS (e.g. MySQL, PostgreSQL)\\nExperience with real time data streaming tools (e.g. Apache Kafka, AWS Kinesis)\\nExperience working with an OLAP or Time Series Databases (e.g. Druid)\\nExperience with a data processing solution. (e.g. AWS Athena, Apache Spark)\\n\\nNice To Have:\\nKnowledge of JavaScript &amp; Node.js\\nExperience building APIs\\nExperience building personalization or recommendation engines\\nExperience with Machine Learning/Data Science\\n\\nOther Tools We Use:\\nAWS\\nDocker &amp; Container Orchestration Tools\\nDruid &amp; Kinesis\\n\\nCompensation:\\nHighly competitive and commensurate with experience.\\n\\nCompany Description:\\nWho We Are:\\n\\nFairygodboss is an early-stage start-up based in New York. Our mission is to improve the workplace for women by creating transparency. We do this by creating a safe, anonymous and supportive place for women to leave job reviews and tips about employer pay, benefits and culture. We’re growing rapidly and expanding our team.\\nIf you're interested in learning the ins and outs of running a digital startup and improving the world for women at the same time, this job is for you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Principal Data Engineer, AdSmart</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Serve as a senior data engineer for audience studio data products.Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and SRAT leadershipDevelop and code the data management services that is core to Audience Studio, under the leadership of the VP ArchitectureSupport product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>As a member of the Product Engineering Team, the Principal, Data Engineer, AdSmart will be directly responsible for data design, management, and development as part of building out the necessary platform and products for NBCUniversal’s AdSmart . NBCUniversal’s audience management products will enable NBCUniversal to better understand its brand’s audiences such as NBC News, Bravo, The Tonight Show, Saturday Night Live, and USA Network as well as audiences that cross brands. The goal is to ensure we know who is watching what, where and when. In turn enabling NBCUniversal’s sales teams to properly align our audiences with the market advertisements that can benefit them the most.\\n\\nYou’re a big thinker who can analyze and evangelize a long range opportunity, architect a ground breaking solution, and roll-up your sleeves to get code out the door when needed. You are data driven and analytical. You understand the concept of a value proposition and evaluation criteria, and you know how to align them with low level milestones to get the work done. You can apply domain knowledge from one technical subject, in order to quickly ramp and deliver on a new one. You know how to learn from failure until you succeed, and you are able to articulate and quantify the reasons for your decisions.\\n\\nYou will be part of the AdSmart's Data Engineering team, participating in the data architecture that will drive both current and future data management initiatives within NBCUniversal’s Audience Studio group.\\n\\nResponsibilities\\nServe as a senior data engineer for audience studio data products.Participate in, and execute, a 12-36 month product roadmap with input from the delivery team, stakeholders, and SRAT leadershipDevelop and code the data management services that is core to Audience Studio, under the leadership of the VP ArchitectureSupport product with the overall roadmap and ensure updates to senior leadership are 100% technically correct.Analyze and report results and adjust the overall engineering strategy accordingly with engineering leadership\\nQualifications/Requirements\\nBachelor’s degree in Computer Science or related field\\n5+ years of software development experience, as a developer or manager\\nFluency in Scala and/or Java programming languages\\nStrong OO &amp; FP design patterns, data structure, and algorithm design skills\\nExtensive experience developing Apache Spark applications\\n2+ years of experience with both relational database design (SQL), non-relational (NoSQL) databases, big data, real-time technologies\\nFamiliar with various cloud data sources and architectures such as AWS/S3, HDFS, Kafka\\nExperience with software containerization, such as Docker\\nExperience developing and / or consuming web interfaces (REST API) and associated skills (HTTP, web services)\\nSelf-directed, ability to multi-task, sharp analytical abilities, excellent communication skills, capable of working effectively in a dynamic environment\\n\\nAdditional Job Requirements:\\nInterested candidate must submit a resume/CV through www.nbcunicareers.com to be considered\\nMust be willing to work in New York, NY\\nDesired Characteristics\\nExperience as a development manager (with direct authority over development staff)\\nExperience with Cluster Management and Container Orchestration technologies such as Mesos, Kubernetes, Hadoop/Yarn\\nExperience with Apache Kafka or similar streaming technologies\\nExperience with digital advertising technologies.\\nAble and eager to learn new technologies\\nAble to easily transition between high-level strategy and day-to-day implementation\\nExcellent teamwork and collaboration skills\\nResults oriented, high energy, self-motivated\\nSub-BusinessTechnology\\nCareer Level\\nExperienced\\nCityNew York\\nState/Province\\nNew York\\nCountryUnited States\\nAbout Us\\nAt NBCUniversal, we believe in the talent of our people. It’s our passion and commitment to excellence that drives NBCU’s vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It’s what makes us uniquely NBCU. Here you can create the extraordinary. Join us.\\nNotices\\nNBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Informatica Engineer</td>\n",
       "      <td>New York, NY 10011</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10011</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum of 1 year of Experience developing and implementing Informatica Powercenter or Informatica Data Quality\\nBachelor’s Degree or Associate’s Degree with 6 years of work experience or equivalent work experience of 12 years</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nIt is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve clients, our employees must be able to travel when needed. This role requires 100% flexibility to travel and work onsite with clients (typically Monday through Thursday).\\nProven success in contributing to a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills</td>\n",
       "      <td>Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements and the way we collaborate, operate and deliver value provides an unparalleled opportunity to grow and advance. Choose Accenture, and make delivering innovative work part of your extraordinary career.\\n\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\n\\n Why Should I Join the Accenture Team?\\nDrive innovation. People in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward.Learn and grow continuously: Follow personalized training to build skills, while expanding your experience defining and implementing solutions on complex client projects with a scope that is unsurpassed in industry.Thrive in our inclusive environment: Bring your whole self to a company that aims to be the most diverse in the world and delivers real-time performance feedback based on individual strengths—not stats.\\n\\n\\nJob Description\\nInformatica Data and Analytics professionals define strategies, develop and deliver solutions that enable the collection, processing and management of information from one or more sources, and the subsequent delivery of information to audiences in support of key business processes.\\n\\nProduce clean, standards based, modern data mapping with Informatica an emphasis on advocacy toward end-users to produce high quality software designs that are well-documented.\\nDemonstrate an understanding of technology and digital frameworks in the context of data integration with Informatica and other technologies.\\nEnsure SQL code and design quality through the execution of test plans and assist in development of standards, methodology and repeatable processes, working closely with internal and external design, business, and technical counterparts.\\n\\nBasic Qualifications\\nMinimum of 1 year of Experience developing and implementing Informatica Powercenter or Informatica Data Quality\\nBachelor’s Degree or Associate’s Degree with 6 years of work experience or equivalent work experience of 12 years\\nPreferred Qualifications\\nExperience with one or more in addition to Informatica ETL tools, including Business Objects Data Services (BODS), DataStage, Ab Initio, Talend, and Pentaho\\nExperience with a full life-cycle development from functional design to deployment\\nDatabase experience (Teradata, Oracle, SQL Server, DB2, Azure SQL)\\nStrong knowledge and experience of SQL\\nUnderstanding of Entity relationship data models and Dimensional Models\\nExperience with development and production support\\nProfessional Skill Requirements\\n\\nIt is currently our objective to assign our people to work near where they live. However, given the nature of our business and our need to serve clients, our employees must be able to travel when needed. This role requires 100% flexibility to travel and work onsite with clients (typically Monday through Thursday).\\nProven success in contributing to a team-oriented environment\\nProven ability to work creatively and analytically in a problem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal skills\\n\\nAll of our consulting professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>This Data Engineer (DE) will work as an individual contributor and technical lead as a member of the CDO (Chief Data Officer) team. Core responsibilities will be creating data pipeline and big data warehousing solution for TickPick. the data engineer will be responsible to develop, construct, tests and maintains solutions, such as databases and large-scale data processing systems. Specific responsibilities include:\\n Ensure architecture will support business requirements\\n\\n Discover opportunities for data acquisition and implement solutions\\n Develop production processes and solutions to model, mine, and surface data\\n Integrate production systems, employing a variety of languages and tools\\n Improve and ensure data reliability, quality, and efficiency\\n Work with other members of the data team and beyond, including data analysts and data scientists\\n Challenge the ethos of TickPick with out of the box thinking to further enhance the capabilities and solutions\\n Grow and maintain a deep knowledge of the business along with stakeholder needs and constraints\\n Work with senior team to review and improve agile delivery and engineering practices\\n Drive innovative ideas, solutions and products through thought leadership and decisive action\\n Advance the engineering culture of TickPick\\n Maintain strong relationships with business and engineering teams to ensure product goals are met\\nRequirements\\nTECHNICAL SKILLS:\\n Experience with latest NoSQL and Big Data Technologies as well as RDBMS\\n Experience with Microservices architecture and solutions\\n Experience with latest Cloud technologies with focus on container-based solutions\\n Extensive knowledge of programming and scripting languages, such as Java, Bash, C++, Phyton, R, Spark, Hive\\n Experience with object-oriented design, coding and testing patterns\\n Experience in engineering software platforms and large-scale data solutions\\n Capability to architect highly scalable distributed systems, using different open source tools.\\n Experience implementing high-performance algorithms.\\n\\nPOSSIBLE INITIATIVES:\\n Create primary cloud-based data platform for TickPick\\n Centralize data ingestion and data pipelining\\n Automate analytical data reporting capabilities\\n Publish data services to support data-driven marketplace operation and marketing efforts\\n Deepen data integration for large marketing partnership opportunities (FanDuel, WeFest, RiotFest etc.).\\n CRM integration and automation\\n\\nKEY SELECTION CRITERIA\\n5+ years of experience in software and data engineering with roles in a comparable company at a comparable scale. Prior experience in a larger company is not required, but an environment where one is schooled in proper systems, processes and procedures is preferred. Proof of points transitioning into and successfully working in a smaller, faster-paced, more hands-on, and entrepreneurial environment is a plus. Hands-on experience technology delivery is required. Well-developed skills in working with peers in a similar-sized environment are essential. The DE needs to be an adept communicator with both technical and non-technical stakeholders, including external parties as required. The successful candidate will be hands-on; intellectually curious; versatile; creative and resourceful; and will demonstrate a bias for action and a commitment to continuous process improvement.\\n\\nCRITICAL COMPETENCIES\\n Leadership &amp; Influencing Skills — Demonstrates stellar leadership skills working with engineering team and\\nstakeholders; effectively leads technology delivery functions; acts as a key member of the company’s engineering\\nteam in all technical matters.\\n Strong Analytical Skills — Curious with strong quantitative and qualitative analytic skills; demonstrates a balance\\nbetween theoretical and practical solutions and is fact-based in decision-making; synthesizes lots of data derived\\nfrom a wide range of sources and is creative in determining appropriate course of action.\\n Initiative &amp; Urgency — Knows how to work independently; ensures responsive and timely follow-up; sets and\\nmeets aggressive deadlines; demonstrates a sense of urgency; prioritizes exceptionally well while utilizing excellent\\norganizational skills.\\n Strong Communication Skills — Crisp and effective oral and written communication skills; ability to communicate\\nat all levels of the organization; builds and maintains strong relationships with peers and prioritizes interaction with\\ndepartments and staff across the org.\\n Creative Thinker —Enjoys thinking of new ways to solve traditional problems; a visionary at heart who is truly\\nsatisfied when satisfying customers’ needs through creative, thoughtful solutions.\\n\\nPERSONAL ATTRIBUTES\\n Low ego and hands on leader who is proactive in helping advance the organization.\\n Dynamic, curious, and flexible as business conditions change.\\n Strategic capabilities to think like an owner of the business in terms of technical excellent, especially with a date-driven approach towards the delivery of products and capabilities.\\n Adaptable, nimble, resilient and passionate, with a real sense of urgency.\\n Sets a high bar for performance in the data organization (and elsewhere), including analytic capability, work ethic, integrity, loyalty and commitment to company values.\\n Results-oriented and proactive effectively working with external partners, including customers and partners.\\n Superior verbal and written communication skills and strong EQ in building relationships with peers and others.\\n Professional style that includes analytic rigor, strategic thinking, and intellectual agility.\\nBenefits\\n Competitive compensation\\n Daily lunch stipend\\n Medical, dental, and vision plans\\n PTO policy\\n 401K plan\\n Fully stocked pantry and keg on site\\n Monthly team happy hours\\n Summer Fridays\\n Work with a fun and fast moving team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Data Engineer I</td>\n",
       "      <td>Newark, NJ 07102</td>\n",
       "      <td>Newark</td>\n",
       "      <td>NJ</td>\n",
       "      <td>07102</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>A Bachelor’s degree or higher in Computer Science, Engineering, Mathematics, Physics, or a related fieldProficiency in SQL and programming languages such as python2+ years of hands on experience in working with data including but not limited to Data Warehouse (DWH) environment with data integration/ETL of large and complex data sets, Data modeling skills such as Star/Snowflake schema design for DWH, or building large scale data-processing systems with experience in Big Data technologies such as MapReduce, Hadoop, Spark, Kafka or AWS equivalents.Familiarity with Database technologies such as AWS Redshift, Oracle, Teradata, or othersFamiliarity with Business Intelligence (BI) and Visualization platforms such as MicroStrategy and AWS QuicksightAbility to communicate effectively and work independently with little supervision to deliver on time quality productsWillingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision\\n\\nThis opportunity is within Audible’s Data Engineering group. The Data Engineering group owns technology platforms and datasets that enable systems and people to uncover new insights and fine-tune operations to meet business goals. We need your help designing and building these.\\n\\nKEY RESPONSIBILITIES\\nApply broad knowledge of technology options, technology platforms, design techniques and approaches across the Data Engineering ecosystem to build systems that meet business needsBuild systems and datasets using software engineering best practices, data management fundamentals, data storage principles, recent advances in distributed systems, and operational excellence best practicesAnalyze source systems, define underlying data sources and transformation requirements, design suitable data models and document the design/specificationsDemonstrate passion for quality and productivity by use of efficient development techniques, standards and guidelinesEffectively communicate with various teams and stakeholders, escalate technical and managerial issues at the right time and resolve conflictsPeer review work. Actively mentor other members of the team, improving their skills, their knowledge of our systems and their ability to get things done\\nHOW DOES AMAZON FIT IN?\\nWe're a part of Amazon, they are our parent company and it's a great partnership. You'll get to play with all of Amazon's technologies like EC2, SQS and S3 but it doesn't stop there. Audible's built on Amazon technology and you'll have insight into the inner workings of the world's leading ecommerce experience. There's a LOT to learn!\\n\\nIf you want to own and solve problems, work with a creative dynamic team, fail fast in a supportive environment whilst growing your career and working on a platform that powers web applications used by millions of customers worldwide we want to hear from you.\\n\\nHands on experience with BI and Visualization platforms such as MicroStrategy and AWS QuicksightExperience with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena\\nAudible is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Data Engineer (AWS)</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s Degree Required: Computer Science or Engineering discipline preferred.\\n3+ years technology experience working in Software Engineering capacity.\\n3+ years working in Python (other modern languages considered).\\n1+ years working within Analytic/Data Warehouse/Data Lake environment.\\n1+ years working with AWS public cloud (certification a plus).\\nExpertise in SQL: 10 out of 10, SQL Ninja analytic capabilities.\\nStrong working knowledge of with Linux Shell.\\nUnderstanding of Data Warehouse principles, including Dimensional Modeling.\\nCreative, flexible, and quick to learn.\\nExperience with Redshift a plus.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDesign, develop, deploy and manage a reliable and scalable data analysis pipeline, using technologies including Python, S3, and Redshift.\\nParticipate in cross-functional initiatives to develop new capabilities, including hands-on development responsibilities.\\nAbility to integrate data from a variety of sources, assuring they adhere to data quality and accessibility standards.\\nDocument processes and standard operating procedures.\\nEvaluate and conduct POC’s with new technologies.\\n\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Founded in 2011 by executives at Equinox, Blink Fitness is a premium quality, value-based fitness brand with more than 90 locations open or in development throughout New York, New Jersey, Pennsylvania and California. Blink puts Mood Above Muscle™, which celebrates the positive feeling you get from exercise, not just the physical benefits.\\nBlink recently launched a franchising system to complement its fast growing company-owned business model and has secured franchise development agreements in Georgia, Illinois, Massachusetts, Michigan, Virginia and upstate NY. This includes an agreement with Golden State Warriors forward Draymond Green, two-time NBA Champion, All Star, Olympic Gold Medalist and Defensive Player of the Year, who announced a franchise development deal to bring 20 gyms to his home state of Michigan and a portion of Illinois.\\nBlink is an exciting and dynamic business that is still in the start-up mode. We are a passionate team with a great entrepreneurial spirit and a willingness to roll up our sleeves to get the work done.\\nWhile Blink has grown rapidly and has already achieved significant profitability, the business is just getting warmed-up. Its leadership has a lofty vision of opening more than 300 locations in the next five years through a combination of company-owned and franchise development.\\nFor more information visit Blink’s consumer website - blinkfitness.com – and its franchise website – blinkfranchising.com.\\nJob Description:\\nBlink is seeking a Data Engineering professional to join our technology team. This role is a hands-on engineering position responsible for the build and maintenance of our cutting edge cloud based data platform.\\nResponsibilities:\\nDesign, develop, deploy and manage a reliable and scalable data analysis pipeline, using technologies including Python, S3, and Redshift.\\nParticipate in cross-functional initiatives to develop new capabilities, including hands-on development responsibilities.\\nAbility to integrate data from a variety of sources, assuring they adhere to data quality and accessibility standards.\\nDocument processes and standard operating procedures.\\nEvaluate and conduct POC’s with new technologies.\\n\\nQualifications:\\nBachelor’s Degree Required: Computer Science or Engineering discipline preferred.\\n3+ years technology experience working in Software Engineering capacity.\\n3+ years working in Python (other modern languages considered).\\n1+ years working within Analytic/Data Warehouse/Data Lake environment.\\n1+ years working with AWS public cloud (certification a plus).\\nExpertise in SQL: 10 out of 10, SQL Ninja analytic capabilities.\\nStrong working knowledge of with Linux Shell.\\nUnderstanding of Data Warehouse principles, including Dimensional Modeling.\\nCreative, flexible, and quick to learn.\\nExperience with Redshift a plus.\\nCompensation and Benefits:\\nCompetitive Base Salary\\nComplimentary Blink membership\\nComprehensive benefits package\\nAnd more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Azure Data Engineer</td>\n",
       "      <td>New York, NY 10011</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10011</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of consulting or client service delivery experience on Azure\\n</td>\n",
       "      <td>DevOps on an Azure platform</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\n</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n Join Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n People in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Azure Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Azure public cloud technologies, our Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of todays corporate and emerging digital applications\\n\\n Role &amp; Responsibilities:Provide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers successfully delivering work efforts\\n\\n (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nBasic Qualifications\\nAt least 5 years of consulting or client service delivery experience on Azure\\nAt least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutions\\nExtensive experience providing practical direction within the Azure Native and HadoopMinimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.\\nExtensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.\\n Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.\\n5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.\\nMinimum of 5 years of RDBMS experience\\nExperience in using Hadoop File Formats and compression techniquesExperience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nMCSA Cloud Platform (Azure) Training &amp; Certification\\nMCSE Cloud Platform &amp; Infratsructiure Training &amp; Certification\\nMCSD Azure Solutions Architect Training &amp; Certification\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nIoT, event-driven, microservices, containers/Kubernetes in the cloud\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\nFamiliarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\n- Multi-cloud experience a plus - Azure, AWS, Google\\n\\nProfessional Skill Requirements\\n Proven ability to build, manage and foster a team-oriented environment\\n Proven ability to work creatively and analytically in a problem-solving environment\\n Desire to work in an information systems environment\\n Excellent communication (written and oral) and interpersonal skills\\n Excellent leadership and management skills\\n Excellent organizational, multi-tasking, and time-management skills\\n Proven ability to work independently\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Sr. Data Engineer (Consultant)</td>\n",
       "      <td>New York, NY 10017</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10017</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Sr. Data Engineer (Consultant) - 170k+ Total compensation - New York\\n\\nWho is hiring?\\n\\nWe are currently working with a leading Data Consulting client in the US thatspecializes in all things data and analytics including big data, modern data architecture, cloud migration, enterprise data management, business intelligence, data visualization, advanced analytics, and machine learning.\\n\\nThey seek to hire a Senior Data Engineer to assist them in building an AWS Cloud-based solution with Python, PySpark Data pipelines for a recommendation engine that they are building for one of their Fitness clients in Manhattan.\\n\\nThis recommendation engine will be taking info from user devices and pushing them into their proprietary platform in order to make fitness suggestions for their clients. This will be a brand new product offering, that will need to be user-friendly and drive more users to the application.\\n\\nWhat will you be doing?\\n\\nThis will require a background in:\\n\\nKnowledge of Data Warehousing and SQL\\nExperience working with Python, Spark &amp; PySpark\\nSome Experience working with Software Development\\nExperience with AWS Redshift and RDS or similar\\nExperience with ETL and Business Intelligence tools\\nKnowledge of data modeling, data access, and data storage techniques\\n\\nYou will be working with the following technologies to successfully build this platform:\\n\\nPython\\nAWS\\nAPI Development\\nETL Integration with Python\\nCode intensive environment\\n\\nWhy you shouldn’t miss this opportunity?\\nAs a Data Engineer at this client, you’ll work in small teams to deliver innovative solutions using core cloud data warehouse tools and Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you’ll be working with some of the most forward-thinking organizations in data and analytics.\\n\\nContract to hire - Salary Range - 130 - 170k+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Google Certified Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About Mediaagility\\n\\nMediaAgility is a Digital Consulting company and a Google Cloud Premier partner. The company is headquartered in Princeton, NJ with offices in US, UK, Mexico, India and Singapore. Our mission is to create customer-centric intelligence to help innovators succeed. We do this through a unique BrainTrust engagement model, Agile development and deep expertise on latest technologies.\\n\\nWe offer full spectrum digital consulting and bring our customers from idea to impact with ‘Strategic Thought Partnership’ to bring the best thinking from across our company, partners like Google and industry experts. We build ‘Intelligence Solutions’ with our years of consulting experience packaged into productized solution patterns to address business challenges. Our focus is on Analytics/ML, Location Intelligence, Conversational Intelligence and Modernising applications / data warehouses. We have helped over a thousand customers across nine countries.\\n\\n\\nJob Description\\n\\nDesign and Develop Analytics and Machine Learning implementations using Python, numpy, scipy, scikit-learn, pandas, Tensorflow, Keras, PyTorch etc.\\n\\nLead the efforts in building end to end streaming and batch data analytics pipelines. From data ingestion, processing, storage, analysis, machine-learning to visualization. Understand big-data principles and best practices. Deliver projects in data analytics, machine learning and AI.\\n\\nLead the efforts in building Industry specific Machine Learning and Deep Learning models on structured and unstructured data. Work towards creating IP.\\n\\nDesign architectures, publish reference code and establish data structure design based on business requirements. Should be pretty hands on.\\n\\nPerform code reviews, ensure code quality and encourage a culture of excellence.\\n\\nBe a front face of the company in front of customers and prospects.\\n\\n\\nRequired Skills\\n\\nMandatory Certification: Google Cloud Certified Professional Data Engineer\\n\\nStrong analytical skills.\\n\\n3+ years of strong technology experience in the field of Big Data, Data Analytics, Data Science or Machine Learning.\\n\\nMachine Learning Experience- Should have experience with training custom models with Python, numpy, scipy, scikit-learn, pandas, Tensorflow, Keras, PyTorch etc.\\n\\nBig Data experience - should have experience in building end to end data pipelines with products like Apache Beam, Kafka, Spark etc.\\n\\nCloud Experience- Should have experience with GCP data products like BigQuery, DataFlow, Firestore, CloudSQL etc.\\n\\nShould understand and be able to command architecture design for Data Analytics and Machine Learning systems.\\n\\nExperience and solid knowledge in Agile (Scrum) Methodologies\\n\\nWorking Knowledge of BI &amp; visualization tools like Google Data Studio, Power-BI, Qlik Sense, Micro-strategy etc\\n\\nFamiliarity with standard source repositories (GIT, BitBucket)\\n\\nExcellent communication skills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Software Engineer - Big Data</td>\n",
       "      <td>New York, NY 10001</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10001</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Our Consumer &amp; Community Banking Group depends on innovators like you to serve nearly 66 million consumers and over 4 million small businesses, municipalities and non-profits. You’ll support the delivery of award winning tools and services that cover everything from personal and small business banking as well as lending, mortgages, credit cards, payments, auto finance and investment advice. This group is also focused on developing and delivering cutting edged mobile applications, digital experiences and next generation banking technology solutions to better serve our clients and customers.\\n\\nAs an experienced Big Data Engineer, your mission is to help lead our team of innovators and technologists toward creating next-level solutions that improve the way our business is run. Your deep knowledge of design, analytics, development, coding, testing and application programming will help your team raise their game, meeting your standards, as well as satisfying both business and functional requirements. Your expertise in various technology domains will be counted on to set strategic direction and solve complex and mission critical problems, internally and externally. Your quest to embracing leading-edge technologies and methodologies inspires your team to follow suit. And best of all, you’ll be able to harness massive amounts of brainpower through our global network of technologists from around the world.\\n\\nBachelor's Degree or better in Engineering, Computer Science or Information Technology\\nCloud Computing - AWS\\nBigData, Hadoop, Hive\\nODS - Cassandra\\nJava 8 / Spark\\nETL - Talend\\nReal time messaging – Kafka / Java on Gaia Application Platform\\nExposure to Machine Learning will be a big +\\n3+ years of experience in middle-tier/backend systems development in Java/Linux\\nBig data background with experience designing and implementing large scale systems\\nWorking experience with Hadoop, Enterprise Java development, NoSQL data platforms (Cassandra), Pub/sub messaging (Rendezvous, AMPS, Kafka, etc.), Stream processing (Storm, Hbase, Nifi, Spark Streaming, etc.), Batch Processing with tools such as Talend, Informatica, or Hive/SQL and Visualization with Tableau.\\nExtensive experience with horizontally scalable and highly available system design and implementation, with focus on performance and resiliency\\nExtensive experience profiling, debugging, and performance tuning complex distributed systems\\nWillingness to commit extra effort to meet deadlines as required on a high profile and business critical project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Requisition no: 503035\\nWork type: Full Time\\nLocation: Medical Center\\nSchool/Department: Biomedical Informatics\\nGrade: Grade 105\\nCategories: Information Technology, Research (Lab and Non-Lab)\\nPosition Summary\\n\\nThe Department of Biomedical Informatics (DBMI) at Columbia University is revolutionizing the clinical research enterprise with the help of information technology. At DBMI, we are building the infrastructure of the future to support and enable better research and dissemination. We have an immediate opening for a talented and self-motivated data engineer developer who can succeed in a collaborative work environment. The ideal candidate will have experience with data pipelines and cloud environments. The candidate will be responsible for data processing, data exchange/transfer/load (ETL), data visualization, DevOps, and software architecture. The ideal candidate will have professional experience in a number of programming languages, databases, and development environments. The candidate should be able to contribute to improving the reliability and quality of data. Experience in clinical medicine, clinical vocabulary, and cloud development are not required but preferred. The successful candidate will contribute to the development of open source solutions together with a community of international researchers.\\n\\nCurrent available position is grant-funded.\\n\\nColumbia University's Department of Biomedical Informatics is internationally recognized as one of the best programs of its kind. Our mission is to improve health for society by focusing on discovery and impact: we develop new informatics methods, enrich the biomedical knowledge base, and enhance the health of the population.\\n\\nResponsibilities\\n\\n1. Software and system design, implementation, and testing (75%)\\n\\n2. Application deployment and configuration (10%)\\n\\n3. Communicate with technical individuals at various grant sites (10%)\\n\\n4. Software requirements specification (5%)\\n\\nMinimum Qualifications\\n\\nBachelor's degree or equivalent in education and experience (computer science, biomedical informatics, information science), plus four years of related experience.\\n\\nOther Requirements\\n\\nGreat communication skills; Experience with one or more compiled programming languages (e.g. Java, Scala, C#, C++, etc.) and one or more interpreted programming languages (Python, JavaScript, Perl, bash, etc.)\\n\\nWorking knowledge of SQL; Experience with big data, NoSQL databases, and health care data a plus.\\n\\nEqual Opportunity Employer / Disability / Veteran\\n\\nColumbia University is committed to the hiring of qualified local residents.\\n\\nApplications open: Sep 12 2019 Eastern Daylight Time\\nApplications close:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Data Engineer – Customer Support Experience</td>\n",
       "      <td>New York, NY 10011</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10011</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a Data Engineer to join Spotify’s Customer Support Systems &amp; Technology R&amp;D department. Our mission is to amplify the voice of the customer to Spotify, while creating effortless and proactive digital help experiences – when and where users need it. We believe by achieving this mission we are enabling support to be a differentiator for Spotify and increasing our competitive advantage.\\nYou will collaborate with highly skilled researchers, engineers, data scientists, product managers, and others in a fast-paced, multi-disciplinary environment. Above all, you will have a hand in impacting brand loyalty and improving the overall Spotify experience.\\n\\nAs a part of our data engineering team, you will be creating data sets that can be easily consumed and analyzed to understand customer pain points, and thereby improving the Spotify product family in turn. You will be instrumental in our journey to transition our systems from traditional databases to distributed data sets that can be easily used to gather insights about our products by data scientists and analysts across the company. This includes, but is not limited to:\\n\\nBuild large-scale batch and real-time data pipelines with data processing frameworks like Scio, Beam, DataFlow on the Google Cloud Platform.\\nUse best practices in continuous integration and delivery.\\nHelp drive optimization, testing and tooling to improve data quality.\\nCollaborate with other software engineers, ML experts, and stakeholders, taking learning and leadership opportunities that will arise every single day.\\nWork in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.\\nWho You Are\\nYou have 3+ years experience in a hands-on role\\nYou know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, Apache Beam, Spark.\\nYou know how to write distributed, high-volume data processing systems in Java or Scala.\\nYou have experience working with Python, the more the better.\\nYou are knowledgeable about data modeling, data access, and data storage techniques.\\nYou appreciate agile software processes, data-driven development, reliability, and responsible experimentation.\\nYou understand the value of partnership within teams.\\nYou understand that product value is a function of usefulness and not technical implementation details.\\nYou are willing to work from our awesome office in New York. We offer relocation packages if you do not currently live in New York.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Data Engineer, Measurement Program, YouTube</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nManage partner technical integration projects and ensure the prompt and proper resolution of technical challenges.\\nDevelop and maintain third-party data validation methodologies, including building and maintaining automated and scalable technical infrastructure.\\nGuarantee the technical aspects of a partner’s integration (both new and ongoing) by providing technical guidance and documentation.\\nIdentify, drive, and optimize new third-party reporting opportunities by leveraging YouTube technologies.\\nWrite and maintain lines of code (Python, C++, etc.) to support your own small to medium scale Extract, Transform, Load (ETL) pipelines.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Minimum qualifications:\\n\\nBachelor's degree in Computer Science, or a related technical field, or equivalent practical experience.\\n3 years of experience with ETL and SQL.\\n2 years of experience working with one or more of the following: C/C++, Java, Go, Python, Unix/Linux systems.\\nScripting experience in Shell, Perl or Python.\\n\\nPreferred qualifications:\\n\\nMBA, Master's degree or PhD.\\n2 years of work experience in a client-facing role.\\nExperience in driving highly cross-functional initiatives that range from structured project management to ambiguous thought leadership.\\nExperience in successfully navigating large organizations in order to complete both individual and collaborative projects.\\nExcellent data management, quantitative, and qualitative skills.\\nAbout the job\\nYouTube's Technology Solutions Organization is a global organization dedicated to developing and managing the company's largest and most strategic partnerships. We work closely with the YouTube product, engineering, and content teams to address our partners' most pressing and complex technology challenges. As a Partner Technology Manager, you'll lead deployments, optimize implementations, and handle integrations to build strong, successful, long-term partnerships.\\nThe YouTube Measurement Program (YTMP) is responsible for ensuring that YouTube is represented fairly in third-party reporting tools, and that internal data feeding those tools is accurate and consistent. We provide partner technical support, analytical services, and thought leadership to several initiatives shaping the long-term direction of YouTube.\\nAs a Data Engineer for the YouTube Measurement Program, you will be the integration manager responsible for the success of some of our most important third-party measurement partnerships. You will optimize and scale our validation programs, drive cross functional buy-in on roadmaps, and bring thought leadership to an important and highly visible ecosystem.\\nAt YouTube, we believe that everyone deserves to have a voice, and that the world is a better place when we listen, share, and build community through our stories. We work together to give everyone the power to share their story, explore what they love, and connect with one another in the process. Working at the intersection of cutting-edge technology and boundless creativity, we move at the speed of culture with a shared goal to show people the world. We explore new ideas, solve real problems, and have fun — and we do it all together.\\nResponsibilities\\nManage partner technical integration projects and ensure the prompt and proper resolution of technical challenges.\\nDevelop and maintain third-party data validation methodologies, including building and maintaining automated and scalable technical infrastructure.\\nGuarantee the technical aspects of a partner’s integration (both new and ongoing) by providing technical guidance and documentation.\\nIdentify, drive, and optimize new third-party reporting opportunities by leveraging YouTube technologies.\\nWrite and maintain lines of code (Python, C++, etc.) to support your own small to medium scale Extract, Transform, Load (ETL) pipelines.\\nAt Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY 10010</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10010</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>dv01 is the world's first end-to-end data management, reporting and analytics platform offering loan level transparency and insight into lending markets, making them more efficient for institutional investors and safer for the world. In a nutshell, we're doing our part to prevent a repeat of 2008.\\n\\nAs the technological hub between lenders and capital markets, dv01 provides all parties with unprecedented data transparency, insight, and analytics. dv01 has integrated data from 16 marketplace lending platforms, including LendingClub, Prosper and SoFi and multiple mortgage servicers. To date, dv01 has provided reporting and analytics on $105 billion of online lending and mortgage loans and $35 billion of securitization coverage.\\n\\nYOU WILL:\\n---------\\n\\nBe at the heart of data processing at dv01. You will own the suite of transformation logic that processes and standardizes our varied input file packages into the dv01 internal data model. You will operate as the bridge between the engineering and finance teams, contributing to a variety of integral processes that drive dv01 on a daily basis. Every new dataset that gets integrated within dv01 will have your fingerprints all over it.\\n\\nBe an owner of dv01's most valuable asset. You'll own the business logic in our data pipeline, encapsulating all the knowledge we've accumulated across hundreds of datasets. The output from the pipeline powers all of dv01's customer offerings and is critical to the success of our business, so you will be a resource to both our clients and all the other teams within dv01 who harness our internal data warehouse.\\n\\nBe customer-facing. You have the opportunity to get direct exposure to high-level contacts at hedge funds, banks, and asset originators, providing valuable insights to help them answer complex questions.\\n\\nWork with state-of-the-art technology. You'll work with popular, modern, and exciting open source technologies like Apache Spark. The skills you develop here will serve you well beyond dv01.\\n\\nQUALIFICATIONS:\\n---------------\\n\\nA well-rounded engineer. You have 3-5+ years of professional programming experience with Apache Spark, Scala, Java, R, or Python. You are able to write thought-out code while accounting for resource and performance constraints and are also capable of performing ad-hoc data investigations with SQL.\\n\\nExcited about big data. You should have 3-5+ years of professional engineering experience working with large datasets in a distributed data processing framework, with exposure to large datasets related to loan products an added plus. You enjoy working with data, from expressing complex business logic as scalable data processing logic to configuring and debugging intricate big data pipelines. You love the intricate details of a thorough investigation, but also stay aware of the bigger picture while operating across multiple threads of work.\\n\\nInterest and experience in both engineering and finance. You're looking to grow your skills in both disciplines and are excited about the synergies between finance and technology.\\n\\nKnowledgeable about consumer credit. You understand how investors evaluate loan portfolios and the complexities of amortization, prepay, and default. You strive to further your knowledge in the credit market.\\n\\nUndergraduate or graduate degree in Finance, Math, or Engineering. Note that we're not anti dropouts if you're a superstar.\\n\\nPerks and Benefits:\\nAlmost 100% Paid Benefits (medical/dental/vision)\\nMonthly Commuter Budget\\nDaily Lunch and Dinner Allowance\\nFree Premium Equinox or ClassPass Membership\\nUnlimited PTO and Remote days\\nCasual, collaborative culture\\nCompany Outings (Happy Hours, Team Yoga, Book Club, etc.)\\n\\nTo get a better idea of what a year at dv01 looks like, check out our 2018 Year in Review page here: https://dv01.co/2018yearinreview/ ( https://dv01.co/2018yearinreview/ ). If that looks like fun to you, get in touch because we'd love to hear from you.\\n\\ndv01 is an equal opportunity employer and all qualified applicants and employees will receive consideration for employment opportunities without regard to race, color, religion, creed, sex, sexual orientation, gender identity or expression, age, national origin or ancestry, citizenship, veteran status, membership in the uniformed services, disability, genetic information or any other basis protected by applicable law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExperience writing software in one or more languages such as Python, Java, Scala, or Go\\nExperience building production-grade data solutions (relational and NoSQL)\\nExperience with systems monitoring/alerting, capacity planning and performance tuning\\nExperience in technical consulting or customer-facing role\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join SADA as a Sr. Data Engineer!\\n\\nYour Mission\\n\\nAs a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.\\n\\nYou will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.\\n\\nPathway to Success\\n\\n#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.\\n\\nYour success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.\\n\\nExpectations\\n\\nRequired Travel - 30% travel to customer sites, conferences, and other related events\\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.\\nTraining - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Data Engineer Certified\\n\\n[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment\\n\\nRequired Qualifications:\\n\\nMastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExperience writing software in one or more languages such as Python, Java, Scala, or Go\\nExperience building production-grade data solutions (relational and NoSQL)\\nExperience with systems monitoring/alerting, capacity planning and performance tuning\\nExperience in technical consulting or customer-facing role\\n\\nUseful Qualifications:\\n\\nExperience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)\\nExperience with IoT architectures and building real-time data streaming pipelines\\nExperience operationalizing machine learning models on large datasets\\nHihg\\nDemonstrated leadership and self-direction -- a willingness to teach others and learn new techniques\\nDemonstrated skills in selecting the right statistical tools given a data analysis problem\\n\\nAbout SADA\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>New York, NY 10011</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10011</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Help us Build the Future of Money\\n\\nGemini Trust Company, LLC (Gemini) is a licensed digital asset exchange and custodian. We built the Gemini platform so customers can buy, sell, and store digital assets (e.g., Bitcoin, Ethereum, and Zcash) in a regulated, secure, and compliant manner.\\n\\nDigital assets and blockchain technology have the power to transform the world for good. This truth, along with our core values, form the bedrock of our company and culture. At Gemini, no job is too small and no project too big as we endeavor to build the future of money. We are a mission-driven, team-based, inclusive, and determined community of thought leaders who invest in each other and the long game. Join us in our mission!\\n\\nTHE DEPARTMENT: DATA ENGINEERING\\n\\nTHE ROLE: SENIOR DATA ENGINEER\\n\\nAs a member of our data engineering team, you'll shape the way we approach data at Gemini by using your engineering, analytical and communication skills to work with teams across the business. You know how to ask the right questions and are passionate about using data to support and drive informed business decisions. You are ready to roll up your sleeves and are excited to take on challenging opportunities and projects. You'll mentor data engineers and analysts and guide our internal teams to use data to improve the product and achieve KPIs. Communicating your insights with leaders across the organization is paramount to success.\\n\\nRESPONSIBILITIES:\\n\\nDesign, architect and implement best-in-class Data Warehousing and reporting solutions\\nLead and participate in design discussions and meetings\\nMentor data engineers and analysts\\nDesign, automate, build, and launch scalable, efficient and reliable data pipelines into production\\nBuild real-time data and reporting solutions\\nDesign, build and enhance dimensional models for Data Warehouse and BI solutions\\nResearch new tools and technologies to improve existing processes\\nDevelop new systems and tools to enable the teams to consume and understand data more intuitively\\nPartner with engineers, project managers, and analysts to deliver insights to the business\\nPerform root cause analysis and resolve production and data issues\\nCreate test plans, test scripts and perform data validation\\nTune SQL queries, reports and ETL pipelines\\nBuild and maintain data dictionary and process documentation\\n\\nMINIMUM QUALIFICATIONS:\\n\\n7+ years experience in data engineering with data warehouse technologies\\n7+ years experience in custom ETL design, implementation and maintenance\\n7+ years experience with schema design and dimensional data modeling\\nExperience building real-time data solutions and processes\\nExperience building and integrating web analytics solutions\\nAdvanced SQL skills is a must\\nSkilled in programming languages Python and/or Java\\nExperience with one or more MPP databases(Redshift, Bigquery, Snowflake, etc)\\nExperience with one or more ETL tools(Informatica, Pentaho, SSIS, Alooma, etc)\\nStrong computer science fundamentals including data structures and algorithms\\nStrong software engineering skills in any server side language, preferable Python\\nExperienced in working collaboratively across different teams and departments\\nStrong technical and business communication\\n\\nPREFERRED QUALIFICATIONS:\\n\\nKafka, HDFS, Hive, Cloud computing, machine learning, text analysis, NLP &amp; Web development experience is a plus\\nNoSQL experience a plus\\nExperience with Continuous integration and deployment\\nKnowledge and experience of financial markets, banking or exchanges\\n\\nIt Pays to Work Here\\n\\nWe take a holistic approach to compensation at Gemini, which includes:\\n\\n\\nCompetitive base salaries across all departments\\nOwnership in the company via profit sharing units\\nAmazing benefits, 401k match contribution, and flexible hours\\nSnacks, Perks, Wellness Outings &amp; Events\\n\\nGemini is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you have a disability or special need that requires accommodation, please let us know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>114 5th Ave (22114), United States of America, New York, New York\\n\\nAt Capital One, we’re building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nSenior Data Engineer\\n\\nBeing Capital One Tech:\\nAt Capital One, we consider ourselves the bank a technology company would build. We’re delivering best-in-class innovation so that our 65 million customers - and counting - can manage their finances with ease. Our reality and vision empower our engineers to use artificial intelligence and machine learning to transform real-time data, software, and algorithms into financial clarity.\\n\\nWe’re all-in on the cloud and a leader in the adoption of open source, RESTful APIs, microservices, and containers. We build our own products and release them with a speed and agility that allows us to get new customer experiences to market quickly. We’re going boldly where no bank has gone before. And, as a founder-led company, we’re inspired and empowered to make, break, do, and do good . So, let’s do something great together.\\n\\nYour #LifeatCapitalOne\\n\\nLooking to work somewhere with the flexibility of a start-up but the financial muscle of a Top-10 bank? You’re in the right place! And here’s what that means for you…\\n\\nYou'll have a flexible work schedule—we want to understand where and when you're at your best so you have a healthy work-life balance. Diversity and Inclusion are cultural norms here—you’ll have access to active local chapters of Women in Tech, Blacks in Tech, and Hispanics in Tech and more. Plus, you’ll be given time to support the next generation of technologists by volunteering with youth programs like Capital One Coders - our engineer-led experience that teaches middle school students in underserved communities how to code. Want to learn more? See what our associates are up to at #LifeatCapitalOne !\\n\\nCalling All Senior Data Engineers:\\nA hub for innovation, our New York presence is expanding, and we need Senior Data Engineers who know their stuff to join our team. As a Capital One Data Engineer , you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One. You’ll work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems. You’ll collaborate with digital product managers, and deliver robust cloud-based solutions to drive powerful experiences that help millions of Americans achieve financial empowerment. Want to learn more? Check out the low-down on our high-tech .\\n\\nWho You Are:\\nYou are fun to work with – you’re excited by a team environment\\n\\nYou are curious. You like to learn new technologies , and you adapt well to change\\n\\nYou are passionate about current state-of-the-art software technologies and tools, with experience implementing them effectively\\n\\nYou are excited about working with cloud-native stack, building on AWS using technologies like Kubernetes and Serverless\\n\\nYou possess a sense of intellectual curiosity and a burning desire to learn\\n\\nYou are motivated and actively looking for ways to contribute\\n\\nYou are passionately focused on the customer and the details that make their experience exceptional\\n\\nYou value data and truth over ego\\n\\nYou possess a strong sense of engineering craftsmanship, take pride in your code\\n\\nYou’re pragmatic - you make the best use of time and resources to find the simplest workable solution\\n\\nYou think and act like an owner, taking personal responsibility for both team and product success\\n\\nYou possess great communication and reasoning skills, including the ability to influence and make a strong case for technology choices\\n\\nYou thrive in collaborative agile teams and are ready to take on new and unexpected challenges while building the next wave of engineering solutions\\n\\nWhat You’ll Own:\\nCollaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies\\n\\nLeading the craftsmanship, security, availability, resilience, and scalability of your solutions\\n\\nBringing a passion to stay on top of current trends, experiment with and learn new technologies, participate in internal &amp; external technology communities, and mentor other members of the engineering community\\n\\nEncouraging innovation, implementation of cutting-edge technologies, outside-of-the-box thinking, teamwork, and self-organization\\n\\nAssisting in the hiring of top engineering talent and maintaining our commitment to diversity and inclusion\\n\\nBasic Qualifications:\\nBachelor’s Degree\\n\\nAt least 4 years of experience in application development\\n\\nAt least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)\\n\\nPreferred Qualifications:\\nMaster's Degree\\n\\n5+ years of experience in application development\\n\\n2+ years of experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink\\n\\n1+ years of experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service\\n\\n1+ years of experience with Ansible / Terraform\\n\\n2+ years of experience with Agile engineering practices\\n\\n1+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)\\n\\n1+ years of experience with NoSQL implementation (Mongo, Cassandra)\\n\\n2+ years of experience developing Java based software solutions\\n\\n2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)\\n\\n2+ years of experience developing software solutions to solve complex business problems\\n\\n2+ years of experience with UNIX/Linux including basic commands and shell scripting\\n\\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Latch's data team is continuing to be built from ground up, and we are still actively hiring. We're covering all aspects of the Latch's data strategy including: analysis, engineering, science, &amp; more. We are firm believers of bringing in smart people that can define the roles for themselves, so come join us and start creating greatness.\\n\\nSmart access isn't about locking doors, it's about opening up new possibilities. Latch is the world's first fully integrated hardware and software system dedicated to bringing seamless access to every door in a modern building. We're looking for the curious and the creative to join our team and help us change the way we access our most valued spaces.\\n\\nResponsibilities\\n\\n\\nUnderstand the data gathered across the entire Latch organization\\nDesign and implement data pipelines, building scalable and optimized enterprise level data systems\\nCollaborate with other teams in the company, both engineering and business counterparts\\nTransform raw data into meaningful sets that are query-able and visualizable.\\nWork closely with Data Analysts and Data Scientists to implement production ready systems\\nBe a helping hand with tools used by other teams such as Sales CRMs, Ops Customer Success tools, Marketing automation or Finance ERP. Data from these tools are very important to us.\\n\\nRequirements\\n\\n\\nBS in Computer Science, Math, related technical field or equivalent practical experience\\n3+ years of general software programming experience in Java or similar languages\\nExcellent grasp of data structures and algorithms\\nSolid level of understanding in SQL\\nKnowledge of database technology, schema design, and query optimization techniques\\nExperience in ETL pipelines and data transformations.\\nExcellent communication skills\\n\\nPreferred Qualifications\\n\\n\\nMS in Computer Science, Mathematics, or related technical field\\nExperience with Map-Reduce technologies such as Spark or Hadoop.\\nUnderstanding of basic data science concepts\\nExperiencing in productionizing machine learning models.\\nAcute sense of data analysis: being able to make sense out of many seemingly unrelated data sets.\\n\\nFounded in 2014, Latch is a venture-backed, high-growth organization that's on a mission to change the way people open, manage, and share their spaces. Today, 1 in 10 new developments in the U.S. depend on our full-building smart access solution to meet the needs of residents and property managers.\\n\\nWe are a team of just over 200 employees, all of whom are passionate self starters with unique backgrounds and unexpected stories. We offer unlimited time off, a competitive health package, and the opportunity to work in a creative, dynamic, and fast-paced office environment. We are located just a quick walk from both Hudson Yards and Penn Station in New York City.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Cyber Data Engineer</td>\n",
       "      <td>Manhattan, NY 10007</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>NY</td>\n",
       "      <td>10007</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>About New York City Cyber Command\\nNYC Cyber Command was created in 2017 by Executive Order to lead the City’s cyber defense efforts, working across more than 100 agencies and offices to prevent, detect, respond, and recover from cyber threats. NYC Cyber Command is committed to protecting NYC infrastructure and critical systems from malicious attacks through the use of the latest technologies, public-private partnerships, and regular training and exercises for City employees.\\n\\nJob Description\\nData Engineers support the Cybersecurity Data Science team, led by the Cybersecurity Lead Data Scientist;Data Science team analyzes data in order to defend against cyber threats to the City. This team is integral to the defense of the City’s information environment, and works directly with the NYC3 Architecture, Engineering, and Threat Management teams;Data Engineers design, build, implement, and operate systems that ingest, normalize, correlate, analyze, and present cybersecurity relevant data from across the City.\\n\\n\\nMinimum Qual Requirements\\n\\n1. A baccalaureate degree, from an accredited college including or supplemented by twenty-four (24) semester credits in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or\\n\\n2. A four-year high school diploma or its equivalent approved by a State’s department of education or a recognized accrediting organization and three years of satisfactory experience in any of the areas described in “1” above; or\\n\\n3. Education and/or experience equivalent to “1” or “2”, above. College education may be substituted for up to two years of the required experience in “2” above on the basis that sixty (60) semester credits from an accredited college is equated to one year of experience. In addition, twenty-four (24) credits from an accredited college or graduate school in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or a certificate of at least 625 hours in computer programming from an accredited technical school (post high school), may be substituted for one year of experience.\\n\\n\\nPreferred Skills\\n\\nThe preferred candidate should possess the following:\\nData Engineers have at least 1 year experience (including internships) analyzing large, high velocity, heterogeneous datasets. They are proficient in using cloud-based tools for ingesting, normalizing, analyzing, and presenting data for future engineering solutions and executive decision makers;Possess at least bachelor’s degrees in computer science or information systems and have specializations in mathematics, number theory, applied cryptography, or statistics or relevant experience;Should have strong practical knowledge of data structures, Java, relational database design, and familiarity with system level and distributed programming;Familiarity with Unix scripting, Web development, and automated testing would be highly desirable, but not necessary.\\n\\n\\nTo Apply\\n\\nInterested applicants with other civil service titles who meet the preferred requirements should also submit a resume for consideration\\nFor City employees, please go to Employee Self Service (ESS), click on Recruiting Activities &gt; Careers, and search for Job ID #407411\\nFor all other applicants, please go to www.nyc.gov/jobs/search and search for Job ID #407411\\n\\nSUBMISSION OF A RESUME IS NOT A GUARANTEE THAT YOU WILL RECEIVE AN INTERVIEW\\nAPPOINTMENTS ARE SUBJECT TO OVERSIGHT APPROVAL\\n\\nThe Department of Information Technology &amp; Telecommunications and the City of New York are equal opportunity employers.\\n\\nDoITT participates in E-Verify\\n\\n\\nHours/Shift\\n\\nDay - Due to the necessary technical support duties of this position in a 24/7 operation, candidate may be required to work various shifts such as weekends and/or nights/evenings.\\n\\n\\nResidency Requirement\\n\\nNew York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nTransform raw data into actionable and meaningful analyses.\\nWork with a cross-functional team to define and clarify analytic goals, develop analysis plans, and ensure data quality standards.\\nIdentify key metrics and develop dashboards/reports to better the understanding of member marketing solutions.\\nWork closely with our data engineer(s) to seamlessly integrate data across audience management platforms and integrate learnings with member profiles for actionable next steps.\\nHelp measure and compare the success of marketing metrics across campaigns.\\nDocument data requirements and think through complex problems to make actionable recommendations.\\nPerform ad-hoc reporting and analyses as needed.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor or Master's degree in Economics, Statistics, Finance, or Mathematics preferred.\\n3-5 years of analytics or quantitative experience in which you extracted meaningful insights from big datasets.\\nExperience with test design, KPI development and post campaign analyses.\\nExperience working in a company with a subscription based business.\\nStrong experience with using BI tools to build dynamic/scalable dashboards. Looker experience a plus.\\nStrong Knowledge of SQL, your queries are optimized for readability and performance. Google BigQuery a plus.\\nStrong conceptual/analytical/creative thinker – ability to push beyond specific requests and identify the business need in advance of execution.\\nAbility to handle multiple tasks, good time management and organizational skills.\\nExcellent communication and organizational skills.</td>\n",
       "      <td>Data Analyst\\nCompany Overview:\\n\\nWW is looking for candidates to help change people’s lives. We are a global wellness technology company inspiring millions of people to adopt healthy habits for real life. We do this through engaging digital experiences, face-to-face workshops and sustainable programs that encourage people to move more, shift their mindset and eat healthier while enjoying the foods they love. By drawing on over five decades of experience and expertise in behavioral science, we build communities in order to deliver wellness for all.\\n\\nTo learn more about WW and jobs with a purpose, visit ww.com.\\n\\nRole Overview:\\nWe are now looking for a talented Data Analyst to join our growing Analytics group to help drive a data-first culture across WW.\\nThe data analyst will work closely with both business and technical teams to proactively identify gaps in our knowledge base across member marketing. The candidate will be charged with providing analytics reporting and data-driven strategic insights, trends, and perspective.\\nYou will work closely with a cross functional team through all development stages, from identifying opportunities, to analyzing the impact of efforts, and discovering areas for improvement.\\nKey Responsibilities:\\nTransform raw data into actionable and meaningful analyses.\\nWork with a cross-functional team to define and clarify analytic goals, develop analysis plans, and ensure data quality standards.\\nIdentify key metrics and develop dashboards/reports to better the understanding of member marketing solutions.\\nWork closely with our data engineer(s) to seamlessly integrate data across audience management platforms and integrate learnings with member profiles for actionable next steps.\\nHelp measure and compare the success of marketing metrics across campaigns.\\nDocument data requirements and think through complex problems to make actionable recommendations.\\nPerform ad-hoc reporting and analyses as needed.\\n\\n\\nGoals / Deliverables:\\n\\nBuild segments in audience management platforms for member marketing teams.\\nDevelop test plans and identify KPIs/targeted audiences for testing across member marketing communications.\\nProvide post test analyses with actionable recommendations and next steps.\\n\\n\\nRequirements:\\nBachelor or Master's degree in Economics, Statistics, Finance, or Mathematics preferred.\\n3-5 years of analytics or quantitative experience in which you extracted meaningful insights from big datasets.\\nExperience with test design, KPI development and post campaign analyses.\\nExperience working in a company with a subscription based business.\\nStrong experience with using BI tools to build dynamic/scalable dashboards. Looker experience a plus.\\nStrong Knowledge of SQL, your queries are optimized for readability and performance. Google BigQuery a plus.\\nStrong conceptual/analytical/creative thinker – ability to push beyond specific requests and identify the business need in advance of execution.\\nAbility to handle multiple tasks, good time management and organizational skills.\\nExcellent communication and organizational skills.\\n\\n\\nAs a company, our purpose is to inspire healthy habits for real life. And as an employer, we inspire the greatest people to do their best work. We provide benefits for real life to help protect your health, finances and overall well-being, including:\\nCompetitive compensation and profit-sharing plan\\nA 401K plan to help you plan for your future, plus company match\\nHealth care coverage starting on your first day\\nTuition reimbursement and online courses to help you reach your career aspirations\\nCommuter benefits\\nYearly well-being allowance for your physical, financial, social and emotional well-being\\nFree WW membership for you plus 3 free WW memberships for your friends and 3 for your family\\nFree fruit, snacks and coffee to get you through your day\\nSummer Fridays, happy hours, and company outings\\nRobust employee referral bonuses\\nDevelopmental opportunities and assignments to grow your career\\nWW is an equal opportunity employer. WW does not discriminate on the basis of sex, race, color, creed, national origin, marital status, age, religion, sexual orientation, gender identity, gender expression, veteran status, or disability.\\nAny offer of employment is contingent upon the satisfactory results of reference and background checks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>X-Mode provides real-time location data and technologies that power location intelligence for advertising and business decisions in financial services, healthcare, high-tech, real-estate, retail, and the public sector. X-Mode's flagship product is a fast-growing big data location platform, which maps daily the precise routes of 10% of the U.S. Population and maps monthly 1 in 3 adult U.S. smartphone users. X-Mode strives to produce and monetize the world's largest location platform and ultimately create a global \"living map\" of 1 billion people with the highest quality location data in order to fuel the best location intelligence business solutions.\\n\\nX-Mode Social, Inc. is looking for a full-time lead data engineer to work on X-Mode's data platform and join our rapidly growing team. For this position, you can work in either our Reston, VA headquarters or remotely. Our technical staff is scattered across the U.S, so you'll need to be comfortable working remotely. We often use videoconferencing tools (like Slack, Google Meet) to coordinate, as well as Jira for tasking, and Bitbucket for source control. We work in short sprints, and we'll count on you to provide estimates for tasks to be completed and delivered. We're looking to hire someone to start right away! Think you've got what it takes? Apply below!\\n\\nWHAT YOU'LL DO:\\n---------------\\n\\n\\nUse big data technologies, processing frameworks, and platforms to solve complex problems related to location\\nBuild, improve, and maintain data pipelines that ingest billions of data points on a daily basis\\nEfficiently query data and provide data sets to help Sales and Client Success teams' with any data evaluation requests\\nEnsure high data quality through analysis, testing, and usage of machine learning algorithms\\n\\nWHO YOU ARE:\\n------------\\n\\n\\n3-5+ years of Spark and Scala experience\\nExperience working with very large databases and batch processing datasets with hundreds of millions of records\\nExperience with Hadoop ecosystem, e.g. Spark, Hive, or Presto/Athena\\nReal-time streaming with Kinesis, Kafka or similar libraries\\n4+ years working with SQL and relational databases\\n3+ years working in Amazon Web Services (AWS)\\nA self-motivated learner who is willing to self-teach\\nWilling to mentor junior developers\\nSelf-starter who can maintain a team-centered outlook\\nBONUS: Experience with Python, Machine Learning\\nBONUS: GIS/Geospatial tools/analysis and any past experience with geolocation data\\n\\nWHAT WE OFFER:\\n--------------\\n\\n\\nCool people, solving cool problems.\\nCompetitive Salary\\nMedical, Dental and Vision\\n15 Days of PTO (Paid Time Off)\\nWe value your input. This is a chance to get in on the \"ground floor\" of a growing company\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Hello, World! Codecademy has helped over 45 million people from around the world upgrade their careers with engaging, accessible, and flexible education on programming and data skills. We provide over 200 hands-on interactive lessons ranging from Python to R to Javascript and everything in between. Our learners have gone on to start companies, new jobs, and new lives thanks to what they’ve learned with Codecademy, and we’re thrilled to be working to take that impact to the next level.\\n\\nCodecademy was started in 2011 by two college students in a dorm room at Columbia that were frustrated by the huge gap between education and employment. A few years later, we are a rapidly growing, diverse team of 75+ in SoHo, NYC. We’ve raised over $40m in venture capital funding from top investors including Union Square Ventures, Kleiner Perkins, Naspers, Y Combinator, and more.\\n\\nIf you want to help build a business that impacts tens of millions of people each year and helps them lead better lives, join us!\\n\\nCodecademy's Infrastructure &amp; Services team is responsible for the infrastructure and operations to deliver our service to millions of users learning to code. We are looking for a Senior Data Engineer to join this team; in this role you will also work closely with our Data Science team. In this role you will take ownership of our event based architecture and data warehousing. You will also need to assess and devise an effective end to end data infrastructure. The ideal candidate will be comfortable with Amazon Redshift, ETLs, and MongoDB.\\n\\nWHAT YOU'LL DO\\nBuild scalable data infrastructure solutions.\\nDesign and optimize new and existing data pipelines.\\nIntegrate new data sources into our existing data architecture.\\nCollaborate with a cross-functional team of software engineers and data scientists.\\nWHAT YOU'LL NEED\\nHands-on experience building and maintaining large scale ETL systems.\\nDeep understanding of database design and data structures.\\nFluency in one of the following languages: Python, Java, Scala.\\nExperience working with cloud-based data platforms (we use AWS).\\nSQL and data warehousing skills - able to write clean and efficient queries.\\nAbility to make pragmatic engineering decisions in a short amount of time\\nStrong project management skills; a proven ability to gather and translate requirements from stakeholders across functions and teams into tangible results.\\nWHAT WILL MAKE YOU STAND OUT\\nExperience with tools in our current warehousing stack: Apache Airflow, Redshift, Segment, Kinesis, S3, Looker.\\nFamiliarity with the database technologies we use in production: MongoDB, PostgreSQL\\nComfort with containerization technologies: Docker, Kubernetes, etc.\\nExperience (or interest in learning to) productionizing machine learning models.\\nAt Codecademy, we are committed to teaching people the skills they need to upgrade their careers. Codecademy aims to educate a richly diverse demographic of users with our product and in order to accomplish this, we believe our team should reflect that rich diversity. Our company celebrates diversity in all of its forms- race, gender, color, national origin, marital status, sexuality, religion, veteran status, age, ability, disability status- and works to create an inclusive workplace where people of all backgrounds and beliefs are empowered to better their futures.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We're looking for a developer experienced with data engineering and interested in expanding into new technologies to join our team in New York. You’ll work with our clients to build great products that delight users while using regular investment time to improve yourself, the company, and our community.\\n\\nAbout thoughtbot\\nthoughtbot works with companies in every step of the process to help identify and solve problems. We lead and participate in product design sprints, build high-quality apps, and then deploy them. We use emerging and effective technologies and methods on both internal and client projects. We believe there is always a better way to do our work, and we want to find it and share it with as many people as possible.\\nAdditionally, we maintain an inclusive work environment where everyone can thrive professionally, as well as have full lives outside of work. thoughtbot does not discriminate on the basis of race, sex, color, religion, age, national origin, marital status, disability, veteran status, genetic information, sexual orientation, gender identity, or any other reason prohibited by law in provision of employment opportunities and benefits. We welcome you to apply and let us know if you need any reasonable accommodations during the interview process.\\nWant to dig deeper? Read more about our Purpose and Values, how we work in our Playbook, or check out this video to hear from our team.\\nRequirements\\nthoughtbot data engineers are able to build high-quality, high-throughput data pipelines. Well-qualified candidates have an excellent knowledge of data engineering, including streaming data, distributed data processing, and big data. Experience with Scala and common data stores like Kafka, Cassandra, and ElasticSearch are a plus. Data pipelines will power user interfaces written in Ruby on Rails, Django, and React. Interest and familiarity with those languages is a plus, as is knowledge of building APIs using GraphQL. Being able to contribute to both sides of the API is a huge plus.\\nBenefits\\nOur team works in a relaxed and educational environment to develop excellent products for our clients. We work at a sustainable pace of 40 hours per week, consulting for clients four days each week. We dedicate our non-client time to improving ourselves, our communities, and thoughtbot. Everything we do is predicated on having a great team and a culture of growing. We use the latest technologies and are willing to try new methods on both internal and client projects.\\nINVESTMENT DAYS\\nWe have an investment day each Friday where we learn new tools and techniques, work on open source, create new products, write blog posts, and try to make ourselves, each other, and the community better. If you’ve used our open source libraries, read our blog, attended the local events we host, or seen us speak at conferences then you’ve seen the fruits of investment time.\\nALWAYS LEARNING\\nWe have a culture of continuous improvement. Investment days are a critical component in this, but we also offer training and conference benefits. We will cover 100% of all expenses incurred when you speak at a conference and a minimum of 50% of your expenses for any conference or training you attend.\\nYou’ll also enjoy working with and learning from a diverse set of teammates across your client projects and during investment time. We dedicate time each week towards working collaboratively on exciting investment projects, wrapping the year up with a two-day event which our teammates often use to build brand new products in exciting new technologies.\\nSTAY FRESH\\nWe offer 25 paid vacation days and 11 paid holidays per year in addition to 10 paid sick days. New parents receive at least 6 weeks paid parental leave, as well as the ability to take up to 6 months off.\\nHEALTHCARE + FINANCIAL\\nWe offer a competitive salary and excellent benefits. We pay 100% of medical, dental, vision, and life insurance premiums for full time employees and 90% of medical premiums for dependents. We also offer a comprehensive 401k plan.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Title             Location       City State  \\\n",
       "0    Data Engineer                    New York, NY         New York   NY     \n",
       "1    Senior Data Engineer – Datasets  New York, NY 10011   New York   NY     \n",
       "2    Director of Data Science         New York, NY 10005   New York   NY     \n",
       "3    Senior Data Engineer             New York, NY 10017   New York   NY     \n",
       "4    Senior Software Engineer - Data  New York, NY         New York   NY     \n",
       "..                               ...           ...              ...   ..     \n",
       "203  Cyber Data Engineer              Manhattan, NY 10007  Manhattan  NY     \n",
       "204  Data Analyst                     New York, NY         New York   NY     \n",
       "205  Lead Data Engineer               New York, NY         New York   NY     \n",
       "206  Senior Data Engineer             New York, NY         New York   NY     \n",
       "207  Data Engineer                    New York, NY         New York   NY     \n",
       "\n",
       "            Zip     Country  \\\n",
       "0    None Found  None Found   \n",
       "1    10011       None Found   \n",
       "2    10005       None Found   \n",
       "3    10017       None Found   \n",
       "4    None Found  None Found   \n",
       "..          ...         ...   \n",
       "203  10007       None Found   \n",
       "204  None Found  None Found   \n",
       "205  None Found  None Found   \n",
       "206  None Found  None Found   \n",
       "207  None Found  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Qualifications  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "4    \\nMaster's or bachelor's degree in Computer Science\\nAt least 10 years of hands-on software development experience in Python, Golang, Java, C++ or Scala\\nStrong Object Oriented Programming skills\\nDeep knowledge in data structures, algorithms, and software design\\nExperience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources\\nHighly proficient with relational and non-relational data storages\\nStrong verbal and written communication skills   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...   \n",
       "203  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "204  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "205  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "206  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "207  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "         Skills  \\\n",
       "0    None Found   \n",
       "1    None Found   \n",
       "2    None Found   \n",
       "3    None Found   \n",
       "4    None Found   \n",
       "..          ...   \n",
       "203  None Found   \n",
       "204  None Found   \n",
       "205  None Found   \n",
       "206  None Found   \n",
       "207  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Responsibilities  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4    \\nWork with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs\\nAdheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time\\nAble to define new architectures and improve existing ones\\nCan be the central focus for code reviews, architecture discussion and bug fixing\\nDemonstrates code and product ownership in production\\nSupport the business teams and product managers in data extracts and data analysis\\nPerforms as a true agile team leader and exhibits competencies in all layers of the application stack\\nDemonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...   \n",
       "203  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "204  \\nTransform raw data into actionable and meaningful analyses.\\nWork with a cross-functional team to define and clarify analytic goals, develop analysis plans, and ensure data quality standards.\\nIdentify key metrics and develop dashboards/reports to better the understanding of member marketing solutions.\\nWork closely with our data engineer(s) to seamlessly integrate data across audience management platforms and integrate learnings with member profiles for actionable next steps.\\nHelp measure and compare the success of marketing metrics across campaigns.\\nDocument data requirements and think through complex problems to make actionable recommendations.\\nPerform ad-hoc reporting and analyses as needed.                                                                                                                                        \n",
       "205  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "206  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "207  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "      Education  \\\n",
       "0    None Found   \n",
       "1    None Found   \n",
       "2    None Found   \n",
       "3    None Found   \n",
       "4    None Found   \n",
       "..          ...   \n",
       "203  None Found   \n",
       "204  None Found   \n",
       "205  None Found   \n",
       "206  None Found   \n",
       "207  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Requirement  \\\n",
       "0    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "2    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "3    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "4    None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "..          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "203  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "204  \\nBachelor or Master's degree in Economics, Statistics, Finance, or Mathematics preferred.\\n3-5 years of analytics or quantitative experience in which you extracted meaningful insights from big datasets.\\nExperience with test design, KPI development and post campaign analyses.\\nExperience working in a company with a subscription based business.\\nStrong experience with using BI tools to build dynamic/scalable dashboards. Looker experience a plus.\\nStrong Knowledge of SQL, your queries are optimized for readability and performance. Google BigQuery a plus.\\nStrong conceptual/analytical/creative thinker – ability to push beyond specific requests and identify the business need in advance of execution.\\nAbility to handle multiple tasks, good time management and organizational skills.\\nExcellent communication and organizational skills.   \n",
       "205  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "206  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "207  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       FullDescriptions  \n",
       "0    -------------\\nAbout Teampay\\n-------------\\n\\nTeampay is the first purchasing software built for fast-growing, technology-enabled businesses. The way companies spend money has changed, rendering expense management tools obsolete. Our products empower businesses to request, approve and track spending in real-time.\\n\\nEvery day, more and more finance teams rely on Teampay to scale company purchasing. Led by an experienced team with prior exits, Teampay has raised double-digit millions of capital from prominent venture investors including CrossCut Ventures, Tribe Capital, Precursor Ventures, and CoVenture.\\n\\nWe're an agile team building software that's revolutionizing how companies spend their money. Joining Teampay at this early stage is an opportunity to grow your skill set, build a company, and get paid to do it.\\n\\n--------------\\nAbout the role\\n--------------\\n\\nAs a data engineer at Teampay, you will architect a data infrastructure that is stable and scalable to support data analytics, reporting, and visualization. Your input and contribution will have a direct impact on our data strategy and technology roadmap.\\n\\nWhat you'll do...\\n\\n\\nCombine and analyze data from various sources to help drive business insights\\nWork with various team and executive stakeholders to define mission-critical metrics and key performance indicators\\nDevelop and maintain a scalable data infrastructure\\nDevelop tools supporting self-service data pipeline management\\nWork closely with teams on design and implementation of data solutions\\nOwn and provide BI development tools for internal use\\n\\nCommon Candidate Qualifications include…\\n\\n\\n3+ years functional experience in a data engineering or business intelligence role\\nProficient in data modeling and systems design skills\\nProficient in SQL\\nProficient in at least one scripting language (ideally Python)\\nExperience building and maintaining robust ETL pipelines\\nExperience with version control systems\\nExperience with BI platforms\\n\\nNice to Have...\\n\\n\\nExperience with AWS\\nMachine learning experience\\nYou enjoy telling a compelling story with data\\n\\nYou would be the first Data Engineer in a seed stage, fast-growing company. As a core member of the engineering team, you will have a strong impact on the future of Teampay. This is an unusual opportunity to join a business with traction at the ground floor.\\n\\n----------------------\\nApply at Teampay if...\\n----------------------\\n\\nYou're a builder. You're passionate about crafting things that matter. You're curious and agile in thought and action. You value authenticity and possess a strong work ethic. You're empathetic and look forward to learning from people unlike yourself. You want to make an impact with a strong team. You look for challenges that force you to grow. You rarely miss a detail and always learn from your mistakes. You have diverse interests outside of work, but are ready to pitch in and be responsive when the pressure is on.\\n\\n-----------\\nInterested?\\n-----------\\n\\nYou can learn more about the product at www.teampay.co ( http://www.teampay.co/ ) and if you'd like to apply, please include a cover note and tell us about something you started, whether it's a club, a team, business or blog.\\n\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "1    Data and information has become an invaluable asset for technology companies that focus on innovation and are dedicated to providing a great user experience. Spotify is taking this concept to heart and driving a “Data as a Product” initiative for its own data consumers within the company. The volume and breadth of data at Spotify is staggering – billions of records of streamed music, app interactions, artist information and user behavior trends flow through our platform on a daily basis. The Vivaldi squad and it’s Tribe are at the center of this initiative. We design and implement new ways and strategies to empower the Spotify community of data scientists, ML, researchers, product designers, fraud investigators, business analysts and the CEO himself.\\n\\nWhat you will do\\nUnderstanding what fuels many of Spotify’s product features such as Discover Weekly, Daily Mix, Podcast offerings, holiday campaigns and others\\nWorking hand-in-hand with the data science community to understand various user or content trends that influence product changes and customer acquisition strategies\\nGetting hands-on experience with Google Cloud Platform and technology/languages such as BigQuery, Scala, Scio, Luigi, Styx and Docker\\nCollaboration on a global scale; our squad offers ongoing opportunities to work in Stockholm with other engineering colleagues\\nCross departmental exposure and flexibility to engage with many teams in the company\\nGaining technical expertise in building a data platform at scale to solve business, product and technical use cases\\nWorking in a supportive team that offers engineers the flexibility to be creative and chase interesting ideas\\nWork closely with cross-functional teams of data and backend engineers, analysts, user researchers, product managers and designers\\nCommunicate insights and recommendations to key stakeholders, engineering and product partners\\n… and of course, having fun! Being passionate about what you do also means celebrating milestones within the team and the tribe!\\nWho you are\\nAn BS/MS in CS or any other relevant fields of study\\nAt least 3 years of work experience in the Data Engineering and Big Data field\\nStrong analytical and problem solving ability\\nCoding skills for analytics and data engineering/manipulation (Scala, Java and Python)\\nStrong communication and data presentation skills (such as Tableau, PowerPoint, Qlik, etc.)\\nExperience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Google’s Cloud Platform\\nYou are capable of tackling very loosely defined problems and thrive when working on a team which has autonomy in their day to day decisions\\nYou are a communicative person that values building strong relationships with colleagues and multiple stakeholders, and have the ability to explain complex topics in simple terms\\nIdeally you have experience working in a large scale, global consumer product company, in an engineering or insights role                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "2    DIRECTOR OF DATA SCIENCE\\n\\nWho We Are\\n\\nMurmuration transforms how political campaigns, advocates, and organizers identify, engage, and mobilize people and communities. Our focus is on driving change and accelerating progress toward a future where every child in America has the opportunity to benefit from a high-quality public education. Our partners include the leading practitioners and funders of efforts to ensure access for all children to a high-quality public education.\\n\\nWhat We Do\\n\\nAt Murmuration, we provide our partners with the skills, knowledge, and tools they need to drive sustained political change—at all levels of government and aspects of civic life. Through the use of predictive intelligence and easy-to-use tools, Murmuration’s partners make informed decisions about who they need to reach, what they need to say, and how to achieve and sustain impact. Every application of this work, as well as shared knowledge and best practices, further improves our collective ability to organize communities and electoral campaigns.\\n\\nMurmuration offers a fully featured political campaign platform that allows our partners to execute large scale electoral, advocacy and organizing efforts across the United States. The m{insights toolset includes a web platform that sits on top of an aggregated dataset from a wide variety of sources, including publicly available data, consumer data, voter-file data, and membership data furnished by Murmuration’s partners. Partners are also offered sophisticated targeting and outreach tools to help activate key audiences and expand their base of support. Murmuration’s data science team also provides deep analysis on our partners’ work, helping craft experiments and polls, building predictive models and measuring impact.\\n\\nAbout the Position\\n\\nThe Director of Data Science will lead a team of analysts and data scientists charged with managing a suite of analytics products that will position our partners for greater impact. The team’s current main projects include: building machine-learning models to predict voter opinions and behavior, running national scale polls for our sector and partner-specific polls on request, and constructing large scale randomized control trials to determine the effectiveness of various outreach methodologies in local and state elections. The Director will also have the remit and be encouraged to expand the scope of analytic offerings for our partners.\\n\\nThe Analytics team is a highly collaborative, friendly, and hard-working group, and we are looking for a Director that embodies those values. The Director of Data Science will report to the Chief Data Scientist and work in close tandem with the Director of Data Management and our Lead Data Engineer.\\n\\nThe Director will:\\n\\nBe an excellent personnel manager, with a track record of providing constructive feedback to direct reports and of helping employees develop their skills and careers\\nHelp manage and coordinate Murmuration’s day-to-day analytics projects, including managing the resources of the Analytics team and planning for scalable growth in order to provide continued high-quality analytics and data science to our partners\\nInteract with our partner organizations to understand their analytic needs, in order to better determine the course of our R&D work\\nCollaborate with the Chief Data Scientist, to define future research priorities and R&D efforts that streamline the production of our existing analytics projects\\nLiaise with the VP of Partnerships to determine project delivery timelines and determine the staffing needs and assignments for partner engagements\\nIdentify staffing needs and collaborate with HR to hire team members as we grow\\nProvide input and recommendations on the future composition and remit of the Analytics team\\nCandidate Profile\\n\\nMurmuration attracts employees with distinctive and diverse backgrounds and accomplishments. Integrity, creativity, flexibility, and drive are key attributes of competitive candidates.\\n\\nThe ideal candidate will have most of the following qualities:\\n\\nExperience managing an analytics team, with a track record of achieving project goals and meeting deadlines while fostering a healthy team environment\\nFamiliarity with modern analytics techniques in statistics and machine learning; it is ideal but not required to be an experienced data scientist, however experience managing data scientists is required\\nStrong problem solving skills as well as the ability to manage several tasks/projects concurrently and prioritize work effectively\\nBe an exceptional team player, with strong interpersonal skills\\nStrong communication skills to interact effectively with various internal and external stakeholders to develop analytics strategy and deepen our partnerships\\n\\nLocation, Compensation and Benefits\\n\\nThe Director of Data Science is a full-time, salaried position based onsite in New York City, with a comprehensive benefits package. Salary for this position is commensurate with experience.\\n\\nAn Equal-Opportunity Employer with a Commitment to Diversity\\n\\nMurmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.  \n",
       "3    Senior Data Engineer\\nNew York, NY\\nAre you an experienced Data Engineer who designs and implements innovative solutions that leverage modern tools and technologies?\\nDo you share our passion for enabling positive change within healthcare and helping patients with chronic conditions like diabetes?\\nIf so, you could be a perfect fit for our team of like-minded professionals who share a common mission and passion for helping others and a desire to build a great company. Come and work with us to build our next generation healthcare platform!\\nCecilia Health is a high-growth, venture-backed healthcare company based in New York City. We partner with pharmaceutical & device companies, payers and ACOs to deliver personalized, technology-enabled coaching to improve treatment, adherence and health outcomes for people living with diabetes and other chronic conditions. Cecelia Health is a high-energy, results-oriented work place that believes our success, as well as the success of our customers and patients, relies primarily on a fantastic team with the passion, drive and skills to change the face of chronic condition management.\\nWe are hiring a Senior Data Engineer in New York City. This role will report to our Chief Technology Officer and join a strong technology team that is continuously innovating our current solutions that allow clinicians to efficiently serve an increasing volume of patients.\\nWHO YOU ARE\\nYou have data in your DNA and at least 5 years of relevant work experience in data engineering requiring application of analytic skills to integrate data into business operations. You are an expert in using modern technology stacks to build intelligent ETL and data processing pipelines. You've worked in a variety of data wrangling roles and have strong knowledge of data manipulation using advanced SQL and other tools. You have a solid understanding of engineering best practices, as well as familiarity with scaling DevOps and engineering initiatives. You want to help develop software platforms that have real-world impact on people with chronic diseases like diabetes.\\nPOSITION RESPONSIBILITIES\\nDefine and lead data lifecycle strategy across data acquisition, data ingestion, data cleansing, normalization and linkage\\nApply various techniques to produce solutions to large-scale optimization problems, including data pre-processing, indexing, blocking, field and record comparison and classification\\nImprove data sharing, increase data repurposing and improve efficiency associated with data management efforts\\nBuild best practices to support tracking chain of custody of data so it can be easily traced back to the source for accuracy and consistency\\nResponsible for the design, development, and documentation of data models and ETL processes that will feed various databases including a data warehouse/lake and various analytics/data science components\\nSupport and enhance existing applications and automation processes on a daily basis using a combination of SSIS, C#, .NET Core, Python, PowerShell, SQL Server, and Snowflake databases\\nEnsure production service levels, performance quality, and resolution of data load failures\\nManage multiple projects independently\\nEvaluate and recommend tools, technologies and processes to ensure the highest quality product platform\\nQUALIFICATIONS\\nRequired Experience\\nBachelor's degree in information science, computer science, engineering or a similar area, with 5+ years of related experience, or comparable real-world development experience\\nMaster Data Management experience including data consolidation, linkage, federation and dissemination\\nStrong knowledge of Python for Data Engineering\\nAdvanced SQL experience (Nested Queries, Complex Joins, Analytic Functions, Time Series)\\nExperienced working in Agile/Dev Operations environment with continuous integration and continuous deployment and application lifecycle management\\nStrong communication skills\\nDesired Experience\\nData integration, application development and secure information management in healthcare, life sciences, or clinical research\\nExperience with scalable, enterprise-level development on virtualized (AWS, Azure, GCP) infrastructure\\nExperience with a cloud-based data warehouse such as Snowflake or Redshift\\nFamiliarity with analytics and business intelligence tools (Tableau, Power BI, Cognos).\\nExperience with Real Time processing and Big Data tools (Hive, Spark, Hadoop, HDFS, Kafka, Lucene, Glue, Airflow…)\\nD9aAMpL07D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "4    Lucid is a market research platform that provides access to authentic, first-party data in over 90 countries. Our products and services enable anyone, in any industry, to ask questions of targeted audiences and find the answers they need – fast. These answers can be used to uncover consumer motivations, increase revenue, and measure the impact of digital advertising. Founded in 2010, Lucid is headquartered in New Orleans, LA with offices in Dallas, New York, London, Sydney, Singapore, Gurgaon, Prague, and Hamburg.\\n\\n\\nApply for this Job\\n\\nThe Opportunity\\n\\nThe Senior Data Engineer will be a key part of our development team and work closely with development team peers, Product Management team, and business and other support teams.\\nResponsibilities\\nWork with the team - Tech and Product Managers executing the product backlog, taking part of its creation and grooming, and understanding the stakeholders needs\\nAdheres to the best practices of software engineering (testing, integration, clean design and concern separation) and helps improve those practices over time\\nAble to define new architectures and improve existing ones\\nCan be the central focus for code reviews, architecture discussion and bug fixing\\nDemonstrates code and product ownership in production\\nSupport the business teams and product managers in data extracts and data analysis\\nPerforms as a true agile team leader and exhibits competencies in all layers of the application stack\\nDemonstrate proficiency in developing software for user interface, business logic, data modeling and systems and component integration\\nQualifications\\nMaster's or bachelor's degree in Computer Science\\nAt least 10 years of hands-on software development experience in Python, Golang, Java, C++ or Scala\\nStrong Object Oriented Programming skills\\nDeep knowledge in data structures, algorithms, and software design\\nExperience with high volume and high performance applications dealing with large amounts of structured and unstructured data from multiple sources\\nHighly proficient with relational and non-relational data storages\\nStrong verbal and written communication skills\\nPreferred Qualifications\\nAWS experience with S3, RDS, Redshift, EMR, Kinesis, Lambda, Elastic Beanstalk and Elasticsearch\\nExperience with Spark, Hadoop and Hive on EMR and non-managed. Can build and run a basic cluster\\nExperience developing ETLs and running job schedulers (e.g. Airflow)\\nExperience with PostgreSQL\\nExperience with NoSQL data sources including Cassandra\\nExperience with Kafka, Redis\\nExperience creating Data Lakes\\n\\n\\nAt Lucid we foster a collaborative and inspiring workplace. We pride ourselves in doing this by recruiting, hiring and retaining diverse, passionate, and forward-thinking talent. Lucid is committed to and encourages an inclusive environment and we are dedicated to providing equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please let us know.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "203  About New York City Cyber Command\\nNYC Cyber Command was created in 2017 by Executive Order to lead the City’s cyber defense efforts, working across more than 100 agencies and offices to prevent, detect, respond, and recover from cyber threats. NYC Cyber Command is committed to protecting NYC infrastructure and critical systems from malicious attacks through the use of the latest technologies, public-private partnerships, and regular training and exercises for City employees.\\n\\nJob Description\\nData Engineers support the Cybersecurity Data Science team, led by the Cybersecurity Lead Data Scientist;Data Science team analyzes data in order to defend against cyber threats to the City. This team is integral to the defense of the City’s information environment, and works directly with the NYC3 Architecture, Engineering, and Threat Management teams;Data Engineers design, build, implement, and operate systems that ingest, normalize, correlate, analyze, and present cybersecurity relevant data from across the City.\\n\\n\\nMinimum Qual Requirements\\n\\n1. A baccalaureate degree, from an accredited college including or supplemented by twenty-four (24) semester credits in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or\\n\\n2. A four-year high school diploma or its equivalent approved by a State’s department of education or a recognized accrediting organization and three years of satisfactory experience in any of the areas described in “1” above; or\\n\\n3. Education and/or experience equivalent to “1” or “2”, above. College education may be substituted for up to two years of the required experience in “2” above on the basis that sixty (60) semester credits from an accredited college is equated to one year of experience. In addition, twenty-four (24) credits from an accredited college or graduate school in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or a certificate of at least 625 hours in computer programming from an accredited technical school (post high school), may be substituted for one year of experience.\\n\\n\\nPreferred Skills\\n\\nThe preferred candidate should possess the following:\\nData Engineers have at least 1 year experience (including internships) analyzing large, high velocity, heterogeneous datasets. They are proficient in using cloud-based tools for ingesting, normalizing, analyzing, and presenting data for future engineering solutions and executive decision makers;Possess at least bachelor’s degrees in computer science or information systems and have specializations in mathematics, number theory, applied cryptography, or statistics or relevant experience;Should have strong practical knowledge of data structures, Java, relational database design, and familiarity with system level and distributed programming;Familiarity with Unix scripting, Web development, and automated testing would be highly desirable, but not necessary.\\n\\n\\nTo Apply\\n\\nInterested applicants with other civil service titles who meet the preferred requirements should also submit a resume for consideration\\nFor City employees, please go to Employee Self Service (ESS), click on Recruiting Activities > Careers, and search for Job ID #407411\\nFor all other applicants, please go to www.nyc.gov/jobs/search and search for Job ID #407411\\n\\nSUBMISSION OF A RESUME IS NOT A GUARANTEE THAT YOU WILL RECEIVE AN INTERVIEW\\nAPPOINTMENTS ARE SUBJECT TO OVERSIGHT APPROVAL\\n\\nThe Department of Information Technology & Telecommunications and the City of New York are equal opportunity employers.\\n\\nDoITT participates in E-Verify\\n\\n\\nHours/Shift\\n\\nDay - Due to the necessary technical support duties of this position in a 24/7 operation, candidate may be required to work various shifts such as weekends and/or nights/evenings.\\n\\n\\nResidency Requirement\\n\\nNew York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "204  Data Analyst\\nCompany Overview:\\n\\nWW is looking for candidates to help change people’s lives. We are a global wellness technology company inspiring millions of people to adopt healthy habits for real life. We do this through engaging digital experiences, face-to-face workshops and sustainable programs that encourage people to move more, shift their mindset and eat healthier while enjoying the foods they love. By drawing on over five decades of experience and expertise in behavioral science, we build communities in order to deliver wellness for all.\\n\\nTo learn more about WW and jobs with a purpose, visit ww.com.\\n\\nRole Overview:\\nWe are now looking for a talented Data Analyst to join our growing Analytics group to help drive a data-first culture across WW.\\nThe data analyst will work closely with both business and technical teams to proactively identify gaps in our knowledge base across member marketing. The candidate will be charged with providing analytics reporting and data-driven strategic insights, trends, and perspective.\\nYou will work closely with a cross functional team through all development stages, from identifying opportunities, to analyzing the impact of efforts, and discovering areas for improvement.\\nKey Responsibilities:\\nTransform raw data into actionable and meaningful analyses.\\nWork with a cross-functional team to define and clarify analytic goals, develop analysis plans, and ensure data quality standards.\\nIdentify key metrics and develop dashboards/reports to better the understanding of member marketing solutions.\\nWork closely with our data engineer(s) to seamlessly integrate data across audience management platforms and integrate learnings with member profiles for actionable next steps.\\nHelp measure and compare the success of marketing metrics across campaigns.\\nDocument data requirements and think through complex problems to make actionable recommendations.\\nPerform ad-hoc reporting and analyses as needed.\\n\\n\\nGoals / Deliverables:\\n\\nBuild segments in audience management platforms for member marketing teams.\\nDevelop test plans and identify KPIs/targeted audiences for testing across member marketing communications.\\nProvide post test analyses with actionable recommendations and next steps.\\n\\n\\nRequirements:\\nBachelor or Master's degree in Economics, Statistics, Finance, or Mathematics preferred.\\n3-5 years of analytics or quantitative experience in which you extracted meaningful insights from big datasets.\\nExperience with test design, KPI development and post campaign analyses.\\nExperience working in a company with a subscription based business.\\nStrong experience with using BI tools to build dynamic/scalable dashboards. Looker experience a plus.\\nStrong Knowledge of SQL, your queries are optimized for readability and performance. Google BigQuery a plus.\\nStrong conceptual/analytical/creative thinker – ability to push beyond specific requests and identify the business need in advance of execution.\\nAbility to handle multiple tasks, good time management and organizational skills.\\nExcellent communication and organizational skills.\\n\\n\\nAs a company, our purpose is to inspire healthy habits for real life. And as an employer, we inspire the greatest people to do their best work. We provide benefits for real life to help protect your health, finances and overall well-being, including:\\nCompetitive compensation and profit-sharing plan\\nA 401K plan to help you plan for your future, plus company match\\nHealth care coverage starting on your first day\\nTuition reimbursement and online courses to help you reach your career aspirations\\nCommuter benefits\\nYearly well-being allowance for your physical, financial, social and emotional well-being\\nFree WW membership for you plus 3 free WW memberships for your friends and 3 for your family\\nFree fruit, snacks and coffee to get you through your day\\nSummer Fridays, happy hours, and company outings\\nRobust employee referral bonuses\\nDevelopmental opportunities and assignments to grow your career\\nWW is an equal opportunity employer. WW does not discriminate on the basis of sex, race, color, creed, national origin, marital status, age, religion, sexual orientation, gender identity, gender expression, veteran status, or disability.\\nAny offer of employment is contingent upon the satisfactory results of reference and background checks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "205  X-Mode provides real-time location data and technologies that power location intelligence for advertising and business decisions in financial services, healthcare, high-tech, real-estate, retail, and the public sector. X-Mode's flagship product is a fast-growing big data location platform, which maps daily the precise routes of 10% of the U.S. Population and maps monthly 1 in 3 adult U.S. smartphone users. X-Mode strives to produce and monetize the world's largest location platform and ultimately create a global \"living map\" of 1 billion people with the highest quality location data in order to fuel the best location intelligence business solutions.\\n\\nX-Mode Social, Inc. is looking for a full-time lead data engineer to work on X-Mode's data platform and join our rapidly growing team. For this position, you can work in either our Reston, VA headquarters or remotely. Our technical staff is scattered across the U.S, so you'll need to be comfortable working remotely. We often use videoconferencing tools (like Slack, Google Meet) to coordinate, as well as Jira for tasking, and Bitbucket for source control. We work in short sprints, and we'll count on you to provide estimates for tasks to be completed and delivered. We're looking to hire someone to start right away! Think you've got what it takes? Apply below!\\n\\nWHAT YOU'LL DO:\\n---------------\\n\\n\\nUse big data technologies, processing frameworks, and platforms to solve complex problems related to location\\nBuild, improve, and maintain data pipelines that ingest billions of data points on a daily basis\\nEfficiently query data and provide data sets to help Sales and Client Success teams' with any data evaluation requests\\nEnsure high data quality through analysis, testing, and usage of machine learning algorithms\\n\\nWHO YOU ARE:\\n------------\\n\\n\\n3-5+ years of Spark and Scala experience\\nExperience working with very large databases and batch processing datasets with hundreds of millions of records\\nExperience with Hadoop ecosystem, e.g. Spark, Hive, or Presto/Athena\\nReal-time streaming with Kinesis, Kafka or similar libraries\\n4+ years working with SQL and relational databases\\n3+ years working in Amazon Web Services (AWS)\\nA self-motivated learner who is willing to self-teach\\nWilling to mentor junior developers\\nSelf-starter who can maintain a team-centered outlook\\nBONUS: Experience with Python, Machine Learning\\nBONUS: GIS/Geospatial tools/analysis and any past experience with geolocation data\\n\\nWHAT WE OFFER:\\n--------------\\n\\n\\nCool people, solving cool problems.\\nCompetitive Salary\\nMedical, Dental and Vision\\n15 Days of PTO (Paid Time Off)\\nWe value your input. This is a chance to get in on the \"ground floor\" of a growing company\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "206  Hello, World! Codecademy has helped over 45 million people from around the world upgrade their careers with engaging, accessible, and flexible education on programming and data skills. We provide over 200 hands-on interactive lessons ranging from Python to R to Javascript and everything in between. Our learners have gone on to start companies, new jobs, and new lives thanks to what they’ve learned with Codecademy, and we’re thrilled to be working to take that impact to the next level.\\n\\nCodecademy was started in 2011 by two college students in a dorm room at Columbia that were frustrated by the huge gap between education and employment. A few years later, we are a rapidly growing, diverse team of 75+ in SoHo, NYC. We’ve raised over $40m in venture capital funding from top investors including Union Square Ventures, Kleiner Perkins, Naspers, Y Combinator, and more.\\n\\nIf you want to help build a business that impacts tens of millions of people each year and helps them lead better lives, join us!\\n\\nCodecademy's Infrastructure & Services team is responsible for the infrastructure and operations to deliver our service to millions of users learning to code. We are looking for a Senior Data Engineer to join this team; in this role you will also work closely with our Data Science team. In this role you will take ownership of our event based architecture and data warehousing. You will also need to assess and devise an effective end to end data infrastructure. The ideal candidate will be comfortable with Amazon Redshift, ETLs, and MongoDB.\\n\\nWHAT YOU'LL DO\\nBuild scalable data infrastructure solutions.\\nDesign and optimize new and existing data pipelines.\\nIntegrate new data sources into our existing data architecture.\\nCollaborate with a cross-functional team of software engineers and data scientists.\\nWHAT YOU'LL NEED\\nHands-on experience building and maintaining large scale ETL systems.\\nDeep understanding of database design and data structures.\\nFluency in one of the following languages: Python, Java, Scala.\\nExperience working with cloud-based data platforms (we use AWS).\\nSQL and data warehousing skills - able to write clean and efficient queries.\\nAbility to make pragmatic engineering decisions in a short amount of time\\nStrong project management skills; a proven ability to gather and translate requirements from stakeholders across functions and teams into tangible results.\\nWHAT WILL MAKE YOU STAND OUT\\nExperience with tools in our current warehousing stack: Apache Airflow, Redshift, Segment, Kinesis, S3, Looker.\\nFamiliarity with the database technologies we use in production: MongoDB, PostgreSQL\\nComfort with containerization technologies: Docker, Kubernetes, etc.\\nExperience (or interest in learning to) productionizing machine learning models.\\nAt Codecademy, we are committed to teaching people the skills they need to upgrade their careers. Codecademy aims to educate a richly diverse demographic of users with our product and in order to accomplish this, we believe our team should reflect that rich diversity. Our company celebrates diversity in all of its forms- race, gender, color, national origin, marital status, sexuality, religion, veteran status, age, ability, disability status- and works to create an inclusive workplace where people of all backgrounds and beliefs are empowered to better their futures.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "207  We're looking for a developer experienced with data engineering and interested in expanding into new technologies to join our team in New York. You’ll work with our clients to build great products that delight users while using regular investment time to improve yourself, the company, and our community.\\n\\nAbout thoughtbot\\nthoughtbot works with companies in every step of the process to help identify and solve problems. We lead and participate in product design sprints, build high-quality apps, and then deploy them. We use emerging and effective technologies and methods on both internal and client projects. We believe there is always a better way to do our work, and we want to find it and share it with as many people as possible.\\nAdditionally, we maintain an inclusive work environment where everyone can thrive professionally, as well as have full lives outside of work. thoughtbot does not discriminate on the basis of race, sex, color, religion, age, national origin, marital status, disability, veteran status, genetic information, sexual orientation, gender identity, or any other reason prohibited by law in provision of employment opportunities and benefits. We welcome you to apply and let us know if you need any reasonable accommodations during the interview process.\\nWant to dig deeper? Read more about our Purpose and Values, how we work in our Playbook, or check out this video to hear from our team.\\nRequirements\\nthoughtbot data engineers are able to build high-quality, high-throughput data pipelines. Well-qualified candidates have an excellent knowledge of data engineering, including streaming data, distributed data processing, and big data. Experience with Scala and common data stores like Kafka, Cassandra, and ElasticSearch are a plus. Data pipelines will power user interfaces written in Ruby on Rails, Django, and React. Interest and familiarity with those languages is a plus, as is knowledge of building APIs using GraphQL. Being able to contribute to both sides of the API is a huge plus.\\nBenefits\\nOur team works in a relaxed and educational environment to develop excellent products for our clients. We work at a sustainable pace of 40 hours per week, consulting for clients four days each week. We dedicate our non-client time to improving ourselves, our communities, and thoughtbot. Everything we do is predicated on having a great team and a culture of growing. We use the latest technologies and are willing to try new methods on both internal and client projects.\\nINVESTMENT DAYS\\nWe have an investment day each Friday where we learn new tools and techniques, work on open source, create new products, write blog posts, and try to make ourselves, each other, and the community better. If you’ve used our open source libraries, read our blog, attended the local events we host, or seen us speak at conferences then you’ve seen the fruits of investment time.\\nALWAYS LEARNING\\nWe have a culture of continuous improvement. Investment days are a critical component in this, but we also offer training and conference benefits. We will cover 100% of all expenses incurred when you speak at a conference and a minimum of 50% of your expenses for any conference or training you attend.\\nYou’ll also enjoy working with and learning from a diverse set of teammates across your client projects and during investment time. We dedicate time each week towards working collaboratively on exciting investment projects, wrapping the year up with a two-day event which our teammates often use to build brand new products in exciting new technologies.\\nSTAY FRESH\\nWe offer 25 paid vacation days and 11 paid holidays per year in addition to 10 paid sick days. New parents receive at least 6 weeks paid parental leave, as well as the ability to take up to 6 months off.\\nHEALTHCARE + FINANCIAL\\nWe offer a competitive salary and excellent benefits. We pay 100% of medical, dental, vision, and life insurance premiums for full time employees and 90% of medical premiums for dependents. We also offer a comprehensive 401k plan.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "\n",
       "[208 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_NYC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
