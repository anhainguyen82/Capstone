{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests import get\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling all links off of the search pages (up to 3000) and putting them in a dataframe to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template=\"http://www.indeed.com/jobs?q=%22Data+Engineer%22&l=Austin%2C+TX&start={}\"\n",
    "max_results=250\n",
    "Linkdf=[]\n",
    "\n",
    "for start in range(0, max_results, 7):\n",
    "    url=url_template.format(start)\n",
    "    html=requests.get(url)\n",
    "    soup=BeautifulSoup(html.content,'html.parser', from_encoding=\"utf-8\")\n",
    "    \n",
    "    #for each in soup.find_all(a_=\"href\"):\n",
    "    page_links=soup.find_all('a',{'href':re.compile(\"/rc/\")})\n",
    "    for items in page_links:\n",
    "        Linkdf.append(items['href'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "len(Linkdf)\n",
    "#print(Linkdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code allows the code to display the full website instead of truncating\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "\n",
    "#Moving it to a data frame\n",
    "data = {'links':Linkdf}\n",
    "df = pd.DataFrame(data, columns=['links'])\n",
    "\n",
    "#append indeed.com to the front of each\n",
    "df['Web'] = 'https://www.indeed.com'\n",
    "df['URL'] = df.Web.str.cat(df.links)\n",
    "\n",
    "#pull out just a list of the websites.\n",
    "websites=list(df['URL'])\n",
    "\n",
    "#Sanity Check\n",
    "#print(websites)\n",
    "len(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites1=set(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(websites1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping through websites...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Descriptions=[]\n",
    "Location=[]\n",
    "FullDescriptions=[]\n",
    "\n",
    "for url in websites1:\n",
    "    response=get(url)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    description_containers= soup.find(class_='jobsearch-jobDescriptionText')\n",
    "    title_containers=soup.find('h3')\n",
    "    try:\n",
    "        location_containers=soup.find('',{'class':'jobsearch-CompanyInfoWithoutHeaderImage'}).find_all('div')[-1]\n",
    "    except:\n",
    "        location_containers='None Found'\n",
    "    \n",
    "    job_descriptions=str(description_containers)\n",
    "    job_title=str(title_containers.text)\n",
    "    try:\n",
    "        locations=str(location_containers.text)\n",
    "    except AttributeError:\n",
    "        locations = 'None Found'\n",
    "    try:\n",
    "        full_descriptions = str(description_containers.text)\n",
    "    except AttributeError:\n",
    "        full_descriptions= 'None Found'\n",
    "    \n",
    "    Descriptions.append(job_descriptions)\n",
    "    Title.append(job_title)\n",
    "    Location.append(locations)\n",
    "    FullDescriptions.append(full_descriptions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting what we want from the Descriptions Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Location' left in for sanity check. Should be removed once code is confirmed to work\n",
    "Descriptions_df = pd.DataFrame(columns = ['Title', 'Location','City', 'State', 'Zip', 'Country', 'Qualifications', 'Skills', 'Responsibilities', 'Education', 'Requirement', 'FullDescriptions'])\n",
    "Country = ['US', 'USA', 'United States', 'United States of Americal']\n",
    "States = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
    "          'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND',\n",
    "          'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']\n",
    "\n",
    "for index, element in enumerate(Descriptions):\n",
    "    soup=BeautifulSoup(element,'lxml')\n",
    "    for values in list(Descriptions_df):\n",
    "        temp_tag = soup.find('b', text=re.compile(values))\n",
    "        try:\n",
    "            ul_tag = temp_tag.find_next('ul')\n",
    "            Descriptions_df.at[index,values] = ul_tag.text\n",
    "        except AttributeError:\n",
    "            Descriptions_df.at[index,values]=\"None Found\"\n",
    "        Descriptions_df.at[index,\"Title\"]=Title[index]\n",
    "        Descriptions_df.at[index,\"Location\"]=Location[index]\n",
    "        Descriptions_df.at[index,\"FullDescriptions\"]=FullDescriptions[index]\n",
    "        words = '|'.join(Country)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Country\"] = temp[0]\n",
    "        words = '|'.join(States)\n",
    "        temp = re.findall(words, Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"State\"] = temp[0]\n",
    "        temp = re.findall(r'\\d+', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"Zip\"] = temp[0]  \n",
    "            \n",
    "        temp = re.findall(r'[\\w w]+,', Location[index])\n",
    "        if len(temp) != 0:\n",
    "            Descriptions_df.at[index,\"City\"] = re.sub(',', '', temp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Country</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Engineer – Elastic Engineer</td>\n",
       "      <td>Austin, TX 73344</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>73344</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Introduction\\nAt IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.\\n\\nYour Role and Responsibilities\\nWe are looking for a Data engineer to deploy complex big data and analytics solutions using Elastic stack. You should have strong experience in deploying and managing Elastic cluster on Kubernetes in multi-site, multi cluster environment in both on-Premise as well as Cloud platforms. You should have applied or expert knowledge in big data platforms. The main use case for such a platform for us would be Real-Time Anomaly Detection and Time Series models on IT operations data like logs, metrics, events, wired data, transaction flow, ITIL process related data, knowledge repositories, etc\\nBusiness Unit/ Team Overview\\nGlobal Technology Services (GTS) at IBM manages the IT infrastructure for some of the world’s leading corporations and with that comes the responsibility of managing enormous amounts of IT data and the opportunity for making better decisions using that data. In GTS analytics team at IBM, Data Scientists, Data Engineers, and BigData IT Architects are developing novel models, cutting edge algorithms, and custom analytics solutions to tackle BigData challenges in the IT Infrastructure space.\\nITOA / AIops provides real time machine-data (log, events, performance, capacity, ITIL data, wire data, etc) analytics solutions that helps customers manage Business Services and manage the quality of the end-user experience\\nIt can tell a client in real time ‘What happened’, ‘Why did it happen’, ‘Will it happen again’ and ‘What to do if it happens again? etc\\nKeeps everyone on the same page by looking at the same Business Transaction data and metrics.\\nKeeps the focus on operational data that translate to the business value the application delivers; dive in deeper when appropriate.\\nIdentify resolution criteria, assign ownership\\nTake lessons learned to improve development, test, deployment, and production processes\\nEducation &amp; Experience\\nMinimum 4 years of relevant experience working on the Elastic based products &amp; distributions specifically used in Real Time ITOA or AIops use-cases processing logs, metrics, events, etc\\nAt least 4 years of experience in development &amp; implementation of logging and metrics solutions in with TB+ / day ingestion per day\\nAt least 5 years of hands-on experience in IT support (Infrastructure / Application) and IT monitoring tools\\nOverall 7+ years of core Big data / Analytics experience in various domains\\nDegree / Master’s degree in computers or equivalent\\nCertifications:\\nElastic certified engineer\\nCertifications showing proficiency in the Usage, design &amp; deployment of ITOA / AIOps solutions like Elastic or Splunk\\nSpecialized certifications on specific technologies like Hadoop, Cloudera, Spark, Kafka, etc\\nJob Responsibilities\\nDeploy Elastic stack cluster on native kubernetes or Kubernetes services and maintain the clusters efficiently\\nLeading end to end deployment of ITOA / AIOps solutions for enterprise customers\\nProvide engineering inputs to Architects and Data scientists on various stages of solution design\\nPerform Integration and deployment of ITOA solution as per design provided by Architects\\nParticipate &amp; be an active member of internal capability building projects\\nTrain &amp; support junior resources as needed\\nProvide resolution to customer queries and issues\\nSkills Required:\\nExcellent knowledge on log analytics, time series data anomaly detection and correlation of events\\nHands-on experience with IT operational data like logs, metrics, events, RDBMS tables, etc and ingesting them into Elastic stack\\nExpert Knowledge on GO/grok/REGEX/Logstash/Fluentd to perform Extract, Transform and Load for IT operational data into big data repositories like Elasticsearch, Cassandra, Hadoop, etc\\nExpert level experience in managing large Elastic cluster and in-depth knowledge in Elastic features\\nAlerting\\nSecurity\\nCurator\\nReporting\\nMonitoring\\nBackup and resiliency\\nKubernetes cluster management\\nPython/R/Scala languages/Scripting Languages in context of Anomaly Detection &amp; Time Series modelling\\nWorking experience with ITIL Framework\\nWorking knowledge on Apache Hadoop, Spark, Airflow, Cassandra and Kafka ecosystem\\nPrior experience in deploying Elastic solutions in production environments processing operational data in terms of at least 500 GB / day\\nExperience with SQL based tools &amp; expertise on any one traditional RDBMS – mySQl; MSSQL;Oracle;DB2 etc\\nPrior experience with DevOps projects, Github, Jira, Travis, etc\\nWorking knowledge on Windows, Linux and AIX platforms\\nWorking knowledge of Top commercial distributions of the above stack – MapR; Cloudera; Hortonworks etc\\nKnowledge on shell scripting\\nGood knowledge on other Top level Apache Big Data technologies like Cassandra, NIFI, Fluentd, Drill, Sentry etc\\nExcellent understanding on HDFS &amp; other similar Map/Reduce paradigms\\nKnowledge on 1-2 NoSQL databases – Redis; MongoDB;Cassandra;Neo4j; VoltDB etc\\nPreferred:\\nExperience with Elastic ML and real-time operations analytics using Apache suit of products like Spark using Python or Scala\\nExperience with Kibana plug-in development and other UI development\\nExperience working with large data sets leveraging distributed systems e.g. Spark/Hadoop.\\nTools &amp; Methods (Experience in at least one in each category or similar if not listed below)\\nLog Analytics – Elastic Search, Apache Solr\\nData Pipelines: Logstash, Fluentd, Kafka, Nifi\\nLanguages: Python, PySpark, Spark, Scala, R, Java, Java Script\\nVisualization : Kibana, Tableau, Cognos\\nMachine learning – Elastic ML, Python, Spark, Tensorflow, H2O\\nStreaming: Spark; Storm;\\nRelational Database technologies: Oracle, Db2, SQL, MySQL,\\nNoSQl DB’s: MongoDB, Cassandra, Neo4J,Redis, VoltDB, CouchDB\\nApache Hadoop Distribution – Apache, Hortonworks, Cloudera, MapR\\nETL technologies: Datastage, Informatica, Pentaho DI, SAS DI, SSIS or R, Python based Data munging\\nCloud technologies: AWS, Azure, IBM Softlayer\\nSoft Skills\\nExcellent Written &amp; Verbal Communication\\nExcellent Analytical &amp; Virtual troubleshooting skills\\nSkills to work in team and collaborative environment\\nCustomer/Vendor interaction &amp; co-ordinations\\n\\nRequired Technical and Professional Expertise\\n7+ years of professional hands on experience in IT operations\\nExcellent knowledge on log analytics, time series data anomaly detection and correlation of events\\nHands-on experience with IT operational data like logs, metrics, events, RDBMS tables, etc and ingesting them into Elastic stack\\nExpert Knowledge on GO/grok/REGEX/Logstash/Fluentd to perform Extract, Transform and Load for IT operational data into big data repositories like Elasticsearch, Cassandra, Hadoop, etc\\nExpert level experience in managing large Elastic cluster and in-depth knowledge in Elastic features\\nPython/R/Scala languages/Scripting Languages in context of Anomaly Detection &amp; Time Series modelling\\nWorking experience with ITIL Framework\\nWorking knowledge on Apache Hadoop, Spark, Airflow, Cassandra and Kafka ecosystem\\nPrior experience in deploying Elastic solutions in production environments processing operational data in terms of at least 500 GB / day\\n\\n\\nPreferred Technical and Professional Expertise\\nExperience with SQL based tools &amp; expertise on any one traditional RDBMS – mySQl; MSSQL;Oracle;DB2 etc\\nPrior experience with DevOps projects, Github, Jira, Travis, etc\\nWorking knowledge on Windows, Linux and AIX platforms\\nWorking knowledge of Top commercial distributions of the above stack – MapR; Cloudera; Hortonworks etc\\nKnowledge on shell scripting\\nGood knowledge on other Top level Apache Big Data technologies like Cassandra, NIFI, Fluentd, Drill, Sentry etc\\nExcellent understanding on HDFS &amp; other similar Map/Reduce paradigms\\nKnowledge on 1-2 NoSQL databases – Redis; MongoDB;Cassandra;Neo4j; VoltDB etc\\n\\n\\n\\nAbout Business Unit\\nAt Global Technology Services (GTS), we help our clients envision the future by offering end-to-end IT and technology support services, supported by an unmatched global delivery network. It's a unique blend of bold new ideas and client-first thinking. If you can restlessly reinvent yourself and solve problems in new ways, work on both technology and business projects, and ask, \"What else is possible?\" GTS is the place for you!\\n\\nYour Life @ IBM\\nWhat matters to you when you’re looking for your next career challenge?\\n\\nMaybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities – where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust – where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.\\n\\nImpact. Inclusion. Infinite Experiences. Do your best work ever.\\n\\nAbout IBM\\nIBM’s greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.\\n\\nLocation Statement\\nFor additional information about location requirements, please discuss with the recruiter following submission of your application.\\n\\nBeing You @ IBM\\nIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX 78746</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78746</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Strong knowledge of statistics, including hands-on experience with SAS, R, Matlab, Machine Learning, AI.\\nExperience working with large datasets (1B+ Records)\\nKnowledge of Big Data or Cloud technologies\\nExperience with version control (e.g. TFS, SVN or Git) and build tools.\\nTableau/BI Tools\\n</td>\n",
       "      <td>Develop logical data models and processes to transform, cleanse, and normalize raw data into high-quality datasets aligned with our analytical requirements.\\nDevelop and maintain comprehensive controls to ensure data quality and completeness.\\nManage data movement through our infrastructure. Streamline existing data workflows to create a flexible, reliable, and faster process.\\nDevelop real-time data transformations and validations.\\nIdentify and onboard new data sources. Collaborate with data vendors and internal stakeholders to define requirements and build interfaces. Troubleshoot and resolve issues with data feeds.\\nWork with our team of researchers to identify and analyze investment opportunities in real estate and fixed income securities markets.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>3-5 years of experience in data analysis and/or management in an enterprise environment within finance, operations or analytics.\\nPassion for data organization, quality, and reliability.\\nMS SQL Server, Oracle, Postgres, Hive, Presto, etc. preferred. Experience developing complex efficient queries, designing and building logical data models, and working with large datasets on a relational database system\\nExperience with at least one language (e.g. Python, C#, Scala, Java).\\nStrong problem-solving skills.\\nMust be an intellectually curious self-starter and motivated to continually learn.\\nProactive, hardworking team player with excellent communication skills\\n</td>\n",
       "      <td>Data Engineer – Austin, TX\\nAmherst is revolutionizing the way U.S. real estate is priced, managed and financed in order to unlock opportunities for all market participants. Driven by data, analytics, and technology, Amherst has a 20-year history of anticipating where the next risks and opportunities are likely to emerge and designing actionable strategies for investors to capitalize on opportunities across residential real estate, commercial real estate and public securities. Amherst, along with its affiliates and subsidiaries, has more than 900 employees, $5 billion under management and approximately $15 billion under advisement and oversight. www.amherst.com.\\nWe are hiring for a Data Engineer to utilize their industry knowledge, technical skills and passion for data to work closely with executive stakeholders and our financial engineering team developing solutions that support and optimize business operations. We’re solving a variety of Big Data challenges and modernizing legacy data loaders as well as exploring the benefits and tradeoffs of other cutting edge tools. In this role, you will be responsible for a variety of duties including; understanding our data, application design and development, SQL query optimization and ensuring accuracy and consistency of data.\\nIdeal Candidate:\\nAnalytical mindset with the ability to structure and process qualitative data and draw insightful conclusions.\\nProblem solver able to take a complex business request and transform it into a clean, simple data solution.\\nQuick learner open to new ideas and technologies, and willing to offer creative solutions.\\nOwnership and prideful in work and brings new ways and ideas to the table.\\nResponsibilities:\\nDevelop logical data models and processes to transform, cleanse, and normalize raw data into high-quality datasets aligned with our analytical requirements.\\nDevelop and maintain comprehensive controls to ensure data quality and completeness.\\nManage data movement through our infrastructure. Streamline existing data workflows to create a flexible, reliable, and faster process.\\nDevelop real-time data transformations and validations.\\nIdentify and onboard new data sources. Collaborate with data vendors and internal stakeholders to define requirements and build interfaces. Troubleshoot and resolve issues with data feeds.\\nWork with our team of researchers to identify and analyze investment opportunities in real estate and fixed income securities markets.\\nRequirements:\\n3-5 years of experience in data analysis and/or management in an enterprise environment within finance, operations or analytics.\\nPassion for data organization, quality, and reliability.\\nMS SQL Server, Oracle, Postgres, Hive, Presto, etc. preferred. Experience developing complex efficient queries, designing and building logical data models, and working with large datasets on a relational database system\\nExperience with at least one language (e.g. Python, C#, Scala, Java).\\nStrong problem-solving skills.\\nMust be an intellectually curious self-starter and motivated to continually learn.\\nProactive, hardworking team player with excellent communication skills\\nBonus Skills:\\nStrong knowledge of statistics, including hands-on experience with SAS, R, Matlab, Machine Learning, AI.\\nExperience working with large datasets (1B+ Records)\\nKnowledge of Big Data or Cloud technologies\\nExperience with version control (e.g. TFS, SVN or Git) and build tools.\\nTableau/BI Tools\\nWhat We Offer:\\nCompetitive salaries\\nChoice of Mac or Windows hardware\\nFlexible vacation days, paid holidays, and work from home options\\nMedical, Dental, Vision, LTD, Life, EAP, and 401K with matching benefits\\nStellar colleagues with proven track records\\nFree sodas, kombucha, cold brew, beer and healthy and unhealthy snacks at our Barton Springs WeWork location\\nFree lunch every day and breakfast tacos on Thursdays!\\nAmherst is proud to be an Equal Opportunity Employer and committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, color, religion, national origin, gender, pregnancy, sexual orientation, gender identity, age, physical or mental disability, genetic information or veteran status, and encourage all applicants to apply.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer/Analytic Manager</td>\n",
       "      <td>Austin, TX 78746</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78746</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>ABOUT THE TEAM:\\nOur team oversees all aspects of eBay’s finance, analytics, and information technology functions – including controllership, procurement, financial planning and analysis, tax, treasury, audit, mergers and acquisitions, and investor relations. At eBay, we love data, so finance plays a critical role in establishing strategic focus, enabling growth, ensuring execution and driving efficiencies across the organization.\\nJOB REQUIREMENTS\\nGlobal Collections analytics team is responsible for delivering business insights and performance insights for Collections and Treasury.\\nWe are currently seeking a Data Engineer/Analytic Manager to lead the team and support our department. In this role you will support internal collections teams, own development and implementation of analysis and help prioritize key bad debt reduction initiatives through decisions based on data without impact on top line revenue.\\nCHARACTERISTICS:\\nIN THIS ROLE THE MANAGER WILL:Deliver insights into customers, processes and products to drive improvements in collections and reducing bad debt for eBay\\nConduct post-mortem loss analysis on strategic growth initiatives and share updates with the collections leadership team\\nPerform exploratory data analysis to develop hypotheses for causes of bad debt\\nDevelop SQL queries to extract data for both analysis and model construction\\nDesign eye-catching visualizations in Tableau and build supporting calculations\\nBuild database tables and schemas to support the assembly of data for analytics\\nDevelop SQL code for ETL projects\\nSupport critical Collections metrics and build out self-serve reporting that will strengthen decision quality through meaningful data\\nDemonstrate the ability to work in ambiguity using experience and proven theory knowledge\\nPartner with other cross functional teams within eBay to use standard methodology, share data analysis and establish business cases\\nProvide cross-functional, data-driven recommendations for lowering bad debt while remaining aligned with other team goals\\nAssess events on customer invoice timeline to drive better business outcomes\\nBASIC QUALIFICATIONS\\nIf you work well in a demanding environment, you learn quickly and possess extensive collection/risk management experience, this is a role you should consider. Being able to demonstrate the ability to breakdown processes, look for immediate opportunities and have a vision for future needs is what we need. In this role you will have the opportunity to scope and size things globally, recommend solutions that are scalable and balanced while considering future growth expectations for the company.\\nAnalytics experience in Tech or Financial Industries\\nProficient in SQL\\nExperience developing Tableau dashboards with a deep understanding of associated data architecture trade-offs\\n+5 years’ analytics/database experience\\n+5 years Software development, software testing, or operational improvement experience preferred\\nExperience working with groups remotely and multiple cultures\\nProficient in MS Office applications, Tableau, SQL, Teradata, and Unix commands\\nAbility to think from the perspective multiple user and employee personas\\nFamiliarity with traditional collection metrics (roll rates, liquidation and charge off rates)\\nProficient in Microsoft Excel, Word and PowerPoint\\nCollections and Loss Financial Data Analysis Experience (preferred)\\nFluency in English is a requirement with excellent communication skills\\nExcellent collaboration skills, team-work, and influencing business outcomes across a matrixed environment\\nBA/BS degree required\\nWe love creating opportunities for others by connecting people from widely diverse backgrounds, perspectives, and geographies. So, being diverse and inclusive isn’t just something we strive for, it is who we are, and part of what we do each and every single day. We want to ensure that as an employee, you feel eBay is a place where, no matter who you are, you feel safe, included, and that you have the opportunity to bring your unique self to work.. To learn about our Diversity &amp; Inclusion click here: https://www.ebayinc.com/our-company/diversity-inclusion/.\\nThis website uses cookies to enhance your experience. By continuing to browse the site, you agree to our use of cookies\\nView our privacy policy\\nView our accessibility info\\neBay Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talent@ebay.com. We will make every effort to respond to your request for disability assistance as soon as possible.\\nFor more information see:\\nEEO is the Law Poster\\nEEO is the Law Poster Supplement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\n\\nOur Data Engineering team builds and maintains a secure, scalable, flexible and user-friendly analytics hub that allows us to make informed and data-driven decisions. They also construct and curate business-critical data sets that allow us to realize the value of all the data we collect.\\nA Data Engineer utilizes a multidisciplinary approach to providing ETL solutions for the business, combining technical, analytical, and domain knowledge. The perfect applicant for this role has strong development skills, experience transforming and profiling data to determine risks associated with proposed analytics solutions, a willingness to continually interface with analysts in order to determine an optimal approach, and an eagerness to explore data sources to understand the availability, utility, and integrity of our data.\\nWhat you'll own:\\nData pipeline / ETL development:\\nBuilding and enhancing data curation pipelines using tools like SQL, Python, Glue, Spark and other AWS technologies\\nFocus on data curation on top of datalake data to produce trusted datasets for analytics teams\\nData Curation:\\nProcessing and cleansing data from a variety of sources to transform collected data into an accessible and curated state for Analysts and Data Scientists\\nMigrating self-serve data pipeline to centrally managed ETL pipelines\\nAdvanced SQL development and performance tuning\\nSome exposure to Spark, Glue or other distributed processing frameworks helpful\\nWork with business data stewards &amp; analytics team to research and identify data quality issues to be resolved in the curation process\\nData Modeling:\\nDesign and build master dimensions to support analytic data requirements\\nReplacing legacy data structures with new datasets sourced from streaming data feeds from the core product and other operational systems\\nDesign, build and support pipelines to deliver business critical datasets\\nResolve complex data design issues &amp; provide optimal solutions that meet business requirements and benefit system performance\\nQuery Engine Expertise &amp; Performance Tuning:\\nAssist Analytics teams with tuning efforts\\nCurated dataset design for performance\\nOrchestration:\\nManagement of job scheduling\\nDependency management mapping and support\\nDocumentation of issue resolution procedures\\nData Access\\nDesign and management of data access controls mapped to curated datasets\\nLeveraging devops best practices, such as IAC and CI/CD to build upon a scalable and extensible data environment\\n\\nExperience you'll need:\\nStrong experience designing and building end-to-end data pipelines\\nExtensive SQL development experience\\nKnowledge of data management fundamentals and data storage principles\\nData modeling:\\nNormalization\\nDimensional/OLAP design and data warehousing\\nMaster data management patterns\\nModeling trade-offs impacting data management &amp; processing/query performance\\nKnowledge of distributed systems as it pertains to data storage, data processing and querying\\nExtensive experience in ETL and DB performance tuning\\nHands on experience with a scripting language (Python, bash, etc.)\\nSome experience with Hadoop, Spark, Kafka, Impala, or other big data technologies helpful\\n\\nFamiliarity with the technology stacks available for:\\nMetadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\nData management, data processing and curation:\\nPostgres, Hadoop, Hive, Impala, Presto, Spark, Glue, etc.\\nExperience in data modeling for batch processing and streaming data feeds; structured and unstructured data\\nExperience in data security / access management, data cataloging and overall data environment management\\n\\nExperience with cloud services such as AWS and APIs helpful\\nYou’d be a great fit if your current track record looks like this:\\n5+ years of progressive experience data engineering and data warehousing\\nExperience with a variety of data management platforms (e.g. RDBMS (Postgres), Hadoop (CDH, EMR))\\nExperience with high performance query engines (Hive, Impala, Presto, Athena, MPP engines like RedShift)\\nStrong capability to manipulate and analyze complex, high-volume data from a variety of sources\\nEffective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language\\nAbility to problem solve independently and prioritize work based on the anticipated business value\\n\\nQualifications\\n\\nnull\\n\\nAdditional Information\\n\\nAll your information will be kept confidential according to EEO guidelines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineering Manager</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience leading, managing and hiring a team of talented engineers\\nExpertise in at least one of the following engineering domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExpertise in at least one of the following data domains: * Predictive analytics (e.g., recommendation systems, predictive maintenance)\\nNatural language processing (e.g., conversational chatbots)\\nDocument understanding\\nImage classification\\nMarketing analytics\\nIoT systems\\nExperience writing software in one or more languages such as Python or Java/Scala\\nExperience in technical consulting or customer-facing role\\nExcellent critical thinking, problem-solving and analytical skills\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join SADA as a Data Engineering Manager!\\n\\nYour Mission\\n\\nAs a Data Engineering Manager at SADA, you will build and lead a growing Data Engineering team as we deliver robust data solutions for our clients on Google Cloud Platform (GCP). You will be responsible for managing a blended team of data engineers and data scientists, so a broad background in Big Data, data warehouse modernization, analytics, disaster recovery, data science, and machine learning is highly advantageous.\\n\\nThe diversity of customers that SADA works with ensures a steady flow of challenging data work. Be prepared to tackle real-world data problems that our customers find too difficult or time-consuming to solve themselves. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of data domain areas. Management here at SADA also means developing people and being a leader.\\n\\nIn this role, you will:\\n\\nBe comfortable working with customer executives to align business outcomes with technical vision and goals.\\nGuide the day-to-day activities of a geographically distributed team, including hiring world-class talent, reviewing work and setting goals.\\nProvide technical and professional leadership and mentorship on a diverse range of subject matter areas, such as Big Data pipelines and data warehouses to statistics and machine learning.\\nDevelop and codify best practices for your team that can be replicated across multiple customer engagements.\\nPartner with your team to develop services and offerings that scale and are repeatable.\\nParticipate in key technical and design discussions with technical leads as a hands-on manager.\\nPartner with other practice leads, architects, project managers, executives and sales personnel to develop statements of work, and then oversee execution by your team with high levels of agility and quality.\\n\\nPathway to Success\\n\\n#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our employees know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.\\n\\nYour success starts by positively impacting the direction of a fast-growing data practice area with vision and passion. You will be measured by your team’s performance on customer engagements, how well your team achieves internal organizational goals, how well you collaborate with and support your team and peers, and the consultative polish you bring to customer interactions.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the management growth track.\\n\\nExpectations\\n\\n\\nRequired Travel - 15-25% travel to customer sites, conferences, and other related events\\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.\\nTraining - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Data Engineer Certified\\n\\n[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment\\n\\nRequired Qualifications:\\n\\nExperience leading, managing and hiring a team of talented engineers\\nExpertise in at least one of the following engineering domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExpertise in at least one of the following data domains: * Predictive analytics (e.g., recommendation systems, predictive maintenance)\\nNatural language processing (e.g., conversational chatbots)\\nDocument understanding\\nImage classification\\nMarketing analytics\\nIoT systems\\nExperience writing software in one or more languages such as Python or Java/Scala\\nExperience in technical consulting or customer-facing role\\nExcellent critical thinking, problem-solving and analytical skills\\n\\nUseful Qualifications:\\n\\nExperience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)\\nExperience in a large scale, high-volume data warehouse environment\\nExperience operationalizing machine learning models on large datasets\\nDemonstrated skills in selecting the right statistical tools given a data analysis problem\\n\\nAbout SADA\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>ABOUT THIS ROLE\\nAs a member of our engineering team, you will work on the design, implementation and delivery of data platform frameworks, pipelines, microservices, and other features that build on our high-value data assets. Your customers will include data analytics, marketing, and leadership teams as well as our Care Partners (hospitals and physicians) and external data partners, in concert with our Enterprise Data Warehouse delivery teams.\\nRESPONSIBILITIES\\nEvaluate and build proofs of concept for Cloud PaaS and IaaS offerings for data liquidity, data management and storage, data pipelines built on both traditional ETL as well as streaming platforms, master data management, data stewardship, record-linkage, NLP services, and more.\\nWrite traditional code and server-less functions using the right language for the task, which may be SQL, Python, C#, Java, PowerShell, SSIS/BIML, and others.\\nParticipate in build-buy-open source decisions for parsing and managing industry standard formats such as FHIR/NDJSON, pipe-and-hat HL7, and x12 EDI\\nEvaluate, select, and apply Cloud and OO design and resiliency patterns\\nBuild APIs and data microservices to share our data with internal and external partners, and write interfaces to public data sets to enrich our analytics data stores\\nProvide subject matter expertise on performance tuning and query optimization to full-stack peers, data analysts, and EDW developers\\nParticipate in building and owning a DevOps culture\\nContinuously document your code, framework standards, and team processes\\nEDUCATION, TRAINING, AND PROFESSIONAL EXPERIENCE\\n5+ years of experience in an enterprise or commercial software development environment\\nExtensive experience developing data-intensive solutions against an RDBMS, such as SQL Server, Postgres or Oracle.\\nHighly skilled writing SQL queries, DML and DDL, CDC/change tracking patterns, indexes and performance tuning.\\nEnterprise development experience coding in at least one, but preferably more than one, procedural/OO language, e.g., C#, Java, JavaScript, Python, C++, PowerShell\\nProficiency in using OOTB components, as well as implementing custom components or frameworks, on at least one traditional ETL platform, preferably SSIS, Informatica, or Talend\\nTeam player who is not afraid to ask questions, take risks, share in owning team victories as well as team failures\\nGood communicator – both written and verbal – with high emotional intelligence\\nAbility to focus on MVP and shipping software while remaining cognizant of the long-term costs of technical debt\\nHealthcare data background a must\\nMUST HAVE THE RIGHT TO WORK IN THE US WITHOUT VISA SPONSORSHIP\\nIdeal candidates come to the table with one or more additional competencies, such as:\\nExposure to Enterprise Data Warehouse , Data Lake, Big Data, unstructured data, in-memory data stores\\nFamiliarity with NoSQL database systems such as MongoDB, Cassandra, CosmosDB, neo4j etc.\\nFamiliarity with Kimball-like star and snowflake data models and columnstore Indexing\\nExperience building metadata-driven data pipeline frameworks for quickly mapping, onboarding, and ingesting data from a wide variety of partner sources\\nEnterprise experience with data movement and management in the Cloud utilizing some combination of Azure and/or AWS features such as Data Factory, Blob Storage, Service Bus, Kafka, Redis, S3 Buckets, Azure Automation, Machine Learning, elastic search, Glue etc.\\nData Science training or experience to better understand and collaborate with one of our key data consumers (notably, this is still an engineering role and not a data science role)\\nCRM experience, such as MS Dynamics or SalesForce\\nABOUT US\\n\\nAt Bright Health, we brought together the brightest minds from the health care industry and consumer technology and together we created Bright Health: a new, brighter approach to healthcare, built for individuals. Our plans are easy to manage, personalized and more affordable, giving people the quality care they deserve. Through our exclusive care partnerships with leading health systems in local communities we are reshaping how people and physicians achieve better health together.\\n\\nBright Health is tripling its footprint in 2019 to offer a variety of health insurance plans to more individuals. Bright Health operates health insurance offerings across Individual and Family Plan segments and the Medicare Advantage space in Alabama, Arizona, Colorado, Ohio, New York and Tennessee.\\n\\nWe’re Making Healthcare Right. Together.\\n\\nWe've won some fun awards like: Great Places to Work, Modern Healthcare, Forbes, etc. But more than anything, we're a group of people who are really dedicated to our mission in healthcare. Come join our growing team!\\n\\nAs an Equal Opportunity Employer, we welcome and employ a diverse employee group committed to meeting the needs of Bright Health, our consumers, and the communities we serve. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>So what’s our story? We believe our value is in helping our clients do something they never dreamed possible. Giving them that certain moment when clarity becomes confidence. Finding a whole new customer segment. Reaching everyone who needs to be reached. Understanding those tiny market nuances. There’s more, of course, but these are the kinds of things that make the difference to our clients – the things that help them sleep at night.\\n\\n\\nAre you self-motivated with a proven track record of delivering results? Do you have a demonstrated ability to think strategically in complex situations, motivate and mobilize resources, and deliver over the top results? This Big Data Engineer role will be provide the opportunity to do all of the above while driving architecture design, data modeling, and implementation of Big Data platform and analytic applications.\\nWhat you will do:\\nFulfill requests from the product team such as building upon our Acxiom Unified Data Layer(UDL) product base, optimizing data processing via Spark/Scala, researching new innovation ideas.\\nOptionally solve additional challenges in our service tier in NodeJS\\nWrite spark jobs and components in Scala programming language\\nWork in an agile teaming environment\\nConsistently provides proven, formal mentorship\\nRegularly lead self and others and/or established as Product SME and/or established as specialist\\nUnderstands how whole picture aligns to overall Acxiom strategy\\nWorks with product manager to maximize the components ROI\\nExpected more thought leadership\\nHave a broad understanding of the external events that may impact applications or systems (networking, operations, etc.)\\nWhat you will need:\\n3+ years experience in Big Data Hadoop, Hive and Spark with hands on expertise in design and implementation of high data volume solution\\nGood knowledge of configuring Spark and working on multi node clusters and distributed data processing framework.\\nHands on knowledge of Big Data Analytics and Predictive Analytics\\nExtremely agile with good knowledge of relational DWs as well as NoSQL DBs like, MongoDB etc.,\\nStrong in Spark Scala pipelines (both ETL &amp; Streaming)\\nProficient in Spark architecture\\nCloud developer experience in leading cloud providers (AWS, Azure, GCP)\\nStrong development skills in Node.js for REST apis\\nExperience with real-time data ingestion using streaming technologies such as Kafka and Kinesis.\\n#GD17\\nPrimary Location City/State:\\nAustin, Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX 78758</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78758</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Summary\\nPosted: Oct 8, 2019\\nWeekly Hours: 40\\nRole Number: 200041718\\nAt Apple, excellent ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis? If so, Apple's Global Business Intelligence (GBI) team is seeking an expert Data Engineer to build high quality, scalable and resilient distributed systems that power apple's analytics platform and data pipelines. Apple's Enterprise Data warehouse system cater to a wide variety of real-time, near real-time and batch analytical solutions. These solutions are integral part of business functions like Sales, Operations, Finance, AppleCare, Marketing and Internet services enabling business drivers to make critical decisions. We use a diverse technology stack such as Teradata, HANA, Vertica, Hadoop, Kafka, Spark, and Cassandra and beyond. Designing, Developing and scaling these big data technologies are a core part of our daily job. The team member will be able think outside of the box and should have passion for building analytics solutions to enable business in making time sensitive and critical decisions.\\nKey Qualifications\\nWe would like for you to have In-depth understanding of data structures and algorithms\\nWe are looking for experience in designing and building dimensional data models to improve accessibility, efficiency, and quality of data\\nDatabase development experience with Relational or MPP/distributed systems such as Oracle/Teradata/Vertica/Hadoop\\nWe are seeking programming experience in building high quality software in Java, Python or Scala preferred\\nExperience in designing and developing ETL data pipelines. Should be proficient in writing Advanced SQLs, Expertise in performance tuning of SQLs\\nYou will demonstrate excellent understanding of development processes and agile methodologies\\nStrong analytical and interpersonal skills\\nSelf-driven, highly motivated and ability to learn quick\\nExperience with or advance courses on data science and machine learning is ideal\\nWork/project experience with big data and advanced programming languages is a plus\\nExperience developing Big Data/Hadoop applications using java, Spark, Hive, Oozie, Kafka, and Map Reduce is a huge plus\\nDescription\\nYou will build and design data structures on MPP platform like Teradata, Hadoop to provide efficient reporting and analytics capability. Design and build highly scalable data pipelines using new generation tools and technologies like Spark, Kafka to induct data from various systems.\\nTranslate complex business requirements into scalable technical solutions meeting data warehousing design standards. Strong understanding of analytics needs and proactive-ness to build generic solutions to improve the efficiency Build dashboards using Self-Service tools like Tableau and perform data analysis to support business\\nCollaborate with multiple cross functional teams and work on solutions which has larger impact on Apple business\\nWe seek a self starter, visionary person with strong leadership capabilities\\nAbility to communicate effectively, both written and verbal, with technical and non-technical multi-functional teams\\nYou will interact with many other group’s internal team to lead and deliver elite products in an exciting rapidly changing environment\\nDynamic, smart people and inspiring, innovative technologies are the norm here. Will you join us in crafting solutions that do not yet exist?\\nEducation &amp; Experience\\nBachelor’s Degree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Junior Data Engineer</td>\n",
       "      <td>Austin, TX 78705</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78705</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a Junior Data Engineer to join our Data Engineering team in Austin. This team member will be responsible for working with the team to expand and optimize our cloud-native big data pipeline architecture, as well as optimize data flow and collection for the teams we support. The ideal candidate is excited to work closely with a team of engineers and analysts and to learn new tools and technologies in the data and data engineering space and already has an understanding of SQL, database structures, and software development processes. Curiosity, a great attitude, and an aptitude for data and software are highly valued.\\n\\nAbout Us\\nWe are the leading technology provider for the $38 billion self-storage industry. Our solutions, which includes more than 15,000 facilities, offers consumers the ability to find, compare and book self-storage, manage the operations of their storage business, and a host of other capabilities that make them better business managers . We provide a suite of industry-leading web marketing tools for self-storage operators and have been chosen as a preferred partner by more major online brands than any other self-storage company.\\n\\nIn 2018, SpareFoot acquired SiteLink and storEDGE, global leaders in self-storage management software and in-house payment processing. The deal allows the combined business to accelerate investment, drive innovation and generate value for both consumers and facility operators.\\n\\nWe are now relaunching the combined company under one name and consolidating the way that all of the merged and acquired entities go to market.\\n\\nFor three consecutive years, SpareFoot has been recognized by Entrepreneur Magazine and the Austin American-Statesman for an exceptional company culture.\\n\\nWhat you’ll do everyday:\\nCreate and maintain optimal data pipeline architecture\\nAssemble large, complex data sets that meet both functional and non-functional requirements\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologies\\nBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics\\nWork closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needs\\nCreate data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our product\\nWork with data and analytics experts to strive for greater functionality in our data systems\\nWhat you need to bring to the table:\\nComputer science degree or equivalent experience\\n2+ years experience in software development, data engineering, BI development, and / or data architecture\\nExperience with Python, SQL, AWS, RESTful APIs, and Tableau or other data visualization tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Healthcare Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>What You'll Do\\nWork closely with the various areas of the business to design and implement requirements and develop processes necessary to provide visibility into the data via the data warehouse\\nEnsure data that is brought into the data warehouse is clean, accurate, available and complete\\nArchitect, develop, implement and test algorithms that consist of value-add routes and build data warehouse infrastructure for automated interpretation of healthcare data\\nIdentify ways to improve data reliability, efficiency and quality. Produce actionable recommendations that address known problems and then implement automated solutions\\nAssist the development teams with business knowledge of various healthcare data that is ingested from various sources\\nNotches in your belt:\\n5+ years of professional SQL programming experience\\nIn depth experience with SQL and NOSQL databases\\n5+ years working with health plan data\\nStrong knowledge of healthcare data, specifically health plan (i.e. medical and Rx claims, eligibility, provider data)\\nDeep understanding with healthcare coding terminologies (CPT, diagnosis, DRG, etc)\\nExperience in designing and building efficient and scalable solutions for big data\\nExperience with query development and optimization\\nExperience with common data warehouse and data lake architecture concepts and best practices\\nProven work experience with algorithm design and implementation\\nStrong problem solving skills\\nAbility to independently manage all phases of development including requirement documentation, building, configuration, bug tracking, testing, and validation\\nStrong experience with cloud-based infrastructure\\nStrong communication skills between business and technical resources\\n\\nGreat to have:\\nExperience providing database endpoints and/or the creation of RESTful API’s\\nPrevious experience in the Python/Django framework\\nPrevious knowledge of data integration processes and tools (specifically AWS based)\\nExperience with healthcare data formats (processing 834, 837, X12 file formats)\\nExperience with health system EHR data\\nExperience leveraging Redshift and other AWS data pipelines / tools\\nAdvanced degree in Computer Science\\nAn active GitHub profile or other public code portfolio\\n\\nHow to Apply\\n\\nTo apply, email careers@icanbwell.com with your resume, your GitHub account, and why you’d love to help us change the face of healthcare. Please use the subject line: “Apply - Healthcare Data Engineer”\\nYou will also enjoy a fun, collaborative, and exciting culture that is fully immersed in Austin &amp; Baltimore’s health and tech communities. We work hard for our mission and are compensated accordingly. We offer competitive salaries, 401(k), and medical, dental, and vision insurance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Senior Systems Administrator (Sharepoint)- Senior Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview\\nBy Light is hiring a Senior Systems Administrator (Sharepoint)- Senior Data Engineer to join our team supporting the Department of Veterans Affairs.\\nResponsibilities\\nExperience in planning and supporting IT business processes\\nWorking knowledge of SharePoint\\n Knowledge of all phases of software development with emphasis on the planning, analysis, modeling, simulation, testing, integration, documentation and presentation phases\\nDocument ITS business processes including both process descriptions (or operating procedures) and process maps\\nAnalyze process gaps and assist in process improvement and/or re-engineering activities\\nCoach management to explore and advise SharePoint technology possibilities\\nMaintain the existing custom SharePoint workflow applications, and managing authentication and authorization in line with VA and FSC policies and processes\\nWork with functional stakeholders to diagnose problems of custom SharePoint workflow applications to troubleshoot for changes and/or updates\\nSolicit/elaborate Functional user requirements in the effort of developing workflows to automate business processes\\nLead the effort in custom workflow solution design on SharePoint to automate business collaboration processes\\nLead the effort in measuring business process performance by designing and maintaining measurement dashboards\\nDetermine how each functional operation works and maintain an awareness of change management as it applies to implementing and/or updating custom SharePoint workflow applications\\nRequired Experience/Qualifications\\nBachelor's Degree\\nA minimum of 5 years’ progressive experience\\nPreferred Experience/Qualifications\\nPractical knowledge in business process improvement and re-engineering disciplines\\nBeing familiar with Lean Six Sigma, ITIL, and/or CMMI practices\\nExperience in leading business process improvement efforts in IT service organizations\\nAbility to standardize business processes and operating procedures\\nAbility to elaborate requirements from business needs\\nExperience in business process mapping and documentation, business process/workflow automation and measurement using proven technologies\\nExperience working with SharePoint as a super user, i.e. ability to design zero-code custom applications and automated workflows on SharePoint\\nSharp skills in PowerApps, MS Flows, Excel VBA, and Power BI\\nExcellent customer service, communication and organizational skills are required\\nCapable to work under pressure, handle multiple tasks simultaneously\\nExperience providing services to the federal government and/or the Dept. of Veterans' Affairs is preferred\\nSpecial Requirements/Security Clearance\\nCandidates must successfully complete a background investigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Austin, TX 78705</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78705</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We are looking for a Data Engineer to join our Data Architecture team in Austin. This team member will be responsible for expanding and optimizing our cloud-native big data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing distributed systems as well as building them from the ground up.\\n\\nThe right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.\\n\\nAbout Us:\\nWe are the leading technology provider for the $38 billion self-storage industry. Our solutions, which includes more than 15,000 facilities, offers consumers the ability to find, compare and book self-storage, manage the operations of their storage business, and a host of other capabilities that make them better business managers . We provide a suite of industry-leading web marketing tools for self-storage operators and have been chosen as a preferred partner by more major online brands than any other self-storage company.\\n\\nIn 2018, SpareFoot acquired SiteLink and storEDGE, global leaders in self-storage management software and in-house payment processing. The deal allows the combined business to accelerate investment, drive innovation and generate value for both consumers and facility operators.\\n\\nWe are now relaunching the combined company under one name and consolidating the way that all of the merged and acquired entities go to market.\\n\\nFor three consecutive years, SpareFoot has been recognized by Entrepreneur Magazine and the Austin American-Statesman for an exceptional company culture.\\n\\nWhat you’ll do everyday:\\nCreate and maintain optimal data pipeline architecture\\nAssemble large, complex data sets that meet both functional and non-functional requirements\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using various SQL and ‘big data’ technologies\\nBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics\\nWork closely with stakeholders including the Executive, Product, Data and Design teams to design and deliver products and functionality to address analytical and functional data needs\\nCreate data tools for analytics and data scientist and business operations team members that assist them in building and optimizing our product\\nWork with data and analytics experts to strive for greater functionality in our data systems\\nWhat you need to bring to the table:\\nComputer science degree or equivalent experience\\n7+ years experience in software development, data engineering, BI development, and / or data architecture\\nExperience with Python, SQL, AWS, RESTful APIs, and Tableau or other data visualization tools\\nConsistent track record of leading successful delivery for a large-scale project or being a key contributor on multiple projects\\nConsistent track record of positively influencing project direction and contributions to cross-functional and/or cross-organizational collaborations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Senior Application Support Analyst (Temp. 6 months)</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Named as one of Fortunes’ 100 Fastest Growing Companies for 2019, EPAM is committed to providing our global team of 30,100+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential.\\n\\nDescription\\n\\nYou are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Senior Application Support Analyst. Scroll down to learn more about the position’s responsibilities and requirements.\\n\\nEPAM Systems is seeking a Cloud Platform/Big Data Support Specialist to provide enterprise-level support to customers of a major cloud service provider. As a cloud support specialist, you will work in a team of experienced support engineers to resolve customer's concerns and issues for using cloud platform and big data products.\\n\\nYou would use your technical expertise and communication skills to understand customer's problem, provide technical assistance, then guide them to resolution. You would also participate in discussion with product engineers to share your insights on customer needs and issues to help product improvements. You will be fully exposed to the cutting edge technologies of a prominent cloud service provider, and play a key role in the growth of their cloud computing products.\\n\\n#LI-DNI\\nWhat You’ll Do\\nProvide technical assistance and support over e-mail, chat and phone as part of a global 24x7-support organization\\nProvide initial response to customer's inquiry, troubleshoot, provide updates, identify root case, and resolve the issue to the satisfaction of customer\\nHandle escalation from customer and lead to satisfactory resolution\\nCo-work with engineers across technical and product domains to resolve complex cross-domain issues\\nConsult with senior engineers and subject matter experts (SME) to accelerate problem resolution\\nHand-off or take-over cases to/from other geographical region to provide around-the-clock issue resolution for premium customers\\nFollow communication guidelines and security policies when communicating with customer\\nCategorize support requests for support and service analytics\\nProduce support documents, perform knowledge sharing and training\\nKeep technical skills up to date with latest cloud technologies\\nWhat You Have\\nA degree in an associated field and/or other advanced certification along with significant experience\\nStrong analytical / troubleshooting / problem solving skills\\nStrong verbal and written communication skills\\nExcellent customer service skills\\nAbility to perform job functions under stress and pressure\\nCommitment to continuous self-learning\\nRegular, reliable attendance\\n3+ years of experience as developer or a combination developer + big data engineer\\nProficient in at least one of the following development languages: Java, Python, .NET, Ruby, PHP, Go or Javascript (NodeJS)\\nHands on experience with RESTful APIs\\nExperience with relational databases (e.g. MySQL, PostgreSQL, etc.)\\nExperience with Big Data architectures and technologies and BI solutions\\nExperience in CI/CD, DevOps and related automation tools (e.g. Jenkins, Chef, Puppet, etc.)\\nAbility to read and understand code and able to write code samples to reproduce customer issues\\nAbility to read and understand logs and stack traces to troubleshoot issues\\nGood oral and written business communication skills in English (CEF Level C1 or above)\\nMust be able to work on the following shifts:\\nEarly week shift from 7:00 AM to 6:00 PM, Sunday to Wednesday\\nLate week shift from 7:00 AM to 6:00 PM, Wednesday to Saturday\\nYes, you will work 4 days and take 3 days off\\nMay need to work on public holidays. If worked on a public holiday, you will be provided with a day-off in lieu\\nNice to have\\nBA/BS degree preferred\\n2+ years of customer support experience preferably in Enterprise software support\\nExperience with PaaS and IaaS technologies\\nExperience with distributed computing frameworks (e.g. Hadoop, Spark, Flink, Storm, Samza, Beam, Airflow, Google Big Query, etc.)\\nExperience with distributed data stores (HBase, Cassandra, Riak, Google Bigtable, Amazon Dynamo DB, etc.) and/or distributed message brokers (Kafka, RabbitMQ, ActiveMQ, Google Pub/Sub, Amazon Kinesis, etc.)\\nExperience with ETL processes and tools (e.g. AWS Glue, Google Dataprep and/or Datafusion, MS SSIS, ODI, IPC, etc.)\\nExperience with any ML library (scikit-learn, XGBoost, pytorch, tensorflow, Spark mllib) or basic understanding of ML concepts\\nWhat We Offer\\nMedical, Dental and Vision Insurance (Subsidized)\\nHealth Savings Account\\nFlexible Spending Accounts (Healthcare, Dependent Care, Commuter)\\nShort-Term and Long-Term Disability (Company Provided)\\nLife and AD&amp;D Insurance (Company Provided)\\nEmployee Assistance Program\\nUnlimited access to LinkedIn learning solutions\\nMatched 401(k) Retirement Savings Plan\\nPaid Time Off\\nLegal Plan and Identity Theft Protection\\nAccident Insurance\\nEmployee Discounts\\nPet Insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Reporting and Data Engineer-Signify Community</td>\n",
       "      <td>Austin, TX 78730</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78730</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in Engineering, Mathematics, Computer Science or related field\\nClinical and healthcare relevance; a Clinical degree isn’t required, but a deep understanding of the healthcare market is.\\n</td>\n",
       "      <td>Solid knowledge of SQL queries and relational databases\\nGood understanding of math and statistics\\nDeep experience with Excel and/or other data manipulation tools\\nFamiliarity with data visualization tools (e.g. Looker, Tableau)\\nProgramming experience with R/Python for data analysis preferred\\n</td>\n",
       "      <td>Assist in the design, preparation and distribution of reports and dashboards for end-users, management, and key stakeholders.\\nUse statistical methods to ensure metrics are well defined and match to the customer’s goals and desired outcomes documented during the discovery process.\\nSupport the standardization of reporting for end-users.\\nServe as internal expert user of the reporting system for input to the ongoing improvement and development of reporting tools.\\nCommunicate reporting enhancements to internal and external users through presentations and documentation.\\nSeek and analyze trends and patterns across communities, populations, and contacts to assist in product management and development.\\n</td>\n",
       "      <td>Bachelor’s degree in Engineering, Mathematics, Computer Science or related field\\nClinical and healthcare relevance; a Clinical degree isn’t required, but a deep understanding of the healthcare market is.\\n</td>\n",
       "      <td>Bachelor’s degree in Engineering, Mathematics, Computer Science or related field\\nClinical and healthcare relevance; a Clinical degree isn’t required, but a deep understanding of the healthcare market is.\\n</td>\n",
       "      <td>Position Overview:\\nData and human connection come together in our mission to bring communities together to collaborate and solve Social Determinants of Health – a career at Signify Community (a subsidiary of Signify Health) is a career with purpose. Our employees are empathic, passionate, confident, innovative change agents who aren’t afraid to take risks and also have fun.\\nReporting to the Director of Data Intelligence, the Reporting and Data Engineer is responsible for building the tools and reports that enable us to discover and share insights about our customers, as well as identifying system and workflow enhancements to Signify Community’s platform. The Reporting and Data Engineer will uncover novel ways of connecting, comparing and using information from across communities, populations and contacts to provide key input for product development and management.\\nThe ideal candidate thrives on the opportunity to collect and assimilate data and has a passion to collaborate closely with other analysts, engineers, and customer facing teams to drive decision-making and build innovative products that help people live better lives.\\nIf this sounds like an exciting opportunity, and you are interested in realizing our mission to bring communities together to collaborate, address unmet social needs, and improve outcomes, contact us to learn more!\\nQualifications:\\nEducation/Licensing Requirements:\\nBachelor’s degree in Engineering, Mathematics, Computer Science or related field\\nClinical and healthcare relevance; a Clinical degree isn’t required, but a deep understanding of the healthcare market is.\\n\\nExperience Requirements:\\n2+ years of experience with healthcare data.\\n2+ years of data analysis\\n\\nEssential Skills/Experience:\\nSolid knowledge of SQL queries and relational databases\\nGood understanding of math and statistics\\nDeep experience with Excel and/or other data manipulation tools\\nFamiliarity with data visualization tools (e.g. Looker, Tableau)\\nProgramming experience with R/Python for data analysis preferred\\n\\nEssential Characteristics:\\nAccuracy/Attention to Detail - Diligently attends to details and pursues quality in accomplishing tasks.\\nAnalysis/Reasoning - Examines data to grasp issues, draw conclusions, and solve complex business problems.\\nCritical Listening - Makes a concerted effort to listen to what people are saying and what they are trying to say; thinks carefully before responding and avoids jumping to conclusions; asks thoughtful questions to elicit more information; maintains positive and supportive demeanor to allow meaningful conversation.\\nAdaptability - Maintains job performance and focus under pressure; adapts to changing needs; able to respond appropriately to new information and schedule changes; handles stress in an appropriate manner; uses appropriate skills, tools, and techniques to manage time when accomplishing specific tasks.\\nOrganization/Time Management - Establishes course of action for self and others to ensure that the work is completed efficiently, without jeopardizing excellent service; prioritizes tasks appropriately; adjusts priorities when appropriate; able to stay focused on the task at hand; allocates appropriate amount of time for completing own work; quickly able to determine roadblocks to accomplishing a goal.\\nEssential Job Responsibilities:\\n\\nTo perform this job successfully, an individual must be able to perform each essential function satisfactorily, with or without reasonable accommodation.\\nAssist in the design, preparation and distribution of reports and dashboards for end-users, management, and key stakeholders.\\nUse statistical methods to ensure metrics are well defined and match to the customer’s goals and desired outcomes documented during the discovery process.\\nSupport the standardization of reporting for end-users.\\nServe as internal expert user of the reporting system for input to the ongoing improvement and development of reporting tools.\\nCommunicate reporting enhancements to internal and external users through presentations and documentation.\\nSeek and analyze trends and patterns across communities, populations, and contacts to assist in product management and development.\\n\\nWorking Conditions:\\nAbility to work well in a fast-paced environment.\\nEnjoys working with people to address their needs, and being a partner in achieving good health.\\nAbility to work at a desk and to use a phone and computer.\\nAbility to use office equipment and machinery effectively.\\nAbility to work effectively with frequent interruptions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Principal Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nLead efforts to design, build, scale, and maintain multiple data pipelines\\nArchitect highly scalable data solutions\\nBe a technical thought leader within the org\\nWork closely with business owners and external stakeholders to provide actionable data\\nEnsure data accuracy and reliability\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience building large scale streaming and batch data pipelines\\nExperience using Big Data technologies (Spark, EMR, hadoop, data lakes, etc.)\\nMastery of multiple databases (e.g. MongoDB, MySQL, etc.)\\nUnderstanding of data security best practices\\n</td>\n",
       "      <td>Crowdskout is looking for a Lead Data Engineer that can both architect and lead efforts around our expanding data pipeline infrastructure. Crowdskout's product has most recently been centered in the CRM space, but we are looking to expand far beyond that. Currently, we process millions of data points through multiple data pipelines to feed a suite of datastores and applications. We are preparing for +10x growth both in the volume of data processed and the speed in which that data can be available and actionable. To accomplish this we are looking for someone who can architect and lead the effort to build out highly scalable data solutions.\\n\\nIf you are highly motivated, super passionate about democracy, and want to join a close-knit team that is looking to build great things together, Crowdskout may be for you. This is a full-time position in Austin, TX.\\n\\nResponsibilities:\\n\\nLead efforts to design, build, scale, and maintain multiple data pipelines\\nArchitect highly scalable data solutions\\nBe a technical thought leader within the org\\nWork closely with business owners and external stakeholders to provide actionable data\\nEnsure data accuracy and reliability\\n\\nRequirements:\\n\\nExperience building large scale streaming and batch data pipelines\\nExperience using Big Data technologies (Spark, EMR, hadoop, data lakes, etc.)\\nMastery of multiple databases (e.g. MongoDB, MySQL, etc.)\\nUnderstanding of data security best practices\\n\\nExtras:\\n\\nAWS data technologies (e.g. Kinesis, Glue, RDS, Athena, Redshift, etc.)\\nExperience building out data warehouse infrastructure\\nDevOps or System Admin experience\\nData Science exploration and modeling\\n\\nCrowdskout is an equal opportunity employer that encourages diversity across all spectrums in its hiring, without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, or any other protected factor. With that being said, we wouldn't be able to accommodate candidates in need of work sponsorship at this time since we are a small company. If you find this role interesting and you hit on the elements above, please apply!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>ABOUT THIS ROLE\\nAs a member of our engineering team, you will work on the design, implementation and delivery of data platform frameworks, pipelines, microservices, and other features that build on our high-value data assets. Your customers will include data analytics, marketing, and leadership teams as well as our Care Partners (hospitals and physicians) and external data partners, in concert with our Enterprise Data Warehouse delivery teams.\\nRESPONSIBILITIES\\nEvaluate and build proofs of concept for Cloud PaaS and IaaS offerings for data liquidity, data management and storage, data pipelines built on both traditional ETL as well as streaming platforms, master data management, data stewardship, record-linkage, NLP services, and more.\\nWrite traditional code and server-less functions using the right language for the task, which may be SQL, Python, C#, Java, PowerShell, SSIS/BIML, and others.\\nParticipate in build-buy-open source decisions for parsing and managing industry standard formats such as FHIR/NDJSON, pipe-and-hat HL7, and x12 EDI\\nEvaluate, select, and apply Cloud and OO design and resiliency patterns\\nBuild APIs and data microservices to share our data with internal and external partners, and write interfaces to public data sets to enrich our analytics data stores\\nProvide subject matter expertise on performance tuning and query optimization to full-stack peers, data analysts, and EDW developers\\nParticipate in building and owning a DevOps culture\\nContinuously document your code, framework standards, and team processes\\nEDUCATION, TRAINING, AND PROFESSIONAL EXPERIENCE\\n1 - 5 years of experience in an enterprise or commercial software development environment\\nExtensive experience developing data-intensive solutions against an RDBMS, such as SQL Server, Postgres or Oracle.\\nHighly skilled writing SQL queries, DML and DDL, CDC/change tracking patterns, indexes and performance tuning.\\nEnterprise development experience coding in at least one, but preferably more than one, procedural/OO language, e.g., C#, Java, JavaScript, Python, C++, PowerShell\\nProficiency in using OOTB components, as well as implementing custom components or frameworks, on at least one traditional ETL platform, preferably SSIS, Informatica, or Talend\\nTeam player who is not afraid to ask questions, take risks, share in owning team victories as well as team failures\\nGood communicator – both written and verbal – with high emotional intelligence\\nAbility to focus on MVP and shipping software while remaining cognizant of the long-term costs of technical debt\\nHealthcare data background a must\\nMUST HAVE THE RIGHT TO WORK IN THE US WITHOUT VISA SPONSORSHIP\\nIdeal candidates come to the table with one or more additional competencies, such as:\\nExposure to Enterprise Data Warehouse , Data Lake, Big Data, unstructured data, in-memory data stores\\nFamiliarity with NoSQL database systems such as MongoDB, Cassandra, CosmosDB, neo4j etc.\\nFamiliarity with Kimball-like star and snowflake data models and columnstore Indexing\\nExperience building metadata-driven data pipeline frameworks for quickly mapping, onboarding, and ingesting data from a wide variety of partner sources\\nEnterprise experience with data movement and management in the Cloud utilizing some combination of Azure and/or AWS features such as Data Factory, Blob Storage, Service Bus, Kafka, Redis, S3 Buckets, Azure Automation, Machine Learning, elastic search, Glue etc.\\nData Science training or experience to better understand and collaborate with one of our key data consumers (notably, this is still an engineering role and not a data science role)\\nCRM experience, such as MS Dynamics or SalesForce\\nABOUT US\\n\\nAt Bright Health, we brought together the brightest minds from the health care industry and consumer technology and together we created Bright Health: a new, brighter approach to healthcare, built for individuals. Our plans are easy to manage, personalized and more affordable, giving people the quality care they deserve. Through our exclusive care partnerships with leading health systems in local communities we are reshaping how people and physicians achieve better health together.\\n\\nBright Health is tripling its footprint in 2019 to offer a variety of health insurance plans to more individuals. Bright Health operates health insurance offerings across Individual and Family Plan segments and the Medicare Advantage space in Alabama, Arizona, Colorado, Ohio, New York and Tennessee.\\n\\nWe’re Making Healthcare Right. Together.\\n\\nWe've won some fun awards like: Great Places to Work, Modern Healthcare, Forbes, etc. But more than anything, we're a group of people who are really dedicated to our mission in healthcare. Come join our growing team!\\n\\nAs an Equal Opportunity Employer, we welcome and employ a diverse employee group committed to meeting the needs of Bright Health, our consumers, and the communities we serve. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Training Solutions Advisor, Google Cloud</td>\n",
       "      <td>Austin, TX 78731</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78731</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nUnderstand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.\\nConnect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.\\nPartner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.\\nSupport escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.\\nPartner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Note: By applying to this position your application is automatically submitted to the following locations: Sunnyvale, CA, USA; Austin, TX, USA; New York, NY, USA; Reston, VA, USA\\nMinimum qualifications:\\n\\nBachelor's degree in Computer Science or a related technical field, or equivalent practical experience.\\n5 years of experience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.\\nExperience working in a customer facing environment in a technology company and helping customers identify solutions that best fit their unique needs.\\nAbility to travel to support Customer Engagements up to 30% of the time.\\n\\nPreferred qualifications:\\n\\nGoogle Cloud Certified e.g. Cloud Architect, Data Engineer or Associate Cloud Engineer or other comparable Cloud Certification.\\nExperience working as a Technical Trainer, Training Consultant/Advisor in a Technology firm or as a Customer Engineer who has led training and Certification discussions with customers.\\nAbility to take customers’ technical requirements and architect a proposal that maps to technical skills required.\\nAbility to quickly learn and understand new training offerings.\\nAbility to work well cross functionally and understand when to pull in specialist knowledge into Customer conversations.\\nExcellent communication and presentation skills.\\nAbout the job\\nThe Google Cloud team helps customers transform and evolve their business through the use of Google’s global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you'll help shape the future of businesses of all sizes and enable them to better use technology to drive innovation.This role will enable you to make a huge impact across Google Cloud’s most strategic accounts and ensure they have Learning Plans that effectively help them develop the knowledge and skills they need to adopt Google Cloud. The role is also an exciting mix of elements as you will be working in Technical Customer Facing activities (usually in partnership with the CE or PSO/TAM Account owner), working in partnership with Cloud Learning GTM leads on Learning Plan development, and working with the Curriculum Tech leads to provide curriculum feedback and validate proposals as required.\\n\\nGoogle Cloud helps millions of employees and organizations empower their employees, serve their customers, and build what’s next for their business — all with technology built in the cloud. Our products are engineered for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. And our teams are dedicated to helping our customers and developers see the benefits of our technology come to life.\\nResponsibilities\\nUnderstand mix of products and roles behind Google Cloud Platform’s curriculum including understanding each module of each course.\\nConnect clients’ business priorities, challenges, and initiatives with actionable training plans to fill skills gaps and build expertise of Google Cloud Platform. Conduct organizational needs-analysis/training scoping sessions with customers and create a training proposal that is customized to the customers’ needs.\\nPartner with trainers who will be delivering training into the account to ensure they are fully briefed re: customer requirements and what preparation is required to successfully deliver.\\nSupport escalations for onsite training classes where students’ expectations do not match original plans outlined in the training proposal.\\nPartner with Google Cloud’s Curriculum and Content team to share insights from the field and feedback on training offerings.\\nAt Google, we don’t just accept difference—we celebrate it, we support it, and we thrive on it for the benefit of our employees, our products and our community. Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing this form.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExperience writing software in one or more languages such as Python, Java, Scala, or Go\\nExperience building production-grade data solutions (relational and NoSQL)\\nExperience with systems monitoring/alerting, capacity planning and performance tuning\\nExperience in technical consulting or customer-facing role\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join SADA as a Sr. Data Engineer!\\n\\nYour Mission\\n\\nAs a Sr. Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes and data warehouses.\\n\\nYou will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as lead client-facing technical discussions for established projects.\\n\\nPathway to Success\\n\\n#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.\\n\\nYour success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.\\n\\nExpectations\\n\\nRequired Travel - 30% travel to customer sites, conferences, and other related events\\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.\\nTraining - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Data Engineer Certified\\n\\n[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment\\n\\nRequired Qualifications:\\n\\nMastery in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExperience writing software in one or more languages such as Python, Java, Scala, or Go\\nExperience building production-grade data solutions (relational and NoSQL)\\nExperience with systems monitoring/alerting, capacity planning and performance tuning\\nExperience in technical consulting or customer-facing role\\n\\nUseful Qualifications:\\n\\nExperience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)\\nExperience with IoT architectures and building real-time data streaming pipelines\\nExperience operationalizing machine learning models on large datasets\\nHihg\\nDemonstrated leadership and self-direction -- a willingness to teach others and learn new techniques\\nDemonstrated skills in selecting the right statistical tools given a data analysis problem\\n\\nAbout SADA\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Principal Data Engineer</td>\n",
       "      <td>Austin, TX 78723</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78723</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Develop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server\\nPlay a lead role in migrating data from legacy systems to cloud platforms\\nCreate custom data sets for use by business analysts and data scientists\\nRapidly prototype new data sets for exploratory analysis\\nUse SQL skills to manage data\\nMonitor database performance and tuning to improve query performance\\nDesign and automate data pipelines to integrate different data sources using SSIS\\nCollaborate with IT partners to move data prototypes into production\\nDevelop real-time data integrations in MS SQL Server\\nEstablish techniques to monitor data quality and implement remediation procedures\\nWorking directly with non-technical users to identify complex needs/requirements and translate into technical solutions\\nDemonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler\\nCreate data training programs\\nTrain analyst community on data sources and best practices\\nLead other staff on SSIS, performance tuning, and database administration\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Develop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server\\nPlay a lead role in migrating data from legacy systems to cloud platforms\\nCreate custom data sets for use by business analysts and data scientists\\nRapidly prototype new data sets for exploratory analysis\\nUse SQL skills to manage data\\nMonitor database performance and tuning to improve query performance\\nDesign and automate data pipelines to integrate different data sources using SSIS\\nCollaborate with IT partners to move data prototypes into production\\nDevelop real-time data integrations in MS SQL Server\\nEstablish techniques to monitor data quality and implement remediation procedures\\nWorking directly with non-technical users to identify complex needs/requirements and translate into technical solutions\\nDemonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler\\nCreate data training programs\\nTrain analyst community on data sources and best practices\\nLead other staff on SSIS, performance tuning, and database administration\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At Texas Mutual, we're creating a stronger, safer Texas. Here, you will explore and create new data sets and work with business and IT partners to support our corporate data strategy. Join one of the best companies to work for in Texas, reporting into the Chief Data Office. Located in the Mueller development, we offer modern sit/stand workstations, free on-site fitness center, and free garage parking.\\nResponsibilities &amp; Qualifications\\nIn this Role:\\nDevelop strategies for data models, automated ETL processes, stored procedures, and views in MS SQL Server\\nPlay a lead role in migrating data from legacy systems to cloud platforms\\nCreate custom data sets for use by business analysts and data scientists\\nRapidly prototype new data sets for exploratory analysis\\nUse SQL skills to manage data\\nMonitor database performance and tuning to improve query performance\\nDesign and automate data pipelines to integrate different data sources using SSIS\\nCollaborate with IT partners to move data prototypes into production\\nDevelop real-time data integrations in MS SQL Server\\nEstablish techniques to monitor data quality and implement remediation procedures\\nWorking directly with non-technical users to identify complex needs/requirements and translate into technical solutions\\nDemonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler\\nCreate data training programs\\nTrain analyst community on data sources and best practices\\nLead other staff on SSIS, performance tuning, and database administration\\nRequired Qualifications:\\nAt least 10 years of related data experience\\nBachelor's degree in a related field\\nExperience in the design, development, implementation and support of SQL Server\\nExperience with SSIS, Alteryx or similar ETL tools ﻿﻿ ﻿﻿\\nOur Benefits:\\nDay one health, dental, and vision insurance\\nPerformance bonus\\n401k plan with 4% basic employer contribution and 100% employer match contribution up to 6%\\nVacation, sick, holiday and volunteer time off\\nLife and disability insurance\\nFlexible spending account\\nFree on-site gym and fitness classes\\nProfessional development\\nTuition reimbursement\\nPet insurance\\nFree identity theft protection\\nCompany-sponsored social and philanthropy events\\nTexas Mutual Insurance Company is an Equal Employment Opportunity employer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Engineer – Elastic Engineer</td>\n",
       "      <td>Austin, TX 73344</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>73344</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Introduction\\nAt IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.\\n\\nYour Role and Responsibilities\\nWe are looking for a Data engineer to deploy complex big data and analytics solutions using Elastic stack. You should have strong experience in deploying and managing Elastic cluster on Kubernetes in multi-site, multi cluster environment in both on-Premise as well as Cloud platforms. You should have applied or expert knowledge in big data platforms. The main use case for such a platform for us would be Real-Time Anomaly Detection and Time Series models on IT operations data like logs, metrics, events, wired data, transaction flow, ITIL process related data, knowledge repositories, etc\\nBusiness Unit/ Team Overview\\nGlobal Technology Services (GTS) at IBM manages the IT infrastructure for some of the world’s leading corporations and with that comes the responsibility of managing enormous amounts of IT data and the opportunity for making better decisions using that data. In GTS analytics team at IBM, Data Scientists, Data Engineers, and BigData IT Architects are developing novel models, cutting edge algorithms, and custom analytics solutions to tackle BigData challenges in the IT Infrastructure space.\\nITOA / AIops provides real time machine-data (log, events, performance, capacity, ITIL data, wire data, etc) analytics solutions that helps customers manage Business Services and manage the quality of the end-user experience\\nIt can tell a client in real time ‘What happened’, ‘Why did it happen’, ‘Will it happen again’ and ‘What to do if it happens again? etc\\nKeeps everyone on the same page by looking at the same Business Transaction data and metrics.\\nKeeps the focus on operational data that translate to the business value the application delivers; dive in deeper when appropriate.\\nIdentify resolution criteria, assign ownership\\nTake lessons learned to improve development, test, deployment, and production processes\\nEducation &amp; Experience\\nMinimum 4 years of relevant experience working on the Elastic based products &amp; distributions specifically used in Real Time ITOA or AIops use-cases processing logs, metrics, events, etc\\nAt least 4 years of experience in development &amp; implementation of logging and metrics solutions in with TB+ / day ingestion per day\\nAt least 5 years of hands-on experience in IT support (Infrastructure / Application) and IT monitoring tools\\nOverall 7+ years of core Big data / Analytics experience in various domains\\nDegree / Master’s degree in computers or equivalent\\nCertifications:\\nElastic certified engineer\\nCertifications showing proficiency in the Usage, design &amp; deployment of ITOA / AIOps solutions like Elastic or Splunk\\nSpecialized certifications on specific technologies like Hadoop, Cloudera, Spark, Kafka, etc\\nJob Responsibilities\\nDeploy Elastic stack cluster on native kubernetes or Kubernetes services and maintain the clusters efficiently\\nLeading end to end deployment of ITOA / AIOps solutions for enterprise customers\\nProvide engineering inputs to Architects and Data scientists on various stages of solution design\\nPerform Integration and deployment of ITOA solution as per design provided by Architects\\nParticipate &amp; be an active member of internal capability building projects\\nTrain &amp; support junior resources as needed\\nProvide resolution to customer queries and issues\\nSkills Required:\\nExcellent knowledge on log analytics, time series data anomaly detection and correlation of events\\nHands-on experience with IT operational data like logs, metrics, events, RDBMS tables, etc and ingesting them into Elastic stack\\nExpert Knowledge on GO/grok/REGEX/Logstash/Fluentd to perform Extract, Transform and Load for IT operational data into big data repositories like Elasticsearch, Cassandra, Hadoop, etc\\nExpert level experience in managing large Elastic cluster and in-depth knowledge in Elastic features\\nAlerting\\nSecurity\\nCurator\\nReporting\\nMonitoring\\nBackup and resiliency\\nKubernetes cluster management\\nPython/R/Scala languages/Scripting Languages in context of Anomaly Detection &amp; Time Series modelling\\nWorking experience with ITIL Framework\\nWorking knowledge on Apache Hadoop, Spark, Airflow, Cassandra and Kafka ecosystem\\nPrior experience in deploying Elastic solutions in production environments processing operational data in terms of at least 500 GB / day\\nExperience with SQL based tools &amp; expertise on any one traditional RDBMS – mySQl; MSSQL;Oracle;DB2 etc\\nPrior experience with DevOps projects, Github, Jira, Travis, etc\\nWorking knowledge on Windows, Linux and AIX platforms\\nWorking knowledge of Top commercial distributions of the above stack – MapR; Cloudera; Hortonworks etc\\nKnowledge on shell scripting\\nGood knowledge on other Top level Apache Big Data technologies like Cassandra, NIFI, Fluentd, Drill, Sentry etc\\nExcellent understanding on HDFS &amp; other similar Map/Reduce paradigms\\nKnowledge on 1-2 NoSQL databases – Redis; MongoDB;Cassandra;Neo4j; VoltDB etc\\nPreferred:\\nExperience with Elastic ML and real-time operations analytics using Apache suit of products like Spark using Python or Scala\\nExperience with Kibana plug-in development and other UI development\\nExperience working with large data sets leveraging distributed systems e.g. Spark/Hadoop.\\nTools &amp; Methods (Experience in at least one in each category or similar if not listed below)\\nLog Analytics – Elastic Search, Apache Solr\\nData Pipelines: Logstash, Fluentd, Kafka, Nifi\\nLanguages: Python, PySpark, Spark, Scala, R, Java, Java Script\\nVisualization : Kibana, Tableau, Cognos\\nMachine learning – Elastic ML, Python, Spark, Tensorflow, H2O\\nStreaming: Spark; Storm;\\nRelational Database technologies: Oracle, Db2, SQL, MySQL,\\nNoSQl DB’s: MongoDB, Cassandra, Neo4J,Redis, VoltDB, CouchDB\\nApache Hadoop Distribution – Apache, Hortonworks, Cloudera, MapR\\nETL technologies: Datastage, Informatica, Pentaho DI, SAS DI, SSIS or R, Python based Data munging\\nCloud technologies: AWS, Azure, IBM Softlayer\\nSoft Skills\\nExcellent Written &amp; Verbal Communication\\nExcellent Analytical &amp; Virtual troubleshooting skills\\nSkills to work in team and collaborative environment\\nCustomer/Vendor interaction &amp; co-ordinations\\n\\nRequired Technical and Professional Expertise\\n7+ years of professional hands on experience in IT operations\\nExcellent knowledge on log analytics, time series data anomaly detection and correlation of events\\nHands-on experience with IT operational data like logs, metrics, events, RDBMS tables, etc and ingesting them into Elastic stack\\nExpert Knowledge on GO/grok/REGEX/Logstash/Fluentd to perform Extract, Transform and Load for IT operational data into big data repositories like Elasticsearch, Cassandra, Hadoop, etc\\nExpert level experience in managing large Elastic cluster and in-depth knowledge in Elastic features\\nPython/R/Scala languages/Scripting Languages in context of Anomaly Detection &amp; Time Series modelling\\nWorking experience with ITIL Framework\\nWorking knowledge on Apache Hadoop, Spark, Airflow, Cassandra and Kafka ecosystem\\nPrior experience in deploying Elastic solutions in production environments processing operational data in terms of at least 500 GB / day\\n\\n\\nPreferred Technical and Professional Expertise\\nExperience with SQL based tools &amp; expertise on any one traditional RDBMS – mySQl; MSSQL;Oracle;DB2 etc\\nPrior experience with DevOps projects, Github, Jira, Travis, etc\\nWorking knowledge on Windows, Linux and AIX platforms\\nWorking knowledge of Top commercial distributions of the above stack – MapR; Cloudera; Hortonworks etc\\nKnowledge on shell scripting\\nGood knowledge on other Top level Apache Big Data technologies like Cassandra, NIFI, Fluentd, Drill, Sentry etc\\nExcellent understanding on HDFS &amp; other similar Map/Reduce paradigms\\nKnowledge on 1-2 NoSQL databases – Redis; MongoDB;Cassandra;Neo4j; VoltDB etc\\n\\n\\n\\nAbout Business Unit\\nAt Global Technology Services (GTS), we help our clients envision the future by offering end-to-end IT and technology support services, supported by an unmatched global delivery network. It's a unique blend of bold new ideas and client-first thinking. If you can restlessly reinvent yourself and solve problems in new ways, work on both technology and business projects, and ask, \"What else is possible?\" GTS is the place for you!\\n\\nYour Life @ IBM\\nWhat matters to you when you’re looking for your next career challenge?\\n\\nMaybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities – where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust – where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.\\n\\nImpact. Inclusion. Infinite Experiences. Do your best work ever.\\n\\nAbout IBM\\nIBM’s greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.\\n\\nLocation Statement\\nFor additional information about location requirements, please discuss with the recruiter following submission of your application.\\n\\nBeing You @ IBM\\nIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBuild and Support scalable and reliable data solutions that can enable self-service reporting and advanced analytics at Cloudflare using modern data lake and EDW technologies (Hadoop, Spark, Cloud, NoSQL etc.) in a agile manner.\\nStrong understanding of business and product data needs.\\nClose partnership with internal stakeholders and partners from Engineering, product, and business(Finance, Sales, Customer Experience, Marketing etc.).\\nActive role in hiring and growing the team in Austin with data Engineers, analysts, and data scientists.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor's or Master's Degree in Computer Science or Engineering or related experience required.\\n3+ years of development experience in Big data space working with Petabytes of data and building large scale data solutions.\\nSolid understanding of Google Cloud Platform, Hadoop, Python, Spark, Hive, and Kafka.\\nExperience in all aspects of data systems(both Big data and traditional) including data schema design, ETL, aggregation strategy, and performance optimization.\\nCapable of working closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality.\\nExperience in hiring data Engineers preferred.\\nExperience in Internet security industry preferred.\\n</td>\n",
       "      <td>About Us\\n\\nAt Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world's largest networks that powers trillions of requests per month. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare have all web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was recognized by the World Economic Forum as a Technology Pioneer and named to Entrepreneur Magazine's Top Company Cultures list.\\n\\nWe realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!\\n\\nAbout the department\\n\\nCloudflare is looking to build and grow our Business Intelligence team responsible for building large-scale enterprise data lake and EDW from different sources and enabling various product and business teams such as Marketing, Customer Support, Sales, Finance with key business dashboards/reporting, insights and recommendation models.\\n\\nAbout the role\\n\\nAs part of this initiative, we are looking for a Big Data(EDW/Analytics) Engineers to come join Cloudflare and help us build a scalable petabyte scale data lake and EDW using modern tech stack from the ground up. Success in this role comes from marrying a strong data engineering background with product and business acumen to deliver scalable data pipeline and BI solutions that can enable self-service analytics at Cloudflare in a simple and standard manner. This person will also play a crucial role in hiring and growing the business intelligence team in Austin in a rapid manner.\\n\\nWhat we look for: Agile Delivery &amp; Execution, Engineering Excellence, Tech Savvy, Business &amp; Product acumen, Creative Problem solver\\n\\nResponsibilities:\\n\\nBuild and Support scalable and reliable data solutions that can enable self-service reporting and advanced analytics at Cloudflare using modern data lake and EDW technologies (Hadoop, Spark, Cloud, NoSQL etc.) in a agile manner.\\nStrong understanding of business and product data needs.\\nClose partnership with internal stakeholders and partners from Engineering, product, and business(Finance, Sales, Customer Experience, Marketing etc.).\\nActive role in hiring and growing the team in Austin with data Engineers, analysts, and data scientists.\\n\\nRequirements:\\n\\nBachelor's or Master's Degree in Computer Science or Engineering or related experience required.\\n3+ years of development experience in Big data space working with Petabytes of data and building large scale data solutions.\\nSolid understanding of Google Cloud Platform, Hadoop, Python, Spark, Hive, and Kafka.\\nExperience in all aspects of data systems(both Big data and traditional) including data schema design, ETL, aggregation strategy, and performance optimization.\\nCapable of working closely with business and product teams to ensure data solutions are aligned with business initiatives and are of high quality.\\nExperience in hiring data Engineers preferred.\\nExperience in Internet security industry preferred.\\n\\nWhat Makes Us Special\\n\\nWe're not just a highly ambitious, large-scale technology company. We're a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.\\n\\nProject Galileo ( https://blog.cloudflare.com/protecting-free-expression-online/ ): We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare's enterprise customers--at no cost.\\n\\nProject Athenian ( https://www.cloudflare.com/athenian/ ): We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.\\n\\nPath Forward Partnership ( https://blog.cloudflare.com/tag/path-forward/ ): Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.\\n\\n1.1.1.1 ( https://1.1.1.1/ ): We released 1.1.1.1 ( https://1.1.1.1/ ) to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here's the deal - we don't store client IP addresses never, ever. We will continue to abide by our privacy policy ( https://developers.cloudflare.com/1.1.1.1/commitment-to-privacy/privacy-policy/privacy-policy/ ) and ensure that no user data is sold to advertisers or used to target consumers.\\n\\nSound like something you'd like to be a part of? We'd love to hear from you!\\n\\nCloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.\\n\\nCloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>6+ years of professional experience as a data engineer or in a similar role\\n4+ years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL\\nHands-on experience writing SQL, Perform SQL optimization and tuning\\nStrong work experience with Python scripting or equivalent\\nWorking knowledge with AWS RDS, Data Lakes and data analytics\\nMust be able to work in a diverse team environment\\nMust possess problem-solving skills and ability to multitask\\nOutstanding analytical skills and problem-solving abilities, drive for results, attention to quality and detail, and a collaborative attitude\\nBachelor’s degree in Computer Science or Engineering, or equivalent experience\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overall responsibility for day-to-day data pipeline operations and manage database acquisition and data delivery methods effectively\\nServe as a lead data engineer to manage the development of web services for data posting and delivery\\nProvide technical leadership and expertise on data integration and data delivery\\nPerform data quality procedures to ensure data consistency and data integrity\\nEnsure proper documentation of data posting and related API objects\\nWork directly with product and engineering team to understand data needs and provide end-to-end data solutions that address customer requirements\\nPerform other duties as assigned\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>6+ years of professional experience as a data engineer or in a similar role\\n4+ years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL\\nHands-on experience writing SQL, Perform SQL optimization and tuning\\nStrong work experience with Python scripting or equivalent\\nWorking knowledge with AWS RDS, Data Lakes and data analytics\\nMust be able to work in a diverse team environment\\nMust possess problem-solving skills and ability to multitask\\nOutstanding analytical skills and problem-solving abilities, drive for results, attention to quality and detail, and a collaborative attitude\\nBachelor’s degree in Computer Science or Engineering, or equivalent experience\\n</td>\n",
       "      <td>About the Role:\\nAs the Data Engineer, you will join our engineering team and build world-class data solutions for RealMassive. You will have oversight of the commercial real estate data pipeline process and develop best practices and recommendations for the data acquisition and data delivery method for the company. You will be the expert on the entire data solution from data collection, transformation, ingestion, and data delivery.\\nThis role reports directly to the VP of Engineering and is located in Austin, TX.\\n\\nResponsibilities:\\nOverall responsibility for day-to-day data pipeline operations and manage database acquisition and data delivery methods effectively\\nServe as a lead data engineer to manage the development of web services for data posting and delivery\\nProvide technical leadership and expertise on data integration and data delivery\\nPerform data quality procedures to ensure data consistency and data integrity\\nEnsure proper documentation of data posting and related API objects\\nWork directly with product and engineering team to understand data needs and provide end-to-end data solutions that address customer requirements\\nPerform other duties as assigned\\nRequirements\\nRequired Qualifications:\\n6+ years of professional experience as a data engineer or in a similar role\\n4+ years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL\\nHands-on experience writing SQL, Perform SQL optimization and tuning\\nStrong work experience with Python scripting or equivalent\\nWorking knowledge with AWS RDS, Data Lakes and data analytics\\nMust be able to work in a diverse team environment\\nMust possess problem-solving skills and ability to multitask\\nOutstanding analytical skills and problem-solving abilities, drive for results, attention to quality and detail, and a collaborative attitude\\nBachelor’s degree in Computer Science or Engineering, or equivalent experience\\nPreferred Qualifications:\\nKnowledge of advanced analytics tools and Agile development methodologies\\nAWS RDS PostgreSQL skills and experience is a plus\\nExperience working with AWS Glue, EMR, and Kinesis\\nExperience with data science and machine learning\\nWorking knowledge with big data and advanced programming languages\\nExperience in working with data visualization tools such as Tableau\\nBenefits\\nBest-in-class benefits: medical, dental, and vision\\nCompany sponsored long-term disability\\nUnlimited vacation\\nMaternity/paternity leave\\nFlexible working options\\nSuite of optional benefits include HSA, FSA, and short-term disability\\nEnjoy catered lunch several times per week + snacks + fancy coffees\\nWhy join us:\\nAt RealMassive, our strength is our knowledgeable, passionate, and creative people. We are commercial real estate’s only real-time, big data solution and property marketplace dedicated to providing innovative technology solutions to modernize a $15 Trillion industry. Our products are designed to break down the barriers that have traditionally isolated data and insights from those who benefit the most from an open data environment. We strive to make the lives of our customers easier through information and access, which in turn helps local communities around the country grow and thrive.\\nWhether you work in engineering, data science, product, sales or operations, at RealMassive, we are all focused on the same mission and vision – to be the world’s source of commercial real estate data and insights. We are focused, dedicated and committed to improving the lives of our customers by staying experimental, agile and innovative.\\nOur culture is built on inclusion, integrity and ideation. We value the voices, thoughts and passions of every employee and believe that success is collaborative, not individual. When you join RealMassive, you’ll instantly help our customers make better decisions, create opportunities, move faster and improve the commercial real estate transaction experience.\\nAbout RealMassive:\\nRealMassive, commercial real estate’s real-time data provider and marketplace, is the industry’s source for searching, finding and evaluating commercial property and markets around the country. The marketplace makes it easy for business owners, entrepreneurs, brokers, developers and investors to find the perfect property to start and grow their business in small rural communities or major metropolitan areas.\\nWith innovative data solutions, RealMassive is the industry’s largest data source of commercial property data that is updated and verified weekly. The data collected, standardized and enriched by RealMassive provides our customers with faster, more effective information to make decisions relating to leasing, acquiring and dispositioning commercial properties.\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\\nHow to apply:\\nIf your career preference is to work in a fast-paced entrepreneurial environment, where your success is measured by your contribution to the team—we should talk.\\nPlease complete the online application as well as the behavioral assessment at the following link: https://assess.predictiveindex.com/Gvjan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sr. Data Engineer - Payments Systems Risk</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMinimum of 2-3 years’ experience in production ETL pipelines, utilizing big data engineering techniques that enable statistical solutions to solve business problems\\nPost graduate degree in Computer Science/ Engineering, Information Science or a related discipline with strong technical experiences highly desired\\nPrevious exposure to financial services, credit cards or merchant analytics is a plus, but not required\\nExtensive experience with SQL and big data technologies (Hadoop, Python , Java, Spark, Hive etc.) tools for large scale data processing, data transformation and machine learning pipelines\\nExperience with data visualization and business intelligence tools like Tableau, Microstrategy, or other programs highly desired\\nExperience with SAS as a statistical package is preferred\\nFamiliarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\n\\nTo ensure that Visa’s payment technology is truly available to everyone, everywhere requires the success of our key bank or merchant partners and internal business units. The Global Data Science group supports these partners by using our extraordinarily rich data set that spans more than 3 billion cards globally and captures more than 100 billion transactions in a single year.\\nAre you skilled at turning hard numbers into compelling stories and useful strategic insights? Do you solve complex data challenges with creative flair? Put those skills to work answering strategic questions for one of the world's most respected and innovative payments companies.\\nAs a Senior Data Engineer, you will be responsible for helping to blueprint and deliver modelled attributes, data assets, and self-serve workflows that solve clients' business objectives. You will get the chance to leverage your business acumen and technical knowledge of big data and data mining techniques. Based on deep understanding and knowledge of big-data engineering techniques, you will develop and maintain data and tools to enable data scientists to draw fact based insights and build models This function is critical in building market-relevant client solutions and intellectual property for Visa.\\nEssential Functions\\nWork with manager and clients to fully understand business requirements and desired business outcomes\\nAssist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions\\nBuild and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa’s data scientists\\nPerform other tasks on R&amp;D, data governance, system infrastructure, analytics tool evaluation, and other cross team functions, on an as-needed basis\\nFind opportunities to create, automate and scale repeatable analyses or build self-service tools for business users\\nExecute data engineering projects ranging from small to large either individually or as part of a project team\\nEnsure project delivery within timelines and budget requirements\\nProvide coaching and mentoring to junior team members\\n\\nQualifications\\n\\nBasic Qualifications:\\n2 years of work experience with a Bachelor’s Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD, or PhD)\\nPreferred Qualifications:\\nMinimum of 2-3 years’ experience in production ETL pipelines, utilizing big data engineering techniques that enable statistical solutions to solve business problems\\nPost graduate degree in Computer Science/ Engineering, Information Science or a related discipline with strong technical experiences highly desired\\nPrevious exposure to financial services, credit cards or merchant analytics is a plus, but not required\\nExtensive experience with SQL and big data technologies (Hadoop, Python , Java, Spark, Hive etc.) tools for large scale data processing, data transformation and machine learning pipelines\\nExperience with data visualization and business intelligence tools like Tableau, Microstrategy, or other programs highly desired\\nExperience with SAS as a statistical package is preferred\\nFamiliarity or experience with data mining and statistical modeling (e.g., regression modeling, clustering techniques, decision trees, etc.) is preferred\\nAdditional Information\\n\\nWork Hours\\nThe incumbent must make themselves available during core business hours.\\nTravel Requirements\\nThe position requires the incumbent to travel for work 5% of the time.\\nPhysical Requirements\\nThis position will be performed in an office setting. The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers, reach with hands and arms, and bend or lift up to 25 pounds.\\nVisa will consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Cloud Solutions Architect</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExpertise in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes the full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio.\\nApplication Development: building custom web and mobile applications on top of the GCP stack.\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExcellent written and verbal communication skills with the ability to interface with and communicate complex technical concepts to a broad range of stakeholders.\\nHands-on experience with cloud computing, traditional on-premises and enterprise data-center technologies.\\nExperience working with engineering and sales teams.\\nExperience producing technical assets or writing technical documentation, including, but not limited to, architecture designs and documentation, statements of work, project plans, and working code samples.\\nTime management with the ability to manage multiple streams.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join SADA as a Cloud Solutions Architect!\\n\\nYour Mission\\n\\nAs a Cloud Solutions Architect at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You’re also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.\\n\\nYou will be an established contributor within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures successfully. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.\\n\\nPathway to Success\\n\\n#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.\\n\\nYou will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the solutions architecture or management growth tracks.\\n\\nExpectations\\n\\nRequired Travel - 30% travel to customer sites, conferences, and other related events.\\nCustomer Facing - This is very customer-facing role. You will usually interact with customers on a daily basis. You will participate on calls and onsite customer meetings to qualify consultative engagements with the engineering teams. You will present architecture proposals and code samples to build trust, confidence, and consensus both externally and internally.\\nTraining - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Cloud Architect Certified\\n\\n[https://cloud.google.com/certification/cloud-architect] and/or Google\\nProfessional Data Engineer Certified\\n[https://cloud.google.com/certification/data-engineer], or able to complete one of the above within the first 45 days of employment.\\n\\nRequired Qualifications:\\n\\nExpertise in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes the full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio.\\nApplication Development: building custom web and mobile applications on top of the GCP stack.\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExcellent written and verbal communication skills with the ability to interface with and communicate complex technical concepts to a broad range of stakeholders.\\nHands-on experience with cloud computing, traditional on-premises and enterprise data-center technologies.\\nExperience working with engineering and sales teams.\\nExperience producing technical assets or writing technical documentation, including, but not limited to, architecture designs and documentation, statements of work, project plans, and working code samples.\\nTime management with the ability to manage multiple streams.\\n\\nUseful Qualifications:\\n\\nDirect experience working with a variety of cloud technologies as well as designing and recommending elegant solutions that drive business outcomes.\\nUnderstanding of infrastructure automation, continuous integration/deployment, relational/NoSQL data stores, security, networking, and cloud-based delivery models.\\nAbility to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance and security.\\nThought leadership with the ability to recommend cloud-native approaches to solve customer business and technical challenges.\\nUnderstanding of best practices, design patterns and reference architectures with an uncanny ability to recommend these as needed.\\n\\nAbout SADA\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nB.S. in Computer Science/Engineering and 5 years of professional software development experience or equivalent.\\n3+ years experience with Python using Django or Flask\\nExperience with Python Celery or other task/job management frameworks\\n5+ years of experience in a software development environment\\nExperience with AWS services\\nExperience with data modeling techniques\\n3+ years experience with PostgreSQL or other SQL server\\n</td>\n",
       "      <td>NarrativeDx is expanding our software development team and we are looking for smart, talented engineers that have a history of getting products complete and in the hands of customers. For this position we are searching for a data engineer with experience architecting data processing pipelines and working alongside data scientists to adapt data models into production systems. The position requires experience developing web applications in python and experience deploying applications on AWS. This position will be involved with a wide variety of development tasks on the engineering team and will be the primary link between our research team and our application development team. Candidates must have experience with: python server application development, web application development with exposure to frontend visualizations of datasets, SQL databases and query optimization, and scaling data processing.\\nRequirements\\nB.S. in Computer Science/Engineering and 5 years of professional software development experience or equivalent.\\n3+ years experience with Python using Django or Flask\\nExperience with Python Celery or other task/job management frameworks\\n5+ years of experience in a software development environment\\nExperience with AWS services\\nExperience with data modeling techniques\\n3+ years experience with PostgreSQL or other SQL server\\nNice to haves:\\nExperience with natural language processing\\nExperience with sentiment classification\\nExperience with computational linguistics\\nBenefits\\nNarrativeDx is a 6 year old company with VC backing. We have a great office in downtown Austin with good parking, a great healthcare package, 401K Match and flexible schedules. We are a small close-knit team where we have fun and work smart and we keep our team small by hiring smart experienced people!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SQL Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor's Degree in Computer Science, Engineering, Math or related technical field (8 years of additional experience can be substituted for education)\\n5+ years' relevant experience\\nExperience in Data Platform Languages such as SSIS and TSQL\\nExperience in Data Platform Tools such as SSIS, SSDT, SSMS and/or Visual Studio\\nExperience working in an environment using Agile methodology\\nExperience in Data Engineering concepts such as ETL, ELT or performance tuning\\nHands on experience writing SQL scripts\\nStrong Communication, Presentation and Facilitation Skills. Must be able to explain data quality issues and impacts to a non-technical audience</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Develop, implement and maintain a scalable data management architecture to support the storage and querying of large datasets\\nCreate and maintain data pipelines to automate the processing of large data sets\\nHelp design and maintain efficient data collection workflows with other groups within the company\\nManage and perform data analysis to identify data quality issues\\nPropose new technologies that could improve the way data is handled\\nManage data security and provide efficient access to engineering teams\\nCommunicate technical data and approaches to both technical and non-technical audiences\\nPerform Database maintenance\\nBuilding and analyzing dashboards and reports\\nEvaluating and defining metrics and perform exploratory analysis\\nMonitoring key product metrics and understanding root causes of changes in metrics\\nEmpower and assist operation and product teams through building key data sets and data-based recommendations\\nAutomating analyses and authoring pipelines via SQL/python based ETL framework</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Overview\\nProSphere is seeking an experienced SQL Data Engineer to provide highly specialized applications and operational analysis. The Engineer assists with planning and supporting network and computing infrastructure and has knowledge of networking technologies. The Engineer is cognizant of all phases of software development with emphasis on the planning, analysis, modeling, simulation, testing, integration, documentation, and presentation phases.\\n\\nThis is full-time position located in Austin, TX. Veterans are encouraged to apply.\\nResponsibilities\\nDevelop, implement and maintain a scalable data management architecture to support the storage and querying of large datasets\\nCreate and maintain data pipelines to automate the processing of large data sets\\nHelp design and maintain efficient data collection workflows with other groups within the company\\nManage and perform data analysis to identify data quality issues\\nPropose new technologies that could improve the way data is handled\\nManage data security and provide efficient access to engineering teams\\nCommunicate technical data and approaches to both technical and non-technical audiences\\nPerform Database maintenance\\nBuilding and analyzing dashboards and reports\\nEvaluating and defining metrics and perform exploratory analysis\\nMonitoring key product metrics and understanding root causes of changes in metrics\\nEmpower and assist operation and product teams through building key data sets and data-based recommendations\\nAutomating analyses and authoring pipelines via SQL/python based ETL framework\\nQualifications\\nBachelor's Degree in Computer Science, Engineering, Math or related technical field (8 years of additional experience can be substituted for education)\\n5+ years' relevant experience\\nExperience in Data Platform Languages such as SSIS and TSQL\\nExperience in Data Platform Tools such as SSIS, SSDT, SSMS and/or Visual Studio\\nExperience working in an environment using Agile methodology\\nExperience in Data Engineering concepts such as ETL, ELT or performance tuning\\nHands on experience writing SQL scripts\\nStrong Communication, Presentation and Facilitation Skills. Must be able to explain data quality issues and impacts to a non-technical audience\\nPhysical Demands\\nTypical office environment. Ability to sit and stand for extended periods of time\\nAbility to lift 5-20 lbs.\\n\\nProSphere offers full-time employees a comprehensive and competitive benefits package including paid vacation, sick leave, holidays, health insurance, life insurance, military leave, training, tuition reimbursement, a wellness program, short- and long-term disability, 401(k) retirement plan with company matches/immediate vesting, commuter benefits, and more.\\n\\nIt is ProSphere’s policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability or any other characteristic protected by applicable federal, state or local law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>7+ Years Data Engineering5+ Years RDBMS Management2+ Years of NoSQLAWS Ecosystem knowledgeSolid Data Modeling/Design Experience</td>\n",
       "      <td>Come join our team at Cerity! We are a cutting edge insure tech company working with some really cool technologies. We have an opening for a data engineer on our growing team. We are looking for highly motivated, startup minded engineers to join our team of stellar engineers. Our data engineers work with latest big data tech including NoSQL (DynamoDB), AWS Aurora, AWS Data pipeline, Kinesis and Snowflake DB. Come help us revolutionize insurance technology.\\n\\nResponsibilities will include:Designing and Implementing RDBMS Data ModelsDesigning and Implementing NoSQL Data ModelsImplementing ETL solutionsWorking with DevOps to establish infrastructure as codeFixing any issues that arise with data functionality\\nRequirements7+ Years Data Engineering5+ Years RDBMS Management2+ Years of NoSQLAWS Ecosystem knowledgeSolid Data Modeling/Design Experience\\nBenefitsCompetitive SalariesAnnual Bonus ProgramGreat Health and other BenefitsUnlimited PTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Austin, TX 78716</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78716</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Who are we? Raybeam Inc. is a software engineering consulting company focused on strategic consulting, business intelligence, and online/database marketing for the past twenty years. We have offices near Boston and San Francisco and support a strong list of clients including Google, Facebook, Microsoft, eBay, One Kings Lane, Disney and Hilton Worldwide. We are in the process of opening a new office in Austin, TX.\\n\\nWhat do we do? We provide technology solutions by architecting and developing enterprise systems using a variety of programming languages, tools and platforms. This can range from building data warehouses, to web applications to implementing reporting platforms. We work in small teams, own the projects that we work on, and have direct input into the business decisions of our clients.\\n\\nWhat are we looking for? We are looking for a technically savvy database-oriented Analyst or Data-Engineer with good people skills and the ability to pick up new business concepts and technologies.\\n\\nThe ideal candidate will possess:\\n\\nA strong to very strong working knowledge of SQL and python.\\nAn ability to write and troubleshoot complex SQL procedures.\\nThe desire to understand business events through data.• An understanding of Data Warehousing and ETL techniques.\\nHigh level understanding in at least one scripting language such as Ruby, Shell, Python.\\nAn interest in learning large data set processing with MapReduce/Hadoop/Pig/etc.\\nA minimum of 8 years experience.\\nLinux skills are a plus.\\nGood client relations skills strongly preferred.\\nIf you are interested in applying for the position please click on the link below to take a 10 minute quiz.\\n\\nhttp://careerseval.raybeam.com/sign_in\\n\\nPlease note that Raybeam, Inc. is not E verified and is unable to provide sponsorship. We will only consider local candidates. Recent grads are encouraged to apply, and an MBA is desirable. This is a full time contract role. Thank You</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.\\nMaster's degree is desired.\\nMust have at least 3 years of experience, preferably with a federal government customer.\\nExperience with big data tools: Hadoop, Spark, Kafka\\nExperience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB\\nExperience with data governance tools: Collibra, Immuta\\nExperience with AWS cloud services: EC2, EMR, RDS, Redshift\\nExperience with object-oriented/object function scripting languages: Python, Java, C++, Scala\\nMust possess strong written and verbal communication skills.\\nSecret or Top Secret clearance is preferred.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Develop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>LMI is currently seeking a data engineer within LMI’s Advanced Analytics service line to support the design and implementation of business critical data management &amp; engineering solutions.\\n\\nThis position is located in Austin, TX\\nResponsibilities\\nThe ideal candidate will have direct, applied experience with one or more of the following areas:\\nDevelop data structures and systems to support the generation of business insightsKnowledge and experience in overall ETL processesMaintain data infrastructure and develop scripts for regular processesDefine, design, and develop data flow diagrams, data dictionaries, and logical and physical modelsDefine data requirements, document data elements, and capture and maintain metadetaIdentify and clean incomplete, incorrect, inaccurate or irrelevant dataIdentify new opportunities to use data to improve business performanceCommunicate and present data by developing reports using Tableau or Business Intelligence toolsAdhere to compliance and audit requirements for data storage, architecture, cybersecurity, etc.\\nQualifications\\nBachelor’s degree in a quantitative field (e.g., engineering, statistics, mathematics, information technology, etc.) is preferred.\\nMaster's degree is desired.\\nMust have at least 3 years of experience, preferably with a federal government customer.\\nExperience with big data tools: Hadoop, Spark, Kafka\\nExperience with relational SQL and NoSQL databases: Postgres, Cassandra, MongoDB\\nExperience with data governance tools: Collibra, Immuta\\nExperience with AWS cloud services: EC2, EMR, RDS, Redshift\\nExperience with object-oriented/object function scripting languages: Python, Java, C++, Scala\\nMust possess strong written and verbal communication skills.\\nSecret or Top Secret clearance is preferred.\\n#LI-SH1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Data Engineer role will require partnering with key internal (intra-team and cross-disciplinary) stakeholders to implement automated reporting deliverables and work with our broader team to contribute to and evolve our data automation, visualization, warehousing and reporting services. The ideal candidate will be able to help support the team in development work by unifying multiple data sources and evolving our process to create and maintain reporting and analytical tools. Additionally, they will be able to act as subject matter expert and be a partner to both our internal and client teams.\\nThey will work with the broader Analytics team to map data sources, create a data lake, develop SQL reporting views to consolidate dimensions and metrics across tables, link the reporting view to visualization tools (Datorama), and customize the reporting templates to meet the client teams’ needs. Additionally, this role will require the Data Engineer to maintain and update existing reports based on requests and changing technologies.\\nWhat you’ll be responsible for:\\nAbility to develop, build, and maintain data architecture, including data warehouse/data lake, that leverages existing automation\\nWork closely with internal insights, reporting &amp; data visualization teams to drive data accuracy, reporting advancement and data warehousing efforts\\nOwn key client data visualization builds and overall maintenance\\nTake lead of data and technology-centric projects and contribute to the evolution of processes, working with the team as well as independently with the goal of driving efficiency\\nDrive implementation, validation and ongoing support of data visualization tools and recurring reports\\nAbility to build custom data models for data mining and reporting\\nServe the client teams as an expert on internal data visualization best practices\\nGrow credibility with internal teams by exceeding expectations and delivering valuable tools\\nDevelop and refine documentation around our tech platforms and data processes\\nContribute to the company's knowledge base by being the forward-thinking data technology expert with a mind for strategic growth\\nManage, mentor and coach broader analytics team on technical skills and platform work\\nDrive advancements in our data visualization platforms including accuracy, efficiency and aesthetics - The Data Engineer will assist in onboarding new client reporting by following internal processes, maintain and update existing reports based on requests and changing technologies.\\nYou’ll need to have:\\nA Bachelor's degree or equivalent years’ experience required (Computer Science, Computer Engineering, Data Analysis/Marketing, or related field); Master’s degree preferred\\nExpertise with data visualization tools such as Datorama, Tableau, Excel, Google Sheets, Google Data Studio required\\nAdvanced/Intermediate experience developing custom SQL\\nExperience in developing data warehouses/data lakes from scratch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Application Support Engineer (English+Japanese)</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Named as one of Fortunes’ 100 Fastest Growing Companies for 2019, EPAM is committed to providing our global team of 30,100+ EPAMers with inspiring careers from day one. EPAMers lead with passion and honesty and think creatively. Our people are the source of our success and we value collaboration, try to always understand our customers’ business, and strive for the highest standards of excellence. No matter where you are located, you’ll join a dedicated, diverse community that will help you discover your fullest potential.\\n\\nDescription\\n\\nYou are curious, persistent, logical and clever – a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as an Application Support Engineer. Scroll down to learn more about the position’s responsibilities and requirements.\\n\\n#LI-DNI\\nWhat You’ll Do\\nProvide support to customers using Cloud Platform products, solutions and APIs, including Big Data and related services\\nProvide technical assistance and support as part of a global 24x7-support organization\\nWork closely with engineers and product managers to improve the product and make our customers successful\\nIncident Management, Problem Management and Change Management: change requests and incident ticket handling for the remaining Infrastructure Teams\\nProvide technical and developer support to customers using Cloud Platform products, solutions and APIs\\nFollow notification and escalation procedures\\nFollow standards for communications with business involving operational issues\\nIdentify and document product bugs and feature requests and work with internal support teams as well as customers to implement effective solutions\\nWork closely with internal support team s to improve cloud products at a senior level\\nWhat You Have\\nA degree in an associated field and/or other advanced certification along with significant experience\\nNative or full professional proficiency level in Japanese and English languages\\n3+ years’ experience fully as developer or a combination developer + big data engineer\\nAbility to read and understand code, able to write code to reproduce customer problems\\nStrong research, analytical and problem-solving skills required to work with petabyte or even exabytes of data\\nFamiliar with web protocols (HTTP, TLS/SSL, etc)\\nFirm understanding of programming (Java, C++, C#, Scala, Python, etc.) and scripting languages (Python/PHP/R)\\nBackground in SQL\\nExperience with BigData architectures and technologies (more than 1TB of data) and BI solutions\\nExperience with distributed computing frameworks (e.g. Hadoop, Spark, Flink, Storm, Samza, Beam, Google Big Query, etc.)\\nExperience with distributed data stores (HBase, Cassandra, Riak, Google Bigtable, Amazon Dynamo DB, etc.) and/or distributed message brokers (Kafka, RabbitMQ, ActiveMQ, Google Pub/Sub, Amazon Kinesis, etc.)\\nNice to have\\nExperience in technical support: familiarity with case prioritization, SLA compliance, and quality\\nExperience with the popular technologies in the machine learning/big data ecosystem\\nExperience with any ML library (scikit¬learn, pytorch, tensorflow, Spark mllib) or basic understanding of ML concepts\\nExperience with PaaS and IaaS technologies\\nJava or Objective C\\nWhat We Offer\\nMedical, Dental and Vision Insurance (Subsidized)\\nHealth Savings Account\\nFlexible Spending Accounts (Healthcare, Dependent Care, Commuter)\\nShort-Term and Long-Term Disability (Company Provided)\\nLife and AD&amp;D Insurance (Company Provided)\\nEmployee Assistance Program\\nUnlimited access to LinkedIn learning solutions\\nMatched 401(k) Retirement Savings Plan\\nPaid Time Off\\nLegal Plan and Identity Theft Protection\\nAccident Insurance\\nEmployee Discounts\\nPet Insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDesign, build, scale, and maintain multiple data pipelines\\nWork closely with business owners and external stakeholders to provide actionable data\\nEnsure data accuracy and reliability\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience building large scale streaming and batch data pipelines\\nExperience using Big Data technologies (Spark, EMR, hadoop, data lakes, etc.)\\nMastery of multiple databases (e.g. MongoDB, MySQL, etc.)\\nUnderstanding of data security best practices\\n</td>\n",
       "      <td>Crowdskout is looking for a Data Engineer that can help us expand our data pipeline infrastructure. Crowdskout's product has most recently been centered in the CRM space, but we are looking to change that. Currently, we process millions of data points through multiple data pipelines to feed into a suite of databases. We are preparing for 10x growth both in the volume of data processed and the speed in which that data can be available and actionable. To accomplish this we are looking for someone who can build out highly scalable data solutions.\\n\\nIf you are highly motivated, super passionate about democracy, and want to join a close-knit team that is looking to build great things together, Crowdskout may be for you. This is a full-time position in Washington, DC; Sacramento, CA; Austin, TX; Raleigh-Durham, NC; Salt Lake City, UT; or Chicago, IL.\\n\\nResponsibilities:\\n\\nDesign, build, scale, and maintain multiple data pipelines\\nWork closely with business owners and external stakeholders to provide actionable data\\nEnsure data accuracy and reliability\\n\\nRequirements:\\n\\nExperience building large scale streaming and batch data pipelines\\nExperience using Big Data technologies (Spark, EMR, hadoop, data lakes, etc.)\\nMastery of multiple databases (e.g. MongoDB, MySQL, etc.)\\nUnderstanding of data security best practices\\n\\nExtras:\\n\\nAWS data technologies (e.g. Kenesis, Glue, RDS, Athena, etc.)\\nExperience building out data warehouse infrastructure\\nSoftware development using PHP\\nDevOps or System Admin experience\\nData Science exploration and modeling\\n\\nCrowdskout is an equal opportunity employer that encourages diversity across all spectrums in its hiring, without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, or any other protected factor. With that being said, we wouldn't be able to accommodate candidates in need of work sponsorship at this time since we are a small company. If you find this role interesting and you hit on the elements above, please apply!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>2+ years of experience with building end-to-end scalable production-grade data pipelines\\nKnowledge of data warehousing\\nUnderstanding of modern data architecture, data modeling, and data management principles\\nExperience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)\\nGood foundation in data structures and algorithms\\nUnderstanding with Relational and NoSQL databases to help teams best organize their data for analysis\\nExperienced in OOP design and development, preferably in Python\\nKnowledge in writing, understanding and tuning PL/SQL and/or T-SQL\\nBS/BA degree in Computer Science, Engineering or related fields or equivalent experience\\n</td>\n",
       "      <td>2+ years of experience with building end-to-end scalable production-grade data pipelines\\nKnowledge of data warehousing\\nUnderstanding of modern data architecture, data modeling, and data management principles\\nExperience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)\\nGood foundation in data structures and algorithms\\nUnderstanding with Relational and NoSQL databases to help teams best organize their data for analysis\\nExperienced in OOP design and development, preferably in Python\\nKnowledge in writing, understanding and tuning PL/SQL and/or T-SQL\\nBS/BA degree in Computer Science, Engineering or related fields or equivalent experience\\n</td>\n",
       "      <td>General Information\\nRef #: 27449\\nFunctional Area: Technology\\nEmployee Type: Full Time\\nLocation: Austin\\nExperienced Required: Please See Below\\nEducation Required: Bachelors Degree\\nJob Posting Shift: 1st\\nDate published: 26-Jun-2019\\nAbout Us:\\nWe are PIMCO, a leading global asset management firm. We manage investments and develop solutions across the full spectrum of asset classes, strategies and vehicles: fixed income, equities, commodities, asset allocation, ETFs, hedge funds and private equity. PIMCO is one of the largest investment managers, actively managing more than $1.84 trillion in assets for clients around the world. PIMCO has over 2,700 employees in 17 offices globally. PIMCO is recognized as an innovator, industry thought leader and trusted advisor to our clients.\\n\\nPIMCO is one of the world’s premier fixed income investment managers with thousands of professionals around the world united in a single purpose: creating opportunities for our clients in every environment. Since 1971, we have brought innovation and expertise to our partnership with the institutions, financial advisors and millions of individual investors who entrust us with their assets. We aspire to cultivate performance and leadership through empowering our people, diversity of thought, and a commitment to an inclusive culture that engages in our global communities.\\nPosition Description:\\nAs a data engineer on our team, we will be working on a number of high-profile projects that will require you to collaborate with key partners and develop data solutions that enable insights into our clients through an evolving data architecture and drive our next generation data platform. In this role you will meet with relevant partners, understand and model data assets, and constantly find opportunities to optimize and evolve the underlining data platform. You will work with software developers, data architects, data analysts, and data scientists to solve complex business problems. Demonstrating lifelong learning and collaboration to fill gaps in knowledge is essential.\\nPosition Requirements:\\n\\n2+ years of experience with building end-to-end scalable production-grade data pipelines\\nKnowledge of data warehousing\\nUnderstanding of modern data architecture, data modeling, and data management principles\\nExperience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)\\nGood foundation in data structures and algorithms\\nUnderstanding with Relational and NoSQL databases to help teams best organize their data for analysis\\nExperienced in OOP design and development, preferably in Python\\nKnowledge in writing, understanding and tuning PL/SQL and/or T-SQL\\nBS/BA degree in Computer Science, Engineering or related fields or equivalent experience\\nPREFERRED QUALIFICATIONS\\nDomain knowledge of Financial Services\\nHands-on experience with working with Cloud technologies, including AWS\\nExperience with CI/CD methodologies\\nExperience with *nix environments, including shell script development\\n\\nWe are PIMCO, a global investment management firm with a singular focus on preserving and enhancing investors’ assets. We manage investments for institutions, financial advisors and individuals, helping millions of people around the world meet their financial goals.\\n\\nOur technology powers the firm’s global trading platform. We employ sophisticated and cutting edge technology tools that support PIMCO’s core investment management strategy.\\n\\nWhy PIMCO? At PIMCO you will join a dynamic, constantly evolving global firm that pushes you to grow, lead and innovate. You will be client- focused and work on technologies that will be put to immediate use. You will be part of a team whose members are encouraged to speak up with an idea or challenge existing views, regardless of title or tenure. You will have the opportunity to receive competitive compensation and other attractive benefits (Please see below).\\n\\nPIMCO’s Technology Team is organized in small, focused, agile groups, that either work closely with business units to deliver value or develop core technologies that lever the product teams. Our environment fosters innovation and promotes entrepreneurial spirit, and we use top of the line tools. PIMCO recognizes the paramount role of tech now and in the future and invests in technology accordingly. Technology careers are available in Newport Beach, Austin, New York, London, Munich, Singapore and Tokyo.\\nBenefits:\\nPIMCO is committed to offering a comprehensive portfolio of employee benefits designed to support the health and wellbeing of you and your family. Benefits vary by location but may include:\\nMedical, dental, and vision coverage\\nLife insurance and travel coverage\\n401(k) (defined contribution) retirement savings, retirement plan, pension contribution from your first day of employment\\nWork/life programs such as flexible work arrangements, parental leave and support, employee assistance plan, commuter benefits, health club discounts, and educational/CFA certification reimbursement programs\\nCommunity involvement opportunities with The PIMCO Foundation in each PIMCO office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nAt least one (1) year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models\\nAt least two (2) years of experience in Scala, Python, Apache Spark and SQL\\nExperience working directly with relational database structures and flat files\\nAbility to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices\\nKnowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations.\\nGood verbal and written communication skills, with both technical and non-technical stakeholders</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBuild pipelines to ingest and maintain complex data sets into Cerebri AI’s proprietary data stores for use in machine learning modeling\\nDevelop and maintain data ontologies for key market segments\\nCollaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data\\nCollaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data\\nWrite quality documentation on the discovery process and software projects\\nWork equally well in a team environment and on your own.\\nCommunicate complex ideas clearly with both team members and clients\\nTravel up to 25%</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Design, develop and build out data pipelines to ingest data into our proprietary data structures, and be a key collaborator in the data discovery and exploratory analysis process during our client engagements.\\nResponsibilities\\nBuild pipelines to ingest and maintain complex data sets into Cerebri AI’s proprietary data stores for use in machine learning modeling\\nDevelop and maintain data ontologies for key market segments\\nCollaborate with data scientists to perform exploratory data analysis and to map data fields into proprietary data stores and to find signals in client data\\nCollaborate with clients to develop pipeline infrastructure, and to ask appropriate questions to gain deep understanding of client data\\nWrite quality documentation on the discovery process and software projects\\nWork equally well in a team environment and on your own.\\nCommunicate complex ideas clearly with both team members and clients\\nTravel up to 25%\\nQualifications\\nAt least one (1) year of experience designing and building data processing solutions and ETL pipelines for varied data formats, ideally at a company that leverages machine learning models\\nAt least two (2) years of experience in Scala, Python, Apache Spark and SQL\\nExperience working directly with relational database structures and flat files\\nAbility to write efficient database queries, functions and views to include complex joins and the identification and development of custom indices\\nKnowledge of professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, continuous integration and development, and operations.\\nGood verbal and written communication skills, with both technical and non-technical stakeholders\\nNice to Haves\\nExperience in Java and/or Scala\\nExperience with data management processing tools such as Kafka, Elasticsearch and Logstash\\nExperience with NoSQL distributed databases such as Cassandra.\\nExperience in business intelligence visualization tools such as Grafana, Superset, Redash or Tableau.\\nExperience with Microsoft Azure or similar cloud computing solutions\\nMaster’s degree or higher in a relevant quantitative subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Austin, TX 78701</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78701</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Position Overview: From software hacking to hardware hacking, we help secure everything from cryptocurrency exchanges and space telescopes to autonomous vehicles and the electric grid. Today, Praetorian is making significant investments in terms of financial and engineering resources to develop a radically new customer experience we call “Security-as-a-Service” to provide customers with a unified, efficient, and data-driven security platform. We are looking to add the right individual to our growing team supporting the next wave of cybersecurity products and solutions.\\n\\nAs part of that investment, Praetorian is seeking a seasoned Data Engineer with a successful track record in data engineering in a hyper growth company setting. You will have the opportunity to work with some of the best security engineers in the world who hail from organizations such as Amazon, CIA, Facebook, Google, Microsoft, NSA, Redhat, Sun Microsystems, and US Air Force. As an Inc. Best Places to Work, Inc. 500 | 5000, Cybersecurity 500, and Austin Fast 50 Award recipient, we are seeking an individual that understands the professional and personal growth attached to this opportunity and who has the corresponding internal drive to maximize it.\\n\\nTo learn more about Praetorian, visit: https://www.praetorian.com/careers\\n[https://www.praetorian.com/careers]\\n\\nCareer opportunity:\\n\\nJoin an industry with massive socio, economic, and political importance in the 21st century\\nWork alongside some of the best and the brightest minds in the security industry\\nLeave an indelible mark on a company where individual input has real impact\\nBe recognized, internally and publicly, for your contributions in a high profile position\\nAlign your career trajectory with a hyper growth company that is on the move\\n\\nCore responsibilities:\\n\\nCreate pipelines to ingest and maintain complex data sets into Praetorian's data stores for use in machine learning models\\nCreate tools to scour the internet to find important security information and ingest it into Praetorian's infrastructure\\nWork with Data Scientist to create and maintain data ontologies for seurity\\nCreate the roadmap of how to continually evolve the data engineering infrastructure and techniques to improve Praetorian's ability to find security information\\nMentor junior data engineers and teach them how to use data engineering techniques to solve real world problems\\nCommunication of complex concepts to team members\\n\\nAccountable for:\\n\\nCreation of data engineering pipeline to find and ingest security vulnerabilities\\nCreation of data engineering tools to help label and validate data\\n\\nRequired qualifications:\\n\\nAt least 8 years experience designing and building data processing/ETL pipelines\\nAt least 8 years experience in Python and Spark or similar technologies\\nAt least 8 years experience with SQL and relational databases\\nAt least 8 years experience parsing flat files\\n8+ years development experience\\nPrior track record in a hyper-growth, high-tech company\\nBachelor's degree or equivalent practical experience\\n\\nDesired qualifications:\\n\\nExperience working with Google Tensorflow\\nExperience with modern technology stacks\\nExperience with micro-services architectures\\nExperience with cloud platforms and SaaS solutions\\nExperience with agile/scrum development practices\\nExperience with test driven development, continuous integration, continuous deployment\\nExperience with Git, JIRA, Confluence\\nExperience with Google Compute, Firebase, and GKE\\nExperience with Docker\\n\\nDesired behaviors:\\n\\nRelentless restlessness to turn theory into practice and develop production worthy code that solves real-world customer problems\\nDetermination to always learn and get better and never rest on ones laurels\\nPersonable individual who enjoys working in a team-oriented environment\\nComfort dealing with ambiguity in an environment where we build the plane as we fly it\\nAbility to work within constraints and to challenge the status quo\\nAbility to self-direct work and truly own the position in a hyper-growth environment\\n\\nCompensation package:\\n\\nCompetitive compensation\\nOwnership opportunity through employee stock option plan\\nHealth, dental, and vision insurance\\n4% company 401K matching vested immediately\\n\\nIn compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.\\n\\nWe are committed to an inclusive and diverse Praetorian. We are an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, disability, veteran status, genetic information, marital status, or any other legally protected status.\\n\\nWe ask that you please include a few paragraphs about yourself and what you are passionate about in your application.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>You are curious, persistent, logical and clever a true techie at heart. You enjoy living by the code of your craft and developing elegant solutions for complex problems. If this sounds like you, this could be the perfect opportunity to join EPAM as a Lead Data Enigneer. Scroll down to learn more about the position’s responsibilities and requirements.\\nWhat You’ll Do\\nArchitecture design of holistic Cloud data ecosystem with a focus on Google Cloud Platform capabilities and features\\nArchitecture design of Production, Staging/QA, and Development infrastructures running is 24/7 environments\\nRobust and consistent Cloud Strategy design aligned with business objectives\\nProvide guidelines for data migration approaches and techniques including ingest, store, process, analyze and explore/visualize data\\nAssistance with data migration and transformation\\nEvangelize Cloud computing expertise internally and externally to drive Cloud Adoption\\nWhat You Have\\nA degree in an associated field and/or other advanced certification along with significant experience\\nIn-depth cloud professional, competent of quickly establishing connections and credibility in how to address the business needs via design and operate cloud-based solutions\\nExperience in Agile or PMI methodology managed projects\\nExperience in enterprise applications, and big data solutions\\nExperience in platform and cloud migrations, including migration factory\\nIn-depth experience with databases and tools analysis\\nIn-depth experience with ETL tools\\nProcesses design and development for the data modeling, mining, and analysis\\nExtensive experience in methodologies and processes for large-scale databases management on-premises and cloud environment\\nIn-depth understanding and knowledge of distributed version control systems like Git\\nStrong understanding of concepts and experience with StackDriver and other cloud-based monitoring tools including application level and logging\\nNice to have\\nGoogle Cloud Certified Professional Data Engineer\\nExperience Creating automated tooling for cloud platforms\\nExperience with architecting and handling large datasets, structured and semi-structured data formats\\nExperience with streaming processing\\nExperience with messaging platforms\\nExperience with performance testing and tuning\\nExperience with GCP based security hardening including IAM, ACL, firewall rules, data traffic encryption\\nWhat We Offer\\nMedical, Dental and Vision Insurance (Subsidized)\\nHealth Savings Account\\nFlexible Spending Accounts (Healthcare, Dependent Care, Commuter)\\nShort-Term and Long-Term Disability (Company Provided)\\nLife and AD&amp;D Insurance (Company Provided)\\nEmployee Assistance Program\\nUnlimited access to LinkedIn learning solutions\\nMatched 401(k) Retirement Savings Plan\\nPaid Time Off\\nLegal Plan and Identity Theft Protection\\nAccident Insurance\\nEmployee Discounts\\nPet Insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX 78723</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78723</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Use advanced SQL skills to manage data\\nMonitor database performance and tuning to improve query performance\\nDesign and automate data pipelines and integrate different data sources using SSIS\\nApply advanced skills to develop real-time data integrations in MS SQL Server\\nEstablish techniques to monitor data quality and implement remediation procedures\\nPartner with non-technical users to identify needs/requirements and then translating the requirements into technical solutions\\nDemonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler\\nProvide advanced technical expertise to staff and end-users\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Use advanced SQL skills to manage data\\nMonitor database performance and tuning to improve query performance\\nDesign and automate data pipelines and integrate different data sources using SSIS\\nApply advanced skills to develop real-time data integrations in MS SQL Server\\nEstablish techniques to monitor data quality and implement remediation procedures\\nPartner with non-technical users to identify needs/requirements and then translating the requirements into technical solutions\\nDemonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler\\nProvide advanced technical expertise to staff and end-users\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>We’re excited you’re considering joining a great place to work! At Texas Mutual, we value our employees. Our service-inspired culture, great compensation and benefits package, award-winning wellness program and excellent career opportunities make Texas Mutual a great place to work. In the Data Engineer role, you will use advanced technical skills and knowledge to develop data models, automated ETL processes, stored procedures, and views in MS SQL Server. You will provide technical expertise to staff and end-users.\\nResponsibilities &amp; Qualifications\\nEssential Functions:\\nUse advanced SQL skills to manage data\\nMonitor database performance and tuning to improve query performance\\nDesign and automate data pipelines and integrate different data sources using SSIS\\nApply advanced skills to develop real-time data integrations in MS SQL Server\\nEstablish techniques to monitor data quality and implement remediation procedures\\nPartner with non-technical users to identify needs/requirements and then translating the requirements into technical solutions\\nDemonstrate competency in T-SQL, including advanced functions and performance tuning, and job scheduler\\nProvide advanced technical expertise to staff and end-users\\nRequir ed Qualifications:\\nBachelor’s degree in a related field.\\nAt least five years of professional data experience for a Data Engineer; at least seven years of professional data experience for a Senior Data Engineer or any equivalent combination of education, training , and experience that provides the skills necessary to perform the essential function of the job.\\nPreferred Qualifications:\\nExperience managing cloud data assets\\nData preparation using Python or R\\nExperience building data pipelines for machine learning\\nNoSQL database experience\\nOur Benefits:\\nDay one health, dental, and vision insurance\\nPerformance bonus\\n401k plan with 4% basic employer contribution and 100% employer match contribution up to 6%\\nVacation, sick, holiday and volunteer time off\\nLife and disability insurance\\nFlexible spending account\\nFree on-site gym and fitness classes\\nProfessional development\\nTuition reimbursement\\nPet insurance\\nFree identity theft protection\\nCompany-sponsored social and philanthropy events\\nTexas Mutual Insurance Company is an Equal Employment Opportunity employer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Data Engineer Lead</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Key Role:\\nLead a team of engineers to design, implement, and manage databases and data delivery systems and transform them into beautiful insights, analysis, and reporting. Comprehend database design and implementation tools, including entity-relationship data modelling and SQL, distributed computing architectures, operating systems, storage technologies, memory management, and networking to create structure and value out of complex and ambiguous technical challenges with little guidance. Leverage experience with structured and unstructured data, streaming and batch data processing, ETL, data wrangling, data ingest, and data access.\\nBasic Qualifications:7+ years of experience with data engineering3+ years of experience with leading a teamExperience with custom or structured ETL design, implementation, and maintenanceExperience with normalized and dimensional data modelsExperience with Microsoft SQL, SSIS, and SSASExperience with Microsoft Azure, including SQL PaaS, SQL DW, and Azure Data Factory (ADF)Experience with Spark and big data engineeringAbility to learn technical concepts quickly and communicate with multiple functional groupsAbility to obtain a security clearanceBA or BS degree\\nAdditional Qualifications:Experience with GitHub, Confluence, and JenkinsPossession of excellent analytical and problem-solving skillsExperience with working in a DevOps environmentMA or MS degree\\nClearance:\\nApplicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.\\nWe’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change.\\n#LI-AH1, APC1, CJ1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>5+ years of experience with building end-to-end scalable production-grade data pipelines\\nIn-depth knowledge of data warehousing and master data management\\nExpertise with modern data architecture, data modeling, and data management principles\\nHands-on experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)\\nSolid foundation in data structures, algorithms, and software design\\nExpertise with Relational and NoSQL databases to help teams best organize their data for analysis\\nSkilled in OOP design and development, preferably in Python\\nAdvanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL\\nBS/BA degree in Computer Science, Engineering or related fields or equivalent experience</td>\n",
       "      <td>5+ years of experience with building end-to-end scalable production-grade data pipelines\\nIn-depth knowledge of data warehousing and master data management\\nExpertise with modern data architecture, data modeling, and data management principles\\nHands-on experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)\\nSolid foundation in data structures, algorithms, and software design\\nExpertise with Relational and NoSQL databases to help teams best organize their data for analysis\\nSkilled in OOP design and development, preferably in Python\\nAdvanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL\\nBS/BA degree in Computer Science, Engineering or related fields or equivalent experience</td>\n",
       "      <td>General Information\\nRef #: 27451\\nFunctional Area: Technology\\nEmployee Type: Full Time\\nLocation: Austin\\nExperienced Required: Please See Below\\nEducation Required: Bachelors Degree\\nJob Posting Shift: 1st\\nDate published: 28-Jun-2019\\nAbout Us:\\nWe are PIMCO, a leading global asset management firm. We manage investments and develop solutions across the full spectrum of asset classes, strategies and vehicles: fixed income, equities, commodities, asset allocation, ETFs, hedge funds and private equity. PIMCO is one of the largest investment managers, actively managing more than $1.84 trillion in assets for clients around the world. PIMCO has over 2,700 employees in 17 offices globally. PIMCO is recognized as an innovator, industry thought leader and trusted advisor to our clients.\\n\\nPIMCO is one of the world’s premier fixed income investment managers with thousands of professionals around the world united in a single purpose: creating opportunities for our clients in every environment. Since 1971, we have brought innovation and expertise to our partnership with the institutions, financial advisors and millions of individual investors who entrust us with their assets. We aspire to cultivate performance and leadership through empowering our people, diversity of thought, and a commitment to an inclusive culture that engages in our global communities.\\nPosition Description:\\nAs a Sr. Data Engineer on our team we will be working on a number of high-profile projects that will require you to collaborate with key partners and develop data solutions that enable insights into our clients through an evolving data architecture and drive our next generation data platform. A data engineer will meet with relevant partners, understand and model data assets, and constantly find opportunities to optimize and evolve the underlining data platform. You will work with software developers, data architects, data analysts, and data scientists to solve complex business problems. Demonstrating lifelong learning and collaboration to fill gaps in knowledge is essential.\\nPosition Requirements:\\n5+ years of experience with building end-to-end scalable production-grade data pipelines\\nIn-depth knowledge of data warehousing and master data management\\nExpertise with modern data architecture, data modeling, and data management principles\\nHands-on experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)\\nSolid foundation in data structures, algorithms, and software design\\nExpertise with Relational and NoSQL databases to help teams best organize their data for analysis\\nSkilled in OOP design and development, preferably in Python\\nAdvanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL\\nBS/BA degree in Computer Science, Engineering or related fields or equivalent experience\\n\\nPREFERRED QUALIFICATIONS\\nDomain knowledge of Financial Services\\nHands-on experience with working with Cloud technologies, including AWS\\nSkilled at designing and implementing ETL frameworks\\nExperience with CI/CD methodologies\\nExperience with *nix environments, including shell script development\\n\\nWe are PIMCO, a global investment management firm with a singular focus on preserving and enhancing investors’ assets. We manage investments for institutions, financial advisors and individuals, helping millions of people around the world meet their financial goals.\\n\\nOur technology powers the firm’s global trading platform. We employ sophisticated and cutting edge technology tools that support PIMCO’s core investment management strategy.\\n\\nWhy PIMCO? At PIMCO you will join a dynamic, constantly evolving global firm that pushes you to grow, lead and innovate. You will be client- focused and work on technologies that will be put to immediate use. You will be part of a team whose members are encouraged to speak up with an idea or challenge existing views, regardless of title or tenure. You will have the opportunity to receive competitive compensation and other attractive benefits (Please see below).\\n\\nPIMCO’s Technology Team is organized in small, focused, agile groups, that either work closely with business units to deliver value or develop core technologies that lever the product teams. Our environment fosters innovation and promotes entrepreneurial spirit, and we use top of the line tools. PIMCO recognizes the paramount role of tech now and in the future and invests in technology accordingly. Technology careers are available in Newport Beach, Austin, New York, London, Munich, Singapore and Tokyo.\\nBenefits:\\nPIMCO is committed to offering a comprehensive portfolio of employee benefits designed to support the health and wellbeing of you and your family. Benefits vary by location but may include:\\nMedical, dental, and vision coverage\\nLife insurance and travel coverage\\n401(k) (defined contribution) retirement savings, retirement plan, pension contribution from your first day of employment\\nWork/life programs such as flexible work arrangements, parental leave and support, employee assistance plan, commuter benefits, health club discounts, and educational/CFA certification reimbursement programs\\nCommunity involvement opportunities with The PIMCO Foundation in each PIMCO office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX 78728</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78728</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Description\\nJob Title: Data Engineer\\nLocation: San Francisco, CA, Austin, TX, San Jose, CA\\nTerms: Full-time\\nAbout Trianz\\nTrianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.\\nWhat We Stand For\\nOur clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.\\nAs a result, Trianz is focusing on three important themes in our engagement model with clients.\\nCrystallize business impact from a top management point of view\\nHelp Clients achieve results from strategy-by making execution predictable through innovative execution techniques\\nCreate a positive, enriching partnership experience in everything we do\\nIndustries, Clients &amp; Practices\\nTrianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:\\nCloud\\nAnalytics\\nDigitization\\nInfrastructure\\nSecurity\\nJob Description\\nOverview\\nData is the way our clients make decisions. It is the core to their business, helping create an experience for customers and providing insights into the effectiveness of our product launch &amp; features.\\n\\nAs a Data Engineer , you will be a part of an early stage team that builds the data pipelines, collection, and storage, and exposes services that make data a first-class citizen. We are looking for a Data Engineer to build a scalable data platform. You'll have ownership of core data pipelines that powers top line metrics; You will also use data expertise to help evolve data models in several components of the data stack; You will help architect, building, and launching scalable data pipelines to support growing data processing and analytics needs. Your efforts will allow access to business and user behavior insights, using huge amounts of data to fuel several teams such as Analytics, Data Science, Marketplace and many others.\\n\\nResponsibilities\\n\\nOwner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth\\nEvolve data model and data schema based on business and engineering needs\\nImplement systems tracking data quality and consistency\\nDevelop tools supporting self-service data pipeline management (ETL)\\nSQL and MapReduce job tuning to improve data processing performance\\n\\nExperience\\n\\n3+ years of relevant professional experience\\nExperience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)\\nProficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)\\nGood understanding of SQL Engine and able to conduct advanced performance tuning\\nStrong skills in scripting language (Python, Ruby, Bash)\\n1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)\\nComfortable working directly with data analytics to bridge Lyft's business goals with data engineering\\n\\nWe are Growing Rapidly: 2019 Highlights\\nTrianz is growing above the average of the professional services industry. Here are some highlights.\\nVoted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.\\nWon the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.\\nWon UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.\\nFeatured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.\\nAchieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.\\nTalk to us, Join us &amp; Develop into Leaders\\nCome join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.\\nWe are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!\\nEqual Opportunity Employer\\nTrianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Senior Cloud Solutions Architect</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nMastery in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio\\nApplication Development: building custom web and mobile applications on top of the GCP stack\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExperience providing oversight and direction of cloud projects\\nExperience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states\\nExperience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP\\nExperience across multiple cloud platforms: GCP, AWS, Azure\\nExperience with container engines: Kubernetes, Docker, AWS Elastic Container Service\\nExperience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation\\nExperience working with engineering and sales teams to elicit customer requirements\\nAbility to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders\\nTime management skills with the ability to manage multiple streams and lead less experienced architects\\nExperience as a technical consultant or another customer-facing technical role\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join SADA as a Sr. Cloud Solutions Architect!\\n\\nYour Mission\\n\\nAs a Sr. Cloud Solutions Architect at SADA, you will work collaboratively with other architects and engineers to design, prototype and lead the deployment of scalable Google Cloud Platform (GCP) architectures. You will work with engineering teams, customers and sales teams to qualify potential engagements, craft robust architectural proposals, and deliver Statements of Work (SOWs) that engineering teams can successfully execute. You’re also hands-on, able to conduct experiments and build functioning prototypes that prove out ideas and build confidence in the solutions you advocate.\\n\\nYou will be a recognized expert within SADA and will develop a reputation with customers as well as the Google Cloud sales and professional services organizations for the quality of your work. You will demonstrate repeated delivery of project architectures that other engineers and architects demur to you for lack of expertise. You will also lead early-stage opportunity technical qualification calls, as well as lead client-facing technical discussions.\\n\\nPathway to Success\\n\\n#BeAChangeAgent: You are a rainmaker! You are way out in front of our delivery organization, meeting with the spectrum of corporate and enterprise customers that need our consultative services. You have your finger on the pulse of their technical needs and take pride in helping them solve their real-world problems on GCP.\\n\\nYou will be measured quarterly by a combination of (a) the volume of signed SOWs that you shepherd through the sales funnel, and (b) the level of customer satisfaction measured at the end of each engagement.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the solution architecture or management growth tracks.\\n\\nExpectations\\n\\nRequired Travel - 30% travel to customer sites, conferences, and other related events.\\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives\\nTraining - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Cloud Architect Certified\\n\\n[https://cloud.google.com/certification/cloud-architect] and/or Google\\nProfessional Data Engineer Certified\\n[https://cloud.google.com/certification/data-engineer], or able to complete one of the above within the first 45 days of employment.\\n\\nRequired Qualifications:\\n\\nMastery in at least one of the following domain areas:\\nInfrastructure Modernization: migrating n-tiered workloads from on-prem and other clouds to GCP with near-zero-downtime (includes full spectrum of lift and shift to complete re-platforming scenarios), or building hybrid-cloud solutions based on Anthos and containerization technologies, such as Docker, Kubernetes, and Istio\\nApplication Development: building custom web and mobile applications on top of the GCP stack\\nData Engineering: data warehouse modernization (including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines and reporting/analytic tools), OLTP/OLAP data migrations, or backup, restore and disaster recovery solutions.\\nExperience providing oversight and direction of cloud projects\\nExperience leading technical design sessions, architecting and documenting technical solutions that are aligned with client business objectives, and identifying gaps between the client's current and desired end states\\nExperience strategizing, designing, architecting and leading the deployment of scalable solutions on GCP\\nExperience across multiple cloud platforms: GCP, AWS, Azure\\nExperience with container engines: Kubernetes, Docker, AWS Elastic Container Service\\nExperience with automation technologies including Terraform, Google Cloud Deployment Manager, AWS Cloud Formation or Microsoft Azure Automation\\nExperience working with engineering and sales teams to elicit customer requirements\\nAbility to communicate across business units and the ability to interface with and communicate complex technical concepts to a broad range of internal and external stakeholders\\nTime management skills with the ability to manage multiple streams and lead less experienced architects\\nExperience as a technical consultant or another customer-facing technical role\\n\\nUseful Qualifications:\\n\\nHands-on experience designing and recommending elegant solutions that drive business outcomes\\nExperience building, designing and migrating complex cloud architectures\\nStrong aptitude for learning new technologies and techniques with a willingness and capability to skill up the team\\nAbility to lead an in-depth client meeting/workshop across a broad range of topics including discovery, cloud compliance, and security\\nDeep understanding of best practices, design patterns, reference and compliance architectures with an uncanny ability to build and recommend these as needed\\nKnowledge and understanding of industry trends, new technologies and the ability to apply these to customer architectures to drive outcomes\\nHighly self-motivated and able to work independently as well as in a team environment\\n\\nAbout SADA\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX 78746</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78746</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nBachelor’s degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.\\n3 or more years of experience as a data engineer on enterprise-level data solutions, specifically as a Data Engineer or ETL Developer.\\n2 or more years of experience working with relational and unstructured databases and enterprise data warehouses, such as work with MySQL, PostgreSQL, MongoDB, SQL Server, or Oracle.\\nExperience with Spark, Presto, Hive and/or other map/reduce \"big data\" systems and services.\\nExperience in SQL and Python for scripting automation.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nDesign, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources\\nParticipate in data architecture discussions to understand target data structures, required data transformations and deliver data pipelines/ETL loading processes that meet requirements.\\nPerform detailed exploration of new internal and external source data to perform source-to-target mapping to inform the development of new data pipelines/flows.\\nWork in close collaboration with your data-minded colleagues focused on back-end (microservice) development, business intelligence reporting, machine learning and artificial intelligence models.\\nInvestigate the root cause of data-related issues and implement viable, sustainable solutions to correct issues.\\nPerform database administration activities such as refreshes, updates, migrations, etc. in support of data pipeline maintenance.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>What you’ll be called: Data Engineer\\n\\nWhere you’ll work: KWRI Headquarters—Austin, TX\\n\\nNamed a Happiest Company to Work for in 2019; one of the Best Places to Work in Austin, TX; and featured on the Training Magazine Training 125 list seven times, Keller Williams Realty International (KWRI) thrives within a creative and collaborative culture where transforming the real estate industry through technology is our primary goal.\\n\\nKW Technology is the foremost provider of real estate solutions, offering the most comprehensive end-to-end portfolio of products, services and training in the industry. Our Data Engineering team converts agent and consumer challenges into intuitive, insight-enhanced technology and consumer experiences using tools such as Python, Hadoop, Spark, MySQL, MongoDB and Snaplogic.\\n\\nWhat you’ll do:\\n\\nDesign, develop and implement data infrastructure and best-in-class pipelines that collect, connect, centralize and curate data from various internal and external data sources. You will ensure that architectures support the needs of the business, and recommend ways to improve data reliability, efficiency.\\n\\nEssential Duties and Responsibilities:\\n\\nDesign, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources\\nParticipate in data architecture discussions to understand target data structures, required data transformations and deliver data pipelines/ETL loading processes that meet requirements.\\nPerform detailed exploration of new internal and external source data to perform source-to-target mapping to inform the development of new data pipelines/flows.\\nWork in close collaboration with your data-minded colleagues focused on back-end (microservice) development, business intelligence reporting, machine learning and artificial intelligence models.\\nInvestigate the root cause of data-related issues and implement viable, sustainable solutions to correct issues.\\nPerform database administration activities such as refreshes, updates, migrations, etc. in support of data pipeline maintenance.\\nMinimum Qualifications:\\n\\nBachelor’s degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.\\n3 or more years of experience as a data engineer on enterprise-level data solutions, specifically as a Data Engineer or ETL Developer.\\n2 or more years of experience working with relational and unstructured databases and enterprise data warehouses, such as work with MySQL, PostgreSQL, MongoDB, SQL Server, or Oracle.\\nExperience with Spark, Presto, Hive and/or other map/reduce \"big data\" systems and services.\\nExperience in SQL and Python for scripting automation.\\nPreferred Qualifications:\\n\\nMaster’s degree in Information Management, Data Science, Analytics or related field.\\nExperience building open source data pipeline systems such as AirFlow, Hadoop or Kafka.\\nFamiliar working in a Cloud environment (AWS or GCP) with a subset of the following tools or their equivalent - Redshift, RDS, S3, EC2, Lambda, Kinesis, Elasticsearch, EMR, BigQuery, GCS.\\nWho are we?\\n\\nKeller Williams Realty Inc. is the largest real estate company by agent count across the globe and is number one in units and volume in the United States. Founded in 1983, we pride ourselves on an agent-centric, technology-driven and education-based culture that rewards agents as stakeholders. Keller Williams Realty International (KWRI), is the company’s corporate headquarters located in Austin, TX. Here, through a focus on cutting edge technology, education, and products and services, we support our agents and associates to create careers worth having, businesses worth owning, lives worth living, experiences worth giving and legacies worth leaving.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Are you looking for a high energy team where you can make a direct contribution to envisioning and architecting next generation Data Analytics platform?\\nAre you looking to join a company with a vision to imagine, design, and create a better world who is also recognized as top places to work for in Silicon Valley?\\nYour next adventure at VMware is only a click away!\\nVMware's Data Analytics Team is looking for a Data Engineer to help build on Next generation Near Realtime BI Platform based on SAP HANA and Hadoop. You will be responsible for building and enhancing the solutions on the existing platform based on the business needs in partnering with fellow Developers and Business groups.\\nResponsibilities:Understand the business capability/requirements and transform them into robust design solutionsPerform hands on work using Python, able to write complex SQL’s, understand API and be able to consume/write API’s as neededPerform report development using enterprise tools such as Tableau, SAP BOBJ and other open source reporting platforms.Perform hands on work using SAP HANA, Hadoop/HAWQ SDI/SLT, Informatica to build next generation NearRealTime data analytics platform.Integrate data sets from difference sources using Informatica, Python, SAP SDI/SLTProtect data integrity and accuracy. Perform root cause analysis of issues that hinder the data quality. Work with data source owner to increase quality and accuracy of the source data.Help data consumers to correctly understand and use the data.Building reports based on the business need.\\nQualifications:5+ years of experience in as a BI/Data Engineer handling large volumes of data.Excellent knowledge of data warehouse technical architecture, infrastructure components, ETL/ELT and reporting/analytic tools.Expertise in writing advanced SQL queries.Experience working with Informatica, SAP SDI/SLTExpertise in SAP HANA, Hive/Hadoop/HawqWorking knowledge of BI Reporting tools like BOBJ and TableauExperience in Python ScriptingFamiliarity with Amazon Web Services (AWS), Redshift is a plusStrong analytical and troubleshooting skillsExcellent verbal and written communication skillsBachelor’s degree in Computer science, Statistics, Mathematics, Engineering or relevant field.\\nThis position is eligible for the IT Apps Hiring FY20 referral campaign\\nVMware (NYSE: VMW) is the global leader in virtualization and cloud infrastructure, two areas that consistently rank as top priorities among CIOs. VMware delivers award-winning, customer-proven solutions that accelerate IT by reducing complexity and enabling more flexible, agile service delivery. Our solutions help organizations of all sizes, lower costs, increase business agility and ensure freedom of choice. We are searching for people who are ready to accelerate, innovate and lead to join our team of more than 20,000 employees in 40+ locations worldwide working to develop innovative solutions that deliver the future of IT through cloud computing. Having the audacity to challenge constraints and problem-solve for tomorrow starts today, and it starts with you. Learn more at www.vmware.com/careers\\n\\nVMware Company Overview: VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com.\\n\\nEqual Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExpertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExperience writing software in one or more languages such as Python, Java, Scala, or Go\\nExperience building production-grade data solutions (relational and NoSQL)\\nExperience with systems monitoring/alerting, capacity planning and performance tuning\\nExperience in technical consulting or other customer-facing role\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Join SADA as a Data Engineer!\\n\\nYour Mission\\n\\nAs a Data Engineer at SADA, you will work collaboratively with architects and other engineers to recommend, prototype, build and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring a combination of batch or streaming data pipelines, data lakes and data warehouses.\\n\\nYou will be recognized as an established contributor by your team. You will contribute design and implementation components for multiple projects. You will work mostly independently with limited oversight. You will also participate in client-facing discussions in areas of expertise.\\n\\nPathway to Success\\n\\n#BeOneStepAhead: At SADA Systems we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.\\n\\nYour success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers, and the consultative polish you bring to customer interactions.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the engineering or management growth tracks.\\n\\nExpectations\\n\\nRequired Travel - 30% travel to customer sites, conferences, and other related events\\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.\\nTraining - Ongoing with a first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Data Engineer Certified\\n\\n[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment\\n\\nRequired Qualifications:\\n\\nExpertise in at least one of the following domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore &amp; disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExperience writing software in one or more languages such as Python, Java, Scala, or Go\\nExperience building production-grade data solutions (relational and NoSQL)\\nExperience with systems monitoring/alerting, capacity planning and performance tuning\\nExperience in technical consulting or other customer-facing role\\n\\nUseful Qualifications:\\n\\nExperience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)\\nExperience with IoT architectures and building real-time data streaming pipelines\\nApplied experience operationalizing machine learning models on large datasets\\nKnowledge and understanding of industry trends and new technologies and ability to apply trends to architectural needs\\nDemonstrated leadership and self-direction -- a willingness to teach others and learn new techniques\\nDemonstrated skills in selecting the right statistical tools given a data analysis problem\\n\\nAbout SADA\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Data Engineer, Research</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Bachelor’s or Master’s in Computer Science, Computer Engineering, Electrical Engineering, Computer Information Systems, MIS, or relevant technology degree from a top-tier school.\\nMinimum of 5+ years SQL Server Development experience preferred.\\nDemonstrate knowledge and ability of database development skills including physical structure, overall architecture, and database analysis.\\nSolid software development skills in an object-oriented programming language (C#, C++, Java, etc.).\\nProgramming skills with statistical software including Python, R, SAS or Matlab a plus.\\nProven database design and implementation experience with the ability to provide end-to-end database solutions and resolve complex database issues.\\nExperience in database query optimization, performance tuning, and monitoring.\\nExpert knowledge of best practices in database design.\\nStrong time management skills with the ability to participate in multiple projects/work streams simultaneously.\\nDetail-oriented, organized, highly motivated and able to work independently and in a team environment.\\nExcellent verbal and written communications skills.\\nSelf-starter who is capable of managing multiple projects and meeting deadlines.\\nExperience with research in securities and financial markets preferred.\\nKnowledge of finance/asset pricing is preferred.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Design, compile and manage efficiently large financial databases from a variety of financial data vendors.\\nDevelop sophisticated code in object-oriented programming languages such as C#, C++ or Python for historical portfolio simulations and tools for investment and performance analysis.\\nConduct data analysis for projects related to research on equities and fixed income markets, retirement research, and other investment research.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Research group at Dimensional is essential both in the successful daily functioning of the firm and helping develop Dimensional’s long-term strategy. The team produces high-quality, expert research on investments and financial markets that is of interest to and helps educate clients. Research is also involved in the design of the firm’s investment approach and the application of that approach through portfolio management and trading.\\nThis position is responsible for the management and development of our proprietary global security-level database. This database is used by the Research team to enhance our understanding of long-term, short-term and intra-day drivers of expected returns and to help develop new investment strategies to meet the needs, goals and preferences of our clients. It will be primarily project-based work, extending existing code infrastructure and creating new ones. This role will be based in our Austin, TX headquarters and reports to a senior member of our Research team.\\nResponsibilities:\\nDesign, compile and manage efficiently large financial databases from a variety of financial data vendors.\\nDevelop sophisticated code in object-oriented programming languages such as C#, C++ or Python for historical portfolio simulations and tools for investment and performance analysis.\\nConduct data analysis for projects related to research on equities and fixed income markets, retirement research, and other investment research.\\nQualifications:\\nBachelor’s or Master’s in Computer Science, Computer Engineering, Electrical Engineering, Computer Information Systems, MIS, or relevant technology degree from a top-tier school.\\nMinimum of 5+ years SQL Server Development experience preferred.\\nDemonstrate knowledge and ability of database development skills including physical structure, overall architecture, and database analysis.\\nSolid software development skills in an object-oriented programming language (C#, C++, Java, etc.).\\nProgramming skills with statistical software including Python, R, SAS or Matlab a plus.\\nProven database design and implementation experience with the ability to provide end-to-end database solutions and resolve complex database issues.\\nExperience in database query optimization, performance tuning, and monitoring.\\nExpert knowledge of best practices in database design.\\nStrong time management skills with the ability to participate in multiple projects/work streams simultaneously.\\nDetail-oriented, organized, highly motivated and able to work independently and in a team environment.\\nExcellent verbal and written communications skills.\\nSelf-starter who is capable of managing multiple projects and meeting deadlines.\\nExperience with research in securities and financial markets preferred.\\nKnowledge of finance/asset pricing is preferred.\\nIt is the policy of the Company to provide equal employment opportunity for all applicants and employees. The Company does not unlawfully discriminate on the basis of race, color, religion, creed, sex, gender, gender identity, gender expression, national origin, age, disability, genetic information, ancestry, medical condition, marital status, covered veteran status, citizenship status, sexual orientation, or any other protected status. This policy applies to all areas of employment including recruitment, hiring, training, job assignment, promotion, compensation, benefits, transfer, discipline, termination, and social and recreational programs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Tech Consulting Manager - Big Data Engineer</td>\n",
       "      <td>Austin, TX 78701</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78701</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Designing, Architecting, and Developing solutions leveraging big data technology (Open Source, AWS, or Microsoft) to ingest, process and analyze large, disparate data sets to exceed business requirements\\nUnifying, enriching, and analyzing customer data to derive insights and opportunities\\nLeveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements\\nClearly communicating findings, recommendations, and opportunities to improve data systems and solutions\\nDemonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches\\nSeeking out information to learn about emerging methodologies and technologies\\nClarifying problems by driving to understand the true issue\\nLooking for opportunities for improving methods and outcomes\\nApplying data driven approach (KPIs) in tying technology solutions to specific business outcomes\\nCollaborating, influencing and building consensus through constructive relationships and effective listening\\nSolving problems by incorporating data into decision making\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>EY delivers unparalleled service in big data, business intelligence, and digital analytics built on a blend of custom-developed methods related to customer analytics, data visualization, and optimization. We leverage best practices and a high degree of business acumen that has been compiled over years of experience to ensure the highest level of execution and satisfaction for our clients. At EY, our methods are not tied to any specific platforms but rather arrived at by analyzing business needs and making sure that the solutions delivered meet all client goals.\\nThe opportunity\\nYou will help our clients navigate the complex world of modern data analytics. We’ll look to you to provide our clients with a unique business perspective on how Big Data analytics can transform and improve their entire organization - starting with key business issues they face. This is a high growth, high visibility area with plenty of opportunities to enhance your skillset and build your career.\\nYour key responsibilities\\nYou’ll spend most of your time working with a wide variety of clients to deliver the latest big data technologies and practices to design, build and maintain scalable and robust solutions that unify, enrich and analyse data from multiple sources.\\nSkills and attributes for success\\nDesigning, Architecting, and Developing solutions leveraging big data technology (Open Source, AWS, or Microsoft) to ingest, process and analyze large, disparate data sets to exceed business requirements\\nUnifying, enriching, and analyzing customer data to derive insights and opportunities\\nLeveraging in-house data platforms as needed and recommending and building new data platforms/solutions as required to exceed business requirements\\nClearly communicating findings, recommendations, and opportunities to improve data systems and solutions\\nDemonstrating deep understanding of big data technology, concepts, tools, features, functions and benefits of different approaches\\nSeeking out information to learn about emerging methodologies and technologies\\nClarifying problems by driving to understand the true issue\\nLooking for opportunities for improving methods and outcomes\\nApplying data driven approach (KPIs) in tying technology solutions to specific business outcomes\\nCollaborating, influencing and building consensus through constructive relationships and effective listening\\nSolving problems by incorporating data into decision making\\nTo qualify for the role you must have\\nA bachelor's degree and approximately six years of related work experience; or a master's degree and approximately five years of related work experience\\nAt least five years hands-on experience with various Big Data technologies in one or more ecosystems: Open Source, Microsoft, or AWS:\\nHadoop, Spark, NoSQL, Streaming, Atlas, Sqoop, HIVE\\nAWS, EMR, Hortonworks, Cassandra, Mongo, Redshift, Kafka\\nAzure, HDInsight, Azure DocumentDB, SQL Server\\nProficiency coding in Java, C#, C++, or Scala\\nExperienced organizing, aggregating, querying, and analyzing large data sets\\nCommunication is essential, must be able to listen and understand the question and develop and deliver clear insights.\\nOutstanding team player.\\nIndependent and able to manage and prioritize workload.\\nAbility to quickly and positively adapt to change.\\nA valid driver’s license in the US; willingness and ability to travel to meet client needs.\\nIdeally, you’ll also have\\nBachelor’s Degree or above in mathematics, information systems, statistics, computer science, or related disciplines\\nWhat we look for\\nWe’re interested in passionate leaders with strong vision and a desire to stay on top of trends in the Big Data industry. If you have a genuine passion for helping businesses achieve the full potential of their data, this role is for you.\\nWhat working at EY offers\\nWe offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package includes medical and dental coverage, both pension and 401(k) plans, a minimum of 15 days of vacation plus ten observed holidays and three paid personal days, and a range of programs and benefits designed to support your physical, financial and social well-being. Plus, we offer:\\nOpportunities to develop new skills and progress your career\\nA collaborative environment where everyone works together to create a better working world\\nExcellent training and development prospects, both through established programs and on-the-job training\\nAbout EY\\nAs a global leader in assurance, tax, transaction and advisory services, we hire and develop the most passionate people in their field to help build a better working world. This starts with a culture that believes in giving you the training, opportunities and creative freedom to make things better. So that whenever you join, however long you stay, the exceptional EY experience lasts a lifetime.\\nJoin us in building a better working world. Apply now.\\n\\nEY provides equal employment opportunities to applicants and employees without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Austin, TX 78701</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78701</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At RetailMeNot, we believe that gaining valuable insights using our data is core to our future success. The Data team at RetailMeNot is responsible for developing core datasets and for exposing data services consumed by product, data science and business teams. Daily, we collect approximately a terabyte of analytics events and process hundreds of terabytes of data. Our team works efficiently to deliver new features for real-time and batch processing services. We use primarily AWS cloud services and Kubernetes to build and deploy services quickly, at scale and with no downtime.\\n\\nThis team is integral to the RetailMeNot business, so we need engineers who can deliver results while understanding the structure of a large system. We provide cross-team leadership that ensures that RetailMeNot code meets a consistent standard while building the platform of the future. With this team, your daily activities will involve oversight, mentoring, delivering key pieces of functionality, and collaborating with technology leadership to plan the technical roadmap for RMN.\\n\\nWe are constantly evolving both the software and the teams that deliver it. If you’re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, then we want you to be a part of the team.\\nWho You Are\\nYou have 4+ years work experience\\nYou are highly skilled using Scala, Python (Spark), Linux, Docker, Git, and Amazon Web Services (or have translatable experience with similar toolsets)\\nYou have extensive SQL experience on a variety of RDBMS, and enjoy optimizing queries as well as designing efficient data models\\nYou have developed scalable solutions using both SQL and NoSQL (Hadoop) databases. Working with data sets comprised of millions or billions of records is comfortable\\nYou are familiar with one or more cluster-computing frameworks (Spark)\\nYou strive to identify simple solutions to complex problems, can identify a minimal viable product and enjoy iterative development\\nYou are able to accurately estimate tasks, identify dependencies and dedicatedly solve problems to ensure commitments are met\\nYou recognize that your success depends upon enabling your fellow team members to succeed; taking time to help others energizes you\\nYou enjoy gathering requirements from non-technical coworkers and delivering solutions that meet their needs and exceed their expectations\\nYou derive satisfaction from enabling the business to succeed and delighting coworkers, not building technology for its own sake\\nYou have a work ethic that inspires your fellow team members to give their best\\nWhat You'll Do\\nImplement data system for both real-time and warehouse applications\\nDevelop ETL processes that ensure data is accurate and available within SLAs\\nEnhance data models by developing integrations with business partners\\nSeek opportunities for performance improvement and implement optimizations\\nCreate dashboards that provide insight into the health of data integrations, ETL processes and data sets\\nMentor junior data engineers on standard methodologies and provide code review when needed\\nWho We Are\\nWe hire intelligent people and give them the autonomy to be creative, have an impact, and share standard methodologies.\\nWe have a generous leave policy for new parents and 401k matching.\\nWe have a thriving Diversity and Inclusion program that gives back to the community and supports multiple Austin events and organizations with like-minded goals throughout the year.\\nWe serve breakfast Mondays and Fridays, lunch four days a week, provide all the snacks you could dream of, and have our own coffee bar run by trained baristas.\\nWe have a Friday board game happy hour for co-workers to mingle or just relax.\\nWe provide reimbursements for cell phones and gym memberships.\\nWe have an outstanding open vacation policy.\\n\\nRewards*\\nWe offer an opportunity to be an integral part of a company that eagerly pursues disruption in its space to continue to drive innovation and lead the competition. Benefits of being an employee of RetailMeNot, Inc. include, but are not limited to the following:\\nCompetitive base &amp; bonus packages; salary negotiableLong Term Incentive PlanPerformance based rewards &amp; recognition for your hard work and serviceVery competitive benefits packages, including best-in-class parental leaveOpen &amp; flexible PTOCell phone &amp; gym membership reimbursementsFully stocked break room &amp; onsite catered breakfasts &amp; lunches multiple days/week\\nSome rewards do not apply to contract workers or interns.\\n\\nAbout Us\\nRetailMeNot, Inc. is a leading savings destination bringing people and the things they love together through savings with retailers, brands, restaurants and pharmacies. RetailMeNot makes everyday life more affordable through online and in-store coupon codes, cash back offers, discount gift cards, and the RetailMeNot Genie browser extension. Savings are also provided in consumers’ mailboxes through the RetailMeNot Everyday™ direct mail package, and at the pharmacy with RxSaver by RetailMeNot.\\n\\nRetailMeNot is a wholly owned subsidiary of Harland Clarke Holdings. http://www.retailmenot.com/corp or follow @RetailMeNot on social media.\\n\\nU.S. Equal Employment Opportunity/Affirmative Action Information\\nIndividuals seeking employment at RetailMeNot, Inc. are considered without regards to race, color, creed, religion, gender, gender identity, national origin, citizenship, age, sex, marital status, ancestry, physical or mental disability, veteran status, sexual orientation, or any other protected classification. You are being given the opportunity to provide the following information in order to help us align with federal and state Equal Employment Opportunity/Affirmative Action record keeping, reporting, and other legal requirements.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>AWS Data Engineer</td>\n",
       "      <td>Austin, TX 78727</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78727</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.</td>\n",
       "      <td>DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\n\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Amazon AWS Data Engineer is responsible for delivering Data On Cloud projects for Amazon AWS based deals. The ideal candidate would be responsible for developing and delivering AWS cloud solutions to meet today’s high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The AWS Data Engineer is a highly performant engineer responsible for delivering Cloud based Big Data and Analytical Solutions at our clients. Responsibilities include building data on cloud solutions at customers, leading Business and IT stakeholders through designing a robust, secure and optimized AWS solutions and ability to be hands-on delivering the target solution. This role will work with customers and leading internal engineering teams in delivering big data soltuions on cloud. Using Amazon AWS public cloud technologies, our AWS Data Engineer professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today’s corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:\\nProvide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on AWS and HadoopAbility to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, RDBMS, DW/DM, NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the AWS platform.\\n- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.Manage small teams of delivery engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.Able to travel up to 100% (M-TH)\\n\\nBasic QualificationsAt least 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)At least 5 years of experience in developing data ingestion, data processing and analytical pipelines for big data, relational databases, NoSQL and data warehouse solutionsExtensive experience providing practical direction within the AWS Native and HadoopExperience with private and public cloud architectures, pros/cons, and migration considerations.Minimum of 5 years of hands-on experience in AWS and Big Data technologies such as Java, Node.js, C##, Python, SQL, EC2, S3, Lambda, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, Kinesis, NiFI etc.Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Hive, NoSQL, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.5+ years of hands on experience in programming languages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix shell/Perl scripting etc.Minimum of 5 years of RDBMS experienceExperience in using Hadoop File Formats and compression techniquesExperience working with DevOps tools such as GitLabs, Jenkins, CodeBuild, CoePipeline CodeDeploy, etc.Bachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\n§ Certified AWS Developer - Associate\\n§ Certified AWS DevOps – Professional (Nice to have)\\n§ Certified AWS Big Data Specialty (Nice to have)\\n\\nNice-to-Have Skills/Qualifications:DevOps on an AWS platform. Multi-cloud experience a plus.Experience developing and deploying ETL solutions on AWS using tools like Talend, Informatica, MatillionStrong in Java, C##, Spark, PySpark, Unix shell/Perl scriptingIoT, event-driven, microservices, containers/Kubernetes in the cloud\\nExperience in Apache Maven a plusUnderstanding and implementation of Data Lake architecturesUnderstanding NoSQL technologies such as MongoDB, Cassandra, Hbase is a plusUnderstanding Solr, Elasticsearch is a plus\\n\\nProfessional Skill Requirements Proven ability to build, manage and foster a team-oriented environment Proven ability to work creatively and analytically in a problem-solving environment Desire to work in an information systems environment Excellent communication (written and oral) and interpersonal skills Excellent leadership and management skills\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At SpringML, we are all about empowering the 'doers' in companies to make smarter decisions with their data. Our predictive analytics products and solutions apply machine learning to today's most pressing business problems so customers get insights they can trust to drive business growth. We are a tight knit, friendly team of passionate and driven people who are dedicated to learning, get excited to solve tough problems and like seeing results, fast.\\nYour primary role will be to design and build data pipelines. You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive. In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies. If you believe you have these skills please email your resume to info@springml.com.\\n\\n\\nRequired Skills:\\n4-7 years Python and Java programming\\n3-5 years knowledge of Java/J2EE\\n3-5 years Hadoop, Big Data ecosystem experience\\n3-5 years of Unix experience\\nBachelors in Computer Science (or equivalent)\\nDuties and Responsibilities:\\nDesign and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.\\nRead, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.\\nMigrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.\\nShould have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.\\nAdditional Skills that are a plus:\\nC, Perl, Javascript or other programming skills and experience a plus\\nProduction support/troubleshooting experience\\nData cleaning/wrangling\\nData visualization and reporting\\nDevops, Kubernetes, Docker containers\\ndpFhuZNHdz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Austin, TX 78728</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78728</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nIngestion of data from multiple, unstructured sources using multiple analytics tools\\nImplementing ETL process\\nMonitoring performance and advising any necessary infrastructure changes\\nDefining data retention policies</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nVoted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.</td>\n",
       "      <td>Job Title: Data Engineer\\n\\nLocation: San Francisco, Chicago, San Jose, Palo Alto, Austin, TX\\n\\nTerms: Full-time, Contract, Contract-2-Hire\\n\\nAbout Trianz\\nTrianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms, and achieve results expected by senior management in their organizations- predictably.\\n\\nWhat We Stand For\\nOur clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So getting things right the first time is absolutely critical.\\n\\nAs a result, Trianz is focusing on three important themes in our engagement model with clients.\\nCrystallize business impact from a top management point of view\\nHelp Clients achieve results from strategy-by making execution predictable through innovative execution techniques\\nCreate a positive, enriching partnership experience in everything we do\\n\\nIndustries, Clients &amp; Practices\\nTrianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:\\n\\nCloud\\nAnalytics\\nDigitization\\nInfrastructure\\nSecurity\\n\\nSr. Data Engineer\\nJob Description\\nResponsibilities\\nIngestion of data from multiple, unstructured sources using multiple analytics tools\\nImplementing ETL process\\nMonitoring performance and advising any necessary infrastructure changes\\nDefining data retention policies\\n\\nRequirements\\n3+ years of relevant professional experience\\nExperience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)\\nProficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)\\nGood understanding of SQL Engine and able to conduct query performance tuning\\nStrong skills in one of the scripting language (Python, Ruby, Bash)\\n1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)\\n\\nWe are Growing Rapidly: 2019 Highlights\\n\\nTrianz is growing rapidly. Here are some highlights.\\n\\nVoted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.\\n\\nWon the “Customer Obsession Award” from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.\\n\\nWon UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.\\n\\nFeatured by IDC in their Spotlight series under the theme of “Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery”.\\n\\nAchieved 50%+ revenue and employee growth compared to prior year’s exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.\\n\\nTalk to us, Join us &amp; Develop into Leaders\\nCome join a dynamic global company. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is what’s fundamental for everyone at Trianz.\\n We are hiring at all levels of Trianz. And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!\\n Equal Opportunity Employer\\nTrianz does not discriminate on the basis of race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Staff Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Collaborate with peers on requirements, designs, code reviews, and testing\\nProduce designs and rough estimates, and implement features based on product requirements\\nDeliver efficient, maintainable, robust Java/Scala based microservices\\nProduce unit and end-to-end tests to improve code quality and maximize code coverage for new and existing features\\nProductize and operationalize machine learning algorithms\\nActively engage in technology discovery that can be applied to the product\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>7-10 years of professional software development experience\\n2+ years of data engineering or related experience\\nStrong Java and/or Scala experience\\nExperience with Agile development practices and continuous delivery\\nProficient understanding of distributed computing principles. microservice architectures and patterns\\nExperience with integration of data from multiple data sources\\nExperience writing unit and integration tests\\nGreat communication skills\\nBS in Computer Science or a related experience\\n</td>\n",
       "      <td>At SailPoint, we do things differently. We understand that a fun-loving work environment can be highly motivating and productive. When smart people work on intriguing problems, and they enjoy coming to work each day, they accomplish great things together. With that philosophy, we’ve assembled the best identity team in the world that is passionate about the power of identity.\\nAs the fastest-growing, independent identity and access management (IAM) provider, SailPoint helps hundreds of global organizations securely and effectively deliver and manage user access from any device to data and applications residing in the data center, on mobile devices, and in the cloud. The company’s innovative product portfolio offers customers an integrated set of core services including identity governance, provisioning, and access management delivered on-premises or from the cloud (IAM-as-a-service).\\nSailPoint is seeking a Sr/Staff Data Software Engineer to help build a new cloud-based identity analytics product incorporating real-time data pipelines, machine learning algorithms and multi-tenancy support. We are looking for well-rounded backend or full stack engineers who are passionate about building and delivering reliable, scalable microservices and infrastructure for SaaS products.\\nResponsibilities\\nCollaborate with peers on requirements, designs, code reviews, and testing\\nProduce designs and rough estimates, and implement features based on product requirements\\nDeliver efficient, maintainable, robust Java/Scala based microservices\\nProduce unit and end-to-end tests to improve code quality and maximize code coverage for new and existing features\\nProductize and operationalize machine learning algorithms\\nActively engage in technology discovery that can be applied to the product\\nRequirements\\n7-10 years of professional software development experience\\n2+ years of data engineering or related experience\\nStrong Java and/or Scala experience\\nExperience with Agile development practices and continuous delivery\\nProficient understanding of distributed computing principles. microservice architectures and patterns\\nExperience with integration of data from multiple data sources\\nExperience writing unit and integration tests\\nGreat communication skills\\nBS in Computer Science or a related experience\\nPreferred\\nExperience with Cloud computing architectures (AWS, Google Cloud)\\nExperience with Kafka, Flink/Spark, Elasticsearch technologies or related\\nExperience integrating data pipelines for machine learning\\nExperience with container technologies (Docker, Kubernetes, etc.)\\nExperience with NoSQL databases, such as Redshift, Cassandra, DynamoDB\\nExperience instrumenting code for gathering production performance metrics\\nCompensation and benefits\\nExperience a Small-company Atmosphere with Big-company Benefits\\nCompetitive pay, 401(k) and comprehensive medical, dental and vision plans\\nRecharge your batteries with a flexible vacation policy and paid holidays\\nGrow with us with both technical and career growth opportunities\\nEnjoy a healthy work-life balance with flexible hours, family-friendly company events and charitable work\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Data Engineer - Associate</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>\\nExperience with RDBMS applications (SQL Server preferred)\\nGood communication skills and experience working with cross-functional teams\\nExposure to the concepts of data warehouse design\\nSQL programming familiarity in large RDBMS systems (T-SQL preferred)</td>\n",
       "      <td>\\nTroubleshoot and resolve issues as they arise related to all BI Tools\\nManage iteration and release cycles and deployments\\nAssist in data modeling and design sessions\\nProactively maintain documentation and training materials</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer (early career)\\nAustin or Chicago\\n(Visa sponsorship not currently offered)\\n\\nMattersight is a leader in enterprise analytics focused on customer and employee interactions and behaviors. Mattersight's Behavioral Analytics service captures and analyzes customer and employee interactions, employee desktop data, and other contextual information to improve operational performance and predict future customer and employee outcomes. Mattersight’s analytics are based on millions of proprietary algorithms and the application of unique behavioral models. The company's SaaS+ delivery model combines analytics in the cloud with deep customer partnerships to drive significant business value. Mattersight's applications are used by leading companies in Healthcare, Insurance, Financial Services, Telecommunications, Cable, Utilities and Government. See What Matters™ by visiting www.Mattersight.com.\\n\\nData Engineer Role &amp; Responsibilities:\\nThe Data Engineer will be part of the Routing Analytics R&amp;D team, which provides the data sources and analytical tools used to help our clients derive maximum value from our Behavioral Routing solution. This position will support the day to day reporting needs of the Routing clients, including Business Monitoring, Insights, Product Development, Analysis &amp; Testing, and others. Responsibilities include troubleshooting issues that occur with existing report deliverables as well as developing new reports and integrating them into the overall service catalog. This role will require development, testing, and configuration management of all BI deliverables in coordination with Data Engineers, Software Engineers, and Testers within the organization. Additionally, the Data Engineer will be responsible for maintaining any documentation and training materials required to support the various business units the group serves.\\n\\nThis individual will also support the Data Warehouse Specialist to troubleshoot issues relating to the warehouse, especially as they impact the reporting environment.\\n\\nTo summarize, the Data Engineer will be responsible for, but not limited to, the following tasks:\\n\\nTroubleshoot and resolve issues as they arise related to all BI Tools\\nManage iteration and release cycles and deployments\\nAssist in data modeling and design sessions\\nProactively maintain documentation and training materials\\n\\nBecause of the data-centric culture and rapid growth of NICE Mattersight, a rich career path exists for the Data Engineer within Mattersight.\\n\\nPreferred Skills/Attributes\\nExperience with RDBMS applications (SQL Server preferred)\\nGood communication skills and experience working with cross-functional teams\\nExposure to the concepts of data warehouse design\\nSQL programming familiarity in large RDBMS systems (T-SQL preferred)\\nExposure to ETL and data integration processes\\n\\nRequired Knowledge, Skills &amp; Abilities\\n\\nPrevious Experience\\nFor this role, Mattersight is not requiring previous work experience in a technical role though some experience in a Business Intelligence environment working with BI visualization tools, relational databases, and/or data warehousing systems is helpful. Experience with ETL applications and data modeling/UML software is also helpful. Required is an interest in data pipelines, data aggregations, and creatively solving data-related problems as well as the motivation to dive deeper into the data engineering world. We’d also like to see a candidate who displays evidence of strong communication skills and the ability to work under pressure.\\nIdeal CandidateAnalyticalDetail OrientedStrong CommunicatorEntrepreneurialResults OrientedTask AgilityOperations-MindedAdept Time ManagerProblem SolverTeam PlayerSeeker of ExcellenceHigh Knowledge Bandwidth\\n\\nCompany Culture &amp; Facts\\nCorporate Culture\\nMattersight values diversity amongst its employees. Employees from all levels of experience and backgrounds are mingled together and are encouraged to learn about projects others are working on. Mattersight fosters teamwork as well as self motivation.\\nEligibility &amp; Location\\nMattersight seeks candidates authorized to work in the United States. The Routing Analytics Data Engineer role will be based out of Mattersight’s Austin location; candidates should anticipate little to no travel.\\nCompensation\\nMattersight is prepared to offer a highly competitive benefits and compensation package for the ideal candidate.\\nFor more information about Mattersight, visit http://www.mattersight.com/. Mattersight is committed to equal opportunity and affirmative action in all employment matters: Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, ancestry, marital or domestic partner status, national origin, disability or medical condition, pregnancy, veteran or military status, sexual orientation, gender identity, or on account of membership or affiliation with anyone in any of the foregoing categories, or any other protected category under federal, state or local law. Although particular legal provisions may differ in various locations in which we do business, our principles are the same worldwide.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Are you passionate about data ecosystems and the ability to use data to drive actionable change? If so this role with our team at Atlassian is for you. As a Data Engineer on the Customers Success and Support team, you'll be driving our business to scale via building and improving our data infrastructures and data pipeline.\\nSome examples of what you will be doing are:\\nArchitect, build, launch, and manage data models to enable analytics\\nDesign, build, and manage data warehouse\\nDesign, build, improve, and manage data pipelines and ETL jobs\\nCreate ETL scripts via SQL/HiveQL/SparkSQL\\nAutomate data pipeline and reporting processes\\nBuild data expertise and own data quality for the awesome pipelines you build\\nSome of the tools/languages/systems you'll use are:\\nHive, Spark, Postgres, Presto\\nAmazon EC2, EMR, S3\\nPython\\nDocker\\nTableau\\nSourceTree and Bitbucket\\nLinux Shell\\nYou'll work together with other data engineers, analysts, project managers, and subject matter experts to deliver impactful outcomes to the organization. You'll participate in multiple concurrent high-visibility projects along with occasional ad-hoc questions from your internal customers.\\n\\nWe continually require modifications to the data pipeline for improvements in quality, speed, and features. In this role you'll focus on building out the future state of our data pipeline. You'll help design event collection infrastructure, build data models, and ETL processes to collect, extract, and clean the data for subsequent reporting and analysis. The target is making our data model more scalable, reliable, maintainable, and better integrated with other parts of our data ecosystem.\\n\\nMore about you\\n\\nYou've been in a data engineering role for 5+ years and have a BS degree in Engineering, Computer Science, or other technical discipline. You value high-quality work with attention to detail and have a track record of delivering both. You combine curiosity with critical thinking and good judgment, and like asking \"why\" to unravel a seemingly complex problem and get to the root cause. You know how to gather, document, and interpret business requirements.\\n\\nYou're a wizard with SQL; better than anyone else you know. You've got practical experience working with large structured or unstructured datasets. You enjoy thinking about improvements to the ways in which data is consumed and then figuring out how to make it happen via reporting platforms and visualizations. You have a nearly insatiable desire to learn new concepts and technologies and apply them to your work.\\n\\nWhen you encounter a problem you come up with multiple solutions, weigh the tradeoffs and efforts, identify the best path forward, and exercise good judgment to drive ahead. You're comfortable interacting with people across all levels of an organization and can field questions during a presentation like a pro. You're self-driven and find ways to be impactful.\\n\\nYou're quick on your feet and take on challenges with ease. You can take an ambiguous assignment and derive valuable insight. You use multiple tools and methods to find solutions, and couple that with intuition and quick tests to prioritize how to unravel complicated problems.\\n\\n\\nMore about our team\\n\\nYou'll be joining a growing analytics and project delivery team located in multiple regions across the globe. We challenge each other constantly to improve our work and ask hard questions. We're direct, focused, and demand excellence, but there's laughter in every meeting because we thoroughly enjoy the work we do and the impact it has. We're constantly growing, learning, adapting, and trying new things. BBQ, tacos, and coffee are a few of our favorite things.\\n\\nMore about our benefits\\n\\nWhether you work in an office or a distributed team, Atlassian is highly collaborative and yes, fun! To support you at work (and play) we offer some fantastic perks: ample time off to relax and recharge, flexible working options, five paid volunteer days a year for your favourite cause, an annual allowance to support your learning &amp; growth, unique ShipIt days, a company paid trip after five years and lots more.\\n\\nMore about Atlassian\\n\\nCreating software that empowers everyone from small startups to the who’s who of tech is why we’re here. We build tools like Jira, Confluence, Bitbucket, and Trello to help teams across the world become more nimble, creative, and aligned—collaboration is the heart of every product we dream of at Atlassian. From Amsterdam and Austin, to Sydney and San Francisco, we’re looking for people who want to write the future and who believe that we can accomplish so much more together than apart. At Atlassian, we’re committed to an environment where everyone has the autonomy and freedom to thrive, as well as the support of like-minded colleagues who are motivated by a common goal to: Unleash the potential of every team.\\n\\nAdditional Information\\n\\nWe believe that the unique contributions of all Atlassians is the driver of our success. To make sure that our products and culture continue to incorporate everyone's perspectives and experience we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status.\\n\\nAll your information will be kept confidential according to EEO guidelines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Square Root is built on understanding our customers' data more deeply than they do, and our data engineers are instrumental in that. In this role, you'll take heterogeneous, unstructured data, mine it for insights, and apply it to business problems in innovative ways. While you don't need tons of experience, you'll need to be sharp and possess a genuine interest in using data to solve business problems. You'll be working side by side with experienced cloud architects and data engineers. We want someone that will grow with us and is comfortable with the idea of programming, algorithmic thinking, and automated testing.\\n\\nSound like your kind of challenge? Dig in to learn more!\\nThe Gig\\nYou'll design, develop, and improve our ETL Engine to allow for rapid customer implementations, minimal customer investment, data quality and consistency, flexibility, durability, and availability.\\nYou'll design scalable systems that work in concurrency and are fault-tolerant.\\nAs part of our implementation process, you'll work directly with customers to integrate their business data. You'll work closely with different teams at Square Root to deliver enhancements, ensure operational stability, and define + refine product features.\\nYou'll help expand the scalability of our system and maintain our customer relationships as we adapt to more clients and larger data volumes.\\nWe're all about driving action from data, and that includes putting our enterprise data at the fingertips of all Radicals to empower our team.\\nYou'll formulate metrics for business users using math, forecast models, and statistical packages with Python to compute them.\\nAbout You\\nA Master's degree in Computer Science, Computer Engineering, Information Technology, Information Sciences, or a related technical field.\\n2+ years' experience in a professional environment. You've practically applied engineering principles to the design and development of a data warehouse and scalable ETL pipeline.\\n2+ years' experience in SQL and Python. You must be able to write joins and use aggregate functions in SQL as well as understand data structures in Python.\\n1+ years' experience working with visualization technology such as Tableau or Looker.\\n1+ years' experience in cloud computing, including Amazon S3 and AWS EMR as well as familiarity with Microsoft Azure or Google Cloud Platform.\\n1+ years' experience with Big Data technologies such as Hadoop, Spark, Hive, and Kafka.\\n1+ years' experience with Linux and Jenkins or similar technology.\\nYou're familiar with a data lake pattern and understand the process of a slowly changing dimension.\\nYou enjoy digging into a problem and investigating potential causes and solutions. You think creatively and use your reasoning skills to discover connections and potential strategies.\\nYou're a thoughtful communicator able to articulate data insights to non-technical customers.\\nYou're an active listener. Once you understand internal and external customer problems, you're excited to propose and debate solutions which elevate the team or business.\\nYou're calm in a crisis and have examples of navigating difficult situations and relationships.\\nYou keep up with the latest and greatest in technology and bring insights back to the team.\\nYou've demonstrated an eagerness to learn, ability to adapt and perfect your work, and willingness to seek out help and put it to good use.\\nOur Radical Culture\\nOur culture is at the core of everything we do. As we grow, we're not only looking to hire the best and brightest, but we're also looking for people that share our values. This is the code we live by:\\nThink big. Do bigger. Big ideas are meant to be pursued. We have a bias for action, iteration, and impact.\\nBe Customer Inspired. Our customers' toughest challenges inspire us to build innovative software. We delight them by deeply understanding their business and driving results.\\nPartner. We're approachable, dependable, and collaborative. We go above and beyond to help our customers, our partners, and one another succeed.\\nThrive. We revere personal and professional growth. We recognize individuality, embrace authenticity, and celebrate each other's success.\\n\\nThe Good Stuff\\nFounded in Austin, Texas, we've been bootstrapped and profitable since our start in 2006. Along the way, we've added a slew of perks inspired by our team of Radicals. Here are some fan favorites:\\nFlexibility: no dress code, unlimited PTO, and paid parental leave\\nCompetitive benefits and compensation\\nA cozy campus of 1920's craftsman homes in downtown\\nTeam lunches, stocked kitchen + bar, coffee-snob compliant coffee machines\\nRadicals get $3,000 a year to learn anything (seriously!). PEOPLE Magazine recognized our Learn Anything program in their 2019 list of \"50 Companies that Care,\" located here (https://square-root.com/2019/07/blog-companies-that-care/).\\nWe're always looking for people to add to and evolve our culture + company. We want you to bring your unique experience and perspective, so don't worry about checking all the boxes. If our values and mission resonate with you, please apply! Additionally, Square Root is an Equal Opportunity Employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender and sexual identity, national origin, disability status, protected veteran status or any other characteristic protected by law.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Five or more years of professional experience as data engineer. Bachelor’s degree in Computer Science or equivalent experience.\\nDemonstrated experience in data warehousing and ETL development.\\nExperience building complex data pipelines using large, disparate data sources.\\nDemonstrated expert knowledge in SQL.\\nDemonstrated experience working with relational databases such as Oracle, Postgres and other modern database technologies.\\nProficiency in modern programming languages such as Python, R, Java.\\nThorough understanding of data movement and transformation tools, such as Informatica, Datastage or equivalent.\\nDemonstrated experience in selecting tools, methods, techniques, and evaluation criteria for designing optimal data engineering solutions.\\nDemonstrated experience in leading complex technical projects, including assigning tasks and selecting team members.\\nAbility to make technical presentations to teams, focus groups, management, and governance committees.\\nExcellent customer service, communication and collaboration skills.\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Design, develop, and automate scalable data engineering solutions by leveraging cloud infrastructure. Extend or migrate existing data pipelines to new cloud environment.\\nLead technical projects involving design and development of data pipelines for complex datasets. Document project plans, outline tasks and milestones, provide estimation of effort.\\nWork closely with business partners to devise and manage data pipelines, load frequency, data delivery mechanisms, and performance tuning.\\nIdentify and implement best practices for data engineering and software development to ensure quality delivery of enterprise solutions.\\nHelp enable team alignment by participating in code reviews, change management and team meetings.\\nDevelop and maintain detailed technical documentation of data engineering solutions.\\nCollaborate with key stakeholders, both internal and external, including enterprise data architect, data modelers, and subject matter experts (SMEs).\\n</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Job Posting Title:\\nSenior Data Engineer\\n-\\nHiring Department:\\nIQ - Information Quest\\n-\\nPosition Open To:\\nAll Applicants\\n-\\nWeekly Scheduled Hours:\\n40\\n-\\nFLSA Status:\\nExempt\\n-\\nEarliest Start Date:\\nImmediately\\n-\\nPosition Duration:\\nExpected to Continue\\n-\\nLocation:\\nUT MAIN CAMPUS\\n-\\nJob Description:\\nYou will be part of a team building the next generation data warehouse platform and will design, develop, and maintain complex extract, transform, and load (ETL) data pipelines using large heterogeneous datasets. You will also build data engineering solutions for complex data models that express academic and administrative business processes. Your expertise with leading technologies and tools such as Oracle, Postgres, Python, etc. will result in a valuable modern data warehouse that supports critical business decisions and data analysis processes. Your collaboration and communication skills will help to establish stakeholder relationships and ensure that your work products are in alignment with project goals. Most importantly, you will be passionate about working with data and will be a significant contributor to our university mission: To transform lives for the benefit of society.\\n-\\nJob Details:\\nGeneral Notes\\nSenior level technical role that builds and supports cloud-based Enterprise Data Warehouse ecosystem.\\nResponsibilities\\nDesign, develop, and automate scalable data engineering solutions by leveraging cloud infrastructure. Extend or migrate existing data pipelines to new cloud environment.\\nLead technical projects involving design and development of data pipelines for complex datasets. Document project plans, outline tasks and milestones, provide estimation of effort.\\nWork closely with business partners to devise and manage data pipelines, load frequency, data delivery mechanisms, and performance tuning.\\nIdentify and implement best practices for data engineering and software development to ensure quality delivery of enterprise solutions.\\nHelp enable team alignment by participating in code reviews, change management and team meetings.\\nDevelop and maintain detailed technical documentation of data engineering solutions.\\nCollaborate with key stakeholders, both internal and external, including enterprise data architect, data modelers, and subject matter experts (SMEs).\\nRequired Qualifications\\nFive or more years of professional experience as data engineer. Bachelor’s degree in Computer Science or equivalent experience.\\nDemonstrated experience in data warehousing and ETL development.\\nExperience building complex data pipelines using large, disparate data sources.\\nDemonstrated expert knowledge in SQL.\\nDemonstrated experience working with relational databases such as Oracle, Postgres and other modern database technologies.\\nProficiency in modern programming languages such as Python, R, Java.\\nThorough understanding of data movement and transformation tools, such as Informatica, Datastage or equivalent.\\nDemonstrated experience in selecting tools, methods, techniques, and evaluation criteria for designing optimal data engineering solutions.\\nDemonstrated experience in leading complex technical projects, including assigning tasks and selecting team members.\\nAbility to make technical presentations to teams, focus groups, management, and governance committees.\\nExcellent customer service, communication and collaboration skills.\\nPreferred Qualifications\\nFive years or more experience as data engineer designing and implementing complex data pipelines.\\nMaster’s degree in Computer Science, Information Technology or related field.\\nExperience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.).\\nExperience with AWS technologies.\\nSalary Range\\n$105,000 + depending on qualifications\\nWorking Conditions\\nMay work around standard office conditions.\\nRepetitive use of a keyboard at a workstation.\\nMay require occasional off-hours work.\\nRequired Materials\\nResume/CV\\n3 work references with their contact information; at least one reference should be from a supervisor\\nLetter of interest\\nImportant for applicants who are NOT current university employees or contingent workers: You will be prompted to submit your resume in the first step of the online job application process. Then, any additional Required Materials will be uploaded in the My Experience section; you can multi-select the additional files or click the Upload button for each file. Before submitting your online job application, ensure that ALL Required Materials have been uploaded. Once your job application has been submitted, you cannot make changes.\\nImportant for Current university employees and contingent workers: As a current university employee or contingent worker, you MUST apply within Workday by searching for Find Jobs. Before you apply though, log-in to Workday, navigate to your Worker Profile, click the Career link in the left hand navigation menu and then update the sections in your Professional Profile. This information will be pulled in to your application. The application is one page and you will need to click the Upload button multiple times in order to attach your Resume, References and any additional Required Materials noted above.\\n-\\nEmployment Eligibility:\\nRegular staff who have been employed in their current position for the last six continuous months are eligible for openings being recruited for through University-Wide or Open Recruiting, to include both promotional opportunities and lateral transfers. Staff who are promotion/transfer eligible may apply for positions without supervisor approval.\\n-\\nRetirement Plan Eligibility:\\nThe retirement plan for this position is Teacher Retirement System of Texas (TRS), subject to the position being at least 20 hours per week and at least 135 days in length.\\n-\\nBackground Checks:\\nA criminal history background check will be required for finalist(s) under consideration for this position.\\n-\\nEqual Opportunity Employer:\\nThe University of Texas at Austin, as an equal opportunity/affirmative action employer , complies with all applicable federal and state laws regarding nondiscrimination and affirmative action. The University is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, national origin, age, marital status, sex, sexual orientation, gender identity, gender expression, disability, religion, or veteran status in employment, educational programs and activities, and admissions.\\n-\\nPay Transparency:\\nThe University of Texas at Austin will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information.\\n-\\nEmployment Eligibility Verification:\\nIf hired, you will be required to complete the federal Employment Eligibility Verification I-9 form. You will be required to present acceptable and original documents to prove your identity and authorization to work in the United States. Documents need to be presented no later than the third day of employment. Failure to do so will result in loss of employment at the university.\\n-\\nE-Verify:\\nThe University of Texas at Austin use E-Verify to check the work authorization of all new hires effective May 2015. The university’s company ID number for purposes of E-Verify is 854197.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Azure Data Architect</td>\n",
       "      <td>Austin, TX 78727</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78727</td>\n",
       "      <td>None Found</td>\n",
       "      <td>At least 5 years of consulting or client service delivery experience on Azure\\n</td>\n",
       "      <td>DevOps on an Azure platform</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Proven ability to build, manage and foster a team-oriented environment\\n</td>\n",
       "      <td>Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery &amp; Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Azure Technical Architect is a highly performant Azure Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data solutions on cloud. Using Azure public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today's corporate and emerging digital applications.\\n\\nRole &amp; Responsibilities:Work with Sales and Bus Dev teams in providing Azure Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS &amp; NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nQualifications\\nBasic Qualifications\\nAt least 5 years of consulting or client service delivery experience on Azure\\nAt least 10 years of experience in big data, database and data warehouse architecture and delivery\\nMinimum of 5 years of professional experience in 2 of the following areas:\\n§ Solution/technical architecture in the cloud\\n§ Big Data/analytics/information analysis/database management in the cloud\\n§ IoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.\\n Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.\\n - Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nMCSA Cloud Platform (Azure) Training &amp; Certification\\nMCSE Cloud Platform &amp; Infratsructiure Training &amp; Certification\\nMCSD Azure Solutions Architect Training &amp; Certification\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nStrong in Power BI, Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience a plus - Azure, AWS, Google\\n\\nProfessional Skill Requirements\\n Proven ability to build, manage and foster a team-oriented environment\\n Proven ability to work creatively and analytically in a problem-solving environment\\n Desire to work in an information systems environment\\n Excellent communication (written and oral) and interpersonal skills\\n Excellent leadership and management skills\\n Excellent organizational, multi-tasking, and time-management skills\\n Proven ability to work independently\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Senior Software Engineer, Data Science</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Machine Learning team at RetailMeNot work on initiatives that enable us to provide users meaningful content at the right place, right time, and right price. We build, test and iterate on Machine Learning models, learning from and leveraging datasets with millions of daily visits. We are looking for software engineers who are not afraid of learning new technologies, have an itch or have been playing around with Machine Learning, and are ready to apply it to solve real and meaningful business problems.\\n\\nAs part of the Machine Learning team you will work together with software engineers, data scientist and data engineers leveraging the latest technology and bringing new ideas to the table and building the world’s largest savings destination. Our teams challenge each other to have fun while connecting shoppers to the brands they love. Some of the technologies that we use today include AWS (EMR), Spark, Docker, Luigi, and Tensorflow.\\n\\nThis team is integral to the RetailMeNot business, so we need engineers who can deliver results while understanding the structure of a large system. We provide cross-team leadership that ensures that RetailMeNot code meets a consistently high standard while building the platform to support the future of the company. Your daily activities will involve oversight, mentoring, delivering key pieces of functionality, and collaborating with technology leadership to plan the technical roadmap for RMN.\\n\\nWe are constantly evolving both the software and the teams that deliver it. If you’re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, then we want you to be a part of the team.\\nWho You Are\\nYou have a Bachelor's degree in computer science or equivalent STEM field, or equivalent work experience\\nYou have an ownership mentality and track record of successful high-quality results. You identify any ambiguous requirements and provide clarity when needed.\\nYou bring a proven understanding and application of computer science fundamentals: data structures, algorithms and design patterns.\\nYou’re proficient with technologies such as Java, AWS.\\nYou have an understanding of systems architecture technologies including Linux, Amazon Web Services, Kubernetes and Docker.\\nYou have high level understanding and interest in Machine Learning, and want to work together with scientist and data engineers to apply it to real life.\\nYou have experience with common software engineering tools such as Git, JIRA, TeamCity, Confluence or similar.\\nYou have 5+ years of application development experience.\\nWhat You'll Do\\nYou will work together with data scientist and data engineer to deliver improved algorithms and experiences using Machine Learning.\\nYou will understand and constantly improve the cloud infrastructure that runs our high performance, consumer-scale site.\\nYou will help support the build and deployment pipeline and when necessary both diagnose and solve production support issues.\\nYou will contribute to the overall system design, architecture, security, scalability, reliability, and performance of applications.\\nYou’ll consistently improve maintainability and stability of the codebase.\\nYou’ll coach and mentor junior engineers on software engineering techniques, process, and new technologies.\\nYou will also have the opportunity to stay ahead of new technologies with an eye to evaluating and potentially incorporating them into your team's architecture.\\nWho We Are\\nWe hire smart people and give them the autonomy to be creative in how they impact the business.\\nWe embrace Diversity and Inclusion as core values and have a thriving program to support it throughout the company. We give back to the community and support multiple Austin events and organizations with like-minded goals.\\nWe have an extraordinary open vacation policy.\\nWe offer a generous leave policy for new parents as well as 401k matching.\\nWe provide lunch four days a week, breakfast twice a week, all the snacks you could dream of, and have our own coffee bar run by trained baristas.\\nWe reimburse expenses for cell phone service and gym memberships.\\n\\nRewards*\\nWe offer an opportunity to be an integral part of a company that eagerly pursues disruption in its space to continue to drive innovation and lead the competition. Benefits of being an employee of RetailMeNot, Inc. include, but are not limited to the following:\\nCompetitive base &amp; bonus packages; salary negotiableLong Term Incentive PlanPerformance based rewards &amp; recognition for your hard work and serviceVery competitive benefits packages, including best-in-class parental leaveOpen &amp; flexible PTOCell phone &amp; gym membership reimbursementsFully stocked break room &amp; onsite catered breakfasts &amp; lunches multiple days/week\\nSome rewards do not apply to contract workers or interns.\\n\\nAbout Us\\nRetailMeNot, Inc. is a leading savings destination bringing people and the things they love together through savings with retailers, brands, restaurants and pharmacies. RetailMeNot makes everyday life more affordable through online and in-store coupon codes, cash back offers, discount gift cards, and the RetailMeNot Genie browser extension. Savings are also provided in consumers’ mailboxes through the RetailMeNot Everyday™ direct mail package, and at the pharmacy with RxSaver by RetailMeNot.\\n\\nRetailMeNot is a wholly owned subsidiary of Harland Clarke Holdings. http://www.retailmenot.com/corp or follow @RetailMeNot on social media.\\n\\nU.S. Equal Employment Opportunity/Affirmative Action Information\\nIndividuals seeking employment at RetailMeNot, Inc. are considered without regards to race, color, creed, religion, gender, gender identity, national origin, citizenship, age, sex, marital status, ancestry, physical or mental disability, veteran status, sexual orientation, or any other protected classification. You are being given the opportunity to provide the following information in order to help us align with federal and state Equal Employment Opportunity/Affirmative Action record keeping, reporting, and other legal requirements.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>The Data Engineer will be part of the Capture &amp; Analytics Data Services team, which provides the data sources and analytical tools used to help our clients derive maximum value from our Behavioral Analytics and Routing solutions. This position will support the day to day reporting needs of numerous clients, including Business Monitoring, Insights, Product Development, Analysis &amp; Testing, Telephony Integration and others. Responsibilities include troubleshooting issues that occur with existing report deliverables as well as developing new reports and integrating them into the overall service catalog. This role will require development, testing, and configuration management of deliverables in coordination with Data Engineers and Testers within the organization, as well as external vendor and customer resources. Additionally, the Data Engineer will be responsible for maintaining any documentation and training materials required to support the various business units the group serves.\\nThis individual will also support the Data Warehouse to troubleshoot issues relating to the warehouse, especially as they impact the reporting environment.\\n\\nTo summarize, the Data Engineer will be responsible for, but not limited to, the following tasks:\\n\\n1. Troubleshoot and resolve issues as they arise related to all Data Services\\n2. Help to improve automation within the data warehouse and customer environments\\n2. Collaborate with vendor and customer resources to build complex data feeds for reportings\\n3. Manage iteration and release cycles and deployments\\n4. Assist in data modeling and design sessions\\n5. Proactively maintain documentation and training materials\\n\\nBecause of the data-centric culture and rapid growth of NICE-Mattersight, a rich career path exists for the Data Engineer within Mattersight.\\n\\nWhat you definitely have:\\n2–5 years report development experience, specifically with Business Intelligence tools (Tableau preferred)Solid experience with RDBMS applications (SQL Server preferred)Good communication skills and experience working with cross-functional teamsStrong understanding of data warehouse design and implementation best practicesAbility to explain principles of data visualizationSQL programming familiarity in large RDBMS systems (T-SQL preferred)Exposure to ETL and data integration processes\\nWhat we’d also love to see:\\nProject experience preparing use cases and user requirementsDesign and development of micro-services-oriented ArchitectureExposure to reporting interfaces that integrate with popular telephony platforms\\nBe You\\n\\nWe are all different and that is powerful. Variability fuels our business and unites our work. It teaches us that strength lies in differences. To see what matters is our culture, and our culture starts with you.\\n\\nYour different perspectives inspire us to be better. Your diversity fosters creativity and accelerates our innovation. Your unique skills and abilities make us stand out. Your background and experiences help us reach our full potential.\\n\\nWe are committed to a workplace that is increasingly diverse and inclusive, so be your best you.\\n\\nNICE Systems is an Equal Opportunity/Affirmative Action Employer, M/F/D/V.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Database Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Data Engineer.\\nTeradata, Hadoop, SQL and Data Analysis, Python R Spark Scala,\\nDebugging, Azure Cloud\\n\\nQualifications\\n\\nB.Tech\\n\\nPrimary Location: US-TX-Austin\\nOrganization: UST USA\\nEmployment Type: Regular Employee\\nJob Type: Full-time\\n Day Job\\nJob Posting: Sep 27, 2019, 11:01:07 AM\\n\\n\\n UST-Global is an Equal Opportunity Employer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>None Found</td>\n",
       "      <td>Halfaker and Associates, LLC, an award winning high growth small business, creates innovative and customer-centric technology solutions in the areas of Cyber Security, Data Analytics, Software Engineering and IT Infrastructure to improve the health, security and well-being of all Americans. Our commitment to excellence and our vision to “Continue to Serve” has resulted in steady growth and an expanding client base across government agencies in the health, defense, security and intelligence sectors. Headquartered in Arlington, VA, we have employees nationwide and were recently named a 2018 Top Work Place by the Washington Post. Please take a moment to browse through our website and learn more about what it means to serve with Halfaker.\\n\\n\\nHalfaker has an opening for a Senior Data Engineer to join our talented, dynamic team. The key responsibilities for this position are:\\n\\nParticipate in technical research and development to enable continuous innovation within the infrastructure\\nResolve a wide range of hardware and network technology issues in a fast-paced environment\\nFirst escalation resource for Tier-3 troubleshooting for all assigned service desk tickets from creation to resolution\\nEnsure that system hardware, operating systems, software systems, and related procedures adhere to organizational SLAs\\nTroubleshooting ranges from basic issues to email related issues for Executive level users\\nProvisioning, installation/configuration, operation, and maintenance of systems hardware and network infrastructure\\nMaintain standard image for Windows platforms\\nOther tasks and projects as assigned\\n\\n\\nRequired Skills\\nHands on Experience with the Informatica PowerCenter v10x Software Installation and Configuration\\nKnowledge and/or Experience with administration of Informatica Enterprise Data Quality,\\nExperience with administration of Informatica DQ and PC modules.\\nExperience with administration of MDM tool\\nThose without Informatica experience but with experience in Jenkins, Puppet, Ansible or Terraform will also be considered.\\nPowerShell scripting is highly desirable\\nExcellent customer service, communication and organizational skills are required\\nExcellent troubleshooting and problem resolution skills for Desktops and Laptops\\nAbility to work independently with minimal direction providing technical and non-technical support to multiple users\\nCapable of working under pressure, while handling multiple tasks simultaneously\\nExperience providing services to the federal government is preferred\\nTravel may be required\\nAbility to work overtime and weekends required on occasion\\nAbility to sit in an office environment for long periods of time\\nRemote and on-call responsibilities may be required\\n\\n\\n\\nRequired Experience\\nBachelor's degree in computer science, systems engineering, or related technical discipline (8 years of additional relevant experience may be substituted for education)\\n5+ years of relevant experience\\nExperience providing IT support to multiple work sites\\nMS Active Directory administration (Users, Groups, GPO, Security, etc.)\\n\\n\\nHalfaker and Associates, LLC, is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/ Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. U.S. Citizenship is required for most positions.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title          Location    City State  \\\n",
       "0   Senior Data Engineer – Elastic Engineer  Austin, TX 73344  Austin  TX     \n",
       "1   Data Engineer                            Austin, TX 78746  Austin  TX     \n",
       "2   Data Engineer/Analytic Manager           Austin, TX 78746  Austin  TX     \n",
       "3   Data Engineer                            Austin, TX        Austin  TX     \n",
       "4   Data Engineering Manager                 Austin, TX        Austin  TX     \n",
       "..                       ...                        ...           ...  ..     \n",
       "68  Azure Data Architect                     Austin, TX 78727  Austin  TX     \n",
       "69  Senior Software Engineer, Data Science   Austin, TX        Austin  TX     \n",
       "70  Data Engineer                            Austin, TX        Austin  TX     \n",
       "71  Database Engineer                        Austin, TX        Austin  TX     \n",
       "72  Senior Data Engineer                     Austin, TX        Austin  TX     \n",
       "\n",
       "           Zip     Country  \\\n",
       "0   73344       None Found   \n",
       "1   78746       None Found   \n",
       "2   78746       None Found   \n",
       "3   None Found  None Found   \n",
       "4   None Found  None Found   \n",
       "..         ...         ...   \n",
       "68  78727       None Found   \n",
       "69  None Found  None Found   \n",
       "70  None Found  None Found   \n",
       "71  None Found  None Found   \n",
       "72  None Found  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Qualifications  \\\n",
       "0   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "2   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4   \\nExperience leading, managing and hiring a team of talented engineers\\nExpertise in at least one of the following engineering domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExpertise in at least one of the following data domains: * Predictive analytics (e.g., recommendation systems, predictive maintenance)\\nNatural language processing (e.g., conversational chatbots)\\nDocument understanding\\nImage classification\\nMarketing analytics\\nIoT systems\\nExperience writing software in one or more languages such as Python or Java/Scala\\nExperience in technical consulting or customer-facing role\\nExcellent critical thinking, problem-solving and analytical skills\\n   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ...   \n",
       "68  At least 5 years of consulting or client service delivery experience on Azure\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "69  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "70  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "71  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "72  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      Skills  \\\n",
       "0   None Found                                                                                                                                                                                                                                                                                                 \n",
       "1   Strong knowledge of statistics, including hands-on experience with SAS, R, Matlab, Machine Learning, AI.\\nExperience working with large datasets (1B+ Records)\\nKnowledge of Big Data or Cloud technologies\\nExperience with version control (e.g. TFS, SVN or Git) and build tools.\\nTableau/BI Tools\\n   \n",
       "2   None Found                                                                                                                                                                                                                                                                                                 \n",
       "3   None Found                                                                                                                                                                                                                                                                                                 \n",
       "4   None Found                                                                                                                                                                                                                                                                                                 \n",
       "..         ...                                                                                                                                                                                                                                                                                                 \n",
       "68  DevOps on an Azure platform                                                                                                                                                                                                                                                                                \n",
       "69  None Found                                                                                                                                                                                                                                                                                                 \n",
       "70  None Found                                                                                                                                                                                                                                                                                                 \n",
       "71  None Found                                                                                                                                                                                                                                                                                                 \n",
       "72  None Found                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Responsibilities  \\\n",
       "0   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "1   Develop logical data models and processes to transform, cleanse, and normalize raw data into high-quality datasets aligned with our analytical requirements.\\nDevelop and maintain comprehensive controls to ensure data quality and completeness.\\nManage data movement through our infrastructure. Streamline existing data workflows to create a flexible, reliable, and faster process.\\nDevelop real-time data transformations and validations.\\nIdentify and onboard new data sources. Collaborate with data vendors and internal stakeholders to define requirements and build interfaces. Troubleshoot and resolve issues with data feeds.\\nWork with our team of researchers to identify and analyze investment opportunities in real estate and fixed income securities markets.\\n   \n",
       "2   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "3   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "4   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "..         ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "68  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "69  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "70  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "71  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "72  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "\n",
       "     Education  \\\n",
       "0   None Found   \n",
       "1   None Found   \n",
       "2   None Found   \n",
       "3   None Found   \n",
       "4   None Found   \n",
       "..         ...   \n",
       "68  None Found   \n",
       "69  None Found   \n",
       "70  None Found   \n",
       "71  None Found   \n",
       "72  None Found   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Requirement  \\\n",
       "0   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "1   3-5 years of experience in data analysis and/or management in an enterprise environment within finance, operations or analytics.\\nPassion for data organization, quality, and reliability.\\nMS SQL Server, Oracle, Postgres, Hive, Presto, etc. preferred. Experience developing complex efficient queries, designing and building logical data models, and working with large datasets on a relational database system\\nExperience with at least one language (e.g. Python, C#, Scala, Java).\\nStrong problem-solving skills.\\nMust be an intellectually curious self-starter and motivated to continually learn.\\nProactive, hardworking team player with excellent communication skills\\n   \n",
       "2   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "3   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "4   None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "..         ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "68   Proven ability to build, manage and foster a team-oriented environment\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "69  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "70  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "71  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "72  None Found                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 FullDescriptions  \n",
       "0   Introduction\\nAt IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.\\n\\nYour Role and Responsibilities\\nWe are looking for a Data engineer to deploy complex big data and analytics solutions using Elastic stack. You should have strong experience in deploying and managing Elastic cluster on Kubernetes in multi-site, multi cluster environment in both on-Premise as well as Cloud platforms. You should have applied or expert knowledge in big data platforms. The main use case for such a platform for us would be Real-Time Anomaly Detection and Time Series models on IT operations data like logs, metrics, events, wired data, transaction flow, ITIL process related data, knowledge repositories, etc\\nBusiness Unit/ Team Overview\\nGlobal Technology Services (GTS) at IBM manages the IT infrastructure for some of the world’s leading corporations and with that comes the responsibility of managing enormous amounts of IT data and the opportunity for making better decisions using that data. In GTS analytics team at IBM, Data Scientists, Data Engineers, and BigData IT Architects are developing novel models, cutting edge algorithms, and custom analytics solutions to tackle BigData challenges in the IT Infrastructure space.\\nITOA / AIops provides real time machine-data (log, events, performance, capacity, ITIL data, wire data, etc) analytics solutions that helps customers manage Business Services and manage the quality of the end-user experience\\nIt can tell a client in real time ‘What happened’, ‘Why did it happen’, ‘Will it happen again’ and ‘What to do if it happens again? etc\\nKeeps everyone on the same page by looking at the same Business Transaction data and metrics.\\nKeeps the focus on operational data that translate to the business value the application delivers; dive in deeper when appropriate.\\nIdentify resolution criteria, assign ownership\\nTake lessons learned to improve development, test, deployment, and production processes\\nEducation & Experience\\nMinimum 4 years of relevant experience working on the Elastic based products & distributions specifically used in Real Time ITOA or AIops use-cases processing logs, metrics, events, etc\\nAt least 4 years of experience in development & implementation of logging and metrics solutions in with TB+ / day ingestion per day\\nAt least 5 years of hands-on experience in IT support (Infrastructure / Application) and IT monitoring tools\\nOverall 7+ years of core Big data / Analytics experience in various domains\\nDegree / Master’s degree in computers or equivalent\\nCertifications:\\nElastic certified engineer\\nCertifications showing proficiency in the Usage, design & deployment of ITOA / AIOps solutions like Elastic or Splunk\\nSpecialized certifications on specific technologies like Hadoop, Cloudera, Spark, Kafka, etc\\nJob Responsibilities\\nDeploy Elastic stack cluster on native kubernetes or Kubernetes services and maintain the clusters efficiently\\nLeading end to end deployment of ITOA / AIOps solutions for enterprise customers\\nProvide engineering inputs to Architects and Data scientists on various stages of solution design\\nPerform Integration and deployment of ITOA solution as per design provided by Architects\\nParticipate & be an active member of internal capability building projects\\nTrain & support junior resources as needed\\nProvide resolution to customer queries and issues\\nSkills Required:\\nExcellent knowledge on log analytics, time series data anomaly detection and correlation of events\\nHands-on experience with IT operational data like logs, metrics, events, RDBMS tables, etc and ingesting them into Elastic stack\\nExpert Knowledge on GO/grok/REGEX/Logstash/Fluentd to perform Extract, Transform and Load for IT operational data into big data repositories like Elasticsearch, Cassandra, Hadoop, etc\\nExpert level experience in managing large Elastic cluster and in-depth knowledge in Elastic features\\nAlerting\\nSecurity\\nCurator\\nReporting\\nMonitoring\\nBackup and resiliency\\nKubernetes cluster management\\nPython/R/Scala languages/Scripting Languages in context of Anomaly Detection & Time Series modelling\\nWorking experience with ITIL Framework\\nWorking knowledge on Apache Hadoop, Spark, Airflow, Cassandra and Kafka ecosystem\\nPrior experience in deploying Elastic solutions in production environments processing operational data in terms of at least 500 GB / day\\nExperience with SQL based tools & expertise on any one traditional RDBMS – mySQl; MSSQL;Oracle;DB2 etc\\nPrior experience with DevOps projects, Github, Jira, Travis, etc\\nWorking knowledge on Windows, Linux and AIX platforms\\nWorking knowledge of Top commercial distributions of the above stack – MapR; Cloudera; Hortonworks etc\\nKnowledge on shell scripting\\nGood knowledge on other Top level Apache Big Data technologies like Cassandra, NIFI, Fluentd, Drill, Sentry etc\\nExcellent understanding on HDFS & other similar Map/Reduce paradigms\\nKnowledge on 1-2 NoSQL databases – Redis; MongoDB;Cassandra;Neo4j; VoltDB etc\\nPreferred:\\nExperience with Elastic ML and real-time operations analytics using Apache suit of products like Spark using Python or Scala\\nExperience with Kibana plug-in development and other UI development\\nExperience working with large data sets leveraging distributed systems e.g. Spark/Hadoop.\\nTools & Methods (Experience in at least one in each category or similar if not listed below)\\nLog Analytics – Elastic Search, Apache Solr\\nData Pipelines: Logstash, Fluentd, Kafka, Nifi\\nLanguages: Python, PySpark, Spark, Scala, R, Java, Java Script\\nVisualization : Kibana, Tableau, Cognos\\nMachine learning – Elastic ML, Python, Spark, Tensorflow, H2O\\nStreaming: Spark; Storm;\\nRelational Database technologies: Oracle, Db2, SQL, MySQL,\\nNoSQl DB’s: MongoDB, Cassandra, Neo4J,Redis, VoltDB, CouchDB\\nApache Hadoop Distribution – Apache, Hortonworks, Cloudera, MapR\\nETL technologies: Datastage, Informatica, Pentaho DI, SAS DI, SSIS or R, Python based Data munging\\nCloud technologies: AWS, Azure, IBM Softlayer\\nSoft Skills\\nExcellent Written & Verbal Communication\\nExcellent Analytical & Virtual troubleshooting skills\\nSkills to work in team and collaborative environment\\nCustomer/Vendor interaction & co-ordinations\\n\\nRequired Technical and Professional Expertise\\n7+ years of professional hands on experience in IT operations\\nExcellent knowledge on log analytics, time series data anomaly detection and correlation of events\\nHands-on experience with IT operational data like logs, metrics, events, RDBMS tables, etc and ingesting them into Elastic stack\\nExpert Knowledge on GO/grok/REGEX/Logstash/Fluentd to perform Extract, Transform and Load for IT operational data into big data repositories like Elasticsearch, Cassandra, Hadoop, etc\\nExpert level experience in managing large Elastic cluster and in-depth knowledge in Elastic features\\nPython/R/Scala languages/Scripting Languages in context of Anomaly Detection & Time Series modelling\\nWorking experience with ITIL Framework\\nWorking knowledge on Apache Hadoop, Spark, Airflow, Cassandra and Kafka ecosystem\\nPrior experience in deploying Elastic solutions in production environments processing operational data in terms of at least 500 GB / day\\n\\n\\nPreferred Technical and Professional Expertise\\nExperience with SQL based tools & expertise on any one traditional RDBMS – mySQl; MSSQL;Oracle;DB2 etc\\nPrior experience with DevOps projects, Github, Jira, Travis, etc\\nWorking knowledge on Windows, Linux and AIX platforms\\nWorking knowledge of Top commercial distributions of the above stack – MapR; Cloudera; Hortonworks etc\\nKnowledge on shell scripting\\nGood knowledge on other Top level Apache Big Data technologies like Cassandra, NIFI, Fluentd, Drill, Sentry etc\\nExcellent understanding on HDFS & other similar Map/Reduce paradigms\\nKnowledge on 1-2 NoSQL databases – Redis; MongoDB;Cassandra;Neo4j; VoltDB etc\\n\\n\\n\\nAbout Business Unit\\nAt Global Technology Services (GTS), we help our clients envision the future by offering end-to-end IT and technology support services, supported by an unmatched global delivery network. It's a unique blend of bold new ideas and client-first thinking. If you can restlessly reinvent yourself and solve problems in new ways, work on both technology and business projects, and ask, \"What else is possible?\" GTS is the place for you!\\n\\nYour Life @ IBM\\nWhat matters to you when you’re looking for your next career challenge?\\n\\nMaybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities – where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust – where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.\\n\\nImpact. Inclusion. Infinite Experiences. Do your best work ever.\\n\\nAbout IBM\\nIBM’s greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.\\n\\nLocation Statement\\nFor additional information about location requirements, please discuss with the recruiter following submission of your application.\\n\\nBeing You @ IBM\\nIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.  \n",
       "1   Data Engineer – Austin, TX\\nAmherst is revolutionizing the way U.S. real estate is priced, managed and financed in order to unlock opportunities for all market participants. Driven by data, analytics, and technology, Amherst has a 20-year history of anticipating where the next risks and opportunities are likely to emerge and designing actionable strategies for investors to capitalize on opportunities across residential real estate, commercial real estate and public securities. Amherst, along with its affiliates and subsidiaries, has more than 900 employees, $5 billion under management and approximately $15 billion under advisement and oversight. www.amherst.com.\\nWe are hiring for a Data Engineer to utilize their industry knowledge, technical skills and passion for data to work closely with executive stakeholders and our financial engineering team developing solutions that support and optimize business operations. We’re solving a variety of Big Data challenges and modernizing legacy data loaders as well as exploring the benefits and tradeoffs of other cutting edge tools. In this role, you will be responsible for a variety of duties including; understanding our data, application design and development, SQL query optimization and ensuring accuracy and consistency of data.\\nIdeal Candidate:\\nAnalytical mindset with the ability to structure and process qualitative data and draw insightful conclusions.\\nProblem solver able to take a complex business request and transform it into a clean, simple data solution.\\nQuick learner open to new ideas and technologies, and willing to offer creative solutions.\\nOwnership and prideful in work and brings new ways and ideas to the table.\\nResponsibilities:\\nDevelop logical data models and processes to transform, cleanse, and normalize raw data into high-quality datasets aligned with our analytical requirements.\\nDevelop and maintain comprehensive controls to ensure data quality and completeness.\\nManage data movement through our infrastructure. Streamline existing data workflows to create a flexible, reliable, and faster process.\\nDevelop real-time data transformations and validations.\\nIdentify and onboard new data sources. Collaborate with data vendors and internal stakeholders to define requirements and build interfaces. Troubleshoot and resolve issues with data feeds.\\nWork with our team of researchers to identify and analyze investment opportunities in real estate and fixed income securities markets.\\nRequirements:\\n3-5 years of experience in data analysis and/or management in an enterprise environment within finance, operations or analytics.\\nPassion for data organization, quality, and reliability.\\nMS SQL Server, Oracle, Postgres, Hive, Presto, etc. preferred. Experience developing complex efficient queries, designing and building logical data models, and working with large datasets on a relational database system\\nExperience with at least one language (e.g. Python, C#, Scala, Java).\\nStrong problem-solving skills.\\nMust be an intellectually curious self-starter and motivated to continually learn.\\nProactive, hardworking team player with excellent communication skills\\nBonus Skills:\\nStrong knowledge of statistics, including hands-on experience with SAS, R, Matlab, Machine Learning, AI.\\nExperience working with large datasets (1B+ Records)\\nKnowledge of Big Data or Cloud technologies\\nExperience with version control (e.g. TFS, SVN or Git) and build tools.\\nTableau/BI Tools\\nWhat We Offer:\\nCompetitive salaries\\nChoice of Mac or Windows hardware\\nFlexible vacation days, paid holidays, and work from home options\\nMedical, Dental, Vision, LTD, Life, EAP, and 401K with matching benefits\\nStellar colleagues with proven track records\\nFree sodas, kombucha, cold brew, beer and healthy and unhealthy snacks at our Barton Springs WeWork location\\nFree lunch every day and breakfast tacos on Thursdays!\\nAmherst is proud to be an Equal Opportunity Employer and committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, color, religion, national origin, gender, pregnancy, sexual orientation, gender identity, age, physical or mental disability, genetic information or veteran status, and encourage all applicants to apply.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "2   ABOUT THE TEAM:\\nOur team oversees all aspects of eBay’s finance, analytics, and information technology functions – including controllership, procurement, financial planning and analysis, tax, treasury, audit, mergers and acquisitions, and investor relations. At eBay, we love data, so finance plays a critical role in establishing strategic focus, enabling growth, ensuring execution and driving efficiencies across the organization.\\nJOB REQUIREMENTS\\nGlobal Collections analytics team is responsible for delivering business insights and performance insights for Collections and Treasury.\\nWe are currently seeking a Data Engineer/Analytic Manager to lead the team and support our department. In this role you will support internal collections teams, own development and implementation of analysis and help prioritize key bad debt reduction initiatives through decisions based on data without impact on top line revenue.\\nCHARACTERISTICS:\\nIN THIS ROLE THE MANAGER WILL:Deliver insights into customers, processes and products to drive improvements in collections and reducing bad debt for eBay\\nConduct post-mortem loss analysis on strategic growth initiatives and share updates with the collections leadership team\\nPerform exploratory data analysis to develop hypotheses for causes of bad debt\\nDevelop SQL queries to extract data for both analysis and model construction\\nDesign eye-catching visualizations in Tableau and build supporting calculations\\nBuild database tables and schemas to support the assembly of data for analytics\\nDevelop SQL code for ETL projects\\nSupport critical Collections metrics and build out self-serve reporting that will strengthen decision quality through meaningful data\\nDemonstrate the ability to work in ambiguity using experience and proven theory knowledge\\nPartner with other cross functional teams within eBay to use standard methodology, share data analysis and establish business cases\\nProvide cross-functional, data-driven recommendations for lowering bad debt while remaining aligned with other team goals\\nAssess events on customer invoice timeline to drive better business outcomes\\nBASIC QUALIFICATIONS\\nIf you work well in a demanding environment, you learn quickly and possess extensive collection/risk management experience, this is a role you should consider. Being able to demonstrate the ability to breakdown processes, look for immediate opportunities and have a vision for future needs is what we need. In this role you will have the opportunity to scope and size things globally, recommend solutions that are scalable and balanced while considering future growth expectations for the company.\\nAnalytics experience in Tech or Financial Industries\\nProficient in SQL\\nExperience developing Tableau dashboards with a deep understanding of associated data architecture trade-offs\\n+5 years’ analytics/database experience\\n+5 years Software development, software testing, or operational improvement experience preferred\\nExperience working with groups remotely and multiple cultures\\nProficient in MS Office applications, Tableau, SQL, Teradata, and Unix commands\\nAbility to think from the perspective multiple user and employee personas\\nFamiliarity with traditional collection metrics (roll rates, liquidation and charge off rates)\\nProficient in Microsoft Excel, Word and PowerPoint\\nCollections and Loss Financial Data Analysis Experience (preferred)\\nFluency in English is a requirement with excellent communication skills\\nExcellent collaboration skills, team-work, and influencing business outcomes across a matrixed environment\\nBA/BS degree required\\nWe love creating opportunities for others by connecting people from widely diverse backgrounds, perspectives, and geographies. So, being diverse and inclusive isn’t just something we strive for, it is who we are, and part of what we do each and every single day. We want to ensure that as an employee, you feel eBay is a place where, no matter who you are, you feel safe, included, and that you have the opportunity to bring your unique self to work.. To learn about our Diversity & Inclusion click here: https://www.ebayinc.com/our-company/diversity-inclusion/.\\nThis website uses cookies to enhance your experience. By continuing to browse the site, you agree to our use of cookies\\nView our privacy policy\\nView our accessibility info\\neBay Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talent@ebay.com. We will make every effort to respond to your request for disability assistance as soon as possible.\\nFor more information see:\\nEEO is the Law Poster\\nEEO is the Law Poster Supplement                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "3   Job Description\\n\\nOur Data Engineering team builds and maintains a secure, scalable, flexible and user-friendly analytics hub that allows us to make informed and data-driven decisions. They also construct and curate business-critical data sets that allow us to realize the value of all the data we collect.\\nA Data Engineer utilizes a multidisciplinary approach to providing ETL solutions for the business, combining technical, analytical, and domain knowledge. The perfect applicant for this role has strong development skills, experience transforming and profiling data to determine risks associated with proposed analytics solutions, a willingness to continually interface with analysts in order to determine an optimal approach, and an eagerness to explore data sources to understand the availability, utility, and integrity of our data.\\nWhat you'll own:\\nData pipeline / ETL development:\\nBuilding and enhancing data curation pipelines using tools like SQL, Python, Glue, Spark and other AWS technologies\\nFocus on data curation on top of datalake data to produce trusted datasets for analytics teams\\nData Curation:\\nProcessing and cleansing data from a variety of sources to transform collected data into an accessible and curated state for Analysts and Data Scientists\\nMigrating self-serve data pipeline to centrally managed ETL pipelines\\nAdvanced SQL development and performance tuning\\nSome exposure to Spark, Glue or other distributed processing frameworks helpful\\nWork with business data stewards & analytics team to research and identify data quality issues to be resolved in the curation process\\nData Modeling:\\nDesign and build master dimensions to support analytic data requirements\\nReplacing legacy data structures with new datasets sourced from streaming data feeds from the core product and other operational systems\\nDesign, build and support pipelines to deliver business critical datasets\\nResolve complex data design issues & provide optimal solutions that meet business requirements and benefit system performance\\nQuery Engine Expertise & Performance Tuning:\\nAssist Analytics teams with tuning efforts\\nCurated dataset design for performance\\nOrchestration:\\nManagement of job scheduling\\nDependency management mapping and support\\nDocumentation of issue resolution procedures\\nData Access\\nDesign and management of data access controls mapped to curated datasets\\nLeveraging devops best practices, such as IAC and CI/CD to build upon a scalable and extensible data environment\\n\\nExperience you'll need:\\nStrong experience designing and building end-to-end data pipelines\\nExtensive SQL development experience\\nKnowledge of data management fundamentals and data storage principles\\nData modeling:\\nNormalization\\nDimensional/OLAP design and data warehousing\\nMaster data management patterns\\nModeling trade-offs impacting data management & processing/query performance\\nKnowledge of distributed systems as it pertains to data storage, data processing and querying\\nExtensive experience in ETL and DB performance tuning\\nHands on experience with a scripting language (Python, bash, etc.)\\nSome experience with Hadoop, Spark, Kafka, Impala, or other big data technologies helpful\\n\\nFamiliarity with the technology stacks available for:\\nMetadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\nData management, data processing and curation:\\nPostgres, Hadoop, Hive, Impala, Presto, Spark, Glue, etc.\\nExperience in data modeling for batch processing and streaming data feeds; structured and unstructured data\\nExperience in data security / access management, data cataloging and overall data environment management\\n\\nExperience with cloud services such as AWS and APIs helpful\\nYou’d be a great fit if your current track record looks like this:\\n5+ years of progressive experience data engineering and data warehousing\\nExperience with a variety of data management platforms (e.g. RDBMS (Postgres), Hadoop (CDH, EMR))\\nExperience with high performance query engines (Hive, Impala, Presto, Athena, MPP engines like RedShift)\\nStrong capability to manipulate and analyze complex, high-volume data from a variety of sources\\nEffective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language\\nAbility to problem solve independently and prioritize work based on the anticipated business value\\n\\nQualifications\\n\\nnull\\n\\nAdditional Information\\n\\nAll your information will be kept confidential according to EEO guidelines.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "4   Join SADA as a Data Engineering Manager!\\n\\nYour Mission\\n\\nAs a Data Engineering Manager at SADA, you will build and lead a growing Data Engineering team as we deliver robust data solutions for our clients on Google Cloud Platform (GCP). You will be responsible for managing a blended team of data engineers and data scientists, so a broad background in Big Data, data warehouse modernization, analytics, disaster recovery, data science, and machine learning is highly advantageous.\\n\\nThe diversity of customers that SADA works with ensures a steady flow of challenging data work. Be prepared to tackle real-world data problems that our customers find too difficult or time-consuming to solve themselves. Engagements vary from being purely consultative to requiring heavy hands-on work and cover a diverse array of data domain areas. Management here at SADA also means developing people and being a leader.\\n\\nIn this role, you will:\\n\\nBe comfortable working with customer executives to align business outcomes with technical vision and goals.\\nGuide the day-to-day activities of a geographically distributed team, including hiring world-class talent, reviewing work and setting goals.\\nProvide technical and professional leadership and mentorship on a diverse range of subject matter areas, such as Big Data pipelines and data warehouses to statistics and machine learning.\\nDevelop and codify best practices for your team that can be replicated across multiple customer engagements.\\nPartner with your team to develop services and offerings that scale and are repeatable.\\nParticipate in key technical and design discussions with technical leads as a hands-on manager.\\nPartner with other practice leads, architects, project managers, executives and sales personnel to develop statements of work, and then oversee execution by your team with high levels of agility and quality.\\n\\nPathway to Success\\n\\n#BeOneStepAhead: At SADA we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our employees know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.\\n\\nYour success starts by positively impacting the direction of a fast-growing data practice area with vision and passion. You will be measured by your team’s performance on customer engagements, how well your team achieves internal organizational goals, how well you collaborate with and support your team and peers, and the consultative polish you bring to customer interactions.\\n\\nAs you continue to execute successfully, we will build a customized development plan together that leads you through the management growth track.\\n\\nExpectations\\n\\n\\nRequired Travel - 15-25% travel to customer sites, conferences, and other related events\\nCustomer Facing - You will interact with customers on a regular basis, sometimes daily, other times weekly/bi-weekly. Common touchpoints occur when qualifying potential opportunities, at project kickoff, throughout the engagement as progress is communicated, and at project close. You can expect to interact with a range of customer stakeholders, including engineers, technical project managers, and executives.\\nTraining - Ongoing with first-week orientation at HQ followed by a 90-day onboarding schedule. Details of the timeline can be shared.\\n\\nJob Requirements\\n\\nRequired Credentials:\\n\\nGoogle Professional Data Engineer Certified\\n\\n[https://cloud.google.com/certification/data-engineer] or able to complete within the first 45 days of employment\\n\\nRequired Qualifications:\\n\\nExperience leading, managing and hiring a team of talented engineers\\nExpertise in at least one of the following engineering domain areas: * Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive).\\nData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.\\nBackup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale.\\nExpertise in at least one of the following data domains: * Predictive analytics (e.g., recommendation systems, predictive maintenance)\\nNatural language processing (e.g., conversational chatbots)\\nDocument understanding\\nImage classification\\nMarketing analytics\\nIoT systems\\nExperience writing software in one or more languages such as Python or Java/Scala\\nExperience in technical consulting or customer-facing role\\nExcellent critical thinking, problem-solving and analytical skills\\n\\nUseful Qualifications:\\n\\nExperience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc)\\nExperience in a large scale, high-volume data warehouse environment\\nExperience operationalizing machine learning models on large datasets\\nDemonstrated skills in selecting the right statistical tools given a data analysis problem\\n\\nAbout SADA\\n\\nValues: We built our core values\\n[https://sadasystems.com/about/company-overview/] on themes that internally compel us to deliver our best to our partners, our customers and to each other. Ensuring a diverse and inclusive workplace where we learn from each other is core to SADA’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer.\\n\\n1. Make them rave\\n2. Be data driven\\n3. Be one step ahead\\n4. Be a change agent\\n5. Do the right thing\\n\\nWork with the best : SADA has been the largest partner in North America for GCP since 2016 and recently announced, at Google NEXT, as the\\n2018 Global Partner of the Year\\n[https://sadasystems.com/blog/google-cloud/gcp/sada-wins-2018-google-cloud-partner-award/]. SADA has also been awarded\\nBest Place to Work\\n[https://sadasystems.com/blog/news/sada-systems-best-places-work-2019/] by Inc. as well as LA Business Journal!\\n\\nBenefits : Unlimited PTO\\n[https://sadasystems.com/blog/news/blogannouncementsblogannouncementsunlimited-pto-yes-unlimited-why-sada-is-the-best-place-to-work/], competitive and attractive compensation, performance-based bonuses, paid holidays, rich medical, dental, vision plans, life, short and long-term disability insurance, 401K with match,\\nprofessional development reimbursement program\\n[https://sadasystems.com/blog/news/blogannouncementsprofessional-development-how-we-do-it-why-it-matters/] as well as Google Certified training programs.\\n\\nBusiness Performance: SADA has been named to the INC 5000 Fastest-Growing Private Companies list for 12 years in a row garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers for the past 5 years. The overall culture continues to evolve with engineering at its core: 3200+ projects completed, 3000+ customers served, 10K+ workloads and 25M+ users migrated to the cloud.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "68  Are you ready to step up to the New and take your technology expertise to the next level?\\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements, in conjunction with the way we collaborate, operate and deliver value, provides an unparalleled opportunity to grow and advance. Choose Accenture and make delivering innovative work part of your extraordinary career.\\nPeople in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/ or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward. We partner with our clients to help transform their data into an ‘Appreciating Business Asset.’\\n\\nAs part of our Data Business Group, you will lead technology innovation for our clients through robust delivery of world-class solutions. You will build better software better! There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape. You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Data Scientist, Data Engineer, or Chief Data Officer covering all aspects of Data including Data Management, Data Governance, Data Intelligence, Knowledge Graphs, and IoT. Come grow your career in Technology at Accenture!\\n\\nThe Azure Technical Architect Delivery is responsible for delivering Data On Cloud projects for Azure based deals. The ideal candidate would also be responsible for developing and delivering Azure cloud solutions to meet todays high demand in areas such as AIML, IoT, advanced analytics, open source, enterprise collaboration, microservices, serverless, etc. The Regional Azure Technical Architect is a highly performant Azure Architect responsible for delivering Cloud based Big Data and Analytical Solutions to our clients. Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing a robust, secure and optimized Azure architectures and ability to be hands-on delivering the target solution. This role will work with customers and leading internal data engineers in delivering big data solutions on cloud. Using Azure public cloud technologies, our Technical Architect professionals implement state of the art, scalable, high performance Data On Cloud solutions that meet the need of today's corporate and emerging digital applications.\\n\\nRole & Responsibilities:Work with Sales and Bus Dev teams in providing Azure Technical Architecture expertise while pursuing client opportunities.Ability to build cloud architectures and provide prescriptive guidance across network, storage, Big Data Platform Services, serverless architectures, hadoop ecosystem, vendor products, RDBMS & NoSQL databases and security.Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.Conduct full technical discovery, identifying pain points, business and technical requirements, “as is” and “to be” scenarios.\\n- Assess the full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.Lead the discussions and early analysis of the cloud concepts as it relates to Clients’s Journey to Cloud methodology so that clear use cases are developed and prioritized as well as transitioning these concepts from ideas to proof-of-concepts all the way to production delivery with the full support of the appropriate teams.Compare solution alternatives across both technical and business parameters which support the define cost and service requirements.Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.Understand the strategic direction set by senior management as it relates to team goals.Use considerable judgment to define solution and seeks guidance on complex problems.Prepare and deliver product messaging in an effort to highlight value proposition and unique differentiators.Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers successfully delivering work efforts (if in an independent contributor role) at a client or within Accenture.\\nExtensive travel may be required\\n\\nQualifications\\nBasic Qualifications\\nAt least 5 years of consulting or client service delivery experience on Azure\\nAt least 10 years of experience in big data, database and data warehouse architecture and delivery\\nMinimum of 5 years of professional experience in 2 of the following areas:\\n§ Solution/technical architecture in the cloud\\n§ Big Data/analytics/information analysis/database management in the cloud\\n§ IoT/event-driven/microservices in the cloud\\nExperience with private and public cloud architectures, pros/cons, and migration considerations.\\nExtensive hands-on experience implementing data migration and data processing using Azure services: Networking, Windows/Linux virtual machines, Container, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture, ARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML Studio, AI/ML, etc.\\n Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.\\n - Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc.\\nBachelors or higher degree in Computer Science or a related discipline.\\n\\nCandidate Must Have Completed The Following Certifications\\nMCSA Cloud Platform (Azure) Training & Certification\\nMCSE Cloud Platform & Infratsructiure Training & Certification\\nMCSD Azure Solutions Architect Training & Certification\\n\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions on Azure\\nStrong in Power BI, Java, C##, Spark, PySpark, Unix shell/Perl scripting\\nFamiliarity with the technology stack available in the industry for metadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\n- Multi-cloud experience a plus - Azure, AWS, Google\\n\\nProfessional Skill Requirements\\n Proven ability to build, manage and foster a team-oriented environment\\n Proven ability to work creatively and analytically in a problem-solving environment\\n Desire to work in an information systems environment\\n Excellent communication (written and oral) and interpersonal skills\\n Excellent leadership and management skills\\n Excellent organizational, multi-tasking, and time-management skills\\n Proven ability to work independently\\n\\nAll of our professionals receive comprehensive training covering business acumen, technical and professional skills development. You'll also have opportunities to hone your functional skills and expertise in an area of specialization. We offer a variety of formal and informal training programs at every level to help you acquire and build specialized skills faster. Learning takes place both on the job and through formal training conducted online, in the classroom, or in collaboration with teammates. The sheer variety of work we do, and the experience it offers, provide an unbeatable platform from which to build a career.\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture (i.e., H1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.\\n\\nAccenture is a Federal Contractor and an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "69  The Machine Learning team at RetailMeNot work on initiatives that enable us to provide users meaningful content at the right place, right time, and right price. We build, test and iterate on Machine Learning models, learning from and leveraging datasets with millions of daily visits. We are looking for software engineers who are not afraid of learning new technologies, have an itch or have been playing around with Machine Learning, and are ready to apply it to solve real and meaningful business problems.\\n\\nAs part of the Machine Learning team you will work together with software engineers, data scientist and data engineers leveraging the latest technology and bringing new ideas to the table and building the world’s largest savings destination. Our teams challenge each other to have fun while connecting shoppers to the brands they love. Some of the technologies that we use today include AWS (EMR), Spark, Docker, Luigi, and Tensorflow.\\n\\nThis team is integral to the RetailMeNot business, so we need engineers who can deliver results while understanding the structure of a large system. We provide cross-team leadership that ensures that RetailMeNot code meets a consistently high standard while building the platform to support the future of the company. Your daily activities will involve oversight, mentoring, delivering key pieces of functionality, and collaborating with technology leadership to plan the technical roadmap for RMN.\\n\\nWe are constantly evolving both the software and the teams that deliver it. If you’re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, then we want you to be a part of the team.\\nWho You Are\\nYou have a Bachelor's degree in computer science or equivalent STEM field, or equivalent work experience\\nYou have an ownership mentality and track record of successful high-quality results. You identify any ambiguous requirements and provide clarity when needed.\\nYou bring a proven understanding and application of computer science fundamentals: data structures, algorithms and design patterns.\\nYou’re proficient with technologies such as Java, AWS.\\nYou have an understanding of systems architecture technologies including Linux, Amazon Web Services, Kubernetes and Docker.\\nYou have high level understanding and interest in Machine Learning, and want to work together with scientist and data engineers to apply it to real life.\\nYou have experience with common software engineering tools such as Git, JIRA, TeamCity, Confluence or similar.\\nYou have 5+ years of application development experience.\\nWhat You'll Do\\nYou will work together with data scientist and data engineer to deliver improved algorithms and experiences using Machine Learning.\\nYou will understand and constantly improve the cloud infrastructure that runs our high performance, consumer-scale site.\\nYou will help support the build and deployment pipeline and when necessary both diagnose and solve production support issues.\\nYou will contribute to the overall system design, architecture, security, scalability, reliability, and performance of applications.\\nYou’ll consistently improve maintainability and stability of the codebase.\\nYou’ll coach and mentor junior engineers on software engineering techniques, process, and new technologies.\\nYou will also have the opportunity to stay ahead of new technologies with an eye to evaluating and potentially incorporating them into your team's architecture.\\nWho We Are\\nWe hire smart people and give them the autonomy to be creative in how they impact the business.\\nWe embrace Diversity and Inclusion as core values and have a thriving program to support it throughout the company. We give back to the community and support multiple Austin events and organizations with like-minded goals.\\nWe have an extraordinary open vacation policy.\\nWe offer a generous leave policy for new parents as well as 401k matching.\\nWe provide lunch four days a week, breakfast twice a week, all the snacks you could dream of, and have our own coffee bar run by trained baristas.\\nWe reimburse expenses for cell phone service and gym memberships.\\n\\nRewards*\\nWe offer an opportunity to be an integral part of a company that eagerly pursues disruption in its space to continue to drive innovation and lead the competition. Benefits of being an employee of RetailMeNot, Inc. include, but are not limited to the following:\\nCompetitive base & bonus packages; salary negotiableLong Term Incentive PlanPerformance based rewards & recognition for your hard work and serviceVery competitive benefits packages, including best-in-class parental leaveOpen & flexible PTOCell phone & gym membership reimbursementsFully stocked break room & onsite catered breakfasts & lunches multiple days/week\\nSome rewards do not apply to contract workers or interns.\\n\\nAbout Us\\nRetailMeNot, Inc. is a leading savings destination bringing people and the things they love together through savings with retailers, brands, restaurants and pharmacies. RetailMeNot makes everyday life more affordable through online and in-store coupon codes, cash back offers, discount gift cards, and the RetailMeNot Genie browser extension. Savings are also provided in consumers’ mailboxes through the RetailMeNot Everyday™ direct mail package, and at the pharmacy with RxSaver by RetailMeNot.\\n\\nRetailMeNot is a wholly owned subsidiary of Harland Clarke Holdings. http://www.retailmenot.com/corp or follow @RetailMeNot on social media.\\n\\nU.S. Equal Employment Opportunity/Affirmative Action Information\\nIndividuals seeking employment at RetailMeNot, Inc. are considered without regards to race, color, creed, religion, gender, gender identity, national origin, citizenship, age, sex, marital status, ancestry, physical or mental disability, veteran status, sexual orientation, or any other protected classification. You are being given the opportunity to provide the following information in order to help us align with federal and state Equal Employment Opportunity/Affirmative Action record keeping, reporting, and other legal requirements.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "70  The Data Engineer will be part of the Capture & Analytics Data Services team, which provides the data sources and analytical tools used to help our clients derive maximum value from our Behavioral Analytics and Routing solutions. This position will support the day to day reporting needs of numerous clients, including Business Monitoring, Insights, Product Development, Analysis & Testing, Telephony Integration and others. Responsibilities include troubleshooting issues that occur with existing report deliverables as well as developing new reports and integrating them into the overall service catalog. This role will require development, testing, and configuration management of deliverables in coordination with Data Engineers and Testers within the organization, as well as external vendor and customer resources. Additionally, the Data Engineer will be responsible for maintaining any documentation and training materials required to support the various business units the group serves.\\nThis individual will also support the Data Warehouse to troubleshoot issues relating to the warehouse, especially as they impact the reporting environment.\\n\\nTo summarize, the Data Engineer will be responsible for, but not limited to, the following tasks:\\n\\n1. Troubleshoot and resolve issues as they arise related to all Data Services\\n2. Help to improve automation within the data warehouse and customer environments\\n2. Collaborate with vendor and customer resources to build complex data feeds for reportings\\n3. Manage iteration and release cycles and deployments\\n4. Assist in data modeling and design sessions\\n5. Proactively maintain documentation and training materials\\n\\nBecause of the data-centric culture and rapid growth of NICE-Mattersight, a rich career path exists for the Data Engineer within Mattersight.\\n\\nWhat you definitely have:\\n2–5 years report development experience, specifically with Business Intelligence tools (Tableau preferred)Solid experience with RDBMS applications (SQL Server preferred)Good communication skills and experience working with cross-functional teamsStrong understanding of data warehouse design and implementation best practicesAbility to explain principles of data visualizationSQL programming familiarity in large RDBMS systems (T-SQL preferred)Exposure to ETL and data integration processes\\nWhat we’d also love to see:\\nProject experience preparing use cases and user requirementsDesign and development of micro-services-oriented ArchitectureExposure to reporting interfaces that integrate with popular telephony platforms\\nBe You\\n\\nWe are all different and that is powerful. Variability fuels our business and unites our work. It teaches us that strength lies in differences. To see what matters is our culture, and our culture starts with you.\\n\\nYour different perspectives inspire us to be better. Your diversity fosters creativity and accelerates our innovation. Your unique skills and abilities make us stand out. Your background and experiences help us reach our full potential.\\n\\nWe are committed to a workplace that is increasingly diverse and inclusive, so be your best you.\\n\\nNICE Systems is an Equal Opportunity/Affirmative Action Employer, M/F/D/V.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "71  Data Engineer.\\nTeradata, Hadoop, SQL and Data Analysis, Python R Spark Scala,\\nDebugging, Azure Cloud\\n\\nQualifications\\n\\nB.Tech\\n\\nPrimary Location: US-TX-Austin\\nOrganization: UST USA\\nEmployment Type: Regular Employee\\nJob Type: Full-time\\n Day Job\\nJob Posting: Sep 27, 2019, 11:01:07 AM\\n\\n\\n UST-Global is an Equal Opportunity Employer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "72  Halfaker and Associates, LLC, an award winning high growth small business, creates innovative and customer-centric technology solutions in the areas of Cyber Security, Data Analytics, Software Engineering and IT Infrastructure to improve the health, security and well-being of all Americans. Our commitment to excellence and our vision to “Continue to Serve” has resulted in steady growth and an expanding client base across government agencies in the health, defense, security and intelligence sectors. Headquartered in Arlington, VA, we have employees nationwide and were recently named a 2018 Top Work Place by the Washington Post. Please take a moment to browse through our website and learn more about what it means to serve with Halfaker.\\n\\n\\nHalfaker has an opening for a Senior Data Engineer to join our talented, dynamic team. The key responsibilities for this position are:\\n\\nParticipate in technical research and development to enable continuous innovation within the infrastructure\\nResolve a wide range of hardware and network technology issues in a fast-paced environment\\nFirst escalation resource for Tier-3 troubleshooting for all assigned service desk tickets from creation to resolution\\nEnsure that system hardware, operating systems, software systems, and related procedures adhere to organizational SLAs\\nTroubleshooting ranges from basic issues to email related issues for Executive level users\\nProvisioning, installation/configuration, operation, and maintenance of systems hardware and network infrastructure\\nMaintain standard image for Windows platforms\\nOther tasks and projects as assigned\\n\\n\\nRequired Skills\\nHands on Experience with the Informatica PowerCenter v10x Software Installation and Configuration\\nKnowledge and/or Experience with administration of Informatica Enterprise Data Quality,\\nExperience with administration of Informatica DQ and PC modules.\\nExperience with administration of MDM tool\\nThose without Informatica experience but with experience in Jenkins, Puppet, Ansible or Terraform will also be considered.\\nPowerShell scripting is highly desirable\\nExcellent customer service, communication and organizational skills are required\\nExcellent troubleshooting and problem resolution skills for Desktops and Laptops\\nAbility to work independently with minimal direction providing technical and non-technical support to multiple users\\nCapable of working under pressure, while handling multiple tasks simultaneously\\nExperience providing services to the federal government is preferred\\nTravel may be required\\nAbility to work overtime and weekends required on occasion\\nAbility to sit in an office environment for long periods of time\\nRemote and on-call responsibilities may be required\\n\\n\\n\\nRequired Experience\\nBachelor's degree in computer science, systems engineering, or related technical discipline (8 years of additional relevant experience may be substituted for education)\\n5+ years of relevant experience\\nExperience providing IT support to multiple work sites\\nMS Active Directory administration (Users, Groups, GPO, Security, etc.)\\n\\n\\nHalfaker and Associates, LLC, is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/ Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. U.S. Citizenship is required for most positions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "[73 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Descriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptions_df.to_csv('Descriptions_df_DE_Austin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
